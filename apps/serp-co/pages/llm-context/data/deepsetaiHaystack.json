[
  {
    "owner": "deepset-ai",
    "repo": "haystack",
    "content": "TITLE: PromptNode Pipeline Example\nDESCRIPTION: This code demonstrates how to use PromptNode within a Haystack pipeline to retrieve documents and then answer a question using the retrieved documents as context. It showcases the integration of EmbeddingRetriever and PromptNode.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack.pipelines import PromptNode\ntop_k = 3\nquery = \"Who are the parents of Arya Stark?\"\nretriever = EmbeddingRetriever(...)\npn = PromptNode(model_name_or_path=\"google/flan-t5-base\", prompt_template=\"question-answering\")\n\npipe = Pipeline()\npipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\npipe.add_node(component=pn, name=\"prompt_node\", inputs=[\"Retriever\"])\n\noutput = pipe.run(query=query,\n                  params={\"Retriever\": {\"top_k\": top_k}},\n                  questions=[query for n in range(0, top_k)],\n                  #documents parameter we need for this task will be automatically populated by the retriever\n                  )\n\noutput[\"results\"]\n```\n\n----------------------------------------\n\nTITLE: Agent Configuration in YAML Format\nDESCRIPTION: This snippet shows an example of an Agent configuration defined in a YAML file. It specifies the components, including PromptNodes, PromptModels, SerpAPIComponent, and PythonRuntime, along with their parameters. It also defines a calculator pipeline and an agent, specifying the tools and their descriptions. The YAML configuration allows for easy and declarative agent setup.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3925-mrkl-agent.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nversion: ignore\n\ncomponents:\n  - name: AgentPromptNode\n    type: PromptNode\n    params:\n      model_name_or_path: DavinciModel\n      stop_words: ['Observation:']\n  - name: DavinciModel\n    type: PromptModel\n    params:\n      model_name_or_path: 'text-davinci-003'\n      api_key: 'XYZ'\n  - name: Serp\n    type: SerpAPIComponent\n    params:\n      api_key: 'XYZ'\n  - name: CalculatorInput\n    type: PromptNode\n    params:\n      model_name_or_path: DavinciModel\n      default_prompt_template: CalculatorTemplate\n      output_variable: python_runtime_input\n  - name: Calculator\n    type: PythonRuntime\n  - name: CalculatorTemplate\n    type: PromptTemplate\n    params:\n      name: calculator\n      prompt_text:  |\n          # Write a simple python function that calculates\n          # $query\n          # Do not print the result; invoke the function and assign the result to final_result variable\n          # Start with import statement\n\npipelines:\n  - name: calculator_pipeline\n    nodes:\n      - name: CalculatorInput\n        inputs: [Query]\n      - name: Calculator\n        inputs: [CalculatorInput]\n\nagents:\n  - name: agent\n    params:\n      prompt_node: AgentPromptNode\n      tools:\n        - name: Search\n          pipeline_or_node: Serp\n          description: >\n            useful for when you need to answer questions about current events.\n            You should ask targeted questions\n        - name: Calculator\n          pipeline_or_node: calculator_pipeline\n          description: >\n            useful for when you need to answer questions about math\n```\n\n----------------------------------------\n\nTITLE: Basic Pipeline Evaluation Example in Haystack\nDESCRIPTION: This snippet demonstrates a basic evaluation of a Haystack pipeline using the `eval` function. It takes a pipeline, input data, and expected output data as input, calculates metrics using Semantic Answer Similarity (SAS), and saves the results to a CSV file. This provides a high-level overview of how to evaluate pipelines in Haystack 2.0.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npipe = Pipeline()\n...\ninputs = [{\"component_name\": {\"query\": \"some question\"}, ...}, ...]\nexpected_output = [{\"another_component_name\": {\"answer\": \"42\"}, ...}, ...]\nresult = eval(pipe, inputs=inputs, expected_output=expected_output)\nmetrics = result.calculate_metrics(Metric.SAS)\nmetrics.save(\"path/to/file.csv\")\n```\n\n----------------------------------------\n\nTITLE: RAG Pipeline Definition with Haystack 2.0 Components (Python)\nDESCRIPTION: This code defines a Retrieval-Augmented Generation (RAG) pipeline using Haystack 2.0 components. It showcases the use of MemoryRetriever, PromptBuilder, ChatGPTGenerator, and RepliesToAnswersConverter components within a Pipeline object. The pipeline is configured to connect these components and process questions to generate answers.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5540-llm-support-2.0.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack.preview.components import MemoryRetriever, PromptBuilder, ChatGPTGenerator, RepliesToAnswersConverter\nfrom haystack.preview.document_stores import MemoryDocumentStore\nfrom haystack.preview.pipeline import Pipeline\n\npipe = Pipeline()\npipe.add_store(\"store\", MemoryDocumentStore())\npipe.add_component(\"retriever\", MemoryRetriever(), store=\"store\")\npipe.add_component(\"prompt_builder\", PromptBuilder(\"deepset/question-answering\"))\npipe.add_component(\"llm\", GPT4Generator(api_key=\"...\"))\npipe.add_component(\"replies_converter\", RepliesToAnswersConverter())\n\npipe.connect(\"retriever\", \"prompt_builder\")\npipe.connect(\"prompt_builder\", \"llm\")\npipe.connect(\"llm\", \"replies_converter\")\n\nquestions = [\"Why?\", \"Why not?\"]\nresults = pipe.run({\n\t\"retriever\": {\"queries\": questions},\n\t\"prompt_builder\": {\"questions\": questions},\n})\n\nassert results == {\n\t\"replies_converter\": {\n    \"answers\": [[Answer(\"Because of this.\")], [Answer(\"Because of that.\")]]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: PromptBuilder Component Definition (Python)\nDESCRIPTION: This code defines the structure for the PromptBuilder component in Haystack 2.0.  It takes a prompt template (either a string or a path to a file), extracts the variables from the template, and sets the input parameters of the component based on these variables. The `run` method renders the template using the input variables and returns a list of prompts.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5540-llm-support-2.0.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@component\nclass PromptBuilder:\n\n    def __init__(self, template: Union[str, Path]):\n        self.template = # Download the template\n        template_variables = # extracts the variables from the template text\n\t\tcomponent.set_input_parameters(**{var: Any for var in template_variables})\n\n  \t@component.output_types(prompts=List[str])\n    def run(self, **kwargs):\n        # Render the template using the variables\n        return {\"prompts\": prompts}\n```\n\n----------------------------------------\n\nTITLE: Programmatic Agent Creation in Haystack\nDESCRIPTION: This snippet demonstrates how to create an Agent programmatically in Haystack using Python. It initializes a search component (SerpAPIComponent), a prompt model (PromptModel), and a calculator pipeline. It then adds these tools to the Agent, enabling it to perform tasks that require web searches and calculations. The example shows how to run the agent with a query.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3925-mrkl-agent.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsearch = SerpAPIComponent(api_key=os.environ.get(\"SERPAPI_API_KEY\"), name=\"Serp\", inputs=[\"Query\"])\n\nprompt_model=PromptModel(model_name_or_path=\"text-davinci-003\", api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\ncalculator = Pipeline()\ncalculator.add_node(PromptNode(\n    model_name_or_path=prompt_model,\n    default_prompt_template=PromptTemplate(prompt_text=\"Write a simple python function that calculates...\"),\n    output_variable=\"python_runtime_input\")  # input\ncalculator.add_node(PythonRuntime())  # actual calculator\n\nprompt_node = PromptNode(\n    model_name_or_path=prompt_model,\n    stop_words=[\"Observation:\"]\n)\n\nagent = Agent(prompt_node=prompt_node)\n# Nodes and pipelines can be added as tools to the agent. Just as nodes can be added to pipelines with add_node()\nagent.add_tool(\"Search\", search, \"useful for when you need to answer questions about current events. You should ask targeted questions\")\nagent.add_tool(\"Calculator\", calculator, \"useful for when you need to answer questions about math\")\n\nresult = agent.run(\"What is 2 to the power of 3?\")\n```\n\n----------------------------------------\n\nTITLE: DeepEvalEvaluator Pipeline Integration - Python\nDESCRIPTION: This code demonstrates how to integrate the DeepEvalEvaluator component into a Haystack pipeline for evaluating RAG outputs. It shows how to add the component to the pipeline, specify the metric and its parameters, and run the pipeline with questions, contexts, and answers as input. The expected output is a DeepEvalResult object containing the metric and its score.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6784-integrations-for-eval-framworks.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\np = Pipeline()\np.add_component(instance=DeepEvalEvaluator(metric=\"Hallucination\", params={\"threshold\": 0.3)}, name=\"evaluator\"))\n# p.add_component(instance=RagasEvaluator()...\n\nquestions = [...]\ncontexts = [...]\nanswers = [...]\n\np.run({\"evaluator\": {\"questions\": questions, \"context\": contexts, \"answer\": answers})\n# {\"evaluator\": DeepEvalResult(metric='hallucination', score=0.817)}\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Pipeline in Python\nDESCRIPTION: This code creates a Haystack pipeline, adds `AddValue` and `Double` nodes to it, connects them, and runs the pipeline with an initial value of 1. Parameters are passed during the `add_node` and `run` stages. The final result is asserted to be 18.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4284-drop-basecomponent.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline()\n\n# Nodes can be initialized as standalone objects.\n# These instances can be added to the Pipeline in several places.\naddition = AddValue(add=1)\n\n# Nodes are added with a name and an node. Note the lack of references to any other node.\npipeline.add_node(\"first_addition\", addition, parameters={\"add\": 3})  # Nodes can store default parameters per node.\npipeline.add_node(\"second_addition\", addition)  # Note that instances can be reused\npipeline.add_node(\"double\", Double())\n\n# Nodes are the connected as input node: [list of output nodes]\npipeline.connect(connect_from=\"first_addition\", connect_to=\"double\")\npipeline.connect(connect_from=\"double\", connect_to=\"second_addition\")\n\npipeline.draw(\"pipeline.png\")\n\n# Pipeline.run() accepts 'data' and 'parameters' only. Such dictionaries can contain\n# anything, depending on what the first node(s) of the pipeline requires.\n# Pipeline does not validate the input: the first node(s) should do so.\nresults = pipeline.run(\n    data={\"value\": 1},\n    parameters = {\"second_addition\": {\"add\": 10}}   # Parameters can be passed at this stage as well\n)\nassert results == {\"value\": 18}\n```\n\n----------------------------------------\n\nTITLE: Selecting Default Prompt Template\nDESCRIPTION: This code snippet shows how to select a specific default template for a task using `use_prompt_template` and then subsequently use it. It generates a question using the specified template and provided documents.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\t  qa = pn.use_prompt_template(\"deepset/question-generation-v2\")\n      qa(documents=[\"Berlin is the capital of Germany.\"])\n```\n\n----------------------------------------\n\nTITLE: Building Indexing and Query Pipelines in Haystack\nDESCRIPTION: This code snippet demonstrates how to construct indexing and query pipelines in Haystack 2.0. It showcases the usage of new nodes like `DocumentEmbedder`, `StringEmbedder`, and `DocumentWriter` to decouple retrieval from document stores. This approach avoids tight coupling between `DocumentStore`s and `Retriever`s, allowing for more flexible and maintainable pipelines.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4370-documentstores-and-retrievers.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack import Pipeline\nfrom haystack.nodes import (\n    TxtConverter,\n    PreProcessor,\n    DocumentWriter,\n    DocumentEmbedder,\n    StringEmbedder,\n    MemoryRetriever,\n    Reader,\n)\nfrom haystack.document_stores import MemoryDocumentStore\n\ndocstore = MemoryDocumentStore()\n\nindexing_pipe = Pipeline()\nindexing_pipe.add_store(\"document_store\", docstore)\nindexing_pipe.add_node(\"txt_converter\", TxtConverter())\nindexing_pipe.add_node(\"preprocessor\", PreProcessor())\nindexing_pipe.add_node(\"embedder\", DocumentEmbedder(model_name=\"deepset/model-name\"))\nindexing_pipe.add_node(\"writer\", DocumentWriter(store=\"document_store\"))\nindexing_pipe.connect(\"txt_converter\", \"preprocessor\")\nindexing_pipe.connect(\"preprocessor\", \"embedder\")\nindexing_pipe.connect(\"embedder\", \"writer\")\n\nindexing_pipe.run(...)\n\nquery_pipe = Pipeline()\nquery_pipe.add_store(\"document_store\", docstore)\nquery_pipe.add_node(\"embedder\", StringEmbedder(model_name=\"deepset/model-name\"))\nquery_pipe.add_node(\"retriever\", MemoryRetriever(store=\"document_store\", retrieval_method=\"embedding\"))\nquery_pipe.add_node(\"reader\", Reader(model_name=\"deepset/model-name\"))\nquery_pipe.connect(\"embedder\", \"retriever\")\nquery_pipe.connect(\"retriever\", \"reader\")\n\nresults = query_pipe.run(...)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Pipeline with Multiple Input Components (Python)\nDESCRIPTION: Illustrates evaluating a Haystack pipeline with multiple input components ('foo' and 'bar'), both of which accept an 'input_query'.  The code shows the required explicit specification of the component name to avoid ambiguity.  This ensures the correct input is passed to each component during evaluation.  A dictionary is used to map each component to its specific input.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ninput = {\n  \"foo\": {\"input_query\": \"This my input query\"},\n  \"bar\": {\"input_query\": \"This my input query\"}\n}\n```\n\n----------------------------------------\n\nTITLE: Pipeline Configuration JSON\nDESCRIPTION: This JSON configuration file demonstrates the structure for defining dependencies, stores, nodes, and pipelines. It includes configurations for sparse and dense retrieval, ranking, and reading components.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4284-drop-basecomponent.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    # A list of \"dependencies\" for the application.\n    # Used to ensure all external nodes are present when loading.\n    \"dependencies\" : [\n        \"haystack == 2.0.0\",\n        \"my_custom_node_module == 0.0.1\",\n    ],\n\n    # Stores are defined here, outside single pipeline graphs.\n    # All pipelines have access to all these docstores.\n    \"stores\": {\n        # Nodes will be able to access them by the name defined here,\n        # in this case `my_first_store` (see the retrievers below).\n        \"my_first_store\": {\n            # class_name is mandatory\n            \"class_name\": \"InMemoryDocumentStore\",\n            # Then come all the additional parameters for the store\n            \"use_bm25\": true\n        },\n        \"my_second_store\": {\n            \"class_name\": \"InMemoryDocumentStore\",\n            \"use_bm25\": false\n        }\n    },\n\n    # Nodes are defined here, outside single pipeline graphs as well.\n    # All pipelines can use these nodes. Instances are re-used across\n    # Pipelines if they happen to share a node.\n    \"nodes\": {\n        # In order to reuse an instance across multiple nodes, instead\n        # of a `class_name` there should be a pointer to another node.\n        \"my_sparse_retriever\": {\n            # class_name is mandatory, unless it's a pointer to another node.\n            \"class_name\": \"BM25Retriever\",\n            # Then come all the additional init parameters for the node\n            \"store_name\": \"my_first_store\",\n            \"top_k\": 5\n        },\n        \"my_dense_retriever\": {\n            \"class_name\": \"EmbeddingRetriever\",\n            \"model_name\": \"deepset-ai/a-model-name\",\n            \"store_name\": \"my_second_store\",\n            \"top_k\": 5\n        },\n        \"my_ranker\": {\n            \"class_name\": \"Ranker\",\n            \"inputs\": [\"documents\", \"documents\"],\n            \"outputs\": [\"documents\"],\n        },\n        \"my_reader\": {\n            \"class_name\": \"Reader\",\n            \"model_name\": \"deepset-ai/another-model-name\",\n            \"top_k\": 3\n        }\n    },\n\n    # Pipelines are defined here. They can reference all nodes above.\n    # All pipelines will get access to all docstores\n    \"pipelines\": {\n        \"sparse_question_answering\": {\n            # Mandatory list of edges. Same syntax as for `Pipeline.connect()`\n            \"edges\": [\n                (\"my_sparse_retriever\", [\"reader\"])\n            ],\n            # To pass some parameters at the `Pipeline.add_node()` stage, add them here.\n            \"parameters\": {\n                \"my_sparse_retriever\": {\n                    \"top_k\": 10\n                }\n            },\n            # Metadata can be very valuable for dC and to organize larger Applications\n            \"metadata\": {\n                \"type\": \"question_answering\",\n                \"description\": \"A test pipeline to evaluate Sparse QA.\",\n                \"author\": \"ZanSara\"\n            },\n            # Other `Pipeline.__init__()` parameters\n            \"max_allowed_loops\": 10,\n        },\n        \"dense_question_answering\": {\n            \"edges\": [\n                (\"my_dense_retriever\", [\"reader\"])\n            ],\n            \"metadata\": {\n                \"type\": \"question_answering\",\n                \"description\": \"A test pipeline to evaluate Sparse QA.\",\n                \"author\": \"an_intern\"\n            }\n        },\n        \"hybrid_question_answering\": {\n            \"edges\": [\n                (\"my_sparse_retriever\", [\"ranker\"]),\n                (\"my_dense_retriever\", [\"ranker\"]),\n                (\"ranker\", [\"reader\"]),\n            ],\n            \"metadata\": {\n                \"type\": \"question_answering\",\n                \"description\": \"A test pipeline to evaluate Hybrid QA.\",\n                \"author\": \"the_boss\"\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Full Agent Trace Example\nDESCRIPTION: This snippet presents a complete interaction trace of the MRKLAgent, including the initial question, the LLM's 'Thought', chosen 'Action', 'Action Input', and the 'Observation' from the invoked tool (Search or Calculator). It demonstrates how the agent iteratively answers the question by breaking it down into sub-questions and utilizing different tools.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3925-mrkl-agent.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nAnswer the following questions as best as you can. You have access to the following tools:\n\nSearch: useful for when you need to answer questions about current events. You should ask targeted questions\nCalculator: useful for when you need to answer questions about math\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Search, Calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought: I need to do some research to answer this question.\nAction: Search\nAction Input: Olivia Wilde's boyfriend\nObservation: First linked in November 2011, Wilde and Sudeikis got engaged in January 2013. They later became parents, welcoming son Otis in 2014 and daughter Daisy in 2016.\nThought: I need to find out his age\nAction: Search\nAction Input: Jason Sudeikis age\nObservation: 47 years\nThought: I need to raise it to the 0.23 power\nAction: Calculator\nAction Input: 47^0.23\nObservation: 2.4242784855673896\nThought: I now know the final answer\nFinal Answer: Jason Sudeikis, Olivia Wilde's boyfriend, is 47 years old and his age raised to the 0.23 power is 2.4242784855673896.\n```\n\n----------------------------------------\n\nTITLE: HFTextEmbedder Component Using EmbeddingBackend in Python\nDESCRIPTION: This code snippet shows how the `HFTextEmbedder` component uses the `HFEmbeddingBackend` to compute embeddings. The `warm_up` method initializes the `embedding_backend` instance, and the `run` method then utilizes the backend to embed input strings. It returns a dictionary containing the list of numpy arrays representing the embeddings.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5390-embedders.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@component\nclass HFTextEmbedder:\n\n    def __init__(self, model_name: str, ... init params ...):\n        self.model_name = model_name\n        self.model_params = ... params ...\n\n    def warm_up(self):\n        self.embedding_backend = HFEmbeddingBackend(self.model_name, **self.model_params)\n\n    @component.output_types(result=List[np.ndarray])\n    def run(self, strings: List[str]):\n        return {\"result\": self.embedding_backend.embed(data)}\n```\n\n----------------------------------------\n\nTITLE: Haystack Pipeline with OpenAI Embedders in Python\nDESCRIPTION: This example demonstrates how to set up indexing and query pipelines using Haystack components and OpenAI embedders. It shows the configuration of TxtConverter, PreProcessor, DocumentWriter, OpenAITextEmbedder, OpenAIDocumentEmbedder, MemoryRetriever and Reader, connecting components within the pipelines.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5390-embedders.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack import Pipeline\nfrom haystack.components import (\n    TxtConverter,\n    PreProcessor,\n    DocumentWriter,\n    OpenAITextEmbedder,\n    OpenAIDocumentEmbedder,\n    MemoryRetriever,\n    Reader,\n)\nfrom haystack.document_stores import MemoryDocumentStore\ndocstore = MemoryDocumentStore()\n\nindexing_pipe = Pipeline()\nindexing_pipe.add_store(\"document_store\", docstore)\nindexing_pipe.add_node(\"txt_converter\", TxtConverter())\nindexing_pipe.add_node(\"preprocessor\", PreProcessor())\nindexing_pipe.add_node(\"embedder\", OpenAIDocumentEmbedder(model_name=\"text-embedding-ada-002\"))\nindexing_pipe.add_node(\"writer\", DocumentWriter(store=\"document_store\"))\nindexing_pipe.connect(\"txt_converter\", \"preprocessor\")\nindexing_pipe.connect(\"preprocessor\", \"embedder\")\nindexing_pipe.connect(\"embedder\", \"writer\")\n\nindexing_pipe.run(...)\n\nquery_pipe = Pipeline()\nquery_pipe.add_store(\"document_store\", docstore)\nquery_pipe.add_node(\"embedder\", OpenAITextEmbedder(model_name=\"text-embedding-ada-002\"))\nquery_pipe.add_node(\"retriever\", MemoryRetriever(store=\"document_store\", retrieval_method=\"embedding\"))\nquery_pipe.add_node(\"reader\", Reader(model_name=\"deepset/model-name\"))\nquery_pipe.connect(\"embedder\", \"retriever\")\nquery_pipe.connect(\"retriever\", \"reader\")\n\nresults = query_pipe.run(...)\n```\n\n----------------------------------------\n\nTITLE: Basic Shaper Pipeline Example - Python\nDESCRIPTION: This code demonstrates a basic Haystack pipeline using the Shaper component to rename the 'query' variable to 'questions' in the invocation context. It loads the pipeline from a YAML configuration, runs a query, and asserts that the 'questions' variable is present and of the correct type in the result's metadata.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3784-shaper.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    from haystack import Pipeline, Document\n\n    with open(\"tmp_config.yml\", \"w\") as tmp_file:\n        tmp_file.write(\n            f\"\"\"\n            version: ignore\n            components:\n            - name: shaper\n              params:\n                inputs:\n                    query:\n                      output: questions\n              type: Shaper\n            pipelines:\n              - name: query\n                nodes:\n                  - name: shaper\n                    inputs:\n                      - Query\n        \"\"\"\n        )\n    pipeline = Pipeline.load_from_yaml(path=\"tmp_config.yml\")\n    result = pipeline.run(\n        query=\"What can you tell me about Berlin?\",\n        documents=[Document(\"Berlin is an amazing city.\"), Document(\"I love Berlin.\")],\n    )\n    assert result\n    # query has been renamed to questions\n    assert isinstance(result[\"meta\"][\"invocation_context\"][\"questions\"], str)\n\n```\n\n----------------------------------------\n\nTITLE: HFDocumentEmbedder Component Using EmbeddingBackend in Python\nDESCRIPTION: This snippet illustrates how the `HFDocumentEmbedder` component leverages the `HFEmbeddingBackend` for embedding `Document` objects.  The `warm_up` method initializes `embedding_backend`. The `run` method extracts text content, computes embeddings, and augments `Document` objects with these embeddings, returning the modified document list.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5390-embedders.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@component\nclass HFDocumentEmbedder:\n\n    def __init__(self, model_name: str, ... init params ...):\n        self.model_name = model_name\n        self.model_params = ... params ...\n\n    def warm_up(self):\n        self.embedding_backend = HFEmbeddingBackend(self.model_name, **self.model_params)\n\n    @component.output_types(result=List[Document])\n    def run(self, documents: List[Document]):\n        text_strings = [document.content for document in data]\n        embeddings = self.embedding_backend.embed(text_strings)\n        documents_with_embeddings = [Document.from_dict(**doc.to_dict, \"embedding\": emb) for doc, emb in zip(documents, embeddings)]\n        return {\"result\": documents_with_embeddings}\n```\n\n----------------------------------------\n\nTITLE: Loading Agent from YAML in Haystack\nDESCRIPTION: This snippet demonstrates how to load an Agent from a YAML file using the `Agent.load_from_yaml` method in Haystack. This allows for configuring and deploying agents by simply pointing to a YAML configuration, enabling easy setup and reproducibility.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3925-mrkl-agent.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nagent = Agent.load_from_yaml(\n    \"test.mrkl.haystack-pipeline.yml\", agent_name=\"agent\"\n)\n```\n\n----------------------------------------\n\nTITLE: TableReader Basic Usage with Pandas DataFrame\nDESCRIPTION: This example demonstrates how to use the TableReader with a pandas DataFrame and shows how to access the answer context using row and column indices after the introduction of the TableCell dataclass. It showcases a basic table question answering pipeline using Haystack's TableReader.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3875-table-cell.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom haystack.nodes import TableReader\nfrom haystack import Document\n\ndata = {\n    \"actors\": [\"brad pitt\", \"leonardo di caprio\", \"george clooney\"],\n    \"age\": [\"58\", \"47\", \"60\"],\n    \"number of movies\": [\"87\", \"53\", \"69\"],\n    \"date of birth\": [\"18 december 1963\", \"11 november 1974\", \"6 may 1961\"],\n}\ntable_doc = Document(content=pd.DataFrame(data), content_type=\"table\")\nreader = TableReader(model_name_or_path=\"google/tapas-base-finetuned-wtq\", max_seq_len=128)\nprediction = reader.predict(query=\"Who was in the most number of movies?\", documents=[table_doc])\nanswer = prediction[\"answers\"][0]\n\n# New feature\n# answer.context -> [[\"actor\", \"age\", \"number of movies\"], [\"Brad Pitt\",...], [...]]\n# answer.offsets_in_context[0] -> (row=1, col=1)\nprint(answer.context[answer.offsets_in_context[0].row][answer.offsets_in_context[0].col])\n```\n\n----------------------------------------\n\nTITLE: Evaluating Pipeline with Explicit Component Specification (Python)\nDESCRIPTION: Demonstrates how to evaluate a Haystack pipeline when multiple components ('foo' and 'bar') both expect the same input 'query'. The explicit component name is specified to ensure the correct input is passed to each component, preventing evaluation failure. This is necessary when ambiguity exists due to multiple components sharing input names.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\neval(pipe, {\"foo\": {\"query\": \"This is the query\"}, \"bar\": {\"query\": \"This is the query\"}})\n```\n\n----------------------------------------\n\nTITLE: Defining PromptTemplate Class\nDESCRIPTION: This code snippet defines the `PromptTemplate` class, which represents an NLP prompt template. It includes attributes for the template name, prompt text, and input variables.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n      class PromptTemplate(BaseTemplate):\n\n          name: str\n          prompt_text: str\n          input_variables: List[str]\n```\n\n----------------------------------------\n\nTITLE: Document Class Definition in Haystack 2.0 (Python)\nDESCRIPTION: Defines the `Document` class using Python's `dataclass` decorator.  It includes fields for `id`, `text`, `array` (numpy.ndarray), `dataframe` (pandas.DataFrame), `blob` (bytes), `mime_type`, `metadata` (dictionary), `id_hash_keys`, `score`, and `embedding` (numpy.ndarray).  The class also includes methods for serialization to dictionary and JSON, and deserialization from dictionary and JSON. It leverages `dataclass` features like default factories and frozen instances.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5738-document-2.0.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dataclass(frozen=True)\nclass Document:\n    id: str = field(default_factory=str)\n    text: Optional[str] = field(default=None)\n    array: Optional[numpy.ndarray] = field(default=None)\n    dataframe: Optional[pandas.DataFrame] = field(default=None)\n    blob: Optional[bytes] = field(default=None)\n    mime_type: str = field(default=\"text/plain\")\n    metadata: Dict[str, Any] = field(default_factory=dict, hash=False)\n    id_hash_keys: List[str] = field(default_factory=lambda: [\"text\", \"array\", \"dataframe\", \"blob\"], hash=False)\n    score: Optional[float] = field(default=None, compare=True)\n    embedding: Optional[numpy.ndarray] = field(default=None, repr=False)\n\n    def to_dict(self):\n        \"\"\"\n        Saves the Document into a dictionary.\n        \"\"\"\n\n    def to_json(self, json_encoder: Optional[Type[DocumentEncoder]] = None, **json_kwargs):\n        \"\"\"\n        Saves the Document into a JSON string that can be later loaded back. Drops all binary data from the blob field.\n        \"\"\"\n\n    @classmethod\n    def from_dict(cls, dictionary):\n        \"\"\"\n        Creates a new Document object from a dictionary of its fields.\n        \"\"\"\n\n    @classmethod\n    def from_json(cls, data, json_decoder: Optional[Type[DocumentDecoder]] = None, **json_kwargs):\n        \"\"\"\n        Creates a new Document object from a JSON string.\n        \"\"\"\n\n    def flatten(self) -> Dict[str, Any]:\n        \"\"\"\n        Returns a dictionary with all the document fields and metadata on the same level.\n        Helpful for filtering in document stores.\n        \"\"\"\n\n```\n\n----------------------------------------\n\nTITLE: Pipeline with MetaFieldRanker in Haystack (Python)\nDESCRIPTION: This snippet illustrates how to integrate the MetaFieldRanker into a Haystack pipeline. It showcases the addition of an InMemoryBM25Retriever and the MetaFieldRanker, connecting them to enable document retrieval followed by ranking based on the 'rating' meta field.  The retriever fetches documents and passes them to the ranker for sorting.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6141-meta-field-ranker.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline()\npipeline.add_component(component=InMemoryBM25Retriever(document_store=document_store, top_k=20)\n, name=\"Retriever\")\npipeline.add_component(component=MetaFieldRanker(meta_field=\"rating\"), name=\"Ranker\")\npipeline.connect(\"Retriever.documents\", \"MetaFieldRanker.documents\")\n```\n\n----------------------------------------\n\nTITLE: Simulated Output Example for Evaluation\nDESCRIPTION: This code shows how to simulate outputs of intermediate components during evaluation. The `simulated_output` dictionary contains the desired outputs for specific components in the pipeline. This allows for targeted evaluation of specific pipeline stages or components and enables testing of the pipeline with known intermediate results. The keys are component names, and the values are dictionaries representing the expected outputs of those components.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsimulated_output = {\n  \"component_name\": {\"answer\": \"120\"},\n  \"another_component_name\": {\"metadata\": {\"id\": 1}}\n}\n```\n\n----------------------------------------\n\nTITLE: Shaper Component Configuration Example - YAML\nDESCRIPTION: This YAML configuration defines a Shaper component within a Haystack pipeline. It specifies how to modify the 'query' and 'documents' variables in the invocation context.  The 'query' variable is expanded to 'questions' using the 'expand' function, while the 'documents' variable is concatenated.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3784-shaper.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n            components:\n            - name: shaper\n              params:\n                inputs:\n                   query:\n                      func: expand\n                      output: questions\n                      params:\n                        expand_target: query\n                        size:\n                          func: len\n                          params:\n                            - documents\n                   documents:\n                      func: concat\n                      params:\n                        docs: documents\n                        delimiter: \" \"\n              type: Shaper\n            pipelines:\n              - name: query\n                nodes:\n                  - name: shaper\n                    inputs:\n                      - Query\n\n```\n\n----------------------------------------\n\nTITLE: Integrating RecentnessRanker in Haystack Pipeline (YAML)\nDESCRIPTION: This YAML configuration demonstrates how to integrate the RecentnessRanker node into a Haystack pipeline. It defines the pipeline structure, including retrievers, rankers, and a prompt node. The RecentnessRanker is configured with a `date_identifier` to specify the metadata field containing the document date, `top_k` to limit the number of returned documents, and `method` to choose the ranking algorithm.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5289-recentness-ranker.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nversion: '1.18.0'\nname: 'Example pipeline'\n\ncomponents:\n- name: DocumentStore\n  type: DeepsetCloudDocumentStore\n- name: EmbeddingRetriever\n  type: EmbeddingRetriever\n  params:\n    document_store: DocumentStore\n    embedding_model: [embedding model here]\n    model_format: sentence_transformers\n    top_k: 30\n- name: BM25Retriever\n  type: BM25Retriever\n  params:\n    document_store: DocumentStore\n    top_k: 30\n- name: JoinDocuments\n  type: JoinDocuments\n  params:\n    top_k_join: 30\n    join_mode: reciprocal_rank_fusion\n- name: Ranker\n  type: SentenceTransformersRanker\n  params:\n    model_name_or_path: [cross-encoder model here]\n    top_k: 15\n- name: RecentnessRanker\n  type: RecentnessRanker\n  params:\n    date_identifier: release_date\n    top_k: 3\n    method: score\n- name: qa_template\n  type: PromptTemplate\n  params:\n    output_parser:\n        type: AnswerParser\n    prompt: \"prompt text here\"\n- name: PromptNode\n  type: PromptNode\n  params:\n    default_prompt_template: qa_template\n    max_length: 300\n    model_kwargs:\n      temperature: 0\n    model_name_or_path: gpt-3.5-turbo\n- name: FileTypeClassifier\n  type: FileTypeClassifier\n- name: TextConverter\n  type: TextConverter\n- name: PDFConverter\n  type: PDFToTextConverter\n- name: Preprocessor\n  params:\n    language: en\n    split_by: word\n    split_length: 200\n    split_overlap: 10\n    split_respect_sentence_boundary: true\n  type: PreProcessor\n\npipelines:\n- name: query\n  nodes:\n    - name: EmbeddingRetriever\n      inputs: [Query]\n    - name: BM25Retriever\n      inputs: [Query]\n    - name: JoinDocuments\n      inputs: [EmbeddingRetriever, BM25Retriever]\n    - name: Ranker\n      inputs: [JoinDocuments]\n    - name: RecentnessRanker\n      inputs: [Ranker]\n    - name: PromptNode\n      inputs: [RecentnessRanker]\n\n- name: indexing\n  nodes:\n  - inputs:\n    - File\n    name: FileTypeClassifier\n  - inputs:\n    - FileTypeClassifier.output_1\n    name: TextConverter\n  - inputs:\n    - FileTypeClassifier.output_2\n    name: PDFConverter\n  - inputs:\n    - TextConverter\n    - PDFConverter\n    name: Preprocessor\n  - inputs:\n    - Preprocessor\n    name: EmbeddingRetriever\n  - inputs:\n    - EmbeddingRetriever\n    name: DocumentStore\n```\n\n----------------------------------------\n\nTITLE: Defining FAQ Indexing Pipeline in YAML - Haystack\nDESCRIPTION: This YAML configuration defines an FAQ indexing pipeline in Haystack. It includes components like ElasticsearchDocumentStore, EmbeddingRetriever, and CsvTextConverter. The indexing pipeline processes CSV files containing question-answer pairs, retrieves embeddings for the questions, and stores the documents in the document store. It leverages sentence-transformers/all-MiniLM-L6-v2 for embeddings and requires Elasticsearch to be running locally.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3550-csv-converter.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# To allow your IDE to autocomplete and validate your YAML pipelines, name them as <name of your choice>.haystack-pipeline.yml\n\nversion: ignore\n\ncomponents:    # define all the building-blocks for Pipeline\n  - name: DocumentStore\n    type: ElasticsearchDocumentStore\n    params:\n      host: localhost\n      embedding_field: question_emb\n      embedding_dim: 384\n      excluded_meta_data:\n        - question_emb\n      similarity: cosine\n  - name: Retriever\n    type: EmbeddingRetriever\n    params:\n      document_store: DocumentStore    # params can reference other components defined in the YAML\n      embedding_model: sentence-transformers/all-MiniLM-L6-v2\n      scale_score: False\n  - name: CSVConverter\n    type: CsvTextConverter\n\npipelines:\n  - name: indexing\n    nodes:\n      - name: CSVConverter\n        inputs: [File]\n      - name: Retriever\n        inputs: [ CSVConverter ]\n      - name: DocumentStore\n        inputs: [ Retriever ]\n```\n\n----------------------------------------\n\nTITLE: MemoryDocumentStore API example\nDESCRIPTION: This code snippet showcases the API that a `MemoryDocumentStore` could offer. It includes the basic contract methods and also custom methods like `bm25_retrieval` and `vector_similarity_retrieval` that are specific to the `MemoryDocumentStore`. These custom methods are used by specialized `Retriever` nodes like `MemoryRetriever`.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4370-documentstores-and-retrievers.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MemoryDocumentStore:\n\n    def filter_documents(self, filters: Dict[str, Any], **kwargs) -> List[Document]:\n        ...\n\n    def write_documents(self, documents: List[Document], **kwargs) -> None:\n        ...\n\n    def delete_documents(self, ids: List[str], **kwargs) -> None:\n        ...\n\n    def bm25_retrieval(\n        self,\n        queries: List[str],   # Note: takes strings!\n        filters: Optional[Dict[str, Any]] = None,\n        top_k: int = 10\n    ) -> List[List[Document]]:\n        ...\n\n    def vector_similarity_retrieval(\n        self,\n        queries: List[np.array],   # Note: takes embeddings!\n        filters: Optional[Dict[str, Any]] = None,\n        top_k: int = 10\n    ) -> List[List[Document]]:\n        ...\n\n    def knn_retrieval(\n        self,\n        queries: List[np.array],   # Note: takes embeddings!\n        filters: Optional[Dict[str, Any]] = None,\n        top_k: int = 10\n    ) -> List[List[Document]]:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Haystack Pipeline Definition using YAML\nDESCRIPTION: This YAML snippet illustrates how to integrate the `JsonConverter` node into a Haystack pipeline definition. The pipeline includes a `JsonConverter` node that takes a file as input, followed by a Retriever and a DocumentStore node.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3959-json-converter.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n...\n\npipelines:\n  - name: indexing\n    nodes:\n      - name: JsonConverter\n        inputs: [File]\n      - name: Retriever\n        inputs: [JsonConverter]\n      - name: DocumentStore\n        inputs: [Retriever]\n```\n\n----------------------------------------\n\nTITLE: PromptNode YAML Configuration\nDESCRIPTION: This YAML configuration defines a Haystack pipeline with two PromptNode components, a PromptModel, a PromptTemplate, and a retriever. It showcases how to configure the pipeline declaratively, reuse the PromptModel instance, and define a custom prompt template.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\ncomponents:\n\n  # can go in pipeline\n  - name: prompt_node\n    params:\n      prompt_template: template\n      model_name_or_path: model\n      output_variable: \"questions\"\n    type: PromptNode\n\n  # can go in pipeline\n  - name: prompt_node_2\n    params:\n      prompt_template: \"question-answering\"\n      model_name_or_path: deepset/model-name\n    type: PromptNode\n\n  # not in pipeline - only needed if you're reusing the model across multiple PromptNode in a pipeline\n  # and hidden from users in the Python beginner world\n  - name: model\n    params:\n      model_name_or_path: google/flan-t5-xl\n    type: PromptModel\n\n  # not in pipeline\n  - name: template\n    params:\n      name: \"question-generation-v2\"\n      prompt_text: \"Given the following $documents, please generate a question. Question:\"\n      input_variables: documents\n    type: PromptTemplate\n\npipelines:\n  - name: question-generation-answering-pipeline\n    nodes:\n      - name: EmbeddingRetriever\n        inputs: [Query]\n      - name: prompt_node\n        inputs: [EmbeddingRetriever]\n      - name: prompt_node_2\n        inputs: [prompt_node]\n```\n\n----------------------------------------\n\nTITLE: Defining HFTextEmbedder and HFDocumentEmbedder Components in Python\nDESCRIPTION: This code defines the structure of `HFTextEmbedder` and `HFDocumentEmbedder` components in Python using decorators.  The `HFTextEmbedder` is for embedding strings into vectors and the `HFDocumentEmbedder` embeds `Document` objects.  The `run` methods are shown with their input and output types.  Dependencies: `haystack` framework.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5390-embedders.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@component\nclass HFTextEmbedder:\n    ...\n\n    @component.output_types(result=List[np.ndarray])\n    def run(self, strings: List[str]):\n        ...\n        return {\"result\": list_of_computed_embeddings}\n\n\n@component\nclass HFDocumentEmbedder:\n    ...\n\n    @component.output_types(result=List[Document])\n    def run(self, documents: List[Document]):\n        ...\n        return {\"result\": list_of_documents_with_embeddings}\n```\n\n----------------------------------------\n\nTITLE: Initializing RecentnessRanker in Python\nDESCRIPTION: This code snippet shows how to instantiate the RecentnessRanker node in Python.  It initializes the ranker with parameters like `date_identifier`, `weight`, `top_k`, and `method`. These parameters control how the date is extracted, the importance of recency, the number of documents returned, and the ranking method, respectively.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5289-recentness-ranker.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\t  ranker = RecentnessRanker(\n\t      date_identifier=\"date\",\n\t      weight=\"0.5\",\n          top_k=3,\n\t      method=\"reciprocal_rank_fusion\",\n\t  )\n```\n\n----------------------------------------\n\nTITLE: Metric Result Class\nDESCRIPTION: This code defines a `MetricResult` class that inherits from `dict`. It provides a `save` method to dump the contained metrics dictionary to a file. This class simplifies the process of saving calculated metrics to disk for later analysis or reporting and promotes reuse.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass MetricResult(dict):\n    def save(self, file: Union[str, Path]):\n        # Dump info to file here\n```\n\n----------------------------------------\n\nTITLE: Multi-PromptNode Pipeline\nDESCRIPTION: This code shows how to use multiple PromptNode components in a Haystack pipeline. It generates questions from retrieved documents using one PromptNode and then answers those questions using another PromptNode, demonstrating how to bind the output of one node to the input of another.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntop_k = 3\nquery = \"Who are the parents of Arya Stark?\"\nretriever = EmbeddingRetriever(...)\nmodel = PromptModel(model_name_or_path=\"google/flan-t5-small\")\n\nqg = PromptNode(prompt_template=\"question-generation\", prompt_model=model, output_variable=\"questions\")\nqa = PromptNode(prompt_template=\"question-answering\", prompt_model=model)\n\npipe = Pipeline()\npipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\npipe.add_node(component=qg, name=\"qg\", inputs=[\"Retriever\"])\npipe.add_node(component=qa, name=\"qa\", inputs=[\"qg\"])\n\nresult = pipe.run(query=query)\n\nprint(result[\"results\"])\n```\n\n----------------------------------------\n\nTITLE: RepliesToAnswersConverter Component in Python\nDESCRIPTION: This code defines the RepliesToAnswersConverter component, which converts LLM string replies into Answer objects. The component takes a list of lists of strings (replies) as input and returns a dictionary containing a list of Answer objects. The implementation iterates through the replies and creates an Answer object for each reply.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5540-llm-support-2.0.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@component\nclass RepliesToAnswersConverter:\n\n    @component.output_types(answers=List[List[Answer]])\n    def run(self, replies: List[List[str]]):\n        return {\"answers\": Answer(answer=answer) for answers in replies for answer in answers}\n```\n\n----------------------------------------\n\nTITLE: Shaper Omitting Output Parameter Example - YAML\nDESCRIPTION: This YAML configuration shows how the Shaper component stores the result of a function invocation in the corresponding input variable when the 'output' parameter is omitted. In this case, the 'expand' function's result is stored back into the 'query' variable.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3784-shaper.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n            components:\n            - name: shaper\n              params:\n                inputs:\n                    query:\n                      func: expand\n              type: Shaper\n            pipelines:\n              - name: query\n                nodes:\n                  - name: shaper\n                    inputs:\n                      - Query\n\n```\n\n----------------------------------------\n\nTITLE: Initializing a Generative QA Pipeline with PromptNode in Python\nDESCRIPTION: This code initializes a generative QA pipeline using Haystack's PromptNode. It sets up an InMemoryDocumentStore, an EmbeddingRetriever, and a PromptNode with a default prompt template. The components are then connected in a pipeline to enable question-answering with references.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4172-shaper-in-prompt-template.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack import Pipeline\nfrom haystack.document_store import InMemoryDocumentStore\nfrom haystack.nodes import PromptNode, EmbeddingRetriever\n\ndocument_store = InMemoryDocumentStore()\nretriever = EmbeddingRetriever(document_store=document_store, ...)\npn = PromptNode(default_prompt_template=\"question-answering-with-references\")\n\np = Pipeline()\np.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\np.add_node(component=pn, name=\"Prompt\", inputs=[\"Retriever\"])\n```\n\n----------------------------------------\n\nTITLE: Defining a DocumentStore Contract\nDESCRIPTION: This code snippet illustrates the basic contract that all `DocumentStore` implementations are expected to follow. It defines abstract methods for common CRUD operations on Documents, such as counting, filtering, writing, and deleting documents. The purpose of this contract is to enforce a consistent API across different `DocumentStore` implementations and to allow specialized nodes to interact with them in a generic way.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4370-documentstores-and-retrievers.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyDocumentStore:\n\n    def count_documents(self, **kwargs) -> int:\n        ...\n\n    def filter_documents(self, filters: Dict[str, Any], **kwargs) -> List[Document]:\n        ...\n\n    def write_documents(self, documents: List[Document], **kwargs) -> None:\n        ...\n\n    def delete_documents(self, ids: List[str], **kwargs) -> None:\n        ...\n```\n\n----------------------------------------\n\nTITLE: TableCell Dataclass Definition\nDESCRIPTION: This code defines the TableCell dataclass, used to represent a table cell by its row and column index. It aims to provide a more intuitive way to locate answers within tables compared to the Span dataclass.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3875-table-cell.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass TableCell:\n    row: int\n    col: int\n    \"\"\"\n    Defining a table cell via the row and column index.\n\n    :param row: Row index of the cell\n    :param col: Column index of the cell\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing MemoryRetriever Node\nDESCRIPTION: This code snippet shows an example implementation of a `MemoryRetriever` node. It's a document-store aware node, so it only works with `MemoryDocumentStore`. The retriever can use methods from the memory document store, which are not specified in the base document store contract. Based on the retrieval method (`bm25` or `embedding`), it invokes the appropriate retrieval function on the document store.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4370-documentstores-and-retrievers.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@node\nclass MemoryRetriever:\n\n    def __init__(self, inputs=['query'], output=\"documents\", stores=[\"documents\"]):\n        self.store_names = stores\n        self.inputs = inputs\n        self.outputs = [output]\n        self.init_parameters = {\"inputs\": inputs, \"output\": output \"stores\": stores}\n\n    def run(\n        self,\n        name: str,\n        data: List[Tuple[str, Any]],\n        parameters: Dict[str, Dict[str, Any]]\n    ) -> Dict[str, Any]:\n\n        retriever_parameters = parameters.get(name, {})\n        stores = retriever_parameters.pop(\"stores\", {})\n        retrieval_method = retriever_parameters.pop(\"retrieval_method\", \"bm25\")\n\n        for store_name in self.store_names:\n            if not isinstance(stores[store_name], MemoryStore):\n                raise ValueError(\"MemoryRetriever only works with MemoryDocumentStore.\")\n\n            if retrieval_method == \"bm25\":\n                documents = stores[store_name].bm25_retrieval(queries=queries, **retriever_parameters)\n            elif retrieval_method == \"embedding\":\n                documents = stores[store_name].vector_similarity_retrieval(queries=queries, **retriever_parameters)\n            ...\n\n        return ({self.outputs[0]: documents}, parameters)\n```\n\n----------------------------------------\n\nTITLE: Defining a PromptTemplate with Shapers in Python\nDESCRIPTION: This code demonstrates how to define a PromptTemplate with output shapers in Haystack. It includes a name, prompt text, output shapers to transform the output, and an output variable. The shaper uses a function to convert strings to answers.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4172-shaper-in-prompt-template.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nPromptTemplate(\n        name=\"question-answering-with-references\",\n        prompt_text=\"Create a concise and informative answer (no more than 50 words) for a given question \"\n        \"based solely on the given documents. You must only use information from the given documents. \"\n        \"Use an unbiased and journalistic tone. Do not repeat text. Cite the documents using Document[number] notation. \"\n        \"If multiple documents contain the answer, cite those documents like as stated in Document[number,number,etc]. \"\n        \"If the documents do not contain the answer to the question, say that answering is not possible given the available information.\\n\"\n        \"{join(documents, '\\n', '\\nDocument[$idx]: $content', {'\\n': ' ', '[': '(', ']': ')'})} \\n Question: {query}; Answer: \",\n        output_shapers=[\n            Shaper(\n                func=\"strings_to_answers\",\n                inputs={\"strings\": \"results\", \"documents\": \"documents\"},\n                outputs=[\"answers\"],\n            )\n        ],\n        output_variable=\"answers\",\n    )\n```\n\n----------------------------------------\n\nTITLE: Example Pipeline Output in Python\nDESCRIPTION: This is an example of the expected output from the generative QA pipeline. It includes the answer, invocation context, debug information, root node, parameters, original query, and relevant documents.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4172-shaper-in-prompt-template.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{'answers': [<Answer {'answer': 'Potable water is the most popular drink, followed by tea and beer as stated in Document[5].', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['fcd62336fb380a69c2d655f8cd072995'], 'meta': {}}>],\n'invocation_context': {'query': 'What is the most popular drink?',\n'documents': [<Document: {'content': 'Beer is the oldest[1][2][3] and most widely consumed[4] type of alcoholic drink in the world, and the third most popular drink overall after potable water and tea.[5] It is produced by the brewing and fermentation of starches, mainly derived from cereal grainsmost commonly from malted barley, though wheat, maize (corn), rice, and oats are also used. During the brewing process, fermentation of the starch sugars in the wort produces ethanol and carbonation in the resulting beer.[6] Most modern beer is brewed with hops, which add bitterness and other flavours and act as a natural preservative and stabilizing agent. Other flavouring agents such as gruit, herbs, or fruits may be included or used instead of hops. In commercial brewing, the natural carbonation effect is often removed during processing and replaced with forced carbonation.[7]', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fcd62336fb380a69c2d655f8cd072995'}>],\n'answers': [<Answer {'answer': 'Potable water is the most popular drink, followed by tea and beer as stated in Document[5].', 'type': 'generative', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_ids': ['fcd62336fb380a69c2d655f8cd072995'], 'meta': {}}>]},\n '_debug': {'PromptNode': {'runtime': {'prompts_used': ['Create a concise and informative answer (no more than 50 words) for a given question based solely on the given documents. You must only use information from the given documents. Use an unbiased and journalistic tone. Do not repeat text. Cite the documents using Document[number] notation. If multiple documents contain the answer, cite those documents like as stated in Document[number,number,etc]. If the documents do not contain the answer to the question, say that answering is not possible given the available information.\\nDocument[1]: Beer is the oldest(1)(2)(3) and most widely consumed(4) type of alcoholic drink in the world, and the third most popular drink overall after potable water and tea.(5) It is produced by the brewing and fermentation of starches, mainly derived from cereal grainsmost commonly from malted barley, though wheat, maize (corn), rice, and oats are also used. During the brewing process, fermentation of the starch sugars in the wort produces ethanol and carbonation in the resulting beer.(6) Most modern beer is brewed with hops, which add bitterness and other flavours and act as a natural preservative and stabilizing agent. Other flavouring agents such as gruit, herbs, or fruits may be included or used instead of hops. In commercial brewing, the natural carbonation effect is often removed during processing and replaced with forced carbonation.(7); \\n Question: What is the most popular drink?; Answer: ']}}},\n 'root_node': 'Query',\n 'params': {},\n 'query': 'What is the most popular drink?',\n 'documents': [<Document: {'content': 'Beer is the oldest[1][2][3] and most widely consumed[4] type of alcoholic drink in the world, and the third most popular drink overall after potable water and tea.[5] It is produced by the brewing and fermentation of starches, mainly derived from cereal grainsmost commonly from malted barley, though wheat, maize (corn), rice, and oats are also used. During the brewing process, fermentation of the starch sugars in the wort produces ethanol and carbonation in the resulting beer.[6] Most modern beer is brewed with hops, which add bitterness and other flavours and act as a natural preservative and stabilizing agent. Other flavouring agents such as gruit, herbs, or fruits may be included or used instead of hops. In commercial brewing, the natural carbonation effect is often removed during processing and replaced with forced carbonation.(7)', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fcd62336fb380a69c2d655f8cd072995'}>],\n 'node_id': 'PromptNode'}\n```\n\n----------------------------------------\n\nTITLE: Adding Prompt Template\nDESCRIPTION: This code snippet demonstrates how to add a new prompt template using the `add_prompt_template` method. It then lists the available templates using `get_prompt_templates_names` to show the newly added template.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n      from haystack.nodes.llm import PromptNode\n      PromptNode.add_prompt_template(PromptTemplate(name=\"sentiment-analysis\",\n                              prompt_text=\"Please give a sentiment for this context. Answer with positive, \"\n                              \"negative or neutral. Context: $documents; Answer:\",\n                              input_variables=[\"documents\"]))\n      PromptNode.get_prompt_templates_names()\n```\n\n----------------------------------------\n\nTITLE: Find Thresholds Method\nDESCRIPTION: This code defines a method to find interesting score thresholds for all available metrics, typically used for error analysis.  It takes a list of metric names as input and returns a dictionary mapping each metric to a list of threshold values (e.g., 25th percentile, 75th percentile, median, average).\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef find_thresholds(self, metrics: List[str]) -> Dict[str, float]:\n\n```\n\n----------------------------------------\n\nTITLE: SentenceTransformers Embedder Initialization in Python\nDESCRIPTION: This example shows how to initialize `SentenceTransformersTextEmbedder` and `SentenceTransformersDocumentEmbedder` with different models for DPR (Dense Passage Retrieval). It is used to encode queries and documents separately. No dependencies are explicitly shown.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5390-embedders.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndpr_query_embedder = SentenceTransformersTextEmbedder(model_name=\"facebook/dpr-question_encoder-single-nq-base\")\ndpr_doc_embedder = SentenceTransformersDocumentEmbedder(model_name=\"facebook/dpr-ctx_encoder-single-nq-base\")\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Filter Conditions in JSON\nDESCRIPTION: This JSON snippet demonstrates a basic filter structure with conditions for 'age' and 'country'. It includes both comparison operators ('>=', '==') and a logical 'OR' operator to combine multiple conditions. This structure aims to provide a clear and extensible way to define filter criteria for Document Stores.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6001-document-store-filter-rework.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"conditions\": [\n    { \"field\": \"age\", \"operator\": \">=\", \"value\": 18 },\n    {\n      \"operator\": \"OR\",\n      \"conditions\": [\n        { \"field\": \"country\", \"operator\": \"==\", \"value\": \"USA\" },\n        { \"field\": \"country\", \"operator\": \"==\", \"value\": \"Canada\" }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Variadic Input PromptBuilder Component in Python\nDESCRIPTION: This code defines a PromptBuilder component with a variadic input using the `@variadic_input` decorator. It shows how the component can accept a template and arbitrary keyword arguments (`**kwargs`) to render prompts. The `run` method loads the template, renders the prompts, and returns a dictionary containing the rendered prompts.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5540-llm-support-2.0.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@component\nclass PromptBuilder:\n\n\t@variadic_input\n  \t@component.output_types(prompts=List[str])\n    def run(self, template: Union[str, Path], **kwargs):\n\t    # ... loads the template ...\n        # ... render the prompts ...\n        return {\"prompts\": prompts}\n```\n\n----------------------------------------\n\nTITLE: Getting Prompt Templates\nDESCRIPTION: This code snippet shows how to use the `get_prompt_templates` method to retrieve a list of `PromptTemplate` instances. This allows users to inspect the actual prompt template and its input parameters.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n      from haystack.nodes.llm import PromptNode\n      PromptNode.get_prompt_templates()\n```\n\n----------------------------------------\n\nTITLE: Evaluation Results Data Structure\nDESCRIPTION: This code shows the expected data structure that the `EvaluationResults` class will receive for initialization. It includes inputs such as query IDs, questions, contexts, answers, predicted answers, and a list of metrics with their corresponding scores.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndata = {\n    \"inputs\": {\n        \"query_id\": [\"53c3b3e6\", \"225f87f7\"],\n        \"question\": [\"What is the capital of France?\", \"What is the capital of Spain?\"],\n        \"contexts\": [\"wiki_France\", \"wiki_Spain\"],\n        \"answer\": [\"Paris\", \"Madrid\"],\n        \"predicted_answer\": [\"Paris\", \"Madrid\"]\n    },\n    \"metrics\":\n        [\n            {\"name\": \"reciprocal_rank\", \"scores\": [0.378064, 0.534964, 0.216058, 0.778642]},\n            {\"name\": \"single_hit\", \"scores\": [1, 1, 0, 1]},\n            {\"name\": \"multi_hit\", \"scores\": [0.706125, 0.454976, 0.445512, 0.250522]},\n            {\"name\": \"context_relevance\", \"scores\": [0.805466, 0.410251, 0.750070, 0.361332]},\n            {\"name\": \"faithfulness\", \"scores\": [0.135581, 0.695974, 0.749861, 0.041999]},\n            {\"name\": \"semantic_answer_similarity\", \"scores\": [0.971241, 0.159320, 0.019722, 1]}\n         ],\n    },\n\n```\n\n----------------------------------------\n\nTITLE: DeepEvalEvaluator Implementation - Python\nDESCRIPTION: This code demonstrates the implementation of the DeepEvalEvaluator component in Python, including the initialization of the metric and the setting of input types based on the selected metric. It defines the DeepEvalEvaluator class, its constructor, and the run method to execute the evaluation logic. It depends on the `deepeval` library, specifically the `BaseMetric` and `FaithfulnessMetric` classes. Note the typo in `Listt[List[str]]`, it should be `List[List[str]]`.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6784-integrations-for-eval-framworks.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom deepeval import BaseMetric, FaithfulnessMetric\n\n@component\nclass DeepEvalEvaluator:\n\tself._metric: BaseMetric\n\n\tdef __init__(self, metric: str, params: Optional[Dict[str, Any]]):\n\t\tparams = {} if params is None\n\t\tif metric == \"Faithfulness\":\n\t\t\tself._metric = FaithfulnessMetric(**params)\n\t\t\tself.set_input_types(questions=List[str], answers=List[str], contexts=Listt[List[str]])\n\t\telif metric == \"ContextRecall\":\n\t\t\t...\n\n\tdef run(self, **kwargs):\n\t\t# Logic to unwrap the inputs based on the metric and\n        # execute the backend code.\n        ...\n```\n\n----------------------------------------\n\nTITLE: Initializing PromptNode\nDESCRIPTION: This code snippet demonstrates how to instantiate a PromptNode with a specified LLM model and use it to perform a question-answering task. It shows the basic usage of PromptNode with a natural language prompt.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\t  from haystack.nodes.llm import PromptNode\n      pn = PromptNode(model_name_or_path=\"google/flan-t5-base\")\n      pn(\"What is the capital of Germany?\")\n```\n\n----------------------------------------\n\nTITLE: Building a custom Haystack Docker image with variable overrides\nDESCRIPTION: This command builds a custom Haystack Docker image by overriding variables defined in the `docker-bake.hcl` file.  It allows specifying a custom branch or tag for the Haystack repository and a suffix for the base image tag. `--no-cache` forces a rebuild without using the cache. Requires Docker, BuildKit, and the `docker-bake.hcl` configuration file.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/docker/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nHAYSTACK_VERSION=mybranch_or_tag BASE_IMAGE_TAG_SUFFIX=latest docker buildx bake gpu --no-cache\n```\n\n----------------------------------------\n\nTITLE: Agent Prompt Example\nDESCRIPTION: This code snippet demonstrates the initial prompt sent to the LLM, instructing it to answer questions using available tools (Search and Calculator). The prompt defines the format for interaction, including 'Thought', 'Action', and 'Action Input', and sets the stage for iterative reasoning.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3925-mrkl-agent.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nAnswer the following questions as best as you can. You have access to the following tools:\n\nSearch: useful for when you need to answer questions about current events. You should ask targeted questions\nCalculator: useful for when you need to answer questions about math\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Search, Calculator]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final Answer\nFinal Answer: the final Answer to the original input question\n\nBegin!\nQuestion: Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\nThought:\n```\n\n----------------------------------------\n\nTITLE: Using PromptTemplate on-the-fly\nDESCRIPTION: This code snippet demonstrates using a `PromptTemplate` instance directly within the `prompt` method, without registering it first.  This enables users to experiment with custom prompts.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n      from haystack.nodes.llm import PromptNode\n      pn = PromptNode(model_name_or_path=\"google/flan-t5-base\")\n      prompt_template = PromptTemplate(name=\"sentiment-analysis\",\n                          prompt_text=\"Please give a sentiment for this context. \"\n                          \"Answer with positive, negative or neutral. Context: $documents; Answer:\",\n                          input_variables=[\"documents\"])\n      pn.prompt(prompt_template, documents=[\"I really enjoyed the recent movie.\"])\n```\n\n----------------------------------------\n\nTITLE: Evaluation Function Tracking Progress with Percentage\nDESCRIPTION: This code shows an alternative approach to tracking evaluation progress by yielding progress percentages. The `eval` function returns the current progress percentage after processing each input, providing users with a visual indication of the evaluation status. After all inputs are processed, it yields 100% and the final `EvaluationResult`. This provides feedback to the user during execution.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef eval(runnable, inputs, expected_outputs):\n    outputs = []\n    total = len(inputs)\n    for i, input_ in enumerate(inputs):\n      output = runnable.run(input_)\n      outputs.append(output)\n      yield 100 * (i / total), None\n\n    yield 100, EvaluationResult(runnable, inputs, outputs, expected_outputs)\n```\n\n----------------------------------------\n\nTITLE: Equivalent Python Expression for Filter Conditions\nDESCRIPTION: This Python snippet shows the equivalent boolean expression for the JSON filter defined above. It uses comparison operators ('>=', '==') and logical operators ('and', 'or') to represent the filter conditions in Python code. This provides a clear understanding of the intended filtering logic.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6001-document-store-filter-rework.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nage >= 18 and (country == \"USA\" or country == \"Canada\")\n```\n\n----------------------------------------\n\nTITLE: Initializing PromptNode with Template\nDESCRIPTION: This code demonstrates how to initialize the PromptNode with a specified model and a default prompt template name. Subsequently the PromptNode is called directly, which will run the specified template on the provided documents. This showcases the usage of a predefined template during initialization.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n      from haystack.nodes.llm import PromptNode\n      pn = PromptNode(model_name_or_path=\"google/flan-t5-base\", prompt_template=\"question-generation\")\n      pn(documents=[\"Berlin is the capital of Germany.\"])\n```\n\n----------------------------------------\n\nTITLE: Individual Aggregate Score Report Method\nDESCRIPTION: This code defines a method that generates a summary of the model's performance across all queries, showing the aggregated scores for all available metrics. This is intended for basic users who want a quick overview of the evaluation results.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef individual_aggregate_score_report():\n\n```\n\n----------------------------------------\n\nTITLE: Table access with Pandas DataFrame vs List[List]\nDESCRIPTION: This code demonstrates the difference in accessing table content using Pandas DataFrame and List[List] representations. It highlights how the Span dataclass can point to the wrong location when the table is converted to a list of lists, motivating the need for the TableCell dataclass.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3875-table-cell.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom haystack import Document\n\ndata = {\n    \"actors\": [\"brad pitt\", \"leonardo di caprio\", \"george clooney\"],\n    \"age\": [\"58\", \"47\", \"60\"],\n    \"number of movies\": [\"87\", \"53\", \"69\"],\n    \"date of birth\": [\"18 december 1963\", \"11 november 1974\", \"6 may 1961\"],\n}\ntable_doc = Document(content=pd.DataFrame(data), content_type=\"table\")\nspan = (0, 0)\nprint(table_doc.content.iloc[span])  # prints \"brad pitt\"\n\ndict_table_doc = table_doc.to_dict()\nprint(dict_table_doc[\"content\"][span[0]][span[1]])  # prints \"actors\"\n```\n\n----------------------------------------\n\nTITLE: Comparison of Old and New Filter Styles in JSON\nDESCRIPTION: This example contrasts the old (Haystack 1.x) and new (proposed for Haystack 2.x) filter styles in JSON.  It illustrates how equivalent filters are structured differently in the old style, requiring more complex parsing. The new style provides a more uniform and easily parsable structure.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6001-document-store-filter-rework.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"number\": {\"$and\": [{\"$lte\": 2}, {\"$gte\": 0}]}}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"number\": {\"$lte\": 2, \"$gte\": 0}}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"operator\": \"AND\",\n    \"conditions\": [\n        { \"field\": \"number\", \"operator\": \"<=\", \"value\": 2 },\n        { \"field\": \"number\", \"operator\": \">=\", \"value\": 0 },\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running a Query on the QA Pipeline in Python\nDESCRIPTION: This code snippet demonstrates how to run a query on the previously defined QA pipeline. The pipeline processes the query and returns the answer generated by the PromptNode, leveraging the retriever for context.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4172-shaper-in-prompt-template.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\np.run(\n    query=\"What is the most popular drink?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Individual Detailed Score Report Method\nDESCRIPTION: This code defines a method that generates a detailed evaluation report, providing the scores of all available metrics for all queries or a subset of queries. It takes an optional `queries` parameter to specify which queries to include in the report.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef individual_detailed_score_report(queries: Union[List[str], str] = \"all\"):\n\n```\n\n----------------------------------------\n\nTITLE: Pipeline Save/Load Example Python\nDESCRIPTION: This code illustrates how to save and load pipelines using custom writer and reader functions. It supports different serialization formats (e.g., JSON, YAML) via user-defined functions.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4284-drop-basecomponent.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack.pipelines import Pipeline, save_pipelines, load_pipelines\n\nquery_pipeline = Pipeline()\nindexing_pipeline = Pipeline()\n# .. assemble the pipelines ...\n\n# Save the pipelines\nsave_pipelines(\n    pipelines={\n        \"query\": query_pipeline,\n        \"indexing\": indexing_pipeline,\n    },\n    path=\"my_pipelines.json\",\n    writer=json.dumps\n)\n\n# Load the pipelines\nnew_pipelines = load_pipelines(\n    path=\"my_pipelines.json\",\n    reader=json.loads\n)\n\nassert new_pipelines[\"query\"] == query_pipeline\nassert new_pipelines[\"indexing\"] == indexing_pipeline\n```\n\n----------------------------------------\n\nTITLE: Evaluation Function Minimal Implementation\nDESCRIPTION: This snippet shows a minimal implementation of the `eval` function. It takes a runnable (either a Pipeline or a Component), a list of inputs, and a list of expected outputs. It runs the runnable with each input, collects the outputs, and returns an `EvaluationResult` object containing the runnable, inputs, outputs, and expected outputs. This example highlights the core logic of the evaluation process.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef eval(runnable: Union[Pipeline, Component], inputs: List[Dict[str, Any]], expected_outputs: List[Dict[str, Any]]) -> EvaluationResult:\n    outputs = []\n    for input_ in inputs:\n      output = runnable.run(input_)\n      outputs.append(output)\n    return EvaluationResult(runnable, inputs, outputs, expected_outputs)\n```\n\n----------------------------------------\n\nTITLE: Answer Dataclass Update for TableCell Support\nDESCRIPTION: This code shows the updated Answer dataclass to support TableCell as a valid type for offsets_in_document and offsets_in_context. It allows representing answer locations within tables using row and column indices.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3875-table-cell.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\n class Answer:\n     answer: str\n     type: Literal[\"generative\", \"extractive\", \"other\"] = \"extractive\"\n     score: Optional[float] = None\n     context: Optional[Union[str, List[List]]] = None\n     offsets_in_document: Optional[List[Span], List[TableCell]] = None\n     offsets_in_context: Optional[List[Span], List[TableCell]] = None\n     document_id: Optional[str] = None\n     meta: Optional[Dict[str, Any]] = None\n```\n\n----------------------------------------\n\nTITLE: Metric Calculation Method\nDESCRIPTION: This code demonstrates a method for calculating metrics within the `EvaluationResult` class. It accepts a `metric` parameter, which can be either a predefined metric from the `Metric` enum or a custom metric calculator function. Based on the `metric`, it either calls a corresponding internal method (e.g., `_calculate_recall`) or executes the custom metric calculator. It allows flexible metric selection.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nMetricsResult = Dict[str, Dict[str, float]]\nMetricCalculator = Callable[..., MetricResult]\n\ndef calculate_metrics(self: EvaluationResult, metric: Union[Metric, MetricCalculator], **kwargs) -> MetricsResult:\n    # Verify if we're calculating a known metric\n    if metric == Metric.RECALL:\n      return self._calculate_recall(**kwargs)\n    elif metric == Metric.MRR:\n      return self._calculate_mrr(**kwargs)\n    # Other metrics...\n\n    # If it's not a known metric it must be a custom one\n    return metric(self, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Shaper Default Parameters Example - YAML\nDESCRIPTION: This YAML configuration demonstrates how to configure the Shaper component with default parameters. The 'expand' function is used on the 'query' variable to create 'questions' without explicitly specifying the 'expand_target' and 'size' parameters, relying on default parameter handling.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3784-shaper.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n            components:\n            - name: shaper\n              params:\n                inputs:\n                    query:\n                      func: expand\n                      output: questions\n              type: Shaper\n            pipelines:\n              - name: query\n                nodes:\n                  - name: shaper\n                    inputs:\n                      - Query\n\n```\n\n----------------------------------------\n\nTITLE: Initializing MetaFieldRanker in Haystack (Python)\nDESCRIPTION: This snippet demonstrates how to instantiate a MetaFieldRanker with specific parameters such as the meta field to sort by (`meta_field`), the weight to assign to the ranking (`weight`), the sorting order (`ascending`), and the number of top documents to return (`top_k`).  It shows a basic example of how to configure the ranker to sort documents based on a 'rating' meta field.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6141-meta-field-ranker.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nranker = MetaFieldRanker(\n    meta_field=\"rating\",\n    weight=\"0.5\",\n    ascending=False,\n    top_k=3,\n)\n```\n\n----------------------------------------\n\nTITLE: Span Dataclass Definition\nDESCRIPTION: This code defines the Span dataclass, used to represent a sequence of characters or table cells via start and end indices. It's used for extractive QA and TableQA.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3875-table-cell.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass Span:\n    start: int\n    end: int\n    \"\"\"\n    Defining a sequence of characters (Text span) or cells (Table span) via start and end index.\n    For extractive QA: Character where answer starts/ends\n    For TableQA: Cell where the answer starts/ends (counted from top left to bottom right of table)\n\n    :param start: Position where the span starts\n    :param end:  Position where the span ends\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing Haystack with pip\nDESCRIPTION: This snippet shows how to install the Haystack LLM framework using pip, the Python package installer.  It installs the stable version of the haystack-ai package.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install haystack-ai\n```\n\n----------------------------------------\n\nTITLE: Find Inputs Below Threshold Method\nDESCRIPTION: This code defines a method to retrieve all queries with a score below a certain threshold for a given metric. This is useful for identifying specific inputs that perform poorly and diagnosing potential issues.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef find_inputs_below_threshold(self, metric: str, threshold: float):\n    \"\"\"Get the all the queries with a score below a certain threshold for a given metric\"\"\"  \n\n```\n\n----------------------------------------\n\nTITLE: Installing Haystack from main branch\nDESCRIPTION: This snippet shows how to install the Haystack LLM framework directly from the main branch on GitHub.  This method allows users to access the latest features and updates before they are officially released.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npip install git+https://github.com/deepset-ai/haystack.git@main\n```\n\n----------------------------------------\n\nTITLE: HFEmbeddingBackend Singleton Implementation in Python\nDESCRIPTION: This code presents an outline of `HFEmbeddingBackend` as a singleton class responsible for embedding computation. The `__init__` method initializes the model. The `embed` method computes the embedding for a given data point. The implementation of `@singleton` decorator is assumed to be provided elsewhere.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5390-embedders.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@singleton  # implementation is out of scope\nclass HFEmbeddingBackend:\n    \"\"\"\n    NOT A COMPONENT!\n    \"\"\"\n    def __init__(self, model_name: str, ... init params ...):\n        \"\"\"\n        init takes the minimum parameters needed at init time, not\n        the params needed at inference, so they're easier to reuse.\n        \"\"\"\n        self.model = ...\n\n    def embed(self, data: str, ... inference params ... ) -> np.ndarray:\n        # compute embedding\n        return embedding\n\n\nclass OpenAIEmbeddingBackend:\n    ... same as above ...\n```\n\n----------------------------------------\n\nTITLE: Implementing DocumentWriter Node\nDESCRIPTION: This code snippet presents an implementation of a `DocumentWriter` node.  It takes a list of documents as input and writes them to the specified document stores using the `write_documents` method defined in the `DocumentStore` contract. The `DocumentWriter` is a document-store agnostic node that works with any `DocumentStore` implementation.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4370-documentstores-and-retrievers.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@node\nclass DocumentWriter:\n\n    def __init__(self, inputs=['documents'], stores=[\"documents\"]):\n        self.store_names = stores\n        self.inputs = inputs\n        self.outputs = []\n        self.init_parameters = {\"inputs\": inputs, \"stores\": stores}\n\n    def run(\n        self,\n        name: str,\n        data: List[Tuple[str, Any]],\n        parameters: Dict[str, Dict[str, Any]]\n    ) -> Dict[str, Any]:\n        writer_parameters = parameters.get(name, {})\n        stores = writer_parameters.pop(\"stores\", {})\n\n        all_documents = []\n        for _, documents in data:\n            all_documents += documents\n\n        for store_name in self.store_names:\n            stores[store_name].write_documents(documents=all_documents, **writer_parameters)\n\n        return ({}, parameters)\n```\n\n----------------------------------------\n\nTITLE: Evaluation Function Tracking Progress with Partials\nDESCRIPTION: This code demonstrates how to track evaluation progress by yielding partial results during the evaluation process. The `eval` function returns an `EvaluationResult` after processing each input, allowing users to monitor the progress and access intermediate results. This approach can be useful for long-running evaluations or when real-time feedback is desired.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef eval(runnable, inputs, expected_outputs):\n    result = EvaluationResult(runnable, inputs, {}, expected_outputs)\n    for input_ in inputs:\n      output = runnable.run(input_)\n      result.append_output(output)\n      yield result\n```\n\n----------------------------------------\n\nTITLE: Example JSON Document Structure\nDESCRIPTION: This JSON snippet shows the expected structure of a JSON file to be used with the `JsonConverter`.  It represents a list of JSON objects, each representing a document with fields such as content, content_type, and meta. The content field is compulsory. Other fields include content_type, meta, id_hash_keys, score, and embedding.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3959-json-converter.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"content\": \"...\",\n        \"content_type\": \"text\", \"meta\": {...}\n    },\n    {\n        \"content\": [[\"h1\", \"h2\"], [\"val1\", \"val2\"]],\n        \"content_type\": \"table\", \"meta\": {...}\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Creating Haystack Virtual Environment\nDESCRIPTION: This command creates a new virtual environment for Haystack using Hatch. The virtual environment isolates project dependencies, ensuring a consistent and reproducible development environment. This step is essential for managing Haystack's dependencies and preventing conflicts with other Python projects.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ hatch shell\n```\n\n----------------------------------------\n\nTITLE: Defining a Haystack Node: AddValue in Python\nDESCRIPTION: This code defines a custom Haystack node called `AddValue` which adds a specified value to the input data. It uses the `@node` decorator and has a `run()` method that performs the addition. The node takes an input named 'value' and outputs a value.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4284-drop-basecomponent.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, Any, List, Tuple\nfrom haystack.pipeline import Pipeline\nfrom haystack.nodes import node\n\n# A Haystack Node. See below for details about this contract.\n# Crucial components are the @node decorator and the `run()` method\n@node\nclass AddValue:\n    def __init__(self, add: int = 1, input_name: str = \"value\", output_name: str = \"value\"):\n        self.add = add\n        self.init_parameters = {\"add\": add}\n        self.inputs = [input_name]\n        self.outputs = [output_name]\n\n    def run(\n        self,\n        name: str,\n        data: List[Tuple[str, Any]],\n        parameters: Dict[str, Any],\n        stores: Dict[str, Any],\n    ):\n        my_parameters = parameters.get(name, {})\n        add = my_parameters.get(\"add\", self.add)\n\n        for _, value in data:\n            value += add\n\n        return ({self.outputs[0]: value}, parameters)\n```\n\n----------------------------------------\n\nTITLE: Checking Hatch Version\nDESCRIPTION: This command verifies the installed version of Hatch, a Python project manager used by Haystack for managing virtual environments, building, and publishing packages. It's a crucial initial step to ensure Hatch is properly installed and accessible from the terminal.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ hatch --version\nHatch, version 1.9.3\n```\n\n----------------------------------------\n\nTITLE: Cloning the Haystack repository\nDESCRIPTION: This code snippet shows how to clone the Haystack GitHub repository using `git`.  Cloning the repository is necessary to access the demo application and other resources provided by Haystack.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/test/test_files/markdown/sample.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n    # git clone https://github.com/deepset-ai/haystack.git\n```\n\n----------------------------------------\n\nTITLE: Defining a Haystack Node: Double in Python\nDESCRIPTION: This code defines a custom Haystack node called `Double` which doubles the input data. It uses the `@node` decorator and has a `run()` method that performs the doubling operation. The node expects an input named 'value' (configurable as 'input_edge').\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4284-drop-basecomponent.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@node\nclass Double:\n    def __init__(self, input_edge: str = \"value\"):\n        self.init_parameters = {\"input_edge\": input_edge}\n        self.inputs = [input_edge]\n        self.outputs = [input_edge]\n\n    def run(\n        self,\n        name: str,\n        data: List[Tuple[str, Any]],\n        parameters: Dict[str, Any],\n        stores: Dict[str, Any],\n    ):\n        for _, value in data:\n            value *= 2\n\n        return ({self.outputs[0]: value}, parameters)\n```\n\n----------------------------------------\n\nTITLE: Individual Detailed Score Report Output\nDESCRIPTION: This example shows the expected output of the `individual_detailed_score_report()` method. It presents the scores for each query, including the question, context, answer, predicted answer, and all available metrics.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n| question | context | answer | predicted_answer | reciprocal_rank | single_hit | multi_hit | context_relevance | faithfulness | semantic_answer_similarity |\n|----------|---------|--------|------------------|-----------------|------------|-----------|-------------------|-------------|----------------------------|\n| What is the capital of France? | wiki_France | Paris | Paris | 0.378064 | 1 | 0.706125 | 0.805466 | 0.135581 | 0.971241 |\n| What is the capital of Spain? | wiki_Spain | Madrid | Madrid | 0.534964 | 1 | 0.454976 | 0.410251 | 0.695974 | 0.159320 |\n\n```\n\n----------------------------------------\n\nTITLE: JsonConverter Class Implementation (Python)\nDESCRIPTION: This code provides a snippet of the `JsonConverter` class implementation, focusing on the `convert` method. The method reads a JSON file, iterates through its contents (assuming each entry represents a document), and converts each entry into a Haystack `Document` object using `Document.from_dict`. It also handles encoding and metadata.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3959-json-converter.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass JsonConverter(BaseConverter):\n    def __init__(self, ...):\n        ...\n\n    def convert(\n        self,\n        file_path: Path,\n        meta: Optional[Dict[str, str]] = None,\n        encoding: Optional[str] = \"UTF-8\",\n        id_hash_keys: Optional[List[str]] = None,\n        ...\n    ) -> List[Document]:\n        if id_hash_keys is None:\n            id_hash_keys = self.id_hash_keys\n\n        documents = []\n        with open(file_path, encoding=encoding, errors=\"ignore\") as f:\n            data = json.load(f)\n            for doc_dict in data:\n                doc_dict = dict(doc_dict)\n                doc_dict['id_hash_keys'] = id_hash_keys\n                doc_dict['meta'] = doc_dict.get('meta', dict())\n\n                if meta:\n                    doc_dict['meta'].update(meta)\n\n                documents.append(Document.from_dict(doc_dict))\n\n        return documents\n```\n\n----------------------------------------\n\nTITLE: Conversion from MongoDB-like to New Filter Style (JSON)\nDESCRIPTION: This example shows how a filter written in a MongoDB-like style (used in Haystack 1.x) can be converted to the new, more structured filter format proposed for Haystack 2.x. It provides a clear mapping between the old and new representations for complex conditions involving multiple fields and operators.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6001-document-store-filter-rework.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"$and\": {\n        \"type\": {\"$eq\": \"article\"},\n        \"$or\": {\"genre\": {\"$in\": [\"economy\", \"politics\"]}, \"publisher\": {\"$eq\": \"nytimes\"}},\n        \"date\": {\"$gte\": \"2015-01-01\", \"$lt\": \"2021-01-01\"},\n        \"rating\": {\"$gte\": 3},\n    }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"operator\": \"AND\",\n    \"conditions\": [\n        { \"field\": \"type\", \"operator\": \"==\", \"value\": \"article\" },\n        {\n            \"operator\": \"OR\",\n            \"conditions\": [\n                { \"field\": \"genre\", \"operator\": \"in\", \"value\": [\"economy\", \"politics\"] },\n                { \"field\": \"publisher\", \"operator\": \"==\", \"value\": \"nytimes\" },\n            ]\n        },\n        { \"field\": \"date\", \"operator\": \">=\", \"value\": \"2015-01-01\" },\n        { \"field\": \"date\", \"operator\": \"<\", \"value\": \"2021-01-01\" },\n        { \"field\": \"rating\", \"operator\": \">=\", \"value\": 3 }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Comparative Aggregate Score Report Method\nDESCRIPTION: This code defines a method that compares the performance of the model with another model based on the aggregated scores for all available metrics.  It takes another `EvaluationResults` object as input for comparison.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef comparative_aggregate_score_report(self, other: \"EvaluationResults\"):\n\n```\n\n----------------------------------------\n\nTITLE: JsonConverter Basic Usage in Haystack (Python)\nDESCRIPTION: This code snippet demonstrates the basic usage of the `JsonConverter` node in a Haystack pipeline. It instantiates the `JsonConverter`, and then calls the `convert` method with a file path to a JSON or JSONL file. The `convert` method returns a list of `Document` objects.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3959-json-converter.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom haystack.nodes import JsonConverter\n\nconverter = JsonConverter()\n\n# Receive back List[Document]\ndocs = converter.convert(\"data_file.json\")\n```\n\n----------------------------------------\n\nTITLE: Updating Embeddings with New Model in Haystack\nDESCRIPTION: This code snippet demonstrates how to update document embeddings using a new model with the proposed Embedder design in Haystack. It retrieves all documents from the `MemoryDocumentStore`, computes new embeddings using `HFDocumentEmbedder`, and overwrites the existing documents with the new embeddings using `write_documents` and `DuplicatePolicy.OVERWRITE`.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5390-embedders.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# get all the documents\ndocs = memory_document_store.filter_documents()\n\n# compute the embedding with the new model\nnew_embedder = HFDocumentEmbedder(model_name=\"new-model\")\ndocs_with_embeddings = new_embedder.run(documents=docs)\n\n# overwrite the documents\nmemory_document_store.write_documents(documents=docs_with_embeddings, policy=DuplicatePolicy.OVERWRITE)\n```\n\n----------------------------------------\n\nTITLE: Individual Aggregate Score Report Output\nDESCRIPTION: This example shows the expected output of the `individual_aggregate_score_report()` method. It presents the aggregated scores for metrics like Reciprocal Rank, Single Hit, Multi Hit, Context Relevance, Faithfulness, and Semantic Answer Similarity.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n{'Reciprocal Rank': 0.448,\n 'Single Hit': 0.5,\n 'Multi Hit': 0.540,\n 'Context Relevance': 0.537,\n 'Faithfulness': 0.452,\n 'Semantic Answer Similarity': 0.478\n }\n \n```\n\n----------------------------------------\n\nTITLE: Evaluating Pipeline with Single Input Component (Python)\nDESCRIPTION: Demonstrates evaluating a Haystack pipeline with a single input component named 'foo' that takes a 'query' as input.  The code shows how to specify the input without explicitly mentioning the component name, assuming there are no other components accepting a 'query' input.  This simplified syntax is intended for simpler pipelines.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\neval(pipe, {\"query\": \"This is the query\"})\n```\n\n----------------------------------------\n\nTITLE: Shaper Order of Invocation Example - YAML\nDESCRIPTION: This YAML configuration highlights the importance of function invocation order in the Shaper component. It demonstrates how the 'query' variable is expanded to 'questions', and then 'questions' is immediately used as input for the 'concat' function applied to 'documents'.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3784-shaper.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n            components:\n            - name: shaper\n              params:\n                inputs:\n                    query:\n                      func: expand\n                      output: questions\n                      params:\n                        expand_target: query\n                        size:\n                          func: len\n                          params:\n                            - documents\n                    documents:\n                      func: concat\n                      output: documents\n                      params:\n                        docs: documents\n                        delimiter: \" \"\n                        num_tokens:\n                            func: len\n                            params:\n                                - questions\n              type: Shaper\n            pipelines:\n              - name: query\n                nodes:\n                  - name: shaper\n                    inputs:\n                      - Query\n\n```\n\n----------------------------------------\n\nTITLE: Using PromptNode with Template Name\nDESCRIPTION: This code snippet illustrates how to use the `prompt` method of PromptNode with a specified prompt template name to perform a task. The example shows question generation using the documents parameter.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n      from haystack.nodes.llm import PromptNode\n      pn = PromptNode(model_name_or_path=\"google/flan-t5-base\")\n\t  pn.prompt(\"question-generation\", documents=[\"Berlin is the capital of Germany.\"])\n```\n\n----------------------------------------\n\nTITLE: LLM Response Example\nDESCRIPTION: This snippet shows the expected response from the LLM after receiving the initial prompt. It includes the 'Thought', 'Action', and 'Action Input' components, guiding the Agent to select the 'Search' tool with the input 'Olivia Wilde's boyfriend'. The 'Observation' field will be populated later with the result of the search.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3925-mrkl-agent.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nI need to do some research to answer this question.\nAction: Search\nAction Input: Olivia Wilde's boyfriend\n```\n\n----------------------------------------\n\nTITLE: Metric Enum Definition\nDESCRIPTION: This snippet defines an Enum called `Metric` listing available metrics, such as Recall, MRR, MAP, EM, F1, and SemanticAnswerSimilarity (SAS). This enum provides a standardized and discoverable way to refer to different metrics, making it easier to select and use them in the evaluation process and for auto-completion in an IDE.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5794-evaluation-haystack-2.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Metric(Enum):\n    RECALL = \"Recall\"\n    MRR = \"Mean Reciprocal Rank\"\n    MAP = \"Mean Average Precision\"\n    EM = \"Exact Match\"\n    F1 = \"F1\"\n    SAS = \"SemanticAnswerSimilarity\"\n```\n\n----------------------------------------\n\nTITLE: Comparative Detailed Score Report Output\nDESCRIPTION: This example shows the expected output of the `comparative_detailed_score_report()` method. It presents a side-by-side comparison of the scores for each query for both models, including all available metrics.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n| question | context | answer | predicted_answer_model_1 | predicted_answer_model_2 | reciprocal_rank_model_1 | reciprocal_rank_model_2 | single_hit_model_1 | single_hit_model_2 | multi_hit_model_1 | multi_hit_model_2 | context_relevance_model_1 | context_relevance_model_2 | faithfulness_model_1 | faithfulness_model_2 | semantic_answer_similarity_model_1 | semantic_answer_similarity_model_2 |\n|----------|---------|--------|--------------------------|--------------------------|-------------------------|-------------------------|--------------------|--------------------|-------------------|-------------------|---------------------------|---------------------------|----------------------|----------------------|------------------------------------|------------------------------------|\n| What is the capital of France? | wiki_France | Paris | Paris | Paris | 0.378064 | 0.378064 | 1 | 1 | 0.706125 | 0.706125 | 0.805466 | 0.805466 | 0.135581 | 0.135581 | 0.971241 | 0.971241 |\n| What is the capital of Spain? | wiki_Spain | Madrid | Madrid | Madrid | 0.534964 | 0.534964 | 1 | 1 | 0.454976 | 0.454976 | 0.410251 | 0.410251 | 0.695974 | 0.695974 | 0.159320 | 0.159320 |\n\n```\n\n----------------------------------------\n\nTITLE: Node Parameter Initialization Example Python\nDESCRIPTION: This code demonstrates how node parameters are initialized and overridden at different stages of the pipeline execution. It showcases the parameter hierarchy, from default values to runtime overrides.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4284-drop-basecomponent.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Node:\n    def __init__(self, value_1: int = 1, value_2: int = 1, value_3: int = 1, value_4: int = 1):\n        ...\n\nnode = Node(value_2=2, value_3=2, value_4=2)\npipeline = Pipeline()\npipeline.add_node(\"node\", node, parameters={\"value_3\": 3, \"value_4\": 3})\n...\npipeline.run(data={...}, parameters={\"node\": {\"value_4\": 4}})\n\n# Node will receive {\"value_1\": 1, \"value_2\": 2, \"value_3\": 3,\"value_4\": 4}\n```\n\n----------------------------------------\n\nTITLE: Listing Prompt Templates\nDESCRIPTION: This code snippet shows how to use the `get_prompt_templates_names` class method of the PromptNode to retrieve a list of available prompt templates. This allows users to examine the default NLP tasks supported by PromptNode.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\t  from haystack.nodes.llm import PromptNode\n      PromptNode.get_prompt_templates_names()\n```\n\n----------------------------------------\n\nTITLE: LLM Generator Component Interface (Python)\nDESCRIPTION: This code defines the interface for an LLM generator component, specifically ChatGPTGenerator, in Haystack 2.0. It inherits from a generic component class and defines a `run` method that takes a list of prompts as input and returns a dictionary containing a list of replies. The component is designed to be modular and reusable with other components in a pipeline.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/5540-llm-support-2.0.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@component\nclass ChatGPTGenerator:\n\n    @component.output_types(replies=List[List[str]])\n    def run(self, prompts: List[str], ... chatgpt specific params...):\n        ...\n        return {'replies': [...]}\n```\n\n----------------------------------------\n\nTITLE: Thresholds data\nDESCRIPTION: This shows the data for thresholds such as 25th percentile, 75th percentile, median, and average of the context relevance metrics.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndata  = {\n    \"thresholds\": [\"25th percentile\", \"75th percentile\", \"median\", \"average\"],\n    \"reciprocal_rank\": [0.378064, 0.534964, 0.216058, 0.778642],\n    \"context_relevance\": [0.805466, 0.410251, 0.750070, 0.361332],\n    \"faithfulness\": [0.135581, 0.695974, 0.749861, 0.041999],\n    \"semantic_answer_similarity\": [0.971241, 0.159320, 0.019722, 1],\n}\n\n```\n\n----------------------------------------\n\nTITLE: Comparative Detailed Score Report Method\nDESCRIPTION: This code defines a method that compares the performance of the model with another model based on the scores of all available metrics for all queries.  It takes another `EvaluationResults` object as input for comparison.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef comparative_detailed_score_report(self, other: \"EvaluationResults\"):\n\n```\n\n----------------------------------------\n\nTITLE: PromptNode __init__ Constructor\nDESCRIPTION: This code snippet displays the `__init__` constructor for the PromptNode class.  It shows the parameters `model_name_or_path` and `prompt_template` which allows initialization with a model and/or a template.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n    def __init__(self, model_name_or_path: str = \"google/flan-t5-base\", prompt_template: Union[str, PromptTemplate] = None):\n```\n\n----------------------------------------\n\nTITLE: Prompt Method Signature\nDESCRIPTION: This code snippet shows the signature of the `prompt` method of the PromptNode class. It highlights that the `prompt_template` parameter can be either a string representing the template name or a `PromptTemplate` instance.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/3665-prompt-node.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    def prompt(self, prompt_template: Union[str, PromptTemplate] = None, *args, **kwargs) -> List[str]:\n```\n\n----------------------------------------\n\nTITLE: Conditional Dataclass Import for Type Checking\nDESCRIPTION: This code snippet demonstrates how to conditionally import dataclasses from either the standard `dataclasses` module or the `pydantic.dataclasses` module based on the `typing.TYPE_CHECKING` flag. This allows for better type checking during development while still using Pydantic dataclasses at runtime.  It resolves issues with mypy and autocomplete tools when using Pydantic dataclasses.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/2170-pydantic-dataclasses.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nif typing.TYPE_CHECKING:\n    from dataclasses import dataclass\nelse:\n    from pydantic.dataclasses import dataclass\n```\n\n----------------------------------------\n\nTITLE: Equivalent Python Expression for Converted Filter\nDESCRIPTION: This Python snippet provides the boolean expression equivalent to the converted JSON filter. It helps illustrate the filtering logic applied across multiple fields, using AND and OR operators, making it easy to understand the query logic.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6001-document-store-filter-rework.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntype == \"article\" and (\n    genre in [\"economy\", \"politics\"] or publisher == \"nytimes\"\n) and date >= \"2015-01-01\" and date < \"2021-01-01\" and rating >= 3\n```\n\n----------------------------------------\n\nTITLE: Defining a Python Function for a Linear Pipeline\nDESCRIPTION: This Python function defines a linear pipeline using the Haystack library. It adds components, connects them, and returns a tuple containing the `Pipeline` instance and a list of `PipelineRunData` instances. The `PipelineRunData` specifies the inputs, expected outputs, and expected run order for the pipeline.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/test/core/pipeline/features/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n@given(\"a pipeline that is linear\", target_fixture=\"pipeline_data\")\ndef pipeline_that_is_linear():\n    pipeline = Pipeline()\n    pipeline.add_component(\"first_addition\", AddFixedValue(add=2))\n    pipeline.add_component(\"second_addition\", AddFixedValue())\n    pipeline.add_component(\"double\", Double())\n    pipeline.connect(\"first_addition\", \"double\")\n    pipeline.connect(\"double\", \"second_addition\")\n\n    return (\n        pipeline,\n        [\n            PipelineRunData(\n                inputs={\"first_addition\": {\"value\": 1}},\n                expected_outputs={\"second_addition\": {\"result\": 7}},\n                expected_run_order=[\"first_addition\", \"double\", \"second_addition\"],\n            )\n        ],\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining a Gherkin Scenario for a Correct Pipeline\nDESCRIPTION: This Gherkin scenario outline defines a test case for running a correct pipeline. It uses the `Given`, `When`, and `Then` keywords to specify the test steps. The `Examples` table provides different kinds of pipelines to test.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/test/core/pipeline/features/README.md#_snippet_0\n\nLANGUAGE: Gherkin\nCODE:\n```\n    Scenario Outline: Running a correct Pipeline\n        Given a pipeline <kind>\n        When I run the Pipeline\n        Then it should return the expected result\n\n        Examples:\n        | kind |\n        | that has no components |\n        | that is linear |\n```\n\n----------------------------------------\n\nTITLE: Comparative Aggregate Score Report Output\nDESCRIPTION: This example shows the expected output of the `comparative_aggregate_score_report()` method. It presents the aggregated scores for both models, including Reciprocal Rank, Single Hit, Multi Hit, Context Relevance, Faithfulness, and Semantic Answer Similarity.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/7462-rag-evaluation.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n{\n    \"model_1\": {\n        'Reciprocal Rank': 0.448,\n        'Single Hit': 0.5,\n        'Multi Hit': 0.540,\n        'Context Relevance': 0.537,\n        'Faithfulness': 0.452,\n        'Semantic Answer Similarity': 0.478\n    },\n    \"model_2\": {\n        'Reciprocal Rank': 0.448,\n        'Single Hit': 0.5,\n        'Multi Hit': 0.540,\n        'Context Relevance': 0.537,\n        'Faithfulness': 0.452,\n        'Semantic Answer Similarity': 0.478\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Installing Docker and Docker Compose\nDESCRIPTION: This code snippet provides commands to update the package manager and install Docker and Docker Compose on a Linux system. These are prerequisites for running the Haystack demo application using Docker Compose.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/test/test_files/markdown/sample.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n    # apt-get update && apt-get install docker && apt-get install docker-compose\n    # service docker start\n```\n\n----------------------------------------\n\nTITLE: Building a multi-platform Haystack Docker image with architecture limitations\nDESCRIPTION: This command builds a Haystack Docker image for a specific architecture, addressing potential errors when building multi-platform images.  The `--set \"*.platform=linux/arm64\"` option limits the build to the ARM64 architecture, suitable for systems like Apple M1. Requires Docker, BuildKit, and the `docker-bake.hcl` configuration file.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/docker/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndocker buildx bake base-cpu --set \"*.platform=linux/arm64\"\n```\n\n----------------------------------------\n\nTITLE: Equivalent Python Expression for Simplified Filter\nDESCRIPTION: This Python snippet shows the simplified boolean expression that is equivalent to the newly proposed JSON filter for the \"number\" field. It uses comparison operators ('<=', '>=') and a logical 'AND' operator to represent the filter conditions in Python code.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/6001-document-store-filter-rework.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnumber <= 2 AND number >= 0\n```\n\n----------------------------------------\n\nTITLE: Defining a Python Function for an Infinite Loop Pipeline\nDESCRIPTION: This Python function defines a pipeline with an infinite loop using the Haystack library. It creates custom components and connects them in a way that creates a loop.  It returns the pipeline and `PipelineRunData` specifying the inputs, but omits the expected outputs and run order as an exception is expected.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/test/core/pipeline/features/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n@given(\"a pipeline that has an infinite loop\", target_fixture=\"pipeline_data\")\ndef pipeline_that_has_an_infinite_loop():\n    def custom_init(self):\n        component.set_input_type(self, \"x\", int)\n        component.set_input_type(self, \"y\", int, 1)\n        component.set_output_types(self, a=int, b=int)\n\n    FakeComponent = component_class(\"FakeComponent\", output={\"a\": 1, \"b\": 1}, extra_fields={\"__init__\": custom_init})\n    pipe.add_component(\"first\", FakeComponent())\n    pipe.add_component(\"second\", FakeComponent())\n    pipe.connect(\"first.a\", \"second.x\")\n    pipe.connect(\"second.b\", \"first.y\")\n    return pipe, [PipelineRunData({\"first\": {\"x\": 1}})]\n```\n\n----------------------------------------\n\nTITLE: Building a specific Haystack Docker image using BuildKit\nDESCRIPTION: This command uses Docker BuildKit and `bake` to build a specific Haystack Docker image.  In this case, it builds the 'gpu' image based on the configurations in the `docker-bake.hcl` file. Requires Docker and BuildKit to be installed.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/docker/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ndocker buildx bake gpu\n```\n\n----------------------------------------\n\nTITLE: Example DELIMITER, PATTERN, CHAR_REPLACEMENT in Python\nDESCRIPTION: Example values of DELIMITER, PATTERN, CHAR_REPLACEMENT\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/proposals/text/4172-shaper-in-prompt-template.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nDELIMITER = \"\\n\"\nPATTERN = \"$content\" # parsable by StringTemplate using data from document.content, document.meta and the index of the document\nCHAR_REPLACEMENT = {\"[\": \"(\", \"}\": \")\"} # just an example what could be passed here\n```\n\n----------------------------------------\n\nTITLE: Defining a Gherkin Scenario for a Bad Pipeline\nDESCRIPTION: This Gherkin scenario outline defines a test case for running a bad pipeline that is expected to raise an exception. The `Examples` table specifies the kind of pipeline and the expected exception.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/test/core/pipeline/features/README.md#_snippet_2\n\nLANGUAGE: Gherkin\nCODE:\n```\n    Scenario Outline: Running a bad Pipeline\n        Given a pipeline <kind>\n        When I run the Pipeline\n        Then it must have raised <exception>\n\n        Examples:\n        | kind | exception |\n        | that has an infinite loop | PipelineMaxLoops |\n```\n\n----------------------------------------\n\nTITLE: Installing Haystack via pip\nDESCRIPTION: This code snippet demonstrates how to install the Haystack library using pip, the Python package installer. It installs the core haystack package, enabling users to leverage Haystack's features for question answering and search.\nSOURCE: https://github.com/deepset-ai/haystack/blob/main/test/test_files/markdown/sample.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install farm-haystack\n```"
  }
]
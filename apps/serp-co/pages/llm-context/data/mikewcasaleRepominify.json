[
  {
    "owner": "mikewcasale",
    "repo": "repominify",
    "content": "TITLE: Azure Client Singletons and Manager Implementation - Python\nDESCRIPTION: Implements singleton patterns for Azure Cosmos DB and Blob Storage clients, plus a comprehensive client manager class. Includes credential handling and client initialization with environment variable configuration.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nimport os\n\nfrom azure.cosmos import (\n    ContainerProxy,\n    CosmosClient,\n    DatabaseProxy,\n)\nfrom azure.identity import DefaultAzureCredential\nfrom azure.storage.blob import BlobServiceClient\nfrom azure.storage.blob.aio import BlobServiceClient as BlobServiceClientAsync\nfrom environs import Env\n\nENDPOINT_ERROR_MSG = \"Could not find connection string in environment variables\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\nclass CosmosClientSingleton:\n    _instance = None\n    _env = Env()\n\n    @classmethod\n    def get_instance(cls):\n        if cls._instance is None:\n            endpoint = os.environ[\"COSMOS_URI_ENDPOINT\"]\n            credential = DefaultAzureCredential()\n            cls._instance = CosmosClient(endpoint, credential)\n        return cls._instance\n\n\nclass BlobServiceClientSingleton:\n    _instance = None\n    _env = Env()\n\n    @classmethod\n    def get_instance(cls):\n        if cls._instance is None:\n            account_url = os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"]\n            credential = DefaultAzureCredential()\n            cls._instance = BlobServiceClient(account_url, credential=credential)\n        return cls._instance\n\n    @classmethod\n    def get_storage_account_name(cls):\n        account_url = os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"]\n        return account_url.split(\"//\")[1].split(\".\")[0]\n\n\nclass BlobServiceClientSingletonAsync:\n    _instance = None\n    _env = Env()\n\n    @classmethod\n    def get_instance(cls):\n        if cls._instance is None:\n            account_url = os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"]\n            credential = DefaultAzureCredential()\n            cls._instance = BlobServiceClientAsync(account_url, credential=credential)\n        return cls._instance\n\n    @classmethod\n    def get_storage_account_name(cls):\n        account_url = os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"]\n        return account_url.split(\"//\")[1].split(\".\")[0]\n\n\ndef get_database_client(database_name: str) -> DatabaseProxy:\n    client = CosmosClientSingleton.get_instance()\n    return client.get_database_client(database_name)\n\n\ndef get_database_container_client(\n    database_name: str, container_name: str\n) -> ContainerProxy:\n    db_client = get_database_client(database_name)\n    return db_client.get_container_client(container_name)\n\n\nclass AzureStorageClientManager:\n    \"\"\"\n    Manages the Azure storage clients for blob storage and Cosmos DB.\n\n    Attributes:\n        azure_storage_blob_url (str): The blob endpoint for azure storage.\n        cosmos_uri_endpoint (str): The uri endpoint for the Cosmos DB.\n        _blob_service_client (BlobServiceClient): The blob service client.\n        _blob_service_client_async (BlobServiceClientAsync): The asynchronous blob service client.\n        _cosmos_client (CosmosClient): The Cosmos DB client.\n        _cosmos_database_client (DatabaseProxy): The Cosmos DB database client.\n        _cosmos_container_client (ContainerProxy): The Cosmos DB container client.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._env = Env()\n        self.azure_storage_blob_url = self._env.str(\n            \"STORAGE_ACCOUNT_BLOB_URL\", ENDPOINT_ERROR_MSG\n        )\n        self.cosmos_uri_endpoint = self._env.str(\n            \"COSMOS_URI_ENDPOINT\", ENDPOINT_ERROR_MSG\n        )\n        credential = DefaultAzureCredential()\n        self._blob_service_client = BlobServiceClient(\n            account_url=os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"], credential=credential\n        )\n        self._blob_service_client_async = BlobServiceClientAsync(\n            account_url=os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"], credential=credential\n        )\n        self._cosmos_client = CosmosClient(\n            url=os.environ[\"COSMOS_URI_ENDPOINT\"], credential=credential\n        )\n\n    def get_blob_service_client(self) -> BlobServiceClient:\n        \"\"\"\n        Returns the blob service client.\n\n        Returns:\n            BlobServiceClient: The blob service client.\n        \"\"\"\n        return self._blob_service_client\n\n    def get_blob_service_client_async(self) -> BlobServiceClientAsync:\n        \"\"\"\n        Returns the asynchronous blob service client.\n\n        Returns:\n            BlobServiceClientAsync: The asynchronous blob service client.\n        \"\"\"\n        return self._blob_service_client_async\n\n    def get_cosmos_client(self) -> CosmosClient:\n        \"\"\"\n        Returns the Cosmos DB client.\n\n        Returns:\n            CosmosClient: The Cosmos DB client.\n        \"\"\"\n        return self._cosmos_client\n\n    def get_cosmos_database_client(self, database_name: str) -> DatabaseProxy:\n        \"\"\"\n        Returns the Cosmos DB database client.\n\n        Args:\n            database_name (str): The name of the database.\n\n        Returns:\n            DatabaseProxy: The Cosmos DB database client.\n        \"\"\"\n        if not hasattr(self, \"_cosmos_database_client\"):\n            self._cosmos_database_client = self._cosmos_client.get_database_client(\n                database=database_name\n            )\n        return self._cosmos_database_client\n\n    def get_cosmos_container_client(\n        self, database_name: str, container_name: str\n    ) -> ContainerProxy:\n        \"\"\"\n        Returns the Cosmos DB container client.\n\n        Args:\n            database_name (str): The name of the database.\n            container_name (str): The name of the container.\n\n        Returns:\n            ContainerProxy: The Cosmos DB container client.\n        \"\"\"\n        if not hasattr(self, \"_cosmos_container_client\"):\n            self._cosmos_container_client = self.get_cosmos_database_client(\n                database_name=database_name\n            ).get_container_client(container=container_name)\n        return self._cosmos_container_client\n```\n\n----------------------------------------\n\nTITLE: Implementing Global Search Endpoint for Knowledge Graph Query\nDESCRIPTION: Defines a POST endpoint for global searching across knowledge graph indices. This function processes the query request, validates index completeness, loads data from multiple sources, and returns search results with context.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n@query_route.post(\n    \"/global\",\n    summary=\"Perform a global search across the knowledge graph index\",\n    description=\"The global query method generates answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole.\",\n    response_model=GraphResponse,\n    responses={200: {\"model\": GraphResponse}},\n)\nasync def global_query(request: GraphRequest):\n    # this is a slightly modified version of the graphrag.query.cli.run_global_search method\n    if isinstance(request.index_name, str):\n        index_names = [request.index_name]\n    else:\n        index_names = request.index_name\n    sanitized_index_names = [sanitize_name(name) for name in index_names]\n    sanitized_index_names_link = {\n        s: i for s, i in zip(sanitized_index_names, index_names)\n    }\n\n    for index_name in sanitized_index_names:\n        if not _is_index_complete(index_name):\n            raise HTTPException(\n                status_code=500,\n                detail=f\"{index_name} not ready for querying.\",\n            )\n\n    COMMUNITY_REPORT_TABLE = \"output/create_final_community_reports.parquet\"\n    ENTITIES_TABLE = \"output/create_final_entities.parquet\"\n    NODES_TABLE = \"output/create_final_nodes.parquet\"\n\n    for index_name in sanitized_index_names:\n        validate_index_file_exist(index_name, COMMUNITY_REPORT_TABLE)\n        validate_index_file_exist(index_name, ENTITIES_TABLE)\n        validate_index_file_exist(index_name, NODES_TABLE)\n\n    if isinstance(request.community_level, int):\n        COMMUNITY_LEVEL = request.community_level\n    else:\n        # Current investigations show that community level 1 is the most useful for global search. Set this as the default value\n        COMMUNITY_LEVEL = 1\n\n    try:\n        links = {\n            \"nodes\": {},\n            \"community\": {},\n            \"entities\": {},\n            \"text_units\": {},\n            \"relationships\": {},\n            \"covariates\": {},\n        }\n        max_vals = {\n            \"nodes\": -1,\n            \"community\": -1,\n            \"entities\": -1,\n            \"text_units\": -1,\n            \"relationships\": -1,\n            \"covariates\": -1,\n        }\n\n        community_dfs = []\n        entities_dfs = []\n        nodes_dfs = []\n\n        for index_name in sanitized_index_names:\n            community_report_table_path = (\n                f\"abfs://{index_name}/{COMMUNITY_REPORT_TABLE}\"\n            )\n            entities_table_path = f\"abfs://{index_name}/{ENTITIES_TABLE}\"\n            nodes_table_path = f\"abfs://{index_name}/{NODES_TABLE}\"\n\n            # read the parquet files into DataFrames and add provenance information\n\n            # note that nodes need to set before communities to that max community id makes sense\n            nodes_df = query_helper.get_df(nodes_table_path)\n            for i in nodes_df[\"human_readable_id\"]:\n                links[\"nodes\"][i + max_vals[\"nodes\"] + 1] = {\n                    \"index_name\": sanitized_index_names_link[index_name],\n                    \"id\": i,\n                }\n            if max_vals[\"nodes\"] != -1:\n                nodes_df[\"human_readable_id\"] += max_vals[\"nodes\"] + 1\n            nodes_df[\"community\"] = nodes_df[\"community\"].apply(\n                lambda x: str(int(x) + max_vals[\"community\"] + 1) if x else x\n            )\n            nodes_df[\"title\"] = nodes_df[\"title\"].apply(lambda x: x + f\"-{index_name}\")\n            nodes_df[\"source_id\"] = nodes_df[\"source_id\"].apply(\n                lambda x: \",\".join([i + f\"-{index_name}\" for i in x.split(\",\")])\n            )\n            max_vals[\"nodes\"] = nodes_df[\"human_readable_id\"].max()\n            nodes_dfs.append(nodes_df)\n\n            community_df = query_helper.get_df(community_report_table_path)\n            for i in community_df[\"community\"].astype(int):\n                links[\"community\"][i + max_vals[\"community\"] + 1] = {\n                    \"index_name\": sanitized_index_names_link[index_name],\n                    \"id\": str(i),\n                }\n            if max_vals[\"community\"] != -1:\n                col = community_df[\"community\"].astype(int) + max_vals[\"community\"] + 1\n                community_df[\"community\"] = col.astype(str)\n            max_vals[\"community\"] = community_df[\"community\"].astype(int).max()\n            community_dfs.append(community_df)\n\n            entities_df = query_helper.get_df(entities_table_path)\n            for i in entities_df[\"human_readable_id\"]:\n                links[\"entities\"][i + max_vals[\"entities\"] + 1] = {\n                    \"index_name\": sanitized_index_names_link[index_name],\n                    \"id\": i,\n                }\n            if max_vals[\"entities\"] != -1:\n                entities_df[\"human_readable_id\"] += max_vals[\"entities\"] + 1\n            entities_df[\"name\"] = entities_df[\"name\"].apply(\n                lambda x: x + f\"-{index_name}\"\n            )\n            entities_df[\"text_unit_ids\"] = entities_df[\"text_unit_ids\"].apply(\n                lambda x: [i + f\"-{index_name}\" for i in x]\n            )\n            max_vals[\"entities\"] = entities_df[\"human_readable_id\"].max()\n            entities_dfs.append(entities_df)\n\n        # merge the dataframes\n        nodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True, sort=False)\n        community_combined = pd.concat(\n            community_dfs, axis=0, ignore_index=True, sort=False\n        )\n        entities_combined = pd.concat(\n            entities_dfs, axis=0, ignore_index=True, sort=False\n        )\n\n        # load custom pipeline settings\n        this_directory = os.path.dirname(\n            os.path.abspath(inspect.getfile(inspect.currentframe()))\n        )\n        data = yaml.safe_load(open(f\"{this_directory}/pipeline-settings.yaml\"))\n        # layer the custom settings on top of the default configuration settings of graphrag\n        parameters = create_graphrag_config(data, \".\")\n\n        # perform async search\n        result = await global_search(\n            config=parameters,\n            nodes=nodes_combined,\n            entities=entities_combined,\n            community_reports=community_combined,\n            community_level=COMMUNITY_LEVEL,\n            response_type=\"Multiple Paragraphs\",\n            query=request.query,\n        )\n\n        # link index provenance to the context data\n        context_data = _update_context(result[1], links)\n\n        return GraphResponse(result=result[0], context_data=context_data)\n    except Exception as e:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\n            message=\"Could not perform global search.\",\n            cause=e,\n            stack=traceback.format_exc(),\n        )\n        raise HTTPException(status_code=500, detail=None)\n```\n\n----------------------------------------\n\nTITLE: Main Index Job Manager Function\nDESCRIPTION: The main function for the index job manager that checks both Kubernetes and CosmosDB to determine if an indexing job should be executed. It handles edge cases like jobs that failed catastrophically (e.g., OOM errors) and schedules jobs based on their request time.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_114\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    \"\"\"\n    There are two places to check to determine if an indexing job should be executed:\n        * Kubernetes: check if there are any active k8s jobs running in the cluster\n        * CosmosDB: check if there are any indexing jobs in a scheduled state\n\n    Ideally if an indexing job has finished or failed, the job status will be reflected in cosmosdb.\n    However, if an indexing job failed due to OOM, the job status will not have been updated in cosmosdb.\n\n    To avoid a catastrophic failure scenario where all indexing jobs are stuck in a scheduled state,\n    both checks are necessary.\n    \"\"\"\n    kubernetes_jobs = list_k8s_jobs(os.environ[\"AKS_NAMESPACE\"])\n\n    azure_storage_client_manager = AzureStorageClientManager()\n    job_container_store_client = (\n        azure_storage_client_manager.get_cosmos_container_client(\n            database_name=\"graphrag\", container_name=\"jobs\"\n        )\n    )\n    # retrieve status of all index jobs that are scheduled or running\n    job_metadata = []\n    for item in job_container_store_client.read_all_items():\n        if item[\"status\"] == PipelineJobState.RUNNING.value:\n            # if index job has running state but no associated k8s job, a catastrophic\n            # failure (OOM for example) occurred. Set job status to failed.\n            if len(kubernetes_jobs) == 0:\n                print(\n                    f\"Indexing job for '{item['human_readable_index_name']}' in 'running' state but no associated k8s job found. Updating to failed state.\"\n                )\n                pipelinejob = PipelineJob()\n                pipeline_job = pipelinejob.load_item(item[\"sanitized_index_name\"])\n                pipeline_job[\"status\"] = PipelineJobState.FAILED.value\n            else:\n                print(\n                    f\"Indexing job for '{item['human_readable_index_name']}' already running. Will not schedule another. Exiting...\"\n                )\n                exit()\n        if item[\"status\"] == PipelineJobState.SCHEDULED.value:\n            job_metadata.append({\n                \"human_readable_index_name\": item[\"human_readable_index_name\"],\n                \"epoch_request_time\": item[\"epoch_request_time\"],\n                \"status\": item[\"status\"],\n                \"percent_complete\": item[\"percent_complete\"],\n            })\n\n    # exit if no 'scheduled' jobs were found\n    if not job_metadata:\n        print(\"No jobs found\")\n        exit()\n    # convert to dataframe for easier processing\n    df = pd.DataFrame(job_metadata)\n    # jobs should be run in the order they were requested - sort by epoch_request_time\n    df.sort_values(by=\"epoch_request_time\", ascending=True, inplace=True)\n    index_to_schedule = df.iloc[0][\"human_readable_index_name\"]\n    print(f\"Scheduling job for index: {index_to_schedule}\")\n    schedule_indexing_job(index_to_schedule)\n```\n\n----------------------------------------\n\nTITLE: Combining DataFrames and Executing Local Search in GraphRAG in Python\nDESCRIPTION: Combines multiple DataFrames from different indices, loads custom pipeline settings, and performs streaming local search. This code concatenates nodes, communities, entities, relationships, text units, and covariates DataFrames before executing a local search query.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n        nodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True)\n        community_combined = pd.concat(community_dfs, axis=0, ignore_index=True)\n        entities_combined = pd.concat(entities_dfs, axis=0, ignore_index=True)\n        text_units_combined = pd.concat(text_units_dfs, axis=0, ignore_index=True)\n        relationships_combined = pd.concat(relationships_dfs, axis=0, ignore_index=True)\n        covariates_combined = (\n            pd.concat(covariates_dfs, axis=0, ignore_index=True)\n            if covariates_dfs != []\n            else None\n        )\n\n        # load custom pipeline settings\n        this_directory = os.path.dirname(\n            os.path.abspath(inspect.getfile(inspect.currentframe()))\n        )\n        data = yaml.safe_load(open(f\"{this_directory}/pipeline-settings.yaml\"))\n        # layer the custom settings on top of the default configuration settings of graphrag\n        parameters = create_graphrag_config(data, \".\")\n\n        # add index_names to vector_store args\n        parameters.embeddings.vector_store[\"index_names\"] = sanitized_index_names\n        # internally write over the get_embedding_description_store\n        # method to use the multi-index collection.\n        import graphrag.query.api\n\n        graphrag.query.api._get_embedding_description_store = (\n            _get_embedding_description_store\n        )\n\n        # perform streaming local search\n        return StreamingResponse(\n            _wrapper(\n                local_search_streaming_internal(\n                    config=parameters,\n                    nodes=nodes_combined,\n                    entities=entities_combined,\n                    community_reports=community_combined,\n                    text_units=text_units_combined,\n                    relationships=relationships_combined,\n                    covariates=covariates_combined,\n                    community_level=COMMUNITY_LEVEL,\n                    response_type=\"Multiple Paragraphs\",\n                    query=request.query,\n                ),\n                links,\n            ),\n            media_type=\"application/json\",\n        )\n    except Exception as e:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\n            message=\"Error encountered while streaming local search response\",\n            cause=e,\n            stack=traceback.format_exc(),\n        )\n        raise HTTPException(status_code=500, detail=None)\n```\n\n----------------------------------------\n\nTITLE: FastAPI Backend Server with Kubernetes Integration for GraphRAG\nDESCRIPTION: Main FastAPI application setup for GraphRAG backend with Kubernetes integration. Includes middleware for exception handling, CORS configuration, route registration, Kubernetes CronJob creation for indexing job management, and a health check endpoint.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_92\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport traceback\nfrom contextlib import asynccontextmanager\n\nimport yaml\nfrom fastapi import (\n    FastAPI,\n    Request,\n    status,\n)\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import Response\nfrom kubernetes import (\n    client,\n    config,\n)\n\nfrom src.api.data import data_route\nfrom src.api.graph import graph_route\nfrom src.api.index import index_route\nfrom src.api.index_configuration import index_configuration_route\nfrom src.api.query import query_route\nfrom src.api.query_streaming import query_streaming_route\nfrom src.api.source import source_route\nfrom src.reporting import ReporterSingleton\n\n\nasync def catch_all_exceptions_middleware(request: Request, call_next):\n    \"\"\"a function to globally catch all exceptions and return a 500 response with the exception message\"\"\"\n    try:\n        return await call_next(request)\n    except Exception as e:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\n            message=\"Unexpected internal server error\",\n            cause=e,\n            stack=traceback.format_exc(),\n        )\n        return Response(\"Unexpected internal server error.\", status_code=500)\n\n\n# deploy a cronjob to manage indexing jobs\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # This function is called when the FastAPI application first starts up.\n    # To manage multiple graphrag indexing jobs, we deploy a k8s cronjob.\n    # This cronjob will act as a job manager that creates/manages the execution of graphrag indexing jobs as they are requested.\n    try:\n        # Check if the cronjob exists and create it if it does not exist\n        config.load_incluster_config()\n        # retrieve the running pod spec\n        core_v1 = client.CoreV1Api()\n        pod_name = os.environ[\"HOSTNAME\"]\n        pod = core_v1.read_namespaced_pod(\n            name=pod_name, namespace=os.environ[\"AKS_NAMESPACE\"]\n        )\n        # load the cronjob manifest template and update PLACEHOLDER values with correct values using the pod spec\n        with open(\"indexing-job-manager-template.yaml\", \"r\") as f:\n            manifest = yaml.safe_load(f)\n        manifest[\"spec\"][\"jobTemplate\"][\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\n            \"image\"\n        ] = pod.spec.containers[0].image\n        manifest[\"spec\"][\"jobTemplate\"][\"spec\"][\"template\"][\"spec\"][\n            \"serviceAccountName\"\n        ] = pod.spec.service_account_name\n        # retrieve list of existing cronjobs\n        batch_v1 = client.BatchV1Api()\n        namespace_cronjobs = batch_v1.list_namespaced_cron_job(\n            namespace=os.environ[\"AKS_NAMESPACE\"]\n        )\n        cronjob_names = [cronjob.metadata.name for cronjob in namespace_cronjobs.items]\n        # create cronjob if it does not exist\n        if manifest[\"metadata\"][\"name\"] not in cronjob_names:\n            batch_v1.create_namespaced_cron_job(\n                namespace=os.environ[\"AKS_NAMESPACE\"], body=manifest\n            )\n    except Exception as e:\n        print(\"Failed to create graphrag cronjob.\")\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\n            message=\"Failed to create graphrag cronjob\",\n            cause=str(e),\n            stack=traceback.format_exc(),\n        )\n    yield  # This is where the application starts up.\n    # shutdown/garbage collection code goes here\n\n\napp = FastAPI(\n    docs_url=\"/manpage/docs\",\n    openapi_url=\"/manpage/openapi.json\",\n    title=\"GraphRAG\",\n    version=os.getenv(\"GRAPHRAG_VERSION\", \"undefined_version\"),\n    lifespan=lifespan,\n)\napp.middleware(\"http\")(catch_all_exceptions_middleware)\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\napp.include_router(data_route)\napp.include_router(index_route)\napp.include_router(query_route)\napp.include_router(query_streaming_route)\napp.include_router(index_configuration_route)\napp.include_router(source_route)\napp.include_router(graph_route)\n\n\n# health check endpoint\n@app.get(\n    \"/health\",\n    summary=\"API health check\",\n)\ndef health_check():\n    \"\"\"Returns a 200 response to indicate the API is healthy.\"\"\"\n    return Response(status_code=status.HTTP_200_OK)\n```\n\n----------------------------------------\n\nTITLE: MultiAzureAISearch Class Implementation\nDESCRIPTION: A class that extends BaseVectorStore to implement Azure AI Search vector storage capabilities. It supports multiple collections, connecting to Azure AI Search, and performing similarity searches using embeddings. The class includes methods for filtering by ID and searching with vectors or text.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\nclass MultiAzureAISearch(BaseVectorStore):\n    \"\"\"The Azure AI Search vector storage implementation.\"\"\"\n\n    def __init__(\n        self,\n        collection_name: str,\n        db_connection: Any,\n        document_collection: Any,\n        query_filter: Any | None = None,\n        **kwargs: Any,\n    ):\n        self.collection_name = collection_name\n        self.db_connection = db_connection\n        self.document_collection = document_collection\n        self.query_filter = query_filter\n        self.kwargs = kwargs\n        self.collections = []\n\n    def add_collection(self, collection_name: str):\n        self.collections.append(collection_name)\n\n    def connect(self, **kwargs: Any) -> Any:\n        \"\"\"Connect to the AzureAI vector store.\"\"\"\n        self.url = kwargs.get(\"url\", None)\n        self.vector_size = kwargs.get(\"vector_size\", 1536)\n\n        self.vector_search_profile_name = kwargs.get(\n            \"vector_search_profile_name\", \"vectorSearchProfile\"\n        )\n\n        if self.url:\n            pass\n        else:\n            not_supported_error = (\n                \"Azure AI Search client is not supported on local host.\"\n            )\n            raise ValueError(not_supported_error)\n\n    def load_documents(\n        self, documents: list[VectorStoreDocument], overwrite: bool = True\n    ) -> None:\n        raise NotImplementedError(\"load_documents() method not implemented\")\n\n    def filter_by_id(self, include_ids: list[str] | list[int]) -> Any:\n        \"\"\"Build a query filter to filter documents by a list of ids.\"\"\"\n        if include_ids is None or len(include_ids) == 0:\n            self.query_filter = None\n            # returning to keep consistency with other methods, but not needed\n            return self.query_filter\n\n        # more info about odata filtering here: https://learn.microsoft.com/en-us/azure/search/search-query-odata-search-in-function\n        # search.in is faster that joined and/or conditions\n        id_filter = \",\".join([f\"{id!s}\" for id in include_ids])\n        self.query_filter = f\"search.in(id, '{id_filter}', ',')\" \n\n        # returning to keep consistency with other methods, but not needed\n        # TODO: Refactor on a future PR\n        return self.query_filter\n\n    def similarity_search_by_vector(\n        self, query_embedding: list[float], k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n        \"\"\"Perform a vector-based similarity search.\"\"\"\n        vectorized_query = VectorizedQuery(\n            vector=query_embedding, k_nearest_neighbors=k, fields=\"vector\"\n        )\n\n        docs = []\n        for collection_name in self.collections:\n            add_on = \"-\" + str(collection_name.split(\"_\")[0])\n            audience = os.environ[\"AI_SEARCH_AUDIENCE\"]\n            db_connection = SearchClient(\n                self.url,\n                collection_name,\n                DefaultAzureCredential(),\n                audience=audience,\n            )\n            response = db_connection.search(\n                vector_queries=[vectorized_query],\n            )\n            mod_response = []\n            for r in response:\n                r[\"id\"] = r.get(\"id\", \"\") + add_on\n                mod_response += [r]\n            docs += mod_response\n        return [\n            VectorStoreSearchResult(\n                document=VectorStoreDocument(\n                    id=doc.get(\"id\", \"\"),\n                    text=doc.get(\"text\", \"\"),\n                    vector=doc.get(\"vector\", []),\n                    attributes=(json.loads(doc.get(\"attributes\", \"{}\"))),\n                ),\n                score=abs(doc[\"@search.score\"]),\n            )\n            for doc in docs\n        ]\n\n    def similarity_search_by_text(\n        self, text: str, text_embedder: TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n        \"\"\"Perform a text-based similarity search.\"\"\"\n        query_embedding = text_embedder(text)\n        if query_embedding:\n            return self.similarity_search_by_vector(\n                query_embedding=query_embedding, k=k\n            )\n        return []\n```\n\n----------------------------------------\n\nTITLE: Exception Handling for Pipeline Failures in Python\nDESCRIPTION: Manages exceptions in the indexing pipeline by updating the job status to failed, logging detailed error information to both local and global logs, and raising an HTTP exception. Provides comprehensive error reporting for debugging.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nexcept Exception as e:\n    pipeline_job.status = PipelineJobState.FAILED\n\n    # update failed state in cosmos db\n    error_details = {\n        \"index\": index_name,\n        \"storage_name\": storage_name,\n    }\n    # log error in local index directory logs\n    workflow_callbacks.on_error(\n        message=f\"Indexing pipeline failed for index '{index_name}'.\",\n        cause=e,\n        stack=traceback.format_exc(),\n        details=error_details,\n    )\n    # log error in global index directory logs\n    reporter.on_error(\n        message=f\"Indexing pipeline failed for index '{index_name}'.\",\n        cause=e,\n        stack=traceback.format_exc(),\n        details=error_details,\n    )\n    raise HTTPException(\n        status_code=500,\n        detail=f\"Error encountered during indexing job for index '{index_name}'.\",\n    )\n```\n\n----------------------------------------\n\nTITLE: End-to-End Indexing and Query Test in Python\nDESCRIPTION: Comprehensive test that runs through the entire indexing process and verifies query functionality, including global and local searches.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_85\n\nLANGUAGE: python\nCODE:\n```\ndef test_indexing_end_to_end(client, run_indexing):\n    passed, index_name = run_indexing\n    assert passed\n\n    query_endpoint = \"/query\"\n    graph_endpoint = \"/graph\"\n\n    request = {\"index_name\": index_name, \"query\": \"Where is Alabama?\"}\n    global_response_new = client.post(\n        f\"{client.base_url}{query_endpoint}/global\", json=request\n    )\n    assert global_response_new.status_code == 200\n    \n    # Additional test logic omitted for brevity\n```\n\n----------------------------------------\n\nTITLE: Scheduling Kubernetes Indexing Jobs with CosmosDB\nDESCRIPTION: Schedules a Kubernetes job to run GraphRAG indexing for a given index name. Uses the Kubernetes API to create a job based on a template, with error handling that updates job status in CosmosDB if scheduling fails.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_111\n\nLANGUAGE: python\nCODE:\n```\ndef schedule_indexing_job(index_name: str):\n    \"\"\"\n    Schedule a k8s job to run graphrag indexing for a given index name.\n    \"\"\"\n    try:\n        config.load_incluster_config()\n        # get container image name\n        core_v1 = client.CoreV1Api()\n        pod_name = os.environ[\"HOSTNAME\"]\n        pod = core_v1.read_namespaced_pod(\n            name=pod_name, namespace=os.environ[\"AKS_NAMESPACE\"]\n        )\n        # retrieve job manifest template and replace necessary values\n        job_manifest = _generate_aks_job_manifest(\n            docker_image_name=pod.spec.containers[0].image,\n            index_name=index_name,\n            service_account_name=pod.spec.service_account_name,\n        )\n        batch_v1 = client.BatchV1Api()\n        batch_v1.create_namespaced_job(\n            body=job_manifest, namespace=os.environ[\"AKS_NAMESPACE\"]\n        )\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\n            \"Index job manager encountered error scheduling indexing job\",\n        )\n        # In the event of a catastrophic scheduling failure, something in k8s or the job manifest is likely broken.\n        # Set job status to failed to prevent an infinite loop of re-scheduling\n        pipelinejob = PipelineJob()\n        pipeline_job = pipelinejob.load_item(sanitize_name(index_name))\n        pipeline_job[\"status\"] = PipelineJobState.FAILED\n```\n\n----------------------------------------\n\nTITLE: Executing GraphRAG Indexing Pipeline\nDESCRIPTION: Asynchronous function that runs the actual indexing pipeline. Sets up a container in Azure Storage, configures pipeline settings including custom prompts, initializes reporters and callbacks, and executes the pipeline. Handles workflow tracking and error reporting.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nasync def _start_indexing_pipeline(index_name: str):\n    # get sanitized name\n    sanitized_index_name = sanitize_name(index_name)\n\n    # update or create new item in container-store in cosmosDB\n    _blob_service_client = BlobServiceClientSingleton().get_instance()\n    if not _blob_service_client.get_container_client(sanitized_index_name).exists():\n        _blob_service_client.create_container(sanitized_index_name)\n    container_store_client = get_database_container_client(\n        database_name=\"graphrag\", container_name=\"container-store\"\n    )\n    container_store_client.upsert_item({\n        \"id\": sanitized_index_name,\n        \"human_readable_name\": index_name,\n        \"type\": \"index\",\n    })\n\n    reporter = ReporterSingleton().get_instance()\n    pipelinejob = PipelineJob()\n    pipeline_job = pipelinejob.load_item(sanitized_index_name)\n    sanitized_storage_name = pipeline_job.sanitized_storage_name\n    storage_name = pipeline_job.human_readable_index_name\n\n    # download nltk dependencies\n    bootstrap()\n\n    # load custom pipeline settings\n    this_directory = os.path.dirname(\n        os.path.abspath(inspect.getfile(inspect.currentframe()))\n    )\n    data = yaml.safe_load(open(f\"{this_directory}/pipeline-settings.yaml\"))\n    # dynamically set some values\n    data[\"input\"][\"container_name\"] = sanitized_storage_name\n    data[\"storage\"][\"container_name\"] = sanitized_index_name\n    data[\"reporting\"][\"container_name\"] = sanitized_index_name\n    data[\"cache\"][\"container_name\"] = sanitized_index_name\n    if \"vector_store\" in data[\"embeddings\"]:\n        data[\"embeddings\"][\"vector_store\"][\"collection_name\"] = (\n            f\"{sanitized_index_name}_description_embedding\"\n        )\n\n    # set prompts for entity extraction, community report, and summarize descriptions.\n    if pipeline_job.entity_extraction_prompt:\n        fname = \"entity-extraction-prompt.txt\"\n        with open(fname, \"w\") as outfile:\n            outfile.write(pipeline_job.entity_extraction_prompt)\n        data[\"entity_extraction\"][\"prompt\"] = fname\n    else:\n        data.pop(\"entity_extraction\")\n    if pipeline_job.community_report_prompt:\n        fname = \"community-report-prompt.txt\"\n        with open(fname, \"w\") as outfile:\n            outfile.write(pipeline_job.community_report_prompt)\n        data[\"community_reports\"][\"prompt\"] = fname\n    else:\n        data.pop(\"community_reports\")\n    if pipeline_job.summarize_descriptions_prompt:\n        fname = \"summarize-descriptions-prompt.txt\"\n        with open(fname, \"w\") as outfile:\n            outfile.write(pipeline_job.summarize_descriptions_prompt)\n        data[\"summarize_descriptions\"][\"prompt\"] = fname\n    else:\n        data.pop(\"summarize_descriptions\")\n\n    # generate the default pipeline and override with custom settings\n    parameters = create_graphrag_config(data, \".\")\n    pipeline_config = create_pipeline_config(parameters, True)\n\n    # reset pipeline job details\n    pipeline_job.status = PipelineJobState.RUNNING\n    pipeline_job.all_workflows = []\n    pipeline_job.completed_workflows = []\n    pipeline_job.failed_workflows = []\n    for workflow in pipeline_config.workflows:\n        pipeline_job.all_workflows.append(workflow.name)\n\n    # create new reporters/callbacks just for this job\n    reporters = []\n    reporter_names = os.getenv(\"REPORTERS\", Reporters.CONSOLE.name.upper()).split(\",\")\n    for reporter_name in reporter_names:\n        try:\n            reporters.append(Reporters[reporter_name.upper()])\n        except KeyError:\n            raise ValueError(f\"Unknown reporter type: {reporter_name}\")\n    workflow_callbacks = load_pipeline_reporter(\n        index_name=index_name,\n        num_workflow_steps=len(pipeline_job.all_workflows),\n        reporting_dir=sanitized_index_name,\n        reporters=reporters,\n    )\n\n    # add pipeline job callback to the callback manager\n    cast(WorkflowCallbacksManager, workflow_callbacks).register(\n        PipelineJobWorkflowCallbacks(pipeline_job)\n    )\n\n    # run the pipeline\n    try:\n        async for workflow_result in run_pipeline_with_config(\n            config_or_path=pipeline_config,\n            callbacks=workflow_callbacks,\n            progress_reporter=None,\n        ):\n            await asyncio.sleep(0)\n            if len(workflow_result.errors or []) > 0:\n                # if the workflow failed, record the failure\n                pipeline_job.failed_workflows.append(workflow_result.workflow)\n                pipeline_job.update_db()\n```\n\n----------------------------------------\n\nTITLE: GraphRAG Indexing Job Runner Script\nDESCRIPTION: A separate script that runs the actual indexing pipeline. It parses command-line arguments to get the index name and executes the indexing process asynchronously. This script is executed by the Kubernetes job created in the main scheduler.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_115\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport asyncio\n\nfrom src import main  # noqa: F401\nfrom src.api.index import _start_indexing_pipeline\n\nparser = argparse.ArgumentParser(description=\"Kickoff indexing job.\")\nparser.add_argument(\"-i\", \"--index-name\", required=True)\nargs = parser.parse_args()\n\nasyncio.run(\n    _start_indexing_pipeline(\n        index_name=args.index_name,\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Delete Index API Endpoint in FastAPI\nDESCRIPTION: API endpoint for deleting an index and all associated resources. It terminates Kubernetes jobs, removes blob storage containers, deletes CosmosDB entries, and removes AI search indexes. Implements comprehensive error handling for each deletion step.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n@index_route.delete(\n    \"/{index_name}\",\n    summary=\"Delete a specified index\",\n    response_model=BaseResponse,\n    responses={200: {\"model\": BaseResponse}},\n)\nasync def delete_index(index_name: str):\n    \"\"\"\n    Delete a specified index.\n    \"\"\"\n    sanitized_index_name = sanitize_name(index_name)\n    try:\n        # kill indexing job if it is running\n        if os.getenv(\"KUBERNETES_SERVICE_HOST\"):  # only found if in AKS\n            _delete_k8s_job(f\"indexing-job-{sanitized_index_name}\", \"graphrag\")\n\n        # remove blob container and all associated entries in cosmos db\n        try:\n            delete_blob_container(sanitized_index_name)\n        except Exception:\n            pass\n\n        # update container-store in cosmosDB\n        try:\n            container_store_client = get_database_container_client(\n                database_name=\"graphrag\", container_name=\"container-store\"\n            )\n            container_store_client.delete_item(\n                item=sanitized_index_name, partition_key=sanitized_index_name\n            )\n        except Exception:\n            pass\n\n        # update jobs database in cosmosDB\n        try:\n            jobs_container = get_database_container_client(\n                database_name=\"graphrag\", container_name=\"jobs\"\n            )\n            jobs_container.delete_item(\n                item=sanitized_index_name, partition_key=sanitized_index_name\n            )\n        except Exception:\n            pass\n\n        index_client = SearchIndexClient(\n            endpoint=ai_search_url,\n            credential=DefaultAzureCredential(),\n            audience=ai_search_audience,\n        )\n        ai_search_index_name = f\"{sanitized_index_name}_description_embedding\"\n        if ai_search_index_name in index_client.list_index_names():\n            index_client.delete_index(ai_search_index_name)\n\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\n            message=f\"Error encountered while deleting all data for index {index_name}.\",\n            stack=None,\n            details={\"container\": index_name},\n        )\n        raise HTTPException(\n            status_code=500, detail=f\"Error deleting index '{index_name}'.\"\n        )\n\n    return BaseResponse(status=\"Success\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Index Creation Endpoint\nDESCRIPTION: Defines a POST endpoint to initiate index building. Validates input parameters, checks for existing data sources, and handles prompts for entity extraction, community reporting, and description summarization. Manages the pipeline job state, including restarting failed jobs.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@index_route.post(\n    \"\",\n    summary=\"Build an index\",\n    response_model=BaseResponse,\n    responses={200: {\"model\": BaseResponse}},\n)\nasync def setup_indexing_pipeline(\n    storage_name: str,\n    index_name: str,\n    entity_extraction_prompt: UploadFile | None = None,\n    community_report_prompt: UploadFile | None = None,\n    summarize_descriptions_prompt: UploadFile | None = None,\n):\n    _blob_service_client = BlobServiceClientSingleton().get_instance()\n    pipelinejob = PipelineJob()\n\n    # validate index name against blob container naming rules\n    sanitized_index_name = sanitize_name(index_name)\n    try:\n        validate_blob_container_name(sanitized_index_name)\n    except ValueError:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Invalid index name: {index_name}\",\n        )\n\n    # check for data container existence\n    sanitized_storage_name = sanitize_name(storage_name)\n    if not _blob_service_client.get_container_client(sanitized_storage_name).exists():\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Storage blob container {storage_name} does not exist\",\n        )\n\n    # check for prompts\n    entity_extraction_prompt_content = (\n        entity_extraction_prompt.file.read().decode(\"utf-8\")\n        if entity_extraction_prompt\n        else None\n    )\n    community_report_prompt_content = (\n        community_report_prompt.file.read().decode(\"utf-8\")\n        if community_report_prompt\n        else None\n    )\n    summarize_descriptions_prompt_content = (\n        summarize_descriptions_prompt.file.read().decode(\"utf-8\")\n        if summarize_descriptions_prompt\n        else None\n    )\n\n    # check for existing index job\n    # it is okay if job doesn't exist, but if it does,\n    # it must not be scheduled or running\n    if pipelinejob.item_exist(sanitized_index_name):\n        existing_job = pipelinejob.load_item(sanitized_index_name)\n        if (PipelineJobState(existing_job.status) == PipelineJobState.SCHEDULED) or (\n            PipelineJobState(existing_job.status) == PipelineJobState.RUNNING\n        ):\n            raise HTTPException(\n                status_code=202,  # request has been accepted for processing but is not complete.\n                detail=f\"Index '{index_name}' already exists and has not finished building.\",\n            )\n        # if indexing job is in a failed state, delete the associated K8s job and pod to allow for a new job to be scheduled\n        if PipelineJobState(existing_job.status) == PipelineJobState.FAILED:\n            _delete_k8s_job(\n                f\"indexing-job-{sanitized_index_name}\", os.environ[\"AKS_NAMESPACE\"]\n            )\n        # reset the pipeline job details\n        existing_job._status = PipelineJobState.SCHEDULED\n        existing_job._percent_complete = 0\n        existing_job._progress = \"\"\n        existing_job._all_workflows = existing_job._completed_workflows = (\n            existing_job._failed_workflows\n        ) = []\n        existing_job._entity_extraction_prompt = entity_extraction_prompt_content\n        existing_job._community_report_prompt = community_report_prompt_content\n        existing_job._summarize_descriptions_prompt = (\n            summarize_descriptions_prompt_content\n        )\n        existing_job._epoch_request_time = int(time())\n        existing_job.update_db()\n    else:\n        pipelinejob.create_item(\n            id=sanitized_index_name,\n            human_readable_index_name=index_name,\n            human_readable_storage_name=storage_name,\n            entity_extraction_prompt=entity_extraction_prompt_content,\n            community_report_prompt=community_report_prompt_content,\n            summarize_descriptions_prompt=summarize_descriptions_prompt_content,\n            status=PipelineJobState.SCHEDULED,\n        )\n\n    return BaseResponse(status=\"Indexing job scheduled\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Logger with Azure Monitor Integration\nDESCRIPTION: Method to initialize a logger with Azure Monitor integration. It generates a unique logger name, attaches the Azure Monitor exporter, and sets up logging handlers with proper configuration for telemetry.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ndef __init_logger(self, connection_string, max_logger_init_retries: int = 10):\n    max_retry = max_logger_init_retries\n    while not (hasattr(self, \"_logger\")):\n        if max_retry == 0:\n            raise Exception(\n                \"Failed to create logger. Could not disambiguate logger name.\"\n            )\n\n        # generate a unique logger name\n        current_time = str(time.time())\n        unique_hash = hashlib.sha256(current_time.encode()).hexdigest()\n        self._logger_name = f\"{self.__class__.__name__}-{unique_hash}\"\n        if self._logger_name not in logging.Logger.manager.loggerDict:\n            # attach azure monitor log exporter to logger provider\n            logger_provider = LoggerProvider()\n            set_logger_provider(logger_provider)\n            exporter = AzureMonitorLogExporter(connection_string=connection_string)\n            get_logger_provider().add_log_record_processor(\n                BatchLogRecordProcessor(\n                    exporter=exporter,\n                    schedule_delay_millis=60000,\n                )\n            )\n            # instantiate new logger\n            self._logger = logging.getLogger(self._logger_name)\n            self._logger.propagate = False\n            # remove any existing handlers\n            self._logger.handlers.clear()\n            # fetch handler from logger provider and attach to class\n            self._logger.addHandler(LoggingHandler())\n            # set logging level\n            self._logger.setLevel(logging.DEBUG)\n\n        # reduce sentinel counter value\n        max_retry -= 1\n```\n\n----------------------------------------\n\nTITLE: Implementing Global Search Streaming Endpoint with FastAPI\nDESCRIPTION: Defines a POST endpoint for global search that streams responses by searching over all AI-generated community reports. The function handles multiple indices, validates their readiness, loads and transforms data from parquet files, and streams the response to the client.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n@query_streaming_route.post(\n    \"/global\",\n    summary=\"Stream a response back after performing a global search\",\n    description=\"The global query method generates answers by searching over all AI-generated community reports in a map-reduce fashion. This is a resource-intensive method, but often gives good responses for questions that require an understanding of the dataset as a whole.\",\n)\nasync def global_search_streaming(request: GraphRequest):\n    # this is a slightly modified version of src.api.query.global_query() method\n    if isinstance(request.index_name, str):\n        index_names = [request.index_name]\n    else:\n        index_names = request.index_name\n    sanitized_index_names = [sanitize_name(name) for name in index_names]\n    sanitized_index_names_link = {\n        s: i for s, i in zip(sanitized_index_names, index_names)\n    }\n\n    for index_name in sanitized_index_names:\n        if not _is_index_complete(index_name):\n            raise HTTPException(\n                status_code=500,\n                detail=f\"{sanitized_index_names_link[index_name]} not ready for querying.\",\n            )\n\n    COMMUNITY_REPORT_TABLE = \"output/create_final_community_reports.parquet\"\n    ENTITIES_TABLE = \"output/create_final_entities.parquet\"\n    NODES_TABLE = \"output/create_final_nodes.parquet\"\n\n    if isinstance(request.community_level, int):\n        COMMUNITY_LEVEL = request.community_level\n    else:\n        # Current investigations show that community level 1 is the most useful for global search. Set this as the default value\n        COMMUNITY_LEVEL = 1\n\n    for index_name in sanitized_index_names:\n        validate_index_file_exist(index_name, COMMUNITY_REPORT_TABLE)\n        validate_index_file_exist(index_name, ENTITIES_TABLE)\n        validate_index_file_exist(index_name, NODES_TABLE)\n    try:\n        links = {\n            \"nodes\": {},\n            \"community\": {},\n            \"entities\": {},\n            \"text_units\": {},\n            \"relationships\": {},\n            \"covariates\": {},\n        }\n        max_vals = {\n            \"nodes\": -1,\n            \"community\": -1,\n            \"entities\": -1,\n            \"text_units\": -1,\n            \"relationships\": -1,\n            \"covariates\": -1,\n        }\n\n        community_dfs = []\n        entities_dfs = []\n        nodes_dfs = []\n\n        for index_name in sanitized_index_names:\n            community_report_table_path = (\n                f\"abfs://{index_name}/{COMMUNITY_REPORT_TABLE}\"\n            )\n            entities_table_path = f\"abfs://{index_name}/{ENTITIES_TABLE}\"\n            nodes_table_path = f\"abfs://{index_name}/{NODES_TABLE}\"\n\n            # read parquet files into DataFrames and add provenance information\n\n            # note that nodes need to set before communities to that max community id makes sense\n            nodes_df = query_helper.get_df(nodes_table_path)\n            for i in nodes_df[\"human_readable_id\"]:\n                links[\"nodes\"][i + max_vals[\"nodes\"] + 1] = {\n                    \"index_name\": sanitized_index_names_link[index_name],\n                    \"id\": i,\n                }\n            if max_vals[\"nodes\"] != -1:\n                nodes_df[\"human_readable_id\"] += max_vals[\"nodes\"] + 1\n            nodes_df[\"community\"] = nodes_df[\"community\"].apply(\n                lambda x: str(int(x) + max_vals[\"community\"] + 1) if x else x\n            )\n            nodes_df[\"title\"] = nodes_df[\"title\"].apply(lambda x: x + f\"-{index_name}\")\n            nodes_df[\"source_id\"] = nodes_df[\"source_id\"].apply(\n                lambda x: \",\".join([i + f\"-{index_name}\" for i in x.split(\",\")])\n            )\n            max_vals[\"nodes\"] = nodes_df[\"human_readable_id\"].max()\n            nodes_dfs.append(nodes_df)\n\n            community_df = query_helper.get_df(community_report_table_path)\n            for i in community_df[\"community\"].astype(int):\n                links[\"community\"][i + max_vals[\"community\"] + 1] = {\n                    \"index_name\": sanitized_index_names_link[index_name],\n                    \"id\": str(i),\n                }\n            if max_vals[\"community\"] != -1:\n                col = community_df[\"community\"].astype(int) + max_vals[\"community\"] + 1\n                community_df[\"community\"] = col.astype(str)\n            max_vals[\"community\"] = community_df[\"community\"].astype(int).max()\n            community_dfs.append(community_df)\n\n            entities_df = query_helper.get_df(entities_table_path)\n            for i in entities_df[\"human_readable_id\"]:\n                links[\"entities\"][i + max_vals[\"entities\"] + 1] = {\n                    \"index_name\": sanitized_index_names_link[index_name],\n                    \"id\": i,\n                }\n            if max_vals[\"entities\"] != -1:\n                entities_df[\"human_readable_id\"] += max_vals[\"entities\"] + 1\n            entities_df[\"name\"] = entities_df[\"name\"].apply(\n                lambda x: x + f\"-{index_name}\"\n            )\n            entities_df[\"text_unit_ids\"] = entities_df[\"text_unit_ids\"].apply(\n                lambda x: [i + f\"-{index_name}\" for i in x]\n            )\n            max_vals[\"entities\"] = entities_df[\"human_readable_id\"].max()\n            entities_dfs.append(entities_df)\n\n        # merge the dataframes\n        nodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True, sort=False)\n        community_combined = pd.concat(\n            community_dfs, axis=0, ignore_index=True, sort=False\n        )\n        entities_combined = pd.concat(\n            entities_dfs, axis=0, ignore_index=True, sort=False\n        )\n\n        # load custom pipeline settings\n        this_directory = os.path.dirname(\n            os.path.abspath(inspect.getfile(inspect.currentframe()))\n        )\n        data = yaml.safe_load(open(f\"{this_directory}/pipeline-settings.yaml\"))\n        # layer the custom settings on top of the default configuration settings of graphrag\n        parameters = create_graphrag_config(data, \".\")\n\n        return StreamingResponse(\n            _wrapper(\n                global_search_streaming_internal(\n                    config=parameters,\n                    nodes=nodes_combined,\n                    entities=entities_combined,\n                    community_reports=community_combined,\n                    community_level=COMMUNITY_LEVEL,\n                    response_type=\"Multiple Paragraphs\",\n                    query=request.query,\n                ),\n                links,\n            ),\n            media_type=\"application/json\",\n        )\n    except Exception as e:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\n            message=\"Error encountered while streaming global search response\",\n            cause=e,\n            stack=traceback.format_exc(),\n        )\n        raise HTTPException(status_code=500, detail=None)\n```\n\n----------------------------------------\n\nTITLE: Reading Data from Azure Storage with Pandas and GraphRAG\nDESCRIPTION: Functions for retrieving different data types (entities, reports, relationships, covariates, text units) from Azure Blob Storage using pandas and GraphRAG indexer adapters. Requires Azure identity authentication and storage connectivity.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_90\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom azure.identity import DefaultAzureCredential\nfrom graphrag.query.indexer_adapters import (\n    read_indexer_covariates,\n    read_indexer_entities,\n    read_indexer_relationships,\n    read_indexer_reports,\n    read_indexer_text_units,\n)\n\nfrom src.api.azure_clients import BlobServiceClientSingleton\n\nstorage_options = {\n    \"account_name\": BlobServiceClientSingleton.get_storage_account_name(),\n    \"account_host\": BlobServiceClientSingleton.get_instance().url.split(\"//\")[1],\n    \"credential\": DefaultAzureCredential(),\n}\n\n\ndef get_entities(\n    entity_table_path: str,\n    entity_embedding_table_path: str,\n    community_level: int = 0,\n) -> pd.DataFrame:\n    entity_df = pd.read_parquet(\n        entity_table_path,\n        storage_options=storage_options,\n    )\n    entity_embedding_df = pd.read_parquet(\n        entity_embedding_table_path,\n        storage_options=storage_options,\n    )\n    return pd.DataFrame(\n        read_indexer_entities(entity_df, entity_embedding_df, community_level)\n    )\n\n\ndef get_reports(\n    entity_table_path: str, community_report_table_path: str, community_level: int\n) -> pd.DataFrame:\n    entity_df = pd.read_parquet(\n        entity_table_path,\n        storage_options=storage_options,\n    )\n    report_df = pd.read_parquet(\n        community_report_table_path,\n        storage_options=storage_options,\n    )\n    return pd.DataFrame(read_indexer_reports(report_df, entity_df, community_level))\n\n\ndef get_relationships(relationships_table_path: str) -> pd.DataFrame:\n    relationship_df = pd.read_parquet(\n        relationships_table_path,\n        storage_options=storage_options,\n    )\n    return pd.DataFrame(read_indexer_relationships(relationship_df))\n\n\ndef get_covariates(covariate_table_path: str) -> pd.DataFrame:\n    covariate_df = pd.read_parquet(\n        covariate_table_path,\n        storage_options=storage_options,\n    )\n    return pd.DataFrame(read_indexer_covariates(covariate_df))\n\n\ndef get_text_units(text_unit_table_path: str) -> pd.DataFrame:\n    text_unit_df = pd.read_parquet(\n        text_unit_table_path,\n        storage_options=storage_options,\n    )\n    return pd.DataFrame(read_indexer_text_units(text_unit_df))\n\n\ndef get_df(\n    table_path: str,\n) -> pd.DataFrame:\n    df = pd.read_parquet(\n        table_path,\n        storage_options=storage_options,\n    )\n    return df\n```\n\n----------------------------------------\n\nTITLE: Validating and Processing Multiple Indices for GraphRAG Query in Python\nDESCRIPTION: Processes data from multiple indices by validating required files, reading them into DataFrames, and adding provenance information. This code transforms node IDs, community IDs, entity data, relationships, and text units to maintain uniqueness across indices.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    for index_name in sanitized_index_names:\n        # check for existence of files the query relies on to validate the index is complete\n        validate_index_file_exist(index_name, COMMUNITY_REPORT_TABLE)\n        validate_index_file_exist(index_name, ENTITIES_TABLE)\n        validate_index_file_exist(index_name, NODES_TABLE)\n        validate_index_file_exist(index_name, RELATIONSHIPS_TABLE)\n        validate_index_file_exist(index_name, TEXT_UNITS_TABLE)\n\n        community_report_table_path = (\n            f\"abfs://{index_name}/{COMMUNITY_REPORT_TABLE}\"\n        )\n        covariates_table_path = f\"abfs://{index_name}/{COVARIATES_TABLE}\"\n        entities_table_path = f\"abfs://{index_name}/{ENTITIES_TABLE}\"\n        nodes_table_path = f\"abfs://{index_name}/{NODES_TABLE}\"\n        relationships_table_path = f\"abfs://{index_name}/{RELATIONSHIPS_TABLE}\"\n        text_units_table_path = f\"abfs://{index_name}/{TEXT_UNITS_TABLE}\"\n\n        # read the parquet files into DataFrames and add provenance information\n\n        # note that nodes need to set before communities to that max community id makes sense\n        nodes_df = query_helper.get_df(nodes_table_path)\n        for i in nodes_df[\"human_readable_id\"]:\n            links[\"nodes\"][i + max_vals[\"nodes\"] + 1] = {\n                \"index_name\": sanitized_index_names_link[index_name],\n                \"id\": i,\n            }\n        if max_vals[\"nodes\"] != -1:\n            nodes_df[\"human_readable_id\"] += max_vals[\"nodes\"] + 1\n        nodes_df[\"community\"] = nodes_df[\"community\"].apply(\n            lambda x: str(int(x) + max_vals[\"community\"] + 1) if x else x\n        )\n        nodes_df[\"id\"] = nodes_df[\"id\"].apply(lambda x: x + f\"-{index_name}\")\n        nodes_df[\"title\"] = nodes_df[\"title\"].apply(lambda x: x + f\"-{index_name}\")\n        nodes_df[\"source_id\"] = nodes_df[\"source_id\"].apply(\n            lambda x: \",\".join([i + f\"-{index_name}\" for i in x.split(\",\")])\n        )\n        max_vals[\"nodes\"] = nodes_df[\"human_readable_id\"].max()\n        nodes_dfs.append(nodes_df)\n\n        community_df = query_helper.get_df(community_report_table_path)\n        for i in community_df[\"community\"].astype(int):\n            links[\"community\"][i + max_vals[\"community\"] + 1] = {\n                \"index_name\": sanitized_index_names_link[index_name],\n                \"id\": str(i),\n            }\n        if max_vals[\"community\"] != -1:\n            col = community_df[\"community\"].astype(int) + max_vals[\"community\"] + 1\n            community_df[\"community\"] = col.astype(str)\n        max_vals[\"community\"] = community_df[\"community\"].astype(int).max()\n        community_dfs.append(community_df)\n\n        entities_df = query_helper.get_df(entities_table_path)\n        for i in entities_df[\"human_readable_id\"]:\n            links[\"entities\"][i + max_vals[\"entities\"] + 1] = {\n                \"index_name\": sanitized_index_names_link[index_name],\n                \"id\": i,\n            }\n        if max_vals[\"entities\"] != -1:\n            entities_df[\"human_readable_id\"] += max_vals[\"entities\"] + 1\n        entities_df[\"id\"] = entities_df[\"id\"].apply(lambda x: x + f\"-{index_name}\")\n        entities_df[\"name\"] = entities_df[\"name\"].apply(\n            lambda x: x + f\"-{index_name}\"\n        )\n        entities_df[\"text_unit_ids\"] = entities_df[\"text_unit_ids\"].apply(\n            lambda x: [i + f\"-{index_name}\" for i in x]\n        )\n        max_vals[\"entities\"] = entities_df[\"human_readable_id\"].max()\n        entities_dfs.append(entities_df)\n\n        relationships_df = query_helper.get_df(relationships_table_path)\n        for i in relationships_df[\"human_readable_id\"].astype(int):\n            links[\"relationships\"][i + max_vals[\"relationships\"] + 1] = {\n                \"index_name\": sanitized_index_names_link[index_name],\n                \"id\": i,\n            }\n        if max_vals[\"relationships\"] != -1:\n            col = (\n                relationships_df[\"human_readable_id\"].astype(int)\n                + max_vals[\"relationships\"]\n                + 1\n            )\n            relationships_df[\"human_readable_id\"] = col.astype(str)\n        relationships_df[\"source\"] = relationships_df[\"source\"].apply(\n            lambda x: x + f\"-{index_name}\"\n        )\n        relationships_df[\"target\"] = relationships_df[\"target\"].apply(\n            lambda x: x + f\"-{index_name}\"\n        )\n        relationships_df[\"text_unit_ids\"] = relationships_df[\"text_unit_ids\"].apply(\n            lambda x: [i + f\"-{index_name}\" for i in x]\n        )\n        max_vals[\"relationships\"] = (\n            relationships_df[\"human_readable_id\"].astype(int).max()\n        )\n        relationships_dfs.append(relationships_df)\n\n        text_units_df = query_helper.get_df(text_units_table_path)\n        text_units_df[\"id\"] = text_units_df[\"id\"].apply(\n            lambda x: f\"{x}-{index_name}\"\n        )\n        text_units_dfs.append(text_units_df)\n\n        index_container_client = blob_service_client.get_container_client(\n            index_name\n        )\n        if index_container_client.get_blob_client(COVARIATES_TABLE).exists():\n            covariates_df = query_helper.get_df(covariates_table_path)\n            if i in covariates_df[\"human_readable_id\"].astype(int):\n                links[\"covariates\"][i + max_vals[\"covariates\"] + 1] = {\n                    \"index_name\": sanitized_index_names_link[index_name],\n                    \"id\": i,\n                }\n            if max_vals[\"covariates\"] != -1:\n                col = (\n                    covariates_df[\"human_readable_id\"].astype(int)\n                    + max_vals[\"covariates\"]\n                    + 1\n                )\n                covariates_df[\"human_readable_id\"] = col.astype(str)\n            max_vals[\"covariates\"] = (\n                covariates_df[\"human_readable_id\"].astype(int).max()\n            )\n            covariates_dfs.append(covariates_df)\n```\n\n----------------------------------------\n\nTITLE: Creating Valid Test Index Data Fixture in Python\nDESCRIPTION: Pytest fixture that prepares a valid test environment by creating temporary containers in Azure Blob Storage, uploading sample data from Wikipedia, and configuring Cosmos DB with metadata for a complete index. It yields the index name for tests and cleans up all resources afterward.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_79\n\nLANGUAGE: python\nCODE:\n```\n@pytest.fixture()\ndef prepare_valid_index_data():\n    \"\"\"Prepare valid test data by uploading the result files of a \\\"valid\\\" indexing run to a new blob container.\"\"\"\n    account_url = os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"]\n    credential = DefaultAzureCredential()\n    blob_service_client = BlobServiceClient(account_url, credential=credential)\n\n    # generate a unique data container name\n    container_name = \"test-data-\" + str(uuid.uuid4())\n    container_name = container_name.replace(\"_\", \"-\").replace(\".\", \"-\").lower()[:63]\n    blob_service_client.create_container(container_name)\n    this_directory = os.path.dirname(\n        os.path.abspath(inspect.getfile(inspect.currentframe()))\n    )\n    # generate a unique index container name\n    index_name = \"test-index-\" + str(uuid.uuid4())\n    index_name = index_name.replace(\"_\", \"-\").replace(\".\", \"-\").lower()[:63]\n    blob_service_client.create_container(index_name)\n    this_directory = os.path.dirname(\n        os.path.abspath(inspect.getfile(inspect.currentframe()))\n    )\n    # use a small amount of sample text to test the data upload endpoint\n    page = wikipedia.page(\"Alaska\")\n    os.makedirs(f\"{this_directory}/data/sample-data\", exist_ok=True)\n    with open(f\"{this_directory}/data/sample-data/sample_text.txt\", \"w\") as f:\n        f.write(page.summary)\n    _upload_files(\n        blob_service_client,\n        f\"{this_directory}/data/sample-data\",\n        container_name,\n    )\n    _upload_files(blob_service_client, f\"{this_directory}/data/test-index\", index_name)\n\n    endpoint = os.environ[\"COSMOS_URI_ENDPOINT\"]\n    credential = DefaultAzureCredential()\n    client = CosmosClient(endpoint, credential)\n\n    container_store = \"container-store\"\n    database = client.get_database_client(container_store)\n    container_container = database.get_container_client(container_store)\n\n    index_item = {\"id\": index_name, \"type\": \"index\"}\n    data_item = {\"id\": container_name, \"type\": \"data\"}\n    container_container.create_item(body=index_item)\n    container_container.create_item(body=data_item)\n\n    container_store = \"jobs\"\n    database = client.get_database_client(container_store)\n    container_jobs = database.get_container_client(container_store)\n\n    index_item = {\n        \"id\": index_name,\n        \"index_name\": index_name,\n        \"storage_name\": container_name,\n        \"all_workflows\": [\n            \"create_base_text_units\",\n            \"create_final_text_units\",\n            \"create_base_extracted_entities\",\n            \"create_summarized_entities\",\n            \"create_base_entity_graph\",\n            \"create_final_entities\",\n            \"create_final_relationships\",\n            \"create_base_documents\",\n            \"create_base_document_graph\",\n            \"create_final_documents\",\n            \"create_final_communities\",\n            \"create_final_community_reports\",\n            \"create_final_covariates\",\n            \"create_base_entity_nodes\",\n            \"create_base_document_nodes\",\n            \"create_final_nodes\",\n        ],\n        \"completed_workflows\": [\n            \"create_base_text_units\",\n            \"create_base_extracted_entities\",\n            \"create_final_covariates\",\n            \"create_summarized_entities\",\n            \"create_base_entity_graph\",\n            \"create_final_entities\",\n            \"create_final_relationships\",\n            \"create_final_communities\",\n            \"create_final_community_reports\",\n            \"create_base_entity_nodes\",\n            \"create_final_text_units\",\n            \"create_base_documents\",\n            \"create_base_document_graph\",\n            \"create_base_document_nodes\",\n            \"create_final_documents\",\n            \"create_final_nodes\",\n        ],\n        \"failed_workflows\": [],\n        \"status\": \"complete\",\n        \"percent_complete\": 100,\n        \"progress\": \"16 out of 16 workflows completed successfully.\",\n    }\n    container_jobs.create_item(body=index_item)\n\n    yield index_name  # test runs here\n\n    # clean up\n    blob_service_client.delete_container(container_name)\n    blob_service_client.delete_container(index_name)\n    container_container.delete_item(item=container_name, partition_key=container_name)\n    container_jobs.delete_item(item=index_name, partition_key=index_name)\n    shutil.rmtree(f\"{this_directory}/data/sample-data\")\n```\n\n----------------------------------------\n\nTITLE: Processing Entities DataFrames with Provenance Tracking\nDESCRIPTION: Processes entity data with unique ID management across multiple indexes. Updates entity IDs and maintains references to text units while preserving index provenance information for accurate source tracking.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_40\n\nLANGUAGE: Python\nCODE:\n```\nentities_df = query_helper.get_df(entities_table_path)\nfor i in entities_df[\"human_readable_id\"]:\n    links[\"entities\"][i + max_vals[\"entities\"] + 1] = {\n        \"index_name\": sanitized_index_names_link[index_name],\n        \"id\": i,\n    }\nif max_vals[\"entities\"] != -1:\n    entities_df[\"human_readable_id\"] += max_vals[\"entities\"] + 1\nentities_df[\"id\"] = entities_df[\"id\"].apply(lambda x: x + f\"-{index_name}\")\nentities_df[\"name\"] = entities_df[\"name\"].apply(lambda x: x + f\"-{index_name}\")\nentities_df[\"text_unit_ids\"] = entities_df[\"text_unit_ids\"].apply(\n    lambda x: [i + f\"-{index_name}\" for i in x]\n)\nmax_vals[\"entities\"] = entities_df[\"human_readable_id\"].max()\nentities_dfs.append(entities_df)\n```\n\n----------------------------------------\n\nTITLE: Implementing Local Search Streaming Endpoint with FastAPI\nDESCRIPTION: Defines a POST endpoint for local search that streams responses by combining relevant data from the AI-extracted knowledge-graph with text chunks of raw documents. This endpoint is suitable for questions about specific entities in the documents.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n@query_streaming_route.post(\n    \"/local\",\n    summary=\"Stream a response back after performing a local search\",\n    description=\"The local query method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?).\",\n)\nasync def local_search_streaming(request: GraphRequest):\n    # this is a slightly modified version of src.api.query.local_query() method\n    if isinstance(request.index_name, str):\n        index_names = [request.index_name]\n    else:\n        index_names = request.index_name\n    sanitized_index_names = [sanitize_name(name) for name in index_names]\n    sanitized_index_names_link = {\n        s: i for s, i in zip(sanitized_index_names, index_names)\n    }\n\n    for index_name in sanitized_index_names:\n        if not _is_index_complete(index_name):\n            raise HTTPException(\n                status_code=500,\n                detail=f\"{sanitized_index_names_link[index_name]} not ready for querying.\",\n            )\n\n    blob_service_client = BlobServiceClientSingleton.get_instance()\n\n    community_dfs = []\n    covariates_dfs = []\n    entities_dfs = []\n    nodes_dfs = []\n    relationships_dfs = []\n    text_units_dfs = []\n    links = {\n        \"nodes\": {},\n        \"community\": {},\n        \"entities\": {},\n        \"text_units\": {},\n        \"relationships\": {},\n        \"covariates\": {},\n    }\n    max_vals = {\n        \"nodes\": -1,\n        \"community\": -1,\n        \"entities\": -1,\n        \"text_units\": -1,\n        \"relationships\": -1,\n        \"covariates\": -1,\n    }\n\n    COMMUNITY_REPORT_TABLE = \"output/create_final_community_reports.parquet\"\n    COVARIATES_TABLE = \"output/create_final_covariates.parquet\"\n    ENTITIES_TABLE = \"output/create_final_entities.parquet\"\n    NODES_TABLE = \"output/create_final_nodes.parquet\"\n    RELATIONSHIPS_TABLE = \"output/create_final_relationships.parquet\"\n    TEXT_UNITS_TABLE = \"output/create_final_text_units.parquet\"\n\n    if isinstance(request.community_level, int):\n        COMMUNITY_LEVEL = request.community_level\n    else:\n```\n\n----------------------------------------\n\nTITLE: Processing Relationships DataFrames with Provenance Tracking\nDESCRIPTION: Processes relationship data with unique ID management, updating source and target references to maintain cross-index relationships. Tracks relationship provenance information in the links dictionary for later reference.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_41\n\nLANGUAGE: Python\nCODE:\n```\nrelationships_df = query_helper.get_df(relationships_table_path)\nfor i in relationships_df[\"human_readable_id\"].astype(int):\n    links[\"relationships\"][i + max_vals[\"relationships\"] + 1] = {\n        \"index_name\": sanitized_index_names_link[index_name],\n        \"id\": i,\n    }\nif max_vals[\"relationships\"] != -1:\n    col = (\n        relationships_df[\"human_readable_id\"].astype(int)\n        + max_vals[\"relationships\"]\n        + 1\n    )\n    relationships_df[\"human_readable_id\"] = col.astype(str)\nrelationships_df[\"source\"] = relationships_df[\"source\"].apply(\n    lambda x: x + f\"-{index_name}\"\n)\nrelationships_df[\"target\"] = relationships_df[\"target\"].apply(\n    lambda x: x + f\"-{index_name}\"\n)\nrelationships_df[\"text_unit_ids\"] = relationships_df[\"text_unit_ids\"].apply(\n    lambda x: [i + f\"-{index_name}\" for i in x]\n)\nmax_vals[\"relationships\"] = (\n    relationships_df[\"human_readable_id\"].astype(int).max()\n)\nrelationships_dfs.append(relationships_df)\n```\n\n----------------------------------------\n\nTITLE: Implementing PipelineJob Class with Cosmos DB Integration in Python\nDESCRIPTION: This snippet defines the PipelineJob class, which manages pipeline job data in a Cosmos DB container. It includes methods for creating, loading, and updating job items, as well as utility methods for checking item existence and calculating completion percentage.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_94\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass PipelineJob:\n    _id: str = field(default=None, init=False)\n    _epoch_request_time: int = field(default=None, init=False)\n    _index_name: str = field(default=None, init=False)\n    _human_readable_index_name: str = field(default=None, init=False)\n    _sanitized_index_name: str = field(default=None, init=False)\n    _human_readable_storage_name: str = field(default=None, init=False)\n    _sanitized_storage_name: str = field(default=None, init=False)\n    _entity_extraction_prompt: str = field(default=None, init=False)\n    _community_report_prompt: str = field(default=None, init=False)\n    _summarize_descriptions_prompt: str = field(default=None, init=False)\n    _all_workflows: List[str] = field(default_factory=list, init=False)\n    _completed_workflows: List[str] = field(default_factory=list, init=False)\n    _failed_workflows: List[str] = field(default_factory=list, init=False)\n    _status: PipelineJobState = field(default=None, init=False)\n    _percent_complete: float = field(default=0, init=False)\n    _progress: str = field(default=\"\", init=False)\n\n    @staticmethod\n    def _jobs_container():\n        azure_storage_client = AzureStorageClientManager()\n        return azure_storage_client.get_cosmos_container_client(\n            database_name=\"graphrag\", container_name=\"jobs\"\n        )\n\n    @classmethod\n    def create_item(cls, id: str, human_readable_index_name: str, human_readable_storage_name: str, entity_extraction_prompt: str | None = None, community_report_prompt: str | None = None, summarize_descriptions_prompt: str | None = None, **kwargs) -> \"PipelineJob\":\n        # Method implementation...\n\n    @classmethod\n    def load_item(cls, id: str) -> \"PipelineJob\":\n        # Method implementation...\n\n    @staticmethod\n    def item_exist(id: str) -> bool:\n        # Method implementation...\n\n    def calculate_percent_complete(self) -> float:\n        # Method implementation...\n\n    def dump_model(self) -> dict:\n        # Method implementation...\n\n    def update_db(self):\n        # Method implementation...\n```\n\n----------------------------------------\n\nTITLE: Implementing Azure Application Insights Workflow Callbacks Class\nDESCRIPTION: A Python class that extends NoopWorkflowCallbacks to provide logging and monitoring capabilities using Azure Application Insights. It initializes a logger with an Azure Monitor exporter and implements various callback methods for workflow events.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nclass ApplicationInsightsWorkflowCallbacks(NoopWorkflowCallbacks):\n    \"\"\"A reporter that writes to an AppInsights Workspace.\"\"\"\n\n    _logger: logging.Logger\n    _logger_name: str\n    _logger_level: int\n    _logger_level_name: str\n    _properties: Dict[str, Any]\n    _workflow_name: str\n    _index_name: str\n    _num_workflow_steps: int\n    _processed_workflow_steps: list[str] = []\n\n    def __init__(\n        self,\n        connection_string: str,\n        logger_name: str | None = None,\n        logger_level: int = logging.INFO,\n        index_name: str = \"\",\n        num_workflow_steps: int = 0,\n        properties: Dict[str, Any] = {},\n    ):\n        \"\"\"\n        Initialize the AppInsightsReporter.\n\n        Args:\n            connection_string (str): The connection string for the App Insights instance.\n            logger_name (str | None, optional): The name of the logger. Defaults to None.\n            logger_level (int, optional): The logging level. Defaults to logging.INFO.\n            index_name (str, optional): The name of an index. Defaults to \"\".\n            num_workflow_steps (int): A list of workflow names ordered by their execution. Defaults to [].\n            properties (Dict[str, Any], optional): Additional properties to be included in the log. Defaults to {}.\n        \"\"\"\n        self._logger: logging.Logger\n        self._logger_name = logger_name\n        self._logger_level = logger_level\n        self._logger_level_name: str = logging.getLevelName(logger_level)\n        self._properties = properties\n        self._workflow_name = \"N/A\"\n        self._index_name = index_name\n        self._num_workflow_steps = num_workflow_steps\n        self._processed_workflow_steps = []  # maintain a running list of workflow steps that get processed\n        \"\"\"Create a new logger with an AppInsights handler.\"\"\"\n        self.__init_logger(connection_string=connection_string)\n```\n\n----------------------------------------\n\nTITLE: Combining DataFrames and Executing Search\nDESCRIPTION: Combines all processed data tables from multiple indexes using pandas concat operation. Handles the optional covariates table and passes the combined data to the local_search function for query execution.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\nnodes_combined = pd.concat(nodes_dfs, axis=0, ignore_index=True)\ncommunity_combined = pd.concat(community_dfs, axis=0, ignore_index=True)\nentities_combined = pd.concat(entities_dfs, axis=0, ignore_index=True)\ntext_units_combined = pd.concat(text_units_dfs, axis=0, ignore_index=True)\nrelationships_combined = pd.concat(relationships_dfs, axis=0, ignore_index=True)\ncovariates_combined = (\n    pd.concat(covariates_dfs, axis=0, ignore_index=True)\n    if covariates_dfs != []\n    else None\n)\n\n# load custom pipeline settings\nthis_directory = os.path.dirname(\n    os.path.abspath(inspect.getfile(inspect.currentframe()))\n)\ndata = yaml.safe_load(open(f\"{this_directory}/pipeline-settings.yaml\"))\n# layer the custom settings on top of the default configuration settings of graphrag\nparameters = create_graphrag_config(data, \".\")\n\n# add index_names to vector_store args\nparameters.embeddings.vector_store[\"index_names\"] = sanitized_index_names\n# internally write over the get_embedding_description_store\n# method to use the multi-index collection.\nimport graphrag.query.api\n\ngraphrag.query.api._get_embedding_description_store = (\n    _get_embedding_description_store\n)\n# perform async search\nresult = await local_search(\n    config=parameters,\n    nodes=nodes_combined,\n    entities=entities_combined,\n    community_reports=community_combined,\n    text_units=text_units_combined,\n    relationships=relationships_combined,\n    covariates=covariates_combined,\n    community_level=COMMUNITY_LEVEL,\n    response_type=\"Multiple Paragraphs\",\n    query=request.query,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining API Response Models with Pydantic in Python\nDESCRIPTION: This snippet defines several Pydantic BaseModel classes for API responses, including BaseResponse, ClaimResponse, EntityResponse, GraphRequest, GraphResponse, GraphDataResponse, IndexNameList, IndexStatusResponse, ReportResponse, RelationshipResponse, StorageNameList, and TextUnitResponse.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_93\n\nLANGUAGE: python\nCODE:\n```\nclass BaseResponse(BaseModel):\n    status: str\n\nclass ClaimResponse(BaseModel):\n    covariate_type: str\n    type: str\n    description: str\n    subject_id: str\n    object_id: str\n    source_text: str\n    text_unit_id: str\n    document_ids: List[str]\n\nclass EntityResponse(BaseModel):\n    name: str\n    description: str\n    text_units: list[str]\n\nclass GraphRequest(BaseModel):\n    index_name: str | List[str]\n    query: str\n    community_level: int | None = None\n\nclass GraphResponse(BaseModel):\n    result: Any\n    context_data: Any\n\nclass GraphDataResponse(BaseModel):\n    nodes: int\n    edges: int\n\nclass IndexNameList(BaseModel):\n    index_name: List[str]\n\nclass IndexStatusResponse(BaseModel):\n    status_code: int\n    index_name: str\n    storage_name: str\n    status: str\n    percent_complete: float\n    progress: str\n\nclass ReportResponse(BaseModel):\n    text: str\n\nclass RelationshipResponse(BaseModel):\n    source: str\n    source_id: int\n    target: str\n    target_id: int\n    description: str\n    text_units: list[str]\n\nclass StorageNameList(BaseModel):\n    storage_name: List[str]\n\nclass TextUnitResponse(BaseModel):\n    text: str\n    source_document: str\n```\n\n----------------------------------------\n\nTITLE: Validating Index Files and Constructing Table Paths\nDESCRIPTION: Validates the existence of required index files and constructs Azure Blob Storage paths for each data table. This ensures all necessary data is present before proceeding with the search operation.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\nfor index_name in sanitized_index_names:\n    # check for existence of files the query relies on to validate the index is complete\n    validate_index_file_exist(index_name, COMMUNITY_REPORT_TABLE)\n    validate_index_file_exist(index_name, ENTITIES_TABLE)\n    validate_index_file_exist(index_name, NODES_TABLE)\n    validate_index_file_exist(index_name, RELATIONSHIPS_TABLE)\n    validate_index_file_exist(index_name, TEXT_UNITS_TABLE)\n\n    community_report_table_path = f\"abfs://{index_name}/{COMMUNITY_REPORT_TABLE}\"\n    covariates_table_path = f\"abfs://{index_name}/{COVARIATES_TABLE}\"\n    entities_table_path = f\"abfs://{index_name}/{ENTITIES_TABLE}\"\n    nodes_table_path = f\"abfs://{index_name}/{NODES_TABLE}\"\n    relationships_table_path = f\"abfs://{index_name}/{RELATIONSHIPS_TABLE}\"\n    text_units_table_path = f\"abfs://{index_name}/{TEXT_UNITS_TABLE}\"\n```\n\n----------------------------------------\n\nTITLE: Creating Index API Router for GraphRAG Pipeline\nDESCRIPTION: Sets up a FastAPI router for index operations with required Azure client instances and environment variables. Establishes connections to Azure services and defines the base route for index operations.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nblob_service_client = BlobServiceClientSingleton.get_instance()\nazure_storage_client_manager = (\n    AzureStorageClientManager()\n)  # TODO: update API to use the AzureStorageClientManager\n\nai_search_url = os.environ[\"AI_SEARCH_URL\"]\nai_search_audience = os.environ[\"AI_SEARCH_AUDIENCE\"]\n\nindex_route = APIRouter(\n    prefix=\"/index\",\n    tags=[\"Index Operations\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Response Stream Wrapper for Search Results in Python\nDESCRIPTION: Wraps the streaming search results to format them for API consumption. This async function processes the stream of search results, updates context information with link data, and formats the output as JSON.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nasync def _wrapper(x, links):\n    context = None\n    async for i in x:\n        if context:\n            yield json.dumps({\"token\": i, \"context\": None}).encode(\"utf-8\") + b\"\\n\"\n        else:\n            context = i\n    context = _update_context(context, links)\n    context = json.dumps({\"token\": \"<EOM>\", \"context\": context}).encode(\"utf-8\") + b\"\\n\"\n    yield context\n```\n\n----------------------------------------\n\nTITLE: Implementing Local Search Endpoint for Knowledge Graph Query\nDESCRIPTION: Defines a POST endpoint for local searching across knowledge graph indices. This function handles query requests by targeting specific entities and text chunks in the knowledge graph rather than community-wide data.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n@query_route.post(\n    \"/local\",\n    summary=\"Perform a local search across the knowledge graph index.\",\n    description=\"The local query method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?).\",\n    response_model=GraphResponse,\n    responses={200: {\"model\": GraphResponse}},\n)\nasync def local_query(request: GraphRequest):\n    if isinstance(request.index_name, str):\n        index_names = [request.index_name]\n    else:\n        index_names = request.index_name\n    sanitized_index_names = [sanitize_name(name) for name in index_names]\n    sanitized_index_names_link = {\n        s: i for s, i in zip(sanitized_index_names, index_names)\n    }\n\n    for index_name in sanitized_index_names:\n        if not _is_index_complete(index_name):\n            raise HTTPException(\n                status_code=500,\n                detail=f\"{index_name} not ready for querying.\",\n            )\n\n    blob_service_client = BlobServiceClientSingleton.get_instance()\n\n    community_dfs = []\n    covariates_dfs = []\n    entities_dfs = []\n    nodes_dfs = []\n    relationships_dfs = []\n    text_units_dfs = []\n\n    links = {\n        \"nodes\": {},\n        \"community\": {},\n        \"entities\": {},\n        \"text_units\": {},\n        \"relationships\": {},\n        \"covariates\": {},\n    }\n    max_vals = {\n        \"nodes\": -1,\n        \"community\": -1,\n        \"entities\": -1,\n        \"text_units\": -1,\n        \"relationships\": -1,\n        \"covariates\": -1,\n    }\n\n    COMMUNITY_REPORT_TABLE = \"output/create_final_community_reports.parquet\"\n    COVARIATES_TABLE = \"output/create_final_covariates.parquet\"\n    ENTITIES_TABLE = \"output/create_final_entities.parquet\"\n    NODES_TABLE = \"output/create_final_nodes.parquet\"\n    RELATIONSHIPS_TABLE = \"output/create_final_relationships.parquet\"\n    TEXT_UNITS_TABLE = \"output/create_final_text_units.parquet\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Graph Statistics Endpoint in Python\nDESCRIPTION: Defines a FastAPI endpoint to retrieve basic graph statistics including node and edge counts. Uses NetworkX to read and analyze the GraphML file stored in Azure Blob Storage.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n@graph_route.get(\n    \"/stats/{index_name}\",\n    summary=\"Retrieve basic graph statistics, number of nodes and edges\",\n    response_model=GraphDataResponse,\n    responses={200: {\"model\": GraphDataResponse}},\n)\nasync def retrieve_graph_stats(index_name: str):\n    # validate index_name and knowledge graph file existence\n    sanitized_index_name = sanitize_name(index_name)\n    graph_file = \"output/summarized_graph.graphml\"  # expected filename of the graph based on the indexing workflow\n    validate_index_file_exist(sanitized_index_name, graph_file)\n    try:\n        storage_client = blob_service_client.get_container_client(sanitized_index_name)\n        blob_data = storage_client.download_blob(graph_file).readall()\n        bytes_io = BytesIO(blob_data)\n        g = nx.read_graphml(bytes_io)\n        return GraphDataResponse(nodes=len(g.nodes), edges=len(g.edges))\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\"Could not retrieve graph data file\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Could not retrieve graph statistics for index '{index_name}'.\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Retrieving Original Blob Container Name in Python\nDESCRIPTION: This function retrieves the original human-readable container name from a sanitized blob name by querying the CosmosDB container-store. It raises an HTTPException if there's an error during retrieval.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef retrieve_original_blob_container_name(sanitized_name: str) -> str | None:\n    \"\"\"\n    Retrieve the original human-readable container name of a sanitized blob name.\n\n    Args:\n    -----\n    sanitized_name (str)\n        The sanitized name to be converted back to the original name.\n\n    Returns: str\n        The original human-readable name.\n    \"\"\"\n    try:\n        container_store_client = (\n            azure_storage_client_manager.get_cosmos_container_client(\n                database_name=\"graphrag\", container_name=\"container-store\"\n            )\n        )\n        for item in container_store_client.read_all_items():\n            if item[\"id\"] == sanitized_name:\n                return item[\"human_readable_name\"]\n    except Exception:\n        raise HTTPException(\n            status_code=500, detail=\"Error retrieving original blob name.\"\n        )\n    return None\n```\n\n----------------------------------------\n\nTITLE: Pipeline Job Workflow Callbacks Implementation in Python\nDESCRIPTION: Class implementing workflow callbacks for pipeline job progress tracking. Extends NoopWorkflowCallbacks to provide job status updates and progress reporting functionality.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_71\n\nLANGUAGE: python\nCODE:\n```\nclass PipelineJobWorkflowCallbacks(NoopWorkflowCallbacks):\n    def __init__(self, pipeline_job: \"PipelineJob\"):\n        self._pipeline_job = pipeline_job\n\n    def on_workflow_start(self, name: str, instance: object) -> None:\n        if self._pipeline_job.status != PipelineJobState.RUNNING:\n            self._pipeline_job.status = PipelineJobState.RUNNING\n        self._pipeline_job.progress = f\"Workflow {name} started.\"\n\n    def on_workflow_end(self, name: str, instance: object) -> None:\n        self._pipeline_job.completed_workflows.append(name)\n        self._pipeline_job.update_db()\n        self._pipeline_job.progress = f\"Workflow {name} complete.\"\n        self._pipeline_job.percent_complete = (\n            self._pipeline_job.calculate_percent_complete()\n        )\n```\n\n----------------------------------------\n\nTITLE: Deleting Kubernetes Jobs and Pods in Python\nDESCRIPTION: Function that deletes a Kubernetes job and its associated pod. Includes environment checks to run only in Kubernetes, loads in-cluster configuration, and handles errors separately for job and pod deletion to ensure cleanup is as complete as possible.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef _delete_k8s_job(job_name: str, namespace: str) -> None:\n    \"\"\"Delete a kubernetes job.\n    Must delete K8s job first and then any pods associated with it\n    \"\"\"\n    # function should only work when running in AKS\n    if not os.getenv(\"KUBERNETES_SERVICE_HOST\"):\n        return None\n    reporter = ReporterSingleton().get_instance()\n    config.load_incluster_config()\n    try:\n        batch_v1 = client.BatchV1Api()\n        batch_v1.delete_namespaced_job(name=job_name, namespace=namespace)\n    except Exception:\n        reporter.on_error(\n            message=f\"Error deleting k8s job {job_name}.\",\n            details={\"container\": job_name},\n        )\n        pass\n    try:\n        core_v1 = client.CoreV1Api()\n        job_pod = _get_pod_name(job_name, os.environ[\"AKS_NAMESPACE\"])\n        if job_pod:\n            core_v1.delete_namespaced_pod(job_pod, namespace=namespace)\n    except Exception:\n        reporter.on_error(\n            message=f\"Error deleting k8s pod for job {job_name}.\",\n            details={\"container\": job_name},\n        )\n        pass\n```\n\n----------------------------------------\n\nTITLE: Fetching Claim Information in Python using FastAPI and Pandas\nDESCRIPTION: Implements an API endpoint to retrieve a single claim by its ID. It reads data from a Parquet file in Azure Blob Storage using Pandas and returns a ClaimResponse.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\n@source_route.get(\n    \"/claim/{index_name}/{claim_id}\",\n    summary=\"Return a single claim.\",\n    response_model=ClaimResponse,\n    responses={200: {\"model\": ClaimResponse}},\n)\nasync def get_claim_info(index_name: str, claim_id: int):\n    sanitized_index_name = sanitize_name(index_name)\n    try:\n        validate_index_file_exist(sanitized_index_name, COVARIATES_TABLE)\n    except ValueError:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Claim data unavailable for index '{index_name}'.\",\n        )\n    try:\n        claims_table = pd.read_parquet(\n            f\"abfs://{sanitized_index_name}/{COVARIATES_TABLE}\",\n            storage_options=storage_options,\n        )\n        claims_table.human_readable_id = claims_table.human_readable_id.astype(\n            float\n        ).astype(int)\n        row = claims_table[claims_table.human_readable_id == claim_id]\n        return ClaimResponse(\n            covariate_type=row[\"covariate_type\"].values[0],\n            type=row[\"type\"].values[0],\n            description=row[\"description\"].values[0],\n            subject_id=row[\"subject_id\"].values[0],\n            object_id=row[\"object_id\"].values[0],\n            source_text=row[\"source_text\"].values[0],\n            text_unit_id=row[\"text_unit_id\"].values[0],\n            document_ids=row[\"document_ids\"].values[0].tolist(),\n        )\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\"Could not get claim.\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error retrieving claim '{claim_id}' from index '{index_name}'.\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Uploading Files to Azure Blob Storage in Python\nDESCRIPTION: Helper function that uploads all files from a local directory to an Azure Blob Storage container. It walks through the directory, preserves relative paths, and handles Windows path compatibility by replacing backslashes with forward slashes.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_77\n\nLANGUAGE: python\nCODE:\n```\ndef _upload_files(blob_service_client, directory, container_name):\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            local_path = os.path.join(root, file)\n            relative_path = os.path.relpath(local_path, directory)\n            # replace backslashes with forward slashes (for Windows compatibility)\n            relative_path = relative_path.replace(\"\\\\\", \"/\")\n            # upload file\n            blob_client = blob_service_client.get_blob_client(\n                blob=relative_path, container=container_name\n            )\n            with open(local_path, \"rb\") as data:\n                blob_client.upload_blob(\n                    data, connection_timeout=120\n                )  # increase timeout for large files\n```\n\n----------------------------------------\n\nTITLE: Get All Indexes API Endpoint in FastAPI\nDESCRIPTION: API endpoint that retrieves all index names from a CosmosDB container. It filters items by type 'index' and returns their human-readable names. Includes error handling to ensure the API remains robust even if database operations fail.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n@index_route.get(\n    \"\",\n    summary=\"Get all indexes\",\n    response_model=IndexNameList,\n    responses={200: {\"model\": IndexNameList}},\n)\nasync def get_all_indexes():\n    \"\"\"\n    Retrieve a list of all index names.\n    \"\"\"\n    items = []\n    try:\n        container_store_client = get_database_container_client(\n            database_name=\"graphrag\", container_name=\"container-store\"\n        )\n        for item in container_store_client.read_all_items():\n            if item[\"type\"] == \"index\":\n                items.append(item[\"human_readable_name\"])\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\"Error retrieving index names\")\n    return IndexNameList(index_name=items)\n```\n\n----------------------------------------\n\nTITLE: Updating Pipeline Job Status After Workflow Completion in Python\nDESCRIPTION: Sets the pipeline job status to complete or failed based on workflow results, calculates completion percentage, logs the outcome, and terminates the process if the job failed. This handles the final steps of an indexing pipeline.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# TODO: exit early if a workflow fails and add more detailed error logging\n\n# if job is done, check if any workflow steps failed\nif len(pipeline_job.failed_workflows) > 0:\n    pipeline_job.status = PipelineJobState.FAILED\nelse:\n    # record the workflow completion\n    pipeline_job.status = PipelineJobState.COMPLETE\n    pipeline_job.percent_complete = 100\n\npipeline_job.progress = (\n    f\"{len(pipeline_job.completed_workflows)} out of \"\n    f\"{len(pipeline_job.all_workflows)} workflows completed successfully.\"\n)\n\nworkflow_callbacks.on_log(\n    message=f\"Indexing pipeline complete for index'{index_name}'.\",\n    details={\n        \"index\": index_name,\n        \"storage_name\": storage_name,\n        \"status_message\": \"indexing pipeline complete\",\n    },\n)\n\ndel workflow_callbacks  # garbage collect\nif pipeline_job.status == PipelineJobState.FAILED:\n    exit(1)  # signal to AKS that indexing job failed\n```\n\n----------------------------------------\n\nTITLE: Get Index Job Status API Endpoint in FastAPI\nDESCRIPTION: API endpoint for retrieving the status of an indexing job. It loads the pipeline job data from storage, returning details like status, completion percentage, and progress. Handles the case when the requested index doesn't exist by raising a 404 error.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n@index_route.get(\n    \"/status/{index_name}\",\n    summary=\"Track the status of an indexing job\",\n    response_model=IndexStatusResponse,\n)\nasync def get_index_job_status(index_name: str):\n    pipelinejob = PipelineJob()  # TODO: fix class so initiliazation is not required\n    sanitized_index_name = sanitize_name(index_name)\n    if pipelinejob.item_exist(sanitized_index_name):\n        pipeline_job = pipelinejob.load_item(sanitized_index_name)\n        return IndexStatusResponse(\n            status_code=200,\n            index_name=pipeline_job.human_readable_index_name,\n            storage_name=pipeline_job.human_readable_storage_name,\n            status=pipeline_job.status.value,\n            percent_complete=pipeline_job.percent_complete,\n            progress=pipeline_job.progress,\n        )\n    raise HTTPException(status_code=404, detail=f\"Index '{index_name}' does not exist.\")\n```\n\n----------------------------------------\n\nTITLE: Executing Workflow Merge (Python)\nDESCRIPTION: This asynchronous function merges workflow data with existing index data, handling different merge scenarios based on the workflow name.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_89\n\nLANGUAGE: Python\nCODE:\n```\nasync def merge_with_graph(\n    merge_with_index: str,\n    workflow_name: str,\n    workflow_index_name: str,\n    workflow_df: pd.DataFrame,\n    workflow_storage: PipelineStorage,\n    merge_with_storage: PipelineStorage,\n    reporter: NoopWorkflowCallbacks | None = None,\n) -> pd.DataFrame:\n    \"\"\"Execute this callback when a workflow ends.\"\"\"\n\n    reporter = reporter or ConsoleWorkflowCallbacks()\n    if workflow_name in workflows_to_merge:\n        reporter.on_log(\n            message=(\n                f\"Starting index merge process. Workflow name: {workflow_name}, \"\n                + f\"Workflow index name: {workflow_index_name}, \"\n                + f\"Existing Index Name: {merge_with_index}.\"\n            )\n        )\n\n        # load existing index table\n        data = await merge_with_storage.get(f\"{workflow_name}.parquet\", as_bytes=True)\n        existing_df = pd.read_parquet(io.BytesIO(data))\n\n        # validate data tables\n        validate_data(workflow_name, workflow_df, existing_df)\n\n        # merge the data tables\n        if workflow_name in workflows_to_concat:\n            workflow_df = pd.concat((existing_df, workflow_df), axis=0).reset_index(\n                drop=True\n            )\n\n        # merge the data graphs\n        if workflow_name in [\"create_base_extracted_entities\"]:\n            workflow_df = merge_two_graphml_dataframes(existing_df, workflow_df)\n\n        # create a Parquet emitter to save files to storage\n        emitter = create_table_emitters(\n            [TableEmitterType.Parquet],\n            workflow_storage,\n            lambda e, s, d: cast(WorkflowCallbacks, reporter).on_error(\n                \"Error emitting table\", e, s, d\n            ),\n        ).pop()\n        # overwrite the workflow data table\n        await emitter.emit(workflow_name, workflow_df)\n\n        # log merge process update\n        reporter.on_log(\n            message=(\n                f\"Completed index merge process. Workflow name: {workflow_name}, \"\n                + f\"Workflow index name: {workflow_index_name}, \"\n                + f\"Existing Index Name: {merge_with_index}.\"\n            )\n        )\n\n        return workflow_df\n```\n\n----------------------------------------\n\nTITLE: Retrieving Kubernetes Pod Name in Python\nDESCRIPTION: Helper function that finds a Kubernetes pod associated with a specific job name in a given namespace. Designed to work only when running in AKS (Azure Kubernetes Service) and includes an environment check to ensure proper operation.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef _get_pod_name(job_name: str, namespace: str) -> str | None:\n    \"\"\"Retrieve the name of a kubernetes pod associated with a given job name.\"\"\"\n    # function should work only when running in AKS\n    if not os.getenv(\"KUBERNETES_SERVICE_HOST\"):\n        return None\n    config.load_incluster_config()\n    v1 = client.CoreV1Api()\n    ret = v1.list_namespaced_pod(namespace=namespace)\n    for i in ret.items:\n        if job_name in i.metadata.name:\n            return i.metadata.name\n    return None\n```\n\n----------------------------------------\n\nTITLE: Creating Index Configuration Router with Prompt Generation in Python\nDESCRIPTION: Establishes a FastAPI router for index configuration with an endpoint to automatically generate custom prompts for entity extraction. Utilizes graphrag's prompt_tune functionality and handles YAML configuration.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nindex_configuration_route = APIRouter(\n    prefix=\"/index/config\", tags=[\"Index Configuration\"]\n)\n\n\n@index_configuration_route.get(\n    \"/prompts\",\n    summary=\"Generate graphrag prompts from user-provided data.\",\n    description=\"Generating custom prompts from user-provided data may take several minutes to run based on the amount of data used.\",\n)\nasync def generate_prompts(storage_name: str, limit: int = 5):\n    \"\"\"\n    Automatically generate custom prompts for entity entraction,\n    community reports, and summarize descriptions based on a sample of provided data.\n    \"\"\"\n    # check for storage container existence\n    blob_service_client = BlobServiceClientSingleton().get_instance()\n    sanitized_storage_name = sanitize_name(storage_name)\n    if not blob_service_client.get_container_client(sanitized_storage_name).exists():\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Data container '{storage_name}' does not exist.\",\n        )\n    this_directory = os.path.dirname(\n        os.path.abspath(inspect.getfile(inspect.currentframe()))\n    )\n\n    # write custom settings.yaml to a file and store in a temporary directory\n    data = yaml.safe_load(open(f\"{this_directory}/pipeline-settings.yaml\"))\n    data[\"input\"][\"container_name\"] = sanitized_storage_name\n    temp_dir = f\"/tmp/{sanitized_storage_name}_prompt_tuning\"\n    shutil.rmtree(temp_dir, ignore_errors=True)\n    os.makedirs(temp_dir, exist_ok=True)\n    with open(f\"{temp_dir}/settings.yaml\", \"w\") as f:\n        yaml.dump(data, f, default_flow_style=False)\n\n    # generate prompts\n    try:\n        await generate_fine_tune_prompts(\n            config=f\"{temp_dir}/settings.yaml\",\n            root=temp_dir,\n            domain=\"\",\n            selection_method=\"random\",\n            limit=limit,\n            skip_entity_types=True,\n            output=f\"{temp_dir}/prompts\",\n        )\n    except Exception as e:\n        reporter = ReporterSingleton().get_instance()\n        error_details = {\n            \"storage_name\": storage_name,\n        }\n        reporter.on_error(\n            message=\"Auto-prompt generation failed.\",\n            cause=e,\n            stack=traceback.format_exc(),\n            details=error_details,\n        )\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error generating prompts for data in '{storage_name}'. Please try a lower limit.\",\n        )\n\n    # zip up the generated prompt files and return the zip file\n    temp_archive = (\n        f\"{temp_dir}/prompts\"  # will become a zip file with the name prompts.zip\n    )\n    shutil.make_archive(temp_archive, \"zip\", root_dir=temp_dir, base_dir=\"prompts\")\n\n    def iterfile(file_path: str):\n        with open(file_path, mode=\"rb\") as file_like:\n            yield from file_like\n\n    return StreamingResponse(iterfile(f\"{temp_archive}.zip\"))\n```\n\n----------------------------------------\n\nTITLE: Initializing BlobWorkflowCallbacks Class\nDESCRIPTION: Class definition and initialization of BlobWorkflowCallbacks with Azure storage configuration. Handles storage account connection, blob client setup, and maintains workflow state tracking.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_63\n\nLANGUAGE: Python\nCODE:\n```\nclass BlobWorkflowCallbacks(NoopWorkflowCallbacks):\n    _blob_service_client: BlobServiceClient\n    _container_name: str\n    _index_name: str\n    _num_workflow_steps: int\n    _processed_workflow_steps: list[str] = []\n    _max_block_count: int = 25000  # 25k blocks per blob\n\n    def __init__(\n        self,\n        storage_account_blob_url: str,\n        container_name: str,\n        blob_name: str = \"\",\n        index_name: str = \"\",\n        num_workflow_steps: int = 0,\n    ):\n        self._storage_account_blob_url = storage_account_blob_url\n        credential = DefaultAzureCredential()\n        self._blob_service_client = BlobServiceClient(\n            storage_account_blob_url, credential=credential\n        )\n        if not blob_name:\n            blob_name = f\"{container_name}/{datetime.now().strftime('%Y-%m-%d-%H:%M:%S:%f')}.logs.txt\"\n        self._index_name = index_name\n        self._num_workflow_steps = num_workflow_steps\n        self._processed_workflow_steps = []\n        self._blob_name = blob_name\n        self._container_name = container_name\n        self._blob_client = self._blob_service_client.get_blob_client(\n            self._container_name, self._blob_name\n        )\n        if not self._blob_client.exists():\n            self._blob_client.create_append_blob()\n        self._num_blocks = 0\n```\n\n----------------------------------------\n\nTITLE: Asynchronous File Upload Helper\nDESCRIPTION: Helper function for asynchronously uploading files to Azure Blob Storage containers with optional overwrite functionality.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nasync def upload_file_async(\n    upload_file: UploadFile, container_client: ContainerClient, overwrite: bool = True\n) -> None:\n    \"\"\"\n    Asynchronously upload a file to the specified blob container.\n    Silently ignore errors that occur when overwrite=False.\n    \"\"\"\n    blob_client = container_client.get_blob_client(upload_file.filename)\n    with upload_file.file as file_stream:\n        try:\n            await blob_client.upload_blob(file_stream, overwrite=overwrite)\n        except Exception:\n            pass\n```\n\n----------------------------------------\n\nTITLE: Merging GraphML Dataframes (Python)\nDESCRIPTION: This function merges two GraphML dataframes by creating a new graph and combining nodes and edges from both input graphs.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_88\n\nLANGUAGE: Python\nCODE:\n```\ndef merge_two_graphml_dataframes(df1, df2):\n    mega_graph = nx.Graph()\n    G1 = nx.read_graphml(io.BytesIO(df1[\"entity_graph\"][0].encode()))\n    G2 = nx.read_graphml(io.BytesIO(df2[\"entity_graph\"][0].encode()))\n    for graph in [G1, G2]:\n        merge_nodes(mega_graph, graph)\n        merge_edges(mega_graph, graph)\n    return pd.DataFrame([{\"entity_graph\": \"\\n\".join(nx.generate_graphml(mega_graph))}])\n```\n\n----------------------------------------\n\nTITLE: Batch File Upload Endpoint\nDESCRIPTION: FastAPI endpoint for uploading multiple files to Azure Blob Storage in batches, with container creation and CosmosDB metadata updates.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n@data_route.post(\n    \"\",\n    summary=\"Upload data to a data storage container\",\n    response_model=BaseResponse,\n    responses={200: {\"model\": BaseResponse}},\n)\nasync def upload_files(\n    files: List[UploadFile], storage_name: str, overwrite: bool = True\n):\n    reporter = ReporterSingleton().get_instance()\n    sanitized_storage_name = sanitize_name(storage_name)\n    try:\n        validate_blob_container_name(sanitized_storage_name)\n    except ValueError:\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Invalid blob container name: '{storage_name}'. Please try a different name.\",\n        )\n    try:\n        blob_service_client = BlobServiceClientSingletonAsync.get_instance()\n        container_client = blob_service_client.get_container_client(\n            sanitized_storage_name\n        )\n        if not await container_client.exists():\n            await container_client.create_container()\n\n        files = [UploadFile(Cleaner(f.file), filename=f.filename) for f in files]\n\n        batch_size = 1000\n        batches = ceil(len(files) / batch_size)\n        for i in range(batches):\n            batch_files = files[i * batch_size : (i + 1) * batch_size]\n            tasks = [\n                upload_file_async(file, container_client, overwrite)\n                for file in batch_files\n            ]\n            await asyncio.gather(*tasks)\n        container_store_client = (\n            azure_storage_client_manager.get_cosmos_container_client(\n                database_name=\"graphrag\", container_name=\"container-store\"\n            )\n        )\n        container_store_client.upsert_item({\n            \"id\": sanitized_storage_name,\n            \"human_readable_name\": storage_name,\n            \"type\": \"data\",\n        })\n        return BaseResponse(status=\"File upload successful.\")\n    except Exception:\n        reporter.on_error(\"Error uploading files.\", details={\"files\": files})\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error uploading files to container '{storage_name}'.\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Fetching Text Unit Information in Python using FastAPI and Pandas\nDESCRIPTION: Implements an API endpoint to retrieve a single base text unit by its ID. It reads data from two Parquet files in Azure Blob Storage using Pandas and returns a TextUnitResponse.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\n@source_route.get(\n    \"/text/{index_name}/{text_unit_id}\",\n    summary=\"Return a single base text unit.\",\n    response_model=TextUnitResponse,\n    responses={200: {\"model\": TextUnitResponse}},\n)\nasync def get_chunk_info(index_name: str, text_unit_id: str):\n    sanitized_index_name = sanitize_name(index_name)\n    validate_index_file_exist(sanitized_index_name, TEXT_UNITS_TABLE)\n    validate_index_file_exist(sanitized_index_name, DOCUMENTS_TABLE)\n    try:\n        text_unit_table = pd.read_parquet(\n            f\"abfs://{sanitized_index_name}/{TEXT_UNITS_TABLE}\",\n            storage_options=storage_options,\n        )\n        docs = pd.read_parquet(\n            f\"abfs://{sanitized_index_name}/{DOCUMENTS_TABLE}\",\n            storage_options=storage_options,\n        )\n        links = {\n            el[\"id\"]: el[\"title\"]\n            for el in docs[[\"id\", \"title\"]].to_dict(orient=\"records\")\n        }\n        text_unit_table[\"source_doc\"] = text_unit_table[\"document_ids\"].apply(\n            lambda x: links[x[0]]\n        )\n        row = text_unit_table[text_unit_table.chunk_id == text_unit_id][\n            [\"chunk\", \"source_doc\"]\n        ]\n        return TextUnitResponse(\n            text=row[\"chunk\"].values[0], source_document=row[\"source_doc\"].values[0]\n        )\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\"Could not get text chunk.\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error retrieving text chunk '{text_unit_id}' from index '{index_name}'.\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Retrieving Original Entity Config Name in Python\nDESCRIPTION: This function retrieves the original human-readable entity config name from a sanitized name by querying the CosmosDB entities container. It raises an HTTPException if there's an error during retrieval.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef retrieve_original_entity_config_name(sanitized_name: str) -> str | None:\n    \"\"\"\n    Retrieve the original human-readable entity config name of a sanitized entity config name.\n\n    Args:\n    -----\n    sanitized_name (str)\n        The sanitized name to be converted back to the original name.\n\n    Returns: str\n        The original human-readable name.\n    \"\"\"\n    try:\n        container_store_client = (\n            azure_storage_client_manager.get_cosmos_container_client(\n                database_name=\"graphrag\", container_name=\"entities\"\n            )\n        )\n        for item in container_store_client.read_all_items():\n            if item[\"id\"] == sanitized_name:\n                return item[\"human_readable_name\"]\n    except Exception:\n        raise HTTPException(\n            status_code=500, detail=\"Error retrieving original entity config name.\"\n        )\n    return None\n```\n\n----------------------------------------\n\nTITLE: Initializing FastAPI Router and Configuring Azure Storage in Python\nDESCRIPTION: Sets up a FastAPI router for source data endpoints and configures Azure Blob Storage connection using environment variables and DefaultAzureCredential.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\nsource_route = APIRouter(\n    prefix=\"/source\",\n    tags=[\"Sources\"],\n)\n\nstorage_account_blob_url = os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"]\nstorage_account_name = storage_account_blob_url.split(\"//\")[1].split(\".\")[0]\nstorage_account_host = storage_account_blob_url.split(\"//\")[1]\nstorage_options = {\n    \"account_name\": storage_account_name,\n    \"account_host\": storage_account_host,\n    \"credential\": DefaultAzureCredential(),\n}\n```\n\n----------------------------------------\n\nTITLE: Checking Index Completeness\nDESCRIPTION: Determines if an index is ready for querying by checking if it exists in the jobs table and if the indexing build job has completed. Returns True only if the index is fully built and ready for use.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_46\n\nLANGUAGE: Python\nCODE:\n```\ndef _is_index_complete(index_name: str) -> bool:\n    \"\"\"\n    Check if an index is ready for querying.\n\n    An index is ready for use only if it exists in the jobs table in cosmos db and\n    the indexing build job has finished (i.e. 100 percent). Otherwise it is not ready.\n\n    Args:\n    -----\n    index_name (str)\n        Name of the index to check.\n\n    Returns: bool\n        True if the index is ready for use, False otherwise.\n    \"\"\"\n    if PipelineJob.item_exist(index_name):\n        pipeline_job = PipelineJob.load_item(index_name)\n        if PipelineJobState(pipeline_job.status) == PipelineJobState.COMPLETE:\n            return True\n    return False\n```\n\n----------------------------------------\n\nTITLE: Retrieving Community Report by ID in Python using FastAPI and Pandas\nDESCRIPTION: Defines an API endpoint to fetch a single community report by its ID. It reads data from a Parquet file in Azure Blob Storage using Pandas and returns a ReportResponse.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\n@source_route.get(\n    \"/report/{index_name}/{report_id}\",\n    summary=\"Return a single community report.\",\n    response_model=ReportResponse,\n    responses={200: {\"model\": ReportResponse}},\n)\nasync def get_report_info(index_name: str, report_id: str):\n    sanitized_index_name = sanitize_name(index_name)\n    validate_index_file_exist(sanitized_index_name, COMMUNITY_REPORT_TABLE)\n    try:\n        report_table = pd.read_parquet(\n            f\"abfs://{sanitized_index_name}/{COMMUNITY_REPORT_TABLE}\",\n            storage_options=storage_options,\n        )\n        row = report_table[report_table.community == report_id]\n        return ReportResponse(text=row[\"full_content\"].values[0])\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\"Could not get report.\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error retrieving report '{report_id}' from index '{index_name}'.\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Updating Context Entries Function\nDESCRIPTION: Function that processes different types of context entries (relationships, claims, sources) and updates them with appropriate index information from a links dictionary. It creates a new dictionary with updated entries for each key in the context.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\ndef _get_embedding_description_store(\n    entities: Any,\n    vector_store_type: str = Any,\n    config_args: dict | None = None,\n):\n    collection_names = [\n        f\"{index_name}_description_embedding\"\n        for index_name in config_args.get(\"index_names\", [])\n    ]\n    ai_search_url = os.environ[\"AI_SEARCH_URL\"]\n    description_embedding_store = MultiAzureAISearch(\n        collection_name=\"multi\",\n        document_collection=None,\n        db_connection=None,\n    )\n    description_embedding_store.connect(url=ai_search_url)\n    for collection_name in collection_names:\n        description_embedding_store.add_collection(collection_name)\n    return description_embedding_store\n```\n\n----------------------------------------\n\nTITLE: Log Writing Implementation\nDESCRIPTION: Internal method to write logs to Azure Blob Storage with block management. Creates new blob when block count approaches limit and appends formatted log entries.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_64\n\nLANGUAGE: Python\nCODE:\n```\ndef _write_log(self, log: dict[str, Any]):\n        if self._num_blocks >= self._max_block_count:\n            self.__init__(self._storage_account_blob_url, self._container_name)\n        blob_client = self._blob_service_client.get_blob_client(\n            self._container_name, self._blob_name\n        )\n        blob_client.append_block(pformat(log, indent=2) + \"\\n\")\n        self._num_blocks += 1\n```\n\n----------------------------------------\n\nTITLE: Retrieving Entity Information in Python using FastAPI and Pandas\nDESCRIPTION: Defines an API endpoint to fetch a single entity by its ID. It reads data from a Parquet file in Azure Blob Storage using Pandas and returns an EntityResponse.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\n@source_route.get(\n    \"/entity/{index_name}/{entity_id}\",\n    summary=\"Return a single entity.\",\n    response_model=EntityResponse,\n    responses={200: {\"model\": EntityResponse}},\n)\nasync def get_entity_info(index_name: str, entity_id: int):\n    sanitized_index_name = sanitize_name(index_name)\n    validate_index_file_exist(sanitized_index_name, ENTITY_EMBEDDING_TABLE)\n    try:\n        entity_table = pd.read_parquet(\n            f\"abfs://{sanitized_index_name}/{ENTITY_EMBEDDING_TABLE}\",\n            storage_options=storage_options,\n        )\n        row = entity_table[entity_table.human_readable_id == entity_id]\n        return EntityResponse(\n            name=row[\"name\"].values[0],\n            description=row[\"description\"].values[0],\n            text_units=row[\"text_unit_ids\"].values[0].tolist(),\n        )\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\"Could not get entity\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error retrieving entity '{entity_id}' from index '{index_name}'.\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Defining FastAPI Router for Graph Operations in Python\nDESCRIPTION: Creates a FastAPI router for graph operations with endpoints to retrieve GraphML files and graph statistics. Uses Azure Blob Storage to access graph data and NetworkX for graph processing.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ngraph_route = APIRouter(\n    prefix=\"/graph\",\n    tags=[\"Graph Operations\"],\n)\n\n\n@graph_route.get(\n    \"/graphml/{index_name}\",\n    summary=\"Retrieve a GraphML file of the knowledge graph\",\n    response_description=\"GraphML file successfully downloaded\",\n)\nasync def retrieve_graphml_file(index_name: str):\n    # validate index_name and graphml file existence\n    sanitized_index_name = sanitize_name(index_name)\n    graphml_filename = \"summarized_graph.graphml\"\n    blob_filepath = f\"output/{graphml_filename}\"  # expected file location of the graph based on the workflow\n    validate_index_file_exist(sanitized_index_name, blob_filepath)\n    try:\n        blob_client = blob_service_client.get_blob_client(\n            container=sanitized_index_name, blob=blob_filepath\n        )\n        blob_stream = blob_client.download_blob().chunks()\n        return StreamingResponse(\n            blob_stream,\n            media_type=\"application/octet-stream\",\n            headers={\"Content-Disposition\": f\"attachment; filename={graphml_filename}\"},\n        )\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\"Could not retrieve graphml file\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Could not retrieve graphml file for index '{index_name}'.\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Retrieving Relationship Information in Python using FastAPI and Pandas\nDESCRIPTION: Defines an API endpoint to fetch a single relationship by its ID. It reads data from two Parquet files in Azure Blob Storage using Pandas and returns a RelationshipResponse.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_55\n\nLANGUAGE: Python\nCODE:\n```\n@source_route.get(\n    \"/relationship/{index_name}/{relationship_id}\",\n    summary=\"Return a single relationship.\",\n    response_model=RelationshipResponse,\n    responses={200: {\"model\": RelationshipResponse}},\n)\nasync def get_relationship_info(index_name: str, relationship_id: int):\n    sanitized_index_name = sanitize_name(index_name)\n    validate_index_file_exist(sanitized_index_name, RELATIONSHIPS_TABLE)\n    validate_index_file_exist(sanitized_index_name, ENTITY_EMBEDDING_TABLE)\n    try:\n        relationship_table = pd.read_parquet(\n            f\"abfs://{sanitized_index_name}/{RELATIONSHIPS_TABLE}\",\n            storage_options=storage_options,\n        )\n        entity_table = pd.read_parquet(\n            f\"abfs://{sanitized_index_name}/{ENTITY_EMBEDDING_TABLE}\",\n            storage_options=storage_options,\n        )\n        row = relationship_table[\n            relationship_table.human_readable_id == str(relationship_id)\n        ]\n        return RelationshipResponse(\n            source=row[\"source\"].values[0],\n            source_id=entity_table[\n                entity_table.name == row[\"source\"].values[0]\n            ].human_readable_id.values[0],\n            target=row[\"target\"].values[0],\n            target_id=entity_table[\n                entity_table.name == row[\"target\"].values[0]\n            ].human_readable_id.values[0],\n            description=row[\"description\"].values[0],\n            text_units=[\n                x[0] for x in row[\"text_unit_ids\"].to_list()\n            ],  # extract text_unit_ids from a list of panda series\n        )\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\"Could not get relationship.\")\n        raise HTTPException(\n            status_code=500,\n            detail=f\"Error retrieving relationship '{relationship_id}' from index '{index_name}'.\",\n        )\n```\n\n----------------------------------------\n\nTITLE: Processing Optional Covariates DataFrames with Provenance Tracking\nDESCRIPTION: Handles covariates data if available, checking existence before processing. Updates IDs to maintain uniqueness and tracks provenance information in the links dictionary for later reference.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\nindex_container_client = blob_service_client.get_container_client(index_name)\nif index_container_client.get_blob_client(COVARIATES_TABLE).exists():\n    covariates_df = query_helper.get_df(covariates_table_path)\n    if i in covariates_df[\"human_readable_id\"].astype(int):\n        links[\"covariates\"][i + max_vals[\"covariates\"] + 1] = {\n            \"index_name\": sanitized_index_names_link[index_name],\n            \"id\": i,\n        }\n    if max_vals[\"covariates\"] != -1:\n        col = (\n            covariates_df[\"human_readable_id\"].astype(int)\n            + max_vals[\"covariates\"]\n            + 1\n        )\n        covariates_df[\"human_readable_id\"] = col.astype(str)\n    max_vals[\"covariates\"] = (\n        covariates_df[\"human_readable_id\"].astype(int).max()\n    )\n    covariates_dfs.append(covariates_df)\n```\n\n----------------------------------------\n\nTITLE: Creating Entity Configuration Fixture in Python\nDESCRIPTION: Pytest fixture that creates an entity configuration with example entities for organizations, geographical locations, and persons. Includes sample text and expected entity relationships.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_83\n\nLANGUAGE: python\nCODE:\n```\n@pytest.fixture\ndef create_entity_config(client):\n    entity_config_name = f\"default-{str(uuid.uuid4())}\"\n    entity_types = [\"ORGANIZATION\", \"GEO\", \"PERSON\"]\n    entity_examples = [\n        {\n            \"entity_types\": \"ORGANIZATION, PERSON\",\n            \"text\": \"The Fed is scheduled to meet on Tuesday and Wednesday...\",\n            \"output\": '(\"entity\"{tuple_delimiter}FED{tuple_delimiter}ORGANIZATION...'\n        },\n        # Additional examples omitted for brevity\n    ]\n    request_data = {\n        \"entity_configuration_name\": entity_config_name,\n        \"entity_types\": entity_types,\n        \"entity_examples\": entity_examples,\n    }\n    response = client.post(\n        url=f\"{client.base_url}{index_endpoint}/config/entity\", json=request_data\n    )\n    yield (entity_config_name, response)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Azure Storage Containers\nDESCRIPTION: FastAPI endpoint that retrieves a list of all data storage containers from CosmosDB container store.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@data_route.get(\n    \"\",\n    summary=\"Get all data storage containers.\",\n    response_model=StorageNameList,\n    responses={200: {\"model\": StorageNameList}},\n)\nasync def get_all_data_storage_containers():\n    \"\"\"\n    Retrieve a list of all data storage containers.\n    \"\"\"\n    items = []\n    try:\n        container_store_client = (\n            azure_storage_client_manager.get_cosmos_container_client(\n                database_name=\"graphrag\", container_name=\"container-store\"\n            )\n        )\n        for item in container_store_client.read_all_items():\n            if item[\"type\"] == \"data\":\n                items.append(item[\"human_readable_name\"])\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\"Error getting list of blob containers.\")\n        raise HTTPException(\n            status_code=500, detail=\"Error getting list of blob containers.\"\n        )\n    return StorageNameList(storage_name=items)\n```\n\n----------------------------------------\n\nTITLE: Processing Community DataFrames with Provenance Tracking\nDESCRIPTION: Processes community data, ensuring unique community IDs across indexes by incrementing values and tracking provenance information in the links dictionary. This is crucial for maintaining proper community relationships in multi-index searches.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_39\n\nLANGUAGE: Python\nCODE:\n```\ncommunity_df = query_helper.get_df(community_report_table_path)\nfor i in community_df[\"community\"].astype(int):\n    links[\"community\"][i + max_vals[\"community\"] + 1] = {\n        \"index_name\": sanitized_index_names_link[index_name],\n        \"id\": str(i),\n    }\nif max_vals[\"community\"] != -1:\n    col = community_df[\"community\"].astype(int) + max_vals[\"community\"] + 1\n    community_df[\"community\"] = col.astype(str)\nmax_vals[\"community\"] = community_df[\"community\"].astype(int).max()\ncommunity_dfs.append(community_df)\n```\n\n----------------------------------------\n\nTITLE: Reporter Singleton Implementation in Python\nDESCRIPTION: Singleton class managing reporter instances with URL validation utility. Handles reporter initialization based on environment configuration.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_72\n\nLANGUAGE: python\nCODE:\n```\nclass ReporterSingleton:\n    _instance: WorkflowCallbacks = None\n\n    @classmethod\n    def get_instance(cls) -> WorkflowCallbacks:\n        if cls._instance is None:\n            reporters = []\n            for reporter_name in os.getenv(\n                \"REPORTERS\", Reporters.CONSOLE.name.upper()\n            ).split(\",\"):\n                try:\n                    reporters.append(Reporters[reporter_name.upper()])\n                except KeyError:\n                    raise ValueError(f\"Found unknown reporter: {reporter_name}\")\n            cls._instance = load_pipeline_reporter(\n                reporting_dir=\"\", reporters=reporters\n            )\n        return cls._instance\n\ndef _is_valid_url(url: str) -> bool:\n    try:\n        result = urlparse(url)\n        return all([result.scheme, result.netloc])\n    except ValueError:\n        return False\n```\n\n----------------------------------------\n\nTITLE: Index Creation and Management Tests in Python\nDESCRIPTION: Test functions for creating, monitoring, and deleting indexes. Includes status checking and validation of the indexing process.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_84\n\nLANGUAGE: python\nCODE:\n```\n@pytest.fixture\ndef index_create(client, data_upload_small):\n    blob_container_name = data_upload_small\n    index_name = f\"test-index-{str(uuid.uuid4())}\"\n    request_data = {\n        \"storage_name\": blob_container_name,\n        \"index_name\": index_name,\n    }\n    response = client.post(url=f\"{client.base_url}{index_endpoint}\", json=request_data)\n    yield (index_name, response)\n```\n\n----------------------------------------\n\nTITLE: Processing Nodes DataFrames with Provenance Tracking\nDESCRIPTION: Processes nodes data, maintaining unique IDs across indexes by adjusting human_readable_id values and adding index-specific suffixes to various fields. Also tracks provenance information in the links dictionary for later reference.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_38\n\nLANGUAGE: Python\nCODE:\n```\n# note that nodes need to set before communities to that max community id makes sense\nnodes_df = query_helper.get_df(nodes_table_path)\nfor i in nodes_df[\"human_readable_id\"]:\n    links[\"nodes\"][i + max_vals[\"nodes\"] + 1] = {\n        \"index_name\": sanitized_index_names_link[index_name],\n        \"id\": i,\n    }\nif max_vals[\"nodes\"] != -1:\n    nodes_df[\"human_readable_id\"] += max_vals[\"nodes\"] + 1\nnodes_df[\"community\"] = nodes_df[\"community\"].apply(\n    lambda x: str(int(x) + max_vals[\"community\"] + 1) if x else x\n)\nnodes_df[\"id\"] = nodes_df[\"id\"].apply(lambda x: x + f\"-{index_name}\")\nnodes_df[\"title\"] = nodes_df[\"title\"].apply(lambda x: x + f\"-{index_name}\")\nnodes_df[\"source_id\"] = nodes_df[\"source_id\"].apply(\n    lambda x: \",\".join([i + f\"-{index_name}\" for i in x.split(\",\")])\n)\nmax_vals[\"nodes\"] = nodes_df[\"human_readable_id\"].max()\nnodes_dfs.append(nodes_df)\n```\n\n----------------------------------------\n\nTITLE: Data Upload and Cleanup Fixture in Python\nDESCRIPTION: Pytest fixture that handles test data upload using Wikipedia content, creates a blob container, and performs cleanup after tests. Includes file operations and API calls for data management.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_82\n\nLANGUAGE: Python\nCODE:\n```\n@pytest.fixture\ndef data_upload_small(client):\n    this_directory = os.path.dirname(\n        os.path.abspath(inspect.getfile(inspect.currentframe()))\n    )\n    # use a small amount of sample text to test the data upload endpoint\n    page = wikipedia.page(\"Alaska\")\n    with open(f\"{this_directory}/data/sample_text.txt\", \"w\") as f:\n        f.write(page.summary)\n\n    # test the upload of data\n    files = [(\"files\", open(f\"{this_directory}/data/sample_text.txt\", \"rb\"))]\n    assert len(files) > 0\n    blob_container_name = f\"test-data-{str(uuid.uuid4())}\"\n    print(f\"Creating blob data container: {blob_container_name}\")\n    response = client.post(\n        url=f\"{client.base_url}/data\",\n        files=files,\n        params={\"storage_name\": blob_container_name},\n    )\n    assert response.status_code == 200\n\n    yield blob_container_name  # test runs here\n\n    # clean up\n    os.remove(f\"{this_directory}/data/sample_text.txt\")\n    print(f\"Deleting blob data container: {blob_container_name}\")\n    response = client.delete(url=f\"{client.base_url}/data/{blob_container_name}\")\n    assert response.status_code == 200\n```\n\n----------------------------------------\n\nTITLE: Updating Context Data with Index Provenance\nDESCRIPTION: Enhances context data with index provenance information for reports, entities, and relationships. This function adds index_name and index_id fields to each context entry to track which index the information came from.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\ndef _update_context(context, links):\n    \"\"\"\n    Update context data.\n    context_keys = ['reports', 'entities', 'relationships', 'claims', 'sources']\n    \"\"\"\n    updated_context = {}\n    for key in context:\n        updated_entry = []\n        if key == \"reports\":\n            updated_entry = [\n                dict(\n                    {k: entry[k] for k in entry},\n                    **{\n                        \"index_name\": links[\"community\"][int(entry[\"id\"])][\"index_name\"],\n                        \"index_id\": links[\"community\"][int(entry[\"id\"])][\"id\"],\n                    },\n                )\n                for entry in context[key]\n            ]\n        if key == \"entities\":\n            updated_entry = [\n                dict(\n                    {k: entry[k] for k in entry},\n                    **{\n                        \"entity\": entry[\"entity\"].split(\"-\")[0],\n                        \"index_name\": links[\"entities\"][int(entry[\"id\"])][\"index_name\"],\n                        \"index_id\": links[\"entities\"][int(entry[\"id\"])][\"id\"],\n                    },\n                )\n                for entry in context[key]\n            ]\n        if key == \"relationships\":\n            updated_entry = [\n                dict(\n                    {k: entry[k] for k in entry},\n                    **{\n                        \n                    }\n                )\n                for entry in context[key]\n            ]\n```\n\n----------------------------------------\n\nTITLE: Generating Kubernetes Job Manifest for Indexing\nDESCRIPTION: Generates a Kubernetes Job manifest by loading a template YAML file and replacing key values with the provided parameters. The manifest includes the container image, job name, and command to run the indexing job.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_112\n\nLANGUAGE: python\nCODE:\n```\ndef _generate_aks_job_manifest(\n    docker_image_name: str,\n    index_name: str,\n    service_account_name: str,\n) -> dict:\n    \"\"\"Generate an AKS Jobs manifest file with the specified parameters.\n\n    The manifest must be valid YAML with certain values replaced by the provided arguments.\n    \"\"\"\n    # NOTE: this file location is relative to the WORKDIR set in Dockerfile-backend\n    with open(\"indexing-job-template.yaml\", \"r\") as f:\n        manifest = yaml.safe_load(f)\n    manifest[\"metadata\"][\"name\"] = f\"indexing-job-{sanitize_name(index_name)}\"\n    manifest[\"spec\"][\"template\"][\"spec\"][\"serviceAccountName\"] = service_account_name\n    manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\"image\"] = docker_image_name\n    manifest[\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\"command\"] = [\n        \"python\",\n        \"run-indexing-job.py\",\n        f\"-i={index_name}\",\n    ]\n    return manifest\n```\n\n----------------------------------------\n\nTITLE: Returning Search Results with Context Data\nDESCRIPTION: Updates context data with index provenance information and returns the search result along with the enhanced context. This provides clients with both the search results and the metadata needed to trace information sources.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\n# link index provenance to the context data\ncontext_data = _update_context(result[1], links)\n\nreturn GraphResponse(result=result[0], context_data=context_data)\n```\n\n----------------------------------------\n\nTITLE: Merging Nodes in NetworkX Graphs (Python)\nDESCRIPTION: This function merges nodes from a subgraph into a target graph, adding new nodes or merging attributes of existing nodes.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_86\n\nLANGUAGE: Python\nCODE:\n```\ndef merge_nodes(target: nx.Graph, subgraph: nx.Graph):\n    \"\"\"Merge nodes from subgraph into target using the operations defined in node_ops.\"\"\"\n    for node in subgraph.nodes:\n        if node not in target.nodes:\n            target.add_node(node, **(subgraph.nodes[node] or {}))\n        else:\n            merge_attributes(target.nodes[node], subgraph.nodes[node])\n```\n\n----------------------------------------\n\nTITLE: Creating API Client Fixture with Authentication in Python\nDESCRIPTION: Pytest fixture that creates an authenticated HTTP session for API testing. It retrieves the deployment URL and APIM subscription key from environment variables and sets up a requests session with the necessary headers.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_78\n\nLANGUAGE: python\nCODE:\n```\n@pytest.fixture(scope=\"session\")\ndef client(request):\n    \"\"\"Return the session base url which is the deployment url authorized with the apim subscription key stored in your .env file\"\"\"\n    deployment_url = os.environ[\"DEPLOYMENT_URL\"]\n    deployment_url = deployment_url.rstrip(\"/\")\n    apim_key = os.environ[\"APIM_SUBSCRIPTION_KEY\"]\n    session = requests.Session()\n    session.headers.update({\"Ocp-Apim-Subscription-Key\": apim_key})\n    session.base_url = deployment_url\n    return session\n```\n\n----------------------------------------\n\nTITLE: Processing Text Units DataFrames\nDESCRIPTION: Processes text units data by adding index-specific suffixes to IDs to maintain uniqueness across indexes. Unlike other tables, there's no need to adjust numeric IDs as text units are referenced by their string IDs.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_42\n\nLANGUAGE: Python\nCODE:\n```\ntext_units_df = query_helper.get_df(text_units_table_path)\ntext_units_df[\"id\"] = text_units_df[\"id\"].apply(lambda x: f\"{x}-{index_name}\")\ntext_units_dfs.append(text_units_df)\n```\n\n----------------------------------------\n\nTITLE: Pipeline Reporter Loader Implementation in Python\nDESCRIPTION: Function to create and configure a workflow callback manager with multiple reporter types including Blob storage, File system, Application Insights, and Console output. Handles reporter initialization and configuration based on environment settings.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ndef load_pipeline_reporter(\n    reporting_dir: str | None,\n    reporters: List[Reporters] | None = [],\n    index_name: str = \"\",\n    num_workflow_steps: int = 0,\n) -> WorkflowCallbacks:\n    callback_manager = WorkflowCallbacksManager()\n    for reporter in reporters:\n        match reporter:\n            case Reporters.BLOB:\n                container_name = \"logs\"\n                if reporting_dir is not None:\n                    container_name = os.path.join(reporting_dir, container_name)\n                blob_service_client = BlobServiceClientSingleton.get_instance()\n                container_root = Path(container_name).parts[0]\n                if not blob_service_client.get_container_client(\n                    container_root\n                ).exists():\n                    blob_service_client.create_container(container_root)\n                callback_manager.register(\n                    BlobWorkflowCallbacks(\n                        storage_account_blob_url=os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"],\n                        container_name=container_name,\n                        blob_name=f\"{datetime.now().strftime('%Y-%m-%d-%H:%M:%S:%f')}.logs.txt\",\n                        index_name=index_name,\n                        num_workflow_steps=num_workflow_steps,\n                    )\n                )\n            case Reporters.FILE:\n                callback_manager.register(FileWorkflowCallbacks(dir=reporting_dir))\n            case Reporters.APP_INSIGHTS:\n                if os.getenv(\"APP_INSIGHTS_CONNECTION_STRING\"):\n                    callback_manager.register(\n                        ApplicationInsightsWorkflowCallbacks(\n                            connection_string=os.environ[\n                                \"APP_INSIGHTS_CONNECTION_STRING\"\n                            ],\n                            index_name=index_name,\n                            num_workflow_steps=num_workflow_steps,\n                        )\n                    )\n            case Reporters.CONSOLE:\n                pass\n            case _:\n                print(f\"WARNING: unknown reporter type: {reporter}. Skipping.\")\n    callback_manager.register(\n        ConsoleWorkflowCallbacks(\n            index_name=index_name, num_workflow_steps=num_workflow_steps\n        )\n    )\n    return callback_manager\n```\n\n----------------------------------------\n\nTITLE: Workflow Step Removal Utility for GraphRAG Pipeline\nDESCRIPTION: A utility function to remove specific steps from a GraphRAG pipeline workflow. The function accepts either a single step name or a list of step names to be removed from the workflow.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_91\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\nfrom graphrag.index.config import PipelineWorkflowReference\n\n\ndef remove_step_from_workflow(\n    workflow: PipelineWorkflowReference, step_names: str | List[str]\n) -> List[PipelineWorkflowReference]:\n    if isinstance(step_names, str):\n        step_names = [step_names]\n    return [step for step in workflow.steps if step.get(\"verb\") not in step_names]\n```\n\n----------------------------------------\n\nTITLE: Implementing Error, Warning and Log Callbacks\nDESCRIPTION: Methods for handling error, warning, and general log events in workflows. These methods format and send appropriate log messages to Application Insights with relevant context.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_61\n\nLANGUAGE: python\nCODE:\n```\ndef on_error(\n    self,\n    message: str,\n    cause: Optional[BaseException] = None,\n    stack: Optional[str] = None,\n    details: Optional[dict] = None,\n) -> None:\n    \"\"\"A call back handler for when an error occurs.\"\"\"\n    details = {} if details is None else details\n    details = {\"cause\": str(cause), \"stack\": stack, **details}\n    self._logger.error(\n        message,\n        exc_info=True,\n        stack_info=False,\n        extra=self._format_details(details=details),\n    )\n\ndef on_warning(self, message: str, details: Optional[dict] = None) -> None:\n    \"\"\"A call back handler for when a warning occurs.\"\"\"\n    self._logger.warning(\n        message, stack_info=False, extra=self._format_details(details=details)\n    )\n\ndef on_log(self, message: str, details: Optional[dict] = None) -> None:\n    \"\"\"A call back handler for when a log message occurs.\"\"\"\n    self._logger.info(\n        message, stack_info=False, extra=self._format_details(details=details)\n    )\n\ndef on_measure(\n    self, name: str, value: float, details: Optional[dict] = None\n) -> None:\n    \"\"\"A call back handler for when a measurement occurs.\"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Implementing ConsoleWorkflowCallbacks Class in Python\nDESCRIPTION: A workflow callback implementation that writes to stdout. It tracks workflow progress, formats messages with appropriate context, and handles logging with configurable levels. The class initializes a unique logger and provides methods for reporting various workflow events.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_66\n\nLANGUAGE: python\nCODE:\n```\nclass ConsoleWorkflowCallbacks(NoopWorkflowCallbacks):\n    \"\"\"A reporter that writes to a stream (sys.stdout).\"\"\"\n\n    _logger: logging.Logger\n    _logger_name: str\n    _logger_level: int\n    _logger_level_name: str\n    _properties: Dict[str, Any]\n    _workflow_name: str\n    _index_name: str\n    _num_workflow_steps: int\n    _processed_workflow_steps: list[str] = []\n\n    def __init__(\n        self,\n        logger_name: str | None = None,\n        logger_level: int = logging.INFO,\n        index_name: str = \"\",\n        num_workflow_steps: int = 0,\n        properties: Dict[str, Any] = {},\n    ):\n        \"\"\"\n        Initialize the ConsoleWorkflowCallbacks.\n\n        Args:\n            logger_name (str | None, optional): The name of the logger. Defaults to None.\n            logger_level (int, optional): The logging level. Defaults to logging.INFO.\n            index_name (str, optional): The name of an index. Defaults to \"\".\n            num_workflow_steps (int): A list of workflow names ordered by their execution. Defaults to [].\n            properties (Dict[str, Any], optional): Additional properties to be included in the log. Defaults to {}.\n        \"\"\"\n        self._logger: logging.Logger\n        self._logger_name = logger_name\n        self._logger_level = logger_level\n        self._logger_level_name: str = logging.getLevelName(logger_level)\n        self._properties = properties\n        self._workflow_name = \"N/A\"\n        self._index_name = index_name\n        self._num_workflow_steps = num_workflow_steps\n        self._processed_workflow_steps = []  # maintain a running list of workflow steps that get processed\n        \"\"\"Create a new logger with an AppInsights handler.\"\"\"\n        self.__init_logger()\n```\n\n----------------------------------------\n\nTITLE: Reporter Types and Configuration in Python\nDESCRIPTION: Enum and configuration classes defining supported reporter types and Application Insights specific configuration. Includes integration with pipeline reporting configuration system.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_73\n\nLANGUAGE: python\nCODE:\n```\nclass Reporters(Enum):\n    BLOB = (1, \"blob\")\n    CONSOLE = (2, \"console\")\n    FILE = (3, \"file\")\n    APP_INSIGHTS = (4, \"app_insights\")\n\nclass PipelineAppInsightsReportingConfig(\n    PipelineReportingConfig[Literal[\"app_insights\"]]\n):\n    type: Literal[\"app_insights\"] = Reporters.APP_INSIGHTS.name.lower()\n    connection_string: str = pydantic_Field(\n        description=\"The connection string for the App Insights instance.\",\n        default=None,\n    )\n    logger_name: str = pydantic_Field(\n        description=\"The name for logger instance\", default=None\n    )\n    logger_level: int = pydantic_Field(\n        description=\"The name of the logger. Defaults to None.\", default=logging.INFO\n    )\n\nPipelineReportingConfigTypes = (\n    reporting.PipelineReportingConfigTypes | PipelineAppInsightsReportingConfig\n)\n```\n\n----------------------------------------\n\nTITLE: Workflow Event Handlers\nDESCRIPTION: Implementation of workflow event handlers for start, end, errors, warnings and generic logs. Each handler formats and writes appropriate log entries to blob storage.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_65\n\nLANGUAGE: Python\nCODE:\n```\ndef on_error(self, message: str, cause: BaseException | None = None, stack: str | None = None, details: dict | None = None):\n        self._write_log({\n            \"type\": \"error\",\n            \"data\": message,\n            \"cause\": str(cause),\n            \"stack\": stack,\n            \"details\": details,\n        })\n\n    def on_workflow_start(self, name: str, instance: object) -> None:\n        self._workflow_name = name\n        self._processed_workflow_steps.append(name)\n        message = f\"Index: {self._index_name} -- \" if self._index_name else \"\"\n        workflow_progress = (f\" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})\" if self._num_workflow_steps else \"\")\n        message += f\"Workflow{workflow_progress}: {name} started.\"\n        details = {\n            \"workflow_name\": name,\n        }\n        if self._index_name:\n            details[\"index_name\"] = self._index_name\n        self._write_log({\n            \"type\": \"on_workflow_start\",\n            \"data\": message,\n            \"details\": details,\n        })\n\n    def on_workflow_end(self, name: str, instance: object) -> None:\n        message = f\"Index: {self._index_name} -- \" if self._index_name else \"\"\n        workflow_progress = (f\" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})\" if self._num_workflow_steps else \"\")\n        message += f\"Workflow{workflow_progress}: {name} complete.\"\n        details = {\n            \"workflow_name\": name,\n        }\n        if self._index_name:\n            details[\"index_name\"] = self._index_name\n        self._write_log({\n            \"type\": \"on_workflow_end\",\n            \"data\": message,\n            \"details\": details,\n        })\n\n    def on_warning(self, message: str, details: dict | None = None):\n        self._write_log({\"type\": \"warning\", \"data\": message, \"details\": details})\n\n    def on_log(self, message: str, details: dict | None = None):\n        self._write_log({\"type\": \"log\", \"data\": message, \"details\": details})\n\n    def on_measure(self, name: str, value: float, details: Optional[dict] = None) -> None:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Storage Container Deletion Endpoint\nDESCRIPTION: FastAPI endpoint for deleting a storage container from Azure Blob Storage and its associated metadata from CosmosDB.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@data_route.delete(\n    \"/{storage_name}\",\n    summary=\"Delete a data storage container\",\n    response_model=BaseResponse,\n    responses={200: {\"model\": BaseResponse}},\n)\nasync def delete_files(storage_name: str):\n    \"\"\"\n    Delete a specified data storage container.\n    \"\"\"\n    sanitized_storage_name = sanitize_name(storage_name)\n    try:\n        delete_blob_container(sanitized_storage_name)\n        container_store_client = (\n            azure_storage_client_manager.get_cosmos_container_client(\n                database_name=\"graphrag\", container_name=\"container-store\"\n            )\n        )\n        container_store_client.delete_item(\n            item=sanitized_storage_name,\n            partition_key=sanitized_storage_name,\n        )\n    except Exception:\n        reporter = ReporterSingleton().get_instance()\n        reporter.on_error(\n            f\"Error deleting container {storage_name}.\",\n            details={\"Container\": storage_name},\n        )\n        raise HTTPException(\n            status_code=500, detail=f\"Error deleting container '{storage_name}'.\"\n        )\n    return BaseResponse(status=\"Success\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Workflow End Callback\nDESCRIPTION: Method triggered when a workflow completes. It logs workflow completion information including the workflow name, index name, and progress tracking details to Application Insights.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ndef on_workflow_end(self, name: str, instance: object) -> None:\n    \"\"\"Execute this callback when a workflow ends.\"\"\"\n    message = f\"Index: {self._index_name} -- \" if self._index_name else \"\"\n    workflow_progress = (\n        f\" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})\"\n        if self._num_workflow_steps\n        else \"\"\n    )  # will take the form \"(1/4)\"\n    message += f\"Workflow{workflow_progress}: {name} complete.\"\n    details = {\n        \"workflow_name\": name,\n        # \"workflow_instance\": str(instance),\n    }\n    if self._index_name:\n        details[\"index_name\"] = self._index_name\n    self._logger.info(\n        message, stack_info=False, extra=self._format_details(details=details)\n    )\n```\n\n----------------------------------------\n\nTITLE: Listing Kubernetes Jobs in a Namespace\nDESCRIPTION: Retrieves a list of all Kubernetes jobs running in a specified namespace using the Kubernetes API. This function is used to check for currently active jobs before scheduling new ones.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_113\n\nLANGUAGE: python\nCODE:\n```\ndef list_k8s_jobs(namespace: str) -> list[str]:\n    \"\"\"List all k8s jobs in a given namespace.\"\"\"\n    config.load_incluster_config()\n    batch_v1 = client.BatchV1Api()\n    jobs = batch_v1.list_namespaced_job(namespace=namespace)\n    job_list = []\n    for job in jobs.items:\n        job_list.append(job.metadata.name)\n    return job_list\n```\n\n----------------------------------------\n\nTITLE: Testing Blob Service Client Methods in Python\nDESCRIPTION: Test methods for verifying the functionality of blob service client retrieval in AzureStorageClientManager, including both synchronous and asynchronous client access.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_75\n\nLANGUAGE: python\nCODE:\n```\n    def test_get_blob_service_client(self):\n        manager = self.get_azure_storage_client_manager()\n        client = manager.get_blob_service_client()\n        assert client == BlobServiceClient\n\n    def test_get_blob_service_client_async(self):\n        manager = self.get_azure_storage_client_manager()\n        client = manager.get_blob_service_client_async()\n        assert client == BlobServiceClientAsync\n```\n\n----------------------------------------\n\nTITLE: Workflow Event Handlers for ConsoleWorkflowCallbacks\nDESCRIPTION: Set of methods that handle different workflow events such as start, end, errors, warnings, and logs. Each method formats appropriate messages with context information like workflow progress, index name, and additional details before logging them to the console.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_69\n\nLANGUAGE: python\nCODE:\n```\ndef on_workflow_start(self, name: str, instance: object) -> None:\n    \"\"\"Execute this callback when a workflow starts.\"\"\"\n    self._workflow_name = name\n    self._processed_workflow_steps.append(name)\n    message = f\"Index: {self._index_name} -- \" if self._index_name else \"\"\n    workflow_progress = (\n        f\" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})\"\n        if self._num_workflow_steps\n        else \"\"\n    )  # will take the form \"(1/4)\"\n    message += f\"Workflow{workflow_progress}: {name} started.\"\n    details = {\n        \"workflow_name\": name,\n        # \"workflow_instance\": str(instance),\n    }\n    if self._index_name:\n        details[\"index_name\"] = self._index_name\n    self._logger.info(\n        message, stack_info=False, extra=self._format_details(details=details)\n    )\n\ndef on_workflow_end(self, name: str, instance: object) -> None:\n    \"\"\"Execute this callback when a workflow ends.\"\"\"\n    message = f\"Index: {self._index_name} -- \" if self._index_name else \"\"\n    workflow_progress = (\n        f\" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})\"\n        if self._num_workflow_steps\n        else \"\"\n    )  # will take the form \"(1/4)\"\n    message += f\"Workflow{workflow_progress}: {name} complete.\"\n    details = {\n        \"workflow_name\": name,\n        # \"workflow_instance\": str(instance),\n    }\n    if self._index_name:\n        details[\"index_name\"] = self._index_name\n    self._logger.info(\n        message, stack_info=False, extra=self._format_details(details=details)\n    )\n\ndef on_error(\n    self,\n    message: str,\n    cause: Optional[BaseException] = None,\n    stack: Optional[str] = None,\n    details: Optional[dict] = None,\n) -> None:\n    \"\"\"A call back handler for when an error occurs.\"\"\"\n    details = {} if details is None else details\n    details = {\"cause\": str(cause), \"stack\": stack, **details}\n    self._logger.error(\n        message,\n        exc_info=True,\n        stack_info=False,\n        extra=self._format_details(details=details),\n    )\n\ndef on_warning(self, message: str, details: Optional[dict] = None) -> None:\n    \"\"\"A call back handler for when a warning occurs.\"\"\"\n    self._logger.warning(\n        message, stack_info=False, extra=self._format_details(details=details)\n    )\n\ndef on_log(self, message: str, details: Optional[dict] = None) -> None:\n    \"\"\"A call back handler for when a log message occurs.\"\"\"\n    self._logger.info(\n        message, stack_info=False, extra=self._format_details(details=details)\n    )\n\ndef on_measure(\n    self, name: str, value: float, details: Optional[dict] = None\n) -> None:\n    \"\"\"A call back handler for when a measurement occurs.\"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Creating FastAPI Router for GraphRAG Streaming Query Operations\nDESCRIPTION: Initializes an APIRouter for streaming query operations with a specific prefix and tag for documentation purposes.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nquery_streaming_route = APIRouter(\n    prefix=\"/query/streaming\",\n    tags=[\"Query Streaming Operations\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Logger for ConsoleWorkflowCallbacks\nDESCRIPTION: Creates a unique logger instance with proper handlers and formatting. The method generates a unique hash-based name for the logger to avoid conflicts and configures a StreamHandler to output to stdout with an appropriate formatter.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndef __init_logger(self, max_logger_init_retries: int = 10):\n    max_retry = max_logger_init_retries\n    while not (hasattr(self, \"_logger\")):\n        if max_retry == 0:\n            raise Exception(\n                \"Failed to create logger. Could not disambiguate logger name.\"\n            )\n\n        # generate a unique logger name\n        current_time = str(time.time())\n        unique_hash = hashlib.sha256(current_time.encode()).hexdigest()\n        self._logger_name = f\"{self.__class__.__name__}-{unique_hash}\"\n        if self._logger_name not in logging.Logger.manager.loggerDict:\n            # instantiate new logger\n            self._logger = logging.getLogger(self._logger_name)\n            self._logger.propagate = False\n            # remove any existing handlers\n            self._logger.handlers.clear()\n            # create a console handler\n            handler = logging.StreamHandler(stream=sys.stdout)\n            # create a formatter and include 'extra_details' in the format string\n            handler.setFormatter(\n                # logging.Formatter(\n                #     \"[%(levelname)s] %(asctime)s - %(message)s \\n %(stack)s\"\n                # )\n                logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\")\n            )\n            self._logger.addHandler(handler)\n            # set logging level\n            self._logger.setLevel(logging.INFO)\n\n        # reduce sentinel counter value\n        max_retry -= 1\n```\n\n----------------------------------------\n\nTITLE: Sanitizing Names for Azure Storage in Python\nDESCRIPTION: This function sanitizes a human-readable name by converting it to a SHA256 hash and truncating it to 128 bits. The result is used as an identifier for container names in Azure Storage and CosmosDB.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef sanitize_name(name: str | None) -> str | None:\n    \"\"\"\n    Sanitize a human-readable name by converting it to a SHA256 hash, then truncate\n    to 128 bit length to ensure it is within the 63 character limit imposed by Azure Storage.\n\n    The sanitized name will be used to identify container names in both Azure Storage and CosmosDB.\n\n    Args:\n    -----\n    name (str)\n        The name to be sanitized.\n\n    Returns: str\n        The sanitized name.\n    \"\"\"\n    if not name:\n        return None\n    name = name.encode()\n    name_hash = hashlib.sha256(name)\n    truncated_hash = name_hash.digest()[:16]  # get the first 16 bytes (128 bits)\n    return truncated_hash.hex()\n```\n\n----------------------------------------\n\nTITLE: Processing Community Level Setting for GraphRAG Search\nDESCRIPTION: Handles the community level setting for GraphRAG search. It checks if the request contains a community level value, defaulting to level 2 if not provided, as investigations show this is most useful for local search.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\nif isinstance(request.community_level, int):\n    COMMUNITY_LEVEL = request.community_level\nelse:\n    # Current investigations show that community level 2 is the most useful for local search. Set this as the default value\n    COMMUNITY_LEVEL = 2\n```\n\n----------------------------------------\n\nTITLE: Dictionary Flattening Utility for Structured Logging\nDESCRIPTION: A utility function that recursively flattens nested dictionaries into a single-level structure for better compatibility with structured logging in Application Insights. This enables nested properties to be represented with key separators.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_62\n\nLANGUAGE: python\nCODE:\n```\ndef unwrap_dict(input_dict, parent_key=\"\", sep=\"_\"):\n    \"\"\"\n    Recursively unwraps a nested dictionary by flattening it into a single-level dictionary.\n\n    Args:\n        input_dict (dict): The input dictionary to be unwrapped.\n        parent_key (str, optional): The parent key to be prepended to the keys of the unwrapped dictionary. Defaults to ''.\n        sep (str, optional): The separator to be used between the parent key and the child key. Defaults to '_'.\n\n    Returns:\n        dict: The unwrapped dictionary.\n    \"\"\"\n    items = []\n    for k, v in input_dict.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(unwrap_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n```\n\n----------------------------------------\n\nTITLE: Deleting Azure Blob Container in Python\nDESCRIPTION: This function deletes a specified Azure Blob container if it exists. It uses the BlobServiceClient to check for container existence and perform the deletion.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef delete_blob_container(container_name: str):\n    \"\"\"\n    Delete a blob container. If it does not exist, do nothing.\n    If exception is raised, the calling function should catch it.\n    \"\"\"\n    if blob_service_client.get_container_client(container_name).exists():\n        blob_service_client.delete_container(container_name)\n```\n\n----------------------------------------\n\nTITLE: Status Property Implementation with PipelineJobState Type\nDESCRIPTION: Getter and setter methods for status property with database update functionality, using PipelineJobState enum type.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_108\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef status(self) -> PipelineJobState:\n    return self._status\n\n@status.setter\ndef status(self, status: PipelineJobState) -> None:\n    self._status = status\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Validating Index File Existence in Azure Blob Storage\nDESCRIPTION: This function checks if a specified index exists in CosmosDB and if the corresponding blob file exists in Azure Blob Storage. It raises a ValueError if either condition is not met.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef validate_index_file_exist(index_name: str, file_name: str):\n    \"\"\"\n    Check if index exists and that the specified blob file exists.\n\n    A \"valid\" index is defined by having an entry in the container-store table in cosmos db.\n    Further checks are done to ensure the blob container and file exist.\n\n    Args:\n    -----\n    index_name (str)\n        Name of the index to validate.\n    file_name (str)\n        The blob file to be validated.\n\n    Raises: ValueError\n    \"\"\"\n    # verify index_name is a valid index by checking container-store in cosmos db\n    try:\n        container_store_client = (\n            azure_storage_client_manager.get_cosmos_container_client(\n                database_name=\"graphrag\", container_name=\"container-store\"\n            )\n        )\n        container_store_client.read_item(index_name, index_name)\n    except Exception:\n        raise ValueError(f\"Container {index_name} is not a valid index.\")\n    # check for file existence\n    index_container_client = blob_service_client.get_container_client(index_name)\n    if not index_container_client.exists():\n        raise ValueError(f\"Container {index_name} not found.\")\n    if not index_container_client.get_blob_client(file_name).exists():\n        raise ValueError(f\"File {file_name} in container {index_name} not found.\")\n```\n\n----------------------------------------\n\nTITLE: Database Record Update Method in PipelineJob Class\nDESCRIPTION: Updates a database record by upserting the current model state using a container reference.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_95\n\nLANGUAGE: Python\nCODE:\n```\nPipelineJob._jobs_container().upsert_item(body=self.dump_model())\n```\n\n----------------------------------------\n\nTITLE: Progress Property Implementation\nDESCRIPTION: Getter and setter methods for progress property with database synchronization when changed.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_110\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef progress(self) -> str:\n    return self._progress\n\n@progress.setter\ndef progress(self, progress: str) -> None:\n    self._progress = progress\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Validating Azure Blob Container Name in Python\nDESCRIPTION: This function validates if a given container name adheres to Azure resource naming rules. It checks for length, starting character, valid characters, and specific patterns like consecutive hyphens.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef validate_blob_container_name(container_name: str):\n    \"\"\"\n    Check if container name is valid based on Azure resource naming rules.\n\n        - A blob container name must be between 3 and 63 characters in length.\n        - Start with a letter or number\n        - All letters used in blob container names must be lowercase.\n        - Contain only letters, numbers, or the hyphen.\n        - Consecutive hyphens are not permitted.\n        - Cannot end with a hyphen.\n\n    Args:\n    -----\n    container_name (str)\n        The blob container name to be validated.\n\n    Raises: ValueError\n    \"\"\"\n    # Check the length of the name\n    if len(container_name) < 3 or len(container_name) > 63:\n        raise ValueError(\n            f\"Container name must be between 3 and 63 characters in length. Name provided was {len(container_name)} characters long.\"\n        )\n\n    # Check if the name starts with a letter or number\n    if not container_name[0].isalnum():\n        raise ValueError(\n            f\"Container name must start with a letter or number. Starting character was {container_name[0]}.\"\n        )\n\n    # Check for valid characters (letters, numbers, hyphen) and lowercase letters\n    if not re.match(\"^[a-z0-9-]+$\", container_name):\n        raise ValueError(\n            f\"Container name must only contain:\\n- lowercase letters\\n- numbers\\n- or hyphens\\nName provided was {container_name}.\"\n        )\n\n    # Check for consecutive hyphens\n    if \"--\" in container_name:\n        raise ValueError(\n            f\"Container name cannot contain consecutive hyphens. Name provided was {container_name}.\"\n        )\n\n    # Check for hyphens at the end of the name\n    if container_name[-1] == \"-\":\n        raise ValueError(\n            f\"Container name cannot end with a hyphen. Name provided was {container_name}.\"\n        )\n```\n\n----------------------------------------\n\nTITLE: Completed Workflows Property Implementation\nDESCRIPTION: Getter and setter methods for completed_workflows list property with database updating.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_106\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef completed_workflows(self) -> List[str]:\n    return self._completed_workflows\n\n@completed_workflows.setter\ndef completed_workflows(self, completed_workflows: List[str]) -> None:\n    self._completed_workflows = completed_workflows\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Implementing Workflow Start Callback\nDESCRIPTION: Method triggered when a workflow starts. It logs workflow start information, including the workflow name, index name, and progress tracking details to Application Insights.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_59\n\nLANGUAGE: python\nCODE:\n```\ndef on_workflow_start(self, name: str, instance: object) -> None:\n    \"\"\"Execute this callback when a workflow starts.\"\"\"\n    self._workflow_name = name\n    self._processed_workflow_steps.append(name)\n    message = f\"Index: {self._index_name} -- \" if self._index_name else \"\"\n    workflow_progress = (\n        f\" ({len(self._processed_workflow_steps)}/{self._num_workflow_steps})\"\n        if self._num_workflow_steps\n        else \"\"\n    )  # will take the form \"(1/4)\"\n    message += f\"Workflow{workflow_progress}: {name} started.\"\n    details = {\n        \"workflow_name\": name,\n        # \"workflow_instance\": str(instance),\n    }\n    if self._index_name:\n        details[\"index_name\"] = self._index_name\n    self._logger.info(\n        message, stack_info=False, extra=self._format_details(details=details)\n    )\n```\n\n----------------------------------------\n\nTITLE: Entity Extraction Prompt Property Implementation\nDESCRIPTION: Getter and setter methods for entity_extraction_prompt with database synchronization.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_102\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef entity_extraction_prompt(self) -> str:\n    return self._entity_extraction_prompt\n\n@entity_extraction_prompt.setter\ndef entity_extraction_prompt(self, entity_extraction_prompt: str) -> None:\n    self._entity_extraction_prompt = entity_extraction_prompt\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Failed Workflows Property Implementation\nDESCRIPTION: Getter and setter methods for failed_workflows list property with database synchronization.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_107\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef failed_workflows(self) -> List[str]:\n    return self._failed_workflows\n\n@failed_workflows.setter\ndef failed_workflows(self, failed_workflows: List[str]) -> None:\n    self._failed_workflows = failed_workflows\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Formatting Details for Workflow Callback Logging\nDESCRIPTION: Helper method that combines base properties with provided details for structured logging. This ensures that all log entries follow a consistent format and include the necessary context information.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_68\n\nLANGUAGE: python\nCODE:\n```\ndef _format_details(self, details: Dict[str, Any] | None = None) -> Dict[str, Any]:\n    \"\"\"\n    Format the details dictionary to comply with the Application Insights structured.\n\n    logging Property column standard.\n\n    Args:\n        details (Dict[str, Any] | None): Optional dictionary containing additional details to log.\n\n    Returns:\n        Dict[str, Any]: The formatted details dictionary with custom dimensions.\n    \"\"\"\n    if not isinstance(details, dict) or (details is None):\n        details = {}\n    return {**self._properties, **details}\n```\n\n----------------------------------------\n\nTITLE: Human Readable Storage Name Property Implementation\nDESCRIPTION: Getter and setter methods for human_readable_storage_name that updates the database when changed.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_100\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef human_readable_storage_name(self) -> str:\n    return self._human_readable_storage_name\n\n@human_readable_storage_name.setter\ndef human_readable_storage_name(self, human_readable_storage_name: str) -> None:\n    self._human_readable_storage_name = human_readable_storage_name\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Formatting Log Details for Application Insights\nDESCRIPTION: Helper method that formats logging details to comply with Application Insights structured logging standards. It combines default properties with provided details and wraps them in a custom_dimensions object.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ndef _format_details(self, details: Dict[str, Any] | None = None) -> Dict[str, Any]:\n    \"\"\"\n    Format the details dictionary to comply with the Application Insights structured\n    logging Property column standard.\n\n    Args:\n        details (Dict[str, Any] | None): Optional dictionary containing additional details to log.\n\n    Returns:\n        Dict[str, Any]: The formatted details dictionary with custom dimensions.\n    \"\"\"\n    if not isinstance(details, dict) or (details is None):\n        details = {}\n    return {\"custom_dimensions\": {**self._properties, **unwrap_dict(details)}}\n```\n\n----------------------------------------\n\nTITLE: Community Report Prompt Property Implementation\nDESCRIPTION: Getter and setter methods for community_report_prompt with database updating when changed.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_103\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef community_report_prompt(self) -> str:\n    return self._community_report_prompt\n\n@community_report_prompt.setter\ndef community_report_prompt(self, community_report_prompt: str) -> None:\n    self._community_report_prompt = community_report_prompt\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Epoch Request Time Property Implementation\nDESCRIPTION: Getter and setter methods for epoch_request_time property with protection against changes after initialization.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_97\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef epoch_request_time(self) -> int:\n    return self._epoch_request_time\n\n@epoch_request_time.setter\ndef epoch_request_time(self, epoch_request_time: int) -> None:\n    if self._epoch_request_time is not None:\n        self._epoch_request_time = epoch_request_time\n    else:\n        raise ValueError(\"ID cannot be changed once set.\")\n```\n\n----------------------------------------\n\nTITLE: Python API Usage Example\nDESCRIPTION: Example demonstrating how to use the repominify Python API for building and analyzing code graphs.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/README.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom repominify import CodeGraphBuilder, ensure_dependencies, configure_logging\n\n# Enable debug logging (optional)\nconfigure_logging(debug=True)\n\n# Check dependencies\nif ensure_dependencies():\n    # Create graph builder\n    builder = CodeGraphBuilder()\n    \n    # Parse the Repomix output file\n    file_entries = builder.parser.parse_file(\"repomix-output.txt\")\n    \n    # Build the graph\n    graph = builder.build_graph(file_entries)\n    \n    # Save outputs and get comparison\n    text_content, comparison = builder.save_graph(\n        \"output_directory\",\n        input_file=\"repomix-output.txt\"\n    )\n    \n    # Print comparison\n    print(comparison)\n```\n\n----------------------------------------\n\nTITLE: Creating Azure Storage Client Manager Test Class in Python\nDESCRIPTION: Defines a TestAzureStorageClientManager class that mocks Azure client interactions for unit testing. The class includes a helper method to create an instance of AzureStorageClientManager with mocked dependencies.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_74\n\nLANGUAGE: python\nCODE:\n```\nclass TestAzureStorageClientManager:\n    @patch(\"src.api.azure_clients.BlobServiceClientAsync.from_connection_string\")\n    @patch(\"src.api.azure_clients.BlobServiceClient.from_connection_string\")\n    @patch(\"src.api.azure_clients.CosmosClient.from_connection_string\")\n    def get_azure_storage_client_manager(\n        self,\n        mock_cosmos_client,\n        mock_blob_service_client,\n        mock_blob_service_client_async,\n    ):\n        mock_blob_service_client.return_value = BlobServiceClient\n        mock_blob_service_client_async.return_value = BlobServiceClientAsync\n        mock_cosmos_client.return_value = CosmosClient\n        manager = AzureStorageClientManager()\n        return manager\n```\n\n----------------------------------------\n\nTITLE: Creating Invalid Test Index Data Fixture in Python\nDESCRIPTION: Pytest fixture that prepares an invalid test environment by creating a Blob Storage container with incomplete data (explicitly deleting a required file). It also configures Cosmos DB with metadata but in a different format than the valid fixture, yielding the container name for tests and cleaning up afterward.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_80\n\nLANGUAGE: python\nCODE:\n```\n@pytest.fixture()\ndef prepare_invalid_index_data():\n    \"\"\"Prepare valid test data by uploading the result files of a \\\"valid\\\" indexing run to a new blob container.\"\"\"\n    account_url = os.environ[\"STORAGE_ACCOUNT_BLOB_URL\"]\n    credential = DefaultAzureCredential()\n    blob_service_client = BlobServiceClient(account_url, credential=credential)\n\n    # generate a unique data container name\n    container_name = \"test-index-pytest-\" + str(uuid.uuid4())\n    container_name = container_name.replace(\"_\", \"-\").replace(\".\", \"-\").lower()[:63]\n    blob_service_client.create_container(container_name)\n    this_directory = os.path.dirname(\n        os.path.abspath(inspect.getfile(inspect.currentframe()))\n    )\n    _upload_files(\n        blob_service_client, f\"{this_directory}/data/test-index\", container_name\n    )\n\n    blob_client = blob_service_client.get_blob_client(\n        container=container_name, blob=\"embedded_graph.1.graphml\"\n    )\n    blob_client.delete_blob()\n\n    endpoint = os.environ[\"COSMOS_URI_ENDPOINT\"]\n    credential = DefaultAzureCredential()\n    client = CosmosClient(endpoint, credential)\n\n    container_store = \"container-store\"\n    database = client.get_database_client(container_store)\n    container_container = database.get_container_client(container_store)\n    index_item = {\"id\": container_name, \"type\": \"index\"}\n    container_container.create_item(body=index_item)\n\n    container_store = \"jobs\"\n    database = client.get_database_client(container_store)\n    container_jobs = database.get_container_client(container_store)\n    index_item = {\n        \"id\": container_name,\n        \"storage_name\": \"data1\",\n        \"task_state\": \"15 steps out of 15 completed.\",\n        \"percent_complete\": \"100.0\",\n    }\n    container_jobs.create_item(body=index_item)\n\n    yield container_name  # test runs here\n\n    # clean up\n    blob_service_client.delete_container(container_name)\n    container_container.delete_item(item=container_name, partition_key=container_name)\n    container_jobs.delete_item(item=container_name, partition_key=container_name)\n```\n\n----------------------------------------\n\nTITLE: Sanitized Index Name Property Implementation\nDESCRIPTION: Getter and setter methods for sanitized_index_name with database update functionality.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_99\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef sanitized_index_name(self) -> str:\n    return self._sanitized_index_name\n\n@sanitized_index_name.setter\ndef sanitized_index_name(self, sanitized_index_name: str) -> None:\n    self._sanitized_index_name = sanitized_index_name\n    self.update_db()\n    self._sanitized_storage_name = sanitized_storage_name\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Testing Cosmos DB Client Methods in Python\nDESCRIPTION: Test methods for verifying the functionality of Cosmos DB client retrieval in AzureStorageClientManager, including access to the main client, database client, and container client.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_76\n\nLANGUAGE: python\nCODE:\n```\n    def test_get_cosmos_client(self):\n        manager = self.get_azure_storage_client_manager()\n        client = manager.get_cosmos_client()\n        assert client == CosmosClient\n\n    @patch(\"src.api.azure_clients.CosmosClient.get_database_client\")\n    def test_get_cosmos_database_client(self, mock_get_database_client):\n        mock_get_database_client.return_value = DatabaseProxy\n        manager = self.get_azure_storage_client_manager()\n        db_name = \"test_database\"\n        client = manager.get_cosmos_database_client(db_name)\n        assert client == DatabaseProxy\n\n    @patch(\"src.api.azure_clients.DatabaseProxy.get_container_client\")\n    @patch(\"src.api.azure_clients.CosmosClient.get_database_client\")\n    def test_get_cosmos_container_client(\n        self, mock_get_database_client, mock_get_container_client\n    ):\n        mock_get_database_client.return_value = DatabaseProxy\n        mock_get_container_client.return_value = ContainerProxy\n        manager = self.get_azure_storage_client_manager()\n        database_name = \"test_database\"\n        container_name = \"test_container\"\n        client = manager.get_cosmos_container_client(\n            database_name=database_name, container_name=container_name\n        )\n        assert client == manager._cosmos_client.get_database_client(\n            database_name\n        ).get_container_client(container_name)\n```\n\n----------------------------------------\n\nTITLE: File Content Cleaner Class\nDESCRIPTION: Class for cleaning file content by removing illegal XML characters before upload, with tracking of the number of changes made.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass Cleaner:\n    def __init__(self, file):\n        self.file = file\n        self.name = file.name\n        self.changes = 0\n\n    def clean(self, val, replacement=\"\"):\n        # fmt: off\n        _illegal_xml_chars_RE = re.compile(\n            \"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1F\\uD800-\\uDFFF\\uFFFE\\uFFFF]\"\n        )\n        # fmt: on\n        self.changes += len(_illegal_xml_chars_RE.findall(val))\n        return _illegal_xml_chars_RE.sub(replacement, val)\n\n    def read(self, n):\n        return self.clean(self.file.read(n).decode()).encode(\n            encoding=\"utf-8\", errors=\"strict\"\n        )\n\n    def name(self):\n        return self.file.name\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        self.file.close()\n```\n\n----------------------------------------\n\nTITLE: Creating Entity Configuration Fixture in Python\nDESCRIPTION: Pytest fixture that creates an entity configuration by making a POST request to the pipeline config endpoint. Returns a unique entity configuration name and includes organization entity type examples.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_81\n\nLANGUAGE: Python\nCODE:\n```\n@pytest.fixture\ndef create_entity_configuration(client):\n    endpoint = \"/pipeline/config\"\n    entity_configuration_name = str(uuid.uuid4())\n    entity_types = [\"ORGANIZATION\"]\n    entity_examples = [\n        {\n            \"Entity_types\": \"ORGANIZATION\",\n            \"Text\": \"Arm's (ARM) stock skyrocketed...\",\n            \"Output\": '(\"entity\"{tuple_delimiter}ARM{tuple_delimiter}ORGANIZATION{tuple_delimiter}...)',\n        }\n    ]\n    request_data = {\n        \"entityConfigurationName\": entity_configuration_name,\n        \"entityTypes\": entity_types,\n        \"entityExamples\": entity_examples,\n    }\n    response = client.post(\n        url=f\"{client.base_url}{endpoint}/entity/types\", json=request_data\n    )\n    assert response.status_code == 200\n    yield entity_configuration_name\n```\n\n----------------------------------------\n\nTITLE: Analysis Output Example\nDESCRIPTION: Example of the analysis output showing file statistics and comparison metrics.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/README.md#2025-04-21_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nAnalysis Complete!\n📊 File Stats:\n────────────────\n  Total Files: 29\n  Total Chars: 143,887\n Total Tokens: 14,752\n       Output: input.txt\n     Security: ✔ No suspicious files detected\n\n📊 File Stats:\n────────────────\n  Total Files: 29\n  Total Chars: 26,254\n Total Tokens: 3,254\n       Output: code_graph.txt\n     Security: ✔ No suspicious files detected\n\n📈 Comparison:\n────────────────\n Char Reduction: 81.8%\nToken Reduction: 77.9%\nSecurity Notes: ✔ No issues found\n```\n\n----------------------------------------\n\nTITLE: Merging Edges in NetworkX Graphs (Python)\nDESCRIPTION: This function merges edges from a subgraph into a target graph, adding new edges or merging attributes of existing edges.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_87\n\nLANGUAGE: Python\nCODE:\n```\ndef merge_edges(target_graph: nx.Graph, subgraph: nx.Graph):\n    \"\"\"Merge edges from subgraph into target using the operations defined in edge_ops.\"\"\"\n    for source, target, edge_data in subgraph.edges(data=True):  # type: ignore\n        if not target_graph.has_edge(source, target):\n            target_graph.add_edge(source, target, **(edge_data or {}))\n        else:\n            merge_attributes(target_graph.edges[(source, target)], edge_data)\n```\n\n----------------------------------------\n\nTITLE: Project Structure Directory Tree\nDESCRIPTION: Directory structure showing the organization of the repominify package and its components.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nrepominify/\n├── repominify/         # Source code\n│   ├── graph.py        # Graph building and analysis\n│   ├── parser.py       # Repomix file parsing\n│   ├── types.py        # Core types and data structures\n│   ├── exporters.py    # Graph export functionality\n│   ├── formatters.py   # Text representation formatting\n│   ├── dependencies.py # Dependency management\n│   ├── logging.py      # Logging configuration\n│   ├── stats.py        # Statistics and comparison\n│   ├── constants.py    # Shared constants\n│   ├── exceptions.py   # Custom exceptions\n│   ├── cli.py         # Command-line interface\n│   └── __init__.py    # Package initialization\n├── tests/             # Test suite\n│   ├── test_end2end.py # End-to-end tests\n│   └── data/          # Test data files\n├── setup.py          # Package configuration\n├── LICENSE           # MIT License\n└── README.md         # This file\n```\n\n----------------------------------------\n\nTITLE: Initializing GraphRAG API Router for Query Operations\nDESCRIPTION: Sets up a FastAPI router for query operations with a '/query' prefix and appropriate tag.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nquery_route = APIRouter(\n    prefix=\"/query\",\n    tags=[\"Query Operations\"],\n)\n```\n\n----------------------------------------\n\nTITLE: ID Property Implementation with Protection\nDESCRIPTION: Getter and setter methods for the ID property with protection against changes after initialization.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_96\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef id(self) -> str:\n    return self._id\n\n@id.setter\ndef id(self, id: str) -> None:\n    if self._id is not None:\n        self._id = id\n    else:\n        raise ValueError(\"ID cannot be changed once set.\")\n```\n\n----------------------------------------\n\nTITLE: Setting Default Community Level for Local Search in Python\nDESCRIPTION: Sets the default community level to 2 for local search based on current investigations showing it's the most useful level.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# Current investigations show that community level 2 is the most useful for local search. Set this as the default value\nCOMMUNITY_LEVEL = 2\n```\n\n----------------------------------------\n\nTITLE: Human Readable Index Name Property Implementation\nDESCRIPTION: Getter and setter methods for human_readable_index_name that updates the database when changed.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_98\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef human_readable_index_name(self) -> str:\n    return self._human_readable_index_name\n\n@human_readable_index_name.setter\ndef human_readable_index_name(self, human_readable_index_name: str) -> None:\n    self._human_readable_index_name = human_readable_index_name\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Sanitized Storage Name Property Implementation\nDESCRIPTION: Getter and setter methods for sanitized_storage_name that updates the database when changed.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_101\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef sanitized_storage_name(self) -> str:\n    return self._sanitized_storage_name\n\n@sanitized_storage_name.setter\ndef sanitized_storage_name(self, sanitized_storage_name: str) -> None:\n    self._sanitized_storage_name = sanitized_storage_name\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Summarize Descriptions Prompt Property Implementation\nDESCRIPTION: Getter and setter methods for summarize_descriptions_prompt with database update functionality.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_104\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef summarize_descriptions_prompt(self) -> str:\n    return self._summarize_descriptions_prompt\n\n@summarize_descriptions_prompt.setter\ndef summarize_descriptions_prompt(self, summarize_descriptions_prompt: str) -> None:\n    self._summarize_descriptions_prompt = summarize_descriptions_prompt\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: All Workflows Property Implementation\nDESCRIPTION: Getter and setter methods for all_workflows list property with database synchronization.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_105\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef all_workflows(self) -> List[str]:\n    return self._all_workflows\n\n@all_workflows.setter\ndef all_workflows(self, all_workflows: List[str]) -> None:\n    self._all_workflows = all_workflows\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Percent Complete Property Implementation\nDESCRIPTION: Getter and setter methods for percent_complete property that updates the database when changed.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_109\n\nLANGUAGE: Python\nCODE:\n```\n@property\ndef percent_complete(self) -> float:\n    return self._percent_complete\n\n@percent_complete.setter\ndef percent_complete(self, percent_complete: float) -> None:\n    self._percent_complete = percent_complete\n    self.update_db()\n```\n\n----------------------------------------\n\nTITLE: Installing repominify via pip\nDESCRIPTION: Command to install the repominify package using pip package manager.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install repominify\n```\n\n----------------------------------------\n\nTITLE: Empty Common Module - Python\nDESCRIPTION: An empty Python module file, likely used as a placeholder or for future implementation.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/tests/data/graphrag-accelerator.txt#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Copyright (c) Microsoft Corporation.\n\n```\n\n----------------------------------------\n\nTITLE: CLI Usage Examples for repominify\nDESCRIPTION: Command line interface examples showing basic usage, output directory specification, and debug logging.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Basic usage\nrepominify path/to/repomix-output.txt\n\n# Specify output directory\nrepominify path/to/repomix-output.txt -o output_dir\n\n# Enable debug logging\nrepominify path/to/repomix-output.txt --debug\n```\n\n----------------------------------------\n\nTITLE: Development Setup Commands\nDESCRIPTION: Commands for setting up the development environment, including cloning the repository and installing dependencies.\nSOURCE: https://github.com/mikewcasale/repominify/blob/main/README.md#2025-04-21_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/mikewcasale/repominify.git\ncd repominify\n\n# Install in development mode with test dependencies\npip install -e '.[dev]'\n\n# Run tests\npytest tests/\n```"
  }
]
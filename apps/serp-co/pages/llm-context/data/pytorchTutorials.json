[
  {
    "owner": "pytorch",
    "repo": "tutorials",
    "content": "TITLE: SAM2 Export Attempt\nDESCRIPTION: Initial attempt to export SAM2 model's predict method using torch.export.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_export_challenges_solutions.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nep = torch.export.export(\n    self._predict,\n    args=(unnorm_coords, labels, unnorm_box, mask_input, multimask_output),\n    kwargs={\"return_logits\": return_logits},\n    strict=False,\n)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Autograd API Mapping\nDESCRIPTION: A complete reference table showing the equivalent C++ functions for Python autograd operations in PyTorch. Each entry includes links to the official C++ API documentation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_autograd.rst#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ntorch.autograd.backward    -> torch::autograd::backward\ntorch.autograd.grad        -> torch::autograd::grad\ntorch.Tensor.detach        -> torch::Tensor::detach\ntorch.Tensor.detach_       -> torch::Tensor::detach_\ntorch.Tensor.backward      -> torch::Tensor::backward\ntorch.Tensor.register_hook -> torch::Tensor::register_hook\ntorch.Tensor.requires_grad -> torch::Tensor::requires_grad_\ntorch.Tensor.retain_grad   -> torch::Tensor::retain_grad\ntorch.Tensor.grad          -> torch::Tensor::grad\ntorch.Tensor.grad_fn       -> torch::Tensor::grad_fn\ntorch.Tensor.set_data      -> torch::Tensor::set_data\ntorch.Tensor.data          -> torch::Tensor::data\ntorch.Tensor.output_nr     -> torch::Tensor::output_nr\ntorch.Tensor.is_leaf       -> torch::Tensor::is_leaf\n```\n\n----------------------------------------\n\nTITLE: Applying Post Training Static Quantization in PyTorch\nDESCRIPTION: This snippet demonstrates post-training static quantization. It involves setting a quantization backend ('qnnpack' recommended for ARM/mobile, 'x86' for x86 CPUs), assigning a default quantization configuration (`qconfig`), preparing the model for quantization using `torch.quantization.prepare` (which inserts observers), running calibration data through the prepared model (not shown in snippet but required in practice), and finally converting the model to a quantized version using `torch.quantization.convert`. This method quantizes both weights and activations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/quantization.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbackend = \"qnnpack\"\nmodel.qconfig = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\nmodel_static_quantized = torch.quantization.prepare(model, inplace=False)\nmodel_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)\n```\n\n----------------------------------------\n\nTITLE: Utilizing GPU Acceleration\nDESCRIPTION: Methods for using CUDA GPUs with PyTorch tensors and models, including checking for CUDA availability, moving tensors between CPU and GPU, and writing device-agnostic code.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntorch.cuda.is_available                                     # check for cuda\nx = x.cuda()                                                # move x's data from \n                                                            # CPU to GPU and return new object\n\nx = x.cpu()                                                 # move x's data from GPU to CPU \n                                                            # and return new object\n\nif not args.disable_cuda and torch.cuda.is_available():     # device agnostic code \n    args.device = torch.device('cuda')                      # and modularity\nelse:                                                       #\n    args.device = torch.device('cpu')                       #\n\nnet.to(device)                                              # recursively convert their \n                                                            # parameters and buffers to \n                                                            # device specific tensors\n\nx = x.to(device)                                            # copy your tensors to a device \n                                                            # (gpu, cpu)\n```\n\n----------------------------------------\n\nTITLE: Implementing Gradient Retrieval and Parameter RRef Creation in Python\nDESCRIPTION: This snippet adds methods to the ParameterServer class for retrieving distributed gradients and creating RRefs for model parameters. These methods are crucial for distributed optimization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_dist_gradients(self, cid):\n    grads = dist_autograd.get_gradients(cid)\n    # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n    # Tensors must be moved in and out of GPU memory due to this.\n    cpu_grads = {}\n    for k, v in grads.items():\n        k_cpu, v_cpu = k.to(\"cpu\"), v.to(\"cpu\")\n        cpu_grads[k_cpu] = v_cpu\n    return cpu_grads\n\ndef get_param_rrefs(self):\n    param_rrefs = [rpc.RRef(param) for param in self.model.parameters()]\n    return param_rrefs\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained Quantized MobileNet v2 in PyTorch\nDESCRIPTION: This snippet demonstrates how to load a pre-quantized version of the MobileNet v2 model directly from the torchvision library. The `pretrained=True` flag loads weights trained on ImageNet, and `quantize=True` ensures the quantized version of the model is returned.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/quantization.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision\nmodel_quantized = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Map Location and Restoring DDP Model Checkpoint - PyTorch - Python\nDESCRIPTION: Restores a DDP model's state from checkpoint on a specific device, configures device mapping for loading (map_location), sets up the SGD optimizer and MSE loss, performs a forward and backward pass, and steps the optimizer. File deletion is synchronized implicitly via the backward pass. Assumes that the DDP model, optimizer, loss, and supporting functions (cleanup, etc.) have been defined. The CHECKPOINT_PATH constant and rank variable must be set. Inputs are synthetic data tensors for demonstration. No explicit distributed barrier is required after optimizer step due to in-built synchronization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# configure map_location properly\nmap_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\nddp_model.load_state_dict(\n    torch.load(CHECKPOINT_PATH, map_location=map_location, weights_only=True))\n\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\noptimizer.zero_grad()\noutputs = ddp_model(torch.randn(20, 10))\nlabels = torch.randn(20, 5).to(rank)\n\nloss_fn(outputs, labels).backward()\noptimizer.step()\n\n# Not necessary to use a dist.barrier() to guard the file deletion below\n# as the AllReduce ops in the backward pass of DDP already served as\n# a synchronization.\n\nif rank == 0:\n    os.remove(CHECKPOINT_PATH)\n\ncleanup()\nprint(f\"Finished running DDP checkpoint example on rank {rank}.\")\n```\n\n----------------------------------------\n\nTITLE: Running DDP with a Model Parallel Architecture Example - PyTorch - Python\nDESCRIPTION: Demonstrates initialization and training of a DDP-wrapped model-parallel network. Sets up device allocation per process, wraps the custom multi-GPU ToyMpModel in DistributedDataParallel, and walks through optimizer and loss setup, forward and backward passes, and process group cleanup. Assumes a distributed context with known rank and world_size and enough GPUs for parallel use. The outputs and targets are randomly generated. Dependencies: torch, torch.nn, torch.optim, setup, cleanup functions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef demo_model_parallel(rank, world_size):\n    print(f\"Running DDP with model parallel example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n    print(f\"Finished running DDP with model parallel example on rank {rank}.\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Training with ZeroRedundancyOptimizer in PyTorch\nDESCRIPTION: This code demonstrates how to use ZeroRedundancyOptimizer in a distributed training setup. It compares memory usage and parameter values when using ZeroRedundancyOptimizer vs. standard Adam optimizer.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/zero_redundancy_optimizer.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef print_peak_memory(prefix, device):\n    if device == 0:\n        print(f\"{prefix}: {torch.cuda.max_memory_allocated(device) // 1e6}MB \")\n\ndef example(rank, world_size, use_zero):\n    torch.manual_seed(0)\n    torch.cuda.manual_seed(0)\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    # create default process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\n    # create local model\n    model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n    print_peak_memory(\"Max memory allocated after creating local model\", rank)\n\n    # construct DDP model\n    ddp_model = DDP(model, device_ids=[rank])\n    print_peak_memory(\"Max memory allocated after creating DDP\", rank)\n\n    # define loss function and optimizer\n    loss_fn = nn.MSELoss()\n    if use_zero:\n        optimizer = ZeroRedundancyOptimizer(\n            ddp_model.parameters(),\n            optimizer_class=torch.optim.Adam,\n            lr=0.01\n        )\n    else:\n        optimizer = torch.optim.Adam(ddp_model.parameters(), lr=0.01)\n\n    # forward pass\n    outputs = ddp_model(torch.randn(20, 2000).to(rank))\n    labels = torch.randn(20, 2000).to(rank)\n    # backward pass\n    loss_fn(outputs, labels).backward()\n\n    # update parameters\n    print_peak_memory(\"Max memory allocated before optimizer step()\", rank)\n    optimizer.step()\n    print_peak_memory(\"Max memory allocated after optimizer step()\", rank)\n\n    print(f\"params sum is: {sum(model.parameters()).sum()}\")\n\n\n\ndef main():\n    world_size = 2\n    print(\"=== Using ZeroRedundancyOptimizer ===\")\n    mp.spawn(example,\n        args=(world_size, True),\n        nprocs=world_size,\n        join=True)\n\n    print(\"=== Not Using ZeroRedundancyOptimizer ===\")\n    mp.spawn(example,\n        args=(world_size, False),\n        nprocs=world_size,\n        join=True)\n\nif __name__==\"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Managing Tensor Dimensions\nDESCRIPTION: Operations for manipulating tensor dimensions in PyTorch, including reshaping, concatenating, transposing, and adding or removing dimensions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nx.size()                                  # return tuple-like object of dimensions\nx = torch.cat(tensor_seq, dim=0)          # concatenates tensors along dim\ny = x.view(a,b,...)                       # reshapes x into size (a,b,...)\ny = x.view(-1,a)                          # reshapes x into size (b,a) for some b\ny = x.transpose(a,b)                      # swaps dimensions a and b\ny = x.permute(*dims)                      # permutes dimensions\ny = x.unsqueeze(dim)                      # tensor with added axis\ny = x.unsqueeze(dim=2)                    # (a,b,c) tensor -> (a,b,1,c) tensor\ny = x.squeeze()                           # removes all dimensions of size 1 (a,1,b,1) -> (a,b)\ny = x.squeeze(dim=1)                      # removes specified dimension of size 1 (a,1,b,1) -> (a,b,1)\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving Fused and Non-Fused Models\nDESCRIPTION: Code for generating two versions of the model (with and without module fusion), quantizing them, and saving them as optimized TorchScript models for mobile deployment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/fuse.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = AnnotatedConvBnReLUModel()\nprint(model)\n\ndef prepare_save(model, fused):\n    model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n    torch.quantization.prepare(model, inplace=True)\n    torch.quantization.convert(model, inplace=True)\n    torchscript_model = torch.jit.script(model)\n    torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n    torch.jit.save(torchscript_model_optimized, \"model.pt\" if not fused else \"model_fused.pt\")\n\nprepare_save(model, False)\n\nmodel = AnnotatedConvBnReLUModel()\nmodel_fused = torch.quantization.fuse_modules(model, [['bn', 'relu']], inplace=False)\nprint(model_fused)\n\nprepare_save(model_fused, True)\n```\n\n----------------------------------------\n\nTITLE: C++ Inference Implementation\nDESCRIPTION: Complete C++ program for loading and running inference with a TorchScript model. Includes error handling, input preparation, and output processing with softmax and top-5 predictions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torchscript_inference.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/script.h>\n#include <torch/nn/functional/activation.h>\n\n\nint main(int argc, const char* argv[]) {\n    if (argc != 2) {\n        std::cerr << \"usage: ts-infer <path-to-exported-model>\\n\";\n        return -1;\n    }\n\n    std::cout << \"Loading model...\\n\";\n\n    // deserialize ScriptModule\n    torch::jit::script::Module module;\n    try {\n        module = torch::jit::load(argv[1]);\n    } catch (const c10::Error& e) {\n        std::cerr << \"Error loading model\\n\";\n        std::cerr << e.msg_without_backtrace();\n        return -1;\n    }\n\n    std::cout << \"Model loaded successfully\\n\";\n\n    torch::NoGradGuard no_grad; // ensures that autograd is off\n    module.eval(); // turn off dropout and other training-time layers/functions\n\n    // create an input \"image\"\n    std::vector<torch::jit::IValue> inputs;\n    inputs.push_back(torch::rand({1, 3, 224, 224}));\n\n    // execute model and package output as tensor\n    at::Tensor output = module.forward(inputs).toTensor();\n\n    namespace F = torch::nn::functional;\n    at::Tensor output_sm = F::softmax(output, F::SoftmaxFuncOptions(1));\n    std::tuple<at::Tensor, at::Tensor> top5_tensor = output_sm.topk(5);\n    at::Tensor top5 = std::get<1>(top5_tensor);\n\n    std::cout << top5[0] << \"\\n\";\n\n    std::cout << \"\\nDONE\\n\";\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Saving FSDP Model Checkpoints to Rank0 CPU in PyTorch\nDESCRIPTION: This code demonstrates how to save FSDP model checkpoints by streaming to the Rank0 CPU. It uses FullStateDictConfig to populate the state_dict only on rank 0 and offload to CPU, avoiding OOM for large models.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nsave_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\nwith FSDP.state_dict_type(\n            model, StateDictType.FULL_STATE_DICT, save_policy\n        ):\n    cpu_state = model.state_dict()\nif rank == 0:\n save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n torch.save(cpu_state, save_name)\n```\n\n----------------------------------------\n\nTITLE: Debugging Quantized Model Weights and Fusion - PyTorch FX - Python\nDESCRIPTION: Demonstrates how to examine weight values before and after quantization, and after explicit fusion of modules (such as Conv2d and BatchNorm) using fuse_fx. The script prints the difference between the fused and quantized convolutional layer weights. fuse_fx expects the model in eval mode. The code depends on torch, the float_model, and quantized_model objects.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    fused = fuse_fx(float_model)\n\n    conv1_weight_after_fuse = fused.conv1[0].weight[0]\n    conv1_weight_after_quant = quantized_model.conv1.weight().dequantize()[0]\n\n    print(torch.max(abs(conv1_weight_after_fuse - conv1_weight_after_quant)))\n```\n\n----------------------------------------\n\nTITLE: Configuring Optimizers in PyTorch\nDESCRIPTION: Examples of using optimizers in PyTorch, including creation and step updates. Optimizers are used to update the model's parameters during training based on the computed gradients.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nopt = optim.x(model.parameters(), ...)      # create optimizer\nopt.step()                                  # update weights\noptim.X                                     # where X is SGD, Adadelta, Adagrad, Adam, SparseAdam, Adamax, ASGD, LBFGS, RMSProp or Rprop\n```\n\n----------------------------------------\n\nTITLE: Implementing QAT Training Loop with Progressive Observer and BatchNorm Disabling\nDESCRIPTION: Code for a training loop that progressively disables observers and freezes batch normalization statistics after certain epochs. It also includes periodic evaluation of the quantized model during training to monitor progress.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnum_epochs = 10\nnum_train_batches = 20\nnum_eval_batches = 20\nnum_observer_update_epochs = 4\nnum_batch_norm_update_epochs = 3\nnum_epochs_between_evals = 2\n\n# QAT takes time and one needs to train over a few epochs.\n# Train and check accuracy after each epoch\nfor nepoch in range(num_epochs):\n    train_one_epoch(prepared_model, criterion, optimizer, data_loader, \"cuda\", num_train_batches)\n\n    # Optionally disable observer/batchnorm stats after certain number of epochs\n    if epoch >= num_observer_update_epochs:\n        print(\"Disabling observer for subseq epochs, epoch = \", epoch)\n        prepared_model.apply(torch.ao.quantization.disable_observer)\n    if epoch >= num_batch_norm_update_epochs:\n        print(\"Freezing BN for subseq epochs, epoch = \", epoch)\n        for n in prepared_model.graph.nodes:\n            # Args: input, weight, bias, running_mean, running_var, training, momentum, eps\n            # We set the `training` flag to False here to freeze BN stats\n            if n.target in [\n                torch.ops.aten._native_batch_norm_legit.default,\n                torch.ops.aten.cudnn_batch_norm.default,\n            ]:\n                new_args = list(n.args)\n                new_args[5] = False\n                n.args = new_args\n        prepared_model.recompile()\n\n    # Check the quantized accuracy every N epochs\n    # Note: If you wish to just evaluate the QAT model (not the quantized model),\n    # then you can just call `torch.ao.quantization.move_exported_model_to_eval/train`.\n    # However, the latter API is not ready yet and will be available in the near future.\n    if (nepoch + 1) % num_epochs_between_evals == 0:\n        prepared_model_copy = copy.deepcopy(prepared_model)\n        quantized_model = convert_pt2e(prepared_model_copy)\n        top1, top5 = evaluate(quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n        print('Epoch %d: Evaluation accuracy on %d images, %2.2f' % (nepoch, num_eval_batches * eval_batch_size, top1.avg))\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network Layers in PyTorch\nDESCRIPTION: Examples of common neural network layers and components in PyTorch, including fully connected layers, convolutional layers, pooling, normalization, and recurrent layers. These building blocks are used to construct complex neural network architectures.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nnn.Linear(m,n)                                # fully connected layer from m to n units\nnn.ConvXd(m,n,s)                              # X dimensional conv layer from m to n channels where X⍷{1,2,3} and the kernel size is s\nnn.MaxPoolXd(s)                               # X dimension pooling layer (notation as above)\nnn.BatchNorm                                  # batch norm layer\nnn.RNN/LSTM/GRU                               # recurrent layers\nnn.Dropout(p=0.5, inplace=False)              # dropout layer for any dimensional input\nnn.Dropout2d(p=0.5, inplace=False)            # 2-dimensional channel-wise dropout\nnn.Embedding(num_embeddings, embedding_dim)   # (tensor-wise) mapping from indices to embedding vectors\n```\n\n----------------------------------------\n\nTITLE: Defining a Double-Backward-Aware Custom Autograd Function - PyTorch - Python\nDESCRIPTION: Implements a custom Function (Square) allowing higher-order gradients in PyTorch. The forward pass saves the input tensor, while the backward pass computes the gradient. The snippet demonstrates registering the operation for autograd using save_for_backward, and validates both first- and second-order derivatives with gradcheck and gradgradcheck. Dependencies: torch, Double precision tensors are recommended.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass Square(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        # Because we are saving one of the inputs use `save_for_backward`\n        # Save non-tensors and non-inputs/non-outputs directly on ctx\n        ctx.save_for_backward(x)\n        return x**2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        # A function support double backward automatically if autograd\n        # is able to record the computations performed in backward\n        x, = ctx.saved_tensors\n        return grad_out * 2 * x\n\n# Use double precision because finite differencing method magnifies errors\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(Square.apply, x)\n# Use gradcheck to verify second-order derivatives\ntorch.autograd.gradgradcheck(Square.apply, x)\n\n```\n\n----------------------------------------\n\nTITLE: TransformerBlock Forward Pass with Sequence and Residual Connections - Python\nDESCRIPTION: This sample demonstrates the standard forward pass for a TransformerBlock, combining norm layers, attention, feedforward layers, and residual connections. Inputs are processed sequentially with normalization and attention/feed-forward modules, returning the output tensor. This skeleton function models typical data flow in Transformer architectures; dependencies include properly defined submodules (attention, attention_norm, feed_forward, ffn_norm).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# forward in a TransformerBlock\ndef forward(self, x):\n    h = x + self.attention(self.attention_norm(x))\n    out = h + self.feed_forward(self.ffn_norm(h))\n    return out\n```\n\n----------------------------------------\n\nTITLE: Defining Distributed Training Loop with FSDP - PyTorch - Python\nDESCRIPTION: This snippet implements the distributed training function fsdp_main, which sets up datasets, initializes distributed samplers and DataLoaders, configures an FSDP-wrapped T5-base model with transformer auto wrapping and (optionally) mixed precision, and orchestrates training and evaluation loops. It also manages distributed process roles, tracks best validation loss, supports model checkpointing, and gathers runtime statistics. Dependencies include PyTorch, FSDP, relevant T5 and dataset utilities, and environment variables for distributed rank and world size. All hyperparameters are configured via the args object. Inputs are argument values and distributed env vars, with outputs being trained models and performance metrics.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    def fsdp_main(args):\n\n        model, tokenizer = setup_model(\"t5-base\")\n\n        local_rank = int(os.environ['LOCAL_RANK'])\n        rank = int(os.environ['RANK'])\n        world_size = int(os.environ['WORLD_SIZE'])\n\n\n        dataset = load_dataset('wikihow', 'all', data_dir='data/')\n        print(dataset.keys())\n        print(\"Size of train dataset: \", dataset['train'].shape)\n        print(\"Size of Validation dataset: \", dataset['validation'].shape)\n\n\n        #wikihow(tokenizer, type_path, num_samples, input_length, output_length, print_text=False)\n        train_dataset = wikihow(tokenizer, 'train', 1500, 512, 150, False)\n        val_dataset = wikihow(tokenizer, 'validation', 300, 512, 150, False)\n\n        sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)\n        sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size)\n\n        setup()\n\n\n        train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n        test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n        cuda_kwargs = {'num_workers': 2,\n                        'pin_memory': True,\n                        'shuffle': False}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n        train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n        val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)\n\n        t5_auto_wrap_policy = functools.partial(\n            transformer_auto_wrap_policy,\n            transformer_layer_cls={\n                T5Block,\n            },\n        )\n        sharding_strategy: ShardingStrategy = ShardingStrategy.SHARD_GRAD_OP #for Zero2 and FULL_SHARD for Zero3\n        torch.cuda.set_device(local_rank)\n\n\n        #init_start_event = torch.cuda.Event(enable_timing=True)\n        #init_end_event = torch.cuda.Event(enable_timing=True)\n\n        #init_start_event.record()\n\n        bf16_ready = (\n        torch.version.cuda\n        and torch.cuda.is_bf16_supported()\n        and LooseVersion(torch.version.cuda) >= \"11.0\"\n        and dist.is_nccl_available()\n        and nccl.version() >= (2, 10)\n        )\n\n        if bf16_ready:\n            mp_policy = bfSixteen\n        else:\n            mp_policy = None # defaults to fp32\n\n        # model is on CPU before input to FSDP\n        model = FSDP(model,\n            auto_wrap_policy=t5_auto_wrap_policy,\n            mixed_precision=mp_policy,\n            #sharding_strategy=sharding_strategy,\n            device_id=torch.cuda.current_device())\n\n        optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n\n        scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n        best_val_loss = float(\"inf\")\n        curr_val_loss = float(\"inf\")\n        file_save_name = \"T5-model-\"\n\n        if rank == 0:\n            time_of_run = get_date_of_run()\n            dur = []\n            train_acc_tracking = []\n            val_acc_tracking = []\n            training_start_time = time.time()\n\n        if rank == 0 and args.track_memory:\n            mem_alloc_tracker = []\n            mem_reserved_tracker = []\n\n        for epoch in range(1, args.epochs + 1):\n            t0 = time.time()\n            train_accuracy = train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n            if args.run_validation:\n                curr_val_loss = validation(model, rank, world_size, val_loader)\n            scheduler.step()\n\n            if rank == 0:\n\n                print(f\"--> epoch {epoch} completed...entering save and stats zone\")\n\n                dur.append(time.time() - t0)\n                train_acc_tracking.append(train_accuracy.item())\n\n                if args.run_validation:\n                    val_acc_tracking.append(curr_val_loss.item())\n\n                if args.track_memory:\n                    mem_alloc_tracker.append(\n                        format_metrics_to_gb(torch.cuda.memory_allocated())\n                    )\n                    mem_reserved_tracker.append(\n                        format_metrics_to_gb(torch.cuda.memory_reserved())\n                    )\n                print(f\"completed save and stats zone...\")\n\n            if args.save_model and curr_val_loss < best_val_loss:\n\n                # save\n                if rank == 0:\n                    print(f\"--> entering save model state\")\n\n                save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n                with FSDP.state_dict_type(\n                    model, StateDictType.FULL_STATE_DICT, save_policy\n                ):\n                    cpu_state = model.state_dict()\n                #print(f\"saving process: rank {rank}  done w state_dict\")\n\n\n                if rank == 0:\n                    print(f\"--> saving model ...\")\n                    currEpoch = (\n                        \"-\" + str(epoch) + \"-\" + str(round(curr_val_loss.item(), 4)) + \".pt\"\n                    )\n                    print(f\"--> attempting to save model prefix {currEpoch}\")\n                    save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n                    print(f\"--> saving as model name {save_name}\")\n\n                    torch.save(cpu_state, save_name)\n\n            if curr_val_loss < best_val_loss:\n\n                best_val_loss = curr_val_loss\n                if rank==0:\n                    print(f\"-->>>> New Val Loss Record: {best_val_loss}\")\n\n        dist.barrier()\n        cleanup()\n\n```\n\n----------------------------------------\n\nTITLE: Parsing Command-Line Arguments and Setting Main Entry Point - PyTorch (Python)\nDESCRIPTION: Parses command-line arguments for MNIST distributed training and sets up the main entry. It defines options for batch size, epoch count, learning rate, CUDA usage, and model saving, sets the random seed, counts CUDA devices, and spawns the distributed FSDP main processes with torch.multiprocessing.spawn. Dependencies include argparse, torch, and registered fsdp_main function. Inputs are the command-line flags; outputs are spawned processes managing distributed training. Must be run as a script.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == '__main__':\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    WORLD_SIZE = torch.cuda.device_count()\n    mp.spawn(fsdp_main,\n        args=(WORLD_SIZE, args),\n        nprocs=WORLD_SIZE,\n        join=True)\n\n```\n\n----------------------------------------\n\nTITLE: Exporting and Quantizing a Model with PyTorch 2 Export API (Python)\nDESCRIPTION: This snippet demonstrates the full process of preparing a model for post training quantization using PyTorch 2's export API. It starts by defining a simple nn.Module, exports the model with example inputs, and applies quantization using the prepare_pt2e and convert_pt2e functions along with the XNNPACKQuantizer. Dependencies include torch >= 2.5, torch.ao.quantization, and backend-specific quantizer modules. Inputs are an nn.Module and example tensors; outputs are a quantized, export-compatible model. Model calibration step is omitted here.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n  import torch\n  class M(torch.nn.Module):\n     def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n\n     def forward(self, x):\n        return self.linear(x)\n\n\n  example_inputs = (torch.randn(1, 5),)\n  m = M().eval()\n\n  # Step 1. program capture\n  # This is available for pytorch 2.5+, for more details on lower pytorch versions\n  # please check `Export the model with torch.export` section\n  m = torch.export.export_for_training(m, example_inputs).module()\n  # we get a model with aten ops\n\n\n  # Step 2. quantization\n  from torch.ao.quantization.quantize_pt2e import (\n    prepare_pt2e,\n    convert_pt2e,\n  )\n\n  from torch.ao.quantization.quantizer.xnnpack_quantizer import (\n    XNNPACKQuantizer,\n    get_symmetric_quantization_config,\n  )\n  # backend developer will write their own Quantizer and expose methods to allow\n  # users to express how they\n  # want the model to be quantized\n  quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())\n  m = prepare_pt2e(m, quantizer)\n\n  # calibration omitted\n\n  m = convert_pt2e(m)\n  # we have a model with aten ops doing integer computations when possible\n\n```\n\n----------------------------------------\n\nTITLE: Basic Training Loop in PyTorch C++\nDESCRIPTION: Shows the standard training loop implementation for MNIST example in PyTorch C++, including forward pass, loss calculation, backward pass, and optimizer step.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nfor (auto& batch : data_loader) {\n  auto data = batch.data.to(device);\n  auto targets = batch.target.to(device);\n  optimizer.zero_grad();\n  auto output = model.forward(data);\n  auto loss = torch::nll_loss(output, targets);\n  loss.backward();\n  optimizer.step();\n}\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Operators (Including Mutable) in C++ using TORCH_LIBRARY\nDESCRIPTION: This C++ snippet demonstrates registering custom operators within the `extension_cpp` namespace using `TORCH_LIBRARY`. It defines the operator schemas using `m.def`, including the new mutable operator `myadd_out`. Crucially, the schema `myadd_out(Tensor a, Tensor b, Tensor(a!) out) -> ()` uses `Tensor(a!)` to denote that the `out` tensor is mutable (an alias). The `TORCH_LIBRARY_IMPL` block then binds these operator definitions to their respective CPU implementations (`mymuladd_cpu`, `mymul_cpu`, `myadd_out_cpu`) using `m.impl`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY(extension_cpp, m) {\n  m.def(\"mymuladd(Tensor a, Tensor b, float c) -> Tensor\");\n  m.def(\"mymul(Tensor a, Tensor b) -> Tensor\");\n  // New!\n  m.def(\"myadd_out(Tensor a, Tensor b, Tensor(a!) out) -> ()\");\n}\n\nTORCH_LIBRARY_IMPL(extension_cpp, CPU, m) {\n  m.impl(\"mymuladd\", &mymuladd_cpu);\n  m.impl(\"mymul\", &mymul_cpu);\n  // New!\n  m.impl(\"myadd_out\", &myadd_out_cpu);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Optimizers\nDESCRIPTION: Optimizers available in PyTorch's optim module, used for updating model parameters during training. These include common optimization algorithms like SGD, Adam, and RMSprop.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nopt = optim.x(model.parameters(), ...)      # create optimizer\nopt.step()                                  # update weights\nopt.zero_grad()                             # clear the gradients\noptim.X                                     # where X is SGD, AdamW, Adam,\n                                            # Adafactor, NAdam, RAdam, Adadelta,\n                                            # Adagrad, SparseAdam, Adamax, ASGD,\n                                            # LBFGS, RMSprop or Rprop\n```\n\n----------------------------------------\n\nTITLE: Implementing Asynchronous Checkpointing with DCP and FSDP in PyTorch\nDESCRIPTION: This code demonstrates how to set up and run a PyTorch model using FSDP and implement asynchronous checkpointing with DCP. It includes model initialization, training loop, and efficient checkpoint saving using StorageWriter with pinned memory.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_async_checkpoint_recipe.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsetup(rank, world_size)\n\n# create a model and move it to GPU with id rank\nmodel = ToyModel().to(rank)\nmodel = FSDP(model)\n\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n# The storage writer defines our 'staging' strategy, where staging is considered the process of copying\n# checkpoints to in-memory buffers. By setting `cached_state_dict=True`, we enable efficient memory copying\n# into a persistent buffer with pinned memory enabled.\n# Note: It's important that the writer persists in between checkpointing requests, since it maintains the\n# pinned memory buffer.\nwriter = StorageWriter(cached_state_dict=True)\ncheckpoint_future = None\nfor step in range(10):\n    optimizer.zero_grad()\n    model(torch.rand(8, 16, device=\"cuda\")).sum().backward()\n    optimizer.step()\n\n    state_dict = { \"app\": AppState(model, optimizer) }\n    if checkpoint_future is not None:\n        # waits for checkpointing to finish, avoiding queuing more then one checkpoint request at a time\n        checkpoint_future.result()\n    dcp.async_save(state_dict, storage_writer=writer, checkpoint_id=f\"{CHECKPOINT_DIR}_step{step}\")\n\ncleanup()\n\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    print(f\"Running fsdp checkpoint example on {world_size} devices.\")\n    mp.spawn(\n        run_fsdp_checkpoint_save_example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True,\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing All-Reduce Collective Communication in PyTorch\nDESCRIPTION: Demonstrates collective communication using dist.all_reduce(). Creates a group of processes and performs a sum reduction operation across all tensors in the group. Uses dist.ReduceOp.SUM as the reduce operator.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n\"\"\" All-Reduce example.\"\"\"\ndef run(rank, size):\n    \"\"\" Simple collective communication. \"\"\"\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1)\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic DDP Training Loop\nDESCRIPTION: Creates a toy neural network model and demonstrates basic training using DistributedDataParallel, including model creation, loss calculation, and optimization steps.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic(rank, world_size):\n    print(f\"Running basic DDP example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(rank)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n    print(f\"Finished running basic DDP example on rank {rank}.\")\n\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)\n```\n\n----------------------------------------\n\nTITLE: Model Output Verification\nDESCRIPTION: Compares the outputs of original PyTorch model and TorchScript model to ensure they produce identical results. Uses softmax and top-5 predictions for comparison.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torchscript_inference.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nunscripted_output = r18(dummy_input)         # Get the unscripted model's prediction...\nscripted_output = r18_scripted(dummy_input)  # ...and do the same for the scripted version\n\nunscripted_top5 = F.softmax(unscripted_output, dim=1).topk(5).indices\nscripted_top5 = F.softmax(scripted_output, dim=1).topk(5).indices\n```\n\n----------------------------------------\n\nTITLE: Training with Float32 using Intel® Extension for PyTorch*\nDESCRIPTION: This code demonstrates how to optimize a PyTorch ResNet50 model for training with Float32 precision on Intel CPUs. It shows the integration of Intel® Extension for PyTorch* (IPEX) to optimize both the model and optimizer for better performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision\nimport intel_extension_for_pytorch as ipex\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n        root=DATA,\n        train=True,\n        transform=transform,\n        download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer)\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    print(batch_idx)\ntorch.save({\n     'model_state_dict': model.state_dict(),\n     'optimizer_state_dict': optimizer.state_dict(),\n     }, 'checkpoint.pth')\n```\n\n----------------------------------------\n\nTITLE: Implementing LLTM CUDA Forward Function with Type Dispatch and Kernel Launch in C++\nDESCRIPTION: This function wraps the CUDA kernel for the LLTM forward pass. It performs preparation (tensor concatenation, matrix multiplication, buffer allocation), then uses AT_DISPATCH_FLOATING_TYPES to dispatch by tensor data type and launches the kernel. Required dependencies are ATen/Torch C++ APIs and a proper CUDA kernel. Inputs are the LLTM input tensors; outputs are a vector of result and intermediate tensors. The function is key for high performance, handling memory and type correctness.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<torch::Tensor> lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  auto X = torch::cat({old_h, input}, /*dim=*/1);\n  auto gates = torch::addmm(bias, X, weights.transpose(0, 1));\n\n  const auto batch_size = old_cell.size(0);\n  const auto state_size = old_cell.size(1);\n\n  auto new_h = torch::zeros_like(old_cell);\n  auto new_cell = torch::zeros_like(old_cell);\n  auto input_gate = torch::zeros_like(old_cell);\n  auto output_gate = torch::zeros_like(old_cell);\n  auto candidate_cell = torch::zeros_like(old_cell);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&] {\n    lltm_cuda_forward_kernel<scalar_t><<<blocks, threads>>>(\n        gates.data<scalar_t>(),\n        old_cell.data<scalar_t>(),\n        new_h.data<scalar_t>(),\n        new_cell.data<scalar_t>(),\n        input_gate.data<scalar_t>(),\n        output_gate.data<scalar_t>(),\n        candidate_cell.data<scalar_t>(),\n        state_size);\n  }));\n\n  return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates};\n}\n\n```\n\n----------------------------------------\n\nTITLE: Tracing a PyTorch Model to TorchScript using Python\nDESCRIPTION: This snippet demonstrates converting a standard PyTorch model (ResNet18) to a TorchScript module using tracing. Dependencies include PyTorch and torchvision. The model instance and an example input are passed to torch.jit.trace, returning a ScriptModule that can be run and serialized. Inputs are model and a dummy tensor; the output is a traced ScriptModule ready for evaluation and export.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_export.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\nimport torchvision\\n\\n# An instance of your model.\\nmodel = torchvision.models.resnet18()\\n\\n# An example input you would normally provide to your model's forward() method.\\nexample = torch.rand(1, 3, 224, 224)\\n\\n# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\\ntraced_script_module = torch.jit.trace(model, example)\n```\n\nLANGUAGE: python\nCODE:\n```\noutput = traced_script_module(torch.ones(1, 3, 224, 224))\\noutput[0, :5]\\n# Output: tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=<SliceBackward>)\n```\n\n----------------------------------------\n\nTITLE: Implementing Polynomial Fitting with PyTorch nn Module\nDESCRIPTION: A PyTorch implementation using the nn module to define a neural network model for fitting a sine function. This approach utilizes built-in layers and loss functions provided by the nn package.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/pytorch_with_examples.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# For this example, the output y is a linear function of (x, x^2, x^3), so\n# we can consider it as a linear layer neural network. Let's prepare the\n# tensor (x, x^2, x^3).\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n# of shape (2000, 3) \n\n# Use the nn package to define our model as a sequence of layers. nn.Sequential\n# is a Module which contains other Modules, and applies them in sequence to\n# produce its output. The Linear Module computes output from input using a\n# linear function, and holds internal Tensors for its weight and bias.\n# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n# to match the shape of y.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\n\n# The nn package also contains definitions of popular loss functions; in this\n# case we will use Mean Squared Error (MSE) as our loss function.\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-6\nfor t in range(2000):\n\n    # Forward pass: compute predicted y by passing x to the model. Module objects\n    # override the __call__ operator so you can call them like functions. When\n    # doing so you pass a Tensor of input data to the Module and it produces\n    # a Tensor of output data.\n    y_pred = model(xx)\n\n    # Compute and print loss. We pass Tensors containing the predicted and true\n    # values of y, and the loss function returns a Tensor containing the\n    # loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero the gradients before running the backward pass.\n    model.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to all the learnable\n    # parameters of the model. Internally, the parameters of each Module are stored\n    # in Tensors with requires_grad=True, so this call will compute gradients for\n    # all learnable parameters in the model.\n    loss.backward()\n\n    # Update the weights using gradient descent. Each parameter is a Tensor, so\n    # we can access its gradients like we did before.\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad\n\n# You can access the first layer of `model` like accessing the first item of a list\nlinear_layer = model[0]\n\n# For linear layer, its parameters are stored as `weight` and `bias`.\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n```\n\n----------------------------------------\n\nTITLE: Calibrating the Model Using Sample Data - PyTorch - Python\nDESCRIPTION: Defines and executes the calibration process, feeding data through the model in eval mode so observers can collect statistics for quantization determination. The function takes a model and data_loader as inputs, runs inference on the data with gradients disabled, and is called with a prepared model and test set. Requires PyTorch and a data_loader compatible with the model's expected input shape.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    def calibrate(model, data_loader):\n        model.eval()\n        with torch.no_grad():\n            for image, target in data_loader:\n                model(image)\n    calibrate(prepared_model, data_loader_test)  # run calibration on sample data\n```\n\n----------------------------------------\n\nTITLE: Complete Flask Image Classification Web Service - Python\nDESCRIPTION: Aggregates all parts into a full Flask app for accepting image uploads, preprocessing them, running inference via DenseNet, and returning predictions as JSON. Dependencies: PyTorch, TorchVision, PIL, Flask. Loads class index mapping if available. Provides two endpoints: root (GET) for info, and /predict (POST) for image uploads. Expects images as multipart form data under the 'file' key; returns JSON with class_id and class_name.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/deployment_with_flask.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport io\\nimport json\\nimport os\\n\\nimport torchvision.models as models\\nimport torchvision.transforms as transforms\\nfrom PIL import Image\\nfrom flask import Flask, jsonify, request\\n\\n\\napp = Flask(__name__)\\nmodel = models.densenet121(pretrained=True)               # Trained on 1000 classes from ImageNet\\nmodel.eval()                                              # Turns off autograd \\n\\n\\n\\nimg_class_map = None\\nmapping_file_path = 'index_to_name.json'                  # Human-readable names for Imagenet classes\\nif os.path.isfile(mapping_file_path):\\n    with open (mapping_file_path) as f:\\n        img_class_map = json.load(f)\\n\\n\\n\\n# Transform input into the form our model expects\\ndef transform_image(infile):\\n    input_transforms = [transforms.Resize(255),           # We use multiple TorchVision transforms to ready the image\\n        transforms.CenterCrop(224),\\n        transforms.ToTensor(),\\n        transforms.Normalize([0.485, 0.456, 0.406],       # Standard normalization for ImageNet model input\\n            [0.229, 0.224, 0.225])]\\n    my_transforms = transforms.Compose(input_transforms)\\n    image = Image.open(infile)                            # Open the image file\\n    timg = my_transforms(image)                           # Transform PIL image to appropriately-shaped PyTorch tensor\\n    timg.unsqueeze_(0)                                    # PyTorch models expect batched input; create a batch of 1\\n    return timg\\n\\n\\n# Get a prediction\\ndef get_prediction(input_tensor):\\n    outputs = model.forward(input_tensor)                 # Get likelihoods for all ImageNet classes\\n    _, y_hat = outputs.max(1)                             # Extract the most likely class\\n    prediction = y_hat.item()                             # Extract the int value from the PyTorch tensor\\n    return prediction\\n\\n# Make the prediction human-readable\\ndef render_prediction(prediction_idx):\\n    stridx = str(prediction_idx)\\n    class_name = 'Unknown'\\n    if img_class_map is not None:\\n        if stridx in img_class_map is not None:\\n            class_name = img_class_map[stridx][1]\\n\\n    return prediction_idx, class_name\\n\\n\\n@app.route('/', methods=['GET'])\\ndef root():\\n    return jsonify({'msg' : 'Try POSTing to the /predict endpoint with an RGB image attachment'})\\n\\n\\n@app.route('/predict', methods=['POST'])\\ndef predict():\\n    if request.method == 'POST':\\n        file = request.files['file']\\n        if file is not None:\\n            input_tensor = transform_image(file)\\n            prediction_idx = get_prediction(input_tensor)\\n            class_id, class_name = render_prediction(prediction_idx)\\n            return jsonify({'class_id': class_id, 'class_name': class_name})\\n\\n\\nif __name__ == '__main__':\\n    app.run()\n```\n\n----------------------------------------\n\nTITLE: Enabling CPU Offload in FSDP for Large Models - PyTorch (Python)\nDESCRIPTION: Shows how to enable parameter and gradient CPU offloading in FSDP for large models that exceed GPU memory. Adds the cpu_offload=CPUOffload(offload_params=True) argument to the FSDP wrapper with optional auto_wrap_policy. dependencies include torch and CPUOffload. Input arguments are the model and wrapping policy; output is an FSDP model with CPU offload enabled. Note this can slow down training due to increased data transfer between host and device.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmodel = FSDP(model,\n    auto_wrap_policy=my_auto_wrap_policy,\n    cpu_offload=CPUOffload(offload_params=True))\n```\n\n----------------------------------------\n\nTITLE: Training Loop Implementation\nDESCRIPTION: Implements the main training loop using distributed autograd for backward passes and distributed optimizer for parameter updates. Handles both DDP gradient synchronization and RPC-based parameter updates.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/rpc_ddp_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _run_trainer(rank, model, opt, criterion):\n    for epoch in range(NUM_EPOCHS):\n        for i, (data, target) in enumerate(get_next_batch()):\n            with dist_autograd.context() as context_id:\n                output = model(data, target)\n                loss = criterion(output, target)\n                dist_autograd.backward(context_id, [loss])\n                opt.step(context_id)\n                if i % 100 == 0:\n                    print(f\"Trainer rank {rank} epoch {epoch} iter {i}: loss {loss.item()}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Linear Model with CUDA - PyTorch - Python\nDESCRIPTION: This snippet initializes a sequential model of ten linear layers with 1024 input and output features on a CUDA device using PyTorch. It creates an input tensor, feeds it through the model, and computes gradients by invoking backward propagation. Dependencies include PyTorch 2.2+ and a CUDA-capable GPU. Inputs are randomly generated and outputs are used for gradient calculation. This establishes the groundwork for subsequent optimizer benchmarking.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/compiling_optimizer.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\n\\nmodel = torch.nn.Sequential(\\n    *[torch.nn.Linear(1024, 1024, False, device=\\\"cuda\\\") for _ in range(10)]\\n)\\ninput = torch.rand(1024, device=\\\"cuda\\\")\\noutput = model(input)\\noutput.sum().backward()\n```\n\n----------------------------------------\n\nTITLE: Tracing a BERT Model with TorchScript for Quantization (Python)\nDESCRIPTION: This Python code demonstrates preparing dummy input tensors and tracing a BERT model using torch.jit.trace for use in graph mode quantization. Inputs are randomized IDs of shape [8, 128] for input_ids, attention_mask, and token_type_ids, reflecting the largest expected batch and sequence length. The output is a TorchScript traced model. It assumes that 'model' and the helper 'ids_tensor' function are defined.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninput_ids = ids_tensor([8, 128], 2)\ntoken_type_ids = ids_tensor([8, 128], 2)\nattention_mask = ids_tensor([8, 128], vocab_size=2)\ndummy_input = (input_ids, attention_mask, token_type_ids)\ntraced_model = torch.jit.trace(model, dummy_input)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Operator in Eager PyTorch\nDESCRIPTION: Python code demonstrating how to use the custom operator in eager mode PyTorch, including importing the operator and applying it to tensors.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../advanced_source/torch_script_custom_ops/test.py\n  :language: python\n  :prepend: import torch\n  :start-after: BEGIN preamble\n  :end-before: END preamble\n```\n\n----------------------------------------\n\nTITLE: Preparing Model for Static Quantization - PyTorch FX - Python\nDESCRIPTION: Prepares a PyTorch model for static quantization using FX by folding BatchNorm layers and inserting observers, resulting in a new graph module. It requires model_to_quantize, a qconfig_mapping, and sample example_inputs. Output is the prepared model ready for calibration. Dependencies are PyTorch quantization.fx utilities and a model in eval mode.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n```\n\n----------------------------------------\n\nTITLE: Predicting Patches from Whole-Slide Images using TIAToolbox\nDESCRIPTION: Uses a predictor to analyze patches from a whole-slide image. The code configures the predictor with appropriate settings for WSI analysis including GPU usage, mask handling, and prediction merging options.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwith suppress_console_output():\n    wsi_output = predictor.predict(\n        imgs=[wsi_path],\n        masks=None,\n        mode=\"wsi\",\n        merge_predictions=False,\n        ioconfig=wsi_ioconfig,\n        return_probabilities=True,\n        save_dir=global_save_dir / \"wsi_predictions\",\n        on_gpu=ON_GPU,\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing GAN Training Loop in PyTorch C++\nDESCRIPTION: Main training loop that handles training both discriminator and generator networks, including loss calculation and optimization steps.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\nfor (int64_t epoch = 1; epoch <= kNumberOfEpochs; ++epoch) {\n  int64_t batch_index = 0;\n  for (torch::data::Example<>& batch : *data_loader) {\n    // Train discriminator with real images.\n    discriminator->zero_grad();\n    torch::Tensor real_images = batch.data;\n    torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0);\n    torch::Tensor real_output = discriminator->forward(real_images).reshape(real_labels.sizes());\n    torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels);\n    d_loss_real.backward();\n\n    // Train discriminator with fake images.\n    torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1});\n    torch::Tensor fake_images = generator->forward(noise);\n    torch::Tensor fake_labels = torch::zeros(batch.data.size(0));\n    torch::Tensor fake_output = discriminator->forward(fake_images.detach()).reshape(fake_labels.sizes());\n    torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels);\n    d_loss_fake.backward();\n\n    torch::Tensor d_loss = d_loss_real + d_loss_fake;\n    discriminator_optimizer.step();\n\n    // Train generator.\n    generator->zero_grad();\n    fake_labels.fill_(1);\n    fake_output = discriminator->forward(fake_images).reshape(fake_labels.sizes());\n    torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels);\n    g_loss.backward();\n    generator_optimizer.step();\n\n    std::printf(\n        \"\\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f\",\n        epoch,\n        kNumberOfEpochs,\n        ++batch_index,\n        batches_per_epoch,\n        d_loss.item<float>(),\n        g_loss.item<float>());\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using torch.cpu.amp.autocast for AMX Acceleration in Python\nDESCRIPTION: Demonstrates leveraging AMX for BFloat16 operations using the `torch.cpu.amp.autocast()` context manager in PyTorch. The code snippet shows applying automatic mixed precision to a model's forward pass on the CPU. It also includes converting the model to `torch.channels_last` memory format, which is recommended for better performance when using AMX.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/amx.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = model.to(memory_format=torch.channels_last)\nwith torch.cpu.amp.autocast():\n   output = model(input)\n```\n\n----------------------------------------\n\nTITLE: Executing the Distributed Training Loop with DistAutograd in PyTorch (Python)\nDESCRIPTION: Implements the main training iteration within the `run_training_loop` function (partially shown with ellipsis). It iterates over the `train_loader`. Crucially, it uses a `dist_autograd.context()` manager to capture the RPC calls made during the forward pass (`net(data)`). After calculating the loss (`F.nll_loss`), it triggers the distributed backward pass using `dist_autograd.backward(cid, [loss])`. An assertion checks if gradients were received by the parameter server (via another RPC call). Finally, `opt.step(cid)` performs the optimizer step using the gradients aggregated across workers for that specific context ID (`cid`). After the loop, it calls `get_accuracy`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef run_training_loop(rank, num_gpus, train_loader, test_loader):\n...\n    for i, (data, target) in enumerate(train_loader):\n        with dist_autograd.context() as cid:\n            model_output = net(data)\n            target = target.to(model_output.device)\n            loss = F.nll_loss(model_output, target)\n            if i % 5 == 0:\n                print(f\"Rank {rank} training batch {i} loss {loss.item()}\")\n            dist_autograd.backward(cid, [loss])\n            # Ensure that dist autograd ran successfully and gradients were\n            # returned.\n            assert remote_method(\n                ParameterServer.get_dist_gradients,\n                net.param_server_rref,\n                cid) != {}\n            opt.step(cid)\n\n     print(\"Training complete!\")\n     print(\"Getting accuracy....\")\n     get_accuracy(test_loader, net)\n```\n\n----------------------------------------\n\nTITLE: Applying FX Graph Mode Static Quantization API in PyTorch\nDESCRIPTION: This snippet provides a high-level overview of the FX Graph Mode static quantization API. It imports necessary modules, gets a default quantization configuration ('x86'), prepares the model for quantization using `prepare_fx` (which requires the float model, a qconfig mapping, and example inputs), defines a calibration function, runs calibration, and finally converts the prepared model to a quantized model using `convert_fx`. This demonstrates the automated nature of FX graph mode quantization compared to eager mode.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.ao.quantization import get_default_qconfig\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\nfrom torch.ao.quantization import QConfigMapping\nfloat_model.eval()\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqconfig = get_default_qconfig(\"x86\") \nqconfig_mapping = QConfigMapping().set_global(qconfig)\ndef calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)\nexample_inputs = (next(iter(data_loader))[0]) # get an example input\nprepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)  # fuse modules and insert observers\ncalibrate(prepared_model, data_loader_test)  # run calibration on sample data\nquantized_model = convert_fx(prepared_model)  # convert the calibrated model to a quantized model\n```\n\n----------------------------------------\n\nTITLE: Creating TorchScript-Compatible Functional QHM Optimizer Class\nDESCRIPTION: Implements a TorchScript-compatible functional optimizer class for QHM that manages optimizer states and calls the update function. This class is annotated with @torch.jit.script to enable TorchScript compilation and uses TorchScript-friendly patterns.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_optim_torchscript.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import Tensor\nfrom typing import List, Optional, Dict\n\n# define this as a TorchScript class\n@torch.jit.script\nclass FunctionalQHM(object):\n    def __init__(self,\n                params: List[Tensor],\n                lr: float,\n                momentum: float,\n                nu: float,\n                weight_decay: float = 0.0,\n                weight_decay_type: str = \"grad\"):\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if momentum < 0.0:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if weight_decay_type not in (\"grad\", \"direct\"):\n            raise ValueError(\"Invalid weight_decay_type value: {}\".format(weight_decay_type))\n\n        self.defaults = {\n            \"lr\": lr,\n            \"momentum\": momentum,\n            \"nu\": nu,\n            \"weight_decay\": weight_decay,\n        }\n        self.weight_decay_type = weight_decay_type\n\n        # NOTE: we only have one param_group here and don't allow user to add additional\n        # param group as it's not a common use case.\n        self.param_group = {\"params\": params}\n\n        self.state = torch.jit.annotate(Dict[torch.Tensor, Dict[str, torch.Tensor]], {})\n\n    def step(self, gradients: List[Optional[Tensor]]):\n        params = self.param_group['params']\n        params_with_grad = []\n        grads = []\n        momentum_buffer_list: List[Tensor] = []\n\n        if len(params) != len(gradients):\n            raise ValueError(\n                \"the gradients passed in does not equal to the size of the parameters!\"\n                + f\"Params length: {len(params)}. \"\n                + f\"Gradients length: {len(gradients)}\"\n            )\n\n        for param, gradient in zip(self.param_group['params'], gradients):\n            if gradient is not None:\n                params_with_grad.append(param)\n                grads.append(gradient)\n                state = self.state[param]\n                state['momentum_buffer'] = torch.zeros_like(param, memory_format=torch.preserve_format)\n                momentum_buffer_list.append(state['momentum_buffer'])\n\n        # calls into the update function we just defined\n        with torch.no_grad():\n            qhm_update(params_with_grad,\n                    grads,\n                    momentum_buffer_list,\n                    self.defaults['lr'],\n                    self.defaults['nu'],\n                    self.defaults['weight_decay'],\n                    self.weight_decay_type,\n                    self.defaults['momentum'])\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Neural Network with Control Flow in PyTorch\nDESCRIPTION: A PyTorch implementation that demonstrates control flow and weight sharing in neural networks. This example creates a model that dynamically chooses between 3rd, 4th, or 5th order polynomials during each forward pass.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/pytorch_with_examples.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# -*- coding: utf-8 -*-\nimport random\nimport torch\nimport math\n\n\nclass DynamicNet(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate five parameters and assign them as members.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n        self.e = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        For the forward pass of the model, we randomly choose either 4, 5\n        and reuse the e parameter to compute the contribution of these orders.\n\n        Since each forward pass builds a dynamic computation graph, we can use normal\n        Python control-flow operators like loops or conditional statements when\n        defining the forward pass of the model.\n\n        Here we also see that it is perfectly safe to reuse the same parameter many\n        times when defining a computational graph.\n        \"\"\"\n        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n        for exp in range(4, random.randint(4, 6)):\n            y = y + self.e * x ** exp\n        return y\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Construct our model by instantiating the class defined above\nmodel = DynamicNet()\n\n# Construct our loss function and an Optimizer. Training this strange model with\n# vanilla stochastic gradient descent is tough, so we use momentum\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\nfor t in range(30000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 2000 == 1999:\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')\n```\n\n----------------------------------------\n\nTITLE: CUDA Kernel Implementation\nDESCRIPTION: CUDA kernel implementation of the multiply-add operation for GPU execution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_5\n\nLANGUAGE: cuda\nCODE:\n```\n__global__ void muladd_kernel(int numel, const float* a, const float* b, float c, float* result) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < numel) result[idx] = a[idx] * b[idx] + c;\n}\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing MNIST Dataset in PyTorch C++\nDESCRIPTION: Demonstrates loading the MNIST dataset using the PyTorch C++ data API (`torch::data::datasets::MNIST`). It applies normalization (`torch::data::transforms::Normalize<>`) to scale pixel values to the range [-1, 1] and uses the `Stack` collation (`torch::data::transforms::Stack<>`) to combine individual image tensors into a single batch tensor via the `.map()` method. Requires the MNIST dataset files to be present in the `./mnist` directory relative to the execution path.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n  auto dataset = torch::data::datasets::MNIST(\"./mnist\")\n      .map(torch::data::transforms::Normalize<>(0.5, 0.5))\n      .map(torch::data::transforms::Stack<>());\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained BERT Tokenizer and Model in PyTorch\nDESCRIPTION: Loads a `BertTokenizer` and a `BertForSequenceClassification` model from the directory specified in `configs.output_dir`. The tokenizer's case handling is set by `configs.do_lower_case`. The loaded model is then moved to the compute device specified by `configs.device`. This prepares the model and tokenizer for subsequent steps like evaluation or quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = BertTokenizer.from_pretrained(\n    configs.output_dir, do_lower_case=configs.do_lower_case)\n\nmodel = BertForSequenceClassification.from_pretrained(configs.output_dir)\nmodel.to(configs.device)\n```\n\n----------------------------------------\n\nTITLE: Applying Mixed Precision Policy to FSDP Model in PyTorch\nDESCRIPTION: This code demonstrates how to apply a mixed precision policy (bfloat16 in this case) to an FSDP-wrapped model. It also includes an auto-wrap policy for efficient model sharding.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = FSDP(model,\n       auto_wrap_policy=t5_auto_wrap_policy,\n       mixed_precision=bfSixteen)\n```\n\n----------------------------------------\n\nTITLE: Applying Activation Functions in PyTorch\nDESCRIPTION: A list of available activation functions in PyTorch's nn module. These non-linear functions are crucial for introducing non-linearity into neural networks, allowing them to learn complex patterns.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnn.X                                  # where X is ReLU, ReLU6, ELU, SELU, PReLU, LeakyReLU, Threshold, HardTanh, Sigmoid, Tanh, LogSigmoid, Softplus, SoftShrink, Softsign, TanhShrink, Softmin, Softmax, Softmax2d or LogSoftmax\n```\n\n----------------------------------------\n\nTITLE: Checking BFloat16 Support for Mixed Precision in FSDP - PyTorch - Python\nDESCRIPTION: This snippet checks at runtime if BFloat16 mixed precision training is natively supported on the current system's CUDA and NCCL versions for FSDP. It combines checks for CUDA version, torch.cuda capabilities, and NCCL library, returning a boolean bf16_ready. Requires torch, torch.distributed, and nccl libraries. Input is the current hardware/software state, output is a flag for BFloat16 support.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    bf16_ready = (\n        torch.version.cuda\n        and torch.cuda.is_bf16_supported()\n        and LooseVersion(torch.version.cuda) >= \"11.0\"\n        and dist.is_nccl_available()\n        and nccl.version() >= (2, 10)\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Using Auto-Generated Methods/Properties for Custom PyTorch Backend (Python)\nDESCRIPTION: Shows examples of using the methods and properties automatically generated by `generate_methods_for_privateuse1_backend` for a custom backend named 'npu'. This includes checking if a tensor/storage is on the NPU device (`is_npu` property) and moving tensors/storage to the NPU (`.npu()` method), simplifying device-specific operations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntorch.Tensor.npu()\ntorch.Tensor.is_npu\ntorch.Storage.npu()\ntorch.Storage.is_npu\n...\n```\n\n----------------------------------------\n\nTITLE: Defining Quant/DeQuant Stubs for Quantization Aware Training in PyTorch\nDESCRIPTION: These lines define `QuantStub` and `DeQuantStub` modules, typically placed within the `__init__` method of a custom model definition. `QuantStub` is used at the beginning of the forward pass to convert input floating-point tensors to quantized tensors, and `DeQuantStub` is used at the end to convert quantized output tensors back to floating-point. These stubs are necessary markers for enabling Quantization Aware Training (QAT).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/quantization.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nself.quant = torch.quantization.QuantStub()\nself.dequant = torch.quantization.DeQuantStub()\n```\n\n----------------------------------------\n\nTITLE: Creating Auto Wrapping Policy for Transformers in FSDP - PyTorch - Python\nDESCRIPTION: This snippet defines an auto_wrap_policy for FSDP using functools.partial and transformer_auto_wrap_policy, specifying T5Block as the transformer layer class. The policy is set before moving the model to the appropriate CUDA device, then wraps the model in FSDP for automatic sharding. It assumes FSDP and the required layer classes are imported. Inputs are the model and layer type; outputs are a wrapped model instance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    t5_auto_wrap_policy = functools.partial(\n            transformer_auto_wrap_policy,\n            transformer_layer_cls={\n                T5Block,\n            },\n        )\n    torch.cuda.set_device(local_rank)\n\n\n    model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy)\n\n```\n\n----------------------------------------\n\nTITLE: Launching Multiple DDP Training Example Variants Based on GPU Count - PyTorch - Python\nDESCRIPTION: Runs multiple DDP demonstrations conditionally based on GPU count, including basic, checkpoint, and model-parallel examples. The script checks for at least two GPUs, computes world_size, and uses run_demo to invoke demo_basic, demo_checkpoint, and demo_model_parallel with appropriate process counts. Intended for use as the script's main entry point. Dependencies: torch, demo_basic, demo_checkpoint, demo_model_parallel, run_demo.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    n_gpus = torch.cuda.device_count()\n    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n    world_size = n_gpus\n    run_demo(demo_basic, world_size)\n    run_demo(demo_checkpoint, world_size)\n    world_size = n_gpus//2\n    run_demo(demo_model_parallel, world_size)\n```\n\n----------------------------------------\n\nTITLE: TorchScript Inference with BERT on GPU using IPEX - Float32 (Python)\nDESCRIPTION: This snippet shows how to perform TorchScript inference with a BERT model on GPU using IPEX with Float32 precision. It includes model loading, data preparation, IPEX optimization, and TorchScript tracing for improved performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import BertModel\nimport intel_extension_for_pytorch as ipex\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.float32)\n\nwith torch.no_grad():\n  d = torch.randint(vocab_size, size=[batch_size, seq_length])\n  d = d.to(\"xpu\")\n  model = torch.jit.trace(model, (d,), check_trace=False, strict=False)\n  model = torch.jit.freeze(model)\n\n  model(data)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Training Loop with PyTorch RPC\nDESCRIPTION: Demonstrates the training loop for distributed model parallel training, using distributed autograd context and distributed optimizer. Shows how to run forward and backward passes across machines and update parameters in a distributed setting.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef run_trainer():\n    batch = 5\n    ntoken = 10\n    ninp = 2\n\n    nhid = 3\n    nindices = 3\n    nlayers = 4\n    hidden = (\n        torch.randn(nlayers, nindices, nhid),\n        torch.randn(nlayers, nindices, nhid)\n    )\n\n    model = rnn.RNNModel('ps', ntoken, ninp, nhid, nlayers)\n\n    # setup distributed optimizer\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model.parameter_rrefs(),\n        lr=0.05,\n    )\n\n    criterion = torch.nn.CrossEntropyLoss()\n\n    def get_next_batch():\n        for _ in range(5):\n            data = torch.LongTensor(batch, nindices) % ntoken\n            target = torch.LongTensor(batch, ntoken) % nindices\n            yield data, target\n\n    # train for 10 iterations\n    for epoch in range(10):\n        for data, target in get_next_batch():\n            # create distributed autograd context\n            with dist_autograd.context() as context_id:\n                hidden[0].detach_()\n                hidden[1].detach_()\n                output, hidden = model(data, hidden)\n                loss = criterion(output, target)\n                # run distributed backward pass\n                dist_autograd.backward(context_id, [loss])\n                # run distributed optimizer\n                opt.step(context_id)\n                # not necessary to zero grads since they are\n                # accumulated into the distributed autograd context\n                # which is reset every iteration.\n        print(\"Training epoch {}\".format(epoch))\n```\n\n----------------------------------------\n\nTITLE: Configuring Distributed DataLoader\nDESCRIPTION: Configuration of DataLoader with DistributedSampler to properly distribute data across multiple GPUs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ddp_series_multigpu.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=32,\n    shuffle=False,  # We don't shuffle\n    sampler=DistributedSampler(train_dataset), # Use the Distributed Sampler here.\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing and Converting a PyTorch FX Model with Custom BackendConfig - Python\nDESCRIPTION: This snippet demonstrates the process of quantizing a PyTorch model using FX graph mode with a custom backend configuration and QConfig mapping. It covers instantiating the model, generating example input tensors, preparing the model for quantization (inserting observer patterns), running calibration, and converting the prepared model to a quantized form. Dependencies include torch, an appropriate MyModel definition, and quantization-related functions such as prepare_fx and convert_fx. The key parameters are the QConfig mapping, backend_config, and input tensor shape/type; the output is a fully quantized model where compatible layers are quantized as per backend constraints.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nexample_inputs = (torch.rand(1, 3, 10, 10, dtype=torch.float),)\nmodel = MyModel(use_bn=True)\nprepared = prepare_fx(model, qconfig_mapping, example_inputs, backend_config=backend_config)\nprepared(*example_inputs)  # calibrate\nconverted = convert_fx(prepared, backend_config=backend_config)\n```\n\n----------------------------------------\n\nTITLE: Accelerating nn.Linear Layers with Semi-Structured Sparsity - PyTorch - Python\nDESCRIPTION: This code snippet demonstrates how to prune nn.Linear layer weights to 2:4 semi-structured sparsity and accelerate inference using PyTorch's to_sparse_semi_structured and SparseSemiStructuredTensor. It requires PyTorch 2.1+, an NVIDIA GPU with semi-structured sparsity support (Compute Capability 8.0+), and proper CUDA setup. Key steps include constructing a binary mask for pruning, replacing dense weights, and benchmarking speedup using torch.utils.benchmark. The output includes a performance comparison, and asserts numerical equivalence between dense and sparse output with a specified tolerance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor\nfrom torch.utils.benchmark import Timer\nSparseSemiStructuredTensor._FORCE_CUTLASS = True\n\n# mask Linear weight to be 2:4 sparse\nmask = torch.Tensor([0, 0, 1, 1]).tile((3072, 2560)).cuda().bool()\nlinear = torch.nn.Linear(10240, 3072).half().cuda().eval()\nlinear.weight = torch.nn.Parameter(mask * linear.weight)\n\nx = torch.rand(3072, 10240).half().cuda()\n\nwith torch.inference_mode():\n    dense_output = linear(x)\n    dense_t = Timer(stmt=\"linear(x)\",\n                    globals={\"linear\": linear,\n                             \"x\": x}).blocked_autorange().median * 1e3\n\n    # accelerate via SparseSemiStructuredTensor\n    linear.weight = torch.nn.Parameter(to_sparse_semi_structured(linear.weight))\n\n    sparse_output = linear(x)\n    sparse_t = Timer(stmt=\"linear(x)\",\n                    globals={\"linear\": linear,\n                             \"x\": x}).blocked_autorange().median * 1e3\n\n    # sparse and dense matmul are numerically equivalent\n    assert torch.allclose(sparse_output, dense_output, atol=1e-3)\n    print(f\"Dense: {dense_t:.3f}ms Sparse: {sparse_t:.3f}ms | Speedup: {(dense_t / sparse_t):.3f}x\")\n\n```\n\n----------------------------------------\n\nTITLE: Transforming Uploaded Image to PyTorch Input Tensor - Python\nDESCRIPTION: Defines a function to transform an uploaded image file into a normalized PyTorch tensor suitable for DenseNet input (shape (1, 3, 224, 224)). Requires TorchVision and PIL. Key parameters include resizing, center cropping, conversion to tensor, and normalization using ImageNet's mean and std. Expects a file-like object and outputs a 4D tensor batch for model inference.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/deployment_with_flask.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef transform_image(infile):\\n    input_transforms = [transforms.Resize(255),\\n        transforms.CenterCrop(224),\\n        transforms.ToTensor(),\\n        transforms.Normalize([0.485, 0.456, 0.406],\\n            [0.229, 0.224, 0.225])]\\n    my_transforms = transforms.Compose(input_transforms)\\n    image = Image.open(infile)\\n    timg = my_transforms(image)\\n    timg.unsqueeze_(0)\\n    return timg\n```\n\n----------------------------------------\n\nTITLE: Exporting Whisper-Tiny ASR Model with Non-Strict Tracing\nDESCRIPTION: This snippet demonstrates the solution to export the Whisper-Tiny model using torch.export with non-strict tracing. It allows successful export by using Python interpreter-based tracing instead of TorchDynamo.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_export_challenges_solutions.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\n# load model\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n\n# dummy inputs for exporting the model\ninput_features = torch.randn(1,80, 3000)\nattention_mask = torch.ones(1, 3000)\ndecoder_input_ids = torch.tensor([[1, 1, 1 , 1]]) * model.config.decoder_start_token_id\n\nmodel.eval()\n\nexported_program: torch.export.ExportedProgram= torch.export.export(model, args=(input_features, attention_mask, decoder_input_ids,), strict=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLTM CUDA Backward Function in C++\nDESCRIPTION: This function prepares the data and launches the CUDA kernel for the LLTM backward pass. It computes gradients for weights, biases, and input tensors using the results from the backward kernel.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_21\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<torch::Tensor> lltm_cuda_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gates,\n    torch::Tensor weights) {\n  auto d_old_cell = torch::zeros_like(new_cell);\n  auto d_gates = torch::zeros_like(gates);\n\n  const auto batch_size = new_cell.size(0);\n  const auto state_size = new_cell.size(1);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(X.type(), \"lltm_backward_cuda\", ([&] {\n    lltm_cuda_backward_kernel<scalar_t><<<blocks, threads>>>(\n        d_old_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        d_gates.packed_accessor32<scalar_t,3,torch::RestrictPtrTraits>(),\n        grad_h.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        grad_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        new_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        input_gate.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        output_gate.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        candidate_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        gates.packed_accessor32<scalar_t,3,torch::RestrictPtrTraits>());\n  }));\n\n  auto d_gate_weights = d_gates.reshape({batch_size, 3*state_size});\n  auto d_weights = d_gate_weights.t().mm(X);\n  auto d_bias = d_gate_weights.sum(/*dim=*/0, /*keepdim=*/true);\n\n  auto d_X = d_gate_weights.mm(weights);\n  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);\n  auto d_input = d_X.slice(/*dim=*/1, state_size);\n\n  return {d_old_h, d_input, d_weights, d_bias, d_old_cell, d_gates};\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Distributed Process Groups in PyTorch\nDESCRIPTION: Initializes the distributed training environment by setting up process groups with the Gloo backend. Includes environment configuration and cleanup functions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n```\n\n----------------------------------------\n\nTITLE: Initializing Agent with Policy, Optimizer, and Observer RRefs in Python\nDESCRIPTION: Defines the constructor for the `Agent` class. It creates remote references (`RRef`) to multiple `Observer` instances using `rpc.remote`. It initializes the `Policy` network (placing it on CUDA), an Adam optimizer (assuming `optim` is imported, e.g., `import torch.optim as optim`), and data structures to store rewards, states from observers (`self.states`), and saved log probabilities (`self.saved_log_probs`, structure depends on `batch` mode). Crucially, it sets up a `threading.Lock` and a `torch.futures.Future` (`self.future_actions`) along with a counter (`self.pending_states`) to manage asynchronous state collection and batched action selection when `batch` mode is enabled. Assumes `OBSERVER_NAME` format string and `world_size` are defined.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport threading\nfrom torch.distributed.rpc import RRef\nimport torch.optim as optim # Added missing import based on usage\nimport torch # Added missing import for torch.zeros/torch.futures\n\n# Assumed constants/variables (need definition elsewhere)\n# OBSERVER_NAME = \"observer_{}\"\n# NUM_STEPS = 500 \n\nclass Agent:\n    def __init__(self, world_size, batch=True):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.policy = Policy(batch).cuda() # Assumes CUDA is available\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.running_reward = 0\n\n        # Assumes OBSERVER_NAME is defined like \"observer_{}\"\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(rpc.remote(ob_info, Observer, args=(batch,)))\n            self.rewards[ob_info.id] = []\n\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n        self.batch = batch\n        self.saved_log_probs = [] if batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.future_actions = torch.futures.Future()\n        self.lock = threading.Lock()\n        self.pending_states = len(self.ob_rrefs)\n```\n\n----------------------------------------\n\nTITLE: Training with BFloat16 using Intel® Extension for PyTorch*\nDESCRIPTION: This code demonstrates how to optimize a PyTorch ResNet50 model for training with BFloat16 mixed precision on Intel CPUs. It integrates Intel® Extension for PyTorch* (IPEX) to optimize both the model and optimizer, and uses torch.cpu.amp for automatic mixed precision.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision\nimport intel_extension_for_pytorch as ipex\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n        root=DATA,\n        train=True,\n        transform=transform,\n        download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    with torch.cpu.amp.autocast():\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n    optimizer.step()\n    print(batch_idx)\ntorch.save({\n     'model_state_dict': model.state_dict(),\n     'optimizer_state_dict': optimizer.state_dict(),\n     }, 'checkpoint.pth')\n```\n\n----------------------------------------\n\nTITLE: Executing the Quantization-Aware Training Loop in PyTorch\nDESCRIPTION: This Python code implements the main loop for Quantization-Aware Training (QAT) in PyTorch over 8 epochs. It calls the `train_one_epoch` function, and progressively freezes quantizer parameters (after epoch 3 using `disable_observer`) and batch normalization statistics (after epoch 2 using `freeze_bn_stats`) to stabilize training and mimic inference conditions. After each epoch, it converts the QAT model to a quantized model using `torch.ao.quantization.convert` and evaluates its performance using an `evaluate` function.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnum_train_batches = 20  \n\n# QAT takes time and one needs to train over a few epochs.\n# Train and check accuracy after each epoch \nfor nepoch in range(8): \n    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches) \n    if nepoch > 3:  \n        # Freeze quantizer parameters \n        qat_model.apply(torch.ao.quantization.disable_observer)\n    if nepoch > 2:  \n        # Freeze batch norm mean and variance estimates \n        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats) \n\n    # Check the accuracy after each epoch \n    quantized_model = torch.ao.quantization.convert(qat_model.eval(), inplace=False)\n    quantized_model.eval()  \n    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)  \n    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))\n```\n\n----------------------------------------\n\nTITLE: Applying TorchScript JIT Compilation to Model (Python)\nDESCRIPTION: Applies TorchScript Just-In-Time (JIT) compilation to the loaded PyTorch model (`net`). This optimizes the model for inference by reducing Python overhead and potentially fusing operations, leading to significant performance improvements (e.g., increasing FPS from ~20 to ~30 in this context). Requires `torch`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnet = torch.jit.script(net)\n```\n\n----------------------------------------\n\nTITLE: Mapping Model Output to Human-Readable Class Name - Python\nDESCRIPTION: Defines a post-processing function to map a numeric class index from the model to a human-readable string using an optional dictionary loaded from 'index_to_name.json'. Returns both the index and class name. Expects the mapping dictionary to be available as a global variable; otherwise returns 'Unknown' for the class name. Handles missing or custom mappings gracefully.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/deployment_with_flask.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef render_prediction(prediction_idx):\\n    stridx = str(prediction_idx)\\n    class_name = 'Unknown'\\n    if img_class_map is not None:\\n        if stridx in img_class_map is not None:\\n            class_name = img_class_map[stridx][1]\\n\\n    return prediction_idx, class_name\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Backend Operator in C++\nDESCRIPTION: Example implementation of a custom operator for a new backend with autograd support.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/extend_dispatcher.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nTensor my_op1(const Tensor& self, const Tensor& other) {\n  // call your backend-specific APIs to implement my_op so that\n  // it matches PyTorch's native behavior\n}\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(<schema_my_op1>, &my_op);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting FSDP Sharding Strategy in PyTorch\nDESCRIPTION: This code demonstrates how to set the sharding strategy for FSDP. It shows the use of SHARD_GRAD_OP (Zero2) strategy where only optimizer states and gradients are sharded, reducing communication overhead at the cost of higher memory footprint.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntorch.cuda.set_device(local_rank)\n\nmodel = FSDP(model,\n       auto_wrap_policy=t5_auto_wrap_policy,\n       mixed_precision=bfSixteen,\n       device_id=torch.cuda.current_device(),\n       sharding_strategy=ShardingStrategy.SHARD_GRAD_OP # ZERO2)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLTM Forward Pass in C++\nDESCRIPTION: This code snippet shows the implementation of the forward pass for the Long Short-Term Memory (LLTM) model in C++. It takes input tensors, performs calculations using PyTorch's C++ API, and returns a vector of output tensors.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n#include <vector>\n\nstd::vector<at::Tensor> lltm_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  auto X = torch::cat({old_h, input}, /*dim=*/1);\n\n  auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1));\n  auto gates = gate_weights.chunk(3, /*dim=*/1);\n\n  auto input_gate = torch::sigmoid(gates[0]);\n  auto output_gate = torch::sigmoid(gates[1]);\n  auto candidate_cell = torch::elu(gates[2], /*alpha=*/1.0);\n\n  auto new_cell = old_cell + candidate_cell * input_gate;\n  auto new_h = torch::tanh(new_cell) * output_gate;\n\n  return {new_h,\n          new_cell,\n          input_gate,\n          output_gate,\n          candidate_cell,\n          X,\n          gate_weights};\n}\n```\n\n----------------------------------------\n\nTITLE: Loading DCP Checkpoint into Non-FSDP Model in Python\nDESCRIPTION: This snippet shows how to load a saved checkpoint into a non-FSDP wrapped model in a non-distributed setup using DCP. It defines a ToyModel class and demonstrates the checkpoint loading process.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed.checkpoint as dcp\nimport torch.nn as nn\n\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef run_checkpoint_load_example():\n    # create the non FSDP-wrapped toy model\n    model = ToyModel()\n    state_dict = {\n        \"model\": model.state_dict(),\n    }\n\n    # since no progress group is initialized, DCP will disable any collectives.\n    dcp.load(\n        state_dict=state_dict,\n        checkpoint_id=CHECKPOINT_DIR,\n    )\n    model.load_state_dict(state_dict[\"model\"])\n\nif __name__ == \"__main__\":\n    print(f\"Running basic DCP checkpoint loading example.\")\n    run_checkpoint_load_example()\n```\n\n----------------------------------------\n\nTITLE: Initializing Basic DDP Training with torchrun and PyTorch Elastic - PyTorch - Python\nDESCRIPTION: Sets up a basic DDP model for distributed training using PyTorch Elastic/torchrun. The ToyModel is defined, assigned to a device based on LOCAL_RANK, wrapped in DistributedDataParallel, and trained for one batch. All process group initialization, model/device setup, forward/backward passes, and cleanup are included. Intended for direct invocation or as a distributed worker launched via torchrun. Requires environment variables LOCAL_RANK, torch, torch.distributed, torch.nn, torch.optim, and DDP.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic():\n    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n    dist.init_process_group(\"nccl\")\n    rank = dist.get_rank()\n    print(f\"Start running basic DDP example on rank {rank}.\")\n    # create model and move it to GPU with id rank\n    device_id = rank % torch.cuda.device_count()\n    model = ToyModel().to(device_id)\n    ddp_model = DDP(model, device_ids=[device_id])\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_id)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n    dist.destroy_process_group()\n    print(f\"Finished running basic DDP example on rank {rank}.\")\n\nif __name__ == \"__main__\":\n    demo_basic()\n```\n\n----------------------------------------\n\nTITLE: Defining Evaluation and Data Loading Functions for GLUE Tasks in PyTorch\nDESCRIPTION: Defines two functions reused from HuggingFace examples. `evaluate` performs model evaluation on a specified GLUE task, handling data loading, distributed evaluation setup, inference loop, metric computation, and result logging. `load_and_cache_examples` loads examples for a task using a processor, converts them to features using the tokenizer, caches them, and returns a `TensorDataset`. These functions are essential for assessing model performance before and after quantization and depend on various configurations passed via `args` and helper functions/classes like `compute_metrics`, `processors`, `DataLoader`, etc.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndef evaluate(args, model, tokenizer, prefix=\"\"):\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n\n    results = {}\n    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        # Note that DistributedSampler samples randomly\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        # multi-gpu eval\n        if args.n_gpu > 1:\n            model = torch.nn.DataParallel(model)\n\n        # Eval!\n        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n        logger.info(\"  Num examples = %d\", len(eval_dataset))\n        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            model.eval()\n            batch = tuple(t.to(args.device) for t in batch)\n\n            with torch.no_grad():\n                inputs = {'input_ids':      batch[0],\n                          'attention_mask': batch[1],\n                          'labels':         batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n                outputs = model(**inputs)\n                tmp_eval_loss, logits = outputs[:2]\n\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == \"classification\":\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == \"regression\":\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n\n        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n        with open(output_eval_file, \"w\") as writer:\n            logger.info(\"***** Eval results {} *****\".format(prefix))\n            for key in sorted(result.keys()):\n                logger.info(\"  %s = %s\", key, str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return results\n\n\ndef load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n        'dev' if evaluate else 'train',\n        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n        str(args.max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(\"Loading features from cached file %s\", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n            # HACK(label indices are swapped in RoBERTa pretrained model)\n            label_list[1], label_list[2] = label_list[2], label_list[1]\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples,\n                                                tokenizer,\n                                                label_list=label_list,\n                                                max_length=args.max_seq_length,\n                                                output_mode=output_mode,\n                                                pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n        )\n        if args.local_rank in [-1, 0]:\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == \"classification\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == \"regression\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset\n```\n\n----------------------------------------\n\nTITLE: Implementing DDP Model Checkpointing\nDESCRIPTION: Demonstrates how to save and load model checkpoints in a distributed setting, with optimizations to save checkpoints from a single process while loading across all processes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef demo_checkpoint(rank, world_size):\n    print(f\"Running DDP checkpoint example on rank {rank}.\")\n    setup(rank, world_size)\n\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n```\n\n----------------------------------------\n\nTITLE: Converting Calibrated Model to Quantized Model - PyTorch FX - Python\nDESCRIPTION: Applies convert_fx to a calibrated FX-prepared PyTorch model, yielding a quantized version suitable for inference. The script then prints the quantized model's architecture. The main input is the prepared_model from previous steps, and the output is a quantized_model object. Relies on torch.ao.quantization.fx and a successfully calibrated model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n    quantized_model = convert_fx(prepared_model)\n    print(quantized_model)\n```\n\n----------------------------------------\n\nTITLE: Applying Post Training Dynamic Quantization in PyTorch\nDESCRIPTION: This snippet applies post-training dynamic quantization to a given model using `torch.quantization.quantize_dynamic`. This method quantizes the weights (e.g., of Linear layers specified in `qconfig_spec`) to `torch.qint8` but keeps activations in floating-point, converting them to int8 only during computation. It is a simpler method but currently has limited operator support, primarily for `nn.Linear` and `nn.LSTM`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/quantization.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_dynamic_quantized = torch.quantization.quantize_dynamic(\n    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Polynomial Fitting with PyTorch Tensors\nDESCRIPTION: A PyTorch implementation of fitting a third-order polynomial to a sine function using PyTorch Tensors. This example still manually implements both forward and backward passes but uses PyTorch Tensors instead of numpy arrays.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/pytorch_with_examples.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# -*- coding: utf-8 -*-\nimport torch\nimport math\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n\n# Create random input and output data\ntorch.manual_seed(42)\nx = torch.randn(200, 1, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n```\n\n----------------------------------------\n\nTITLE: Illustrating PyTorch 2 Export QAT Flow in Python\nDESCRIPTION: This snippet outlines the core workflow for Quantization-Aware Training (QAT) using PyTorch 2 Export. It demonstrates importing necessary modules, defining a simple `torch.nn.Module`, capturing the model graph using `torch.export.export_for_training`, preparing the model for QAT with a specified quantizer (`XNNPACKQuantizer`), converting the model post-training (`convert_pt2e`), and setting the final quantized model to evaluation mode using `torch.ao.quantization.move_exported_model_to_eval`. Note that the actual training step is omitted for brevity.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch._export import capture_pre_autograd_graph\nfrom torch.ao.quantization.quantize_pt2e import (\n  prepare_qat_pt2e,\n  convert_pt2e,\n)\nfrom torch.ao.quantization.quantizer.xnnpack_quantizer import (\n  XNNPACKQuantizer,\n  get_symmetric_quantization_config,\n)\n\nclass M(torch.nn.Module):\n   def __init__(self):\n      super().__init__()\n      self.linear = torch.nn.Linear(5, 10)\n\n   def forward(self, x):\n      return self.linear(x)\n\n\nexample_inputs = (torch.randn(1, 5),)\nm = M()\n\n# Step 1. program capture\n# This is available for pytorch 2.5+, for more details on lower pytorch versions\n# please check `Export the model with torch.export` section\nm = torch.export.export_for_training(m, example_inputs).module()\n# we get a model with aten ops\n\n# Step 2. quantization-aware training\n# backend developer will write their own Quantizer and expose methods to allow\n# users to express how they want the model to be quantized\nquantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())\nm = prepare_qat_pt2e(m, quantizer)\n\n# train omitted\n\nm = convert_pt2e(m)\n# we have a model with aten ops doing integer computations when possible\n\n# move the quantized model to eval mode, equivalent to `m.eval()`\ntorch.ao.quantization.move_exported_model_to_eval(m)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Function Runtime in Microseconds - torch.utils.benchmark - Python\nDESCRIPTION: Defines a utility function using torch.utils.benchmark.Timer to measure and return the mean execution time (in microseconds) of an arbitrary function call. This facilitates performance comparison between eager and compiled optimizer steps. Dependencies include torch.utils.benchmark. The function is generic: inputs are a callable and its arguments; output is the mean runtime in microseconds for one blocked autorange step.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/compiling_optimizer.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch.utils.benchmark as benchmark\\n\\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\\n    t0 = benchmark.Timer(\\n        stmt=\\\"f(*args, **kwargs)\\\", globals={\\\"args\\\": args, \\\"kwargs\\\": kwargs, \\\"f\\\": f}\\n    )\\n    return t0.blocked_autorange().mean * 1e6\n```\n\n----------------------------------------\n\nTITLE: Inference in Imperative Mode with BFloat16 using Intel® Extension for PyTorch*\nDESCRIPTION: This code demonstrates how to optimize a pre-trained BERT model for inference with BFloat16 mixed precision using Intel® Extension for PyTorch* (IPEX) in imperative mode. It uses torch.cpu.amp.autocast for automatic mixed precision.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import BertModel\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\n######################################################\n\nwith torch.no_grad():\n  with torch.cpu.amp.autocast():\n    model(data)\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch C++ inference with Intel optimization\nDESCRIPTION: This C++ example shows how to use the Intel Extension for PyTorch in a libtorch inference application. It loads a model, prepares input in channels last format, and performs inference. The extension provides optimization automatically when linked.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/script.h>\n#include <iostream>\n#include <memory>\n\nint main(int argc, const char* argv[]) {\n    torch::jit::script::Module module;\n    try {\n        module = torch::jit::load(argv[1]);\n    }\n    catch (const c10::Error& e) {\n        std::cerr << \"error loading the model\\n\";\n        return -1;\n    }\n    std::vector<torch::jit::IValue> inputs;\n    // make sure input data are converted to channels last format\n    inputs.push_back(torch::ones({1, 3, 224, 224}).to(c10::MemoryFormat::ChannelsLast));\n\n    at::Tensor output = module.forward(inputs).toTensor();\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto Wrap Policy in FSDP for Model Sharding - PyTorch (Python)\nDESCRIPTION: Sets up an auto_wrap_policy with FSDP, enabling selective wrapping of model submodules depending on parameter count for improved efficiency. Uses functools.partial to specify a minimum parameter threshold, sets device, constructs the MNIST Net and wraps it with FSDP using the policy. Requires torch, torch.distributed, Net model, and size_based_auto_wrap_policy. The auto_wrap_policy allows efficient parameter sharding and granular communication. Main input is the min_num_params parameter; output is an FSDP-wrapped model with hierarchical sharding.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmy_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=20000\n    )\ntorch.cuda.set_device(rank)\nmodel = Net().to(rank)\n\nmodel = FSDP(model,\n    auto_wrap_policy=my_auto_wrap_policy)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLTM CUDA Forward Function in C++\nDESCRIPTION: This function prepares the data and launches the CUDA kernel for the LLTM forward pass. It uses AT_DISPATCH_FLOATING_TYPES to handle different data types and creates PackedAccessors for efficient data transfer to the GPU.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<torch::Tensor> lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  auto X = torch::cat({old_h, input}, /*dim=*/1);\n  auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1));\n\n  const auto batch_size = old_cell.size(0);\n  const auto state_size = old_cell.size(1);\n\n  auto gates = gate_weights.reshape({batch_size, 3, state_size});\n  auto new_h = torch::zeros_like(old_cell);\n  auto new_cell = torch::zeros_like(old_cell);\n  auto input_gate = torch::zeros_like(old_cell);\n  auto output_gate = torch::zeros_like(old_cell);\n  auto candidate_cell = torch::zeros_like(old_cell);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&] {\n    lltm_cuda_forward_kernel<scalar_t><<<blocks, threads>>>(\n        gates.packed_accessor32<scalar_t,3,torch::RestrictPtrTraits>(),\n        old_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        new_h.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        new_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        input_gate.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        output_gate.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>(),\n        candidate_cell.packed_accessor32<scalar_t,2,torch::RestrictPtrTraits>());\n  }));\n\n  return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates};\n}\n```\n\n----------------------------------------\n\nTITLE: Debugging Quantized Models and Evaluating with Debug Mode - PyTorch - Python\nDESCRIPTION: Creates a quantized model using a debug flag, invokes evaluation, and prints the computation graph, allowing a developer to inspect quantization steps and weights. The debug mode enables floating-point emulation of quantized arithmetic, making debugging easier (numerics approximate for per-channel quantization, exact for per-tensor). Depends on a quantization config, input traced model, and uses print utilities for introspection.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Evaluate the debug model\ntime_model_evaluation(quantized_model_debug, configs, tokenizer)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nquantized_model_debug = quantize_dynamic_jit(traced_model, qconfig_dict, debug=True)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(str(quantized_model_debug.graph))\n\n```\n\nLANGUAGE: python\nCODE:\n```\n# Get the size of the debug model\nprint_size_of_model(quantized_model_debug)\n\n```\n\n----------------------------------------\n\nTITLE: Obtaining Top Class Prediction from PyTorch Model - Python\nDESCRIPTION: Implements a function that takes a prepared input tensor and runs inference using the global DenseNet model object, extracting the predicted class index with max likelihood. Relies on the model being loaded with pre-trained ImageNet weights and set to eval mode. Receives a 4D tensor, outputs the predicted class as an integer.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/deployment_with_flask.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_prediction(input_tensor):\\n    outputs = model.forward(input_tensor)\\n    _, y_hat = outputs.max(1)\\n    prediction = y_hat.item()\\n    return prediction\n```\n\n----------------------------------------\n\nTITLE: Accelerating 2:4 Sparse BERT Model for Inference in PyTorch\nDESCRIPTION: This snippet demonstrates how to accelerate a 2:4 sparse BERT model for inference. It converts the model to half precision, applies sparsity acceleration to linear layers, and evaluates the model's performance and execution time.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel = model.cuda().half()\n# accelerate for sparsity\nfor fqn, module in model.named_modules():\n    if isinstance(module, nn.Linear) and \"layer\" in fqn:\n        module.weight = nn.Parameter(to_sparse_semi_structured(module.weight))\n\nwith torch.inference_mode():\n    predictions = trainer.predict(tokenized_squad_dataset[\"validation\"])\nstart_logits, end_logits = predictions.predictions\nmetrics_sparse = compute_metrics(\n    start_logits,\n    end_logits,\n    tokenized_squad_dataset[\"validation\"],\n    squad_dataset[\"validation\"],\n)\nprint(\"sparse eval metrics: \", metrics_sparse)\nsparse_perf = measure_execution_time(\n    model,\n    batch_sizes,\n    tokenized_squad_dataset[\"validation\"],\n)\nprint(\"sparse perf metrics: \", sparse_perf)\n\n# sparse eval metrics:  {'exact_match': 78.43897824030275, 'f1': 86.48718950090766}\n# sparse perf metrics:  {4: 12.621004460379481, 16: 15.368514601141214, 64: 58.702805917710066, 256: 244.19364519417286}\n```\n\n----------------------------------------\n\nTITLE: Training ResNet50 on GPU with IPEX - Float32 (Python)\nDESCRIPTION: This snippet demonstrates training a ResNet50 model on GPU using IPEX with Float32 precision. It includes data loading, model initialization, and the training loop with IPEX optimizations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision\nimport intel_extension_for_pytorch as ipex\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n        root=DATA,\n        train=True,\n        transform=transform,\n        download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\nmodel = model.to(\"xpu\")\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.float32)\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    print(batch_idx)\ntorch.save({\n     'model_state_dict': model.state_dict(),\n     'optimizer_state_dict': optimizer.state_dict(),\n     }, 'checkpoint.pth')\n\n```\n\n----------------------------------------\n\nTITLE: Applying IPEX Operator Optimization to a PyTorch Model\nDESCRIPTION: This Python snippet defines a simple PyTorch model with Conv2d and ReLU layers. It then demonstrates how to apply Intel® Extension for PyTorch* (IPEX) operator optimization by importing `intel_extension_for_pytorch` and calling `ipex.optimize(model)` on the model instance. This modifies the model in-place to use optimized kernels where available.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchserve_with_ipex_2.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x \n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\nprint(model)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Custom LLTM CPU Performance in Python\nDESCRIPTION: Measures the execution time of the forward and backward passes of the custom `LLTM` module on the CPU. It initializes random tensors, creates an `LLTM` instance, and runs the forward and backward operations repeatedly within a loop, accumulating the total time using the `time` module. Requires PyTorch and the custom `LLTM` module defined previously.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport torch\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} s | Backward {:.3f} s'.format(forward, backward))\n```\n\n----------------------------------------\n\nTITLE: Partitioning Datasets for Distributed Training in PyTorch (Python)\nDESCRIPTION: Defines two Python classes: Partition and DataPartitioner, which help divide a dataset into non-overlapping partitions for distributed model training. These classes ensure different processes work on distinct data fragments using deterministic shuffling based on a provided random seed. Dependencies include a dataset compatible with indexing, a sizes list specifying partition proportions (defaulting to [0.7, 0.2, 0.1]), and 'random.Random'. The input is any indexable dataset, and the output is partitioned dataset objects ready for distributed DataLoader use.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n\"\"\" Dataset partitioning helper \"\"\"\nclass Partition(object):\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()  # from random import Random\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])\n```\n\n----------------------------------------\n\nTITLE: Optimizing a PyTorch Model with IPEX and Channels Last\nDESCRIPTION: Defines a simple PyTorch CNN model, prepares it for inference, generates input data, and then applies Intel® Extension for PyTorch* optimizations using `ipex.optimize`. It demonstrates how to optionally disable the default automatic conversion to channels-last memory format. Finally, the optimized model is traced and frozen using `torch.jit` for deployment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchserve_with_ipex_2.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x \n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\nimport intel_extension_for_pytorch as ipex\n############################### code changes ###############################\nipex.disable_auto_channels_last() # omit this line for channels_last (default) \n############################################################################\nmodel = ipex.optimize(model)\n\nwith torch.no_grad():\n    model = torch.jit.trace(model, data)\n    model = torch.jit.freeze(model)\n```\n\n----------------------------------------\n\nTITLE: Model Training Function Implementation\nDESCRIPTION: Defines a comprehensive training function that handles model training, validation, learning rate scheduling, and best model saving.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n  since = time.time()\n\n  best_model_wts = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n\n  for epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      else:\n        model.eval()\n\n      running_loss = 0.0\n      running_corrects = 0\n\n      for inputs, labels in dataloaders[phase]:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(phase == 'train'):\n          outputs = model(inputs)\n          _, preds = torch.max(outputs, 1)\n          loss = criterion(outputs, labels)\n\n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n      if phase == 'train':\n        scheduler.step()\n\n      epoch_loss = running_loss / dataset_sizes[phase]\n      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n        phase, epoch_loss, epoch_acc))\n\n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    print()\n\n  time_elapsed = time.time() - since\n  print('Training complete in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))\n  print('Best val Acc: {:4f}'.format(best_acc))\n\n  model.load_state_dict(best_model_wts)\n  return model\n```\n\n----------------------------------------\n\nTITLE: Defining the TrainerNet Class Initialization in Python\nDESCRIPTION: Defines the `TrainerNet` class inheriting from `torch.nn.Module`. The `__init__` method acquires a remote reference (RRef) to the central parameter server using `rpc.remote`. This RRef (`self.param_server_rref`) allows the trainer process to interact with the `ParameterServer` instance residing on a different process without copying it locally. The `get_parameter_server` helper function (defined previously) is used to obtain the handle to the singleton server.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# --------- Trainers --------------------\n\n# nn.Module corresponding to the network trained by this trainer. The\n# forward() method simply invokes the network on the given parameter\n# server.\nclass TrainerNet(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        self.num_gpus = num_gpus\n        self.param_server_rref = rpc.remote(\n            \"parameter_server\", get_parameter_server, args=(num_gpus,))\n```\n\n----------------------------------------\n\nTITLE: Defining a DCGAN Generator Module in PyTorch C++\nDESCRIPTION: This code constructs a DCGANGeneratorImpl class, serving as the generator for a DCGAN, with stacked transposed convolutions and batch normalization. It registers each submodule with PyTorch's registry for automatic management. Core dependencies include torch::nn::Module, torch::nn::ConvTranspose2dOptions, batch normalization, and initializer list registration, supporting learning-based image generation tasks.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\n  struct DCGANGeneratorImpl : nn::Module {\n    DCGANGeneratorImpl(int kNoiseSize)\n        : conv1(nn::ConvTranspose2dOptions(kNoiseSize, 256, 4)\n                    .bias(false)),\n          batch_norm1(256),\n          conv2(nn::ConvTranspose2dOptions(256, 128, 3)\n                    .stride(2)\n                    .padding(1)\n                    .bias(false)),\n          batch_norm2(128),\n          conv3(nn::ConvTranspose2dOptions(128, 64, 4)\n                    .stride(2)\n                    .padding(1)\n                    .bias(false)),\n          batch_norm3(64),\n          conv4(nn::ConvTranspose2dOptions(64, 1, 4)\n                    .stride(2)\n                    .padding(1)\n                    .bias(false))\n   {\n     // register_module() is needed if we want to use the parameters() method later on\n     register_module(\"conv1\", conv1);\n     register_module(\"conv2\", conv2);\n     register_module(\"conv3\", conv3);\n     register_module(\"conv4\", conv4);\n     register_module(\"batch_norm1\", batch_norm1);\n     register_module(\"batch_norm2\", batch_norm2);\n     register_module(\"batch_norm3\", batch_norm3);\n   }\n```\n\n----------------------------------------\n\nTITLE: TorchScript Inference with BERT on GPU using IPEX - Float16 (Python)\nDESCRIPTION: This snippet shows TorchScript inference with a BERT model on GPU using IPEX with Float16 precision. It includes model loading, data preparation, IPEX optimization, Float16 autocast, and TorchScript tracing for improved performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import BertModel\nimport intel_extension_for_pytorch as ipex\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.float16)\n\nwith torch.no_grad():\n  d = torch.randint(vocab_size, size=[batch_size, seq_length])\n  d = d.to(\"xpu\")\n  with torch.xpu.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=False):\n    model = torch.jit.trace(model, (d,), check_trace=False, strict=False)\n    model = torch.jit.freeze(model)\n\n    model(data)\n\n```\n\n----------------------------------------\n\nTITLE: Setting up Finetuning for QAT Model in PyTorch\nDESCRIPTION: Enables gradient calculation for all parameters in the QAT-prepared model (`model_ft`) for finetuning. Moves the model to the specified `device` (GPU if available, as finetuning can run on GPU). Defines the `CrossEntropyLoss` criterion. Sets up the SGD optimizer with a lower learning rate (`1e-3`) and weight decay, as the entire model is being trained. Defines a `StepLR` learning rate scheduler with different parameters for finetuning. Finally, calls the `train_model` function to perform the finetuning.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfor param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)\n```\n\n----------------------------------------\n\nTITLE: Working with Datasets in PyTorch\nDESCRIPTION: Examples of dataset classes in PyTorch for handling and organizing data. These classes provide a standard interface for accessing data during training and evaluation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nDataset                    # abstract class representing dataset\nTensorDataset              # labelled dataset in the form of tensors\nConcatDataset              # concatenation of Datasets\n```\n\n----------------------------------------\n\nTITLE: Optimizing PyTorch model for Intel XPU with IPEX\nDESCRIPTION: This snippet demonstrates the required code changes to optimize a PyTorch model for Intel XPU (GPU) acceleration. It shows moving the model and data to XPU device, applying IPEX optimization with float16 precision, and using autocast for mixed precision.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\n#################### code changes ################\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.float16)\n#################### code changes ################\n\nwith torch.no_grad():\n  d = torch.randint(vocab_size, size=[batch_size, seq_length])\n  ################################# code changes ######################################\n  d = d.to(\"xpu\")\n  with torch.xpu.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=False):\n  ################################# code changes ######################################\n    model = torch.jit.trace(model, (d,), check_trace=False, strict=False)\n    model = torch.jit.freeze(model)\n\n    model(data)\n```\n\n----------------------------------------\n\nTITLE: Applying Post-Training Static Quantization to MobileNetV2 - PyTorch - Python\nDESCRIPTION: Shows the procedure for post-training static quantization of a PyTorch MobileNetV2 model by loading the model, fusing layers, setting up quantization configuration, preparing the model (which inserts observers), calibrating with a subset of the training data, and converting to a quantized version. The model is then evaluated and its size and accuracy are reported. Dependencies: torch.ao.quantization, a loaded MobileNetV2 model, calibration data loader, and a criterion. Key parameters include qconfig and the number of calibration batches.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnum_calibration_batches = 32\n\nmyModel = load_model(saved_model_dir + float_model_file).to('cpu')  \nmyModel.eval()  \n\n# Fuse Conv, bn and relu  \nmyModel.fuse_model()  \n\n# Specify quantization configuration  \n# Start with simple min/max range estimation and per-tensor quantization of weights \nmyModel.qconfig = torch.ao.quantization.default_qconfig\nprint(myModel.qconfig)  \ntorch.ao.quantization.prepare(myModel, inplace=True)\n\n# Calibrate first \nprint('Post Training Quantization Prepare: Inserting Observers')  \nprint('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv) \n\n# Calibrate with the training set \nevaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)  \nprint('Post Training Quantization: Calibration done') \n\n# Convert to quantized model  \ntorch.ao.quantization.convert(myModel, inplace=True)\n# You may see a user warning about needing to calibrate the model. This warning can be safely ignored.\n# This warning occurs because not all modules are run in each model runs, so some\n# modules may not be calibrated.\nprint('Post Training Quantization: Convert done') \nprint('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv) \n\nprint(\"Size of model after quantization\") \nprint_size_of_model(myModel)  \n\ntop1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches) \nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n```\n\n----------------------------------------\n\nTITLE: Quantizing a Model Using prepare_fx and convert_fx with BackendConfig in Python\nDESCRIPTION: This snippet defines a PyTorch Module, creates sample input, prepares it for quantization (calibration) using the backend config and qconfig mappings, and then converts it to a quantized model. Models using fused and quantized ops will reflect the configuration passed. Dependencies include all previous configuration steps and the availability of FX graph mode quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass MyModel(torch.nn.Module):\n    def __init__(self, use_bn: bool):\n        super().__init__()\n        self.linear = torch.nn.Linear(10, 3)\n        self.conv = torch.nn.Conv2d(3, 3, 3)\n        self.bn = torch.nn.BatchNorm2d(3)\n        self.relu = torch.nn.ReLU()\n        self.sigmoid = torch.nn.Sigmoid()\n        self.use_bn = use_bn\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.conv(x)\n        if self.use_bn:\n            x = self.bn(x)\n        x = self.relu(x)\n        x = self.sigmoid(x)\n        return x\n\nexample_inputs = (torch.rand(1, 3, 10, 10, dtype=torch.float),)\nmodel = MyModel(use_bn=False)\nprepared = prepare_fx(model, qconfig_mapping, example_inputs, backend_config=backend_config)\nprepared(*example_inputs)  # calibrate\nconverted = convert_fx(prepared, backend_config=backend_config)\n```\n\n----------------------------------------\n\nTITLE: Exporting a Model for Training using torch.export in Python\nDESCRIPTION: Demonstrates how to export a PyTorch model (`float_model`) specifically for training purposes using `torch.export.export_for_training`. This function captures the model's graph in a way that's suitable for subsequent QAT steps. Example inputs (`example_inputs`) with the correct shape are required for tracing. The code shows the method for PyTorch 2.5+ and includes a commented-out reference to the older `capture_pre_autograd_graph` relevant for PyTorch 2.4 and earlier.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._export import capture_pre_autograd_graph\n\nexample_inputs = (torch.rand(2, 3, 224, 224),)\n# for pytorch 2.5+\nexported_model = torch.export.export_for_training(float_model, example_inputs).module()\n# for pytorch 2.4 and before\n# from torch._export import capture_pre_autograd_graph\n```\n\n----------------------------------------\n\nTITLE: Computing Cross-Entropy Loss with Loss Parallel in PyTorch - Python\nDESCRIPTION: This sequence demonstrates the usage of the loss_parallel context manager to compute cross-entropy loss on sharded vocabulary outputs. Model predictions and labels are flattened, and loss.backward is called within the context. Requires torch.nn.functional, the loss_parallel context, and a sharded model prepared for loss parallelism.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn.functional as F\nfrom torch.distributed.tensor.parallel import loss_parallel\n\npred = model(input_ids)\nwith loss_parallel():\n    # assuming pred and labels are of the shape [batch, seq, vocab]\n    loss = F.cross_entropy(pred.flatten(0, 1), labels.flatten(0, 1))\n    loss.backward()\n```\n\n----------------------------------------\n\nTITLE: Implementing Post-Training Quantization with Built-in Metrics\nDESCRIPTION: Python code to perform accuracy-driven post-training quantization on a PyTorch model using Intel Neural Compressor. Uses MNIST test data for both calibration and evaluation with built-in top-1 accuracy metric.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nmodel.eval()\n\nfrom torchvision import datasets, transforms\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data', train=False, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                   ])),\n    batch_size=1)\n\n# launch code for Intel® Neural Compressor\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./conf.yaml\")\nquantizer.model = model\nquantizer.calib_dataloader = test_loader\nquantizer.eval_dataloader = test_loader\nq_model = quantizer()\nq_model.save('./output')\n```\n\n----------------------------------------\n\nTITLE: Defining Mixed Precision Policies in PyTorch FSDP\nDESCRIPTION: This snippet demonstrates how to define different mixed precision policies for FSDP training, including fp16, bfloat16, and fp32. It specifies parameter, gradient communication, and buffer precisions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nbfSixteen = MixedPrecision(\n    param_dtype=torch.bfloat16,\n    # Gradient communication precision.\n    reduce_dtype=torch.bfloat16,\n    # Buffer precision.\n    buffer_dtype=torch.bfloat16,\n)\n\nfp32_policy = MixedPrecision(\n    param_dtype=torch.float32,\n    # Gradient communication precision.\n    reduce_dtype=torch.float32,\n    # Buffer precision.\n    buffer_dtype=torch.float32,\n)\n```\n\n----------------------------------------\n\nTITLE: Mounting Google Drive in Google Colab for Data Access\nDESCRIPTION: This code snippet shows how to mount Google Drive in a Colab notebook. This allows access to files stored in Google Drive, which is useful for tutorials that require specific datasets.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/colab.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n```\n\n----------------------------------------\n\nTITLE: Training and Timing FSDP MNIST Model with CUDA Events - PyTorch (Python)\nDESCRIPTION: Implements distributed training for a PyTorch MNIST model wrapped with FSDP, with timing using CUDA events. The snippet shows initialization of CUDA events, model wrapping, setting optimizer and scheduler, running training/testing, and saving the state dict after synchronization. Requires torch, torch.distributed, and the Net model definition. Inputs include command-line arguments for controlling epochs, learning rate, and save behavior. Outputs include elapsed CUDA event times and saved models if enabled. Constraints include distributed setup and correct rank/world_size logic.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninit_end_event = torch.cuda.Event(enable_timing=True)\n\nmodel = Net().to(rank)\n\nmodel = FSDP(model)\n\noptimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\nscheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\ninit_start_event.record()\nfor epoch in range(1, args.epochs + 1):\n    train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n    test(model, rank, world_size, test_loader)\n    scheduler.step()\n\ninit_end_event.record()\n\nif rank == 0:\n    init_end_event.synchronize()\n    print(f\"CUDA event elapsed time: {init_start_event.elapsed_time(init_end_event) / 1000}sec\")\n    print(f\"{model}\")\n\nif args.save_model:\n    # use a barrier to make sure training is done on all ranks\n    dist.barrier()\n    states = model.state_dict()\n    if rank == 0:\n        torch.save(states, \"mnist_cnn.pt\")\n\ncleanup()\n```\n\n----------------------------------------\n\nTITLE: Using CommDebugMode to Track Collective Operations with PyTorch DTensor\nDESCRIPTION: This snippet demonstrates how to use CommDebugMode context manager to track collective operations in distributed training. It shows how to capture, print, log, and generate JSON dumps of operation-level collective tracing information at different noise levels.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_comm_debug_mode.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# The model used in this example is a MLPModule applying Tensor Parallel\ncomm_mode = CommDebugMode()\n    with comm_mode:\n        output = model(inp)\n\n# print the operation level collective tracing information\nprint(comm_mode.generate_comm_debug_tracing_table(noise_level=0))\n\n# log the operation level collective tracing information to a file\ncomm_mode.log_comm_debug_tracing_table_to_file(\n    noise_level=1, file_name=\"transformer_operation_log.txt\"\n)\n\n# dump the operation level collective tracing information to json file,\n# used in the visual browser below\ncomm_mode.generate_json_dump(noise_level=2)\n```\n\n----------------------------------------\n\nTITLE: Instantiating PyTorch DataLoader in Python\nDESCRIPTION: This snippet demonstrates the basic instantiation of a PyTorch `DataLoader`. The `DataLoader` wraps a `Dataset` and provides an iterable over it, loading data in batches. Key parameters include the `dataset` object and `batch_size`. It simplifies the process of feeding data to a model during training or evaluation, handling batching automatically.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nDataLoader(dataset, batch_size=1, ...)      # loads data batches agnostic \n                                                # of structure of individual data points\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Fusing MobileNetV2 for Quantization - PyTorch - Python\nDESCRIPTION: Demonstrates the instantiation of a MobileNetV2 model from a saved file, preparation for quantization via module fusion, and initial accuracy evaluation. Fusion is performed using the model's fuse_model() method, which optimizes the model by combining convolution, batch normalization, and ReLU layers prior to quantization. Relies on custom load_model(), torch, and a prepared data loader, and outputs model evaluation with summary statistics.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndata_path = '~/.data/imagenet'\nsaved_model_dir = 'data/' \nfloat_model_file = 'mobilenet_pretrained_float.pth' \nscripted_float_model_file = 'mobilenet_quantization_scripted.pth' \nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth' \n\ntrain_batch_size = 30 \neval_batch_size = 50 \n\ndata_loader, data_loader_test = prepare_data_loaders(data_path) \ncriterion = nn.CrossEntropyLoss() \nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')  \n \n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access  \n# while also improving numerical accuracy. While this can be used with any model, this is \n# especially common with quantized models.  \n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv) \nfloat_model.eval()  \n\n# Fuses modules \nfloat_model.fuse_model()  \n\n# Note fusion of Conv+BN+Relu and Conv+Relu \nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)  \n```\n\n----------------------------------------\n\nTITLE: Calibrating the Quantization-Ready Model in PyTorch - Python\nDESCRIPTION: Defines a calibration function to run through the data loader, passing representative batches through the prepared model to collect activation statistics. This step must be performed before converting to a quantized model. The function takes the prepared model and data loader as inputs, operating in eval/no_grad mode for efficiency.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)\ncalibrate(prepared_model, data_loader_test)  # run calibration on sample data\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch Model to TorchScript\nDESCRIPTION: Shows how to load a pretrained ResNet18 model, convert it to TorchScript format, and verify the output matches the original model. Includes input preparation and model saving.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torchscript_inference.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nr18 = models.resnet18(pretrained=True)       # We now have an instance of the pretrained model\nr18_scripted = torch.jit.script(r18)         # *** This is the TorchScript export\ndummy_input = torch.rand(1, 3, 224, 224)     # We should run a quick test\n```\n\n----------------------------------------\n\nTITLE: Evaluating BERT Model Performance in Python\nDESCRIPTION: This code snippet prints the evaluation results and timing information for both the original FP32 BERT model and the quantized INT8 version. It uses a custom time_model_evaluation function to measure performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\neval_duration_time = eval_end_time - eval_start_time\nprint(result)\nprint(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\n# Evaluate the original FP32 BERT model\ntime_model_evaluation(model, configs, tokenizer)\n\n# Evaluate the INT8 BERT model after the dynamic quantization\ntime_model_evaluation(quantized_model, configs, tokenizer)\n```\n\n----------------------------------------\n\nTITLE: Configuring XNNPACK Quantizer for Symmetric Quantization\nDESCRIPTION: Code for importing and configuring the XNNPACK quantizer with symmetric quantization for QAT. This establishes the quantization settings to be applied globally to the model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.ao.quantization.quantizer.xnnpack_quantizer import (\n    XNNPACKQuantizer,\n    get_symmetric_quantization_config,\n)\nquantizer = XNNPACKQuantizer()\nquantizer.set_global(get_symmetric_quantization_config(is_qat=True))\n```\n\n----------------------------------------\n\nTITLE: PyTorch Inference Profiling for Large Batch Size\nDESCRIPTION: This Python code snippet demonstrates how to profile PyTorch inference for a larger batch size (256) using the torch.profiler module, with MKLDNN fast math mode enabled.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nX = torch.rand(256, 64, 64, device=device)\nwith torch.set_grad_enabled(False):\n    for _ in range(50):\n        model(X) #Warmup\n    with profile(activities=[ProfilerActivity.CPU]) as prof:\n        with record_function(\"mymodel_inference\"):\n            for _ in range(100):\n                model(X)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n```\n\n----------------------------------------\n\nTITLE: Loading Cached Artifacts with torch.compile\nDESCRIPTION: This snippet shows how to load previously saved cache artifacts to jump-start the torch.compile cache. It uses the load_cache_artifacts() function to populate the modular caches with the saved data.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_compile_caching_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Potentially download/fetch the artifacts from the database\ntorch.compiler.load_cache_artifacts(artifact_bytes)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating and Benchmarking PyTorch CUDA RPC with TensorPipe (Python)\nDESCRIPTION: This Python code splits a neural network across two workers using PyTorch's distributed RPC framework, illustrating how to directly transmit CUDA tensors between GPUs using TensorPipe's CUDA-aware channels. It defines a simple model class, exploitation of direct GPU-to-GPU communication versus staged CPU transfer, and a comparative timing measurement between the two methods. Required dependencies are PyTorch 1.8+ with distributed and multiprocessing modules; recommended to run on two GPUs. The main function launches two worker processes, each initializing RPC and optionally setting a device map for direct CUDA paths. Inputs are 1000x1000 tensors; outputs are printed execution times for both CPU and CUDA RPC modes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/cuda_rpc.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed.autograd as autograd\nimport torch.distributed.rpc as rpc\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nimport os\nimport time\n\n\nclass MyModule(nn.Module):\n    def __init__(self, device, comm_mode):\n        super().__init__()\n        self.device = device\n        self.linear = nn.Linear(1000, 1000).to(device)\n        self.comm_mode = comm_mode\n\n    def forward(self, x):\n        # x.to() is a no-op if x is already on self.device\n        y = self.linear(x.to(self.device))\n        return y.cpu() if self.comm_mode == \"cpu\" else y\n\n    def parameter_rrefs(self):\n        return [rpc.RRef(p) for p in self.parameters()]\n\n\ndef measure(comm_mode):\n    # local module on \"worker0/cuda:0\"\n    lm = MyModule(\"cuda:0\", comm_mode)\n    # remote module on \"worker1/cuda:1\"\n    rm = rpc.remote(\"worker1\", MyModule, args=(\"cuda:1\", comm_mode))\n    # prepare random inputs\n    x = torch.randn(1000, 1000).cuda(0)\n\n    tik = time.time()\n    for _ in range(10):\n        with autograd.context() as ctx:\n            y = rm.rpc_sync().forward(lm(x))\n            autograd.backward(ctx, [y.sum()])\n    # synchronize on \"cuda:0\" to make sure that all pending CUDA ops are\n    # included in the measurements\n    torch.cuda.current_stream(\"cuda:0\").synchronize()\n    tok = time.time()\n    print(f\"{comm_mode} RPC total execution time: {tok - tik}\")\n\n\ndef run_worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=128)\n\n    if rank == 0:\n        options.set_device_map(\"worker1\", {0: 1})\n        rpc.init_rpc(\n            f\"worker{rank}\",\n            rank=rank,\n            world_size=2,\n            rpc_backend_options=options\n        )\n        measure(comm_mode=\"cpu\")\n        measure(comm_mode=\"cuda\")\n    else:\n        rpc.init_rpc(\n            f\"worker{rank}\",\n            rank=rank,\n            world_size=2,\n            rpc_backend_options=options\n        )\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__==\"__main__\":\n    world_size = 2\n    mp.spawn(run_worker, nprocs=world_size, join=True)\n\n```\n\n----------------------------------------\n\nTITLE: Creating TorchServe Model Archive\nDESCRIPTION: Uses torch-model-archiver to package the custom handler into a model archive (MAR) file for deployment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torchserve_vertexai_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n!torch-model-archiver \\\n-f \\\n--model-name <your_model_name> \\\n--version 1.0 \\\n --handler model_artifacts/handler.py \\\n--export-path model_artifacts\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Path for Cornell Movie Dialogs Corpus in Google Colab\nDESCRIPTION: This snippet demonstrates how to set up the correct file path for accessing the Cornell Movie Dialogs Corpus data stored in Google Drive. It's part of the setup for the Chatbot Tutorial.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/colab.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncorpus_name = \"cornell\"\ncorpus = os.path.join(\"/content/gdrive/My Drive/data\", corpus_name)\n```\n\n----------------------------------------\n\nTITLE: Preparing BERT Model for Pruning with Parameterizations in PyTorch\nDESCRIPTION: Code that prepares the BERT model for pruning by inserting fake-sparsity parameterizations for training. This modifies the model's weight access to return masked weights (mask * weight) during inference and training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Prepare the model, insert fake-sparsity paramterizations for training\nsparsifier.prepare(model, sparse_config)\nprint(model.bert.encoder.layer[0].output)\n\n# BertOutput(\n#   (dense): ParametrizedLinear(\n#     in_features=3072, out_features=768, bias=True\n```\n\n----------------------------------------\n\nTITLE: Calculating Model Accuracy with Remote Inference in PyTorch (Python)\nDESCRIPTION: Defines the `get_accuracy` function to evaluate the trained model using a `test_loader`. It sets the `model` (an instance of `TrainerNet`) to evaluation mode (`model.eval()`) and disables gradient computation (`torch.no_grad()`). Inside the loop, it calls `model(data)`, which internally triggers an RPC to the parameter server for the forward pass. It then calculates predictions, compares them to targets (potentially moving them to the GPU if available), sums up correct predictions, and finally prints the overall accuracy.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef get_accuracy(test_loader, model):\n    model.eval()\n    correct_sum = 0\n    # Use GPU to evaluate if possible\n    device = torch.device(\"cuda:0\" if model.num_gpus > 0\n        and torch.cuda.is_available() else \"cpu\")\n    with torch.no_grad():\n        for i, (data, target) in enumerate(test_loader):\n            out = model(data, -1)\n            pred = out.argmax(dim=1, keepdim=True)\n            pred, target = pred.to(device), target.to(device)\n            correct = pred.eq(target.view_as(pred)).sum().item()\n            correct_sum += correct\n\n    print(f\"Accuracy {correct_sum / len(test_loader.dataset)}\")\n```\n\n----------------------------------------\n\nTITLE: Getting Default QConfig and Setting Global - PyTorch - Python\nDESCRIPTION: Obtains the recommended qconfig for x86 backend with get_default_qconfig and applies it globally using QConfigMapping. The code depends on PyTorch quantization utilities. Input is the backend string (\"x86\"), and the output is the qconfig and an updated QConfigMapping object for quantization preparation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n    # The old 'fbgemm' is still available but 'x86' is the recommended default.\n    qconfig = get_default_qconfig(\"x86\") \n    qconfig_mapping = QConfigMapping().set_global(qconfig)\n```\n\n----------------------------------------\n\nTITLE: Applying Dynamic Quantization to a PyTorch Model\nDESCRIPTION: Uses `torch.quantization.quantize_dynamic` to apply dynamic quantization to the input `model`. It targets `torch.nn.Linear` modules for quantization and specifies `torch.qint8` as the target data type for weights. The resulting quantized model is stored in `quantized_model` and then printed to show its structure.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\nprint(quantized_model)\n```\n\n----------------------------------------\n\nTITLE: FeedForward Layer Implementation\nDESCRIPTION: Shows the forward pass implementation of the FeedForward layer in a Transformer model, demonstrating the SwiGLU activation pattern.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x):\n    return self.w2(F.silu(self.w1(x)) * self.w3(x))\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Embedding, Norm, and Output Layers with Sequence Parallel in PyTorch - Python\nDESCRIPTION: This example demonstrates advanced module parallelization with explicit layouts for embedding, norm, and output layers using parallelize_module. Row-wise and column-wise sharding is controlled using the input_layouts and output_layouts keys, employing SequenceParallel for the norm layer to enable sharded computation along the sequence dimension. Required for combining various parallelization strategies in end-to-end model parallelism.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel = parallelize_module(\n    model,\n    tp_mesh,\n    {\n        \"tok_embeddings\": RowwiseParallel(\n            input_layouts=Replicate(),\n            output_layouts=Shard(1),\n        ),\n        \"norm\": SequenceParallel(),\n        \"output\": ColwiseParallel(\n            input_layouts=Shard(1),\n            output_layouts=Replicate()\n        ),\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Running FSDP MNIST Training with Auto Wrap Policy - Bash\nDESCRIPTION: Runs the MNIST FSDP training script with the auto_wrap_policy applied and outputs the CUDA timing. This assumes FSDP_mnist.py includes auto_wrap_policy logic. Outputs elapsed CUDA event timing after completion. Requires a valid multi-GPU environment and Python dependencies.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython FSDP_mnist.py\n\nCUDA event elapsed time on training loop 41.89130859375sec\n```\n\n----------------------------------------\n\nTITLE: Configuring QConfigMapping with Priorities - PyTorch - Python\nDESCRIPTION: This block sets up detailed mapping rules for applying quantization configurations in PyTorch using QConfigMapping. Multiple assignment methods show how to target qconfigs to specific object types, method names, and module name patterns, in a defined priority order. Dependencies include torch, QConfigMapping, and correctly defined qconfig_opt values. Key parameters dictate priority and specificity of quantization, accepting module types, name regexes, and override order.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n  qconfig_mapping = (QConfigMapping()\n      .set_global(qconfig_opt)  # qconfig_opt is an optional qconfig, either a valid qconfig or None\n      .set_object_type(torch.nn.Conv2d, qconfig_opt)  # can be a callable...\n      .set_object_type(\"reshape\", qconfig_opt)  # ...or a string of the method\n      .set_module_name_regex(\"foo.*bar.*conv[0-9]+\", qconfig_opt) # matched in order, first match takes precedence\n      .set_module_name(\"foo.bar\", qconfig_opt)\n      .set_module_name_object_type_order()\n  )\n      # priority (in increasing order): global, object_type, module_name_regex, module_name\n      # qconfig == None means fusion and quantization should be skipped for anything\n      # matching the rule (unless a higher priority match is found)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking BERT Models (Python)\nDESCRIPTION: This function benchmarks different versions of the BERT model (float, eager mode quantized, and graph mode quantized) using dummy input. It measures the average inference time over 50 iterations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef benchmark(model):\n    model = torch.jit.load(model)\n    model.eval()\n    torch.set_num_threads(1)\n    input_ids = ids_tensor([8, 128], 2)\n    token_type_ids = ids_tensor([8, 128], 2)\n    attention_mask = ids_tensor([8, 128], vocab_size=2)\n    elapsed = 0\n    for _i in range(50):\n        start = time.time()\n        output = model(input_ids, token_type_ids, attention_mask)\n        end = time.time()\n        elapsed = elapsed + (end - start)\n    print('Elapsed time: ', (elapsed / 50), ' s')\n    return\nprint(\"Running benchmark for Float model\")\nbenchmark(args.jit_model_path_float)\nprint(\"Running benchmark for Eager Mode Quantized model\")\nbenchmark(args.jit_model_path_eager)\nprint(\"Running benchmark for Graph Mode Quantized model\")\nbenchmark(args.jit_model_path_graph)\n```\n\n----------------------------------------\n\nTITLE: Testing Custom PyTorch Operator with opcheck in Python\nDESCRIPTION: This Python snippet defines helper functions (`sample_inputs`, `reference_muladd`) to generate test data and provide a reference implementation for the custom operator `extension_cpp.mymuladd`. It then iterates through sample inputs, compares the custom operator's output against the reference using `torch.testing.assert_close`, and uses `torch.library.opcheck` to verify the operator registration and API usage compliance. This ensures both functional correctness and proper integration with PyTorch's dispatch system.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef sample_inputs(device, *, requires_grad=False):\n    def make_tensor(*size):\n        return torch.randn(size, device=device, requires_grad=requires_grad)\n\n    def make_nondiff_tensor(*size):\n        return torch.randn(size, device=device, requires_grad=False)\n\n    return [\n        [make_tensor(3), make_tensor(3), 1],\n        [make_tensor(20), make_tensor(20), 3.14],\n        [make_tensor(20), make_nondiff_tensor(20), -123],\n        [make_nondiff_tensor(2, 3), make_tensor(2, 3), -0.3],\n    ]\n\ndef reference_muladd(a, b, c):\n    return a * b + c\n\nsamples = sample_inputs(device, requires_grad=True)\nsamples.extend(sample_inputs(device, requires_grad=False))\nfor args in samples:\n    # Correctness test\n    result = torch.ops.extension_cpp.mymuladd(*args)\n    expected = reference_muladd(*args)\n    torch.testing.assert_close(result, expected)\n\n    # Use opcheck to check for incorrect usage of operator registration APIs\n    torch.library.opcheck(torch.ops.extension_cpp.mymuladd.default, args)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Inference Profiling for Small Batch Size\nDESCRIPTION: This Python code snippet demonstrates how to profile PyTorch inference for a small batch size (10) using the torch.profiler module.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nX = torch.rand(10, 64, 64, device=device)\nwith torch.set_grad_enabled(False):\n    for _ in range(50):\n        model(X) #Warmup\n    with profile(activities=[ProfilerActivity.CPU]) as prof:\n        with record_function(\"mymodel_inference\"):\n            for _ in range(100):\n                model(X)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n```\n\n----------------------------------------\n\nTITLE: Defining MobileNetV2 Model Architecture for Quantization\nDESCRIPTION: This code defines the MobileNetV2 model architecture with modifications to enable quantization. It includes helper functions, custom layers, and the main MobileNetV2 class with quantization-specific adjustments.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.ao.quantization import QuantStub, DeQuantStub\n\ndef _make_divisible(v, divisor, min_value=None):  \n    \"\"\" \n    This function is taken from the original tf repo. \n    It ensures that all layers have a channel number that is divisible by 8 \n    It can be seen here:  \n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py  \n    :param v: \n    :param divisor: \n    :param min_value: \n    :return:  \n    \"\"\" \n    if min_value is None: \n        min_value = divisor \n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor) \n    # Make sure that round down does not go down by more than 10%.  \n    if new_v < 0.9 * v: \n        new_v += divisor  \n    return new_v  \n\n\nclass ConvBNReLU(nn.Sequential):  \n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1): \n        padding = (kernel_size - 1) // 2  \n        super(ConvBNReLU, self).__init__( \n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),  \n            nn.BatchNorm2d(out_planes, momentum=0.1), \n            # Replace with ReLU \n            nn.ReLU(inplace=False)  \n        ) \n\n\nclass InvertedResidual(nn.Module):  \n    def __init__(self, inp, oup, stride, expand_ratio): \n        super(InvertedResidual, self).__init__()  \n        self.stride = stride  \n        assert stride in [1, 2] \n\n        hidden_dim = int(round(inp * expand_ratio)) \n        self.use_res_connect = self.stride == 1 and inp == oup  \n\n        layers = [] \n        if expand_ratio != 1: \n            # pw  \n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1)) \n        layers.extend([ \n            # dw  \n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim), \n            # pw-linear \n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),  \n            nn.BatchNorm2d(oup, momentum=0.1),  \n        ])  \n        self.conv = nn.Sequential(*layers)  \n        # Replace torch.add with floatfunctional  \n        self.skip_add = nn.quantized.FloatFunctional()  \n\n    def forward(self, x): \n        if self.use_res_connect:  \n            return self.skip_add.add(x, self.conv(x)) \n        else: \n            return self.conv(x) \n\n\nclass MobileNetV2(nn.Module): \n    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):  \n        \"\"\" \n        MobileNet V2 main class \n        Args: \n            num_classes (int): Number of classes  \n            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount  \n            inverted_residual_setting: Network structure  \n            round_nearest (int): Round the number of channels in each layer to be a multiple of this number \n            Set to 1 to turn off rounding \n        \"\"\" \n        super(MobileNetV2, self).__init__() \n        block = InvertedResidual  \n        input_channel = 32  \n        last_channel = 1280 \n\n        if inverted_residual_setting is None: \n            inverted_residual_setting = [ \n                # t, c, n, s  \n                [1, 16, 1, 1],  \n                [6, 24, 2, 2],  \n                [6, 32, 3, 2],  \n                [6, 64, 4, 2],  \n                [6, 96, 3, 1],  \n                [6, 160, 3, 2], \n                [6, 320, 1, 1], \n            ] \n\n        # only check the first element, assuming user knows t,c,n,s are required  \n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4: \n            raise ValueError(\"inverted_residual_setting should be non-empty \" \n                             \"or a 4-element list, got {}\".format(inverted_residual_setting)) \n\n        # building first layer  \n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)  \n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest) \n        features = [ConvBNReLU(3, input_channel, stride=2)] \n        # building inverted residual blocks \n        for t, c, n, s in inverted_residual_setting:  \n            output_channel = _make_divisible(c * width_mult, round_nearest) \n            for i in range(n):  \n                stride = s if i == 0 else 1 \n                features.append(block(input_channel, output_channel, stride, expand_ratio=t)) \n                input_channel = output_channel  \n        # building last several layers  \n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))  \n        # make it nn.Sequential \n        self.features = nn.Sequential(*features)  \n        self.quant = QuantStub()  \n        self.dequant = DeQuantStub()  \n        # building classifier \n        self.classifier = nn.Sequential(  \n            nn.Dropout(0.2),  \n            nn.Linear(self.last_channel, num_classes),  \n        ) \n\n        # weight initialization \n        for m in self.modules():  \n            if isinstance(m, nn.Conv2d):  \n                nn.init.kaiming_normal_(m.weight, mode='fan_out') \n                if m.bias is not None:  \n                    nn.init.zeros_(m.bias)  \n            elif isinstance(m, nn.BatchNorm2d): \n                nn.init.ones_(m.weight) \n                nn.init.zeros_(m.bias)  \n            elif isinstance(m, nn.Linear):  \n                nn.init.normal_(m.weight, 0, 0.01)  \n                nn.init.zeros_(m.bias)  \n\n    def forward(self, x): \n        x = self.quant(x) \n        x = self.features(x)  \n        x = x.mean([2, 3])  \n        x = self.classifier(x)  \n        x = self.dequant(x) \n        return x  \n\n    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization \n    # This operation does not change the numerics \n    def fuse_model(self, is_qat=False): \n        fuse_modules = torch.ao.quantization.fuse_modules_qat if is_qat else torch.ao.quantization.fuse_modules\n        for m in self.modules():  \n            if type(m) == ConvBNReLU: \n                fuse_modules(m, ['0', '1', '2'], inplace=True)\n            if type(m) == InvertedResidual: \n                for idx in range(len(m.conv)):  \n                    if type(m.conv[idx]) == nn.Conv2d:  \n                        fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Functions for Remote Method Calls in Python\nDESCRIPTION: This snippet defines helper functions for calling methods on remote objects using RPC and RRefs. These functions facilitate remote method invocation and are essential for distributed training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef call_method(method, rref, *args, **kwargs):\n    return method(rref.local_value(), *args, **kwargs)\n\ndef remote_method(method, rref, *args, **kwargs):\n    args = [method, rref] + list(args)\n    return rpc.rpc_sync(rref.owner(), call_method, args=args, kwargs=kwargs)\n```\n\n----------------------------------------\n\nTITLE: Comparing Model Sizes Before and After Quantization in PyTorch\nDESCRIPTION: This snippet defines a helper function `print_model_size` to calculate and print the size of a PyTorch model by saving its state dictionary to a temporary file and checking the file size. It then uses this function to compare the sizes of the original floating-point MobileNet v2 model and its quantized version, demonstrating the size reduction achieved through quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/quantization.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = torchvision.models.mobilenet_v2(pretrained=True)\n\nimport os\nimport torch\n\ndef print_model_size(mdl):\n    torch.save(mdl.state_dict(), \"tmp.pt\")\n    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n    os.remove('tmp.pt')\n\nprint_model_size(model)\nprint_model_size(model_quantized)\n```\n\n----------------------------------------\n\nTITLE: Initializing PatchPredictor with Pretrained or Custom Model - TIAToolbox - Python\nDESCRIPTION: This snippet demonstrates initializing a PatchPredictor instance, either with a pretrained model (by name) or by loading a custom PyTorch model and assigning a preprocessing function. To use a PyTorch model directly, you must construct the model, load weights, and attach a preprocessing function ensuring data formatting matches model requirements. Dependencies: tiatoolbox, torchvision (for model), PIL, torch, and transforms. Key parameters include model architecture, number of classes, batch size, and model weights. Expected inputs are a PyTorch model object (or name string), and outputs are a configured PatchPredictor instance ready for prediction.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Importing a pretrained PyTorch model from TIAToolbox \npredictor = PatchPredictor(pretrained_model='resnet18-kather100k', batch_size=32) \n\n# Users can load any PyTorch model architecture instead using the following script\nmodel = vanilla.CNNModel(backbone=\"resnet18\", num_classes=9) # Importing model from torchvision.models.resnet18\nmodel.load_state_dict(torch.load(weights_path, map_location=\"cpu\", weights_only=True), strict=True)\ndef preproc_func(img):\n    img = PIL.Image.fromarray(img)\n    img = transforms.ToTensor()(img)\n    return img.permute(1, 2, 0)\nmodel.preproc_func = preproc_func\npredictor = PatchPredictor(model=model, batch_size=32)\n```\n\n----------------------------------------\n\nTITLE: Implementing Polynomial Fitting with NumPy\nDESCRIPTION: A numpy implementation of fitting a third-order polynomial to a sine function. This example manually implements both the forward and backward passes through the network using numpy operations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/pytorch_with_examples.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport math\n\n# Create random input and output data\nnp.random.seed(42)\nx = np.random.randn(200, 1)\ny = np.sin(x)\n\n# Randomly initialize weights\na = np.random.randn()\nb = np.random.randn()\nc = np.random.randn()\nd = np.random.randn()\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    # y = a + b * x + c * x^2 + d * x^3\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = np.square(y_pred - y).sum()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\nprint(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')\n```\n\n----------------------------------------\n\nTITLE: Registering Autograd Backward Kernel for Custom Operator - PyTorch Python\nDESCRIPTION: Defines and registers backward and context setup functions for autograd (training) support of the custom operator 'mymuladd' using torch.library.register_autograd. The backward function uses saved tensors and checks which gradients are required, returning gradients in the correct order; the context setup function saves only the needed tensors. These require the operator to be registered prior and depend on PyTorch's autograd system, with expected inputs being tensors for forward and gradient for backward.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _backward(ctx, grad):\n    a, b = ctx.saved_tensors\n    grad_a, grad_b = None, None\n    if ctx.needs_input_grad[0]:\n        grad_a = grad * b\n    if ctx.needs_input_grad[1]:\n        grad_b = grad * a\n    return grad_a, grad_b, None\n\ndef _setup_context(ctx, inputs, output):\n    a, b, c = inputs\n    saved_a, saved_b = None, None\n    if ctx.needs_input_grad[0]:\n        saved_b = b\n    if ctx.needs_input_grad[1]:\n        saved_a = a\n    ctx.save_for_backward(saved_a, saved_b)\n\n# This code adds training support for the operator. You must provide us\n# the backward formula for the operator and a `setup_context` function\ntorch.library.register_autograd(\n    \"extension_cpp::mymuladd\", _backward, setup_context=_setup_context)\n\n```\n\n----------------------------------------\n\nTITLE: Loading Custom Operator Shared Library at Runtime - PyTorch Python\nDESCRIPTION: Demonstrates how to discover and load a compiled custom operator shared library ('.so') at runtime using torch.ops.load_library, ensuring registration of its operators. The script searches for files matching '_C*.so' in the package directory, checks there is exactly one, and then loads it. This method is useful when avoiding Python.h in C++ or '_C' modules, and relies on Python's pathlib and PyTorch's dynamic library loading.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom pathlib import Path\n\nso_files = list(Path(__file__).parent.glob(\"_C*.so\"))\nassert (\n    len(so_files) == 1\n), f\"Expected one _C*.so file, found {len(so_files)}\"\ntorch.ops.load_library(so_files[0])\n\nfrom . import ops\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom PyTorch Module Compatible with skip_init in Python\nDESCRIPTION: Defines a custom `MyModule` demonstrating how to make it compatible with `skip_init`. Key requirements shown are accepting a `device` kwarg in `__init__` and passing it correctly when creating parameters (`nn.Parameter`), buffers (`register_buffer`), and submodules (`nn.Linear`, `nn.Sequential`). It also illustrates safe initialization within the constructor using `torch.no_grad()` and `torch.nn.init` functions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/skip_param_init.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import nn\n\nclass MyModule(torch.nn.Module):\n  def __init__(self, foo, bar, device=None):\n    super().__init__()\n\n    # ==== Case 1: Module creates parameters directly. ====\n    # Pass device along to any created parameters.\n    self.param1 = nn.Parameter(torch.empty((foo, bar), device=device))\n    self.register_parameter('param2', nn.Parameter(torch.empty(bar, device=device)))\n\n    # To ensure support for the meta device, avoid using ops except those in\n    # torch.nn.init on parameters in your module's constructor.\n    with torch.no_grad():\n        nn.init.kaiming_uniform_(self.param1)\n        nn.init.uniform_(self.param2)\n\n\n    # ==== Case 2: Module creates submodules. ====\n    # Pass device along recursively. All submodules will need to support\n    # them as well; this is the case for all torch.nn provided modules.\n    self.fc = nn.Linear(bar, 5, device=device)\n\n    # This also works with containers.\n    self.linears = nn.Sequential(\n        nn.Linear(5, 5, device=device),\n        nn.Linear(5, 1, device=device)\n    )\n\n\n    # ==== Case 3: Module creates buffers. ====\n    # Pass device along during buffer tensor creation.\n    self.register_buffer('some_buffer', torch.ones(7, device=device))\n\n...\n```\n\n----------------------------------------\n\nTITLE: Using Compiled Autograd Context Manager for Backward Pass Configuration\nDESCRIPTION: Shows an alternative method to enable Compiled Autograd and apply specific compiler configurations for the backward pass using the `torch._dynamo.compiled_autograd.enable` context manager. This approach provides a scoped way to control Compiled Autograd, applying `torch.compile(fullgraph=True)` only to the `loss.backward()` call within the context.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/compiled_autograd_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train(model, x):\n   model = torch.compile(model)\n   loss = model(x).sum()\n   with torch._dynamo.compiled_autograd.enable(torch.compile(fullgraph=True)):\n      loss.backward()\n```\n\n----------------------------------------\n\nTITLE: Compiling a function with torch.compile in Python\nDESCRIPTION: This Python code snippet demonstrates how to use `torch.compile` to optimize a simple function `foo` for either CPU or XPU execution. It defines a function that performs sine and cosine operations on input tensors and then compiles it using `torch.compile`. The compiled function is then executed with random input tensors.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/inductor_windows.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\ndevice=\"cpu\" # or \"xpu\" for XPU\ndef foo(x, y):\n    a = torch.sin(x)\n    b = torch.cos(x)\n    return a + b\nopt_foo1 = torch.compile(foo)\nprint(opt_foo1(torch.randn(10, 10).to(device), torch.randn(10, 10).to(device)))\n```\n\n----------------------------------------\n\nTITLE: Converting a Calibrated Model to a Quantized Model with convert_pt2e - Python\nDESCRIPTION: After calibration, convert_pt2e is used to convert the prepared model into its quantized form. This function outputs the quantized model, which can be printed for inspection. It is the final step before deploying or benchmarking the quantized model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nquantized_model = convert_pt2e(prepared_model)\nprint(quantized_model)\n```\n\n----------------------------------------\n\nTITLE: Executing Environment Episode and Agent Interaction in Observer (Python)\nDESCRIPTION: Implements the `run_episode` method within the `Observer` class. This method interacts with the Gym environment for a fixed number of steps (`n_steps`). It prepares the state tensor, calls the agent's `select_action` function remotely using `rpc.rpc_sync` to get an action, steps the environment, calculates and accumulates rewards, computes discounted rewards (Return `R`) when the episode ends or `n_steps` are reached, and resets the environment if needed. Finally, it returns the calculated rewards and the episode duration to the calling agent. Note the use of nested RPC as this method is called by the Agent via RPC.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass Observer:\n    ...\n\n    def run_episode(self, agent_rref, n_steps):\n        state, ep_reward = self.env.reset(), NUM_STEPS # NUM_STEPS needs to be defined elsewhere\n        rewards = torch.zeros(n_steps)\n        start_step = 0\n        for step in range(n_steps):\n            state = torch.from_numpy(state).float().unsqueeze(0)\n            # send the state to the agent to get an action\n            action = rpc.rpc_sync(\n                agent_rref.owner(),\n                self.select_action,\n                args=(agent_rref, self.id, state)\n            )\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n            rewards[step] = reward\n\n            if done or step + 1 >= n_steps:\n                curr_rewards = rewards[start_step:(step + 1)]\n                R = 0\n                for i in range(curr_rewards.numel() -1, -1, -1):\n                    R = curr_rewards[i] + args.gamma * R\n                    curr_rewards[i] = R\n                state = self.env.reset()\n                if start_step == 0:\n                    ep_reward = min(ep_reward, step - start_step + 1)\n                start_step = step + 1\n\n        return [rewards, ep_reward]\n```\n\n----------------------------------------\n\nTITLE: Accessing Tensor Elements in CUDA Kernels Using Raw Pointer Arithmetic (C++)\nDESCRIPTION: This snippet shows how to index into a 3D tensor flattened for CUDA kernels using stride-based pointer arithmetic. It is necessary when using raw pointers in kernels instead of more abstract tensor access methods. Dependencies are knowledge of tensor dimensions and stride; improper calculation can lead to errors or incorrect results.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\ngates.data<scalar_t>()[n*3*state_size + row*state_size + column]\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Post-Training Quantization with Custom Metric\nDESCRIPTION: Python code for accuracy-driven post-training quantization with a custom-defined Top1Metric class. The metric tracks model accuracy by counting correct predictions and calculating the percentage over the entire dataset.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nmodel.eval()\n\nfrom torchvision import datasets, transforms\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data', train=False, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                   ])),\n    batch_size=1)\n\n# define a customized metric\nclass Top1Metric(object):\n    def __init__(self):\n        self.correct = 0\n    def update(self, output, label):\n        pred = output.argmax(dim=1, keepdim=True)\n        self.correct += pred.eq(label.view_as(pred)).sum().item()\n    def reset(self):\n        self.correct = 0\n    def result(self):\n        return 100. * self.correct / len(test_loader.dataset)\n\n# launch code for Intel® Neural Compressor\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./conf.yaml\")\nquantizer.model = model\nquantizer.calib_dataloader = test_loader\nquantizer.eval_dataloader = test_loader\nquantizer.metric = Top1Metric()\nq_model = quantizer()\nq_model.save('./output')\n```\n\n----------------------------------------\n\nTITLE: Optimizing PyTorch Model with IPEX for GPU (Python)\nDESCRIPTION: This snippet shows how to use Intel Extension for PyTorch (IPEX) to optimize a PyTorch model for GPU execution. It includes moving the model to the XPU device and applying IPEX optimization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom nn Module in PyTorch\nDESCRIPTION: A PyTorch implementation using a custom nn.Module subclass for the polynomial model. This demonstrates how to create custom modules by implementing the forward method to define the network's behavior.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/pytorch_with_examples.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Define our custom module for the polynomial model\nclass Polynomial3(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate four parameters and assign them as\n        member parameters.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Tensors.\n        \"\"\"\n        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n\n\n# Create an instance of our model\nmodel = Polynomial3()\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters (defined \n# with torch.nn.Parameter) which are members of the model.\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\nfor t in range(2000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')\n```\n\n----------------------------------------\n\nTITLE: Preparing a Model for Quantization Aware Training in PyTorch\nDESCRIPTION: This snippet outlines the process for Quantization Aware Training (QAT). First, a QAT-specific quantization configuration is assigned using `get_default_qat_qconfig` with the appropriate backend. Then, `torch.quantization.prepare_qat` modifies the model to insert fake quantization modules. The model is then trained (the actual training loop is represented by the comment). Finally, after training, the model is switched to evaluation mode (`.eval()`) and converted to a truly quantized model using `torch.quantization.convert`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/quantization.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel.qconfig = torch.quantization.get_default_qat_qconfig(backend)\nmodel_qat = torch.quantization.prepare_qat(model, inplace=False)\n# quantization aware training goes here\nmodel_qat = torch.quantization.convert(model_qat.eval(), inplace=False)\n```\n\n----------------------------------------\n\nTITLE: Reference Implementation: Reference Quantized Model Linear Operator in PyTorch - Python\nDESCRIPTION: Provides a more detailed reference implementation of a quantized linear layer using int16 accumulation, explicit bias scaling, and quantization clamping. Useful for benchmarking and backend development. Inputs include quantized tensors, scales, zero points, bias, and output specs; returns a quantized output tensor.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Reference Quantized Pattern for quantized linear\ndef quantized_linear(x_int8, x_scale, x_zero_point, weight_int8, weight_scale, weight_zero_point, bias_fp32, output_scale, output_zero_point):\n    x_int16 = x_int8.to(torch.int16)\n    weight_int16 = weight_int8.to(torch.int16)\n    acc_int32 = torch.ops.out_dtype(torch.mm, torch.int32, (x_int16 - x_zero_point), (weight_int16 - weight_zero_point))\n    bias_scale = x_scale * weight_scale\n    bias_int32 = out_dtype(torch.ops.aten.div.Tensor, torch.int32, bias_fp32, bias_scale)\n    acc_int32 = acc_int32 + bias_int32\n    acc_int32 = torch.ops.out_dtype(torch.ops.aten.mul.Scalar, torch.int32, acc_int32, x_scale * weight_scale / output_scale) + output_zero_point\n    out_int8 = torch.ops.aten.clamp(acc_int32, qmin, qmax).to(torch.int8)\n    return out_int8\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Quantized or Unquantized BERT Model with Huggingface Utilities (Python)\nDESCRIPTION: Defines an evaluation function that loads datasets, handles output directories, sets up data loaders, and iterates over tasks for model performance evaluation. It uses utilities from transformers for dataset loading and PyTorch for DataLoader creation. Inputs include model, tokenizer, and args with evaluation parameters, and the output is a results dictionary. Assumes dependencies and proper dataset/model setup.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(args, model, tokenizer, prefix=\"\"):\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n\n    results = {}\n    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        # Note that DistributedSampler samples randomly\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        # multi-gpu eval\n```\n\n----------------------------------------\n\nTITLE: Setting Number of PyTorch Threads for Performance Comparison - Python\nDESCRIPTION: Configures PyTorch to use only a single CPU thread and prints the parallel configuration to help benchmark inference performance of quantized versus standard models under comparable conditions. This ensures deterministic performance data. Prerequisite: PyTorch must be installed and imported as torch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntorch.set_num_threads(1)\nprint(torch.__config__.parallel_info())\n\n```\n\n----------------------------------------\n\nTITLE: Training Step Function for CUDA Graph Capture\nDESCRIPTION: Defines a separate training step function containing all compute operations that will be captured by CUDA Graph, including forward/backward passes and optimizer updates.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nvoid training_step(\n    Net& model,\n    torch::optim::Optimizer& optimizer,\n    torch::Tensor& data,\n    torch::Tensor& targets,\n    torch::Tensor& output,\n    torch::Tensor& loss) {\n  optimizer.zero_grad();\n  output = model.forward(data);\n  loss = torch::nll_loss(output, targets);\n  loss.backward();\n  optimizer.step();\n}\n```\n\n----------------------------------------\n\nTITLE: Serializing and Comparing Baseline Float Model - PyTorch - Python\nDESCRIPTION: Evaluates the size and accuracy of the baseline float model, then saves it using torch.jit.script. Scripted models are suitable for deployment and efficient inference. The code depends on float_model, an evaluation utility, and a criterion. Outputs accuracy and writes the serialized file to disk.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n    scripted_float_model_file = \"resnet18_scripted.pth\"\n\n    print(\"Size of baseline model\")\n    print_size_of_model(float_model)\n\n    top1, top5 = evaluate(float_model, criterion, data_loader_test)\n    print(\"Baseline Float Model Evaluation accuracy: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n    torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)\n```\n\n----------------------------------------\n\nTITLE: Exporting and Loading ONNX Models\nDESCRIPTION: Code for exporting PyTorch models to ONNX format and loading ONNX models. ONNX provides interoperability between different deep learning frameworks.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntorch.onnx.export(model, dummy data, xxxx.proto)       # exports an ONNX formatted  \n                                                       # model using a trained model, dummy\n                                                       # data and the desired file name\n\nmodel = onnx.load(\"alexnet.proto\")                     # load an ONNX model\nonnx.checker.check_model(model)                        # check that the model \n                                                       # IR is well formed  \n                    \nonnx.helper.printable_graph(model.graph)               # print a human readable \n                                                       # representation of the graph\n```\n\n----------------------------------------\n\nTITLE: Inference in Imperative Mode with Float32 using Intel® Extension for PyTorch*\nDESCRIPTION: This code demonstrates how to optimize a pre-trained PyTorch ResNet50 model for inference with Float32 precision using Intel® Extension for PyTorch* (IPEX) in imperative mode. It requires minimal code changes to enable performance optimizations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\nwith torch.no_grad():\n  model(data)\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Functions and Preparing Dataset for Quantized Model Evaluation (Python)\nDESCRIPTION: This snippet includes necessary imports, sets up warning filters, seeds the random generator, and defines the AverageMeter class to track metrics during model evaluation. It prepares the groundwork for loading and processing datasets (e.g., ImageNet) and evaluating quantized models. Dependencies are torch, torchvision, numpy, warnings, and the local data directory containing model and dataset files. AverageMeter tracks average and current values for various metrics, and is reusable in model validation loops.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    import os\n    import sys\n    import time\n    import numpy as np\n\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader\n\n    import torchvision\n    from torchvision import datasets\n    from torchvision.models.resnet import resnet18\n    import torchvision.transforms as transforms\n\n    # Set up warnings\n    import warnings\n    warnings.filterwarnings(\n        action='ignore',\n        category=DeprecationWarning,\n        module=r'.*'\n    )\n    warnings.filterwarnings(\n        action='default',\n        module=r'torch.ao.quantization'\n    )\n\n    # Specify random seed for repeatable results\n    _ = torch.manual_seed(191009)\n\n\n    class AverageMeter(object):\n        \"\"\"Computes and stores the average and current value\"\"\"\n        def __init__(self, name, fmt=':f'):\n            self.name = name\n            self.fmt = fmt\n            self.reset()\n\n        def reset(self):\n            self.val = 0\n            self.avg = 0\n            self.sum = 0\n            self.count = 0\n\n        def update(self, val, n=1):\n            self.val = val\n            self.sum += val * n\n            self.count += n\n\n```\n\n----------------------------------------\n\nTITLE: Calibrating the Prepared Model for Static PTQ in Python\nDESCRIPTION: Performs calibration on the prepared model by running it with example data (`example_inputs`). This step is necessary for static quantization to allow observers to collect statistics about activation ranges. Includes commented-out code showing how to calibrate using a custom data loader.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# We use the dummy data as an example here\nprepared_model(*example_inputs)\n\n# Alternatively: user can define the dataset to calibrate\n# def calibrate(model, data_loader):\n#     model.eval()\n#     with torch.no_grad():\n#         for image, target in data_loader:\n#             model(image)\n# calibrate(prepared_model, data_loader_test)  # run calibration on sample data\n```\n\n----------------------------------------\n\nTITLE: Worker Process Setup and Initialization\nDESCRIPTION: Sets up worker processes for distributed training, initializing both RPC framework and DDP process groups. Handles different roles including master, parameter server, and trainers.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/rpc_ddp_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef run_worker(rank, world_size):\n    if rank in [0, 1]:\n        run_trainer(rank, world_size)\n    elif rank == 2:\n        run_master()\n    else:\n        run_parameter_server()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking ResNet50 Inference Time in PyTorch (Python)\nDESCRIPTION: This Python script sets up a basic ResNet50 model for inference benchmarking. It initializes a pre-trained ResNet50 model (weights not loaded), sets it to evaluation mode, creates random input data tensor, performs 100 warm-up inference runs, times another 100 inference iterations, and prints the average inference time in milliseconds. This code serves as the baseline example used throughout the tutorial to demonstrate the impact of CPU configurations (like OMP_NUM_THREADS and core pinning) on performance. Dependencies include PyTorch (`torch`) and Torchvision (`torchvision.models`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchserve_with_ipex.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\nimport time\n\nmodel = models.resnet50(pretrained=False)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n# warm up\nfor _ in range(100):\n    model(data)\n\nstart = time.time()\nfor _ in range(100):\n    model(data)\nend = time.time()\nprint('Inference took {:.2f} ms in average'.format((end-start)/100*1000))\n```\n\n----------------------------------------\n\nTITLE: Implementing Mutable Custom Operator `myadd_out` in C++ for CPU\nDESCRIPTION: This C++ function `myadd_out_cpu` implements a custom operator that performs element-wise addition of two input tensors (`a` and `b`) and writes the result into a pre-allocated output tensor (`out`), thus mutating it. It includes checks for tensor size compatibility, data type (float), contiguity of the output tensor, and device type (CPU). It ensures input tensors are contiguous before accessing their data pointers and performs the addition loop.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n// An example of an operator that mutates one of its inputs.\nvoid myadd_out_cpu(const at::Tensor& a, const at::Tensor& b, at::Tensor& out) {\n  TORCH_CHECK(a.sizes() == b.sizes());\n  TORCH_CHECK(b.sizes() == out.sizes());\n  TORCH_CHECK(a.dtype() == at::kFloat);\n  TORCH_CHECK(b.dtype() == at::kFloat);\n  TORCH_CHECK(out.dtype() == at::kFloat);\n  TORCH_CHECK(out.is_contiguous());\n  TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CPU);\n  TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CPU);\n  TORCH_INTERNAL_ASSERT(out.device().type() == at::DeviceType::CPU);\n  at::Tensor a_contig = a.contiguous();\n  at::Tensor b_contig = b.contiguous();\n  const float* a_ptr = a_contig.data_ptr<float>();\n  const float* b_ptr = b_contig.data_ptr<float>();\n  float* result_ptr = out.data_ptr<float>();\n  for (int64_t i = 0; i < out.numel(); i++) {\n    result_ptr[i] = a_ptr[i] + b_ptr[i];\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Custom LLTM GPU Performance in Python with CUDA\nDESCRIPTION: Measures the execution time of the forward and backward passes of the custom `LLTM` module on a CUDA-enabled GPU. It ensures CUDA is available, creates tensors directly on the CUDA device, moves the `LLTM` module to the device using `.to(cuda_device)`, and uses `torch.cuda.synchronize()` after operations to ensure accurate timing. Requires PyTorch with CUDA support and the custom `LLTM` module.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nassert torch.cuda.is_available()\ncuda_device = torch.device(\"cuda\")  # device object representing GPU\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\n# Note the device=cuda_device arguments here\nX = torch.randn(batch_size, input_features, device=cuda_device)\nh = torch.randn(batch_size, state_size, device=cuda_device)\nC = torch.randn(batch_size, state_size, device=cuda_device)\n\nrnn = LLTM(input_features, state_size).to(cuda_device)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    torch.cuda.synchronize()\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    torch.cuda.synchronize()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5))\n```\n\n----------------------------------------\n\nTITLE: Sharing Ownership with std::shared_ptr for Modules in PyTorch C++\nDESCRIPTION: This code illustrates the use of std::shared_ptr for reference semantics in module passing, aligning module object handling with common Python practices. Dependencies include <memory> (for std::shared_ptr) and the PyTorch C++ API. The main function creates a module instance with std::make_shared and demonstrates passing shared module ownership into a function.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n  struct Net : torch::nn::Module {};\n\n  void a(std::shared_ptr<Net> net) { }\n\n  int main() {\n    auto net = std::make_shared<Net>();\n    a(net);\n  }\n```\n\n----------------------------------------\n\nTITLE: Fetching Global Parameter RRefs in TrainerNet using PyTorch RPC in Python\nDESCRIPTION: Implements the `get_global_param_rrefs` method within the `TrainerNet` class (partially shown with ellipsis). It uses `remote_method` (assumed to be `torch.distributed.rpc.remote_method`) to invoke the `get_param_rrefs` method (defined within the `ParameterServer` class) on the remote `ParameterServer` instance via its RRef (`self.param_server_rref`). This call retrieves and returns a list of RRefs pointing to the parameters stored on the server, which are necessary for initializing the `DistributedOptimizer`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass TrainerNet(nn.Module):\n...\n    def get_global_param_rrefs(self):\n        remote_params = remote_method(\n            ParameterServer.get_param_rrefs,\n            self.param_server_rref)\n        return remote_params\n```\n\n----------------------------------------\n\nTITLE: Implementing 2:4 Sparse Pruning for BERT with PyTorch\nDESCRIPTION: Code that sets up weight norm (magnitude) sparsification for the BERT model with 2:4 structured sparsity using the torch.ao.pruning package. It configures the sparsifier to apply to all linear layers in the BERT encoder with a specific sparsity pattern.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsparsifier = WeightNormSparsifier(\n    # apply sparsity to all blocks\n    sparsity_level=1.0,\n    # shape of 4 elemens is a block\n    sparse_block_shape=(1, 4),\n    # two zeros for every block of 4\n    zeros_per_block=2\n)\n\n# add to config if nn.Linear and in the BERT model.\nsparse_config = [\n    {\"tensor_fqn\": f\"{fqn}.weight\"}\n    for fqn, module in model.named_modules()\n    if isinstance(module, nn.Linear) and \"layer\" in fqn\n]\n```\n\n----------------------------------------\n\nTITLE: HuggingFace T5 Model Setup\nDESCRIPTION: Function to initialize the T5 model and tokenizer from HuggingFace pretrained models.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for C++ Build\nDESCRIPTION: CMake configuration file for building the C++ inference program. Specifies dependencies on LibTorch and sets up the build process.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torchscript_inference.rst#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(custom_ops)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(ts-infer ts-infer.cpp)\ntarget_link_libraries(ts-infer \"${TORCH_LIBRARIES}\")\nset_property(TARGET ts-infer PROPERTY CXX_STANDARD 11)\n```\n\n----------------------------------------\n\nTITLE: Checking torch.compile Device Support and Optimizer Initialization - PyTorch - Python\nDESCRIPTION: This snippet ensures the CUDA device meets the minimum compute capability requirement (>= 7.0) needed for torch.compile by checking via torch.cuda.get_device_capability(). If unsupported, the script exits. It then instantiates the Adam optimizer for all model parameters with a learning rate of 0.01. This prepares the environment and optimizer state for subsequent compilation and benchmarking.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/compiling_optimizer.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# exit cleanly if we are on a device that doesn't support torch.compile\\nif torch.cuda.get_device_capability() < (7, 0):\\n    print(\\\"Exiting because torch.compile is not supported on this device.\\\")\\n    import sys\\n    sys.exit(0)\\n\\n\\nopt = torch.optim.Adam(model.parameters(), lr=0.01)\n```\n\n----------------------------------------\n\nTITLE: Building and Running PyTorch C++ Model Inference Application - Shell\nDESCRIPTION: Demonstrates the shell commands required to build a C++ application with CMake and Make, then run the resulting executable passing the path to a traced TorchScript model (e.g., traced_resnet_model.pt). Ensures successful compilation and execution, and displays the successful output ('ok') that indicates the model was loaded correctly. Prerequisites: C++ compiler, Make, CMake, and a valid traced TorchScript model. Expected input is the path to the model file.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_export.rst#2025-04-22_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nroot@4b5a67132e81:/example-app# cd build\\nroot@4b5a67132e81:/example-app/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\\n-- The C compiler identification is GNU 5.4.0\\n-- The CXX compiler identification is GNU 5.4.0\\n-- Check for working C compiler: /usr/bin/cc\\n-- Check for working C compiler: /usr/bin/cc -- works\\n-- Detecting C compiler ABI info\\n-- Detecting C compiler ABI info - done\\n-- Detecting C compile features\\n-- Detecting C compile features - done\\n-- Check for working CXX compiler: /usr/bin/c++\\n-- Check for working CXX compiler: /usr/bin/c++ -- works\\n-- Detecting CXX compiler ABI info\\n-- Detecting CXX compiler ABI info - done\\n-- Detecting CXX compile features\\n-- Detecting CXX compile features - done\\n-- Looking for pthread.h\\n-- Looking for pthread.h - found\\n-- Looking for pthread_create\\n-- Looking for pthread_create - not found\\n-- Looking for pthread_create in pthreads\\n-- Looking for pthread_create in pthreads - not found\\n-- Looking for pthread_create in pthread\\n-- Looking for pthread_create in pthread - found\\n-- Found Threads: TRUE\\n-- Configuring done\\n-- Generating done\\n-- Build files have been written to: /example-app/build\\nroot@4b5a67132e81:/example-app/build# make\\nScanning dependencies of target example-app\\n[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\\n[100%] Linking CXX executable example-app\\n[100%] Built target example-app\n```\n\nLANGUAGE: sh\nCODE:\n```\nroot@4b5a67132e81:/example-app/build# ./example-app <path_to_model>/traced_resnet_model.pt\\nok\n```\n\nLANGUAGE: sh\nCODE:\n```\nroot@4b5a67132e81:/example-app/build# make\\nScanning dependencies of target example-app\\n[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\\n[100%] Linking CXX executable example-app\\n[100%] Built target example-app\\nroot@4b5a67132e81:/example-app/build# ./example-app traced_resnet_model.pt\\n-0.2698 -0.0381  0.4023 -0.3010 -0.0448\\n[ Variable[CPUFloatType]{1,5} ]\n```\n\n----------------------------------------\n\nTITLE: Performing Graph Mode Dynamic Quantization on a Traced BERT Model (Python)\nDESCRIPTION: This Python snippet calls quantize_dynamic_jit to perform graph mode dynamic quantization on the traced BERT model using the given qconfig_dict. Output is a quantized TorchScript model suitable for efficient inference on CPUs. It assumes 'traced_model' and 'qconfig_dict' are defined and imported from PyTorch, and expects the input model to be traced with the largest expected shape.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nquantized_model = quantize_dynamic_jit(traced_model, qconfig_dict)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLTM Backward Pass in C++\nDESCRIPTION: This code snippet implements the backward pass for the LLTM model in C++. It computes the gradients with respect to the inputs of the forward pass, including helper functions for derivative calculations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// tanh'(z) = 1 - tanh^2(z)\ntorch::Tensor d_tanh(torch::Tensor z) {\n  return 1 - z.tanh().pow(2);\n}\n\n// elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) < 0, else 0}\ntorch::Tensor d_elu(torch::Tensor z, torch::Scalar alpha = 1.0) {\n  auto e = z.exp();\n  auto mask = (alpha * (e - 1)) < 0;\n  return (z > 0).type_as(z) + mask.type_as(z) * (alpha * e);\n}\n\nstd::vector<torch::Tensor> lltm_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights) {\n  auto d_output_gate = torch::tanh(new_cell) * grad_h;\n  auto d_tanh_new_cell = output_gate * grad_h;\n  auto d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell;\n\n  auto d_old_cell = d_new_cell;\n  auto d_candidate_cell = input_gate * d_new_cell;\n  auto d_input_gate = candidate_cell * d_new_cell;\n\n  auto gates = gate_weights.chunk(3, /*dim=*/1);\n  d_input_gate *= d_sigmoid(gates[0]);\n  d_output_gate *= d_sigmoid(gates[1]);\n  d_candidate_cell *= d_elu(gates[2]);\n\n  auto d_gates =\n      torch::cat({d_input_gate, d_output_gate, d_candidate_cell}, /*dim=*/1);\n\n  auto d_weights = d_gates.t().mm(X);\n  auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true);\n\n  auto d_X = d_gates.mm(weights);\n  const auto state_size = grad_h.size(1);\n  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);\n  auto d_input = d_X.slice(/*dim=*/1, state_size);\n\n  return {d_old_h, d_input, d_weights, d_bias, d_old_cell};\n}\n```\n\n----------------------------------------\n\nTITLE: Loading, Fusing, and Configuring a Model for QAT in PyTorch\nDESCRIPTION: This Python code snippet demonstrates preparing a model for Quantization-Aware Training (QAT) using PyTorch. It loads a floating-point model, fuses compatible modules using `qat_model.fuse_model(is_qat=True)`, defines an SGD optimizer, and sets the quantization configuration (`qconfig`) specifically for QAT using `torch.ao.quantization.get_default_qat_qconfig('x86')`. Requires a `load_model` function and saved model file.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nqat_model = load_model(saved_model_dir + float_model_file)  \nqat_model.fuse_model(is_qat=True)  \n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001) \n# The old 'fbgemm' is still available but 'x86' is the recommended default. \nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n```\n\n----------------------------------------\n\nTITLE: Initializing Device Mesh for Tensor Parallel\nDESCRIPTION: Sets up a DeviceMesh that connects 8 GPUs within a host for Tensor Parallel training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.device_mesh import init_device_mesh\n\ntp_mesh = init_device_mesh(\"cuda\", (8,))\n```\n\n----------------------------------------\n\nTITLE: Executing PyTorch Model Inference Loop with FPS Logging in Python\nDESCRIPTION: This Python snippet demonstrates a loop for continuously reading frames from a source (like a camera via `cap`), preprocessing them, running inference using a PyTorch model (`net`) within a `torch.no_grad()` context for efficiency, and logging the frames per second (FPS). It requires a capture object (`cap`), a preprocessing function (`preprocess`), and a loaded PyTorch model (`net`). The loop converts image color space (BGR to RGB), creates a batch, performs inference, and calculates/prints FPS periodically.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nframe_count = 0\n\nwith torch.no_grad():\n    while True:\n        # read frame\n        ret, image = cap.read()\n        if not ret:\n            raise RuntimeError(\"failed to read frame\")\n\n        # convert opencv output from BGR to RGB\n        image = image[:, :, [2, 1, 0]]\n        permuted = image\n\n        # preprocess\n        input_tensor = preprocess(image)\n\n        # create a mini-batch as expected by the model\n        input_batch = input_tensor.unsqueeze(0)\n\n        # run model\n        output = net(input_batch)\n        # do something with output ...\n\n        # log model performance\n        frame_count += 1\n        now = time.time()\n        if now - last_logged > 1:\n            print(f\"{frame_count / (now-last_logged)} fps\")\n            last_logged = now\n            frame_count = 0\n```\n\n----------------------------------------\n\nTITLE: Compiling Forward/Backward Separately using Lambda and torch.compile\nDESCRIPTION: Illustrates compiling the forward and backward passes separately, allowing different configurations. The model's forward pass is compiled first. Then, Compiled Autograd is enabled globally, and the backward pass (`loss.backward()`) is explicitly compiled using a lambda function passed to `torch.compile`, specifying `fullgraph=True` specifically for the backward compilation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/compiled_autograd_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef train(model, x):\n    model = torch.compile(model)\n    loss = model(x).sum()\n    torch._dynamo.config.compiled_autograd = True\n    torch.compile(lambda: loss.backward(), fullgraph=True)()\n```\n\n----------------------------------------\n\nTITLE: Inference in TorchScript Mode with BFloat16 using Intel® Extension for PyTorch*\nDESCRIPTION: This code demonstrates how to set up a pre-trained BERT model for inference with BFloat16 mixed precision using Intel® Extension for PyTorch* (IPEX) in TorchScript mode. It shows initialization of the model and data setup (but the script is incomplete).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import BertModel\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\n```\n\n----------------------------------------\n\nTITLE: Initializing TrainerNet and DistributedOptimizer in PyTorch Training Loop (Python)\nDESCRIPTION: Defines the initial setup part of the `run_training_loop` function. It instantiates the `TrainerNet`, retrieves the list of remote parameter references (RRefs) by calling `net.get_global_param_rrefs()`, and then initializes the `torch.distributed.optim.DistributedOptimizer`. The optimizer is configured with the fetched parameter RRefs and a local optimizer algorithm (here, `torch.optim.SGD` with a learning rate of 0.03). This setup prepares the distributed training environment for a worker.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef run_training_loop(rank, num_gpus, train_loader, test_loader):\n    # Runs the typical nueral network forward + backward + optimizer step, but\n    # in a distributed fashion.\n    net = TrainerNet(num_gpus=num_gpus)\n    # Build DistributedOptimizer.\n    param_rrefs = net.get_global_param_rrefs()\n    opt = DistributedOptimizer(optim.SGD, param_rrefs, lr=0.03)\n```\n\n----------------------------------------\n\nTITLE: Deploying Model to Vertex AI Endpoint\nDESCRIPTION: Creates a Vertex AI endpoint and deploys the model with specified compute resources including NVIDIA Tesla P100 GPU.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torchserve_vertexai_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nendpoint = aiplatform.Endpoint.create(display_name=ENDPOINT_DISPLAY_NAME)\n\nmodel.deploy(\n    endpoint=endpoint,\n    deployed_model_display_name=MODEL_DISPLAY_NAME,\n    machine_type=\"n1-standard-8\",\n    accelerator_type=\"NVIDIA_TESLA_P100\",\n    accelerator_count=1,\n    traffic_percentage=100,\n    deploy_request_timeout=1200,\n    sync=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Model to Evaluation Mode in PyTorch - Python\nDESCRIPTION: Sets the model instance used for quantization to evaluation mode, ensuring that layers such as dropout and batch normalization behave consistently during post training quantization. It is a required step before export or calibration. The function call takes no parameters and returns no value, modifying the model in-place.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_to_quantize.eval()\n```\n\n----------------------------------------\n\nTITLE: Implementing RNN Model with Distributed Components in PyTorch\nDESCRIPTION: Creates an RNN model with embedding table and decoder on a parameter server, and LSTM locally. Uses PyTorch's RPC framework to create remote objects and communicate between components during the forward pass.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass RNNModel(nn.Module):\n    def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(RNNModel, self).__init__()\n\n        # setup embedding table remotely\n        self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout))\n        # setup LSTM locally\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n        # setup decoder remotely\n        self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout))\n\n    def forward(self, input, hidden):\n        # pass input to the remote embedding table and fetch emb tensor back\n        emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)\n        output, hidden = self.rnn(emb, hidden)\n        # pass output to the rremote decoder and get the decoded output back\n        decoded = _remote_method(Decoder.forward, self.decoder_rref, output)\n        return decoded, hidden\n```\n\n----------------------------------------\n\nTITLE: Activating Max-Autotune Compilation with PyTorch Inductor (Python)\nDESCRIPTION: Demonstrates how to enable max-autotune mode in the PyTorch Inductor CPU backend for a neural network containing a linear layer followed by ReLU. Relies on the 'torch' package with the Inductor backend and uses 'torch.no_grad' and 'torch.cpu.amp.autocast' to ensure inference mode and automatic mixed precision. Requires setting specific environment variables ('TORCHINDUCTOR_FREEZING=1') and using the config interface to log autotuning results, ensuring the model weights are frozen and both compile/inference occur inside a no_grad context. Input is a random tensor; output is the model's computed predictions after compilation and autotuned backend selection.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/max_autotune_on_CPU_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch._inductor import config\nconfig.trace.log_autotuning_results = True # enable the log of autotuning results\n\nclass M(torch.nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        bias,\n        **kwargs,\n    ):\n        super().__init__()\n        self.linear = torch.nn.Linear(\n            in_features,\n            out_features,\n            bias,\n            **kwargs,\n        )\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.relu(x)\n        return x\n\namp_enabled = True\nbatch_size = 64\nin_features = 16\nout_features = 32\nbias = True\n\nx = torch.randn(batch_size, in_features)\nmodel = M(in_features, out_features, bias)\n\nwith torch.no_grad(), torch.cpu.amp.autocast(enabled=amp_enabled):\n    compiled = torch.compile(model, mode=\"max-autotune\") # turn on \"max-autotune\" mode\n    y = compiled(x)\n\n```\n\n----------------------------------------\n\nTITLE: FSDP Main Training Setup\nDESCRIPTION: Main function that sets up FSDP training environment, including data loading, model wrapping, and training initialization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef fsdp_main(rank, world_size, args):\n    setup(rank, world_size)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    dataset1 = datasets.MNIST('../data', train=True, download=True,\n                        transform=transform)\n    dataset2 = datasets.MNIST('../data', train=False,\n                        transform=transform)\n\n    sampler1 = DistributedSampler(dataset1, rank=rank, num_replicas=world_size, shuffle=True)\n    sampler2 = DistributedSampler(dataset2, rank=rank, num_replicas=world_size)\n\n    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n    cuda_kwargs = {'num_workers': 2,\n                    'pin_memory': True,\n                    'shuffle': False}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n    my_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=100\n    )\n    torch.cuda.set_device(rank)\n    init_start_event = torch.cuda.Event(enable_timing=True)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Optimizer with DistributedOptimizer\nDESCRIPTION: Shows how to register the custom functional optimizer with PyTorch's DistributedOptimizer system. This allows the distributed training framework to use the TorchScript-compatible implementation automatically.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_optim_torchscript.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.optim import DistributedOptimizer\n\nDistributedOptimizer.functional_optim_map[QHM] = FunctionalQHM\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Operator and Backward Kernel in PyTorch C++ Backend\nDESCRIPTION: This C++ snippet illustrates how to implement a backend-specific operator and its backward kernel, and register them with PyTorch's dispatcher using the TORCH_LIBRARY_IMPL macro. The my_op2_backward function must be filled with backend logic to align with PyTorch's autograd behavior, while the registrations connect your kernels to the PrivateUse1 dispatch key. Replace <schema_my_op2> and <schema_my_op2_backward> with actual operator schemas as per PyTorch conventions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/extend_dispatcher.rst#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nTensor my_op2_backward(const Tensor& self, const Tensor& other) {\n  // call your backend-specific APIs to implement my_op2_backward so that\n  // it matches PyTorch's native behavior\n}\n\n// Note backward kernel is still registered to PrivateUse1 instead of AutogradPrivateUse1.\n// PyTorch will wrap your backward kernel with proper autograd setup and then link to it in\n// my_op2's AutogradPrivateUse1 kernel.\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(<schema_my_op2>, &my_op2);\n  m.impl(<schema_my_op2_backward>, &my_op2_backward);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Registering Complex Autograd Backward Kernel Using Custom Operator - PyTorch Python\nDESCRIPTION: Shows a backward function for autograd that leverages a custom-registered C++ operator 'mymul' through torch.ops.extension_cpp.mymul.default, improving maintainability and extensibility (e.g., supporting device dispatch). The backward and context setup functions ensure proper gradient propagation for the hybrid CPU/CUDA kernel scenario. All operator registrations must be set up before using this.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef _backward(ctx, grad):\n    a, b = ctx.saved_tensors\n    grad_a, grad_b = None, None\n    if ctx.needs_input_grad[0]:\n        grad_a = torch.ops.extension_cpp.mymul.default(grad, b)\n    if ctx.needs_input_grad[1]:\n        grad_b = torch.ops.extension_cpp.mymul.default(grad, a)\n    return grad_a, grad_b, None\n\n\ndef _setup_context(ctx, inputs, output):\n    a, b, c = inputs\n    saved_a, saved_b = None, None\n    if ctx.needs_input_grad[0]:\n        saved_b = b\n    if ctx.needs_input_grad[1]:\n        saved_a = a\n    ctx.save_for_backward(saved_a, saved_b)\n\n```\n\n----------------------------------------\n\nTITLE: Preparing Model for Quantization-Aware Training\nDESCRIPTION: Code snippet for preparing a model for Quantization-Aware Training by inserting fake quantizes and performing appropriate QAT fusions. This transforms the exported model into a form suitable for QAT.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprepared_model = prepare_qat_pt2e(exported_model, quantizer)\nprint(prepared_model)\n```\n\n----------------------------------------\n\nTITLE: Defining a Multi-GPU Model with Model Parallelism - PyTorch - Python\nDESCRIPTION: Implements a toy model (ToyMpModel) utilizing model parallelism by placing its submodules on different GPUs (dev0, dev1). The forward pass moves data between devices as necessary, enabling computation across multiple GPUs. This class is designed to be wrapped by DistributedDataParallel (DDP) to leverage both data and model parallelism for large-scale training. Requires torch, torch.nn, and at least two CUDA-capable devices. Inputs are batched tensors; outputs are batch-computed results on dev1.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)\n```\n\n----------------------------------------\n\nTITLE: Applying IPEX Graph Optimization using TorchScript\nDESCRIPTION: This Python snippet builds upon the previous example. After applying IPEX operator optimization (`ipex.optimize`), it further optimizes the model's graph using TorchScript. It traces the model with example data using `torch.jit.trace` within a `torch.no_grad()` context and then freezes the resulting TorchScript module using `torch.jit.freeze`. This enables graph-level fusions (like Conv+ReLU) for better performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchserve_with_ipex_2.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x \n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\n# torchscript \nwith torch.no_grad():\n    model = torch.jit.trace(model, data)\n    model = torch.jit.freeze(model)\n```\n\n----------------------------------------\n\nTITLE: Simulating Data Parallel Sharding with PyTorch SPMD Simulation (Python)\nDESCRIPTION: This snippet illustrates simulation of the data parallel sharding mode in embedding table placement using the spmd_sharing_simulation function and ShardingType.DATA_PARALLEL. In data parallel mode, all devices receive full copies of each table, which is suitable for balanced workloads but increases memory usage. Requires the same infrastructural dependencies as other snippets and is relevant for comparison to sharded modes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/sharding.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nspmd_sharing_simulation(ShardingType.DATA_PARALLEL)\n```\n\n----------------------------------------\n\nTITLE: Exporting MViT Video Classifier with Dynamic Batch Size\nDESCRIPTION: This snippet shows the solution to the static batch size issue by specifying a dynamic batch dimension during export. It allows the exported model to handle different batch sizes within a specified range.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_export_challenges_solutions.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom torchvision.models.video import MViT_V1_B_Weights, mvit_v1_b\nimport traceback as tb\n\n\nmodel = mvit_v1_b(weights=MViT_V1_B_Weights.DEFAULT)\n\n# Create a batch of 2 videos, each with 16 frames of shape 224x224x3.\ninput_frames = torch.randn(2,16, 224, 224, 3)\n\n# Transpose to get [1, 3, num_clips, height, width].\ninput_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n\n# Export the model.\nbatch_dim = torch.export.Dim(\"batch\", min=2, max=16)\nexported_program = torch.export.export(\n    model,\n    (input_frames,),\n    # Specify the first dimension of the input x as dynamic\n    dynamic_shapes={\"x\": {0: batch_dim}},\n)\n\n# Create a batch of 4 videos, each with 16 frames of shape 224x224x3.\ninput_frames = torch.randn(4,16, 224, 224, 3)\ninput_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\ntry:\n    exported_program.module()(input_frames)\nexcept Exception:\n    tb.print_exc()\n```\n\n----------------------------------------\n\nTITLE: Serializing Quantized BERT Model in Python\nDESCRIPTION: This code demonstrates how to serialize and save a quantized BERT model using torch.jit.trace and torch.jit.save. It creates dummy input tensors for tracing the model before saving.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef ids_tensor(shape, vocab_size):\n    #  Creates a random int32 tensor of the shape within the vocab size\n    return torch.randint(0, vocab_size, shape=shape, dtype=torch.int, device='cpu')\n\ninput_ids = ids_tensor([8, 128], 2)\ntoken_type_ids = ids_tensor([8, 128], 2)\nattention_mask = ids_tensor([8, 128], vocab_size=2)\ndummy_input = (input_ids, attention_mask, token_type_ids)\ntraced_model = torch.jit.trace(quantized_model, dummy_input)\ntorch.jit.save(traced_model, \"bert_traced_eager_quant.pt\")\n```\n\n----------------------------------------\n\nTITLE: Asynchronously Saving FSDP Distributed Checkpoints with PyTorch DCP in Python\nDESCRIPTION: This snippet demonstrates performing asynchronous distributed checkpoint saving in a multi-GPU PyTorch training job with torch.distributed.checkpoint.async_save. It defines an AppState wrapper to manage state_dict operations for both model and optimizer using the Stateful protocol to ensure compatibility with DCP. The code sets up distributed processes, constructs a simple neural network (ToyModel), wraps it in FSDP, and coordinates checkpointing across devices with checkpoint requests managed by future objects to limit memory usage. Prerequisites: PyTorch v2.4.0+, NCCL backend support, multi-GPU availability. Inputs: model/optimizer states; Outputs: asynchronously saved checkpoints in checkpoint directories. Key parameters include world_size, rank, and checkpoint_id.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_async_checkpoint_recipe.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.distributed.checkpoint as dcp\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\nfrom torch.distributed.checkpoint.stateful import Stateful\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass AppState(Stateful):\n    \"\"\"This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n    with the Stateful protocol, DCP will automatically call state_dict/load_stat_dict as needed in the\n    dcp.save/load APIs.\n\n    Note: We take advantage of this wrapper to hande calling distributed state dict methods on the model\n    and optimizer.\n    \"\"\"\n\n    def __init__(self, model, optimizer=None):\n        self.model = model\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        # this line automatically manages FSDP FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n        model_state_dict, optimizer_state_dict = get_state_dict(model, optimizer)\n        return {\n            \"model\": model_state_dict,\n            \"optim\": optimizer_state_dict\n        }\n\n    def load_state_dict(self, state_dict):\n        # sets our state dicts on the model and optimizer, now that we've loaded\n        set_state_dict(\n            self.model,\n            self.optimizer,\n            model_state_dict=state_dict[\"model\"],\n            optim_state_dict=state_dict[\"optim\"]\n        )\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355 \"\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef run_fsdp_checkpoint_save_example(rank, world_size):\n    print(f\"Running basic FSDP checkpoint saving example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create a model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    model = FSDP(model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n    checkpoint_future = None\n    for step in range(10):\n        optimizer.zero_grad()\n        model(torch.rand(8, 16, device=\"cuda\")).sum().backward()\n        optimizer.step()\n\n        # waits for checkpointing to finish if one exists, avoiding queuing more then one checkpoint request at a time\n        if checkpoint_future is not None:\n            checkpoint_future.result()\n\n        state_dict = { \"app\": AppState(model, optimizer) }\n        checkpoint_future = dcp.async_save(state_dict, checkpoint_id=f\"{CHECKPOINT_DIR}_step{step}\")\n\n    cleanup()\n\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    print(f\"Running async checkpoint example on {world_size} devices.\")\n    mp.spawn(\n        run_fsdp_checkpoint_save_example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True,\n    )\n\n```\n\n----------------------------------------\n\nTITLE: Preparing the Exported Model for PTQ in Python\nDESCRIPTION: Uses the `prepare_pt2e` function to prepare the exported FX graph model for quantization. This function applies transformations like folding BatchNorm layers and inserting observers based on the provided `X86InductorQuantizer` configuration.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprepared_model = prepare_pt2e(exported_model, quantizer)\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Quantization and BackendConfig Modules in Python\nDESCRIPTION: This snippet imports required modules from torch and torch.ao for quantization, including observer and configuration classes, BackendConfig, and FX graph mode quantization utilities. It establishes dependencies needed for all subsequent configuration and quantization steps. Ensure PyTorch is installed with the 'torch.ao.quantization' subpackage available.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.ao.quantization import (\n    default_weight_observer,\n    get_default_qconfig_mapping,\n    MinMaxObserver,\n    QConfig,\n    QConfigMapping,\n)\nfrom torch.ao.quantization.backend_config import (\n    BackendConfig,\n    BackendPatternConfig,\n    DTypeConfig,\n    DTypeWithConstraints,\n    ObservationType,\n)\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Deployed Model\nDESCRIPTION: Demonstrates how to make online predictions using the deployed endpoint and process the response.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torchserve_vertexai_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninstances = [{\"prompt\": \"An examplePup dog with a baseball jersey.\"}]\nresponse = endpoint.predict(instances=instances)\n\nwith open(\"img.jpg\", \"wb\") as g:\n    g.write(base64.b64decode(response.predictions[0]))\n\ndisplay.Image(\"img.jpg\")\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Image Frames for MobileNetV2 (Python)\nDESCRIPTION: Defines a sequence of torchvision transforms to prepare captured image frames for input into the MobileNetV2 model. It first converts the image (expected as a PIL Image or NumPy array) into a PyTorch tensor with Channel-Height-Width (CHW) format, then normalizes the tensor's pixel values using the standard ImageNet mean and standard deviation. Finally, it adds a batch dimension (unsqueeze) to make the tensor compatible with model input requirements ([C, H, W] -> [1, C, H, W]).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import transforms\n\npreprocess = transforms.Compose([\n    # convert the frame to a CHW torch tensor for training\n    transforms.ToTensor(),\n    # normalize the colors to the range that mobilenet_v2/3 expect\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(image)\n# The model can handle multiple images simultaneously so we need to add an\n# empty dimension for the batch.\n# [3, 224, 224] -> [1, 3, 224, 224]\ninput_batch = input_tensor.unsqueeze(0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Transformer Model for Pipeline Parallelism in Python\nDESCRIPTION: Defines a transformer model class with configurable architecture parameters including embedding dimension, number of layers, heads, and vocabulary size. The model includes token embeddings, transformer decoder layers, layer normalization, and output projection.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/pipelining_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nfrom dataclasses import dataclass\n\n@dataclass\nclass ModelArgs:\n   dim: int = 512\n   n_layers: int = 8\n   n_heads: int = 8\n   vocab_size: int = 10000\n\nclass Transformer(nn.Module):\n   def __init__(self, model_args: ModelArgs):\n      super().__init__()\n\n      self.tok_embeddings = nn.Embedding(model_args.vocab_size, model_args.dim)\n\n      # Using a ModuleDict lets us delete layers witout affecting names,\n      # ensuring checkpoints will correctly save and load.\n      self.layers = torch.nn.ModuleDict()\n      for layer_id in range(model_args.n_layers):\n            self.layers[str(layer_id)] = nn.TransformerDecoderLayer(model_args.dim, model_args.n_heads)\n\n      self.norm = nn.LayerNorm(model_args.dim)\n      self.output = nn.Linear(model_args.dim, model_args.vocab_size)\n\n   def forward(self, tokens: torch.Tensor):\n      # Handling layers being 'None' at runtime enables easy pipeline splitting\n      h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\n\n      for layer in self.layers.values():\n            h = layer(h, h)\n\n      h = self.norm(h) if self.norm else h\n      output = self.output(h).clone() if self.output else h\n      return output\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries, Loading Model, and Capturing FX Graph in Python\nDESCRIPTION: Imports necessary PyTorch libraries for quantization and model handling. Loads a pre-trained ResNet18 model, sets it to evaluation mode, creates dummy input data, and captures the FX graph using `capture_pre_autograd_graph`. This exported model is the input for the quantization process. Note: Mentions alternative `torch._dynamo.export` for older PyTorch 2.1 versions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\nimport copy\nfrom torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\nimport torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq\nfrom torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer\nfrom torch._export import capture_pre_autograd_graph\n\n# Create the Eager Model\nmodel_name = \"resnet18\"\nmodel = models.__dict__[model_name](pretrained=True)\n\n# Set the model to eval mode\nmodel = model.eval()\n\n# Create the data, using the dummy data here as an example\ntraced_bs = 50\nx = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)\nexample_inputs = (x,)\n\n# Capture the FX Graph to be quantized\nwith torch.no_grad():\n     # if you are using the PyTorch nightlies or building from source with the pytorch master,\n    # use the API of `capture_pre_autograd_graph`\n    # Note 1: `capture_pre_autograd_graph` is also a short-term API, it will be updated to use the official `torch.export` API when that is ready.\n    exported_model = capture_pre_autograd_graph(\n        model,\n        example_inputs\n    )\n    # Note 2: if you are using the PyTorch 2.1 release binary or building from source with the PyTorch 2.1 release branch,\n    # please use the API of `torch._dynamo.export` to capture the FX Graph.\n    # exported_model, guards = torch._dynamo.export(\n    #     model,\n    #     *copy.deepcopy(example_inputs),\n    #     aten_graph=True,\n    # )\n```\n\n----------------------------------------\n\nTITLE: Setting Up Distributed Training\nDESCRIPTION: Imports for distributed training and multiprocessing in PyTorch, which enable parallel computation across multiple GPUs or machines.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch.distributed as dist             # distributed communication\nfrom torch.multiprocessing import Process    # memory sharing processes\n```\n\n----------------------------------------\n\nTITLE: Compiling and Running the Quantized Model with TorchInductor in Python\nDESCRIPTION: Compiles the converted quantized model using `torch.compile`, which lowers the model to the Inductor backend (potentially using the C++ wrapper if enabled). Then, runs the optimized model with example inputs to demonstrate its usage or for benchmarking.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    optimized_model = torch.compile(converted_model)\n\n    # Running some benchmark\n    optimized_model(*example_inputs)\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Class for Distributed RL Training\nDESCRIPTION: Agent class that manages distributed training, handles action selection and reward collection from multiple observers. Includes policy network initialization and optimization setup.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport gym\nimport numpy as np\n\nimport torch\nimport torch.distributed.rpc as rpc\nimport torch.optim as optim\nfrom torch.distributed.rpc import RRef, rpc_async, remote\nfrom torch.distributions import Categorical\n\nclass Agent:\n    def __init__(self, world_size):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.saved_log_probs = {}\n        self.policy = Policy()\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.eps = np.finfo(np.float32).eps.item()\n        self.running_reward = 0\n        self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(remote(ob_info, Observer))\n            self.rewards[ob_info.id] = []\n            self.saved_log_probs[ob_info.id] = []\n```\n\n----------------------------------------\n\nTITLE: Comparing Batch and Non-Batch Processing Performance in Python\nDESCRIPTION: This main function runs the distributed reinforcement learning setup with varying numbers of observers and compares the performance of batch and non-batch processing modes. It demonstrates how to measure and analyze the impact of batching on execution time.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    for world_size in range(2, 12):\n        delays = []\n        for batch in [True, False]:\n            tik = time.time()\n            mp.spawn(\n                run_worker,\n                args=(world_size, args.num_episode, batch),\n                nprocs=world_size,\n                join=True\n            )\n            tok = time.time()\n            delays.append(tok - tik)\n\n        print(f\"{world_size}, {delays[0]}, {delays[1]}\")\n\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Skipping PyTorch Module Initialization using skip_init in Python\nDESCRIPTION: This snippet shows how to use `torch.nn.utils.skip_init` to instantiate an `nn.Linear` module without performing the default parameter initialization. This avoids wasted computation and allows for direct application of custom initialization schemes like `nn.init.orthogonal_` afterwards. Requires `torch.nn` and `torch.nn.utils.skip_init`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/skip_param_init.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn\nfrom torch.nn.utils import skip_init\n\nm = skip_init(nn.Linear, 10, 5)\n\n# Example: Do custom, non-default parameter initialization.\nnn.init.orthogonal_(m.weight)\n```\n\n----------------------------------------\n\nTITLE: Setup Script for C++ Extension\nDESCRIPTION: Python setup.py script to build the C++ extension using setuptools and torch.utils.cpp_extension.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import setup, Extension\nfrom torch.utils import cpp_extension\n\nsetup(name='lltm_cpp',\n      ext_modules=[cpp_extension.CppExtension('lltm_cpp', ['lltm.cpp'])],\n      cmdclass={'build_ext': cpp_extension.BuildExtension})\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorch Data Pipeline and Helper Functions\nDESCRIPTION: Sets up the data pipeline for Fashion-MNIST dataset including imports, transforms, dataloaders and helper functions for visualization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tensorboard_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# transforms\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# datasets\ntrainset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=True,\n    transform=transform)\ntestset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=False,\n    transform=transform)\n\n# dataloaders\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                        shuffle=True, num_workers=2)\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                        shuffle=False, num_workers=2)\n\n# constant for classes\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# helper function to show an image\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n```\n\n----------------------------------------\n\nTITLE: Initializing and Testing a Custom Backend in PyTorch Distributed - Python\nDESCRIPTION: This Python snippet demonstrates the initialization and usage of a custom distributed backend ('dummy_collectives') in PyTorch. After environment variable setup and importing the extension, the code shows how to initialize the process group with a composite backend string to dispatch collectives on CPU and CUDA tensors to different backends. It performs an all_reduce collective on both CPU and CUDA (if available), showing exception handling for unsupported collectives. Dependencies include the installed 'dummy_collectives' extension, PyTorch, and access to required backend libraries. The key parameters are the backend string, rank, and world_size, controlling process group setup and dispatch behavior.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\n# importing dummy_collectives makes torch.distributed recognize `dummy`\n# as a valid backend.\nimport dummy_collectives\n\nimport torch.distributed as dist\n\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '29500'\n\n# Alternatively:\n# dist.init_process_group(\"dummy\", rank=0, world_size=1)\ndist.init_process_group(\"cpu:gloo,cuda:dummy\", rank=0, world_size=1)\n\n# this goes through gloo\nx = torch.ones(6)\ndist.all_reduce(x)\nprint(f\"cpu allreduce: {x}\")\n\n# this goes through dummy\nif torch.cuda.is_available():\n    y = x.cuda()\n    dist.all_reduce(y)\n    print(f\"cuda allreduce: {y}\")\n\n    try:\n        dist.broadcast(y, 0)\n    except RuntimeError:\n        print(\"got RuntimeError when calling broadcast\")\n\n```\n\n----------------------------------------\n\nTITLE: Lowering Quantized Model into TorchInductor Backend (Python)\nDESCRIPTION: This snippet uses torch.compile within a no_grad() context to further optimize the quantized model for execution on an Intel GPU backend. The compiled model can immediately be run for performance benchmarking or deployment. This step is necessary to generate efficient backend code using Intel-specific kernels via TorchInductor. Dependency: torch (torch.compile).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    optimized_model = torch.compile(converted_model)\n\n    # Running some benchmark\n    optimized_model(*example_inputs)\n\n```\n\n----------------------------------------\n\nTITLE: Registering LLTM Forward/Backward Operations via pybind11 in PyTorch C++ Extension\nDESCRIPTION: This snippet shows how to expose forward and backward LLTM operations to Python from C++ using pybind11 in a TORCH_EXTENSION. It defines the Python-callable 'forward' and 'backward' methods, which internally validate inputs and delegate computation to CUDA-implemented functions. The main dependencies are ATen/Torch C++ APIs, pybind11, and CUDA-backed implementation files. Inputs/outputs are PyTorch tensors; the snippet acts as the extension entry point.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n  return lltm_cuda_forward(input, weights, bias, old_h, old_cell);\n}\n\nstd::vector<torch::Tensor> lltm_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights) {\n  CHECK_INPUT(grad_h);\n  CHECK_INPUT(grad_cell);\n  CHECK_INPUT(input_gate);\n  CHECK_INPUT(output_gate);\n  CHECK_INPUT(candidate_cell);\n  CHECK_INPUT(X);\n  CHECK_INPUT(gate_weights);\n  CHECK_INPUT(weights);\n\n  return lltm_cuda_backward(\n      grad_h,\n      grad_cell,\n      new_cell,\n      input_gate,\n      output_gate,\n      candidate_cell,\n      X,\n      gate_weights,\n      weights);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &lltm_forward, \"LLTM forward (CUDA)\");\n  m.def(\"backward\", &lltm_backward, \"LLTM backward (CUDA)\");\n}\n\n```\n\n----------------------------------------\n\nTITLE: Running FSDP Checkpoint Example with CUDA Devices in Python\nDESCRIPTION: This snippet demonstrates how to run an FSDP checkpoint example using multiple CUDA devices. It uses torch.cuda to get the device count and multiprocessing to spawn processes for each device.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nworld_size = torch.cuda.device_count()\nprint(f\"Running fsdp checkpoint example on {world_size} devices.\")\nmp.spawn(\n    run_fsdp_checkpoint_load_example,\n    args=(world_size,),\n    nprocs=world_size,\n    join=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch, TorchVision, PIL, and Flask Libraries - Python\nDESCRIPTION: Imports the essential Python packages: torchvision's models and transforms for deep learning, PIL for image manipulation, and Flask for web serving. These dependencies must be installed in your Python 3 environment. Required versions are PyTorch >=1.5, TorchVision >=0.6.0, and Flask >=1.1.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/deployment_with_flask.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision.models as models\\nimport torchvision.transforms as transforms\\nfrom PIL import Image\\nfrom flask import Flask, jsonify, request\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchServe Launcher for JeMalloc\nDESCRIPTION: These lines are intended for TorchServe's `config.properties` file. They configure the CPU launcher to be enabled and specify arguments to bind the workload to the first socket (node_id 0) and enable the JeMalloc memory allocator for potentially improved performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchserve_with_ipex_2.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncpu_launcher_enable=true\ncpu_launcher_args=--node_id 0 --enable_jemalloc\n```\n\n----------------------------------------\n\nTITLE: Initializing Distributed Training Environment in Python\nDESCRIPTION: Sets up the distributed training environment by initializing process groups, device settings, and global variables needed for pipeline parallelism including rank, world size, and pipeline process group.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/pipelining_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch.distributed as dist\nfrom torch.distributed.pipelining import pipeline, SplitPoint, PipelineStage, ScheduleGPipe\n\nglobal rank, device, pp_group, stage_index, num_stages\ndef init_distributed():\n   global rank, device, pp_group, stage_index, num_stages\n   rank = int(os.environ[\"LOCAL_RANK\"])\n   world_size = int(os.environ[\"WORLD_SIZE\"])\n   device = torch.device(f\"cuda:{rank}\") if torch.cuda.is_available() else torch.device(\"cpu\")\n   dist.init_process_group()\n\n   # This group can be a sub-group in the N-D parallel case\n   pp_group = dist.new_group()\n   stage_index = rank\n   num_stages = world_size\n```\n\n----------------------------------------\n\nTITLE: Distributed Synchronous SGD Training Loop using PyTorch Distributed (Python)\nDESCRIPTION: Defines a training loop function for distributed synchronous SGD, suitable for use with PyTorch's distributed support. Each process gets its data partition, initializes the model and optimizer, and performs training with gradient synchronization after each backward pass by calling average_gradients. Requires torch, torch.nn, torch.optim, torch.distributed, and the DataPartitioner class, as well as a defined Net model. Inputs are distributed rank/size and partitioned data, outputs are training progress and loss per epoch, and constraints include previous initialization of the distributed process group.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n\"\"\" Distributed Synchronous SGD Example \"\"\"\ndef run(rank, size):\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n    optimizer = optim.SGD(model.parameters(),\n                          lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        print('Rank ', dist.get_rank(), ', epoch ',\n              epoch, ': ', epoch_loss / num_batches)\n```\n\n----------------------------------------\n\nTITLE: Creating QConfigMapping for Modules to Meet Backend Constraints in Python\nDESCRIPTION: This snippet constructs observers and QConfig objects that comply with the DTypeConfig constraints and assembles a QConfigMapping that applies the same QConfig to all relevant module types. All items in fused patterns must share an identical QConfig. The observers determine quantization ranges and accuracy during calibration.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Note: Here we use a quant_max of 127, but this could be up to 255 (see `quint8_with_constraints`)\nactivation_observer = MinMaxObserver.with_args(quant_min=0, quant_max=127, eps=2 ** -12)\nqconfig = QConfig(activation=activation_observer, weight=default_weight_observer)\n\n# Note: All individual items of a fused pattern, e.g. Conv2d and ReLU in\n# (Conv2d, ReLU), must have the same QConfig\nqconfig_mapping = QConfigMapping() \\\n    .set_object_type(torch.nn.Linear, qconfig) \\\n    .set_object_type(torch.nn.Conv2d, qconfig) \\\n    .set_object_type(torch.nn.BatchNorm2d, qconfig) \\\n    .set_object_type(torch.nn.ReLU, qconfig)\n```\n\n----------------------------------------\n\nTITLE: Manual Backward and Double Backward with Custom Functions (Cube, CubeBackward) - PyTorch - Python\nDESCRIPTION: This advanced snippet manually codes both backward and backward-backward paths in cases where autograd cannot track operations (e.g., non-torch code or custom extensions). The Cube function delegates gradient computation to CubeBackward, which itself defines forward and its own backward for double backward. This construct is helpful for integrating exotic, non-differentiable, or third-party code with full autograd support. Dependencies: torch, custom cube implementations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef cube_forward(x):\n    return x**3\n\ndef cube_backward(grad_out, x):\n    return grad_out * 3 * x**2\n\ndef cube_backward_backward(grad_out, sav_grad_out, x):\n    return grad_out * sav_grad_out * 6 * x\n\ndef cube_backward_backward_grad_out(grad_out, x):\n    return grad_out * 3 * x**2\n\nclass Cube(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return cube_forward(x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, = ctx.saved_tensors\n        return CubeBackward.apply(grad_out, x)\n\nclass CubeBackward(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, grad_out, x):\n        ctx.save_for_backward(x, grad_out)\n        return cube_backward(grad_out, x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, sav_grad_out = ctx.saved_tensors\n        dx = cube_backward_backward(grad_out, sav_grad_out, x)\n        dgrad_out = cube_backward_backward_grad_out(grad_out, x)\n        return dgrad_out, dx\n\n```\n\n----------------------------------------\n\nTITLE: Preparing Scripted TorchScript Module - PyTorch - Python\nDESCRIPTION: Illustrates how to define a custom PyTorch nn.Module, convert it into a TorchScript scripted module, and prepare it for future input bundling. Dependencies include torch, torch.jit, torch.utils, and torch.utils.bundled_inputs. The example Net class implements a single linear layer. The output is a scripted TorchScript module ready for bundling, requiring input shape (1, 10).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/bundled_inputs.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\nimport torch.jit\\nimport torch.utils\\nimport torch.utils.bundled_inputs\\n\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.lin = nn.Linear(10, 1)\\n\\n    def forward(self, x):\\n        return self.lin(x)\\n\\nmodel = Net()\\nscripted_module = torch.jit.script(model)\n```\n\n----------------------------------------\n\nTITLE: Creating ProcessGroup with Legacy TCPStore via init_method Override - PyTorch - Python\nDESCRIPTION: Shows how to configure PyTorch ProcessGroup initialization to use the legacy TCPStore backend by including the use_libuv=0 query parameter in the init_method string. Backend selection string and network address/port are set explicitly. This method gives users a lower-priority override compared to directly passing use_libuv to TCPStore. Dependencies: torch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TCPStore_libuv_backend.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed as dist\n\naddr = \"localhost\"\nport = 23456\ndist.init_process_group(\n    backend=\"cpu:gloo,cuda:nccl\",\n    rank=0,\n    world_size=1,\n    init_method=f\"tcp://{addr}:{port}?use_libuv=0\",\n)\ndist.destroy_process_group()\n\n```\n\n----------------------------------------\n\nTITLE: Converting Trained Model to Quantized Model for Inference\nDESCRIPTION: Code for converting a QAT-trained model to a fully quantized model and setting it to evaluation mode. This produces the final optimized model ready for deployment and inference.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nquantized_model = convert_pt2e(prepared_model)\n\n# move certain ops like dropout to eval mode, equivalent to `m.eval()`\ntorch.ao.quantization.move_exported_model_to_eval(m)\n\nprint(quantized_model)\n\ntop1, top5 = evaluate(quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Final evaluation accuracy on %d images, %2.2f' % (num_eval_batches * eval_batch_size, top1.avg))\n```\n\n----------------------------------------\n\nTITLE: Hybrid Model Implementation\nDESCRIPTION: Implements a hybrid model that combines distributed embedding lookups via RPC with locally replicated linear layers using DDP. The model performs remote embedding lookups on a parameter server and local forward passes through FC layers.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/rpc_ddp_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass HybridModel(torch.nn.Module):\n    def __init__(self, remote_emb_module, device):\n        super(HybridModel, self).__init__()\n        self.remote_emb_module = remote_emb_module\n        self.fc = DDP(torch.nn.Linear(16, 8).cuda(device), device_ids=[device])\n\n    def forward(self, indices, offsets):\n        emb_lookup = self.remote_emb_module(indices, offsets)\n        return self.fc(emb_lookup.cuda(self.fc.device))\n```\n\n----------------------------------------\n\nTITLE: Configuring `prepare_fx` for Custom Static Quantization Modules in Python\nDESCRIPTION: Shows how to configure `prepare_fx` for post-training static quantization or quantization-aware training when using custom modules. The `prepare_custom_config_dict` maps the original float module (`FP32NonTraceable`) to its observed version (`ObservedNonTraceable`) under the 'static' key.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# post training static quantization or\n# quantization aware training (that produces a statically quantized module)v\nprepare_custom_config_dict = {\n    \"float_to_observed_custom_module_class\": {\n        \"static\": {\n            FP32NonTraceable: ObservedNonTraceable,\n        }\n    },\n}\n\nmodel_prepared = prepare_fx(\n    model_fp32,\n    qconfig_mapping,\n    example_inputs,\n    prepare_custom_config_dict=prepare_custom_config_dict)\n```\n\n----------------------------------------\n\nTITLE: Example Output of torchfrtrace Analysis\nDESCRIPTION: This snippet shows the expected output from running the `torchfrtrace` command on the traces generated by the example script. The output indicates that not all ranks participated in collective operation #5 ('nccl:all_reduce'). Specifically, it identifies that rank 1 was missing from the collective initiated by rank 0, providing details like tensor sizes and the code location (stack trace) where the problematic collective was called.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$torchfrtrace --prefix \"trace_\" /tmp/\nNot all ranks joining collective 5 at entry 4\ngroup info: 0:default_pg\ncollective: nccl:all_reduce\nmissing ranks: {1}\ninput sizes: [[3, 4]]\noutput sizes: [[3, 4]]\nexpected ranks: 2\ncollective state: scheduled\ncollective stack trace:\n  all_reduce at /home/cpio/local/pytorch/torch/distributed/distributed_c10d.py:2696\n  wrapper at /home/cpio/local/pytorch/torch/distributed/c10d_logger.py:83\n  <module> at /home/cpio/test/crash.py:44\n```\n\n----------------------------------------\n\nTITLE: Advanced Per-Channel Quantization for x86 Architectures - PyTorch - Python\nDESCRIPTION: Illustrates the use of advanced per-channel quantization (e.g., intended for x86 architectures) on a MobileNetV2 model, including loading, fusing, setting qconfig with get_default_qconfig('x86'), observer preparation, data-driven calibration, quantization, and final evaluation. Per-channel quantization and histogram observer yield improved accuracy compared to basic quantization. Dependencies: torch.ao.quantization, model loading, preprocessed data, and evaluation function. Highlights the necessity of per-channel config and use of optimized observers for better inference performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nper_channel_quantized_model = load_model(saved_model_dir + float_model_file)  \nper_channel_quantized_model.eval()  \nper_channel_quantized_model.fuse_model()  \n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nprint(per_channel_quantized_model.qconfig)  \n\ntorch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches) \ntorch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches) \n```\n\n----------------------------------------\n\nTITLE: Loading Custom PyTorch Operator\nDESCRIPTION: Python code demonstrating how to load and verify the custom operator using torch.ops.load_library after building the shared library.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> torch.ops.load_library(\"warp_perspective.so\")\n>>> print(torch.ops.my_ops.warp_perspective)\n<built-in method custom::warp_perspective of PyCapsule object at 0x7ff51c5b7bd0>\n```\n\n----------------------------------------\n\nTITLE: Setting Up HistoEncoder Model for Feature Extraction in TIAToolbox\nDESCRIPTION: Initializes the HistoEncoder model with a wrapper and configures input/output settings for feature extraction from WSIs. The code creates the encoder, defines preprocessing transforms, and sets up the I/O configuration for patch extraction at a specific resolution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# create the model\nencoder = F.create_encoder(\"prostate_medium\")\nmodel = HistoEncWrapper(encoder)\n\n# set the pre-processing function\nnorm=transforms.Normalize(mean=[0.662, 0.446, 0.605],std=[0.169, 0.190, 0.155])\ntrans = [\n    transforms.ToTensor(),\n    norm,\n]\nmodel.preproc_func = transforms.Compose(trans)\n\nwsi_ioconfig = IOSegmentorConfig(\n    input_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n    patch_input_shape=[224, 224],\n    output_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n    patch_output_shape=[224, 224],\n    stride_shape=[224, 224],\n)\n```\n\n----------------------------------------\n\nTITLE: FSDP Validation Loop Implementation\nDESCRIPTION: Validation function for FSDP model evaluation with distributed loss calculation and progress tracking.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef validation(model, rank, world_size, val_loader):\n    model.eval()\n    correct = 0\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(3).to(local_rank)\n    if rank == 0:\n        inner_pbar = tqdm.tqdm(\n            range(len(val_loader)), colour=\"green\", desc=\"Validation Epoch\"\n        )\n    with torch.no_grad():\n        for batch in val_loader:\n            for key in batch.keys():\n                batch[key] = batch[key].to(local_rank)\n            output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"])\n            fsdp_loss[0] += output[\"loss\"].item()  # sum up batch loss\n            fsdp_loss[1] += len(batch)\n\n            if rank==0:\n                inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    val_loss = fsdp_loss[0] / fsdp_loss[1]\n    if rank == 0:\n        inner_pbar.close()\n        print(f\"Validation Loss: {val_loss:.4f}\")\n    return val_loss\n```\n\n----------------------------------------\n\nTITLE: TorchScript Inference with BERT on GPU using IPEX - BFloat16 (Python)\nDESCRIPTION: This snippet demonstrates TorchScript inference with a BERT model on GPU using IPEX with BFloat16 precision. It includes model loading, data preparation, IPEX optimization, BFloat16 autocast, and TorchScript tracing for improved performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import BertModel\nimport intel_extension_for_pytorch as ipex\n\nmodel = BertModel.from_pretrained(args.model_name)\nmodel.eval()\n\nvocab_size = model.config.vocab_size\nbatch_size = 1\nseq_length = 512\ndata = torch.randint(vocab_size, size=[batch_size, seq_length])\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\n\nwith torch.no_grad():\n  d = torch.randint(vocab_size, size=[batch_size, seq_length])\n  d = d.to(\"xpu\")\n  with torch.xpu.amp.autocast(enabled=True, dtype=torch.bfloat16, cache_enabled=False):\n    model = torch.jit.trace(model, (d,), check_trace=False, strict=False)\n    model = torch.jit.freeze(model)\n\n    model(data)\n\n```\n\n----------------------------------------\n\nTITLE: Loading a PyTorch C++ Extension via JIT Compilation in Python\nDESCRIPTION: Demonstrates loading a C++ extension (`lltm.cpp`) just-in-time using `torch.utils.cpp_extension.load`. This function compiles the C++ source file (`lltm.cpp`) into a shared library and imports it as a Python module named `lltm_cpp` on the fly, eliminating the need for a separate `setup.py` file for simple extensions. Requires PyTorch and the C++ source file (`lltm.cpp`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.cpp_extension import load\n\nlltm_cpp = load(name=\"lltm_cpp\", sources=[\"lltm.cpp\"])\n```\n\n----------------------------------------\n\nTITLE: Main Pipeline Parallel Execution in Python\nDESCRIPTION: Implements the main execution logic for pipeline parallel training, including model initialization, data preparation, loss function definition, and schedule creation using GPipe scheduling strategy.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/pipelining_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n   init_distributed()\n   num_microbatches = 4\n   model_args = ModelArgs()\n   model = Transformer(model_args)\n\n   # Dummy data\n   x = torch.ones(32, 500, dtype=torch.long)\n   y = torch.randint(0, model_args.vocab_size, (32, 500), dtype=torch.long)\n   example_input_microbatch = x.chunk(num_microbatches)[0]\n\n   # Option 1: Manual model splitting\n   stage = manual_model_split(model)\n\n   # Option 2: Tracer model splitting\n   # stage = tracer_model_split(model, example_input_microbatch)\n\n   model.to(device)\n   x = x.to(device)\n   y = y.to(device)\n\n   def tokenwise_loss_fn(outputs, targets):\n      loss_fn = nn.CrossEntropyLoss()\n      outputs = outputs.reshape(-1, model_args.vocab_size)\n      targets = targets.reshape(-1)\n      return loss_fn(outputs, targets)\n\n   schedule = ScheduleGPipe(stage, n_microbatches=num_microbatches, loss_fn=tokenwise_loss_fn)\n\n   if rank == 0:\n      schedule.step(x)\n   elif rank == 1:\n      losses = []\n      output = schedule.step(target=y, losses=losses)\n      print(f\"losses: {losses}\")\n   dist.destroy_process_group()\n```\n\n----------------------------------------\n\nTITLE: Capturing FX Graph for Quantization with PyTorch Export (Python)\nDESCRIPTION: This snippet performs all necessary imports, initializes a ResNet-18 model with pretrained weights, moves it to an Intel GPU, creates dummy input data, and captures the FX Graph using torch.export's export_for_training utility. It is the first step in preparing the model for post-training quantization, and it outputs an FX-graph-converted model suitable for further quantization steps. Required dependencies include torch, torchvision, and PyTorch export quantization modules.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\nfrom torch.ao.quantization.quantize_pt2e import prepare_pt2e, convert_pt2e\nimport torch.ao.quantization.quantizer.xpu_inductor_quantizer as xpuiq\nfrom torch.ao.quantization.quantizer.xpu_inductor_quantizer import XPUInductorQuantizer\nfrom torch.export import export_for_training\n\n# Create the Eager Model\nmodel_name = \"resnet18\"\nmodel = models.__dict__[model_name](weights=models.ResNet18_Weights.DEFAULT)\n\n# Set the model to eval mode\nmodel = model.eval().to(\"xpu\")\n\n# Create the data, using the dummy data here as an example\ntraced_bs = 50\nx = torch.randn(traced_bs, 3, 224, 224, device=\"xpu\").contiguous(memory_format=torch.channels_last)\nexample_inputs = (x,)\n\n# Capture the FX Graph to be quantized\nwith torch.no_grad():\n    exported_model = export_for_training(\n        model,\n        example_inputs,\n    ).module()\n\n```\n\n----------------------------------------\n\nTITLE: Applying FSDP and Tensor Parallel using 2-D PyTorch DeviceMesh\nDESCRIPTION: This snippet demonstrates initializing a 2-D `DeviceMesh` for 8-way Data Parallelism (DP) and 8-way Tensor Parallelism (TP) across 64 GPUs. It extracts sub-meshes for TP (`tp_mesh` for intra-host) and DP (`dp_mesh` for inter-host). Tensor Parallelism is applied first using `parallelize_module` on the `tp_mesh`, followed by FSDP on the `dp_mesh`, enabling combined 2-D parallelism. Dependencies include `torch.distributed.device_mesh`, `torch.distributed.tensor.parallel`, and `torch.distributed.fsdp`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.device_mesh import init_device_mesh\nfrom torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\n# i.e. 2-D mesh is [dp, tp], training on 64 GPUs that performs 8 way DP and 8 way TP\nmesh_2d = init_device_mesh(\"cuda\", (8, 8))\ntp_mesh = mesh_2d[\"tp\"] # a submesh that connects intra-host devices\ndp_mesh = mesh_2d[\"dp\"] # a submesh that connects inter-host devices\n\nmodel = Model(...)\n\ntp_plan = {...}\n\n# apply Tensor Parallel intra-host on tp_mesh\nmodel_tp = parallelize_module(model, tp_mesh, tp_plan)\n# apply FSDP inter-host on dp_mesh\nmodel_2d = FSDP(model_tp, device_mesh=dp_mesh, use_orig_params=True, ...)\n```\n\n----------------------------------------\n\nTITLE: Defining CNN Model Architecture for Fashion-MNIST\nDESCRIPTION: Implements a Convolutional Neural Network model customized for Fashion-MNIST dataset with single channel input and appropriate layer dimensions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tensorboard_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n```\n\n----------------------------------------\n\nTITLE: Using Join with DistributedDataParallel and ZeroRedundancyOptimizer in Python\nDESCRIPTION: Example of using the Join context manager with both DistributedDataParallel and ZeroRedundancyOptimizer for distributed training. It shows how to initialize and use multiple Joinable instances within the Join context.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/generic_join.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.optim import ZeroRedundancyOptimizer as ZeRO\nfrom torch.optim import Adam\n\ndef worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n    optim = ZeRO(model.parameters(), Adam, lr=0.01)\n    # Rank 1 gets one more input than rank 0\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    num_inputs = 0\n    # Pass both `model` and `optim` into `Join()`\n    with Join([model, optim]):\n        for input in inputs:\n            num_inputs += 1\n            loss = model(input).sum()\n            loss.backward()\n            optim.step()\n\n    print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Model, Criterion, Optimizer, and Scheduler for Training Head in PyTorch\nDESCRIPTION: Instantiates the combined model using `create_combined_model`. Moves the model to the CPU, as quantized models currently run only on CPU. Defines the `CrossEntropyLoss` criterion. Sets up the SGD optimizer (`optimizer_ft`), configured to only train the parameters of the newly added head (implicitly, as the quantized feature extractor has no trainable parameters) with a specific learning rate and momentum. Defines a learning rate scheduler (`StepLR`) to decay the learning rate.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch.optim as optim\nnew_model = create_combined_model(model_fe)\nnew_model = new_model.to('cpu')\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are only training the head.\noptimizer_ft = optim.SGD(new_model.parameters(), lr=0.01, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n```\n\n----------------------------------------\n\nTITLE: Generating HTA Trace with Memory/Queue Counters in Python\nDESCRIPTION: Initializes the HTA `TraceAnalysis` class with the path to a trace directory and then calls `generate_trace_with_counters` to create a new trace file augmented with memory bandwidth and CUDA stream queue length information. This is useful for visualizing resource utilization over time.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. code-block:: python\n\n  analyzer = TraceAnalysis(trace_dir = \"/path/to/trace/folder\")\n  analyzer.generate_trace_with_counters()\n```\n\n----------------------------------------\n\nTITLE: Implementing the LLTM CUDA Forward Kernel for Parallel Elementwise Computation (C++)\nDESCRIPTION: This kernel performs the elementwise SIMD computations for the LLTM forward pass, applying activation functions and updating states for each sequence element in parallel with CUDA grid-stride looping. It directly operates on raw data pointers, requiring knowledge of tensor layout. Inputs are pointers to input, output, and gate tensors along with the state size; outputs are written in-place in the returning tensors. Dependencies are proper CUDA launch and prior buffer allocation. Assumes memory validity and expected tensor dimensions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_15\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename scalar_t>\n__global__ void lltm_cuda_forward_kernel(\n    const scalar_t* __restrict__ gates,\n    const scalar_t* __restrict__ old_cell,\n    scalar_t* __restrict__ new_h,\n    scalar_t* __restrict__ new_cell,\n    scalar_t* __restrict__ input_gate,\n    scalar_t* __restrict__ output_gate,\n    scalar_t* __restrict__ candidate_cell,\n    size_t state_size) {\n  const int column = blockIdx.x * blockDim.x + threadIdx.x;\n  const int index = blockIdx.y * state_size + column;\n  const int gates_row = blockIdx.y * (state_size * 3);\n  if (column < state_size) {\n    input_gate[index] = sigmoid(gates[gates_row + column]);\n    output_gate[index] = sigmoid(gates[gates_row + state_size + column]);\n    candidate_cell[index] = elu(gates[gates_row + 2 * state_size + column]);\n    new_cell[index] =\n        old_cell[index] + candidate_cell[index] * input_gate[index];\n    new_h[index] = tanh(new_cell[index]) * output_gate[index];\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Upgrading PyTorch and torchvision to Nightly Build - Shell\nDESCRIPTION: Uninstalls any existing versions of PyTorch and torchvision, and installs the pre-release (nightly) build to enable access to beta features required for dynamic quantization experiments. The commands assume a bash-like shell environment and require proper pip permissions. The commands are specific for systems where the CUDA 10.1 toolkit is available and PyTorch's nightly wheels are compatible.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nyes y | pip uninstall torch torchvision\nyes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n```\n\n----------------------------------------\n\nTITLE: Defining DType Constraints and DTypeConfig for Quantized Operators in Python\nDESCRIPTION: This code sets up dtype constraints for quantized tensors using DTypeWithConstraints, specifying allowed value and scale ranges. It then creates a DTypeConfig for quantized ops, detailing the input, output, weight, and bias data types. These configurations are prerequisites for backend pattern definitions and enforce quantization parameter constraints.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquint8_with_constraints = DTypeWithConstraints(\n    dtype=torch.quint8,\n    quant_min_lower_bound=0,\n    quant_max_upper_bound=255,\n    scale_min_lower_bound=2 ** -12,\n)\n\n# Specify the dtypes passed to the quantized ops in the reference model spec\nweighted_int8_dtype_config = DTypeConfig(\n    input_dtype=quint8_with_constraints,\n    output_dtype=quint8_with_constraints,\n    weight_dtype=torch.qint8,\n    bias_dtype=torch.float)\n```\n\n----------------------------------------\n\nTITLE: Using the TorchScript-Compatible Distributed Optimizer\nDESCRIPTION: Demonstrates how to use the custom distributed optimizer in a distributed training setup. The DistributedOptimizer will automatically use the TorchScript-compatible implementation for better performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_optim_torchscript.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n...\nremote_params_list = [...]\ndist_optim = DistributedOptimizer(\n    QHM, remote_params_list, *args, **kwargs\n)\n```\n\n----------------------------------------\n\nTITLE: FSDP Training Loop Implementation\nDESCRIPTION: Training function that handles distributed training with FSDP, including loss calculation and optimization steps.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    ddp_loss = torch.zeros(2).to(rank)\n    if sampler:\n        sampler.set_epoch(epoch)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(rank), target.to(rank)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target, reduction='sum')\n        loss.backward()\n        optimizer.step()\n        ddp_loss[0] += loss.item()\n        ddp_loss[1] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n    if rank == 0:\n        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))\n```\n\n----------------------------------------\n\nTITLE: Specifying Quantization Configuration with QConfig Dictionary (Python)\nDESCRIPTION: This short Python snippet defines the qconfig_dict for dynamic quantization, applying per_channel_dynamic_qconfig globally. Required dependency: torch.quantization. This dictionary is used as an argument to quantization functions to specify how each submodule should be quantized, with the empty string key applying the setting to the whole model by default.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nqconfig_dict = {'': per_channel_dynamic_qconfig}\n```\n\n----------------------------------------\n\nTITLE: Configuring Loss Parallel with Sharded Outputs in PyTorch Tensor Parallel - Python\nDESCRIPTION: This code configures the last projection layer to output a DTensor, required for loss parallelism in PyTorch. By specifying use_local_output=False in ColwiseParallel, the output remains sharded, allowing the loss_parallel context manager to operate efficiently. Prerequisites include a sharded model and the presence of a loss_parallel implementation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel = parallelize_module(\n    model,\n    tp_mesh,\n    {\n        \"tok_embeddings\": RowwiseParallel(\n            input_layouts=Replicate(),\n            output_layouts=Shard(1),\n        ),\n        \"norm\": SequenceParallel(),\n        \"output\": ColwiseParallel(\n            input_layouts=Shard(1),\n            # use DTensor as the output\n            use_local_output=False,\n        ),\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling and Using Compiled Autograd Globally with torch.compile\nDESCRIPTION: Demonstrates the basic setup for using Compiled Autograd. It globally enables the feature by setting `torch._dynamo.config.compiled_autograd = True`. Then, it defines a `train` function, which performs a forward pass, calculates loss, and executes `loss.backward()`. This function is decorated with `@torch.compile` to optimize it, allowing Compiled Autograd to capture and compile the backward pass.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/compiled_autograd_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = Model()\nx = torch.randn(10)\n\ntorch._dynamo.config.compiled_autograd = True\n@torch.compile\ndef train(model, x):\n   loss = model(x).sum()\n   loss.backward()\n\ntrain(model, x) \n```\n\n----------------------------------------\n\nTITLE: Manual Model Partitioning for Pipeline Parallelism in Python\nDESCRIPTION: Implements manual model partitioning by selectively deleting portions of the model based on stage index. Creates two separate stages with different layer configurations for pipeline parallel execution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/pipelining_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef manual_model_split(model) -> PipelineStage:\n   if stage_index == 0:\n      # prepare the first stage model\n      for i in range(4, 8):\n            del model.layers[str(i)]\n      model.norm = None\n      model.output = None\n\n   elif stage_index == 1:\n      # prepare the second stage model\n      for i in range(4):\n            del model.layers[str(i)]\n      model.tok_embeddings = None\n\n   stage = PipelineStage(\n      model,\n      stage_index,\n      num_stages,\n      device,\n   )\n   return stage\n```\n\n----------------------------------------\n\nTITLE: Training and Visualizing the Model with Frozen Quantized Feature Extractor in PyTorch\nDESCRIPTION: Calls the `train_model` function (assumed to be defined elsewhere in the tutorial) to train the `new_model`. Passes the model, criterion, optimizer, learning rate scheduler, number of epochs, and the device ('cpu') as arguments. After training, it calls `visualize_model` (also assumed defined elsewhere) to display the model's performance or predictions, followed by `plt.tight_layout()` for plot formatting.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnew_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=25, device='cpu')\n\nvisualize_model(new_model)\nplt.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Traditional PyTorch Module Initialization and Re-initialization in Python\nDESCRIPTION: This snippet demonstrates the standard way of creating a PyTorch `nn.Linear` module, which involves default parameter initialization, followed by explicit re-initialization using `nn.init.orthogonal_`. This highlights the potential inefficiency where the initial default initialization is discarded.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/skip_param_init.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn\n\n# Initializes weight from the default distribution: uniform(-1/sqrt(10), 1/sqrt(10)).\nm = nn.Linear(10, 5)\n\n# Re-initialize weight from a different distribution.\nnn.init.orthogonal_(m.weight)\n```\n\n----------------------------------------\n\nTITLE: Loading and Deploying Quantized PyTorch Model\nDESCRIPTION: This code snippet demonstrates how to load a quantized model using Intel Neural Compressor's utility function for deployment or performance benchmarking.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom neural_compressor.utils.pytorch import load\nint8_model = load('./output', model)\n```\n\n----------------------------------------\n\nTITLE: Minimal CMake Configuration for TorchScript C++ App - CMake\nDESCRIPTION: Basic CMake file for building a C++ application that depends on libtorch. Finds PyTorch with find_package, defines the executable, links Torch libraries, and requires C++11 or above. Key for compiling the standalone TorchScript inference binary.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(example_app)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(example_app main.cpp)\ntarget_link_libraries(example_app \"${TORCH_LIBRARIES}\")\ntarget_compile_features(example_app PRIVATE cxx_range_for)\n\n```\n\n----------------------------------------\n\nTITLE: Saving Models with Distributed Checkpoint in PyTorch\nDESCRIPTION: This code demonstrates how to save an FSDP-wrapped model using Distributed Checkpoint (DCP). It creates a toy model, wraps it with FSDP, performs training steps, and then saves the model and optimizer state using DCP's save functionality.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.distributed.checkpoint as dcp\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\nfrom torch.distributed.checkpoint.stateful import Stateful\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass AppState(Stateful):\n    \"\"\"This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n    with the Stateful protocol, DCP will automatically call state_dict/load_stat_dict as needed in the\n    dcp.save/load APIs.\n\n    Note: We take advantage of this wrapper to hande calling distributed state dict methods on the model\n    and optimizer.\n    \"\"\"\n\n    def __init__(self, model, optimizer=None):\n        self.model = model\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        # this line automatically manages FSDP FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n        model_state_dict, optimizer_state_dict = get_state_dict(self.model, self.optimizer)\n        return {\n            \"model\": model_state_dict,\n            \"optim\": optimizer_state_dict\n        }\n\n    def load_state_dict(self, state_dict):\n        # sets our state dicts on the model and optimizer, now that we've loaded\n        set_state_dict(\n            self.model,\n            self.optimizer,\n            model_state_dict=state_dict[\"model\"],\n            optim_state_dict=state_dict[\"optim\"]\n        )\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355 \"\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef run_fsdp_checkpoint_save_example(rank, world_size):\n    print(f\"Running basic FSDP checkpoint saving example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create a model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    model = FSDP(model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n    optimizer.zero_grad()\n    model(torch.rand(8, 16, device=\"cuda\")).sum().backward()\n    optimizer.step()\n\n    state_dict = { \"app\": AppState(model, optimizer) }\n    dcp.save(state_dict, checkpoint_id=CHECKPOINT_DIR)\n\n    cleanup()\n\n\nif __name__ == \"__main__\":\n    world_size = torch.cuda.device_count()\n    print(f\"Running fsdp checkpoint example on {world_size} devices.\")\n    mp.spawn(\n        run_fsdp_checkpoint_save_example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True,\n    )\n```\n\n----------------------------------------\n\nTITLE: Exporting a PyTorch Model with torch.export - Python\nDESCRIPTION: Uses torch.export.export_for_training to export a model for PT2E quantization, accepting both fixed and dynamic input shapes. Dependencies include PyTorch 2.5+ and an initialized model. The function requires example inputs and optionally dynamic shape constraints, returning an exportable module for further quantization steps.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nexample_inputs = (torch.rand(2, 3, 224, 224),)\n# for pytorch 2.5+\nexported_model = torch.export.export_for_training(model_to_quantize, example_inputs).module()\n\n# for pytorch 2.4 and before\n# from torch._export import capture_pre_autograd_graph\n# exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs)\n\n# or capture with dynamic dimensions\n# for pytorch 2.5+\ndynamic_shapes = tuple(\n  {0: torch.export.Dim(\"dim\")} if i == 0 else None\n  for i in range(len(example_inputs))\n)\nexported_model = torch.export.export_for_training(model_to_quantize, example_inputs, dynamic_shapes=dynamic_shapes).module()\n\n# for pytorch 2.4 and before\n# dynamic_shape API may vary as well\n# from torch._export import dynamic_dim\n# exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs, constraints=[dynamic_dim(example_inputs[0], 0)])\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom LSTM Variant in Python\nDESCRIPTION: Python implementation of a custom LSTM variant called LLTM (Long-Long-Term-Memory) that lacks a forget gate and uses ELU activation. Subclasses torch.nn.Module to define the forward pass.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        # 3 * state_size for input gate, output gate and candidate cell gate.\n        # input_features + state_size because we will multiply with [input, h].\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        old_h, old_cell = state\n        X = torch.cat([old_h, input], dim=1)\n\n        # Compute the input, output and candidate cell gates with one MM.\n        gate_weights = F.linear(X, self.weights, self.bias)\n        # Split the combined gate weight matrix into its components.\n        gates = gate_weights.chunk(3, dim=1)\n\n        input_gate = torch.sigmoid(gates[0])\n        output_gate = torch.sigmoid(gates[1])\n        # Here we use an ELU instead of the usual tanh.\n        candidate_cell = F.elu(gates[2])\n\n        # Compute the new cell state.\n        new_cell = old_cell + candidate_cell * input_gate\n        # Compute the new hidden state and output.\n        new_h = torch.tanh(new_cell) * output_gate\n\n        return new_h, new_cell\n```\n\n----------------------------------------\n\nTITLE: CUDA Graph Warm-up Implementation\nDESCRIPTION: Shows how to perform warm-up iterations before graph capture to prepare CUDA cache and libraries like CUBLAS and CUDNN, which improves performance consistency.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nat::cuda::CUDAStream warmupStream = at::cuda::getStreamFromPool();\nat::cuda::setCurrentCUDAStream(warmupStream);\nfor (int iter = 0; iter < num_warmup_iters; iter++) {\n  training_step(model, optimizer, data, targets, output, loss);\n}\n```\n\n----------------------------------------\n\nTITLE: FeedForward Layer Parallelization Plan\nDESCRIPTION: Defines the tensor parallel plan for the FeedForward layer using ColwiseParallel and RowwiseParallel strategies.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module\n\nlayer_tp_plan = {\n    \"feed_foward.w1\": ColwiseParallel(),\n    \"feed_forward.w2\": RowwiseParallel(),\n    \"feed_forward.w3\": ColwiseParallel(),\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Standard Scaled Dot Product Attention in PyTorch\nDESCRIPTION: Demonstrates a basic example of performing Scaled Dot Product Attention (SDPA) using `torch.nn.functional.scaled_dot_product_attention` within the `sdpa_kernel` context manager to specify the backend (e.g., FlashAttention). It initializes random query, key, and value tensors on a CUDA device and computes the attention output for a single process.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/context_parallel.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.attention import sdpa_kernel, SDPBackend\n\n\ndef sdpa_example():\n    assert torch.cuda.is_available()\n    torch.cuda.set_device(\"cuda:0\")\n    torch.cuda.manual_seed(0)\n\n    batch = 8\n    nheads = 8\n    qkv_len = 8192\n    dim = 32\n    backend = SDPBackend.FLASH_ATTENTION\n    dtype = (\n        torch.bfloat16\n        if backend == SDPBackend.FLASH_ATTENTION\n        or backend == SDPBackend.CUDNN_ATTENTION\n        else torch.float32\n    )\n\n    qkv = [\n        torch.rand(\n            (batch, nheads, qkv_len, dim),\n            dtype=dtype,\n            requires_grad=True,\n            device='cuda',\n        )\n        for _ in range(3)\n    ]\n    # specify the SDPBackend to use\n    with sdpa_kernel(backend):\n        out = F.scaled_dot_product_attention(*qkv, is_causal=True)\n\n\nif __name__ == \"__main__\":\n    sdpa_example()\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for Custom Operator\nDESCRIPTION: CMakeLists.txt file contents for building the custom operator into a shared library. It sets up the project, finds the required PyTorch package, and defines the library target.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n.. literalinclude:: ../advanced_source/torch_script_custom_ops/CMakeLists.txt\n  :language: cpp\n```\n\n----------------------------------------\n\nTITLE: Setting Up DDP Process Group\nDESCRIPTION: Function to initialize the distributed process group with NCCL backend, setting up communication between GPU processes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ddp_series_multigpu.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef ddp_setup(rank: int, world_size: int):\n    \"\"\"\n    Args:\n        rank: Unique identifier of each process\n       world_size: Total number of processes\n    \"\"\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n    torch.cuda.set_device(rank)\n    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n```\n\n----------------------------------------\n\nTITLE: FSDP Training Loop Implementation\nDESCRIPTION: Main training function implementing FSDP training loop with distributed loss calculation and progress tracking.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(2).to(local_rank)\n\n    if sampler:\n        sampler.set_epoch(epoch)\n    if rank==0:\n        inner_pbar = tqdm.tqdm(\n            range(len(train_loader)), colour=\"blue\", desc=\"r0 Training Epoch\"\n        )\n    for batch in train_loader:\n        for key in batch.keys():\n            batch[key] = batch[key].to(local_rank)\n        optimizer.zero_grad()\n        output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"] )\n        loss = output[\"loss\"]\n        loss.backward()\n        optimizer.step()\n        fsdp_loss[0] += loss.item()\n        fsdp_loss[1] += len(batch)\n        if rank==0:\n            inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    train_accuracy = fsdp_loss[0] / fsdp_loss[1]\n\n\n    if rank == 0:\n        inner_pbar.close()\n        print(\n                f\"Train Epoch: \\t{epoch}, Loss: \\t{train_accuracy:.4f}\"\n            )\n    return train_accuracy\n```\n\n----------------------------------------\n\nTITLE: Registering FakeTensor/Meta Kernel for Custom Operator - PyTorch Python\nDESCRIPTION: Defines a FakeTensor (meta or abstract) kernel in Python for the custom operator 'mymuladd', enabling 'torch.compile' and symbolic tracing mechanisms. It verifies shape, dtype, and device properties of inputs and returns a dummy tensor with matching metadata using torch.empty_like. This registration requires prior loading of the C++ custom operator, and depends on the torch.library APIs. Inputs are tensors and a scalar; this stub does not perform real computation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom . import _C\n\n@torch.library.register_fake(\"extension_cpp::mymuladd\")\ndef _(a, b, c):\n    torch._check(a.shape == b.shape)\n    torch._check(a.dtype == torch.float)\n    torch._check(b.dtype == torch.float)\n    torch._check(a.device == b.device)\n    return torch.empty_like(a)\n\n```\n\n----------------------------------------\n\nTITLE: Enabling oneDNN Verbose Logging in Python to Verify AMX Usage\nDESCRIPTION: Shows how to programmatically enable verbose logging from the oneDNN backend in PyTorch using `torch.backends.mkldnn.verbose`. Wrapping model execution within this context manager prints detailed information about the underlying oneDNN kernel implementations being used, which helps confirm if AMX instructions (`avx512_core_amx_bf16` or `avx512_core_amx_int8`) are being dispatched.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/amx.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):\n   with torch.cpu.amp.autocast():\n       model(input)\n```\n\n----------------------------------------\n\nTITLE: Custom MulConstant Autograd Function\nDESCRIPTION: Implements a custom multiplication function that takes a non-tensor parameter. Demonstrates how to handle non-tensor arguments in custom autograd functions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_autograd.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/torch.h>\n\nusing namespace torch::autograd;\n\nclass MulConstant : public Function<MulConstant> {\n public:\n  static torch::Tensor forward(AutogradContext *ctx, torch::Tensor tensor, double constant) {\n    ctx->saved_data[\"constant\"] = constant;\n    return tensor * constant;\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    return {grad_outputs[0] * ctx->saved_data[\"constant\"].toDouble(), torch::Tensor()};\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Calibrating the Prepared Quantization Model (Python)\nDESCRIPTION: This snippet runs dummy or user-provided calibration data through the prepared model, activating the observers and collecting statistics required for quantization. Calibration can be performed by directly calling the model with input data or looping over a custom dataset. Calibration is necessary for static post-training quantization before final conversion. All dependencies are as previously imported; the data loader pattern is supported but optional.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# We use the dummy data as an example here\nprepared_model(*example_inputs)\n\n# Alternatively: user can define the dataset to calibrate\n# def calibrate(model, data_loader):\n#     model.eval()\n#     with torch.no_grad():\n#         for image, target in data_loader:\n#             model(image)\n# calibrate(prepared_model, data_loader_test)  # run calibration on sample data\n\n```\n\n----------------------------------------\n\nTITLE: Warming Up, Benchmarking, and Comparing Optimizer Step Performance - PyTorch - Python\nDESCRIPTION: Performs five warmup runs to trigger compilation of the optimizer step function, then benchmarks runtime for both eager and compiled function calls. An assertion checks that the compiled version is faster. Finally, results are printed in microseconds. This finalizes and demonstrates the real-world performance gain from using torch.compile with the optimizer.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/compiling_optimizer.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Warmup runs to compile the function\\nfor _ in range(5):\\n    fn()\\n\\neager_runtime = benchmark_torch_function_in_microseconds(opt.step)\\ncompiled_runtime = benchmark_torch_function_in_microseconds(fn)\\n\\nassert eager_runtime > compiled_runtime\\n\\nprint(f\\\"eager runtime: {eager_runtime}us\\\")\\nprint(f\\\"compiled runtime: {compiled_runtime}us\\\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Hybrid Sharding Data Parallel (HSDP) with DeviceMesh\nDESCRIPTION: This code shows how to apply Hybrid Sharding Data Parallel to a model using DeviceMesh, which combines FSDP within a host and DDP across hosts in a 2D strategy.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_device_mesh.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n\nfrom torch.distributed.device_mesh import init_device_mesh\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy\n\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\n# HSDP: MeshShape(2, 4)\nmesh_2d = init_device_mesh(\"cuda\", (2, 4))\nmodel = FSDP(\n    ToyModel(), device_mesh=mesh_2d, sharding_strategy=ShardingStrategy.HYBRID_SHARD\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing RPC for a Trainer Worker in PyTorch (Python)\nDESCRIPTION: Defines the `run_worker` function, which serves as the entry point for each trainer process in the distributed setup. Its primary role is to initialize the PyTorch RPC framework for this specific worker using `rpc.init_rpc`. It assigns a unique name (`trainer_{rank}`) and its rank within the total `world_size` to the worker process. This function sets up the communication layer before the worker starts its training loop (defined in `run_training_loop`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Main loop for trainers.\ndef run_worker(rank, world_size, num_gpus, train_loader, test_loader):\n    print(f\"Worker rank {rank} initializing RPC\")\n    rpc.init_rpc(\n        name=f\"trainer_{rank}\",\n        rank=rank,\n        world_size=world_size)\n\n    print(f\"Worker {rank} done initializing RPC\")\n```\n\n----------------------------------------\n\nTITLE: Loading a Serialized TorchScript Model using LibTorch in C++\nDESCRIPTION: This C++ snippet demonstrates loading a serialized PyTorch TorchScript model using the LibTorch API. It requires the torch/script.h header, proper linking to LibTorch libraries, and a valid path to a serialized ScriptModule (.pt file). The program checks the command-line argument, deserializes the model, and prints status output. C++17 is required for compilation. The loaded module can be further used for inference.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_export.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/script.h> // One-stop header.\\n\\n#include <iostream>\\n#include <memory>\\n\\nint main(int argc, const char* argv[]) {\\n  if (argc != 2) {\\n    std::cerr << \"usage: example-app <path-to-exported-script-module>\\n\";\\n    return -1;\\n  }\\n\\n  torch::jit::script::Module module;\\n  try {\\n    // Deserialize the ScriptModule from a file using torch::jit::load().\\n    module = torch::jit::load(argv[1]);\\n  }\\n  catch (const c10::Error& e) {\\n    std::cerr << \"error loading the model\\n\";\\n    return -1;\\n  }\\n\\n  std::cout << \"ok\\n\";\\n}\n```\n\n----------------------------------------\n\nTITLE: Illustrating Recompilation due to Autograd Structure Changes\nDESCRIPTION: Demonstrates a common cause of recompilation with Compiled Autograd: changes in the autograd graph structure leading to the loss tensor. The code iterates through different PyTorch operations (`add`, `sub`, `mul`, `div`), each creating a `loss` with a unique autograd history. Compiling `loss.backward()` inside the loop (`eager` backend) triggers recompilations due to these structural differences.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/compiled_autograd_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntorch._dynamo.config.compiled_autograd = True\nx = torch.randn(10, requires_grad=True)\nfor op in [torch.add, torch.sub, torch.mul, torch.div]:\n   loss = op(x, x).sum()\n   torch.compile(lambda: loss.backward(), backend=\"eager\")()\n```\n\n----------------------------------------\n\nTITLE: Custom Autograd Function Implementation\nDESCRIPTION: Implements a custom LinearFunction that inherits from torch::autograd::Function. Shows how to create custom autograd operations with forward and backward passes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_autograd.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/torch.h>\n\nusing namespace torch::autograd;\n\nclass LinearFunction : public Function<LinearFunction> {\n public:\n  static torch::Tensor forward(\n      AutogradContext *ctx, torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor()) {\n    ctx->save_for_backward({input, weight, bias});\n    auto output = input.mm(weight.t());\n    if (bias.defined()) {\n      output += bias.unsqueeze(0).expand_as(output);\n    }\n    return output;\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto saved = ctx->get_saved_variables();\n    auto input = saved[0];\n    auto weight = saved[1];\n    auto bias = saved[2];\n\n    auto grad_output = grad_outputs[0];\n    auto grad_input = grad_output.mm(weight);\n    auto grad_weight = grad_output.t().mm(input);\n    auto grad_bias = torch::Tensor();\n    if (bias.defined()) {\n      grad_bias = grad_output.sum(0);\n    }\n\n    return {grad_input, grad_weight, grad_bias};\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Transformers and scikit-learn - Shell\nDESCRIPTION: Installs the necessary Python packages for running the tutorial, including a specific version of the HuggingFace Transformers library and scikit-learn for computing evaluation metrics. This requires pip to be available in the system, and internet access. This step is necessary to ensure that subsequent Python imports and ML code will work properly.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install sklearn\npip install transformers==4.29.2\n```\n\n----------------------------------------\n\nTITLE: Defining a Layer Parallelization Plan with Sequence and Tensor Parallelism - Python\nDESCRIPTION: This code block outlines a sample layer_tp_plan dictionary for parallelizing a TransformerBlock with both sequence and tensor parallelism. It specifies sharding/replication strategies for each submodule, including norm, attention, feed-forward, and output layers. The plan guides parallelize_module to properly assign input/output tensor layouts for efficient computation; dependencies include the parallelization classes (e.g., SequenceParallel, PrepareModuleInput, ColwiseParallel, RowwiseParallel).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlayer_tp_plan = {\n    # Now the input and output of SequenceParallel has Shard(1) layouts,\n    # to represent the input/output tensors sharded on the sequence dimension\n    \"attention_norm\": SequenceParallel(),\n    \"attention\": PrepareModuleInput(\n        input_layouts=(Shard(1),),\n        desired_input_layouts=(Replicate(),),\n    ),\n    \"attention.wq\": ColwiseParallel(),\n    \"attention.wk\": ColwiseParallel(),\n    \"attention.wv\": ColwiseParallel(),\n    \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)),\n    \"ffn_norm\": SequenceParallel(),\n    \"feed_forward\": PrepareModuleInput(\n        input_layouts=(Shard(1),),\n        desired_input_layouts=(Replicate(),),\n    ),\n    \"feed_forward.w1\": ColwiseParallel(),\n    \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)),\n    \"feed_forward.w3\": ColwiseParallel(),\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Multiprocessing Simulation for TorchRec Sharding\nDESCRIPTION: This code sets up a multiprocessing environment to simulate SPMD execution across multiple GPUs, demonstrating table-wise sharding of embedding tables.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/sharding.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport multiprocess\n   \ndef spmd_sharing_simulation(\n    sharding_type: ShardingType = ShardingType.TABLE_WISE,\n    world_size = 2,\n):\n  ctx = multiprocess.get_context(\"spawn\")\n  processes = []\n  for rank in range(world_size):\n      p = ctx.Process(\n          target=single_rank_execution,\n          args=(\n              rank,\n              world_size,\n              gen_constraints(sharding_type),\n              ebc,\n              \"nccl\"\n          ),\n      )\n      p.start()\n      processes.append(p)\n\n  for p in processes:\n      p.join()\n      assert 0 == p.exitcode\n```\n\nLANGUAGE: python\nCODE:\n```\nspmd_sharing_simulation(ShardingType.TABLE_WISE)\n```\n\n----------------------------------------\n\nTITLE: Configuring FX Quantization to Skip Non-Traceable Modules in Python\nDESCRIPTION: Illustrates configuring `prepare_fx` to skip tracing into specified non-traceable submodules during quantization. This is done using the `prepare_custom_config_dict` argument, specifying modules either by name (`non_traceable_module_name`) or by class (`non_traceable_module_class`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nqconfig_mapping = QConfigMapping.set_global(qconfig)\n\nprepare_custom_config_dict = {\n    # option 1\n    \"non_traceable_module_name\": \"non_traceable_submodule\",\n    # option 2\n    \"non_traceable_module_class\": [MNonTraceable],\n}\nmodel_prepared = prepare_fx(\n    model_fp32,\n    qconfig_mapping,\n    example_inputs,\n    prepare_custom_config_dict=prepare_custom_config_dict,\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Weight Attributes in TorchScript Model (Python)\nDESCRIPTION: This snippet shows how to directly access weight attributes in a TorchScript quantized model. It prints the weight tensor of a specific layer in the BERT model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(quantized_model.bert.encoder.layer._c.getattr('0').attention.self.query.weight)\n```\n\n----------------------------------------\n\nTITLE: Assigning Submodules Post-Construction in PyTorch C++ (Pythonic Style)\nDESCRIPTION: This snippet demonstrates assigning submodules after default constructing a holder within a module, which allows for more flexible or dynamic module instantiation patterns familiar to Python developers. It requires PyTorch's C++ API, especially module holders and register_module. The code shows how to declare an empty holder and populate it in the constructor body.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n  struct Net : torch::nn::Module {\n    Net(int64_t N, int64_t M) {\n      linear = register_module(\"linear\", torch::nn::Linear(N, M));\n    }\n    torch::nn::Linear linear{nullptr}; // construct an empty holder\n  };\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Size and Accuracy in PyTorch Quantization\nDESCRIPTION: This snippet demonstrates how to compare the size and accuracy of a baseline float model with a quantized model. It prints the model sizes and evaluates the accuracy on a test dataset.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Baseline model size and accuracy\nprint(\"Size of baseline model\")\nprint_size_of_model(float_model)\n\ntop1, top5 = evaluate(float_model, criterion, data_loader_test)\nprint(\"Baseline Float Model Evaluation accuracy: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n\n# Quantized model size and accuracy\nprint(\"Size of model after quantization\")\n# export again to remove unused weights\nquantized_model = torch.export.export_for_training(quantized_model, example_inputs).module()\nprint_size_of_model(quantized_model)\n\ntop1, top5 = evaluate(quantized_model, criterion, data_loader_test)\nprint(\"[before serilaization] Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n```\n\n----------------------------------------\n\nTITLE: Configuring Graph Mode Dynamic Quantization APIs using PyTorch (Python)\nDESCRIPTION: This Python code imports necessary PyTorch quantization APIs and demonstrates tracing or scripting a float model for use with the quantization process. It then applies the graph mode dynamic quantization using per-channel dynamic configuration. Dependencies include 'torch' (PyTorch library), and the target input is a float TorchScript model. The output is a quantized TorchScript model with dynamic activation quantization and static per-channel weight quantization. Limitations: actual 'float_model' definition is omitted and the snippet assumes the presence of a traced/scribed model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.quantization import per_channel_dynamic_qconfig\nfrom torch.quantization import quantize_dynamic_jit\n\nts_model = torch.jit.script(float_model) # or torch.jit.trace(float_model, input)\n\nquantized = quantize_dynamic_jit(ts_model, {'': per_channel_dynamic_qconfig})\n```\n\n----------------------------------------\n\nTITLE: Predicting Patch Labels and Evaluating Classifier Performance - TIAToolbox - Python\nDESCRIPTION: This snippet shows using the PatchPredictor's predict() method to classify image patches, followed by evaluation with accuracy_score and a normalized confusion matrix. The suppress_console_output() context manager is used to silence output during prediction, and patch_list, label_list, and ON_GPU should be defined with patch images, ground truth labels, and a hardware flag, respectively. The confusion matrix is visualized using pandas.DataFrame. Required dependencies: tiatoolbox, scikit-learn (for accuracy_score and confusion_matrix), pandas. Inputs are lists of patches and true labels, and outputs are the predictions, an accuracy score, and a confusion matrix dataframe.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith suppress_console_output():\n    output = predictor.predict(imgs=patch_list, mode=\"patch\", on_gpu=ON_GPU)\n\nacc = accuracy_score(label_list, output[\"predictions\"])\nlogger.info(\"Classification accuracy: %f\", acc)\n\n# Creating and visualizing the confusion matrix for patch classification results\nconf = confusion_matrix(label_list, output[\"predictions\"], normalize=\"true\")\ndf_cm = pd.DataFrame(conf, index=class_names, columns=class_names)\ndf_cm\n```\n\n----------------------------------------\n\nTITLE: FSDP Validation Loop Implementation\nDESCRIPTION: Validation function for evaluating model performance in distributed setting, including accuracy calculation and loss tracking.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef test(model, rank, world_size, test_loader):\n    model.eval()\n    correct = 0\n    ddp_loss = torch.zeros(3).to(rank)\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(rank), target.to(rank)\n            output = model(data)\n            ddp_loss[0] += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            ddp_loss[1] += pred.eq(target.view_as(pred)).sum().item()\n            ddp_loss[2] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n\n    if rank == 0:\n        test_loss = ddp_loss[0] / ddp_loss[2]\n        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n            test_loss, int(ddp_loss[1]), int(ddp_loss[2]),\n            100. * ddp_loss[1] / ddp_loss[2]))\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA-backed Custom Operator with Registration - PyTorch C++\nDESCRIPTION: Implements a CUDA kernel-backed custom operator 'mymuladd' in C++, performing element-wise multiplication and addition on tensors with input checks for sizes, data type, and device. The function ensures inputs are contiguous and launches a CUDA kernel; the operator is registered using TORCH_LIBRARY_IMPL for CUDA. Dependencies include a defined CUDA kernel 'muladd_kernel' and linkage to the PyTorch C++/CUDA extensions. Expects float32 CUDA tensors of identical shape; output is a tensor with the same shape. It must be used in a properly set up CUDA-compatible build environment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nat::Tensor mymuladd_cuda(const at::Tensor& a, const at::Tensor& b, double c) {\n  TORCH_CHECK(a.sizes() == b.sizes());\n  TORCH_CHECK(a.dtype() == at::kFloat);\n  TORCH_CHECK(b.dtype() == at::kFloat);\n  TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CUDA);\n  TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CUDA);\n  at::Tensor a_contig = a.contiguous();\n  at::Tensor b_contig = b.contiguous();\n  at::Tensor result = torch::empty(a_contig.sizes(), a_contig.options());\n  const float* a_ptr = a_contig.data_ptr<float>();\n  const float* b_ptr = b_contig.data_ptr<float>();\n  float* result_ptr = result.data_ptr<float>();\n\n  int numel = a_contig.numel();\n  muladd_kernel<<<(numel+255)/256, 256>>>(numel, a_ptr, b_ptr, c, result_ptr);\n  return result;\n}\n\nTORCH_LIBRARY_IMPL(extension_cpp, CUDA, m) {\n  m.impl(\"mymuladd\", &mymuladd_cuda);\n}\n```\n\n----------------------------------------\n\nTITLE: Training ResNet50 on GPU with IPEX - BFloat16 (Python)\nDESCRIPTION: This snippet shows how to train a ResNet50 model on GPU using IPEX with BFloat16 precision. It includes data loading, model initialization, and the training loop with IPEX optimizations and BFloat16 autocast.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision\nimport intel_extension_for_pytorch as ipex\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((224, 224)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n        root=DATA,\n        train=True,\n        transform=transform,\n        download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset,\n        batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\nmodel = model.to(\"xpu\")\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer, dtype=torch.bfloat16)\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    with torch.xpu.amp.autocast(enabled=True, dtype=torch.bfloat16):\n        output = model(data)\n        loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    print(batch_idx)\ntorch.save({\n     'model_state_dict': model.state_dict(),\n     'optimizer_state_dict': optimizer.state_dict(),\n     }, 'checkpoint.pth')\n\n```\n\n----------------------------------------\n\nTITLE: Loading and evaluating the base ResNet18 model\nDESCRIPTION: This code loads a pre-trained ResNet18 model, evaluates its performance, and measures its size. It serves as a baseline before applying quantization techniques.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/README.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Load pretrained resnet18 model\nfloat_model = models.resnet18(pretrained=True)\n\n# Move the model to CPU\nfloat_model.to('cpu')\n\n# Set the model to evaluation mode\nfloat_model.eval()\n\n# Print the model size\nprint_size_of_model(float_model)\n\n# Evaluate the model performance\nnum_eval_batches = 10\ntop1, top5 = evaluate(float_model, eval_dataloader, neval_batches=num_eval_batches)\nprint('Baseline model accuracy: %2.2f%%' % top1.avg)\n```\n\n----------------------------------------\n\nTITLE: Defining a Statically Quantized Version of a Custom Module in Python\nDESCRIPTION: Outlines the structure for defining a `StaticQuantNonTraceable` class, the statically quantized counterpart to `FP32NonTraceable`. It requires a `from_observed` class method to handle conversion from the observed state.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass StaticQuantNonTraceable:\n\n    @classmethod\n    def from_observed(cls, ...):\n        ...\n```\n\n----------------------------------------\n\nTITLE: GPU Usage in PyTorch\nDESCRIPTION: Code snippets for managing GPU usage in PyTorch, including checking CUDA availability, moving tensors between CPU and GPU, and writing device-agnostic code. These utilities enable efficient utilization of GPU resources for deep learning tasks.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntorch.cuda.is_available()                                 # check for cuda\nx.cuda()                                                # move x's data from CPU to GPU and return new object\nx.cpu()                                                 # move x's data from GPU to CPU and return new object\n\nif not args.disable_cuda and torch.cuda.is_available(): # device agnostic code and modularity\n    args.device = torch.device('cuda')                  #\nelse:                                                   #\n    args.device = torch.device('cpu')                   #\n\nnet.to(device)                                          # recursively convert their parameters and buffers to device specific tensors\nmytensor.to(device)                                     # copy your tensors to a device (gpu, cpu)\n```\n\n----------------------------------------\n\nTITLE: Configuring Backward Prefetch in PyTorch FSDP\nDESCRIPTION: This snippet shows how to configure backward prefetch in FSDP. Setting it to BACKWARD_PRE allows the next FSDP unit's parameters to be requested earlier, potentially increasing training speed at the cost of slightly higher memory consumption.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntorch.cuda.set_device(local_rank)\n\nmodel = FSDP(model,\n       auto_wrap_policy=t5_auto_wrap_policy,\n       mixed_precision=bfSixteen,\n       device_id=torch.cuda.current_device(),\n       backward_prefetch = BackwardPrefetch.BACKWARD_PRE)\n```\n\n----------------------------------------\n\nTITLE: Loading Serialized Quantized BERT Model in Python\nDESCRIPTION: This snippet shows how to load a previously serialized quantized BERT model using torch.jit.load. This allows for easy reuse of the quantized model in future applications.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nloaded_quantized_model = torch.jit.load(\"bert_traced_eager_quant.pt\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up BERT Model and SQuAD Dataset for Question Answering in PyTorch\nDESCRIPTION: Code that loads a pre-trained BERT model and tokenizer, then sets up the SQuAD dataset for training by applying preprocessing functions. It also creates a data collator for batching and padding the tokenized data.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# load model\nmodel_name = \"bert-base-cased\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\nmodel = transformers.AutoModelForQuestionAnswering.from_pretrained(model_name)\nprint(f\"Loading tokenizer: {model_name}\")\nprint(f\"Loading model: {model_name}\")\n\n# set up train and val dataset\nsquad_dataset = datasets.load_dataset(\"squad\")\ntokenized_squad_dataset = {}\ntokenized_squad_dataset[\"train\"] = squad_dataset[\"train\"].map(\n    lambda x: preprocess_train_function(x, tokenizer), batched=True\n)\ntokenized_squad_dataset[\"validation\"] = squad_dataset[\"validation\"].map(\n    lambda x: preprocess_validation_function(x, tokenizer),\n    batched=True,\n    remove_columns=squad_dataset[\"train\"].column_names,\n)\ndata_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for Building a LibTorch C++ Application\nDESCRIPTION: This snippet provides a minimal CMake configuration for building a C++ application with LibTorch. It requires CMake >=3.0 and a valid LibTorch installation. The configuration uses find_package(Torch REQUIRED) and links the Torch libraries to the binary target. The project must specify C++17 standard and include the example-app.cpp file.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_export.rst#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\\nproject(custom_ops)\\n\\nfind_package(Torch REQUIRED)\\n\\nadd_executable(example-app example-app.cpp)\\ntarget_link_libraries(example-app \"${TORCH_LIBRARIES}\")\\nset_property(TARGET example-app PROPERTY CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Autograd Function and Registering Kernels in PyTorch C++ Backend\nDESCRIPTION: This C++ snippet demonstrates how to create a custom subclass of torch::autograd::Function to manually control forward and backward (autograd) behavior for an operator. The MyAddFunction implements both forward and backward; a wrapper function myadd_autograd invokes the custom autograd. Kernel registration uses separate dispatch keys for autograd and inference (AutogradPrivateUse1 and PrivateUse1, respectively). Actual schemas (<myadd_schema>) must be replaced with the correct PyTorch operator signatures.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/extend_dispatcher.rst#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nclass MyAddFunction : public torch::autograd::Function<MyAddFunction> {\n  public:\n  static Tensor forward(AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n    at::AutoNonVariableTypeMode g;\n    return myadd(self, other);\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto grad_output = grad_outputs[0];\n    return {grad_output, grad_output};\n  }\n};\n\nTensor myadd_autograd(const Tensor& self, const Tensor& other) {\n  return MyAddFunction::apply(self, other)[0];\n}\n\n// Register the autograd kernel to AutogradPrivateUse1\nTORCH_LIBRARY_IMPL(aten, AutogradPrivateUse1, m) {\n  m.impl(<myadd_schema>, &myadd_autograd);\n}\n\n// Register the inference kernel to PrivateUse1\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(<myadd_schema>, &myadd);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting TorchScript Graph of compute() - Python (textual graph output)\nDESCRIPTION: Illustrates the TorchScript IR graph of the compute function after being scripted. The graph shows tensor selection, comparison, conditionals, matrix multiplication, and addition as low-level primitives. Output via compute.graph property in Python; not code but essential for understanding model structure.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> compute.graph\ngraph(%x : Dynamic\n    %y : Dynamic) {\n  %14 : int = prim::Constant[value=1]()\n  %2 : int = prim::Constant[value=0]()\n  %7 : int = prim::Constant[value=42]()\n  %z.1 : int = prim::Constant[value=5]()\n  %z.2 : int = prim::Constant[value=10]()\n  %4 : Dynamic = aten::select(%x, %2, %2)\n  %6 : Dynamic = aten::select(%4, %2, %2)\n  %8 : Dynamic = aten::eq(%6, %7)\n  %9 : bool = prim::TensorToBool(%8)\n  %z : int = prim::If(%9)\n    block0() {\n      -> (%z.1)\n    }\n    block1() {\n      -> (%z.2)\n    }\n  %13 : Dynamic = aten::matmul(%x, %y)\n  %15 : Dynamic = aten::add(%13, %z, %14)\n  return (%15);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring and Running a Faulty PyTorch Distributed Script with Flight Recorder Enabled\nDESCRIPTION: This Python script sets up a PyTorch distributed environment using `torch.distributed.init_process_group`. It configures environment variables to enable NCCL debugging and Flight Recorder tracing (`TORCH_NCCL_DEBUG_INFO_TEMP_FILE`, `TORCH_NCCL_DUMP_ON_TIMEOUT`, `TORCH_NCCL_TRACE_BUFFER_SIZE`). The script then performs several `all_reduce` collective operations. Crucially, rank 0 performs an additional `all_reduce` operation that other ranks do not, intentionally causing a hang or timeout that Flight Recorder can capture. It depends on `torch`, `torch.distributed`, `os`, and `datetime`. The script expects environment variables `LOCAL_RANK`, `WORLD_SIZE`, `MASTER_ADDR`, and `MASTER_PORT` to be set, typically by a launcher like `torchrun`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\nimport os\nimport torch\nimport torch.distributed as dist\n\nlocal_rank = int(os.environ[\"LOCAL_RANK\"])\nworld_size = int(os.environ[\"WORLD_SIZE\"])\nassert world_size <= 8, \"world size must be less than or equal to 8\"\nos.environ[\"TORCH_NCCL_DEBUG_INFO_TEMP_FILE\"] = \"/tmp/trace_\"\nos.environ[\"TORCH_NCCL_DUMP_ON_TIMEOUT\"] = \"1\"\nos.environ[\"TORCH_NCCL_TRACE_BUFFER_SIZE\"] = \"2000\"\ndevice = torch.device(f\"cuda:{local_rank}\")\nprint(f\"{local_rank=} {world_size=} master addr: {os.environ['MASTER_ADDR']} master port: {os.environ['MASTER_PORT']} {device=}\")\n\n# Initialize the process group with a small timeout so that jobs fail quickly\ndist.init_process_group(\"nccl\", world_size=world_size, rank=local_rank, timeout=timedelta(seconds=1))\n\na = torch.full((3, 4), float(local_rank), device=device)\n# Write some collectives to populate Flight Recorder data\nfor i in range(2):\n  print(f\"calling allreduce on {local_rank=}\")\n  f = dist.all_reduce(a)\n\n# rank0 is doing an additional collective\nif local_rank == 0:\n  print(\"rank0 is doing an allreduce on tensor b, but other ranks forgot\")\n  b = torch.full((4,5), float(local_rank), device=device)\n  f = dist.all_reduce(b)\n\nfor i in range(2):\n  print(f\"calling allreduce on {local_rank=}\")\n  f = dist.all_reduce(a)\n\ntorch.cuda.synchronize(device=device)\nprint(f\"{local_rank=} exiting\")\n```\n\n----------------------------------------\n\nTITLE: Comparing FX Graph Mode and Eager Mode Quantized Models - PyTorch - Python\nDESCRIPTION: Evaluates and compares both FX graph mode and eager mode quantized ResNet18 models for size and accuracy, and serializes the eager quantized model. This snippet highlights consistency in model accuracy and storage footprint between approaches. It uses torchvision, PyTorch's quantization API, and requires a valid criterion and test dataset loader.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n    print(\"Size of Fx graph mode quantized model\")\n    print_size_of_model(quantized_model)\n    top1, top5 = evaluate(quantized_model, criterion, data_loader_test)\n    print(\"FX graph mode quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n\n    from torchvision.models.quantization.resnet import resnet18\n    eager_quantized_model = resnet18(pretrained=True, quantize=True).eval()\n    print(\"Size of eager mode quantized model\")\n    eager_quantized_model = torch.jit.script(eager_quantized_model)\n    print_size_of_model(eager_quantized_model)\n    top1, top5 = evaluate(eager_quantized_model, criterion, data_loader_test)\n    print(\"eager mode quantized model Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n    eager_mode_model_file = \"resnet18_eager_mode_quantized.pth\"\n    torch.jit.save(eager_quantized_model, saved_model_dir + eager_mode_model_file)\n```\n\n----------------------------------------\n\nTITLE: Launching DDP Training Job via torchrun CLI - PyTorch - Bash\nDESCRIPTION: Provides a bash command to launch a distributed DDP job using 'torchrun' with multiple processes and nodes, specifying rendezvous settings and the target Python script (elastic_ddp.py). Assumes a properly configured environment, including $MASTER_ADDR and elastic_ddp.py script. Intended for Linux cluster environments supporting torchrun.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes=2 --nproc_per_node=8 --rdzv_id=100 --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:29400 elastic_ddp.py\n```\n\n----------------------------------------\n\nTITLE: Defining DCGAN Discriminator with nn::Sequential in PyTorch C++\nDESCRIPTION: Constructs a DCGAN discriminator model using the `nn::Sequential` container in the PyTorch C++ API. The model consists of four convolutional layers (`nn::Conv2d`) with batch normalization (`nn::BatchNorm2d`) and LeakyReLU activations (`nn::LeakyReLU`), followed by a Sigmoid layer (`nn::Sigmoid`) to output a probability. Uses `nn::Conv2dOptions` and `nn::LeakyReLUOptions` for detailed layer configuration, such as stride, padding, bias, and negative slope.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\n  nn::Sequential discriminator(\n    // Layer 1\n    nn::Conv2d(\n        nn::Conv2dOptions(1, 64, 4).stride(2).padding(1).bias(false)),\n    nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n    // Layer 2\n    nn::Conv2d(\n        nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).bias(false)),\n    nn::BatchNorm2d(128),\n    nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n    // Layer 3\n    nn::Conv2d(\n        nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).bias(false)),\n    nn::BatchNorm2d(256),\n    nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n    // Layer 4\n    nn::Conv2d(\n        nn::Conv2dOptions(256, 1, 3).stride(1).padding(0).bias(false)),\n    nn::Sigmoid());\n```\n\n----------------------------------------\n\nTITLE: Selecting Action with Asynchronous Batching in Agent (Python)\nDESCRIPTION: Implements the static `select_action_batch` method for the `Agent`, leveraging `@rpc.functions.async_execution` for efficient batching. When called by an observer, it copies the observer's state into a pre-allocated tensor `self.states`. It returns a `torch.futures.Future` that will eventually hold the action for that specific observer. Using a lock, it decrements `self.pending_states`. When this counter reaches zero (indicating states from all observers have arrived), it performs a single forward pass through the policy network using the batched `self.states` (assumed on CUDA), samples actions for all observers simultaneously using `Categorical`, saves the batch of log probabilities, and sets the results on the shared `self.future_actions` Future. This triggers the callbacks on the individual futures returned to observers, sending them their respective actions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Agent:\n    ...\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def select_action_batch(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        self.states[ob_id].copy_(state)\n        future_action = self.future_actions.then(\n            lambda future_actions: future_actions.wait()[ob_id].item()\n        )\n\n        with self.lock:\n            self.pending_states -= 1\n            if self.pending_states == 0:\n                self.pending_states = len(self.ob_rrefs)\n                probs = self.policy(self.states.cuda()) # Assumes states need moving to CUDA\n                m = Categorical(probs)\n                actions = m.sample()\n                self.saved_log_probs.append(m.log_prob(actions).t()[0]) # Transpose might depend on Categorical/policy output shape\n                future_actions = self.future_actions\n                self.future_actions = torch.futures.Future()\n                future_actions.set_result(actions.cpu()) # Move actions to CPU before setting result\n        return future_action\n```\n\n----------------------------------------\n\nTITLE: Defining a Pattern for Subgraph Matcher (conv + relu) - PyTorch - Python\nDESCRIPTION: This function constructs a functional reference pattern (conv2d followed by relu) for use with SubgraphMatcherWithNameNodeMap, which supports annotation by name mapping. This is foundational for writing generalized pattern matchers that trigger annotations in PT2E quantization flows. Inputs are activations, weights, and biases; the output is a tuple of the pattern and a node name to IR node mapping.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef conv_relu_pattern(input, weight, bias):\n    conv = torch.nn.functional.conv2d(input, weight, bias)\n    output = torch.nn.functional.relu(conv)\n    # returns an additional dict that includes a map from name to node that we want to annotate\n```\n\n----------------------------------------\n\nTITLE: Converting the Calibrated Model to a Quantized Model in Python\nDESCRIPTION: Uses the `convert_pt2e` function to convert the prepared and calibrated model into a fully quantized model. This function replaces observers with quantization/dequantization nodes and adjusts weights based on the collected statistics.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconverted_model = convert_pt2e(prepared_model)\n```\n\n----------------------------------------\n\nTITLE: Defining Gradient-only BFloat16 Precision in PyTorch FSDP\nDESCRIPTION: This snippet shows how to set only the gradient communication to happen in reduced precision (bfloat16) while keeping parameters and buffers in full precision. This can be useful when intra-node communication is the main bottleneck.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ngrad_bf16 = MixedPrecision(reduce_dtype=torch.bfloat16)\n```\n\n----------------------------------------\n\nTITLE: Performance-only Quantization with Dummy Dataset\nDESCRIPTION: This snippet shows how to perform performance-only quantization using Intel Neural Compressor with a dummy dataset for a PyTorch FX graph mode model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nmodel.eval()\n\n# launch code for Intel® Neural Compressor\nfrom neural_compressor.experimental import Quantization, common\nfrom neural_compressor.experimental.data.datasets.dummy_dataset import DummyDataset\nquantizer = Quantization(\"./conf.yaml\")\nquantizer.model = model\nquantizer.calib_dataloader = common.DataLoader(DummyDataset([(1, 1, 28, 28)]))\nq_model = quantizer()\nq_model.save('./output')\n```\n\n----------------------------------------\n\nTITLE: Reading and Preparing Patch Data for Classification - Python\nDESCRIPTION: This snippet generates lists of patch file paths and their corresponding class labels using a directory of extracted patch images. It defines a label dictionary mapping tissue types to numerical IDs, aggregates all patch file paths for each class, and extends the label list accordingly. It visualizes the dataset distribution with matplotlib and logs summary statistics using the TIAToolbox logger. Key parameters are dataset_path and image_ext; dependencies include matplotlib, pathlib, and TIAToolbox utilities. The code outputs label and patch lists, produces a bar chart, and logs dataset statistics for class distribution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Read the patch data and create a list of patches and a list of corresponding labels\ndataset_path = global_save_dir / \"kather100k-validation-sample\"\n\n# Set the path to the dataset\nimage_ext = \".tif\"  # file extension of each image\n\n# Obtain the mapping between the label ID and the class name\nlabel_dict = {\n    \"BACK\": 0, # Background (empty glass region)\n    \"NORM\": 1, # Normal colon mucosa\n    \"DEB\": 2,  # Debris\n    \"TUM\": 3,  # Colorectal adenocarcinoma epithelium\n    \"ADI\": 4,  # Adipose\n    \"MUC\": 5,  # Mucus\n    \"MUS\": 6,  # Smooth muscle\n    \"STR\": 7,  # Cancer-associated stroma\n    \"LYM\": 8,  # Lymphocytes\n}\n\nclass_names = list(label_dict.keys())\nclass_labels = list(label_dict.values())\n\n# Generate a list of patches and generate the label from the filename\npatch_list = []\nlabel_list = []\nfor class_name, label in label_dict.items():\n    dataset_class_path = dataset_path / class_name\n    patch_list_single_class = grab_files_from_dir(\n        dataset_class_path,\n        file_types=\"*\" + image_ext,\n    )\n    patch_list.extend(patch_list_single_class)\n    label_list.extend([label] * len(patch_list_single_class))\n\n# Show some dataset statistics\nplt.bar(class_names, [label_list.count(label) for label in class_labels])\nplt.xlabel(\"Patch types\")\nplt.ylabel(\"Number of patches\")\n\n# Count the number of examples per class\nfor class_name, label in label_dict.items():\n    logger.info(\n        \"Class ID: %d -- Class Name: %s -- Number of images: %d\",\n        label,\n        class_name,\n        label_list.count(label),\n    )\n\n# Overall dataset statistics\nlogger.info(\"Total number of patches: %d\", (len(patch_list)))\n\n```\n\n----------------------------------------\n\nTITLE: Partitioning MNIST Dataset with Transforms and Distributed DataLoader (Python)\nDESCRIPTION: Implements a helper function to partition the MNIST dataset for distributed training in PyTorch. This function downloads and transforms the dataset, determines partition sizes based on the world size, and partitions data according to the rank of the current process. Requires torch, torchvision.datasets, torchvision.transforms, torch.distributed, and the DataPartitioner class. Inputs are implicit but include distributed rank/size and the location for MNIST data storage, outputs are a DataLoader for the current partition and its batch size. Constraints: expects the distributed process group to be initialized.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n\"\"\" Partitioning MNIST \"\"\"\ndef partition_dataset():\n    dataset = datasets.MNIST('./data', train=True, download=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))\n                             ]))\n    size = dist.get_world_size()\n    bsz = 128 // size\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(partition,\n                                         batch_size=bsz,\n                                         shuffle=True)\n    return train_set, bsz\n```\n\n----------------------------------------\n\nTITLE: Implementing Blocking Point-to-Point Communication in PyTorch\nDESCRIPTION: Demonstrates blocking point-to-point communication using dist.send() and dist.recv(). Process 0 sends a tensor to process 1, which receives it. Both processes block until the communication is completed.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        dist.send(tensor=tensor, dst=1)\n    else:\n        # Receive tensor from process 0\n        dist.recv(tensor=tensor, src=0)\n    print('Rank ', rank, ' has data ', tensor[0])\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-quantized MobileNetV2 Model (Python)\nDESCRIPTION: Loads a pre-trained and pre-quantized version of the MobileNetV2 model provided by torchvision. Setting `pretrained=True` downloads weights trained on ImageNet, and `quantize=True` ensures the model uses int8 operations optimized for the 'qnnpack' engine. Requires `torchvision.models`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import models\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Computation Graph for Custom Square Function - torchviz - Python\nDESCRIPTION: Visualizes the gradient computation graph for a custom autograd function using torchviz. After computing the gradient, torchviz.make_dot is used to highlight dependencies among input, output, and gradient tensors. Requires torch, torchviz, and enables analysis of higher-order derivative paths.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torchviz\n\nx = torch.tensor(1., requires_grad=True).clone()\nout = Square.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out})\n\n```\n\n----------------------------------------\n\nTITLE: Using Custom LSTM Variant in Python\nDESCRIPTION: Example of how to use the custom LLTM implementation in Python code.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nnew_h, new_C = rnn(X, (h, C))\n```\n\n----------------------------------------\n\nTITLE: Simulating Row-Wise Sharding with PyTorch SPMD Simulation (Python)\nDESCRIPTION: This snippet demonstrates how to simulate row-wise sharding of embedding tables using the spmd_sharing_simulation function in Python with PyTorch. Row-wise sharding splits large tables by rows to distribute memory load across devices, enabling support for tables too large for a single device. Dependencies include access to the spmd_sharing_simulation function and the ShardingType enum. The function likely logs or prints the resulting sharding plans for analysis.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/sharding.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nspmd_sharing_simulation(ShardingType.ROW_WISE)\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Actions and Reward Handling\nDESCRIPTION: Methods for the Agent class to handle action selection and reward reporting from observers via RPC calls. Includes state processing and action sampling using the policy network.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Agent:\n    ...\n    def select_action(self, ob_id, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        probs = self.policy(state)\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item()\n\n    def report_reward(self, ob_id, reward):\n        self.rewards[ob_id].append(reward)\n```\n\n----------------------------------------\n\nTITLE: Composing Fork/Wait with Loops in TorchScript\nDESCRIPTION: Shows how to combine dynamic parallelism with loops to launch multiple parallel tasks. This example creates 100 parallel instances of a function, collects their results, and sums them.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch-script-parallelism.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom typing import List\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for _ in range(100):\n        futures.append(torch.jit.fork(foo, x))\n\n    results = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.sum(torch.stack(results))\n\nprint(example(torch.ones([])))\n```\n\n----------------------------------------\n\nTITLE: Compiling the Optimizer Step Function with torch.compile - PyTorch - Python\nDESCRIPTION: This snippet defines a helper function that wraps the Adam optimizer's step() method and compiles it using torch.compile with fullgraph=False, allowing for dynamic graph compilation. The compiled function is intended for performance benchmarking and provides a direct comparison with the original (eager) optimizer step. Prerequisites include PyTorch 2.2+ and a compatible CUDA device. No direct inputs or outputs; function impacts optimizer state.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/compiling_optimizer.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile(fullgraph=False)\\ndef fn():\\n    opt.step()\n```\n\n----------------------------------------\n\nTITLE: Finalizing RL Episode and Updating Model in PyTorch Distributed Agent (Python)\nDESCRIPTION: Implements the Agent class finish_episode method for aggregating rewards and log probabilities from multiple observers and updating the reinforcement learning policy via gradient descent. The method computes the running reward using the minimum of the episode rewards, normalizes returns, computes policy loss, runs backward propagation on the policy, and performs an optimization step. Dependencies include torch (torch.tensor), optimizer, agent arguments (gamma, log_interval), and observer data collection. Returns the minimum reward per episode and clears temporary storage.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass Agent:\n    ...\n    def finish_episode(self):\n      # joins probs and rewards from different observers into lists\n      R, probs, rewards = 0, [], []\n      for ob_id in self.rewards:\n          probs.extend(self.saved_log_probs[ob_id])\n          rewards.extend(self.rewards[ob_id])\n\n      # use the minimum observer reward to calculate the running reward\n      min_reward = min([sum(self.rewards[ob_id]) for ob_id in self.rewards])\n      self.running_reward = 0.05 * min_reward + (1 - 0.05) * self.running_reward\n\n      # clear saved probs and rewards\n      for ob_id in self.rewards:\n          self.rewards[ob_id] = []\n          self.saved_log_probs[ob_id] = []\n\n      policy_loss, returns = [], []\n      for r in rewards[::-1]:\n          R = r + args.gamma * R\n          returns.insert(0, R)\n      returns = torch.tensor(returns)\n      returns = (returns - returns.mean()) / (returns.std() + self.eps)\n      for log_prob, R in zip(probs, returns):\n          policy_loss.append(-log_prob * R)\n      self.optimizer.zero_grad()\n      policy_loss = torch.cat(policy_loss).sum()\n      policy_loss.backward()\n      self.optimizer.step()\n      return min_reward\n\n```\n\n----------------------------------------\n\nTITLE: Bundling Inputs for Multiple Methods - PyTorch - Python\nDESCRIPTION: Demonstrates bundling of sample inputs for multiple TorchScript-exported methods in a model. The sample_input dictionary maps methods to tuples of corresponding input arguments. This creates a bundled_model where each callable can have its own test inputs attached, supporting advanced use cases for testing or profiling multi-call models.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/bundled_inputs.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# For each method create a list of inputs and each input is a tuple of arguments\\nexample_dict = {'a' : 1, 'b' : 2}\\nsample_input = {\\n    scripted_module.forward : [(torch.zeros(1,10),)],\\n    scripted_module.foo : [(example_dict,)]\\n}\\n\\n# Create model with bundled inputs, if type(sample_input) is Dict then each callable key is mapped to its corresponding bundled input\\nbundled_model = bundle_inputs(scripted_module, sample_input)\n```\n\n----------------------------------------\n\nTITLE: Custom Class Serialization Implementation - C++\nDESCRIPTION: Implementation of def_pickle for custom C++ class serialization, defining __getstate__ and __setstate__ methods for the MyStackClass.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nBEGIN def_pickle\nEND def_pickle\n```\n\n----------------------------------------\n\nTITLE: Using BFloat16 Autocast for Mixed Precision Quantization in PyTorch Inductor\nDESCRIPTION: This snippet demonstrates how to wrap a model in torch.autocast context to enable int8-mixed-bf16 quantization. When lowered into the Inductor CPP Backend, various operators will handle int8 computation with either int8 or BFloat16 output based on the presence of quantization nodes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwith torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16, enabled=True), torch.no_grad():\n    # Turn on Autocast to use int8-mixed-bf16 quantization. After lowering into Inductor CPP Backend,\n    # For operators such as QConvolution and QLinear:\n    # * The input data type is consistently defined as int8, attributable to the presence of a pair\n        of quantization and dequantization nodes inserted at the input.\n    # * The computation precision remains at int8.\n    # * The output data type may vary, being either int8 or BFloat16, contingent on the presence\n    #   of a pair of quantization and dequantization nodes at the output.\n    # For non-quantizable pointwise operators, the data type will be inherited from the previous node,\n    # potentially resulting in a data type of BFloat16 in this scenario.\n    # For quantizable pointwise operators such as QMaxpool2D, it continues to operate with the int8\n    # data type for both input and output.\n    optimized_model = torch.compile(converted_model)\n\n    # Running some benchmark\n    optimized_model(*example_inputs)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Worker Processes for Distributed Training in PyTorch\nDESCRIPTION: Initializes the RPC framework and launches parameter server and trainer processes. Uses multiprocessing to spawn separate processes for distributed training roles.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 1:\n        rpc.init_rpc(\"trainer\", rank=rank, world_size=world_size)\n        _run_trainer()\n    else:\n        rpc.init_rpc(\"ps\", rank=rank, world_size=world_size)\n        # parameter server do nothing\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__==\"__main__\":\n    world_size = 2\n    mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Non-Blocking Point-to-Point Communication in PyTorch\nDESCRIPTION: Shows non-blocking point-to-point communication using dist.isend() and dist.irecv(). Process 0 sends a tensor to process 1 asynchronously. The script continues execution and uses req.wait() to ensure communication completion.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Non-blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    req = None\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        req = dist.isend(tensor=tensor, dst=1)\n        print('Rank 0 started sending')\n    else:\n        # Receive tensor from process 0\n        req = dist.irecv(tensor=tensor, src=0)\n        print('Rank 1 started receiving')\n    req.wait()\n    print('Rank ', rank, ' has data ', tensor[0])\n```\n\n----------------------------------------\n\nTITLE: C++ Inference Script\nDESCRIPTION: C++ code showing how to load and run a TorchScript model that uses the custom class.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/script.h>\n#include <iostream>\n#include <memory>\n\nint main(int argc, const char* argv[]) {\n  torch::jit::Module module = torch::jit::load(\"foo.pt\");\n  \n  std::vector<torch::jit::IValue> inputs;\n  module.run_method(\"forward\", inputs);\n  \n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Training Loop Modification for Resume Support\nDESCRIPTION: Changes to the training loop to support resuming from the last completed epoch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ddp_series_fault_tolerance.rst#2025-04-22_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\ndef train(self, max_epochs: int):\n-  for epoch in range(max_epochs):\n+  for epoch in range(self.epochs_run, max_epochs):\n      self._run_epoch(epoch)\n```\n\n----------------------------------------\n\nTITLE: Generating Methods/Properties for Custom PyTorch Backend (Python)\nDESCRIPTION: Automatically generates backend-specific methods (e.g., `Tensor.npu()`) and properties (e.g., `Tensor.is_npu`) for Tensor, Module, and Storage classes after renaming the PrivateUse1 backend using `torch.rename_privateuse1_backend`. The `torch.utils.generate_methods_for_privateuse1_backend` function takes flags (`for_tensor`, `for_module`, `for_storage`) to control generation and an optional `unsupported_dtype` list to exclude specific data types.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntorch.rename_privateuse1_backend(\"npu\")\nunsupported_dtype = [torch.quint8]\ntorch.utils.generate_methods_for_privateuse1_backend(for_tensor=True, for_module=True, for_storage=True, unsupported_dtype=unsupported_dtype)\n```\n\n----------------------------------------\n\nTITLE: Script Execution Modification for torchrun\nDESCRIPTION: Changes to the script execution method to use torchrun instead of manual multiprocessing.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ddp_series_fault_tolerance.rst#2025-04-22_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\nif __name__ == \"__main__\":\n   import sys\n   total_epochs = int(sys.argv[1])\n   save_every = int(sys.argv[2])\n-  world_size = torch.cuda.device_count()\n-  mp.spawn(main, args=(world_size, total_epochs, save_every,), nprocs=world_size)\n+  main(save_every, total_epochs)\n\n- python multigpu.py 50 10\n+ torchrun --standalone --nproc_per_node=4 multigpu_torchrun.py 50 10\n```\n\n----------------------------------------\n\nTITLE: Visualizing Whole-Slide Image Thumbnails with TIAToolbox\nDESCRIPTION: Generates and displays a thumbnail of a whole-slide image at a specified resolution. The code opens a WSI using WSIReader and creates a thumbnail at a specified magnification (measured in microns per pixel).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\noverview_resolution = (\n    4  # the resolution in which we desire to merge and visualize the patch predictions\n)\n# the unit of the `resolution` parameter. Can be \"power\", \"level\", \"mpp\", or \"baseline\"\noverview_unit = \"mpp\"\nwsi = WSIReader.open(wsi_path)\nwsi_overview = wsi.slide_thumbnail(resolution=overview_resolution, units=overview_unit)\nplt.figure(), plt.imshow(wsi_overview)\nplt.axis(\"off\")\n```\n\n----------------------------------------\n\nTITLE: Bundling Example Inputs to Scripted Model - PyTorch - Python\nDESCRIPTION: Demonstrates how to create a list of example inputs (for 'forward') and attach them to a TorchScript module using the bundle_inputs utility. The sample input tuple must match the model input signature. This step creates a bunded_model with embedded sample inputs, suitable for later retrieval or testing. bundle_inputs comes from torch.utils.bundled_inputs; ensure it is imported.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/bundled_inputs.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# For each method create a list of inputs and each input is a tuple of arguments\\nsample_input = [(torch.zeros(1,10),)]\\n\\n# Create model with bundled inputs, if type(input) is list then the input is bundled to 'forward'\\nbundled_model = bundle_inputs(scripted_module, sample_input)\n```\n\n----------------------------------------\n\nTITLE: Iterating Over Batches from a Data Loader in PyTorch C++\nDESCRIPTION: Provides an example loop for iterating through batches produced by a PyTorch C++ `data_loader`. It accesses the batch (`torch::data::Example`), which contains `data` and `target` fields due to the `Stack` collation. The code prints the size of the batch data tensor along the first dimension and the integer values of the target labels for demonstration. Requires an instantiated `data_loader`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_21\n\nLANGUAGE: cpp\nCODE:\n```\n  for (torch::data::Example<>& batch : *data_loader) {\n    std::cout << \"Batch size: \" << batch.data.size(0) << \" | Labels: \";\n    for (int64_t i = 0; i < batch.data.size(0); ++i) {\n      std::cout << batch.target[i].item<int64_t>() << \" \";\n    }\n    std::cout << std::endl;\n  }\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchServe CPU Launcher for IPEX\nDESCRIPTION: Provides configuration settings for TorchServe's `config.properties` file to enable and configure the CPU launcher provided by Intel® Extension for PyTorch*. This specific configuration enables the launcher and directs it to bind the workload to the physical cores of the first CPU socket (node 0) for optimized performance and resource allocation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchserve_with_ipex_2.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\ncpu_launcher_enable=true\ncpu_launcher_args=--node_id 0\n```\n\n----------------------------------------\n\nTITLE: Implementing static post-training quantization on ResNet18\nDESCRIPTION: This snippet demonstrates static post-training quantization on the ResNet18 model. It defines a quantization-compatible model, adds observers to collect statistics during calibration, and converts the model to int8.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/README.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Define a quantization-compatible model class for static quantization\nclass QuantizableResnet18(nn.Module):\n    def __init__(self, model_fp32):\n        super(QuantizableResnet18, self).__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.quantization.QuantStub()\n        self.model_fp32 = model_fp32\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.quantization.DeQuantStub()\n    \n    def forward(self, x):\n        # Quantize inputs\n        x = self.quant(x)\n        # Forward pass through the model\n        x = self.model_fp32(x)\n        # Dequantize outputs\n        x = self.dequant(x)\n        return x\n\n# Create a quantization-compatible model\nqmodel = QuantizableResnet18(float_model)\n\n# Set the model to CPU and evaluation mode\nqmodel.to('cpu')\nqmodel.eval()\n\n# Set the quantization configuration\nqmodel.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n\n# Prepare the model for static quantization\ntorch.quantization.prepare(qmodel, inplace=True)\n\n# Calibrate the model with the training data\nwith torch.no_grad():\n    for i, (image, target) in enumerate(train_dataloader):\n        qmodel(image)\n        if i >= 10:\n            break\n\n# Convert the model to a quantized version\ntorch.quantization.convert(qmodel, inplace=True)\n\n# Print the size of the quantized model\nprint_size_of_model(qmodel)\n\n# Evaluate the quantized model\ntop1, top5 = evaluate(qmodel, eval_dataloader, neval_batches=num_eval_batches)\nprint('Quantized model accuracy: %2.2f%%' % top1.avg)\n```\n\n----------------------------------------\n\nTITLE: Loading QAT Model Checkpoints\nDESCRIPTION: Code for loading QAT model checkpoints by recreating the exact export and preparation steps that were used initially. This ensures the model structure matches the saved state.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._export import capture_pre_autograd_graph\nfrom torch.ao.quantization.quantizer.xnnpack_quantizer import (\n    XNNPACKQuantizer,\n    get_symmetric_quantization_config,\n)\nfrom torchvision.models.resnet import resnet18\n\nexample_inputs = (torch.rand(2, 3, 224, 224),)\nfloat_model = resnet18(pretrained=False)\nexported_model = capture_pre_autograd_graph(float_model, example_inputs)\nquantizer = XNNPACKQuantizer()\nquantizer.set_global(get_symmetric_quantization_config(is_qat=True))\nprepared_model = prepare_qat_pt2e(exported_model, quantizer)\nprepared_model.load_state_dict(torch.load(checkpoint_path))\n\n# resume training or perform inference\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Post-Training Quantization for X86 Inductor in Python\nDESCRIPTION: Instantiates the `X86InductorQuantizer` specific to the x86 backend and applies the default static quantization configuration using `set_global` and `get_default_x86_inductor_quantization_config`. This sets up the quantizer for static PTQ.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquantizer = X86InductorQuantizer()\nquantizer.set_global(xiq.get_default_x86_inductor_quantization_config())\n```\n\n----------------------------------------\n\nTITLE: Evaluating Fused MobileNetV2 and Model Serialization - PyTorch - Python\nDESCRIPTION: Evaluates the fused MobileNetV2 model using a specified criterion and data loader, prints model size, and serializes the scripted model using torch.jit.save. The evaluation function computes accuracy statistics, and the model is saved both for later PyTorch use and to facilitate further quantization steps. Dependencies include torch, criterion function, and prepared data loaders. Accuracy and model size serve as baselines for subsequent quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnum_eval_batches = 1000\n\nprint(\"Size of baseline model\") \nprint_size_of_model(float_model)  \n\ntop1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches) \nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg)) \ntorch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)\n```\n\n----------------------------------------\n\nTITLE: Running a C++ Application with TorchScript Operator\nDESCRIPTION: Shell command to execute a C++ application that uses a serialized PyTorch model with a custom TorchScript operator, showing the output tensor.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\n$ ./example_app example.pt\n11.4125   5.8262   9.5345   8.6111  12.3997\n 7.4683  13.5969   9.0850  11.0698   9.4008\n 7.4597  15.0926  12.5727   8.9319   9.0666\n 9.4834  11.1747   9.0162  10.9521   8.6269\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n[ Variable[CPUFloatType]{8,5} ]\n```\n\n----------------------------------------\n\nTITLE: Initializing Process Group with Shared File System in PyTorch\nDESCRIPTION: Initializes a distributed process group using a shared file system that all processes can access. The shared file is used for coordination between processes, with each process writing its information to the file and waiting for others to do the same.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndist.init_process_group(\n    init_method='file:///mnt/nfs/sharedfile',\n    rank=args.rank,\n    world_size=4)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch for AWS Graviton\nDESCRIPTION: Command to install PyTorch which supports AWS Graviton3 optimizations starting with version 2.0.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install torch\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for PyTorch with Intel Extension\nDESCRIPTION: This CMake configuration file sets up a C++ project that links against both PyTorch (libtorch) and Intel Extension for PyTorch. It automatically enables Intel optimizations when the extension is found.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(example-app)\n\nfind_package(intel_ext_pt_cpu REQUIRED)\n\nadd_executable(example-app example-app.cpp)\ntarget_link_libraries(example-app \"${TORCH_LIBRARIES}\")\n\nset_property(TARGET example-app PROPERTY CXX_STANDARD 14)\n```\n\n----------------------------------------\n\nTITLE: Setting Up RPC Workers for Distributed Reinforcement Learning in Python\nDESCRIPTION: This function initializes RPC for both agent and observer roles, runs episodes, and handles logging. It demonstrates how to set up a distributed environment for reinforcement learning tasks.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef run_worker(rank, world_size, n_episode, batch, print_log=True):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size, batch)\n        for i_episode in range(n_episode):\n            last_reward, running_reward = agent.run_episode(n_steps=NUM_STEPS)\n\n            if print_log:\n                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n                    i_episode, last_reward, running_reward))\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from agents\n    rpc.shutdown()\n```\n\n----------------------------------------\n\nTITLE: Using Loss Functions in PyTorch\nDESCRIPTION: A list of available loss functions in PyTorch's nn module. These loss functions are used to compute the error between the model's predictions and the true targets during training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnn.X                                        # where X is BCELoss, CrossEntropyLoss, L1Loss, MSELoss, NLLLoss, SoftMarginLoss, MultiLabelSoftMarginLoss, CosineEmbeddingLoss, KLDivLoss, MarginRankingLoss, HingeEmbeddingLoss or CosineEmbeddingLoss\n```\n\n----------------------------------------\n\nTITLE: Pattern Matching with FX Graph Nodes for Quantization Annotation - PyTorch - Python\nDESCRIPTION: This snippet demonstrates direct pattern matching in the FX IR for identifying and annotating specific operation sequences, like conv followed by relu, by traversing the FX graph and checking function targets and inputs. Dependencies required are torch and the FX GraphModule structure. The pattern matcher should be adapted for real-life use with error checking and node annotation code.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor n in gm.graph.nodes:\n      if n.op != \"call_function\" or n.target not in [\n          torch.ops.aten.relu.default,\n          torch.ops.aten.relu_.default,\n      ]:\n          continue\n      relu_node = n\n      maybe_conv_node = n.args[0]\n      if (\n          not isinstance(maybe_conv_node, Node)\n          or maybe_conv_node.op != \"call_function\"\n          or maybe_conv_node.target\n          not in [\n              torch.ops.aten.conv1d.default,\n              torch.ops.aten.conv2d.default,\n          ]\n      ):\n          continue\n\n      # annotate conv and relu nodes\n      ...\n```\n\n----------------------------------------\n\nTITLE: Creating Combined Model with Quantized Feature Extractor and New Head in PyTorch\nDESCRIPTION: Defines a function `create_combined_model` that takes a quantized ResNet-18 model (`model_fe`) as input. It isolates the feature extraction layers (convolutional layers, pooling, etc.) into an `nn.Sequential` block, ensuring input quantization (`model_fe.quant`) and output dequantization (`model_fe.dequant`) are correctly placed. It then defines a new classifier head (`new_head`) with Dropout and a Linear layer using `num_ftrs`. Finally, it combines the feature extractor and the new head, including an `nn.Flatten` layer, into a new sequential model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn\n\ndef create_combined_model(model_fe):\n  # Step 1. Isolate the feature extractor.\n  model_fe_features = nn.Sequential(\n    model_fe.quant,  # Quantize the input\n    model_fe.conv1,\n    model_fe.bn1,\n    model_fe.relu,\n    model_fe.maxpool,\n    model_fe.layer1,\n    model_fe.layer2,\n    model_fe.layer3,\n    model_fe.layer4,\n    model_fe.avgpool,\n    model_fe.dequant,  # Dequantize the output\n  )\n\n  # Step 2. Create a new \"head\"\n  new_head = nn.Sequential(\n    nn.Dropout(p=0.5),\n    nn.Linear(num_ftrs, 2),\n  )\n\n  # Step 3. Combine, and don't forget the quant stubs.\n  new_model = nn.Sequential(\n    model_fe_features,\n    nn.Flatten(1),\n    new_head,\n  )\n  return new_model\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with the Neural Network\nDESCRIPTION: Runs a forward pass on the model to get prediction probabilities and the predicted class, to verify the model works correctly.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nX = torch.rand(1, 64, 64, device=device)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Device Guard for PrivateUse1\nDESCRIPTION: Custom device guard implementation for managing device, stream, and event switching in the new backend.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nstruct CustomGuardImpl final : public c10::impl::DeviceGuardImplInterface {\n  // Implementation of guard in new backend\n}\n\nC10_REGISTER_GUARD_IMPL(PrivateUse1, CustomGuardImpl);\n```\n\n----------------------------------------\n\nTITLE: PyTorch ITT Sample Implementation with Custom Range Labeling\nDESCRIPTION: Complete sample code demonstrating how to use PyTorch ITT APIs to label different iterations during model inference. The sample includes a simple model with Conv2d and Linear layers and shows both methods of custom range labeling.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/profile_with_itt.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# sample.py\n\nimport torch\nimport torch.nn as nn\n\nclass ITTSample(nn.Module):\n  def __init__(self):\n    super(ITTSample, self).__init__()\n    self.conv = nn.Conv2d(3, 5, 3)\n    self.linear = nn.Linear(292820, 1000)\n\n  def forward(self, x):\n    x = self.conv(x)\n    x = x.view(x.shape[0], -1)\n    x = self.linear(x)\n    return x\n\ndef main():\n  m = ITTSample\n  # unmark below code for XPU\n  # m = m.to(\"xpu\")\n  x = torch.rand(10, 3, 244, 244)\n  # unmark below code for XPU\n  # x = x.to(\"xpu\")\n  with torch.autograd.profiler.emit_itt():\n    for i in range(3)\n      # Labeling a region with pair of range_push and range_pop\n      #torch.profiler.itt.range_push(f'iteration_{i}')\n      #m(x)\n      #torch.profiler.itt.range_pop()\n\n      # Labeling a region with range scope\n      with torch.profiler.itt.range(f'iteration_{i}'):\n        m(x)\n```\n\n----------------------------------------\n\nTITLE: Configuring Experiment Parameters and Reproducibility - Python\nDESCRIPTION: Sets up an argparse.Namespace object with high-level parameters for the MRPC task evaluation, including paths to model/data, tokenizer settings, and output options. Defines and invokes a set_seed function to fix randomness for reproducibility. Relies on previously imported transformers, NumPy, random, and torch libraries. Assures consistent experiment runs by controlling global config state.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconfigs = Namespace()\n\n# The output directory for the fine-tuned model, $OUT_DIR.\nconfigs.output_dir = \"./MRPC/\"\n\n# The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.\nconfigs.data_dir = \"./glue_data/MRPC\"\n\n# The model name or path for the pre-trained model.\nconfigs.model_name_or_path = \"bert-base-uncased\"\n# The maximum length of an input sequence\nconfigs.max_seq_length = 128\n\n# Prepare GLUE task.\nconfigs.task_name = \"MRPC\".lower()\nconfigs.processor = processors[configs.task_name]()\nconfigs.output_mode = output_modes[configs.task_name]\nconfigs.label_list = configs.processor.get_labels()\nconfigs.model_type = \"bert\".lower()\nconfigs.do_lower_case = True\n\n# Set the device, batch size, topology, and caching flags.\nconfigs.device = \"cpu\"\nconfigs.per_gpu_eval_batch_size = 8\nconfigs.n_gpu = 0\nconfigs.local_rank = -1\nconfigs.overwrite_cache = False\n\n# Set random seed for reproducibility.\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42)\n\n```\n\n----------------------------------------\n\nTITLE: Using PyTorch Vulkan Backend in C++ (LibTorch)\nDESCRIPTION: C++ code snippet demonstrating basic usage of the Vulkan backend via the LibTorch API. It shows how to check Vulkan availability (`at::is_vulkan_available`), create a CPU tensor, move it to the Vulkan device (`.vulkan()`), load a TorchScript model (`torch::jit::load`), perform inference on the Vulkan device, and copy the output tensor back to the CPU (`.cpu()`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nat::is_vulkan_available()\nauto tensor = at::rand({1, 2, 2, 3}, at::device(at::kCPU).dtype(at::kFloat));\nauto tensor_vulkan = t.vulkan();\nauto module = torch::jit::load(\"$PATH\");\nauto tensor_output_vulkan = module.forward(inputs).toTensor();\nauto tensor_output = tensor_output.cpu();\n```\n\n----------------------------------------\n\nTITLE: Saving and Returning Intermediates in Custom Autograd Function (Sinh) - PyTorch - Python\nDESCRIPTION: Defines a custom autograd Function (Sinh) that computes sinh(x) and returns intermediate results to enable proper gradient flows for double backward. Intermediates are saved via save_for_backward and are also returned as outputs, so future gradient computations can track their creation. Includes a sinh wrapper, input preparation, and validation for higher-order gradients. Dependencies: torch, double precision.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Sinh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.save_for_backward(expx, expnegx)\n        # In order to be able to save the intermediate results, a trick is to\n        # include them as our outputs, so that the backward graph is constructed\n        return (expx - expnegx) / 2, expx, expnegx\n\n    @staticmethod\n    def backward(ctx, grad_out, _grad_out_exp, _grad_out_negexp):\n        expx, expnegx = ctx.saved_tensors\n        grad_input = grad_out * (expx + expnegx) / 2\n        # We cannot skip accumulating these even though we won't use the outputs\n        # directly. They will be used later in the second backward.\n        grad_input += _grad_out_exp * expx\n        grad_input -= _grad_out_negexp * expnegx\n        return grad_input\n\ndef sinh(x):\n    # Create a wrapper that only returns the first output\n    return Sinh.apply(x)[0]\n\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(sinh, x)\ntorch.autograd.gradgradcheck(sinh, x)\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Process Group with TCP Connection in PyTorch\nDESCRIPTION: Sets up a distributed process group using TCP for communication. All worker processes connect to the process with rank 0 at the specified IP address and port to exchange connection information.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndist.init_process_group(\n    init_method='tcp://10.1.1.20:23456',\n    rank=args.rank,\n    world_size=4)\n```\n\n----------------------------------------\n\nTITLE: Creating a Wrapper Class for HistoEncoder in TIAToolbox\nDESCRIPTION: Implements a wrapper class for HistoEncoder that conforms to the TIAToolbox ModelABC interface. The wrapper handles the forward pass and batch inference process, adapting the HistoEncoder model to work with TIAToolbox's inference engines.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass HistoEncWrapper(ModelABC):\n    \"\"\"Wrapper for HistoEnc model that conforms to tiatoolbox ModelABC interface.\"\"\"\n\n    def __init__(self: HistoEncWrapper, encoder) -> None:\n        super().__init__()\n        self.feat_extract = encoder\n\n    def forward(self: HistoEncWrapper, imgs: torch.Tensor) -> torch.Tensor:\n        \"\"\"Pass input data through the model.\n\n        Args:\n            imgs (torch.Tensor):\n                Model input.\n\n        \"\"\"\n        out = F.extract_features(self.feat_extract, imgs, num_blocks=2, avg_pool=True)\n        return out\n\n    @staticmethod\n    def infer_batch(\n        model: nn.Module,\n        batch_data: torch.Tensor,\n        *,\n        on_gpu: bool,\n    ) -> list[np.ndarray]:\n        \"\"\"Run inference on an input batch.\n\n        Contains logic for forward operation as well as i/o aggregation.\n\n        Args:\n            model (nn.Module):\n                PyTorch defined model.\n            batch_data (torch.Tensor):\n                A batch of data generated by\n                `torch.utils.data.DataLoader`.\n            on_gpu (bool):\n                Whether to run inference on a GPU.\n\n        \"\"\"\n        img_patches_device = batch_data.to('cuda') if on_gpu else batch_data\n        model.eval()\n        # Do not compute the gradient (not training)\n        with torch.inference_mode():\n            output = model(img_patches_device)\n        return [output.cpu().numpy()]\n```\n\n----------------------------------------\n\nTITLE: Using Join with DistributedDataParallel in Python\nDESCRIPTION: Example of using the Join context manager with DistributedDataParallel for distributed training with uneven inputs across ranks. It initializes the distributed environment, creates a DDP model, and uses Join to handle uneven inputs in the training loop.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/generic_join.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.distributed.algorithms.join import Join\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nBACKEND = \"nccl\"\nWORLD_SIZE = 2\nNUM_INPUTS = 5\n\ndef worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n    # Rank 1 gets one more input than rank 0\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    num_inputs = 0\n    with Join([model]):\n        for input in inputs:\n            num_inputs += 1\n            loss = model(input).sum()\n            loss.backward()\n\n    print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\")\n\ndef main():\n    mp.spawn(worker, nprocs=WORLD_SIZE, join=True)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Importing Sequence Parallel Utilities for PyTorch Distributed Tensor Parallelism - Python\nDESCRIPTION: This snippet imports key sequence parallel utility classes from torch.distributed.tensor.parallel. These components (PrepareModuleInput, SequenceParallel) enable sharded computation on the sequence dimension, and are prerequisites for the sharding plans later in this guide. No additional parameters required; import these to construct advanced module parallelization plans.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.tensor.parallel import (\n    PrepareModuleInput,\n    SequenceParallel,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating an Exponential Custom Autograd Function with Double Backward Support - PyTorch - Python\nDESCRIPTION: Presents a custom Function (Exp) where the forward saves the output tensor for backward. The backward multiplies the saved result by the incoming gradient, aiding double backward. This code validates gradients up to second order using gradcheck and gradgradcheck, requiring torch and double precision.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Exp(torch.autograd.Function):\n    # Simple case where everything goes well\n    @staticmethod\n    def forward(ctx, x):\n        # This time we save the output\n        result = torch.exp(x)\n        # Note that we should use `save_for_backward` here when\n        # the tensor saved is an ouptut (or an input).\n        ctx.save_for_backward(result)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        result, = ctx.saved_tensors\n        return result * grad_out\n\nx = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n# Validate our gradients using gradcheck\ntorch.autograd.gradcheck(Exp.apply, x)\ntorch.autograd.gradgradcheck(Exp.apply, x)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring and Initializing the XNNPACK Quantizer for Post Training Quantization - Python\nDESCRIPTION: Imports the XNNPACKQuantizer and configures it for model quantization using symmetric quantization. The quantizer allows setting global, type-based, or module-specific quantization configs. Dependencies include PyTorch's torch.ao.quantization.quantizer.xnnpack_quantizer module. Configuration is mandatory before quantization preparations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.ao.quantization.quantizer.xnnpack_quantizer import (\n  XNNPACKQuantizer,\n  get_symmetric_quantization_config,\n)\nquantizer = XNNPACKQuantizer()\nquantizer.set_global(get_symmetric_quantization_config())\n```\n\n----------------------------------------\n\nTITLE: Implementing Graceful Restart Structure in PyTorch\nDESCRIPTION: Basic structure for implementing graceful restarts in distributed training, showing the main entry point and training loop with snapshot handling.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ddp_series_fault_tolerance.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n  load_snapshot(snapshot_path)\n  initialize()\n  train()\n\ndef train():\n  for batch in iter(dataset):\n    train_step(batch)\n\n    if should_checkpoint:\n      save_snapshot(snapshot_path)\n```\n\n----------------------------------------\n\nTITLE: Averaging Model Gradients Across Processes with PyTorch Distributed (Python)\nDESCRIPTION: Implements a function to average gradients of model parameters across all distributed processes using dist.all_reduce and ReduceOp.SUM. Operates in-place on model parameters' gradients, dividing by the world size for true averaging. Dependencies are torch.distributed and a model with registered parameters. Expected input: a model object post-backward pass; output: no return, but gradients are averaged and ready for optimizer step. Requires all_reduce to be available and initialized for the current process group.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n\"\"\" Gradient averaging. \"\"\"\ndef average_gradients(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n        param.grad.data /= size\n```\n\n----------------------------------------\n\nTITLE: Running the DeviceMesh 2D Setup with TorchRun\nDESCRIPTION: Command line instruction to run the 2D parallel setup with DeviceMesh using TorchRun, requiring 8 processes per node.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_device_mesh.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntorchrun --nproc_per_node=8 2d_setup_with_device_mesh.py\n```\n\n----------------------------------------\n\nTITLE: Preparing Model for Post Training Quantization with prepare_pt2e - Python\nDESCRIPTION: Prepares the exported model for quantization by folding BatchNorm operations and inserting observers using prepare_pt2e. Requires the exported model and a configured quantizer as inputs. Outputs an instrumented model, ready for calibration, and prints its graph for verification.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprepared_model = prepare_pt2e(exported_model, quantizer)\nprint(prepared_model.graph)\n```\n\n----------------------------------------\n\nTITLE: Applying Context Parallel for Distributed SDPA in PyTorch\nDESCRIPTION: Illustrates how to enable Context Parallel for Scaled Dot Product Attention (SDPA) in a distributed PyTorch setting. It initializes a process group and device mesh, then uses the `context_parallel` context manager to shard the input query, key, and value tensors (`cp_qkv`) in-place along the sequence dimension (index 2). Within this context, `F.scaled_dot_product_attention` is automatically replaced by Ring Attention. The sharded output is then unsharded using `context_parallel_unshard` for verification against the non-parallel version. Requires `RANK` and `WORLD_SIZE` environment variables for distributed setup.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/context_parallel.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# file: cp_sdpa_example.py\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn.functional as F\nfrom torch.distributed.device_mesh import init_device_mesh\nfrom torch.distributed.tensor.experimental import context_parallel\nfrom torch.distributed.tensor.experimental._attention import context_parallel_unshard\nfrom torch.nn.attention import sdpa_kernel, SDPBackend\n\n\ndef context_parallel_sdpa_example(world_size: int, rank: int):\n    assert torch.cuda.is_available()\n    assert dist.is_nccl_available()\n    torch.cuda.set_device(f\"cuda:{rank}\")\n    torch.cuda.manual_seed(0)\n\n    dist.init_process_group(\n        backend=\"nccl\",\n        init_method=\"env://\",\n        world_size=world_size,\n        rank=rank,\n    )\n    device_mesh = init_device_mesh(\n        device_type=\"cuda\", mesh_shape=(world_size,), mesh_dim_names=(\"cp\",)\n    )\n\n    batch = 8\n    nheads = 8\n    qkv_len = 64\n    dim = 32\n    backend = SDPBackend.FLASH_ATTENTION\n    dtype = (\n        torch.bfloat16\n        if backend == SDPBackend.FLASH_ATTENTION\n        or backend == SDPBackend.CUDNN_ATTENTION\n        else torch.float32\n    )\n\n    qkv = [\n        torch.rand(\n            (batch, nheads, qkv_len, dim),\n            dtype=dtype,\n            requires_grad=True,\n            device='cuda',\n        )\n        for _ in range(3)\n    ]\n    # specify the SDPBackend to use\n    with sdpa_kernel(backend):\n        out = F.scaled_dot_product_attention(*qkv, is_causal=True)\n\n    # make a clean copy of QKV for output comparison\n    cp_qkv = [t.detach().clone() for t in qkv]\n\n    with sdpa_kernel(backend):\n        # This `context_parallel()` performs two actions:\n        # 1. Shard the tensor objects in `buffers` in-place along the dimension\n        #    specified in `buffer_seq_dims`, the tensors in `buffers` and their\n        #    sharding dims in `buffer_seq_dims` are organized in the same order.\n        # 2. Replace the execution of `F.scaled_dot_product_attention` with a\n        #    context-paralleled-enabled Ring Attention.\n        with context_parallel(\n            device_mesh, buffers=tuple(cp_qkv), buffer_seq_dims=(2, 2, 2)\n        ):\n            cp_out = F.scaled_dot_product_attention(*cp_qkv, is_causal=True)\n\n        # The output `cp_out` is still sharded in the same way as QKV\n        # the `context_parallel_unshard` API allows users to easily\n        # unshard to gain the full tensor.\n        (cp_out,) = context_parallel_unshard(device_mesh, [cp_out], [2])\n\n    assert torch.allclose(\n        cp_out,\n        out,\n        atol=(1e-08 if dtype == torch.float32 else 1e-03 * world_size),\n    )\n\n\nif __name__ == \"__main__\":\n    rank = int(os.environ[\"RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n\n    try:\n        context_parallel_sdpa_example(world_size, rank)\n    finally:\n        dist.barrier()\n        dist.destroy_process_group()\n```\n\n----------------------------------------\n\nTITLE: Calculating Predictions and Plotting PR Curves with TensorBoard in Python\nDESCRIPTION: This Python code calculates class probabilities for the test set using a trained PyTorch model (`net`) and the `testloader`. It then defines a helper function `add_pr_curve_tensorboard` which takes a class index, test probabilities, and test labels to log a precision-recall (PR) curve for that specific class to TensorBoard using `writer.add_pr_curve`. Finally, it iterates through all classes, calling the helper function to plot PR curves for each one. It depends on `torch`, `torch.nn.functional` (as `F`), the trained `net`, `testloader`, the `writer` instance, and a `classes` list.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tensorboard_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Full Backward Graph Capture Despite Forward Breaks\nDESCRIPTION: Highlights Compiled Autograd's ability to capture a single, full backward graph even if the compiled forward function contains graph breaks (`torch._dynamo.graph_break()`). The code compares the number of backward graphs generated with and without Compiled Autograd enabled for the `loss.backward()` call, using `aot_eager` backend and counters to verify that Compiled Autograd produces only one graph.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/compiled_autograd_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile(backend=\"aot_eager\")\ndef fn(x):\n   # 1st graph\n   temp = x + 10\n   torch._dynamo.graph_break()\n   # 2nd graph\n   temp = temp + 10\n   torch._dynamo.graph_break()\n   # 3rd graph\n   return temp.sum()\n\nx = torch.randn(10, 10, requires_grad=True)\ntorch._dynamo.utils.counters.clear()\nloss = fn(x)\n\n# 1. base torch.compile \nloss.backward(retain_graph=True)\nassert(torch._dynamo.utils.counters[\"stats\"][\"unique_graphs\"] == 3)\ntorch._dynamo.utils.counters.clear()\n\n# 2. torch.compile with compiled autograd\nwith torch._dynamo.compiled_autograd.enable(torch.compile(backend=\"aot_eager\")):\n   loss.backward()\n\n# single graph for the backward\nassert(torch._dynamo.utils.counters[\"stats\"][\"unique_graphs\"] == 1)\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Data Loader in PyTorch C++\nDESCRIPTION: Shows how to create a data loader using `torch::data::make_data_loader` in the PyTorch C++ API. It takes ownership of the previously defined and transformed `dataset` object via `std::move`. This basic loader uses default settings for batch size (1) and the number of worker threads (0, using the main thread).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\n  auto data_loader = torch::data::make_data_loader(std::move(dataset));\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Ensemble Models with Fork/Wait\nDESCRIPTION: Optimizes the ensemble model by running all individual models in parallel. This implementation launches each model as a separate task using torch.jit.fork(), collects their results with torch.jit.wait(), and then combines them.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch-script-parallelism.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x : torch.Tensor) -> torch.Tensor:\n    # Launch tasks for each model\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for model in self.models:\n        futures.append(torch.jit.fork(model, x))\n\n    # Collect the results from the launched tasks\n    results : List[torch.Tensor] = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.stack(results).sum(dim=0)\n```\n\n----------------------------------------\n\nTITLE: Setting up 2D Parallel Pattern Without DeviceMesh in PyTorch\nDESCRIPTION: This code demonstrates the manual setup of process groups for a 2D parallel pattern in PyTorch without using DeviceMesh. It involves calculating shard groups and replicate groups, then assigning them to each rank.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_device_mesh.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed as dist\n\n# Understand world topology\nrank = int(os.environ[\"RANK\"])\nworld_size = int(os.environ[\"WORLD_SIZE\"])\nprint(f\"Running example on {rank=} in a world with {world_size=}\")\n\n# Create process groups to manage 2-D like parallel pattern\ndist.init_process_group(\"nccl\")\ntorch.cuda.set_device(rank)\n\n# Create shard groups (e.g. (0, 1, 2, 3), (4, 5, 6, 7))\n# and assign the correct shard group to each rank\nnum_node_devices = torch.cuda.device_count()\nshard_rank_lists = list(range(0, num_node_devices // 2)), list(range(num_node_devices // 2, num_node_devices))\nshard_groups = (\n    dist.new_group(shard_rank_lists[0]),\n    dist.new_group(shard_rank_lists[1]),\n)\ncurrent_shard_group = (\n    shard_groups[0] if rank in shard_rank_lists[0] else shard_groups[1]\n)\n\n# Create replicate groups (for example, (0, 4), (1, 5), (2, 6), (3, 7))\n# and assign the correct replicate group to each rank\ncurrent_replicate_group = None\nshard_factor = len(shard_rank_lists[0])\nfor i in range(num_node_devices // 2):\n    replicate_group_ranks = list(range(i, num_node_devices, shard_factor))\n    replicate_group = dist.new_group(replicate_group_ranks)\n    if rank in replicate_group_ranks:\n        current_replicate_group = replicate_group\n```\n\n----------------------------------------\n\nTITLE: Testing the Flask Image Classifier API with curl - Shell\nDESCRIPTION: Uses curl to send a POST multipart form-data request to the Flask server on 'localhost:5000/predict' with an attached file for guessing the image class. Assumes the Flask server is running and 'kitten.jpg' is present. Returns a JSON response with class_id and class_name.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/deployment_with_flask.rst#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ncurl -X POST -H \\\"Content-Type: multipart/form-data\\\" http://localhost:5000/predict -F \\\"file=@kitten.jpg\\\"\n```\n\n----------------------------------------\n\nTITLE: Integrated Real-Time Inference Script (Python)\nDESCRIPTION: Combines the previous steps into a single script for real-time inference. It imports necessary libraries, sets the 'qnnpack' engine, initializes OpenCV video capture with specific parameters, defines image preprocessing transforms, loads the quantized MobileNetV2 model, applies JIT compilation, and initializes timers to measure performance. This script forms the basis for the main inference loop.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport torch\nimport numpy as np\nfrom torchvision import models, transforms\n\nimport cv2\nfrom PIL import Image\n\ntorch.backends.quantized.engine = 'qnnpack'\n\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n# jit model to take it from ~20fps to ~30fps\nnet = torch.jit.script(net)\n\nstarted = time.time()\nlast_logged = time.time()\n```\n\n----------------------------------------\n\nTITLE: CUDA Graph Capture Implementation in PyTorch C++\nDESCRIPTION: Demonstrates how to use the CUDA Graph API in PyTorch C++ to capture the training step operations on a dedicated CUDA stream.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nat::cuda::CUDAGraph graph;\nat::cuda::CUDAStream captureStream = at::cuda::getStreamFromPool();\nat::cuda::setCurrentCUDAStream(captureStream);\n\ngraph.capture_begin();\ntraining_step(model, optimizer, data, targets, output, loss);\ngraph.capture_end();\n```\n\n----------------------------------------\n\nTITLE: Refactoring Parent Module to Use Custom Non-Traceable Submodule\nDESCRIPTION: Demonstrates modifying the parent module `M` to instantiate and use the custom `FP32NonTraceable` submodule. This allows the main quantization flow (`prepare_fx`, `convert_fx`) to handle the traceable parts while relying on custom configurations for the non-traceable submodule.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# refactor parent class to call FP32NonTraceable\nclass M(nn.Module):\n\n   def __init__(self):\n        ...\n        self.non_traceable_submodule = FP32NonTraceable(...)\n\n    def forward(self, x):\n        x = self.traceable_code_1(x)\n        # this part will be quantized manually\n        x = self.non_traceable_submodule(x)\n        x = self.traceable_code_1(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Validating Custom Autograd Function with Gradcheck in PyTorch - Python\nDESCRIPTION: These snippets show how to initialize a tensor with gradients enabled and test a custom autograd function using PyTorch's gradcheck and gradgradcheck utilities. Dependencies include PyTorch and a previously defined Cube custom Function. The x tensor is created with a floating-point value, gradient tracking, and double precision. The gradcheck tests whether the output gradients from Cube.apply are numerically correct, while gradgradcheck further verifies the correctness of the double backward (second derivative) computation. Inputs: tensor x and custom Cube function. Outputs: Boolean flags indicating the validity of gradients. Limitations: Assumes the Cube function is correctly implemented and available in scope.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nx = torch.tensor(2., requires_grad=True, dtype=torch.double)\n\ntorch.autograd.gradcheck(Cube.apply, x)\ntorch.autograd.gradgradcheck(Cube.apply, x)\n```\n\n----------------------------------------\n\nTITLE: Optimizing a Model for Vulkan Backend (Python)\nDESCRIPTION: Python script demonstrating how to optimize a TorchScript model (`script_model`) specifically for the Vulkan backend using `torch.utils.mobile_optimizer.optimize_for_mobile`. The optimized model (`script_model_vulkan`) is then saved to a new file (`mobilenet2-vulkan.pt`). This step rewrites the graph with Vulkan-specific operators and typically includes automatic tensor transfers between CPU and Vulkan.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\nscript_model_vulkan = optimize_for_mobile(script_model, backend='vulkan')\ntorch.jit.save(script_model_vulkan, \"mobilenet2-vulkan.pt\")\n```\n\n----------------------------------------\n\nTITLE: Implementing and Registering a CPU Element-wise Multiplication Kernel - PyTorch C++\nDESCRIPTION: Implements a custom CPU kernel 'mymul' for element-wise multiplication of float32 CPU tensors in C++, with checks for size and type, and making sure input tensors are contiguous. The function iterates through tensor elements for multiplication, storing the result in a newly allocated tensor. The kernel is registered with TORCH_LIBRARY extensions and its implementation for the CPU device. It requires PyTorch's C++ extension infrastructure and is intended for CPU operations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\n// New! a mymul_cpu kernel\nat::Tensor mymul_cpu(const at::Tensor& a, const at::Tensor& b) {\n  TORCH_CHECK(a.sizes() == b.sizes());\n  TORCH_CHECK(a.dtype() == at::kFloat);\n  TORCH_CHECK(b.dtype() == at::kFloat);\n  TORCH_CHECK(a.device().type() == at::DeviceType::CPU);\n  TORCH_CHECK(b.device().type() == at::DeviceType::CPU);\n  at::Tensor a_contig = a.contiguous();\n  at::Tensor b_contig = b.contiguous();\n  at::Tensor result = torch::empty(a_contig.sizes(), a_contig.options());\n  const float* a_ptr = a_contig.data_ptr<float>();\n  const float* b_ptr = b_contig.data_ptr<float>();\n  float* result_ptr = result.data_ptr<float>();\n  for (int64_t i = 0; i < result.numel(); i++) {\n    result_ptr[i] = a_ptr[i] * b_ptr[i];\n  }\n  return result;\n}\n\nTORCH_LIBRARY(extension_cpp, m) {\n  m.def(\"mymuladd(Tensor a, Tensor b, float c) -> Tensor\");\n  // New! defining the mymul operator\n  m.def(\"mymul(Tensor a, Tensor b) -> Tensor\");\n}\n\nTORCH_LIBRARY_IMPL(extension_cpp, CPU, m) {\n  m.impl(\"mymuladd\", &mymuladd_cpu);\n  // New! registering the cpu kernel for the mymul operator\n  m.impl(\"mymul\", &mymul_cpu);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Operator with TorchScript\nDESCRIPTION: Registration code that makes the custom operator available to both Python and TorchScript. Uses TORCH_LIBRARY macro to define the operator in the my_ops namespace.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY(my_ops, m) {\n  m.def(\"warp_perspective\", &warp_perspective);\n}\n```\n\n----------------------------------------\n\nTITLE: Applying FX Quantization to a Specific Submodule in Python\nDESCRIPTION: Demonstrates how to apply FX Graph Mode Quantization (`prepare_fx`) specifically to a designated traceable submodule (`model_fp32.traceable_submodule`) within a larger model. It uses a `QConfigMapping` to define quantization settings.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nqconfig_mapping = QConfigMapping().set_global(qconfig)\nmodel_fp32.traceable_submodule = \\\n  prepare_fx(model_fp32.traceable_submodule, qconfig_mapping, example_inputs)\n```\n\n----------------------------------------\n\nTITLE: Downloading WSI, Patch Data, and Model Weights - Python\nDESCRIPTION: This snippet demonstrates how to use TIAToolbox utility functions to download a sample whole slide image (WSI), a subset of the Kather100k validation patches, and pretrained PyTorch ResNet18 model weights. After downloading, patch data is extracted from a ZIP archive. The code logs progress with the TIAToolbox logger. Required dependencies are tiatoolbox, pathlib, and zipfile. The function assumes an active internet connection and valid URLs; outputs are files written to disk and logging statements indicating download status.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwsi_path = global_save_dir / \"sample_wsi.svs\"\npatches_path = global_save_dir / \"kather100k-validation-sample.zip\"\nweights_path = global_save_dir / \"resnet18-kather100k.pth\"\n\nlogger.info(\"Download has started. Please wait...\")\n\n# Downloading and unzip a sample whole-slide image\ndownload_data(\n    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs\",\n    wsi_path,\n)\n\n# Download and unzip a sample of the validation set used to train the Kather 100K dataset\ndownload_data(\n    \"https://tiatoolbox.dcs.warwick.ac.uk/datasets/kather100k-validation-sample.zip\",\n    patches_path,\n)\nwith ZipFile(patches_path, \"r\") as zipfile:\n    zipfile.extractall(path=global_save_dir)\n\n# Download pretrained model weights for WSI classification using ResNet18 architecture \ndownload_data(\n    \"https://tiatoolbox.dcs.warwick.ac.uk/models/pc/resnet18-kather100k.pth\",\n    weights_path,\n)\n\nlogger.info(\"Download is complete.\")\n\n```\n\n----------------------------------------\n\nTITLE: Tracer-based Model Splitting in Python\nDESCRIPTION: Implements automatic model partitioning using the pipeline API's tracer-based splitting functionality. Splits the model at specified points using a split specification.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/pipelining_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef tracer_model_split(model, example_input_microbatch) -> PipelineStage:\n   pipe = pipeline(\n      module=model,\n      mb_args=(example_input_microbatch,),\n      split_spec={\n         \"layers.4\": SplitPoint.BEGINNING,\n      }\n   )\n   stage = pipe.build_stage(stage_index, device, pp_group)\n   return stage\n```\n\n----------------------------------------\n\nTITLE: Data Loading and Transformation Setup\nDESCRIPTION: Configures data loading pipelines with transformations for training and validation datasets using torchvision.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchvision import transforms, datasets\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize(224),\n        transforms.RandomCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n                                              shuffle=True, num_workers=8)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Creating Dummy Python Module with Stable API for Cross-version Compatibility - PyTorch C++\nDESCRIPTION: Implements a stable, Py_LIMITED_API-compatible empty module '_C' in plain C++, allowing the custom operator library to be loaded in Python without Pybind11. This approach enables the extension to work across multiple CPython versions by leveraging only the limited stable Python C API. The module triggers registration via static initializers. Dependencies include Python.h and PyTorch's C++ extensions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n#include <Python.h>\n\nextern \"C\" {\n  /* Creates a dummy empty _C module that can be imported from Python.\n    The import from Python will load the .so consisting of this file\n    in this extension, so that the TORCH_LIBRARY static initializers\n    below are run. */\n  PyObject* PyInit__C(void)\n  {\n      static struct PyModuleDef module_def = {\n          PyModuleDef_HEAD_INIT,\n          \"_C\",   /* name of module */\n          NULL,   /* module documentation, may be NULL */\n          -1,     /* size of per-interpreter state of the module,\n                    or -1 if the module keeps state in global variables. */\n          NULL,   /* methods */\n      };\n      return PyModule_Create(&module_def);\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Defining ImageNet Data Loaders with PyTorch - Python\nDESCRIPTION: Defines a function to create PyTorch DataLoader objects for the ImageNet training and validation datasets. Utilizes torchvision transforms for preprocessing, applies normalization, and uses samplers for random and sequential data feeding. The function returns a training DataLoader and a test DataLoader, with their batch sizes configured via separate variables; requires torchvision, torch, and data_path to be set up, as well as variables for train_batch_size and eval_batch_size.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef prepare_data_loaders(data_path):  \n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],  \n                                     std=[0.229, 0.224, 0.225])\n    dataset = torchvision.datasets.ImageNet(\n        data_path, split=\"train\", transform=transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    dataset_test = torchvision.datasets.ImageNet(\n        data_path, split=\"val\", transform=transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset) \n    test_sampler = torch.utils.data.SequentialSampler(dataset_test) \n\n    data_loader = torch.utils.data.DataLoader(  \n        dataset, batch_size=train_batch_size, \n        sampler=train_sampler)  \n\n    data_loader_test = torch.utils.data.DataLoader( \n        dataset_test, batch_size=eval_batch_size, \n        sampler=test_sampler) \n\n    return data_loader, data_loader_test  \n```\n\n----------------------------------------\n\nTITLE: Setting up 2D Parallel Pattern With DeviceMesh in PyTorch\nDESCRIPTION: This code demonstrates how to use DeviceMesh to simplify the setup of a 2D parallel pattern. It shows how to initialize a device mesh and access the underlying process groups.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_device_mesh.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.device_mesh import init_device_mesh\nmesh_2d = init_device_mesh(\"cuda\", (2, 4), mesh_dim_names=(\"replicate\", \"shard\"))\n\n# Users can access the underlying process group thru `get_group` API.\nreplicate_group = mesh_2d.get_group(mesh_dim=\"replicate\")\nshard_group = mesh_2d.get_group(mesh_dim=\"shard\")\n```\n\n----------------------------------------\n\nTITLE: Registering CUDA Implementation of Custom Add Operator\nDESCRIPTION: CUDA implementation registration for the custom add operator using TORCH_LIBRARY_IMPL to handle GPU execution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/dispatcher.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY_IMPL(myops, CUDA, m) {\n  m.impl(\"myadd\", myadd_cuda);\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Module Passing Semantics in PyTorch C++\nDESCRIPTION: This snippet demonstrates various ways modules can be allocated and passed (by value, reference, pointer) in C++ using a simple struct inheriting from torch::nn::Module. No special dependencies aside from PyTorch's C++ API are required. It clarifies how standard C++ function parameters deal with ownership and semantics for module objects when used in the PyTorch frontend.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n  struct Net : torch::nn::Module { };\n\n  void a(Net net) { }\n  void b(Net& net) { }\n  void c(Net* net) { }\n\n  int main() {\n    Net net;\n    a(net);\n    a(std::move(net));\n    b(net);\n    c(&net);\n  }\n```\n\n----------------------------------------\n\nTITLE: Enabling Inductor C++ Wrapper in Python (Optional)\nDESCRIPTION: Optionally configures TorchInductor to use the C++ wrapper instead of the default Python wrapper by setting `torch._inductor.config.cpp_wrapper` to `True`. This can reduce Python overhead and enable pure C++ deployment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Optional: using the C++ wrapper instead of default Python wrapper\nimport torch._inductor.config as config\nconfig.cpp_wrapper = True\n```\n\n----------------------------------------\n\nTITLE: Training a PyTorch Model and Logging to TensorBoard in Python\nDESCRIPTION: This Python code snippet implements a training loop for a PyTorch neural network (`net`). It iterates through the `trainloader` dataset, performs forward and backward passes, updates model weights using the `optimizer`, and calculates the loss using `criterion`. Every 1000 mini-batches, it logs the running loss scalar and a Matplotlib figure showing predictions vs. actuals to TensorBoard using a `SummaryWriter` instance (`writer`). The helper function `plot_classes_preds` is assumed to be defined elsewhere.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tensorboard_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nrunning_loss = 0.0\nfor epoch in range(1):  # loop over the dataset multiple times\n\n    for i, data in enumerate(trainloader, 0):\n\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 1000 == 999:    # every 1000 mini-batches...\n\n            # ...log the running loss\n            writer.add_scalar('training loss',\n                            running_loss / 1000,\n                            epoch * len(trainloader) + i)\n\n            # ...log a Matplotlib Figure showing the model's predictions on a\n            # random mini-batch\n            writer.add_figure('predictions vs. actuals',\n                            plot_classes_preds(net, inputs, labels),\n                            global_step=epoch * len(trainloader) + i)\n            running_loss = 0.0\nprint('Finished Training')\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment, Logger, and Helper Functions for BERT Quantization Workflow (Python)\nDESCRIPTION: This snippet sets up the execution environment for the BERT quantization workflow, including necessary package imports, logger initialization, NumPy random seed helpers, and thread configuration. It also defines a function for creating random integer input tensors for test inputs. Required dependencies: torch, numpy, transformers, tqdm, argparse, os, sys. Expected input includes presence of a BERT or compatible model and the output is basic environment initialization along with available version/config info.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport numpy as np\nimport os\nimport random\nimport sys\nimport time\nimport torch\n\nfrom argparse import Namespace\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom tqdm import tqdm\nfrom transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\nfrom transformers import glue_compute_metrics as compute_metrics\nfrom transformers import glue_output_modes as output_modes\nfrom transformers import glue_processors as processors\nfrom transformers import glue_convert_examples_to_features as convert_examples_to_features\nfrom torch.quantization import per_channel_dynamic_qconfig\nfrom torch.quantization import quantize_dynamic_jit\n\ndef ids_tensor(shape, vocab_size):\n    #  Creates a random int32 tensor of the shape within the vocab size\n    return torch.randint(0, vocab_size, shape=shape, dtype=torch.int, device='cpu')\n\n# Setup logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.WARN)\n\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(\n   logging.WARN)  # Reduce logging\n\nprint(torch.__version__)\n\ntorch.set_num_threads(1)\nprint(torch.__config__.parallel_info())\n```\n\n----------------------------------------\n\nTITLE: Illustrating Recompilation due to Tensor Shape Changes\nDESCRIPTION: Shows another common recompilation trigger for Compiled Autograd: varying shapes of tensors involved in the backward pass. The code iterates, creating tensors `x` of different shapes (`10x10`, `100x100`, `10x10`). Compiling `loss.backward()` inside the loop (`eager` backend) with Compiled Autograd enabled results in recompilations when the shape of `x` changes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/compiled_autograd_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntorch._dynamo.config.compiled_autograd = True\nfor i in [10, 100, 10]:\n   x = torch.randn(i, i, requires_grad=True)\n   loss = x.sum()\n   torch.compile(lambda: loss.backward(), backend=\"eager\")()\n```\n\n----------------------------------------\n\nTITLE: Inference with ResNet50 on GPU using IPEX - Float32 (Python)\nDESCRIPTION: This snippet demonstrates how to perform inference with a ResNet50 model on GPU using IPEX with Float32 precision. It includes model loading, data preparation, and IPEX optimization for inference.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\nimport intel_extension_for_pytorch as ipex\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(memory_format=torch.channels_last)\ndata = data.to(memory_format=torch.channels_last)\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.float32)\n\nwith torch.no_grad():\n  model(data)\n\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Inflatable Argument Classes for Bundled Inputs - PyTorch - Python\nDESCRIPTION: Defines a function that returns a tuple of randomized tensors using custom inflation logic. The function demonstrates how to construct InflatableArg instances for complex input types and specify custom inflation format expressions. This is useful when model inputs require special runtime handling during inflation. Dependencies: torch, torch.utils.bundled_inputs. The result is an InflatableArg that generates two random-like tensors for the requested input size and dtype.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/bundled_inputs.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef create_example(*size, dtype=None):\\n    \"\"\"Generate a tuple of 2 random tensors both of the specified size\"\"\"\\n\\n    deflated_input = (torch.zeros(1, dtype=dtype).expand(*size), torch.zeros(1, dtype=dtype).expand(*size))\\n\\n    # {0} is how you access your deflated value in the inflation expression\\n    return torch.utils.bundled_inputs.InflatableArg(\\n        value=stub,\\n        fmt=\"(torch.randn_like({0}[0]), torch.randn_like({0}[1]))\",\\n    )\n```\n\n----------------------------------------\n\nTITLE: Checking CPU Information with lscpu\nDESCRIPTION: Command to display CPU and NUMA node information on Linux systems\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/xeon_run_cpu.rst#2025-04-22_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ lscpu\n...\nCPU(s):                  224\n  On-line CPU(s) list:   0-223\nVendor ID:               GenuineIntel\n  Model name:            Intel (R) Xeon (R) CPU Max 9480\n    CPU family:          6\n    Model:               143\n    Thread(s) per core:  2\n    Core(s) per socket:  56\n    Socket(s):           2\n...\nNUMA:\n  NUMA node(s):          2\n  NUMA node0 CPU(s):     0-55,112-167\n  NUMA node1 CPU(s):     56-111,168-223\n...\n```\n\n----------------------------------------\n\nTITLE: Using torch.export in PyTorch\nDESCRIPTION: Reference to PyTorch's export functionality that custom operators can integrate with. This allows models with custom operations to be exported for deployment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/custom_ops_landing_page.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntorch.export\n```\n\n----------------------------------------\n\nTITLE: Generated C++ Wrapper Code and Loader for CPU\nDESCRIPTION: Shows the generated code when the Inductor C++ wrapper is enabled for CPU. It includes a C++ function `inductor_entry_cpp` using PyTorch C++ APIs (ATen) for tensor operations and kernel calls. Accompanying Python code loads this C++ function from a compiled extension module using `CppWrapperCodeCache` and wraps it for Python invocation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/inductor_cpp_wrapper_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<at::Tensor> inductor_entry_cpp(const std::vector<at::Tensor>& args) {\n    at::Tensor arg0_1 = args[0];\n    at::Tensor constant0 = args[1];\n    auto buf0 = at::empty_strided({19L, }, {1L, }, at::device(at::kCPU).dtype(at::kFloat));\n    cpp_fused_add_lift_fresh_0((long*)(constant0.data_ptr()), (float*)(arg0_1.data_ptr()), (float*)(buf0.data_ptr()));\n    arg0_1.reset();\n    return {buf0};\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nmodule = CppWrapperCodeCache.load(cpp_wrapper_src, 'inductor_entry_cpp', 'c2buojsvlqbywxe3itb43hldieh4jqulk72iswa2awalwev7hjn2', False)\n\ndef _wrap_func(f):\n    def g(args):\n        args_tensor = [arg if isinstance(arg, torch.Tensor) else torch.tensor(arg) for arg in args]\n        constants_tensor = [constant0]\n        args_tensor.extend(constants_tensor)                    \n\n        return f(args_tensor)\n    return g\ncall = _wrap_func(module.inductor_entry_cpp)\n```\n\n----------------------------------------\n\nTITLE: Building a Custom PyTorch Distributed Extension - Python\nDESCRIPTION: This Python setup script configures and builds a custom PyTorch distributed extension, conditionally supporting either CUDA or C++ based on torch.cuda availability. It uses setuptools and torch.utils.cpp_extension to define source files and include directories, and builds the module for use as a distributed backend. Dependencies include PyTorch, setuptools, and access to the appropriate C++/CUDA compiler toolchain. The extension requires the source file at 'src/dummy.cpp'. The script outputs a build-ready extension module that can be imported and used by PyTorch distributed features.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport torch\nfrom setuptools import setup\nfrom torch.utils import cpp_extension\n\nsources = [\"src/dummy.cpp\"]\ninclude_dirs = [f\"{os.path.dirname(os.path.abspath(__file__))}/include/\"]\n\nif torch.cuda.is_available():\n    module = cpp_extension.CUDAExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\nelse:\n    module = cpp_extension.CppExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\n\nsetup(\n    name = \"Dummy-Collectives\",\n    version = \"0.0.1\",\n    ext_modules = [module],\n    cmdclass={'build_ext': cpp_extension.BuildExtension}\n)\n\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network Layers\nDESCRIPTION: Common neural network layers and components provided by PyTorch's nn module, including fully connected layers, convolutional layers, pooling, normalization, and recurrent layers.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nnn.Linear(m,n)                                # fully connected layer from \n                                              # m to n units\n\nnn.ConvXd(m,n,s)                              # X dimensional conv layer from \n                                              # m to n channels where X⍷{1,2,3} \n                                              # and the kernel size is s\n\nnn.MaxPoolXd(s)                               # X dimension pooling layer \n                                              # (notation as above)\n\nnn.BatchNormXd                                # batch norm layer\nnn.RNN/LSTM/GRU                               # recurrent layers\nnn.Dropout(p=0.5, inplace=False)              # dropout layer for any dimensional input\nnn.Dropout2d(p=0.5, inplace=False)            # 2-dimensional channel-wise dropout\nnn.Embedding(num_embeddings, embedding_dim)   # (tensor-wise) mapping from \n                                              # indices to embedding vectors\n```\n\n----------------------------------------\n\nTITLE: Computing Metrics for Question Answering Models in Python\nDESCRIPTION: A function that computes evaluation metrics for question answering models by comparing predicted answers with reference answers. It extracts the best answer spans based on logit scores and handles cases where no valid answers are found.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef compute_metrics(start_logits, end_logits, features, examples):\n    n_best = 20\n    max_answer_length = 30\n    metric = evaluate.load(\"squad\")\n\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    # for example in tqdm(examples):\n    for example in examples:\n        example_id = example[\"id\"]\n        context = example[\"context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip answers that are not fully in the context\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # Skip answers with a length that is either < 0\n                    # or > max_answer_length\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[\n                            offsets[start_index][0] : offsets[end_index][1]\n                        ],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    theoretical_answers = [\n        {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples\n    ]\n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n```\n\n----------------------------------------\n\nTITLE: Custom Operator Usage - Python\nDESCRIPTION: Example of using the custom operator in a PyTorch module.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass TryCustomOp(torch.nn.Module):\n    def __init__(self):\n        super(TryCustomOp, self).__init__()\n        self.f = torch.classes.my_classes.MyStackClass([\"foo\", \"bar\"])\n\n    def forward(self):\n        return torch.ops.my_classes.manipulate_instance(self.f)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Dynamic/Weight-Only Quantized Modules in Python\nDESCRIPTION: Shows the structure for defining a custom quantized module (`DynamicQuantNonTraceable` or `WeightOnlyQuantMNonTraceable`) for dynamic or weight-only quantization modes. It requires a `from_float` class method (implied, often named `from_observed` or similar in practice for consistency, converting directly from float) and configuration in `prepare_custom_config_dict` to mark the original float class as non-traceable.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass DynamicQuantNonTraceable: # or WeightOnlyQuantMNonTraceable\n    ...\n    @classmethod\n    def from_observed(cls, ...): # Note: In dynamic/weight-only, this often converts directly from the float module\n        ...\n\n    prepare_custom_config_dict = {\n        \"non_traceable_module_class\": [\n            FP32NonTraceable\n        ]\n    }\n```\n\n----------------------------------------\n\nTITLE: Creating a model evaluation function\nDESCRIPTION: This function evaluates a model on the specified data loader. It computes accuracy and average throughput (images per second) and returns these metrics along with the total time taken.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/README.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef evaluate(model, data_loader, neval_batches):\n    model.eval()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    cnt = 0\n    with torch.no_grad():\n        for image, target in data_loader:\n            output = model(image)\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            print('.', end = '')\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n            if cnt >= neval_batches:\n                 return top1, top5\n\n    return top1, top5\n\ndef load_model(model_file):\n    model = torch.jit.load(model_file)\n    model.eval()\n    return model\n\ndef print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\ndef prepare_data_loaders(data_path):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    dataset = torchvision.datasets.ImageNet(\n        data_path, split=\"train\",\n        transform=transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    dataset_test = torchvision.datasets.ImageNet(\n        data_path, split=\"val\",\n        transform=transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset)\n    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=train_batch_size,\n        sampler=train_sampler)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=eval_batch_size,\n        sampler=test_sampler)\n\n    return data_loader, data_loader_test\n\n# Sample execution (requires torchvision)\nfrom torchvision import models\nimport time\n```\n\n----------------------------------------\n\nTITLE: Extracting Features from Whole-Slide Images using HistoEncoder and TIAToolbox\nDESCRIPTION: Creates a DeepFeatureExtractor with the HistoEncoder model and extracts features from a WSI. The extractor automatically generates a tissue mask using Otsu thresholding to process only tissue-containing patches.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# create the feature extractor and run it on the WSI\nextractor = DeepFeatureExtractor(model=model, auto_generate_mask=True, batch_size=32, num_loader_workers=4, num_postproc_workers=4)\nwith suppress_console_output():\n    out = extractor.predict(imgs=[wsi_path], mode=\"wsi\", ioconfig=wsi_ioconfig, save_dir=global_save_dir / \"wsi_features\",)\n```\n\n----------------------------------------\n\nTITLE: Utilizing int8-mixed-bf16 Quantization with Autocast Context (Python)\nDESCRIPTION: This code demonstrates the use of torch.amp.autocast for int8-mixed-bf16 quantization. The model is run inside an autocast context, which enables some operations (e.g., convolutions) to output BFloat16 instead of float32, possibly improving performance and lowering memory use. Inputs must be int8, with output datatype depending on operator type and quantization flow. This requires PyTorch AMP support for Intel XPU and a quantized model already compiled.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith torch.amp.autocast(device_type=\"xpu\", dtype=torch.bfloat16), torch.no_grad():\n        # Turn on Autocast to use int8-mixed-bf16 quantization. After lowering into indcutor backend,\n        # For operators such as QConvolution and QLinear:\n        # * The input data type is consistently defined as int8, attributable to the presence of a pair\n        #    of quantization and dequantization nodes inserted at the input.\n        # * The computation precision remains at int8.\n        # * The output data type may vary, being either int8 or BFloat16, contingent on the presence\n        #   of a pair of quantization and dequantization nodes at the output.\n        # For non-quantizable pointwise operators, the data type will be inherited from the previous node,\n        # potentially resulting in a data type of BFloat16 in this scenario.\n        # For quantizable pointwise operators such as QMaxpool2D, it continues to operate with the int8\n        # data type for both input and output.\n        optimized_model = torch.compile(converted_model)\n\n        # Running some benchmark\n        optimized_model(*example_inputs)\n\n```\n\n----------------------------------------\n\nTITLE: Annotating Tensors with Derived Quantization Parameters - PyTorch Quantization - Python\nDESCRIPTION: This code shows how to specify quantization parameters for a tensor as a function of its input tensors, using DerivedQuantizationSpec. It defines a derive_qparams_fn that computes output scale and zero_point based on observer output from two inputs (for instance, activations and weights), and applies the resulting quantization spec in node annotation. Prerequisites are torch and types/classes for QuantizationAnnotation and DerivedQuantizationSpec. The input includes the relevant FX nodes and corresponding spec mappings.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef derive_qparams_fn(obs_or_fqs: List[ObserverOrFakeQuantize]) -> Tuple[Tensor, Tensor]:\n    assert len(obs_or_fqs) == 2, \\\n        \"Expecting two obs/fqs, one for activation and one for weight, got: {}\".format(len(obs_or_fq))\n    act_obs_or_fq = obs_or_fqs[0]\n    weight_obs_or_fq = obs_or_fqs[1]\n    act_scale, act_zp = act_obs_or_fq.calculate_qparams()\n    weight_scale, weight_zp = weight_obs_or_fq.calculate_qparams()\n    return torch.tensor([act_scale * weight_scale]).to(torch.float32), torch.tensor([0]).to(torch.int32)\n\nbias_qspec = DerivedQuantizationSpec(\n    derived_from=[(input_act, node), (weight, node)],\n    derive_qparams_fn=derive_qparams_fn,\n    dtype=torch.int32,\n    quant_min=-2**31,\n    quant_max=2**31 - 1,\n    qscheme=torch.per_tensor_symmetric,\n)\ninput_qspec_map = {input_act: act_quantization_spec, weight: weight_quantization_spec, bias: bias_qspec}\nnode.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n    input_qspec_map=input_qspec_map,\n    output_qspec=act_quantization_spec,\n    _annotated=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Launching PyTorch Sample Application with Bash Script\nDESCRIPTION: This Bash script is designed to launch a PyTorch sample application. It determines the base folder, activates a Python environment, changes to the correct directory, and runs the sample Python script.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/profile_with_itt.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# launch.sh\n\n#!/bin/bash\n\n# Retrieve the directory path where the path contains both the sample.py and launch.sh so that this bash script can be invoked from any directory\nBASEFOLDER=$( cd -- \"$( dirname -- \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\n<Activate a Python environment>\ncd ${BASEFOLDER}\npython sample.py\n```\n\n----------------------------------------\n\nTITLE: SAM2 Export Solution with Helper Class\nDESCRIPTION: Solution for exporting SAM2 model by creating a helper class that inherits from torch.nn.Module to wrap the _predict method.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_export_challenges_solutions.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass ExportHelper(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(_, *args, **kwargs):\n        return self._predict(*args, **kwargs)\n\nmodel_to_export = ExportHelper()\nep = torch.export.export(\n     model_to_export,\n     args=(unnorm_coords, labels, unnorm_box, mask_input,  multimask_output),\n     kwargs={\"return_logits\": return_logits},\n     strict=False,\n     )\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment for PyTorch Quantization Tutorial\nDESCRIPTION: This snippet sets up the environment for the FX Graph Mode quantization tutorial. It includes necessary imports (os, sys, time, numpy, torch, torchvision), defines helper classes and functions (`AverageMeter` for tracking metrics, `accuracy` for computing top-k accuracy, `evaluate` for model evaluation, `load_model` to load a pretrained ResNet18, `print_size_of_model` to check model size, `prepare_data_loaders` for ImageNet data), sets up warning filters, specifies a random seed for reproducibility, defines file paths and batch sizes, prepares data loaders, loads the floating-point model, and creates a copy for quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nimport time\nimport numpy as np\n\nimport torch\nfrom torch.ao.quantization import get_default_qconfig, QConfigMapping\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, fuse_fx\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.models.resnet import resnet18\nimport torchvision.transforms as transforms\n\n# Set up warnings\nimport warnings\nwarnings.filterwarnings(\n    action='ignore',\n    category=DeprecationWarning,\n    module=r'.*'\n)\nwarnings.filterwarnings(\n    action='default',\n    module=r'torch.ao.quantization'\n)\n\n# Specify random seed for repeatable results\n_ = torch.manual_seed(191009)\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef evaluate(model, criterion, data_loader):\n    model.eval()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    cnt = 0\n    with torch.no_grad():\n        for image, target in data_loader:\n            output = model(image)\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n    print('')\n\n    return top1, top5\n\ndef load_model(model_file):\n    model = resnet18(pretrained=False)\n    state_dict = torch.load(model_file, weights_only=True)\n    model.load_state_dict(state_dict)\n    model.to(\"cpu\")\n    return model\n\ndef print_size_of_model(model):\n    if isinstance(model, torch.jit.RecursiveScriptModule):\n        torch.jit.save(model, \"temp.p\")\n    else:\n        torch.jit.save(torch.jit.script(model), \"temp.p\")\n    print(\"Size (MB):\", os.path.getsize(\"temp.p\")/1e6)\n    os.remove(\"temp.p\")\n\ndef prepare_data_loaders(data_path):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    dataset = torchvision.datasets.ImageNet(\n        data_path, split=\"train\", transform=transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    dataset_test = torchvision.datasets.ImageNet(\n        data_path, split=\"val\", transform=transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset)\n    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=train_batch_size,\n        sampler=train_sampler)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=eval_batch_size,\n        sampler=test_sampler)\n\n    return data_loader, data_loader_test\n\ndata_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'resnet18_pretrained_float.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\nexample_inputs = (next(iter(data_loader))[0])\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to(\"cpu\")\nfloat_model.eval()\n\n# create another instance of the model since\n# we need to keep the original model around\nmodel_to_quantize = load_model(saved_model_dir + float_model_file).to(\"cpu\")\n```\n\n----------------------------------------\n\nTITLE: Installing Intel OpenMP Runtime Library\nDESCRIPTION: Commands to install Intel OpenMP Runtime Library using pip or conda\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/xeon_run_cpu.rst#2025-04-22_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ pip install intel-openmp\n```\n\nLANGUAGE: console\nCODE:\n```\n$ conda install mkl\n```\n\n----------------------------------------\n\nTITLE: Exporting Model with Dynamic Dimensions in PyTorch 2.5+\nDESCRIPTION: Code for exporting a PyTorch model with dynamic dimensions using torch.export.export_for_training. This approach allows the first dimension to be dynamic, which is useful for handling variable batch sizes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndynamic_shapes = tuple(\n  {0: torch.export.Dim(\"dim\")} if i == 0 else None\n  for i in range(len(example_inputs))\n)\nexported_model = torch.export.export_for_training(float_model, example_inputs, dynamic_shapes=dynamic_shapes).module()\n```\n\n----------------------------------------\n\nTITLE: Compiling and Saving Cache Artifacts with torch.compile\nDESCRIPTION: This snippet demonstrates how to compile a function using torch.compile, execute it, and then save the cache artifacts for later use. It uses the save_cache_artifacts() function to retrieve the portable cache data.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_compile_caching_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile\ndef fn(x, y):\n    return x.sin() @ y\n\na = torch.rand(100, 100, dtype=dtype, device=device)\nb = torch.rand(100, 100, dtype=dtype, device=device)\n\nresult = fn(a, b)\n\nartifacts = torch.compiler.save_cache_artifacts()\n\nassert artifacts is not None\nartifact_bytes, cache_info = artifacts\n\n# Now, potentially store artifact_bytes in a database\n# You can use cache_info for logging\n```\n\n----------------------------------------\n\nTITLE: Underlying Implementation Pattern of skip_init using Meta Device in Python\nDESCRIPTION: This snippet reveals the two-step process behind `torch.nn.utils.skip_init`. First, the module (`nn.Linear`) is instantiated on the 'meta' device, where `torch.nn.init` operations become no-ops, effectively skipping initialization. Second, `to_empty(device='cpu')` is called to materialize the module structure with uninitialized parameters on the target device (CPU in this case). This pattern requires the module to correctly handle the `device` argument.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/skip_param_init.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# 1. Initialize module on the meta device; all torch.nn.init ops have\n# no-op behavior on the meta device.\nm = nn.Linear(10, 5, device='meta')\n\n# 2. Materialize an uninitialized (empty) form of the module on the CPU device.\n# The result of this is a module instance with uninitialized parameters.\nm.to_empty(device='cpu')\n```\n\n----------------------------------------\n\nTITLE: Training PyTorch Model\nDESCRIPTION: This snippet shows the training loop for a PyTorch model, including loss calculation, backpropagation, and optimizer step.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nloss = F.nll_loss(output, target)\nloss.backward()\noptimizer.step()\nprint('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n      epoch, batch_idx * len(data), len(train_loader.dataset),\n      100. * batch_idx / len(train_loader), loss.item()))\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Forward and Backward LSTM Layers with Fork/Wait\nDESCRIPTION: Optimizes the bidirectional LSTM implementation by running forward and backward LSTM cells in parallel using torch.jit.fork() and torch.jit.wait(). This modification improves performance by overlapping the execution of both cells.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch-script-parallelism.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x : torch.Tensor) -> torch.Tensor:\n    # Forward layer - fork() so this can run in parallel to the backward\n    # layer\n    future_f = torch.jit.fork(self.cell_f, x)\n\n    # Backward layer. Flip input in the time dimension (dim 0), apply the\n    # layer, then flip the outputs in the time dimension\n    x_rev = torch.flip(x, dims=[0])\n    output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n    output_b_rev = torch.flip(output_b, dims=[0])\n\n    # Retrieve the output from the forward layer. Note this needs to happen\n    # *after* the stuff we want to parallelize with\n    output_f, _ = torch.jit.wait(future_f)\n\n    return torch.cat((output_f, output_b_rev), dim=2)\n```\n\n----------------------------------------\n\nTITLE: Using torch.autograd in PyTorch\nDESCRIPTION: Reference to PyTorch's automatic differentiation system that custom operators can integrate with. This allows custom operations to support gradient computation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/custom_ops_landing_page.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntorch.autograd\n```\n\n----------------------------------------\n\nTITLE: Building a PyTorch C++ Extension with setuptools in Python\nDESCRIPTION: This Python snippet provides an example setup.py script for building a PyTorch C++ extension (out-of-tree backend) using setuptools and torch.utils.cpp_extension. It specifies the package name, C++ source files, include directories, compiler and linker flags, and custom build extensions. The example assumes existence of variables such as torch_xla_sources, include_dirs, and extra_compile_args, which must be defined as required by your backend.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/extend_dispatcher.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name='torch_xla',\n    ext_modules=[\n        CppExtension(\n            '_XLAC',\n            torch_xla_sources,\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            library_dirs=library_dirs,\n            extra_link_args=extra_link_args + \\\n                [make_relative_rpath('torch_xla/lib')],\n        ),\n    ],\n    cmdclass={\n        'build_ext': Build,  # Build is a derived class of BuildExtension\n    }\n    # more configs...\n)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Distributed Joinable Counter with PyTorch Join Context - Python\nDESCRIPTION: This code defines a custom Counter class that integrates with PyTorch's Join context manager to participate in distributed training synchronization. It uses distributed communication primitives (all_reduce, broadcast) to count and sync the number of processed inputs across ranks. The class relies on torch.distributed, torch.multiprocessing, and torch's distributed join API, and demonstrates custom hook implementation via inheritance from JoinHook and Joinable. The implementation includes argument-based customization (sync_max_count), process group handling, collective communication, and context-managed workflow. Inputs consist of per-rank tensors, outputs are synchronized counters, and running requires at least two CUDA-capable devices.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/generic_join.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.distributed.algorithms.join import Join, Joinable, JoinHook\n\nBACKEND = \"nccl\"\nWORLD_SIZE = 2\nNUM_INPUTS = 5\n\nclass CounterJoinHook(JoinHook):\n    r\"\"\"\n    Join hook for :class:`Counter`.\n\n    Arguments:\n        counter (Counter): the :class:`Counter` object using this hook.\n        sync_max_count (bool): whether to sync the max count once all ranks\n            join.\n    \"\"\"\n    def __init__(\n        self,\n        counter,\n        sync_max_count\n    ):\n        self.counter = counter\n        self.sync_max_count = sync_max_count\n\n    def main_hook(self):\n        r\"\"\"\n        Shadows the counter's all-reduce by all-reducing a dim-1 zero tensor.\n        \"\"\"\n        t = torch.zeros(1, device=self.counter.device)\n        dist.all_reduce(t)\n\n    def post_hook(self, is_last_joiner: bool):\n        r\"\"\"\n        Synchronizes the max count across all :class:`Counter` s if\n        ``sync_max_count=True``.\n        \"\"\"\n        if not self.sync_max_count:\n            return\n        rank = dist.get_rank(self.counter.process_group)\n        common_rank = self.counter.find_common_rank(rank, is_last_joiner)\n        if rank == common_rank:\n            self.counter.max_count = self.counter.count.detach().clone()\n        dist.broadcast(self.counter.max_count, src=common_rank)\n\nclass Counter(Joinable):\n    r\"\"\"\n    Example :class:`Joinable` that counts the number of training iterations\n    that it participates in.\n    \"\"\"\n    def __init__(self, device, process_group):\n        super(Counter, self).__init__()\n        self.device = device\n        self.process_group = process_group\n        self.count = torch.tensor([0], device=device).float()\n        self.max_count = torch.tensor([0], device=device).float()\n\n    def __call__(self):\n        r\"\"\"\n        Counts the number of inputs processed on this iteration by all ranks\n        by all-reducing a dim-1 one tensor; increments its own internal count.\n        \"\"\"\n        Join.notify_join_context(self)\n        t = torch.ones(1, device=self.device).float()\n        dist.all_reduce(t)\n        self.count += t\n\n    def join_hook(self, **kwargs) -> JoinHook:\n        r\"\"\"\n        Return a join hook that shadows the all-reduce in :meth:`__call__`.\n        \n        This join hook supports the following keyword arguments:\n            sync_max_count (bool, optional): whether to synchronize the maximum\n                count across all ranks once all ranks join; default is ``False``.\n        \"\"\"\n        sync_max_count = kwargs.get(\"sync_max_count\", False)\n        return CounterJoinHook(self, sync_max_count)\n\n    @property\n    def join_device(self) -> torch.device:\n        return self.device\n\n    @property\n    def join_process_group(self):\n        return self.process_group\n\n    def find_common_rank(self, rank, to_consider):\n        r\"\"\"\n        Returns the max rank of the ones to consider over the process group.\n        \"\"\"\n        common_rank = torch.tensor([rank if to_consider else -1], device=self.device)\n        dist.all_reduce(common_rank, op=dist.ReduceOp.MAX, group=self.process_group)\n        common_rank = common_rank.item()\n        return common_rank\n\ndef worker(rank):\n    assert torch.cuda.device_count() >= WORLD_SIZE\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    counter = Counter(torch.device(f\"cuda:{rank}\"), dist.group.WORLD)\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    with Join([counter], sync_max_count=True):\n        for _ in inputs:\n            counter()\n\n    print(f\"{int(counter.count.item())} inputs processed before rank {rank} joined!\")\n    print(f\"{int(counter.max_count.item())} inputs processed across all ranks!\")\n\ndef main():\n    mp.spawn(worker, nprocs=WORLD_SIZE, join=True)\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n----------------------------------------\n\nTITLE: Quantizing PyTorch Model with Intel Neural Compressor\nDESCRIPTION: This code demonstrates how to use Intel Neural Compressor to quantize a PyTorch model. It sets up the quantizer, specifies the model, training function, and evaluation dataloader.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(\"./conf.yaml\")\nquantizer.model = model\nquantizer.q_func = training_func\nquantizer.eval_dataloader = test_loader\nq_model = quantizer()\nq_model.save('./output')\n```\n\n----------------------------------------\n\nTITLE: Initializing TraceAnalysis with trace directory\nDESCRIPTION: Code to import and initialize the TraceAnalysis class from HTA with a path to the trace files. This is the starting point for any analysis with HTA.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom hta.trace_analysis import TraceAnalysis\ntrace_dir = \"/path/to/folder/with/traces\"\nanalyzer = TraceAnalysis(trace_dir=trace_dir)\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Vision Modules\nDESCRIPTION: Imports for PyTorch's vision package, which provides datasets, model architectures, and image transformations for computer vision tasks. These utilities simplify working with image data and pre-trained models.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import datasets, models, transforms     # vision datasets, architectures & transforms\nimport torchvision.transforms as transforms              # composable transforms\n```\n\n----------------------------------------\n\nTITLE: Applying Activation Functions\nDESCRIPTION: Activation functions available in PyTorch's nn module, used for introducing non-linearity in neural networks. These include common activations like ReLU, sigmoid, and tanh, as well as specialized variants.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnn.X                                  # where X is ReLU, ReLU6, ELU, SELU, PReLU, LeakyReLU, \n                                      # RReLu, CELU, GELU, Threshold, Hardshrink, HardTanh,\n                                      # Sigmoid, LogSigmoid, Softplus, SoftShrink, \n                                      # Softsign, Tanh, TanhShrink, Softmin, Softmax, \n                                      # Softmax2d, LogSoftmax or AdaptiveSoftmaxWithLoss\n```\n\n----------------------------------------\n\nTITLE: Setting MKLDNN Matmul Minimum Dimension Threshold in Bash\nDESCRIPTION: This command sets the minimum dimension threshold for MKLDNN matmul operations, which can improve performance for smaller batch sizes by using OpenBLAS instead.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ export TORCH_MKLDNN_MATMUL_MIN_DIM=64\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Quantization Specification - PyTorch Quantization - Python\nDESCRIPTION: This snippet demonstrates how to annotate FX graph nodes so that two input tensors share the same quantization parameters by using SharedQuantizationSpec. The annotation is applied by creating and mapping QuantizationSpec or SharedQuantizationSpec objects to input nodes, followed by setting them within the node's meta quantization_annotation. Dependencies include torch, QuantizationAnnotation, QuantizationSpec, and SharedQuantizationSpec, and the expected inputs are FX nodes corresponding to input_act0, input_act1, and add_node.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninput_qspec_map = {}\nshare_qparams_with_input_act0_qspec = SharedQuantizationSpec((input_act0, add_node))\ninput_qspec_map = {input_act0: act_quantization_spec, input_act1: share_qparams_with_input_act0_qspec}\n\nadd_node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n    input_qspec_map=input_qspec_map,\n    output_qspec=act_quantization_spec,\n    _annotated=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Ring-Allreduce for Tensor Summation using PyTorch Distributed (Python)\nDESCRIPTION: Provides a manual implementation of the ring-allreduce algorithm for summing tensors across processes using point-to-point nonblocking send/receive operations with PyTorch distributed. Each process circulates its tensor, accumulating received data. The function expects send and recv tensors as arguments and synchronizes using isend/recv for correct ordering. Dependencies: torch.distributed, correct group/size/rank setup, and compatible tensor shapes. All processes must participate to prevent deadlocks, and performance is suboptimal for large tensors unless chunking/buffering is added.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"\"\" Implementation of a ring-reduce with addition. \"\"\"\ndef allreduce(send, recv):\n   rank = dist.get_rank()\n   size = dist.get_world_size()\n   send_buff = send.clone()\n   recv_buff = send.clone()\n   accum = send.clone()\n\n   left = ((rank - 1) + size) % size\n   right = (rank + 1) % size\n\n   for i in range(size - 1):\n       if i % 2 == 0:\n           # Send send_buff\n           send_req = dist.isend(send_buff, right)\n           dist.recv(recv_buff, left)\n           accum[:] += recv_buff[:]\n       else:\n           # Send recv_buff\n           send_req = dist.isend(recv_buff, right)\n           dist.recv(send_buff, left)\n           accum[:] += send_buff[:]\n       send_req.wait()\n   recv[:] = accum[:]\n```\n\n----------------------------------------\n\nTITLE: Performing Linear Algebra Operations\nDESCRIPTION: Basic linear algebra operations in PyTorch, including matrix multiplication, matrix-vector multiplication, and matrix transpose.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nret = A.mm(B)       # matrix multiplication\nret = A.mv(x)       # matrix-vector multiplication\nx = x.t()           # matrix transpose\n```\n\n----------------------------------------\n\nTITLE: Implementing dynamic quantization on ResNet18\nDESCRIPTION: This snippet demonstrates dynamic quantization, which quantizes weights ahead of time but performs quantization for activations dynamically during inference. It's applied to the same ResNet18 model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/README.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Specify quantization configuration for dynamic quantization\nquantized_model = torch.quantization.quantize_dynamic(\n    float_model,  # the model to be quantized\n    {nn.Linear, nn.Conv2d},  # a set of layers to quantize\n    dtype=torch.qint8  # the target dtype for quantized weights\n)\n\n# Print model size after quantization\nprint_size_of_model(quantized_model)\n\n# Evaluate the dynamically quantized model\ntop1, top5 = evaluate(quantized_model, eval_dataloader, neval_batches=num_eval_batches)\nprint('Dynamically quantized model accuracy: %2.2f%%' % top1.avg)\n```\n\n----------------------------------------\n\nTITLE: Refactoring PyTorch Module to Isolate Traceable Submodule\nDESCRIPTION: Illustrates refactoring a PyTorch model by extracting the traceable code into a separate `FP32Traceable` submodule. The main module `M` then instantiates and calls this submodule, allowing FX quantization to be applied specifically to the `traceable_submodule`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass FP32Traceable(nn.Module):\n    def forward(self, x):\n        x = traceable_code(x)\n        return x\n\nclass M(nn.Module):\n    def __init__(self):\n        self.traceable_submodule = FP32Traceable(...)\n    def forward(self, x):\n        x = self.traceable_code_1(x)\n        # We'll only symbolic trace/quantize this submodule\n        x = self.traceable_submodule(x)\n        x = self.traceable_code_2(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Getting temporal breakdown of GPU usage\nDESCRIPTION: Code to analyze how GPUs spend time between compute, non-compute, and idle states. This generates a dataframe showing the temporal breakdown for each rank.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nanalyzer = TraceAnalysis(trace_dir = \"/path/to/trace/folder\")\ntime_spent_df = analyzer.get_temporal_breakdown()\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Running Bundled Inputs for Multiple Methods - PyTorch - Python\nDESCRIPTION: Retrieves details of all bundled input functions and uses them to run the PyTorch model's exported methods with each provided test input. The code iterates over all bundled functions, fetches the associated input getter, and invokes the function on each bundled input. It showcases how to use the advanced get_bundled_inputs_functions_and_info API.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/bundled_inputs.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nall_info = bundled_model.get_bundled_inputs_functions_and_info()\\n\\n# The return type for get_bundled_inputs_functions_and_info is complex, but essentially we are retrieving the name\\n# of a function we can use to get the bundled input for our models method\\nfor func_name in all_info.keys():\\n    input_func_name = all_info[func_name]['get_inputs_function_name'][0]\\n    func_to_run = getattr(bundled_model, input_func_name)\\n    # retrieve input\\n    sample_input = func_to_run()\\n    model_function = getattr(bundled_model, func_name)\\n    for i in range(len(sample_input)):\\n        print(model_function(*sample_input[i]))\n```\n\n----------------------------------------\n\nTITLE: Defining Symmetric Quantization Configuration for XPUInductorQuantizer (Python)\nDESCRIPTION: This snippet provides a function to create a symmetric quantization configuration for int8 quantization, using HistogramObserver and PerChannelMinMaxObserver for activations and weights, respectively. It sets parameters such as eps, quant_min, quant_max, and quantization schemes, and produces a QuantizationConfig instance. The configuration is then set to the quantizer for use in post-training static quantization. Dependencies include various torch.ao.quantization modules for observers, quantizer, and quant config.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.ao.quantization.observer import HistogramObserver, PerChannelMinMaxObserver\nfrom torch.ao.quantization.quantizer.quantizer import QuantizationSpec\nfrom torch.ao.quantization.quantizer.xnnpack_quantizer_utils import QuantizationConfig\nfrom typing import Any, Optional, TYPE_CHECKING\nif TYPE_CHECKING:\n    from torch.ao.quantization.qconfig import _ObserverOrFakeQuantizeConstructor\ndef get_xpu_inductor_symm_quantization_config():\n    extra_args: dict[str, Any] = {\"eps\": 2**-12}\n    act_observer_or_fake_quant_ctr = HistogramObserver\n    act_quantization_spec = QuantizationSpec(\n        dtype=torch.int8,\n        quant_min=-128,\n        quant_max=127,\n        qscheme=torch.per_tensor_symmetric,  # Change the activation quant config to symmetric\n        is_dynamic=False,\n        observer_or_fake_quant_ctr=act_observer_or_fake_quant_ctr.with_args(\n            **extra_args\n        ),\n    )\n\n    weight_observer_or_fake_quant_ctr: _ObserverOrFakeQuantizeConstructor = (\n        PerChannelMinMaxObserver\n    )\n\n    weight_quantization_spec = QuantizationSpec(\n        dtype=torch.int8,\n        quant_min=-128,\n        quant_max=127,\n        qscheme=torch.per_channel_symmetric, # Same as the default config, the only supported option for weight\n        ch_axis=0,  # 0 corresponding to weight shape = (oc, ic, kh, kw) of conv\n        is_dynamic=False,\n        observer_or_fake_quant_ctr=weight_observer_or_fake_quant_ctr.with_args(\n            **extra_args\n        ),\n    )\n\n    bias_quantization_spec = None  # will use placeholder observer by default\n    quantization_config = QuantizationConfig(\n        act_quantization_spec,\n        act_quantization_spec,\n        weight_quantization_spec,\n        bias_quantization_spec,\n        False,\n    )\n    return quantization_config\n\n# Then, set the quantization configuration to the quantizer. \nquantizer = XPUInductorQuantizer()\nquantizer.set_global(get_xpu_inductor_symm_quantization_config())\n\n```\n\n----------------------------------------\n\nTITLE: TCPStore Initialization with Legacy Backend and listen_fd - PyTorch - Python\nDESCRIPTION: Demonstrates how to initialize TCPStore using a listen_fd file descriptor, which is unsupported with the libuv backend but allowed with the legacy backend by passing use_libuv=False. Attempting to initialize with a listen_fd without disabling libuv will raise NotImplementedError. Inputs include a socket created/bound on localhost and its detached file descriptor. Dependencies: torch, socket.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TCPStore_libuv_backend.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport socket\n\nimport torch\nimport torch.distributed as dist\n\nlisten_sock: socket.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nlisten_sock.bind((\"localhost\", 0))\naddr, port, *_ = listen_sock.getsockname()\nlisten_fd = listen_sock.detach()\n\ntcpstore = dist.TCPStore(addr, port, 1, True, master_listen_fd=listen_fd)  # expect NotImplementedError\ntcpstore = dist.TCPStore(addr, port, 1, True, master_listen_fd=listen_fd, use_libuv=False)  # OK. Use legacy backend\n\n```\n\n----------------------------------------\n\nTITLE: Reporting PyTorch GEMM Autotuning Results (Shell)\nDESCRIPTION: Shows the format of autotuning logs emitted when using max-autotune mode in PyTorch for GEMM/linear operations. The output includes names of candidate kernels, their measured latencies, and relative speedup percentages, facilitating insights into which backend is selected. It depends on enabling the proper logging configuration and executing a successfully compiled and autotuned model. Outputs are for illustration and may vary per environment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/max_autotune_on_CPU_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nAUTOTUNE linear_unary(64x16, 32x16, 32)\ncpp_packed_gemm_0 0.2142 ms 100.0% \n_linear_pointwise 0.2441 ms 87.7% \n\n```\n\n----------------------------------------\n\nTITLE: Importing Computer Vision Utilities\nDESCRIPTION: Imports for PyTorch's vision module (torchvision), which provides datasets, model architectures, and image transformations for computer vision tasks.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision import datasets, models, transforms     # vision datasets, \n                                                         # architectures & \n                                                         # transforms\n\nimport torchvision.transforms as transforms              # composable transforms\n```\n\n----------------------------------------\n\nTITLE: Declaring CUDA Helper Functions for LLTM Operations (Sigmoid, ELU, etc.) in C++\nDESCRIPTION: This snippet defines device-level helper functions for activation and their derivatives (sigmoid, tanh, elu, d_sigmoid, d_tanh, d_elu) using CUDA-specific modifiers. These functions are used in the CUDA kernels for fast and accurate element-wise computation. Dependencies are CUDA runtime and ATen headers; all functions are templated and run on GPU.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/extension.h>\n\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#include <vector>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t sigmoid(scalar_t z) {\n  return 1.0 / (1.0 + exp(-z));\n}\n\n```\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t d_sigmoid(scalar_t z) {\n  const auto s = sigmoid(z);\n  return (1.0 - s) * s;\n}\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t d_tanh(scalar_t z) {\n  const auto t = tanh(z);\n  return 1 - (t * t);\n}\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t elu(scalar_t z, scalar_t alpha = 1.0) {\n  return fmax(0.0, z) + fmin(0.0, alpha * (exp(z) - 1.0));\n}\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t d_elu(scalar_t z, scalar_t alpha = 1.0) {\n  const auto e = exp(z);\n  const auto d_relu = z < 0.0 ? 0.0 : 1.0;\n  return d_relu + (((alpha * (e - 1.0)) < 0.0) ? (alpha * e) : 0.0);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto-Tuning Quantization in YAML\nDESCRIPTION: YAML configuration file for Intel Neural Compressor quantization settings. Specifies the model framework as PyTorch FX, defines the evaluation metric as top-1 accuracy, and sets the relative accuracy criterion to 1%.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# conf.yaml\nmodel:\n    name: LeNet\n    framework: pytorch_fx\n\nevaluation:\n    accuracy:\n        metric:\n            topk: 1\n\ntuning:\n  accuracy_criterion:\n    relative: 0.01\n```\n\n----------------------------------------\n\nTITLE: Measuring Model Execution Time for Different Batch Sizes in PyTorch\nDESCRIPTION: A helper function that measures the execution time of a model for different batch sizes. It prepares input data for the model, runs inference in torch.inference_mode(), and returns the median execution time for each batch size.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef measure_execution_time(model, batch_sizes, dataset):\n    dataset_for_model = dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n    dataset_for_model.set_format(\"torch\")\n    model.cuda()\n    batch_size_to_time_sec = {}\n    for batch_size in batch_sizes:\n        batch = {\n            k: dataset_for_model[k][:batch_size].to(model.device)\n            for k in dataset_for_model.column_names\n        }\n\n        with torch.inference_mode():\n            timer = benchmark.Timer(\n                stmt=\"model(**batch)\", globals={\"model\": model, \"batch\": batch}\n            )\n            p50 = timer.blocked_autorange().median * 1000\n        batch_size_to_time_sec[batch_size] = p50\n    return batch_size_to_time_sec\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Inference Speed of Scripted PyTorch Models\nDESCRIPTION: This Python snippet defines a `run_benchmark` function to measure the inference performance of a TorchScript model. The function loads a model from a file using `torch.jit.load`, runs it in evaluation mode over a fixed number of batches from a data loader, calculates the total elapsed time, and prints the average inference time per image. It is subsequently called to compare the speed of a scripted floating-point model and a scripted quantized model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef run_benchmark(model_file, img_loader):  \n    elapsed = 0 \n    model = torch.jit.load(model_file)  \n    model.eval()  \n    num_batches = 5 \n    # Run the scripted model on a few batches of images \n    for i, (images, target) in enumerate(img_loader): \n        if i < num_batches: \n            start = time.time() \n            output = model(images)  \n            end = time.time() \n            elapsed = elapsed + (end-start) \n        else: \n            break \n    num_images = images.size()[0] * num_batches \n\n    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000)) \n    return elapsed  \n\nrun_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)  \n\nrun_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)\n```\n\n----------------------------------------\n\nTITLE: Configuring `convert_fx` for Custom Static Quantization Modules in Python\nDESCRIPTION: Illustrates configuring `convert_fx` for static quantization with custom modules. The `convert_custom_config_dict` maps the observed custom module (`ObservedNonTraceable`) to its final statically quantized version (`StaticQuantNonTraceable`) under the 'static' key.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nconvert_custom_config_dict = {\n    \"observed_to_quantized_custom_module_class\": {\n        \"static\": {\n            ObservedNonTraceable: StaticQuantNonTraceable,\n        }\n    },\n}\nmodel_quantized = convert_fx(\n    model_prepared,\n    convert_custom_config_dict)\n```\n\n----------------------------------------\n\nTITLE: Executing TorchScript Model Inference Using PyTorch C++ API - C++\nDESCRIPTION: Illustrates how to prepare input data, execute a loaded TorchScript (script::Module) model's forward pass, and extract the result as a tensor in C++. Uses torch::jit::IValue and torch::ones to construct a suitable input and demonstrate output slicing/printing for result inspection. Depends on libtorch and assumes a previously-loaded script::Module object named 'module'. Input is a tensor with shape {1, 3, 224, 224}, and output is the result tensor printed to stdout.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_export.rst#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n// Create a vector of inputs.\\nstd::vector<torch::jit::IValue> inputs;\\ninputs.push_back(torch::ones({1, 3, 224, 224}));\\n\\n// Execute the model and turn its output into a tensor.\\nat::Tensor output = module.forward(inputs).toTensor();\\nstd::cout << output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) << '\\\\n';\n```\n\n----------------------------------------\n\nTITLE: Using torch.add in PyTorch\nDESCRIPTION: Example of referencing a built-in PyTorch tensor operator. This shows the syntax for referring to a function in the torch namespace.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/custom_ops_landing_page.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntorch.add\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch with Vulkan on macOS (Shell)\nDESCRIPTION: Shell command to build and install PyTorch from source on macOS with Vulkan backend support enabled. Sets environment variables `USE_VULKAN=1`, `USE_VULKAN_SHADERC_RUNTIME=1`, `USE_VULKAN_WRAPPER=0`, `MACOSX_DEPLOYMENT_TARGET`, `CC`, and `CXX` for the build process. Assumes being in the PyTorch root directory (`PYTORCH_ROOT`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd PYTORCH_ROOT\nUSE_VULKAN=1 USE_VULKAN_SHADERC_RUNTIME=1 USE_VULKAN_WRAPPER=0 MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Training Data for QA Task - Transformers/HuggingFace - Python\nDESCRIPTION: Begins definition of a helper function for preparing training examples using the HuggingFace tokenizer for BERT QA fine-tuning. The function tokenizes question-context pairs with fixed maximum sequence length, truncation on the second sequence, and retains offset mapping to enable span annotation. Dependencies include the tokenizer and a batch of input examples. The function expects to output preprocessed features suitable for use in PyTorch datasets, though the full function body is not shown here.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_train_function(examples, tokenizer):\n    inputs = tokenizer(\n        [q.strip() for q in examples[\"question\"]],\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n\n```\n\n----------------------------------------\n\nTITLE: Serializing a TorchScript Script Function with Custom Op - Python\nDESCRIPTION: Loads the custom operator library and defines then serializes the compute function using compute.save. Prepares a model 'example.pt' for cross-language deployment. Requires torch and the compiled op library. Inputs are tensors; the output is saved as a TorchScript file.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntorch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z\n\ncompute.save(\"example.pt\")\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Declarations Sample in C++\nDESCRIPTION: Sample from RegistrationDeclarations.h showing how PyTorch operators are declared with their schemas and dispatch information.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/extend_dispatcher.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nTensor abs(const Tensor & self); // {\"schema\": \"aten::abs(Tensor self) -> Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor & abs_(Tensor & self); // {\"schema\": \"aten::abs_(Tensor(a!) self) -> Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor & abs_out(Tensor & out, const Tensor & self); // {\"schema\": \"aten::abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor absolute(const Tensor & self); // {\"schema\": \"aten::absolute(Tensor self) -> Tensor\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor & absolute_(Tensor & self); // {\"schema\": \"aten::absolute_(Tensor(a!) self) -> Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor & absolute_out(Tensor & out, const Tensor & self); // {\"schema\": \"aten::absolute.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor angle(const Tensor & self); // {\"schema\": \"aten::angle(Tensor self) -> Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor & angle_out(Tensor & out, const Tensor & self); // {\"schema\": \"aten::angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor sgn(const Tensor & self); // {\"schema\": \"aten::sgn(Tensor self) -> Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\n```\n\n----------------------------------------\n\nTITLE: Timing Model Evaluation in PyTorch\nDESCRIPTION: Defines a function `time_model_evaluation` that wraps the call to the previously defined `evaluate` function. It records the time before and after the evaluation process using `time.time()` and returns the evaluation results obtained from `evaluate`. This allows for measuring and comparing the inference performance of different models, such as the original FP32 model versus the quantized INT8 model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n```\n\n----------------------------------------\n\nTITLE: Using TORCH_LIBRARY C++ API in PyTorch\nDESCRIPTION: Reference to the C++ macro for registering custom operators in PyTorch. This is used when implementing custom operators in C++.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/custom_ops_landing_page.rst#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY\n```\n\n----------------------------------------\n\nTITLE: Setting Up TensorBoard Writer and Data Visualization\nDESCRIPTION: Configures TensorBoard SummaryWriter and implements functions to visualize data and model predictions with probabilities.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tensorboard_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.tensorboard import SummaryWriter\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')\n\n# helper functions\ndef images_to_probs(net, images):\n    '''\n    Generates predictions and corresponding probabilities from a trained\n    network and a list of images\n    '''\n    output = net(images)\n    # convert output probabilities to predicted class\n    _, preds_tensor = torch.max(output, 1)\n    preds = np.squeeze(preds_tensor.numpy())\n    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n\ndef plot_classes_preds(net, images, labels):\n    '''\n    Generates matplotlib Figure using a trained network, along with images\n    and labels from a batch, that shows the network's top prediction along\n    with its probability, alongside the actual label, coloring this\n    information based on whether the prediction was correct or not.\n    Uses the \"images_to_probs\" function.\n    '''\n    preds, probs = images_to_probs(net, images)\n    # plot the images in the batch, along with predicted and true labels\n    fig = plt.figure(figsize=(12, 48))\n    for idx in np.arange(4):\n        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n        matplotlib_imshow(images[idx], one_channel=True)\n        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n            classes[preds[idx]],\n            probs[idx] * 100.0,\n            classes[labels[idx]]))\n```\n\n----------------------------------------\n\nTITLE: Downloading the MRPC GLUE Dataset - Shell\nDESCRIPTION: Downloads the MRPC dataset from the GLUE benchmark using a provided Python script, saving the data to the glue_data directory. The '--tasks' argument specifies that only the MRPC subset should be downloaded. The script must be present (such as from a gist link mentioned earlier); otherwise, download it first. This step is critical before model training and evaluation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython download_glue_data.py --data_dir='glue_data' --tasks='MRPC'\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Tensor Operations with PrivateUse1\nDESCRIPTION: Implementation of custom tensor operations and CPU fallback mechanism for the new backend using TORCH_LIBRARY_IMPL.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nat::Tensor wrapper_Custom_Tensor_add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {\n  // Implementation of add kernel in new backend\n  ...\n}\n\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  ...\n  m.impl(\"add.Tensor\", TORCH_FN(wrapper_Custom_Tensor_add));\n  ...\n}\n\nvoid custom_cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {\n  // Add some hints about new devices that do not support and need to fall back to cpu\n  at::native::cpu_fallback(op, stack);\n}\n\nTORCH_LIBRARY_IMPL(_, PrivateUse1, m) {\n  m.fallback(torch::CppFunction::makeFromBoxedFunction<&custom_cpu_fallback>());\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning Image and Label Mapping Files from TorchServe - Shell\nDESCRIPTION: Shell commands to download a sample image and class-label mapping for ImageNet classification. Uses git and cp commands; requires git to be installed. Useful for quickly obtaining testing files, but not strictly required to run the Flask app.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/deployment_with_flask.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/pytorch/serve\\ncp serve/examples/image_classifier/kitten.jpg .\\ncp serve/examples/image_classifier/index_to_name.json .\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Neural Network Components\nDESCRIPTION: Imports for PyTorch's neural network API, including the computation graph, tensor operations, neural network modules, functional layers, and optimizers.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch.autograd as autograd         # computation graph\nfrom torch import Tensor                  # tensor node in the computation graph\nimport torch.nn as nn                     # neural networks\nimport torch.nn.functional as F           # layers, activations and more\nimport torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Saving Quantized and Float Models - PyTorch - Python\nDESCRIPTION: Performs model size assessment, evaluation, and attempts serialization/deserialization for both quantized and float models in PyTorch. The code uses print_size_of_model and evaluate functions, compares before/after quantization, and demonstrates multiple commented-out model saving strategies (full save, state_dict, scripted). Highlights caveats with some serialization methods due to module attribute errors, and prefers torch.jit.script for saving. Inputs depend on float_model, quantized_model, test data, and required criterion.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n    print(\"Size of model before quantization\")\n    print_size_of_model(float_model)\n    print(\"Size of model after quantization\")\n    print_size_of_model(quantized_model)\n    top1, top5 = evaluate(quantized_model, criterion, data_loader_test)\n    print(\"[before serilaization] Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n\n    fx_graph_mode_model_file_path = saved_model_dir + \"resnet18_fx_graph_mode_quantized.pth\"\n\n    # this does not run due to some erros loading convrelu module:\n    # ModuleAttributeError: 'ConvReLU2d' object has no attribute '_modules'\n    # save the whole model directly\n    # torch.save(quantized_model, fx_graph_mode_model_file_path)\n    # loaded_quantized_model = torch.load(fx_graph_mode_model_file_path, weights_only=False)\n\n    # save with state_dict\n    # torch.save(quantized_model.state_dict(), fx_graph_mode_model_file_path)\n    # import copy\n    # model_to_quantize = copy.deepcopy(float_model)\n    # prepared_model = prepare_fx(model_to_quantize, {\"\": qconfig})\n    # loaded_quantized_model = convert_fx(prepared_model)\n    # loaded_quantized_model.load_state_dict(torch.load(fx_graph_mode_model_file_path), weights_only=True)\n\n    # save with script\n    torch.jit.save(torch.jit.script(quantized_model), fx_graph_mode_model_file_path)\n    loaded_quantized_model = torch.jit.load(fx_graph_mode_model_file_path)\n\n    top1, top5 = evaluate(loaded_quantized_model, criterion, data_loader_test)\n    print(\"[after serialization/deserialization] Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n```\n\n----------------------------------------\n\nTITLE: Preallocating Tensors for CUDA Graph Optimization\nDESCRIPTION: Shows how to preallocate tensors for reuse in the training loop, which is necessary before applying CUDA Graphs to optimize performance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::TensorOptions FloatCUDA =\n    torch::TensorOptions(device).dtype(torch::kFloat);\ntorch::TensorOptions LongCUDA =\n    torch::TensorOptions(device).dtype(torch::kLong);\n\ntorch::Tensor data = torch::zeros({kTrainBatchSize, 1, 28, 28}, FloatCUDA);\ntorch::Tensor targets = torch::zeros({kTrainBatchSize}, LongCUDA);\ntorch::Tensor output = torch::zeros({1}, FloatCUDA);\ntorch::Tensor loss = torch::zeros({1}, FloatCUDA);\n\nfor (auto& batch : data_loader) {\n  data.copy_(batch.data);\n  targets.copy_(batch.target);\n  training_step(model, optimizer, data, targets, output, loss);\n}\n```\n\n----------------------------------------\n\nTITLE: Starting the Flask Server for the Image Classifier API - Shell\nDESCRIPTION: Command for launching the Flask app from the shell. FLASK_APP is set to 'app.py', and flask run starts the web server (default on port 5000). Requires Flask to be installed. Expects 'app.py' to be present in the working directory.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/deployment_with_flask.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nFLASK_APP=app.py flask run\n```\n\n----------------------------------------\n\nTITLE: Setting up PyTorch Dependencies and Device for Graviton\nDESCRIPTION: Imports the required PyTorch packages and defines the CPU device that will be used for inference on AWS Graviton3.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\n# AWS Graviton3 cpu\ndevice = (\"cpu\")\nprint(f\"Using {device} device\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Post-Training Quantization for X86 Inductor in Python\nDESCRIPTION: Instantiates the `X86InductorQuantizer` and applies the default dynamic quantization configuration by setting `is_dynamic=True` in `get_default_x86_inductor_quantization_config`. This configures the quantizer specifically for dynamic PTQ.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nquantizer = X86InductorQuantizer()\nquantizer.set_global(xiq.get_default_x86_inductor_quantization_config(is_dynamic=True))\n```\n\n----------------------------------------\n\nTITLE: Defining a Non-Traceable Module Wrapper in Python\nDESCRIPTION: Illustrates wrapping non-traceable logic within a dedicated `FP32NonTraceable` module. This step is part of the process for defining custom observed and quantized versions for parts of the model that cannot be made traceable.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass FP32NonTraceable:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Global Settings for Fine-Tuned BERT (Python)\nDESCRIPTION: This Python snippet prepares and configures global variables and settings for BERT model evaluation with the MRPC task. It sets model/data paths, loads task processors, initializes the tokenizer and model (in TorchScript mode), and defines reproducible random seeds. Dependencies include Huggingface Transformers and torch. Key parameters: model paths, tokenizer paths, maximum input length, and computing device. Outputs: instantiated model, tokenizer, and configuration namespace.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconfigs = Namespace()\n\n# The output directory for the fine-tuned model, $OUT_DIR.\nconfigs.output_dir = \"./MRPC/\"\n\n# The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.\nconfigs.data_dir = \"./glue_data/MRPC\"\n\n# The model name or path for the pre-trained model.\nconfigs.model_name_or_path = \"bert-base-uncased\"\n# The maximum length of an input sequence\nconfigs.max_seq_length = 128\n\n# Prepare GLUE task.\nconfigs.task_name = \"MRPC\".lower()\nconfigs.processor = processors[configs.task_name]()\nconfigs.output_mode = output_modes[configs.task_name]\nconfigs.label_list = configs.processor.get_labels()\nconfigs.model_type = \"bert\".lower()\nconfigs.do_lower_case = True\n\n# Set the device, batch size, topology, and caching flags.\nconfigs.device = \"cpu\"\nconfigs.per_gpu_eval_batch_size = 8\nconfigs.n_gpu = 0\nconfigs.local_rank = -1\nconfigs.overwrite_cache = False\n\n# Set random seed for reproducibility.\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42)\n\ntokenizer = BertTokenizer.from_pretrained(\n    configs.output_dir, do_lower_case=configs.do_lower_case)\n\nmodel = BertForSequenceClassification.from_pretrained(configs.output_dir, torchscript=True)\nmodel.to(configs.device)\n```\n\n----------------------------------------\n\nTITLE: Selecting Action without Batching in Agent (Python)\nDESCRIPTION: Implements the static `select_action` method for the `Agent`, designed for non-batched operation. It retrieves the local agent instance from the provided `agent_rref`, computes action probabilities using the policy network on the received state (assumed to be moved to CUDA), samples an action using `torch.distributions.Categorical`, stores the log probability of the action in a dictionary keyed by `ob_id`, and returns the sampled action as a Python integer.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributions import Categorical\n\nclass Agent:\n    ...\n\n    @staticmethod\n    def select_action(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        probs = self.policy(state.cuda()) # Assumes state needs moving to CUDA\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item()\n```\n\n----------------------------------------\n\nTITLE: Processing PyTorch Model Output for Top Class Predictions in Python\nDESCRIPTION: This Python code processes the output tensor from a PyTorch classification model. It applies the softmax function to get probabilities, enumerates them, sorts them in descending order based on probability, and prints the top 10 predictions along with their confidence percentages and corresponding class labels (from the `classes` list). It depends on the model's output tensor (`output`) and a list/dictionary of class names (`classes`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntop = list(enumerate(output[0].softmax(dim=0)))\ntop.sort(key=lambda x: x[1], reverse=True)\nfor idx, val in top[:10]:\n    print(f\"{val.item()*100:.2f}% {classes[idx]}\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Floating Point and Quantized Models - PyTorch - Python\nDESCRIPTION: Invokes model evaluation and timing for both the original (FP32) and quantized (INT8) models, comparing their F1 scores and runtime performance. Utilizes previously defined evaluation/timing utilities. Requires pre-trained traced and quantized models, configuration, and tokenizer; outputs are metrics and timing per model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntime_model_evaluation(traced_model, configs, tokenizer)\ntime_model_evaluation(quantized_model, configs, tokenizer)\n\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Registration\nDESCRIPTION: Registration of the custom operator in PyTorch's dispatcher system with schema definition.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY(extension_cpp, m) {\n   m.def(\"mymuladd(Tensor a, Tensor b, float c) -> Tensor\");\n}\n```\n\n----------------------------------------\n\nTITLE: Uploading Model Artifacts to GCS\nDESCRIPTION: Copies model artifacts to Google Cloud Storage bucket for centralized storage and tracking.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torchserve_vertexai_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nBUCKET_NAME = \"your-bucket-name-unique\"  # @param {type:\"string\"}\nBUCKET_URI = f\"gs://{BUCKET_NAME}/\"\n\n# Will copy the artifacts into the bucket\n!gsutil cp -r model_artifacts $BUCKET_URI\n```\n\n----------------------------------------\n\nTITLE: Running DDP MNIST Training and Displaying CUDA Timing - Bash\nDESCRIPTION: Shows the bash command to invoke DDP-based training for MNIST and outputs the CUDA event timing. Assumes DDP_mnist.py is configured for DDP as in previous snippets. Outputs elapsed CUDA event time to standard output. Requires an environment with DDP dependencies, multiple GPUs, and Python 3.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython DDP_mnist.py\n\nCUDA event elapsed time on training loop 39.77766015625sec\n```\n\n----------------------------------------\n\nTITLE: Configuring Sharding for Embedding and Output Layers in PyTorch Transformer - Python\nDESCRIPTION: This snippet shows configuring the sharding strategies for the embedding and final projection layers using parallelize_module. Row-wise and column-wise sharding are selectively applied, with Replicate used for input/output layouts as needed. This setup prepares the model for efficient distributed training; dependencies include PyTorch distributed tensor parallel classes such as RowwiseParallel and ColwiseParallel.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = parallelize_module(\n    model,\n    tp_mesh,\n    {\n        \"tok_embeddings\": RowwiseParallel(\n            input_layouts=Replicate(),\n        ),\n        \"output\": ColwiseParallel(\n            output_layouts=Replicate(),\n        ),\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Model Graph Post-Preparation - PyTorch FX - Python\nDESCRIPTION: Shows how to display the computational graph of a quantization-prepared PyTorch model. After preparation, print(prepared_model.graph) outputs the graph structure, useful for inspection and debugging. Input is the prepared FX module, and output is the textual graph view. Dependencies are minimal beyond the previous preparation step.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n    print(prepared_model.graph)\n```\n\n----------------------------------------\n\nTITLE: Installing TorchVision (Shell/Pip)\nDESCRIPTION: Command to install the `torchvision` library using pip. TorchVision provides access to popular datasets, model architectures, and common image transformations for computer vision, needed here to get a pretrained model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npip install torchvision\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Dynamic Quantization using `prepare_fx` and `convert_fx`\nDESCRIPTION: Demonstrates the workflow for post-training dynamic quantization using custom modules. `prepare_fx` is configured to skip the non-traceable module. `convert_fx` uses `convert_custom_config_dict` to map the original float module (`FP32NonTraceable`) directly to its dynamically quantized version (`DynamicQuantNonTraceable`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# The example is for post training quantization\nmodel_fp32.eval()\nmodel_prepared = prepare_fx(\n    model_fp32,\n    qconfig_mapping,\n    example_inputs,\n    prepare_custom_config_dict=prepare_custom_config_dict) # prepare_custom_config_dict defined in previous snippet\n\nconvert_custom_config_dict = {\n    \"observed_to_quantized_custom_module_class\": {\n        \"dynamic\": {\n            FP32NonTraceable: DynamicQuantNonTraceable,\n        }\n    },\n}\nmodel_quantized = convert_fx(\n    model_prepared,\n    convert_custom_config_dict)\n```\n\n----------------------------------------\n\nTITLE: Converting Calibrated Model to Quantized Model (Python)\nDESCRIPTION: After calibration, this code converts the prepared FX module to a quantized model using convert_pt2e. The resulting model has weights and activations quantized per the quantization settings selected earlier and is ready for backend deployment. Dependency: torch.ao.quantization.quantize_pt2e.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconverted_model = convert_pt2e(prepared_model)\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting TorchScript Graph with Custom Operator - Python (textual graph output)\nDESCRIPTION: Shows the TorchScript IR of the scripted compute function that includes a call to a registered custom operator (my_ops::warp_perspective). The IR illustrates constant creation, operator invocation, and the computational flow. This is inspection output obtained from compute.graph.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> compute.graph\ngraph(%x.1 : Dynamic\n    %y : Dynamic) {\n    %20 : int = prim::Constant[value=1]()\n    %16 : int[] = prim::Constant[value=[0, -1]]()\n    %14 : int = prim::Constant[value=6]()\n    %2 : int = prim::Constant[value=0]()\n    %7 : int = prim::Constant[value=42]()\n    %z.1 : int = prim::Constant[value=5]()\n    %z.2 : int = prim::Constant[value=10]()\n    %13 : int = prim::Constant[value=3]()\n    %4 : Dynamic = aten::select(%x.1, %2, %2)\n    %6 : Dynamic = aten::select(%4, %2, %2)\n    %8 : Dynamic = aten::eq(%6, %7)\n    %9 : bool = prim::TensorToBool(%8)\n    %z : int = prim::If(%9)\n      block0() {\n        -> (%z.1)\n      }\n      block1() {\n        -> (%z.2)\n      }\n    %17 : Dynamic = aten::eye(%13, %14, %2, %16)\n    %x : Dynamic = my_ops::warp_perspective(%x.1, %17)\n    %19 : Dynamic = aten::matmul(%x, %y)\n    %21 : Dynamic = aten::add(%19, %z, %20)\n    return (%21);\n  }\n```\n\n----------------------------------------\n\nTITLE: Pruning BERT Model and Evaluating Performance in PyTorch\nDESCRIPTION: This snippet demonstrates taking a pruning step, updating the mask, and evaluating the model's performance after pruning. It uses a sparsifier to apply pruning and computes metrics on a validation dataset.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nsparsifier.step()\nwith torch.autocast(\"cuda\"):\n    with torch.inference_mode():\n        predictions = trainer.predict(tokenized_squad_dataset[\"validation\"])\n    pruned = compute_metrics(\n        *predictions.predictions,\n        tokenized_squad_dataset[\"validation\"],\n        squad_dataset[\"validation\"],\n    )\nprint(\"pruned eval metrics:\", pruned)\n# pruned eval metrics: {'exact_match': 40.59602649006622, 'f1': 56.51610004515979}\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for Linking Custom Operator Subdirectory - CMake\nDESCRIPTION: A more advanced CMake configuration that adds the custom operator (warp_perspective) as a subdirectory, linking its shared library to the main inference executable. Includes the linker flag -Wl,--no-as-needed to ensure operator symbols are loaded. Prerequisites: existing main.cpp, operator CMake target in subdir, libtorch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(example_app)\n\nfind_package(Torch REQUIRED)\n\nadd_subdirectory(warp_perspective)\n\nadd_executable(example_app main.cpp)\ntarget_link_libraries(example_app \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(example_app -Wl,--no-as-needed warp_perspective)\ntarget_compile_features(example_app PRIVATE cxx_range_for)\n\n```\n\n----------------------------------------\n\nTITLE: Model Prediction Visualization Function\nDESCRIPTION: Implements a function to visualize model predictions on validation data with a configurable grid layout.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef visualize_model(model, rows=3, cols=3):\n  was_training = model.training\n  model.eval()\n  current_row = current_col = 0\n  fig, ax = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n\n  with torch.no_grad():\n    for idx, (imgs, lbls) in enumerate(dataloaders['val']):\n      imgs = imgs.cpu()\n      lbls = lbls.cpu()\n\n      outputs = model(imgs)\n      _, preds = torch.max(outputs, 1)\n\n      for jdx in range(imgs.size()[0]):\n        imshow(imgs.data[jdx], ax=ax[current_row, current_col])\n        ax[current_row, current_col].axis('off')\n        ax[current_row, current_col].set_title('predicted: {}'.format(class_names[preds[jdx]]))\n\n        current_col += 1\n```\n\n----------------------------------------\n\nTITLE: Tensor Dimensionality Operations in PyTorch\nDESCRIPTION: Operations for manipulating tensor dimensions in PyTorch, including reshaping, transposing, and concatenating. These functions are essential for preparing data and adjusting model outputs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nx.size()                              # return tuple-like object of dimensions\ntorch.cat(tensor_seq, dim=0)          # concatenates tensors along dim\nx.view(a,b,...)                       # reshapes x into size (a,b,...)\nx.view(-1,a)                          # reshapes x into size (b,a) for some b\nx.transpose(a,b)                      # swaps dimensions a and b\nx.permute(*dims)                      # permutes dimensions\nx.unsqueeze(dim)                      # tensor with added axis\nx.unsqueeze(dim=2)                    # (a,b,c) tensor -> (a,b,1,c) tensor\n```\n\n----------------------------------------\n\nTITLE: CPU Implementation of Multiply-Add Operation\nDESCRIPTION: C++ implementation of the multiply-add operation for CPU backend, including input validation and computation logic.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nat::Tensor mymuladd_cpu(at::Tensor a, const at::Tensor& b, double c) {\n  TORCH_CHECK(a.sizes() == b.sizes());\n  TORCH_CHECK(a.dtype() == at::kFloat);\n  TORCH_CHECK(b.dtype() == at::kFloat);\n  TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CPU);\n  TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CPU);\n  at::Tensor a_contig = a.contiguous();\n  at::Tensor b_contig = b.contiguous();\n  at::Tensor result = torch::empty(a_contig.sizes(), a_contig.options());\n  const float* a_ptr = a_contig.data_ptr<float>();\n  const float* b_ptr = b_contig.data_ptr<float>();\n  float* result_ptr = result.data_ptr<float>();\n  for (int64_t i = 0; i < result.numel(); i++) {\n    result_ptr[i] = a_ptr[i] * b_ptr[i] + c;\n  }\n  return result;\n}\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TCPStore Initialization with Libuv - PyTorch - Python\nDESCRIPTION: This snippet measures the initialization time of PyTorch's distributed TCPStore using environmental configuration variables. It logs timing data using the logging package and demonstrates store construction using torch.distributed.TCPStore. This is useful to evaluate store startup latency at scale, with environment variables passed in to set up rank, size, address, and port. Outputs are info log lines including the measured initialization duration. Dependencies: torch (PyTorch), logging, os.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TCPStore_libuv_backend.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport os\n\nfrom time import perf_counter\n\nimport torch\nimport torch.distributed as dist\n\nlogger: logging.Logger = logging.getLogger(__name__)\n\n# Env var are preset when launching the benchmark\nenv_rank = os.environ.get(\"RANK\", 0)\nenv_world_size = os.environ.get(\"WORLD_SIZE\", 1)\nenv_master_addr = os.environ.get(\"MASTER_ADDR\", \"localhost\")\nenv_master_port = os.environ.get(\"MASTER_PORT\", \"23456\")\n\nstart = perf_counter()\ntcp_store = dist.TCPStore(\n    env_master_addr,\n    int(env_master_port),\n    world_size=int(env_world_size),\n    is_master=(int(env_rank) == 0),\n)\nend = perf_counter()\ntime_elapsed = end - start\nlogger.info(\n    f\"Complete TCPStore init with rank={env_rank}, world_size={env_world_size} in {time_elapsed} seconds.\"\n)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Backend Header Class Definition in C++\nDESCRIPTION: Header file defining the BackendDummy class that inherits from PyTorch's Backend class. Includes declarations for allgather and allreduce operations along with the WorkDummy class for handling asynchronous operations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#include <torch/python.h>\n\n#include <torch/csrc/distributed/c10d/Backend.hpp>\n#include <torch/csrc/distributed/c10d/Work.hpp>\n#include <torch/csrc/distributed/c10d/Store.hpp>\n#include <torch/csrc/distributed/c10d/Types.hpp>\n#include <torch/csrc/distributed/c10d/Utils.hpp>\n\n#include <pybind11/chrono.h>\n\nnamespace c10d {\n\nclass BackendDummy : public Backend {\n  public:\n    BackendDummy(int rank, int size);\n\n    c10::intrusive_ptr<Work> allgather(\n        std::vector<std::vector<at::Tensor>>& outputTensors,\n        std::vector<at::Tensor>& inputTensors,\n        const AllgatherOptions& opts = AllgatherOptions()) override;\n\n    c10::intrusive_ptr<Work> allreduce(\n        std::vector<at::Tensor>& tensors,\n        const AllreduceOptions& opts = AllreduceOptions()) override;\n};\n\nclass WorkDummy : public Work {\n  public:\n    WorkDummy(\n      OpType opType,\n      c10::intrusive_ptr<c10::ivalue::Future> future)\n      : Work(-1, opType),\n      future_(std::move(future)) {}\n    bool isCompleted() override;\n    bool isSuccess() const override;\n    bool wait(std::chrono::milliseconds timeout = kUnsetTimeout) override;\n    virtual c10::intrusive_ptr<c10::ivalue::Future> getFuture() override;\n\n  private:\n    c10::intrusive_ptr<c10::ivalue::Future> future_;\n};\n} // namespace c10d\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Threads Without Core Pinning in Python\nDESCRIPTION: This Python code snippet demonstrates setting the global number of threads for PyTorch operations using `torch.set_num_threads`. The number of threads is calculated by dividing the total number of physical CPU cores (`num_physical_cores`) by the number of TorchServe workers (`num_workers`). This configuration is presented as an alternative for comparison against core pinning solutions and explicitly does not affinitize threads to specific cores, which can lead to performance issues like thread migration and inefficient NUMA access as detailed in the surrounding text. It requires the `torch` library and access to system/configuration parameters `num_physical_cores` and `num_workers`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchserve_with_ipex.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntorch.set_num_threads(num_physical_cores/num_workers)\n```\n\n----------------------------------------\n\nTITLE: Basic Dispatcher If-Statement Implementation\nDESCRIPTION: Example showing a basic implementation of dispatch logic using if-statements, demonstrating why the dispatcher system is more powerful than manual dispatch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/dispatcher.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nclass MyAddFunction : ... {\npublic:\n  static Tensor forward(\n    AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n\n    if (self.device().type() == DeviceType::CPU) {\n      return add_cpu(self, other);\n    } else if (self.device().type() == DeviceType::CUDA) {\n      return add_cuda(self, other);\n    } else {\n      TORCH_CHECK(0, \"Unsupported device \", self.device().type());\n    }\n  }\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Renaming PrivateUse1 Backend Identifier in PyTorch (C++)\nDESCRIPTION: Renames the internal 'PrivateUse1' backend identifier to a user-friendly custom name (e.g., 'npu') using the C++ API `c10::register_privateuse1_backend`. This provides the C++ interface for customizing the backend name, mirroring the Python functionality.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nc10::register_privateuse1_backend(\"npu\")\n```\n\n----------------------------------------\n\nTITLE: Registering AMP Support for PrivateUse1\nDESCRIPTION: Implementation of automatic mixed precision support for the new backend using AutocastPrivateUse1.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY_IMPL(aten, AutocastPrivateUse1, m) {\n  ...\n  KERNEL_PRIVATEUSEONE(<operator>, <policy>)\n  ...\n}\n\nTORCH_LIBRARY_IMPL(_, AutocastPrivateUse1, m) {\n  m.fallback(torch::CppFunction::makeFallthrough());\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing LLTM CUDA Forward Kernel in C++/CUDA\nDESCRIPTION: This snippet defines the CUDA kernel for the forward pass of the LLTM layer. It uses PackedTensorAccessor32 for efficient data access and implements the core LLTM computations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename scalar_t>\n__global__ void lltm_cuda_forward_kernel(\n    const torch::PackedTensorAccessor32<scalar_t,3,torch::RestrictPtrTraits> gates,\n    const torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> old_cell,\n    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> new_h,\n    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> new_cell,\n    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> input_gate,\n    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> output_gate,\n    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> candidate_cell) {\n  //batch index\n  const int n = blockIdx.y;\n  // column index\n  const int c = blockIdx.x * blockDim.x + threadIdx.x;\n  if (c < gates.size(2)){\n    input_gate[n][c] = sigmoid(gates[n][0][c]);\n    output_gate[n][c] = sigmoid(gates[n][1][c]);\n    candidate_cell[n][c] = elu(gates[n][2][c]);\n    new_cell[n][c] =\n        old_cell[n][c] + candidate_cell[n][c] * input_gate[n][c];\n    new_h[n][c] = tanh(new_cell[n][c]) * output_gate[n][c];\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Adam Optimizers for GAN in PyTorch C++\nDESCRIPTION: Creates Adam optimizers for both generator and discriminator networks with specific learning rates and beta parameters.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_23\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::optim::Adam generator_optimizer(\n    generator->parameters(), torch::optim::AdamOptions(2e-4).betas(std::make_tuple(0.5, 0.5)));\ntorch::optim::Adam discriminator_optimizer(\n    discriminator->parameters(), torch::optim::AdamOptions(5e-4).betas(std::make_tuple(0.5, 0.5)));\n```\n\n----------------------------------------\n\nTITLE: Using Custom Operator in TorchScript Scripted Function - Python\nDESCRIPTION: Loads a shared library containing a custom operator and defines a TorchScript function using the custom op inside scripted code. The function applies my_ops.warp_perspective to x (via torch.ops), then multiplies by y and adds a conditional constant. Requires PyTorch and the compiled custom op. Expects tensors as inputs; returns processed output.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntorch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z\n```\n\n----------------------------------------\n\nTITLE: Implementing Polynomial Fitting with PyTorch Autograd\nDESCRIPTION: A PyTorch implementation that uses autograd to automatically compute gradients. This example eliminates the need to manually implement the backward pass by leveraging PyTorch's automatic differentiation capabilities.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/pytorch_with_examples.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# -*- coding: utf-8 -*-\nimport torch\nimport math\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n\n# Create Tensors to hold input and outputs.\n# By default, requires_grad=False, which indicates that we do not need to\n# compute gradients with respect to these Tensors during the backward pass.\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Create random Tensors for weights. For a third order polynomial, we need\n# 4 weights: y = a + b * x + c * x^2 + d * x^3\n# Setting requires_grad=True indicates that we want to compute gradients with\n# respect to these Tensors during the backward pass.\na = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nb = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nc = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nd = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y using operations on Tensors.\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss using operations on Tensors.\n    # Now loss is a Tensor of shape (1,)\n    # loss.item() gets the scalar value held in the loss.\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Use autograd to compute the backward pass. This call will compute the\n    # gradient of loss with respect to all Tensors with requires_grad=True.\n    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n    # the gradient of the loss with respect to a, b, c, d respectively.\n    loss.backward()\n\n    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n    # because weights have requires_grad=True, but we don't need to track this\n    # in autograd.\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Fused and Non-Fused Models on Android Device\nDESCRIPTION: ADB commands to push the benchmark tool and model files to an Android device and run performance tests on both the fused and non-fused versions of the model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/fuse.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nadb push build_android/bin/speed_benchmark_torch /data/local/tmp\nadb push model.pt /data/local/tmp\nadb push model_fused.pt /data/local/tmp\nadb shell \"/data/local/tmp/speed_benchmark_torch --model=/data/local/tmp/model.pt\" --input_dims=\"1,3,224,224\" --input_type=\"float\"\nadb shell \"/data/local/tmp/speed_benchmark_torch --model=/data/local/tmp/model_fused.pt\" --input_dims=\"1,3,224,224\" --input_type=\"float\"\n```\n\n----------------------------------------\n\nTITLE: Setting USE_LIBUV Environment Variable for PyTorch Distributed TCPStore (Python)\nDESCRIPTION: This Python code snippet demonstrates how to set the 'USE_LIBUV' environment variable to '0' using the 'os' module before initializing a PyTorch distributed process group. Setting this variable instructs PyTorch to use the legacy TCPStore backend when `dist.init_process_group` is called with a TCP-based `init_method`. Dependencies include the 'os', 'torch', and 'torch.distributed' modules. The example initializes a single-process group and then destroys it.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TCPStore_libuv_backend.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed as dist\n\naddr = \"localhost\"\nport = 23456\nos.environ[\"USE_LIBUV\"] = \"0\"\ndist.init_process_group(\n    backend=\"cpu:gloo,cuda:nccl\",\n    rank=0,\n    world_size=1,\n    init_method=f\"tcp://{addr}:{port}\",\n)\ndist.destroy_process_group()\n```\n\n----------------------------------------\n\nTITLE: Running the TorchScript Inference Application (No Model) - Shell\nDESCRIPTION: Execution of the compiled C++ inference binary without providing the required model file shows the expected usage message. Useful for confirming correct argument handling in the app.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n$ ./example_app\nusage: example_app <path-to-exported-script-module>\n\n```\n\n----------------------------------------\n\nTITLE: Copying Vulkan Model to Android Test App Assets (Shell)\nDESCRIPTION: Shell command to copy the Vulkan-optimized model file (`mobilenet2-vulkan.pt`) into the assets directory of the PyTorch Android test application. This makes the model accessible within the test app.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\ncp mobilenet2-vulkan.pt $PYTORCH_ROOT/android/test_app/app/src/main/assets/\n```\n\n----------------------------------------\n\nTITLE: Extracting and Visualizing Features from WSIs using UMAP and TIAToolbox in Python\nDESCRIPTION: This code defines a UMAP reducer function, loads features extracted from a WSI, reduces them to 3D space, and visualizes them alongside patch-level predictions. It demonstrates feature extraction, dimensionality reduction, and visualization techniques for whole slide images.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# First we define a function to calculate the umap reduction\ndef umap_reducer(x, dims=3, nns=10):\n    \"\"\"UMAP reduction of the input data.\"\"\"\n    reducer = umap.UMAP(n_neighbors=nns, n_components=dims, metric=\"manhattan\", spread=0.5, random_state=2)\n    reduced = reducer.fit_transform(x)\n    reduced -= reduced.min(axis=0)\n    reduced /= reduced.max(axis=0)\n    return reduced\n\n# load the features output by our feature extractor\npos = np.load(global_save_dir / \"wsi_features\" / \"0.position.npy\")\nfeats = np.load(global_save_dir / \"wsi_features\" / \"0.features.0.npy\")\npos = pos / 8 # as we extracted at 0.5mpp, and we are overlaying on a thumbnail at 4mpp\n\n# reduce the features into 3 dimensional (rgb) space\nreduced = umap_reducer(feats)\n\n# plot the prediction map the classifier again\noverlay = overlay_prediction_mask(\n    wsi_overview,\n    pred_map,\n    alpha=0.5,\n    label_info=label_color_dict,\n    return_ax=True,\n)\n\n# plot the feature map reduction\nplt.figure()\nplt.imshow(wsi_overview)\nplt.scatter(pos[:,0], pos[:,1], c=reduced, s=1, alpha=0.5)\nplt.axis(\"off\")\nplt.title(\"UMAP reduction of HistoEnc features\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating BERT for Question Answering with PyTorch\nDESCRIPTION: Code for training a BERT model on the SQuAD dataset using the Hugging Face Trainer. It evaluates the model with different batch sizes, computes the F1 score, and measures inference time, using torch.autocast for half-precision (fp16) evaluation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntraining_args = transformers.TrainingArguments(\n    \"trainer\",\n    num_train_epochs=1,\n    lr_scheduler_type=\"constant\",\n    per_device_train_batch_size=64,\n    per_device_eval_batch_size=512,\n)\n\ntrainer = transformers.Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_squad_dataset[\"train\"],\n    eval_dataset=tokenized_squad_dataset[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\n\n# batch sizes to compare for eval\nbatch_sizes = [4, 16, 64, 256]\n# 2:4 sparsity require fp16, so we cast here for a fair comparison\nwith torch.autocast(\"cuda\"):\n    with torch.inference_mode():\n        predictions = trainer.predict(tokenized_squad_dataset[\"validation\"])\n    start_logits, end_logits = predictions.predictions\n    fp16_baseline = compute_metrics(\n        start_logits,\n        end_logits,\n        tokenized_squad_dataset[\"validation\"],\n        squad_dataset[\"validation\"],\n    )\n    fp16_time = measure_execution_time(\n        model,\n        batch_sizes,\n        tokenized_squad_dataset[\"validation\"],\n    )\nprint(\"fp16\", fp16_baseline)\nprint(\"cuda_fp16 time\", fp16_time)\n\n# fp16 {'exact_match': 78.53358561967833, 'f1': 86.9280493093186}\n# cuda_fp16 time {4: 10.927572380751371, 16: 19.607915310189128, 64: 73.18846387788653, 256: 286.91255673766136}\n```\n\n----------------------------------------\n\nTITLE: Python Testing Script for Custom Class\nDESCRIPTION: Python script demonstrating how to load and use the custom C++ class from Python and TorchScript.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\ntorch.classes.load_library('build/libcustom_class.so')\ns = torch.classes.MyStackClass('foo')\ns.push('bar')\ns.push('baz')\nprint(s.pop())\nprint(s.pop())\nprint(s.pop())\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for PyTorch C++ Project\nDESCRIPTION: CMake build configuration file that sets up the project dependencies and compiler settings for using LibTorch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(dcgan)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(dcgan dcgan.cpp)\ntarget_link_libraries(dcgan \"${TORCH_LIBRARIES}\")\nset_property(TARGET dcgan PROPERTY CXX_STANDARD 14)\n```\n\n----------------------------------------\n\nTITLE: Reference Implementation: Q/DQ Quantized Linear Operator in PyTorch - Python\nDESCRIPTION: Implements a quantized linear operator using dequantize, computation in fp32, and re-quantization for Q/DQ pattern. Dependencies include PyTorch and quantized_decomposed ops. Key parameters are quantized inputs, quantization scales/zero points, weight, bias, and output quantization specs. Returns the quantized tensor result.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef quantized_linear(x_int8, x_scale, x_zero_point, weight_int8, weight_scale, weight_zero_point, bias_fp32, output_scale, output_zero_point):\n   x_fp32 = torch.ops.quantized_decomposed.dequantize_per_tensor(\n            x_i8, x_scale, x_zero_point, x_quant_min, x_quant_max, torch.int8)\n   weight_fp32 = torch.ops.quantized_decomposed.dequantize_per_tensor(\n            weight_i8, weight_scale, weight_zero_point, weight_quant_min, weight_quant_max, torch.int8)\n   weight_permuted = torch.ops.aten.permute_copy.default(weight_fp32, [1, 0]);\n   out_fp32 = torch.ops.aten.addmm.default(bias_fp32, x_fp32, weight_permuted)\n   out_i8 = torch.ops.quantized_decomposed.quantize_per_tensor(\n   out_fp32, out_scale, out_zero_point, out_quant_min, out_quant_max, torch.int8)\n   return out_i8\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning and Squashing Mask in Pruned BERT Model\nDESCRIPTION: This code snippet shows how to fine-tune a pruned BERT model and then squash the mask to fuse it with the weights. It results in a zeroed-out 2:4 dense model, demonstrating the effect of pruning on model weights.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrainer.train()\nsparsifier.squash_mask()\ntorch.set_printoptions(edgeitems=4)\nprint(model.bert.encoder.layer[0].intermediate.dense.weight)\n\n# Parameter containing:\n# tensor([[ 0.0000, -0.0237,  0.0000,  0.0130,  ..., -0.0462, -0.0000, 0.0000, -0.0272],\n#        [ 0.0436, -0.0000, -0.0000,  0.0492,  ..., -0.0000,  0.0844,  0.0340, -0.0000],\n#        [-0.0302, -0.0350,  0.0000,  0.0000,  ...,  0.0303,  0.0175, -0.0000,  0.0000],\n#        [ 0.0000, -0.0000, -0.0529,  0.0327,  ...,  0.0213,  0.0000, -0.0000,  0.0735],\n#        ...,\n#        [ 0.0000, -0.0000, -0.0258, -0.0239,  ..., -0.0000, -0.0000,  0.0380,  0.0562],\n#        [-0.0432, -0.0000,  0.0000, -0.0598,  ...,  0.0000, -0.0000,  0.0262  -0.0227],\n#        [ 0.0244,  0.0921, -0.0000, -0.0000,  ..., -0.0000, -0.0784,  0.0000,  0.0761],\n#        [ 0.0000,  0.0225, -0.0395, -0.0000,  ..., -0.0000,  0.0684, -0.0344, -0.0000]], device='cuda:0', requires_grad=True)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Validation Data for QA Task - Transformers/HuggingFace - Python\nDESCRIPTION: Defines a helper function for preprocessing validation examples using the HuggingFace tokenizer. It tokenizes questions and context, controls truncation and padding, and manages offset mappings and sample mapping for correct association between input examples and model outputs. The main parameters include a batch of examples and a tokenizer instance. The function returns the processed dictionary inputs ready for model consumption, handling mapping between input spans and dataset examples.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess_validation_function(examples, tokenizer):\n    inputs = tokenizer(\n        [q.strip() for q in examples[\"question\"]],\n        examples[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs\n\n```\n\n----------------------------------------\n\nTITLE: Converting Checkpoint Formats Programmatically in Python\nDESCRIPTION: This Python snippet shows how to convert checkpoint formats programmatically using the dcp_to_torch_save and torch_save_to_dcp functions from the format_utils module.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed.checkpoint as DCP\nfrom torch.distributed.checkpoint.format_utils import dcp_to_torch_save, torch_save_to_dcp\n\nCHECKPOINT_DIR = \"checkpoint\"\nTORCH_SAVE_CHECKPOINT_DIR = \"torch_save_checkpoint.pth\"\n\n# convert dcp model to torch.save (assumes checkpoint was generated as above)\ndcp_to_torch_save(CHECKPOINT_DIR, TORCH_SAVE_CHECKPOINT_DIR)\n\n# converts the torch.save model back to DCP\ndcp_to_torch_save(TORCH_SAVE_CHECKPOINT_DIR, f\"{CHECKPOINT_DIR}_new\")\n```\n\n----------------------------------------\n\nTITLE: FSDP Training Setup and Imports\nDESCRIPTION: Imports required packages for FSDP training including PyTorch core libraries, transformers, and distributed training utilities.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, GPT2TokenizerFast\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport functools\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n checkpoint_wrapper,\n CheckpointImpl,\n apply_activation_checkpointing_wrapper)\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    BackwardPrefetch,\n    ShardingStrategy,\n    FullStateDictConfig,\n    StateDictType,\n)\nfrom torch.distributed.fsdp.wrap import (\n    transformer_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom summarization_dataset import *\nfrom transformers.models.t5.modeling_t5 import T5Block\nfrom typing import Type\nimport time\nimport tqdm\nfrom datetime import datetime\n```\n\n----------------------------------------\n\nTITLE: Distributed Training Process Management\nDESCRIPTION: Helper functions for initializing and cleaning up distributed training processes using NCCL backend.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef setup():\n    # initialize the process group\n    dist.init_process_group(\"nccl\")\n\ndef cleanup():\n    dist.destroy_process_group()\n```\n\n----------------------------------------\n\nTITLE: Installing Intel Neural Compressor\nDESCRIPTION: Commands for installing Intel Neural Compressor from pip or conda. Supports Python versions 3.6, 3.7, 3.8, and 3.9.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# install stable version from pip\npip install neural-compressor\n\n# install nightly version from pip\npip install -i https://test.pypi.org/simple/ neural-compressor\n\n# install stable version from from conda\nconda install neural-compressor -c conda-forge -c intel\n```\n\n----------------------------------------\n\nTITLE: Redirecting via HTML Meta Refresh - HTML\nDESCRIPTION: This snippet embeds an HTML meta tag using reStructuredText's raw directive to perform an immediate redirect to a new tutorial URL. There are no external dependencies beyond standard HTML rendering in documentation systems that support raw HTML insertion. It expects to be used in environments like Sphinx documentation where such directives are processed, and the meta refresh will cause the browser to navigate instantly to the specified link. No parameters or inputs are necessary, but using raw HTML may not work if the documentation builder disallows raw content.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchrec_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n.. raw:: html\n\n   <meta http-equiv=\\\"Refresh\\\" content=\\\"0; url='https://pytorch.org/tutorials/intermediate/torchrec_intro_tutorial.html'\\\" />\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch and Distributed Modules - Python\nDESCRIPTION: This short snippet demonstrates importing the necessary Python modules for distributed training using PyTorch. The 'torch' and 'torch.distributed' modules are imported as well as the standard 'os' module, setting up the basic environment needed to configure distributed jobs and utilize Flight Recorder features.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed as dist\nimport os\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom LLTM Module in Python with C++ Backend\nDESCRIPTION: Defines `LLTMFunction`, a custom `torch.autograd.Function`, to wrap C++ forward and backward implementations (`lltm_cpp.forward`, `lltm_cpp.backward`). It also defines `LLTM`, a `torch.nn.Module`, which uses `LLTMFunction` for its forward pass and manages parameters (weights, bias). Requires the `lltm_cpp` Python module (built from C++ source) and PyTorch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport torch\n\n# Our module!\nimport lltm_cpp\n\nclass LLTMFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, weights, bias, old_h, old_cell):\n        outputs = lltm_cpp.forward(input, weights, bias, old_h, old_cell)\n        new_h, new_cell = outputs[:2]\n        variables = outputs[1:] + [weights]\n        ctx.save_for_backward(*variables)\n\n        return new_h, new_cell\n\n    @staticmethod\n    def backward(ctx, grad_h, grad_cell):\n        outputs = lltm_cpp.backward(\n            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)\n        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs\n        return d_input, d_weights, d_bias, d_old_h, d_old_cell\n\n\nclass LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        return LLTMFunction.apply(input, self.weights, self.bias, *state)\n```\n\n----------------------------------------\n\nTITLE: Implementing QHM Parameter Update Function for TorchScript\nDESCRIPTION: Defines a function that contains the core update logic for Quasi-Hyperbolic Momentum (QHM) optimization. This function is separated from the optimizer class to make it TorchScript-friendly, allowing for better performance and multithreaded execution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_optim_torchscript.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import Tensor\nfrom typing import List\n\n\ndef qhm_update(params: List[Tensor],\n            dp_list: List[Tensor],\n            momentum_buffer_list: List[Tensor],\n            lr: float,\n            nu: float,\n            weight_decay: float,\n            weight_decay_type: str,\n            momentum: float):\n\n    for p, d_p, momentum_buffer in zip(params, dp_list, momentum_buffer_list):\n        if weight_decay != 0:\n            if weight_decay_type == \"grad\":\n                d_p.add_(weight_decay, p)\n            elif weight_decay_type == \"direct\":\n                p.mul_(1.0 - lr * weight_decay)\n            else:\n                raise ValueError(\"Invalid weight decay type provided\")\n\n        momentum_buffer.mul_(momentum).add_(1.0 - momentum, d_p)\n\n        p.data.add_(-lr * nu, momentum_buffer)\n        p.data.add_(-lr * (1.0 - nu), d_p)\n```\n\n----------------------------------------\n\nTITLE: Defining a Single Epoch Training Function in PyTorch\nDESCRIPTION: This snippet defines the `train_one_epoch` function in Python using PyTorch. It handles the training loop for a single epoch, including data loading, forward pass, loss calculation, backpropagation, optimizer step, and accuracy metric updates using `AverageMeter` and `accuracy` helpers. It takes the model, loss criterion, optimizer, data loader, device, and number of batches as input.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):  \n    model.train() \n    top1 = AverageMeter('Acc@1', ':6.2f') \n    top5 = AverageMeter('Acc@5', ':6.2f') \n    avgloss = AverageMeter('Loss', '1.5f')  \n\n    cnt = 0 \n    for image, target in data_loader: \n        start_time = time.time()  \n        print('.', end = '')  \n        cnt += 1  \n        image, target = image.to(device), target.to(device) \n        output = model(image) \n        loss = criterion(output, target)  \n        optimizer.zero_grad() \n        loss.backward() \n        optimizer.step()  \n        acc1, acc5 = accuracy(output, target, topk=(1, 5))  \n        top1.update(acc1[0], image.size(0)) \n        top5.update(acc5[0], image.size(0)) \n        avgloss.update(loss, image.size(0)) \n        if cnt >= ntrain_batches: \n            print('Loss', avgloss.avg)  \n\n            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}' \n                  .format(top1=top1, top5=top5))  \n            return  \n\n    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}' \n          .format(top1=top1, top5=top5))  \n    return\n```\n\n----------------------------------------\n\nTITLE: Defining Policy Network and Parsing Arguments in Python\nDESCRIPTION: Defines the `Policy` neural network model using `torch.nn.Module` for the CartPole agent. It includes layers for processing state inputs and outputting action probabilities using softmax. The constructor accepts a `batch` argument to configure the softmax dimension for handling single or batched states. The snippet also includes initial imports, command-line argument parsing using `argparse` for configuration (gamma, seed, episodes), and setting the PyTorch random seed.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nparser = argparse.ArgumentParser(description='PyTorch RPC Batch RL example')\nparser.add_argument('--gamma', type=float, default=1.0, metavar='G',\n                    help='discount factor (default: 1.0)')\nparser.add_argument('--seed', type=int, default=543, metavar='N',\n                    help='random seed (default: 543)')\nparser.add_argument('--num-episode', type=int, default=10, metavar='E',\n                    help='number of episodes (default: 10)')\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\n\nclass Policy(nn.Module):\n    def __init__(self, batch=True):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n        self.dim = 2 if batch else 1\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=self.dim)\n```\n\n----------------------------------------\n\nTITLE: Implementing LLTM CUDA Backward Kernel in C++/CUDA\nDESCRIPTION: This snippet defines the CUDA kernel for the backward pass of the LLTM layer. It computes gradients for various components of the LLTM using PackedTensorAccessor32 for efficient data access.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\ntemplate <typename scalar_t>\n__global__ void lltm_cuda_backward_kernel(\n    torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> d_old_cell,\n    torch::PackedTensorAccessor32<scalar_t,3,torch::RestrictPtrTraits> d_gates,\n    const torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> grad_h,\n    const torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> grad_cell,\n    const torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> new_cell,\n    const torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> input_gate,\n    const torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> output_gate,\n    const torch::PackedTensorAccessor32<scalar_t,2,torch::RestrictPtrTraits> candidate_cell,\n    const torch::PackedTensorAccessor32<scalar_t,3,torch::RestrictPtrTraits> gate_weights) {\n  //batch index\n  const int n = blockIdx.y;\n  // column index\n  const int c = blockIdx.x * blockDim.x + threadIdx.x;\n  if (c < d_gates.size(2)){\n    const auto d_output_gate = tanh(new_cell[n][c]) * grad_h[n][c];\n    const auto d_tanh_new_cell = output_gate[n][c] * grad_h[n][c];\n    const auto d_new_cell =\n        d_tanh(new_cell[n][c]) * d_tanh_new_cell + grad_cell[n][c];\n\n\n    d_old_cell[n][c] = d_new_cell;\n    const auto d_candidate_cell = input_gate[n][c] * d_new_cell;\n    const auto d_input_gate = candidate_cell[n][c] * d_new_cell;\n\n    d_gates[n][0][c] =\n        d_input_gate * d_sigmoid(gate_weights[n][0][c]);\n    d_gates[n][1][c] =\n        d_output_gate * d_sigmoid(gate_weights[n][1][c]);\n    d_gates[n][2][c] =\n        d_candidate_cell * d_elu(gate_weights[n][2][c]);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules and Configuring Logging - PyTorch - Python\nDESCRIPTION: Imports modules necessary for model loading, data processing, and evaluation, including NumPy, PyTorch, transformers, and tqdm. Logging is configured for warning-level output, helping reduce unnecessary messages and focus on important information. Prints PyTorch version to confirm installation. Prerequisites: Requires all listed Python packages to be installed (especially transformers >=4.x and PyTorch).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport numpy as np\nimport os\nimport random\nimport sys\nimport time\nimport torch\n\nfrom argparse import Namespace\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom tqdm import tqdm\nfrom transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\nfrom transformers import glue_compute_metrics as compute_metrics\nfrom transformers import glue_output_modes as output_modes\nfrom transformers import glue_processors as processors\nfrom transformers import glue_convert_examples_to_features as convert_examples_to_features\n\n# Setup logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.WARN)\n\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(\n   logging.WARN)  # Reduce logging\n\nprint(torch.__version__)\n\n```\n\n----------------------------------------\n\nTITLE: Preparing Model for Quantization using prepare_pt2e (Python)\nDESCRIPTION: The code prepares an exported FX graph model for post-training quantization using prepare_pt2e, folding BatchNorm layers and inserting quantization observers. The quantizer instance must be previously configured with the desired quantization settings. Outputs a prepared model requiring calibration. Dependencies: torch.ao.quantization.quantize_pt2e.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprepared_model = prepare_pt2e(exported_model, quantizer)\n\n```\n\n----------------------------------------\n\nTITLE: Defining RST Card for TorchServe Vertex AI Tutorial\nDESCRIPTION: This RST snippet defines a card description for a tutorial on deploying models in Vertex AI using TorchServe. It includes metadata such as description, image, link, and tags.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/recipes_index.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:card_description: Learn how to deploy model in Vertex AI with TorchServe\n:image: ../_static/img/thumbnails/cropped/generic-pytorch-logo.png\n:link: ../recipes/torchserve_vertexai_tutorial.html\n:tags: Production\n```\n\n----------------------------------------\n\nTITLE: Configuring Whole-Slide Inference with IOPatchPredictorConfig - TIAToolbox - Python\nDESCRIPTION: This snippet initializes an IOPatchPredictorConfig object to configure patch extraction and input resolution for whole-slide inference in TIAToolbox. Parameters include input_resolutions (a list of dictionaries specifying units and resolution), patch_input_shape (patch size as [height, width]), and stride_shape (step size between patches, allowing for overlapping or non-overlapping extraction). Required dependency: tiatoolbox. Inputs are configuration parameters describing WSI reading, and the output is a configuration object for use with PatchPredictor's predict() in 'wsi' mode.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwsi_ioconfig = IOPatchPredictorConfig(\n    input_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n    patch_input_shape=[224, 224],\n    stride_shape=[224, 224],\n)\n```\n\n----------------------------------------\n\nTITLE: BLIP Export Error Fix\nDESCRIPTION: Solution for the 'Cannot mutate tensors with frozen storage' error when exporting BLIP model by cloning the tensor before mutation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_export_challenges_solutions.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntext.input_ids = text.input_ids.clone() # clone the tensor\ntext.input_ids[:,0] = self.tokenizer.bos_token_id\n```\n\n----------------------------------------\n\nTITLE: Overlaying Prediction Maps on Whole-Slide Images with TIAToolbox\nDESCRIPTION: Visualizes patch-level predictions by overlaying them on the WSI thumbnail. The code sets up a color mapping for different tissue classes, merges predictions at the specified resolution, and creates an overlay of predictions on the WSI image.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Visualization of whole-slide image patch-level prediction\n# first set up a label to color mapping\nlabel_color_dict = {}\nlabel_color_dict[0] = (\"empty\", (0, 0, 0))\ncolors = cm.get_cmap(\"Set1\").colors\nfor class_name, label in label_dict.items():\n    label_color_dict[label + 1] = (class_name, 255 * np.array(colors[label]))\n\npred_map = predictor.merge_predictions(\n    wsi_path,\n    wsi_output[0],\n    resolution=overview_resolution,\n    units=overview_unit,\n)\noverlay = overlay_prediction_mask(\n    wsi_overview,\n    pred_map,\n    alpha=0.5,\n    label_info=label_color_dict,\n    return_ax=True,\n)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Setting up PyTorch C++/CUDA Extension using setuptools\nDESCRIPTION: This snippet shows how to set up a PyTorch C++/CUDA extension using setuptools. It defines the extension module and specifies the source files for compilation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='lltm',\n    ext_modules=[\n        CUDAExtension('lltm_cuda', [\n            'lltm_cuda.cpp',\n            'lltm_cuda_kernel.cu',\n        ])\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })\n```\n\n----------------------------------------\n\nTITLE: Running MNIST Training in PyTorch C++ Frontend\nDESCRIPTION: Shell command for executing the compiled MNIST example binary to train the model. The output shows the training progress with loss values and accuracy metrics for multiple epochs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ ./mnist\nTrain Epoch: 1 [59584/60000] Loss: 0.4232\nTest set: Average loss: 0.1989 | Accuracy: 0.940\nTrain Epoch: 2 [59584/60000] Loss: 0.1926\nTest set: Average loss: 0.1338 | Accuracy: 0.959\nTrain Epoch: 3 [59584/60000] Loss: 0.1390\nTest set: Average loss: 0.0997 | Accuracy: 0.969\nTrain Epoch: 4 [59584/60000] Loss: 0.1239\nTest set: Average loss: 0.0875 | Accuracy: 0.972\n...\n```\n\n----------------------------------------\n\nTITLE: Trainer Setup Configuration\nDESCRIPTION: Configures the trainer processes with hybrid model initialization, parameter collection, and distributed optimizer setup. Manages both local and remote parameters through RRefs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/rpc_ddp_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _setup_trainer(rank, remote_emb_module):\n    model = HybridModel(remote_emb_module, rank)\n    # Get RRefs of embedding table\n    remote_params = remote_emb_module.remote_parameters()\n    # Create RRefs for local parameters\n    local_params = [RRef(p) for p in model.fc.parameters()]\n    # Combine all RRefs\n    model_params = remote_params + local_params\n    # Create distributed optimizer\n    opt = DistributedOptimizer(\n        torch.optim.SGD,\n        model_params,\n        lr=0.05\n    )\n    criterion = torch.nn.CrossEntropyLoss()\n    return model, opt, criterion\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantization-Aware Training in YAML\nDESCRIPTION: YAML configuration file for quantization-aware training with Intel Neural Compressor. Specifies the model framework, quantization approach, evaluation metric, and accuracy criterion for auto-tuning.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# conf.yaml\nmodel:\n    name: LeNet\n    framework: pytorch_fx\n\nquantization:\n    approach: quant_aware_training\n\nevaluation:\n    accuracy:\n        metric:\n            topk: 1\n\ntuning:\n    accuracy_criterion:\n        relative: 0.01\n```\n\n----------------------------------------\n\nTITLE: Simplified Process Group Initialization with torchrun\nDESCRIPTION: Demonstrates the simplified process group initialization using torchrun compared to manual setup.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ddp_series_fault_tolerance.rst#2025-04-22_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n- def ddp_setup(rank, world_size):\n+ def ddp_setup():\n-     \"\"\"\n-     Args:\n-         rank: Unique identifier of each process\n-         world_size: Total number of processes\n-     \"\"\"\n-     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n-     os.environ[\"MASTER_PORT\"] = \"12355\"\n-     init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n+     init_process_group(backend=\"nccl\")\n     torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and Intel GPU Backend Dependencies (bash)\nDESCRIPTION: This command installs the required PyTorch stack and Triton backend for Intel GPUs from the official index. It is a prerequisite for running all subsequent Python code in the tutorial. Dependencies installed include torch, torchvision, torchaudio, and pytorch-triton-xpu.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install torch torchvision torchaudio pytorch-triton-xpu --index-url https://download.pytorch.org/whl/xpu\n```\n\n----------------------------------------\n\nTITLE: Implementing warpPerspective Custom Operator in C++\nDESCRIPTION: Core implementation of the warpPerspective operator that performs perspective transformation on images using OpenCV. Handles tensor conversion between PyTorch and OpenCV formats with zero-copy operations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {\n  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data_ptr<float>());\n\n  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data_ptr<float>());\n\n  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8});\n\n  torch::Tensor output = torch::from_blob(output_mat.ptr<float>(), /*sizes=*/{8, 8});\n  return output.clone();\n}\n```\n\n----------------------------------------\n\nTITLE: Python Extension Registration and Creation\nDESCRIPTION: Additional header and implementation code for exposing the backend to Python through pybind11. Includes backend registration and creation methods.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nclass BackendDummy : public Backend {\n    static c10::intrusive_ptr<Backend> createBackendDummy(\n        const c10::intrusive_ptr<::c10d::Store>& store,\n        int rank,\n        int size,\n        const std::chrono::duration<float>& timeout);\n\n    static void BackendDummyConstructor() __attribute__((constructor)) {\n        py::object module = py::module::import(\"torch.distributed\");\n        py::object register_backend =\n            module.attr(\"Backend\").attr(\"register_backend\");\n        register_backend(\"dummy\", py::cpp_function(createBackendDummy));\n    }\n}\n```\n\nLANGUAGE: C++\nCODE:\n```\nc10::intrusive_ptr<Backend> BackendDummy::createBackendDummy(\n        const c10::intrusive_ptr<::c10d::Store>& /* unused */,\n        int rank,\n        int size,\n        const std::chrono::duration<float>& /* unused */) {\n    return c10::make_intrusive<BackendDummy>(rank, size);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"createBackendDummy\", &BackendDummy::createBackendDummy);\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Duration Differences in PyTorch Traces using TraceDiff\nDESCRIPTION: Shows how to visualize the top 10 operators with the largest duration changes between traces, excluding ProfilerStep entries to avoid overshadowing other operators.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_trace_diff_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = compare_traces_output.sort_values(by=\"diff_duration\", ascending=False)\n# The duration differerence can be overshadowed by the \"ProfilerStep\",\n# so we can filter it out to show the trend of other operators.\ndf = df.loc[~df.index.str.startswith(\"ProfilerStep\")].head(10)\nTraceDiff.visualize_duration_diff(df)\n```\n\n----------------------------------------\n\nTITLE: Downloading the GLUE Dataset Using Command Line Script (Shell)\nDESCRIPTION: Executes a shell command to download the GLUE dataset for the MRPC task using a provided Python script. Requires that 'download_glue_data.py' is present in the working directory, as well as Python and internet access. The command outputs data to 'glue_data' and is intended to be run before running downstream tasks. No code output generated; instead, files are downloaded and extracted.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython download_glue_data.py --data_dir='glue_data' --tasks='MRPC'\n```\n\n----------------------------------------\n\nTITLE: Using torch.sum in PyTorch\nDESCRIPTION: Example of referencing a built-in PyTorch tensor operator. This shows the syntax for referring to a function in the torch namespace.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/custom_ops_landing_page.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntorch.sum\n```\n\n----------------------------------------\n\nTITLE: Running FSDP MNIST Training Script and Displaying CUDA Event Timing - Bash\nDESCRIPTION: Demonstrates the command-line invocation to train the MNIST model with FSDP and display elapsed CUDA timing. Assumes FSDP_mnist.py is in the working directory and configured as in previous snippets. Outputs elapsed CUDA training time to the console. Expects Python 3 and all necessary Python prerequisites to be installed.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython FSDP_mnist.py\n\nCUDA event elapsed time on training loop 40.67462890625sec\n```\n\n----------------------------------------\n\nTITLE: List of Supported Vulkan Operators (Text)\nDESCRIPTION: A list of PyTorch operators supported by the Vulkan backend at the time the document was written. Execution will fail if a model contains operators not present in this list when run on the Vulkan backend. This list enables certain torchvision models like image classifiers.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n_adaptive_avg_pool2d\n_cat\nadd.Scalar\nadd.Tensor\nadd_.Tensor\naddmm\navg_pool2d\nclamp\nconvolution\nempty.memory_format\nempty_strided\nhardtanh_\nmax_pool2d\nmean.dim\nmm\nmul.Scalar\nrelu_\nreshape\nselect.int\nslice.Tensor\ntranspose.int\ntranspose_\nunsqueeze\nupsample_nearest2d\nview\n```\n\n----------------------------------------\n\nTITLE: Preparing a ResNet-18 Model for Quantization-Aware Training (QAT) in PyTorch\nDESCRIPTION: Loads a pre-trained, non-quantized ResNet-18 model (`quantize=False`). Fuses Conv-BN-ReLU modules using `model.fuse_model()` for better quantization compatibility. Creates the combined model with the custom head using the previously defined `create_combined_model` function. Sets the quantization configuration (`qconfig`) for the feature extractor part (`model_ft[0]`) to the default QAT configuration. Finally, uses `torch.quantization.prepare_qat` to insert fake quantization modules into the model, preparing it for training that simulates quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n  # notice `quantize=False`\n  model = models.resnet18(pretrained=True, progress=True, quantize=False)\n  num_ftrs = model.fc.in_features\n\n  # Step 1\n  model.train()\n  model.fuse_model()\n  # Step 2\n  model_ft = create_combined_model(model)\n  model_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n  # Step 3\n  model_ft = torch.quantization.prepare_qat(model_ft, inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Loading PyTorch C++/CUDA Extension using JIT\nDESCRIPTION: This snippet demonstrates how to load a PyTorch C++/CUDA extension using Just-In-Time (JIT) compilation. It's a simpler alternative to the setuptools method.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.utils.cpp_extension import load\n\nlltm = load(name='lltm', sources=['lltm_cuda.cpp', 'lltm_cuda_kernel.cu'])\n```\n\n----------------------------------------\n\nTITLE: Creating and Manipulating Tensors\nDESCRIPTION: Methods for tensor creation and manipulation in PyTorch. These functions allow creating tensors with different initialization methods and controlling autograd behavior.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nx = torch.randn(*size)              # tensor with independent N(0,1) entries\nx = torch.[ones|zeros](*size)       # tensor with all 1's [or 0's]\nx = torch.tensor(L)                 # create tensor from [nested] list or ndarray L\ny = x.clone()                       # clone of x\nwith torch.no_grad():               # code wrap that stops autograd from tracking tensor history\nrequires_grad=True                  # arg, when set to True, tracks computation \n                                    # history for future derivative calculations\n```\n\n----------------------------------------\n\nTITLE: Referencing C++ Custom Operator Example in PyTorch\nDESCRIPTION: Example of how to reference the C++ custom operator tutorial in PyTorch documentation. This shows the syntax for referencing other documentation pages.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/custom_ops_landing_page.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:ref:`cpp-custom-ops-tutorial`\n```\n\n----------------------------------------\n\nTITLE: Exporting BLIP Model with torch.export\nDESCRIPTION: Demonstrates how to export a BLIP model for image captioning with batch_size=1. Sets up model configuration, loads pretrained weights, and exports using torch.export.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_export_challenges_solutions.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom models.blip import blip_decoder\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimage_size = 384\nimage = torch.randn(1, 3,384,384).to(device)\ncaption_input = \"\"\n\nmodel_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\nmodel = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\nmodel.eval()\nmodel = model.to(device)\n\nexported_program: torch.export.ExportedProgram= torch.export.export(model, args=(image,caption_input,), strict=False)\n```\n\n----------------------------------------\n\nTITLE: Flight Recorder Dump File Structure - JSON\nDESCRIPTION: This example provides the structure of an unpickled Flight Recorder diagnostic file in JSON format. It illustrates the fields for version, process group configuration, status, and a list of recorded entries each embedding stack traces, operation metadata, and state information. This structure is for users and developers to understand what data is collected and how it can be parsed for downstream analysis.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"version\": \"2.5\",\n  \"pg_config\": {\n    \"0\": {\n    \"name\": \"0\",\n    \"desc\": \"default_pg\",\n    \"ranks\": \"[0, 1]\"\n    }\n  },\n  \"pg_status\": {\n    \"0\": {\n    \"last_enqueued_collective\": 2,\n    \"last_started_collective\": -1,\n    \"last_completed_collective\": 2\n    }\n  },\n  \"entries\": [\n  {\n    \"frames\": [\n    {\n    \"name\": \"test_short_pickle\",\n    \"filename\": \"pytorch/test/distributed/test_c10d_nccl.py\",\n    \"line\": 3647\n    },\n    {\n    \"name\": \"spawn_main\",\n    \"filename\": \".conda/envs/pytorch-3.10/lib/python3.10/multiprocessing/spawn.py\",\n    \"line\": 116\n    },\n    {\n    \"name\": \"<module>\",\n    \"filename\": \"<string>\",\n    \"line\": 1\n    }\n    ],\n    \"record_id\": 0,\n    \"pg_id\": 0,\n    \"process_group\": (\"0\", \"default_pg\"),\n    \"collective_seq_id\": 1,\n    \"p2p_seq_id\": 0,\n    \"op_id\": 1,\n    \"profiling_name\": \"nccl:all_reduce\",\n    \"time_created_ns\": 1724779239936775119,\n    \"input_sizes\": [[3, 4]],\n    \"input_dtypes\": [\"Float\"],\n    \"output_sizes\": [[3, 4]],\n    \"output_dtypes\": [\"Float\"],\n    \"state\": \"completed\",\n    \"time_discovered_started_ns\": null,\n    \"time_discovered_completed_ns\": 1724779239975811724,\n    \"retired\": true,\n    \"timeout_ms\": 600000,\n    \"is_p2p\": false\n    },\n    ...\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Backend Methods in C++\nDESCRIPTION: Implementation file containing the dummy allgather and allreduce operations that set output tensors to zero. This demonstrates the basic structure for implementing collective operations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n#include \"dummy.hpp\"\n\nnamespace c10d {\n\nc10::intrusive_ptr<Work> BackendDummy::allgather(\n        std::vector<std::vector<at::Tensor>>& outputTensors,\n        std::vector<at::Tensor>& inputTensors,\n        const AllgatherOptions& /* unused */) {\n    for (auto& outputTensorVec : outputTensors) {\n        for (auto& outputTensor : outputTensorVec) {\n            outputTensor.zero_();\n        }\n    }\n\n    auto future = c10::make_intrusive<c10::ivalue::Future>(\n        c10::ListType::create(c10::ListType::create(c10::TensorType::get())));\n    future->markCompleted(c10::IValue(outputTensors));\n    return c10::make_intrusive<WorkDummy>(OpType::ALLGATHER, std::move(future));\n}\n\nc10::intrusive_ptr<Work> BackendDummy::allreduce(\n        std::vector<at::Tensor>& tensors,\n        const AllreduceOptions& opts) {\n    for (auto& tensor : tensors) {\n        tensor.zero_();\n    }\n\n    auto future = c10::make_intrusive<c10::ivalue::Future>(\n        c10::ListType::create(c10::TensorType::get()));\n    future->markCompleted(c10::IValue(tensors));\n    return c10::make_intrusive<WorkDummy>(OpType::ALLGATHER, std::move(future));\n}\n} // namespace c10d\n```\n\n----------------------------------------\n\nTITLE: Quantization with Default QConfigMapping (No Quantization Due to Backend Constraints) - Python\nDESCRIPTION: This snippet illustrates what happens when the default QConfigMapping is used with PyTorch FX quantization. Because the default mapping does not meet the required dtype constraints of the specified backend, quantization is skipped and layers remain in their floating-point (unquantized) or fused forms. Dependencies are similar to the first snippet (PyTorch FX quantization modules). Inputs and outputs are the same model architecture, but without any quantization effects.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nexample_inputs = (torch.rand(1, 3, 10, 10, dtype=torch.float),)\nmodel = MyModel(use_bn=True)\nprepared = prepare_fx(model, get_default_qconfig_mapping(), example_inputs, backend_config=backend_config)\nprepared(*example_inputs)  # calibrate\nconverted = convert_fx(prepared, backend_config=backend_config)\n```\n\n----------------------------------------\n\nTITLE: HTML Navigation Structure for PyTorch Tutorials\nDESCRIPTION: HTML markup defining the navigation and container structure for displaying PyTorch tutorial cards. Includes dropdown filter menu and card container setup.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/recipes_index.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n</div>\n    </div>\n\n    <div id=\"tutorial-cards-container\">\n\n    <nav class=\"navbar navbar-expand-lg navbar-light tutorials-nav col-12\">\n        <div class=\"tutorial-tags-container\">\n            <div id=\"dropdown-filter-tags\">\n                <div class=\"tutorial-filter-menu\">\n                    <div class=\"tutorial-filter filter-btn all-tag-selected\" data-tag=\"all\">All</div>\n                </div>\n            </div>\n        </div>\n    </nav>\n\n    <hr class=\"tutorials-hr\">\n\n    <div class=\"row\">\n\n    <div id=\"tutorial-cards\">\n    <div class=\"list\">\n```\n\n----------------------------------------\n\nTITLE: Generating GPU kernel breakdown analysis\nDESCRIPTION: Code to analyze the time spent in different types of kernels (compute, communication, memory) across all ranks. Returns two dataframes with kernel statistics and metrics.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nanalyzer = TraceAnalysis(trace_dir = \"/path/to/trace/folder\")\nkernel_type_metrics_df, kernel_metrics_df = analyzer.get_gpu_kernel_breakdown()\n```\n\n----------------------------------------\n\nTITLE: Implementing Meta Refresh Redirection in HTML\nDESCRIPTION: This snippet implements a client-side redirection using the HTML meta refresh tag. It automatically redirects the visitor to the PyTorch tutorials beginner section for YouTube series after loading the page.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/introyt.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"0; url='https://pytorch.org/tutorials/beginner/introyt/introyt_index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Building and Running the PyTorch C++ DCGAN Example\nDESCRIPTION: Contains shell commands demonstrating how to build the DCGAN C++ example using `make` and then run the resulting executable (`./dcgan`). The included sample output verifies that the program runs and successfully loads data batches from the MNIST dataset, printing batch sizes and labels as defined in the iteration loop. Requires `make`, a C++ toolchain, and a configured build system (likely CMake).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\n  root@fa350df05ecf:/home/build# make\n  Scanning dependencies of target dcgan\n  [ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n  [100%] Linking CXX executable dcgan\n  [100%] Built target dcgan\n  root@fa350df05ecf:/home/build# make\n  [100%] Built target dcgan\n  root@fa350df05ecf:/home/build# ./dcgan\n  Batch size: 64 | Labels: 5 2 6 7 2 1 6 7 0 1 6 2 3 6 9 1 8 4 0 6 5 3 3 0 4 6 6 6 4 0 8 6 0 6 9 2 4 0 2 8 6 3 3 2 9 2 0 1 4 2 3 4 8 2 9 9 3 5 8 0 0 7 9 9\n  Batch size: 64 | Labels: 2 2 4 7 1 2 8 8 6 9 0 2 2 9 3 6 1 3 8 0 4 4 8 8 8 9 2 6 4 7 1 5 0 9 7 5 4 3 5 4 1 2 8 0 7 1 9 6 1 6 5 3 4 4 1 2 3 2 3 5 0 1 6 2\n  Batch size: 64 | Labels: 4 5 4 2 1 4 8 3 8 3 6 1 5 4 3 6 2 2 5 1 3 1 5 0 8 2 1 5 3 2 4 4 5 9 7 2 8 9 2 0 6 7 4 3 8 3 5 8 8 3 0 5 8 0 8 7 8 5 5 6 1 7 8 0\n  Batch size: 64 | Labels: 3 3 7 1 4 1 6 1 0 3 6 4 0 2 5 4 0 4 2 8 1 9 6 5 1 6 3 2 8 9 2 3 8 7 4 5 9 6 0 8 3 0 0 6 4 8 2 5 4 1 8 3 7 8 0 0 8 9 6 7 2 1 4 7\n  Batch size: 64 | Labels: 3 0 5 5 9 8 3 9 8 9 5 9 5 0 4 1 2 7 7 2 0 0 5 4 8 7 7 6 1 0 7 9 3 0 6 3 2 6 2 7 6 3 3 4 0 5 8 8 9 1 9 2 1 9 4 4 9 2 4 6 2 9 4 0\n  Batch size: 64 | Labels: 9 6 7 5 3 5 9 0 8 6 6 7 8 2 1 9 8 8 1 1 8 2 0 7 1 4 1 6 7 5 1 7 7 4 0 3 2 9 0 6 6 3 4 4 8 1 2 8 6 9 2 0 3 1 2 8 5 6 4 8 5 8 6 2\n  Batch size: 64 | Labels: 9 3 0 3 6 5 1 8 6 0 1 9 9 1 6 1 7 7 4 4 4 7 8 8 6 7 8 2 6 0 4 6 8 2 5 3 9 8 4 0 9 9 3 7 0 5 8 2 4 5 6 2 8 2 5 3 7 1 9 1 8 2 2 7\n  Batch size: 64 | Labels: 9 1 9 2 7 2 6 0 8 6 8 7 7 4 8 6 1 1 6 8 5 7 9 1 3 2 0 5 1 7 3 1 6 1 0 8 6 0 8 1 0 5 4 9 3 8 5 8 4 8 0 1 2 6 2 4 2 7 7 3 7 4 5 3\n  Batch size: 64 | Labels: 8 8 3 1 8 6 4 2 9 5 8 0 2 8 6 6 7 0 9 8 3 8 7 1 6 6 2 7 7 4 5 5 2 1 7 9 5 4 9 1 0 3 1 9 3 9 8 8 5 3 7 5 3 6 8 9 4 2 0 1 2 5 4 7\n  Batch size: 64 | Labels: 9 2 7 0 8 4 4 2 7 5 0 0 6 2 0 5 9 5 9 8 8 9 3 5 7 5 4 7 3 0 5 7 6 5 7 1 6 2 8 7 6 3 2 6 5 6 1 2 7 7 0 0 5 9 0 0 9 1 7 8 3 2 9 4\n  Batch size: 64 | Labels: 7 6 5 7 7 5 2 2 4 9 9 4 8 7 4 8 9 4 5 7 1 2 6 9 8 5 1 2 3 6 7 8 1 1 3 9 8 7 9 5 0 8 5 1 8 7 2 6 5 1 2 0 9 7 4 0 9 0 4 6 0 0 8 6\n  ...\n```\n\n----------------------------------------\n\nTITLE: Running QAT Example with Inductor Freezing Enabled\nDESCRIPTION: Example command to run the Quantization-Aware Training example with the Inductor freezing feature enabled. This is necessary since the freezing feature is not enabled by default in PyTorch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_FREEZING=1 python example_x86inductorquantizer_qat.py\n```\n\n----------------------------------------\n\nTITLE: Verifying PyTorch Installation (Python)\nDESCRIPTION: A simple Python script to import the `torch` library and print its version number. This is used to verify that PyTorch has been successfully built and installed.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nprint(torch.__version__)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Backend Kernels in C++\nDESCRIPTION: Example showing how to register custom operator implementations for a new backend using TORCH_LIBRARY_IMPL.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/extend_dispatcher.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(<schema_my_op1>, &my_op1);\n  m.impl(<schema_my_op2>, &my_op2);\n  m.impl(<schema_my_op2_backward>, &my_op2_backward);\n}\n```\n\n----------------------------------------\n\nTITLE: Folder Layouts for LibTorch and Example Application (Shell)\nDESCRIPTION: These shell code listings present typical directory layouts for a LibTorch installation and an example C++ application project. They aid in understanding file/folder placement and are useful for configuring build systems or referencing header/library locations. The structure distinguishes between library files and the application source.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_export.rst#2025-04-22_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nlibtorch/\\n  bin/\\n  include/\\n  lib/\\n  share/\n```\n\nLANGUAGE: sh\nCODE:\n```\nexample-app/\\n  CMakeLists.txt\\n  example-app.cpp\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Model to Evaluation Mode\nDESCRIPTION: This snippet sets the PyTorch model intended for quantization (`model_to_quantize`) to evaluation mode using the `.eval()` method. This is a necessary step before performing post-training quantization as it disables layers like dropout and uses batch normalization's running statistics.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel_to_quantize.eval()\n```\n\n----------------------------------------\n\nTITLE: Configuring setuptools for PyTorch C++ Extension\nDESCRIPTION: Setup script to build a custom C++ PyTorch operator using setuptools. Creates a warp_perspective operator that depends on OpenCV libraries and disables Python ABI suffix in the output filename.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name=\"warp_perspective\",\n    ext_modules=[\n        CppExtension(\n            \"warp_perspective\",\n            [\"example_app/warp_perspective/op.cpp\"],\n            libraries=[\"opencv_core\", \"opencv_imgproc\"],\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension.with_options(no_python_abi_suffix=True)},\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Running the PyTorch RPC Parameter Server in Python\nDESCRIPTION: Defines global variables (`param_server`, `global_lock`) and functions (`get_parameter_server`, `run_parameter_server`) to initialize and run a singleton parameter server using PyTorch RPC. `get_parameter_server` ensures only one instance of the `ParameterServer` class (assumed to be defined elsewhere) is created across all processes using a `Lock`. `run_parameter_server` initializes RPC for the server role (rank 0) and waits for all trainer workers to complete using `rpc.shutdown()`. This server process primarily hosts the model and responds to RPC requests from trainers.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# The global parameter server instance.\nparam_server = None\n# A lock to ensure we only have one parameter server.\nglobal_lock = Lock()\n\n\ndef get_parameter_server(num_gpus=0):\n    \"\"\"\n    Returns a singleton parameter server to all trainer processes\n    \"\"\"\n    global param_server\n    # Ensure that we get only one handle to the ParameterServer.\n    with global_lock:\n        if not param_server:\n            # construct it once\n            param_server = ParameterServer(num_gpus=num_gpus)\n        return param_server\n\ndef run_parameter_server(rank, world_size):\n    # The parameter server just acts as a host for the model and responds to\n    # requests from trainers.\n    # rpc.shutdown() will wait for all workers to complete by default, which\n    # in this case means that the parameter server will wait for all trainers\n    # to complete, and then exit.\n    print(\"PS master initializing RPC\")\n    rpc.init_rpc(name=\"parameter_server\", rank=rank, world_size=world_size)\n    print(\"RPC initialized! Running parameter server...\")\n    rpc.shutdown()\n    print(\"RPC shutdown on parameter server.\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Final Quantized and Trained Model in PyTorch\nDESCRIPTION: Calls the `visualize_model` function (assumed defined elsewhere) to display the performance or predictions of the final, converted quantized model (`model_quantized_and_trained`). Uses `plt.ioff()`, `plt.tight_layout()`, and `plt.show()` to manage and display the resulting plot.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nvisualize_model(model_quantized_and_trained)\n\nplt.ioff()\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Referencing Python Custom Operator Example in PyTorch\nDESCRIPTION: Example of how to reference the Python custom operator tutorial in PyTorch documentation. This shows the syntax for referencing other documentation pages.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/custom_ops_landing_page.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:ref:`python-custom-ops-tutorial`\n```\n\n----------------------------------------\n\nTITLE: Defining RST Table of Contents for PyTorch Tutorials\nDESCRIPTION: This RST snippet defines a hidden table of contents for various PyTorch tutorials and recipes, covering topics such as neural networks, model saving/loading, profiling, quantization, and deployment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/recipes_index.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :hidden:\n\n   /recipes/recipes/defining_a_neural_network\n   /recipes/torch_logs\n   /recipes/recipes/what_is_state_dict\n   /recipes/recipes/saving_and_loading_models_for_inference\n   /recipes/recipes/saving_and_loading_a_general_checkpoint\n   /recipes/recipes/saving_multiple_models_in_one_file\n   /recipes/recipes/warmstarting_model_using_parameters_from_a_different_model\n   /recipes/recipes/save_load_across_devices\n   /recipes/recipes/zeroing_out_gradients\n   /recipes/recipes/profiler_recipe\n   /recipes/recipes/profile_with_itt\n   /recipes/recipes/Captum_Recipe\n   /recipes/recipes/tensorboard_with_pytorch\n   /recipes/recipes/dynamic_quantization\n   /recipes/recipes/amp_recipe\n   /recipes/recipes/tuning_guide\n   /recipes/recipes/xeon_run_cpu\n   /recipes/recipes/intel_extension_for_pytorch\n   /recipes/compiling_optimizer\n   /recipes/torch_compile_backend_ipex\n   /recipes/torchscript_inference\n   /recipes/deployment_with_flask\n   /recipes/distributed_rpc_profiling\n   /recipes/zero_redundancy_optimizer\n   /recipes/cuda_rpc\n   /recipes/distributed_optim_torchscript\n   /recipes/mobile_interpreter\n   /recipes/distributed_comm_debug_mode\n   /recipes/torch_export_challenges_solutions\n```\n\n----------------------------------------\n\nTITLE: Visualizing Computation Graph with Torchviz in PyTorch - Python\nDESCRIPTION: This snippet demonstrates generating a computation graph visualization for custom autograd operations using torchviz. After running Cube.apply on tensor x, it computes the gradient with create_graph=True, capturing higher order derivatives. torchviz.make_dot visualizes the graph, using a mapping of named nodes to the values for clarity. Dependencies include torch, torchviz, and a custom Cube autograd Function. Inputs: tensor x, Cube.apply function. Outputs: a rendered computation graph image. Limitations: Cube must be defined and torchviz installed; suitable for inspecting autograd graph structure.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nout = Cube.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})\n```\n\n----------------------------------------\n\nTITLE: Defining C++ Interface for a Mixed C++/CUDA PyTorch Extension\nDESCRIPTION: Shows the C++ source file (`lltm_cuda.cpp`) for a mixed C++/CUDA PyTorch extension. It includes the PyTorch C++ extension API (`torch/extension.h`), declares forward and backward functions implemented in CUDA (`lltm_cuda_forward`, `lltm_cuda_backward`), and defines a C++ wrapper function (`lltm_forward`) that performs input checks (ensuring tensors are CUDA tensors and contiguous) using `TORCH_CHECK` macros before potentially calling the actual CUDA implementations. Requires the PyTorch C++ API headers.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/extension.h>\n\n#include <vector>\n\n// CUDA forward declarations\n\nstd::vector<torch::Tensor> lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell);\n\nstd::vector<torch::Tensor> lltm_cuda_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights);\n\n// C++ interface\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n\nstd::vector<torch::Tensor> lltm_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  CHECK_INPUT(input);\n  CHECK_INPUT(weights);\n  CHECK_INPUT(bias);\n  CHECK_INPUT(old_h);\n  CHECK_INPUT(old_cell);\n\n```\n\n----------------------------------------\n\nTITLE: Waiting for Model Inference Futures with torch.jit.wait in Python\nDESCRIPTION: This snippet waits for the completion of multiple asynchronous tasks (futures) representing model inferences, collects their results, and computes the sum of their outputs along a specified dimension. It requires torch (PyTorch) and relies on torch.jit.wait to resolve the futures. The input is expected to be a list of futures, and the output is a single tensor representing the summed results. Limitations include the presumption that all futures will return compatible tensors for stacking and summation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch-script-parallelism.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresults = [torch.jit.wait(fut) for fut in futures]\nreturn torch.stack(results).sum(dim=0)\n```\n\n----------------------------------------\n\nTITLE: Basic Tensor Operations with Autograd in C++\nDESCRIPTION: Creates tensors with gradient tracking and performs basic operations. Shows how to create tensors that require gradients and perform operations that build the computational graph.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_autograd.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nauto x = torch::ones({2, 2}, torch::requires_grad());\nstd::cout << x << std::endl;\n```\n\n----------------------------------------\n\nTITLE: Verifying PyTorch Installation (Shell)\nDESCRIPTION: Checks if PyTorch was installed correctly by executing a simple Python command inline. This command imports the `torch` library and prints its version.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ python -c \"import torch; print(torch.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Setting MASTER_ADDR from SLURM nodelist for torchrun Jobs - Bash\nDESCRIPTION: Bash script to set the MASTER_ADDR environment variable using SLURM's node list utilities, ensuring a consistent rendezvous address for all distributed jobs in a cluster. Intended for use in SLURM managed clusters when launching torchrun scripts. Assumes scontrol and SLURM_NODELIST are available.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/ddp_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport MASTER_ADDR=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)\n```\n\n----------------------------------------\n\nTITLE: Defining a Compute Function in Vanilla PyTorch - Python\nDESCRIPTION: Defines a basic compute function that adds a constant (5 or 10) to the matrix multiplication of two tensors depending on whether the top-left entry of x equals 42. No dependencies outside basic PyTorch are required. It expects tensor inputs x and y, directly returning the modified result.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z\n```\n\n----------------------------------------\n\nTITLE: Implementing Learning Rate Scheduling\nDESCRIPTION: Learning rate schedulers available in PyTorch's optim.lr_scheduler module, used for adjusting the learning rate during training. These include step-based, periodic, and adaptive schedulers.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nscheduler = optim.X(optimizer,...)      # create lr scheduler\nscheduler.step()                        # update lr after optimizer updates weights\noptim.lr_scheduler.X                    # where X is LambdaLR, MultiplicativeLR,\n                                        # StepLR, MultiStepLR, ExponentialLR,\n                                        # CosineAnnealingLR, ReduceLROnPlateau, CyclicLR,\n                                        # OneCycleLR, CosineAnnealingWarmRestarts,\n```\n\n----------------------------------------\n\nTITLE: Creating Vertex AI Model\nDESCRIPTION: Initializes Vertex AI and uploads the model to Model Registry using the PyTorch pre-built container and model artifacts from GCS.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torchserve_vertexai_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom google.cloud import aiplatform as vertexai\nPYTORCH_PREDICTION_IMAGE_URI = (\n    \"us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.1-12:latest\"\n)\nMODEL_DISPLAY_NAME = \"stable_diffusion_1_5-unique\"\nMODEL_DESCRIPTION = \"stable_diffusion_1_5 container\"\n\nvertexai.init(project='your_project', location='us-central1', staging_bucket=BUCKET_NAME)\n\nmodel = aiplatform.Model.upload(\n    display_name=MODEL_DISPLAY_NAME,\n    description=MODEL_DESCRIPTION,\n    serving_container_image_uri=PYTORCH_PREDICTION_IMAGE_URI,\n    artifact_uri=BUCKET_URI,\n)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Dependencies\nDESCRIPTION: Commands for installing the nightly build of PyTorch and torchvision, including options for CPU and CUDA support.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install numpy\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n```\n\n----------------------------------------\n\nTITLE: Calculating and Comparing PyTorch Model Sizes\nDESCRIPTION: Defines a function `print_size_of_model` that saves the model's `state_dict` to a temporary file, gets its size using `os.path.getsize`, prints the size in MB, and then deletes the temporary file. This function is then called on both the original (`model`) and the dynamically quantized (`quantized_model`) BERT models to demonstrate the size reduction achieved through quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dynamic_quantization_bert_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)\n```\n\n----------------------------------------\n\nTITLE: Using DeviceMesh for Custom Parallel Solutions in PyTorch\nDESCRIPTION: This code demonstrates how to slice child meshes from a parent mesh for complex custom parallel compositions. It shows initialization of a 3D mesh and accessing different dimensions for various parallel strategies.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_device_mesh.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.device_mesh import init_device_mesh\nmesh_3d = init_device_mesh(\"cuda\", (2, 2, 2), mesh_dim_names=(\"replicate\", \"shard\", \"tp\"))\n\n# Users can slice child meshes from the parent mesh.\nhsdp_mesh = mesh_3d[\"replicate\", \"shard\"]\ntp_mesh = mesh_3d[\"tp\"]\n\n# Users can access the underlying process group thru `get_group` API.\nreplicate_group = hsdp_mesh[\"replicate\"].get_group()\nshard_group = hsdp_mesh[\"shard\"].get_group()\ntp_group = tp_mesh.get_group()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Distributed Training in PyTorch\nDESCRIPTION: Imports for distributed training in PyTorch, including the distributed communication package and multiprocessing for memory sharing. These tools enable efficient training across multiple GPUs or machines.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch.distributed as dist          # distributed communication\nfrom multiprocessing import Process       # memory sharing processes\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Update Parameter Server in PyTorch\nDESCRIPTION: Implementation of a parameter server class that handles batch updates from multiple trainers. Uses @rpc.functions.async_execution decorator to manage asynchronous gradient updates and model distribution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport threading\nimport torchvision\nimport torch\nimport torch.distributed.rpc as rpc\nfrom torch import optim\n\nnum_classes, batch_update_size = 30, 5\n\nclass BatchUpdateParameterServer(object):\n    def __init__(self, batch_update_size=batch_update_size):\n        self.model = torchvision.models.resnet50(num_classes=num_classes)\n        self.lock = threading.Lock()\n        self.future_model = torch.futures.Future()\n        self.batch_update_size = batch_update_size\n        self.curr_update_size = 0\n        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n        for p in self.model.parameters():\n            p.grad = torch.zeros_like(p)\n\n    def get_model(self):\n        return self.model\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def update_and_fetch_model(ps_rref, grads):\n        self = ps_rref.local_value()\n        with self.lock:\n            self.curr_update_size += 1\n            for p, g in zip(self.model.parameters(), grads):\n                p.grad += g\n            fut = self.future_model\n            if self.curr_update_size >= self.batch_update_size:\n                for p in self.model.parameters():\n                    p.grad /= self.batch_update_size\n                self.curr_update_size = 0\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                fut.set_result(self.model)\n                self.future_model = torch.futures.Future()\n        return fut\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for MNIST with PyTorch\nDESCRIPTION: This CMake script sets up a C++ project for MNIST digit recognition using PyTorch. It configures the project, finds required packages, optionally downloads the MNIST dataset, and sets up the build process with necessary libraries and compiler features.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\nproject(mnist)\nset(CMAKE_CXX_STANDARD 17)\n\nfind_package(Torch REQUIRED)\nfind_package(Threads REQUIRED)\n\noption(DOWNLOAD_MNIST \"Download the MNIST dataset from the internet\" ON)\nif (DOWNLOAD_MNIST)\n  message(STATUS \"Downloading MNIST dataset\")\n  execute_process(\n    COMMAND python ${CMAKE_CURRENT_LIST_DIR}/../tools/download_mnist.py\n      -d ${CMAKE_BINARY_DIR}/data\n    ERROR_VARIABLE DOWNLOAD_ERROR)\n  if (DOWNLOAD_ERROR)\n    message(FATAL_ERROR \"Error downloading MNIST dataset: ${DOWNLOAD_ERROR}\")\n  endif()\nendif()\n\nadd_executable(mnist mnist.cpp)\ntarget_compile_features(mnist PUBLIC cxx_range_for)\ntarget_link_libraries(mnist ${TORCH_LIBRARIES} ${CMAKE_THREAD_LIBS_INIT})\n\nif (MSVC)\n  file(GLOB TORCH_DLLS \"${TORCH_INSTALL_PREFIX}/lib/*.dll\")\n  add_custom_command(TARGET mnist\n                     POST_BUILD\n                     COMMAND ${CMAKE_COMMAND} -E copy_if_different\n                     ${TORCH_DLLS}\n                     $<TARGET_FILE_DIR:mnist>)\nendif (MSVC)\n```\n\n----------------------------------------\n\nTITLE: Example Output of CommDebugMode with MLPModule\nDESCRIPTION: This shows sample output from CommDebugMode when applied to an MLPModule at noise level 0. It displays the collective operation counts at module level, showing where operations like all_reduce occur in the forward pass of the model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_comm_debug_mode.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nExpected Output:\n    Global\n      FORWARD PASS\n        *c10d_functional.all_reduce: 1\n        MLPModule\n          FORWARD PASS\n            *c10d_functional.all_reduce: 1\n            MLPModule.net1\n            MLPModule.relu\n            MLPModule.net2\n              FORWARD PASS\n                *c10d_functional.all_reduce: 1\n```\n\n----------------------------------------\n\nTITLE: Running Environment Collection Script for Bug Reports\nDESCRIPTION: Command to collect environment information when submitting a bug report for PyTorch tutorials.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m torch.utils.collect_env\n```\n\n----------------------------------------\n\nTITLE: Custom Operator Registration - C++\nDESCRIPTION: Registration of a custom operator within a TORCH_LIBRARY block that uses the custom C++ class.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes.rst#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nBEGIN def_free\nEND def_free\n```\n\n----------------------------------------\n\nTITLE: Python Setup Configuration\nDESCRIPTION: Python setup script for building the C++ extension using PyTorch's cpp_extension module.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/process_group_cpp_extension_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# file name: setup.py\nimport os\n```\n\n----------------------------------------\n\nTITLE: Inline JIT Compilation of a Custom TorchScript Operator\nDESCRIPTION: Python code that defines a custom operator's C++ source inline and JIT compiles it using PyTorch's C++ extension utilities, making it available in Python without external files.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.utils.cpp_extension\n\nop_source = \"\"\"\n#include <opencv2/opencv.hpp>\n#include <torch/script.h>\n\ntorch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {\n  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data<float>());\n  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data<float>());\n\n  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64});\n\n  torch::Tensor output =\n    torch::from_blob(output_mat.ptr<float>(), /*sizes=*/{64, 64});\n  return output.clone();\n}\n\nTORCH_LIBRARY(my_ops, m) {\n  m.def(\"warp_perspective\", &warp_perspective);\n}\n\"\"\"\n\ntorch.utils.cpp_extension.load_inline(\n    name=\"warp_perspective\",\n    cpp_sources=op_source,\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True,\n)\n\nprint(torch.ops.my_ops.warp_perspective)\n```\n\n----------------------------------------\n\nTITLE: Registering Submodules and Parameterized Module Constructors in PyTorch C++\nDESCRIPTION: This snippet describes how to create a module with submodules, registering them using register_module in the constructor's initializer list. Dependencies are torch::nn::Module and torch::nn::Linear (or other modules to be registered). It enables parameter sharing and automatic management of module parameters for composite modules.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\n  struct Net : torch::nn::Module {\n    Net(int64_t N, int64_t M)\n      : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n    { }\n    torch::nn::Linear linear;\n  };\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Using Bundled Inputs - PyTorch - Python\nDESCRIPTION: Shows how to retrieve all bundled inputs from a model using get_all_bundled_inputs, and how to run the model's forward method with these instrumented inputs. Returns the output of the model when called with the first bundled input. Make sure the bundled_model interface provides get_all_bundled_inputs().\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/bundled_inputs.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsample_inputs = bundled_model.get_all_bundled_inputs()\\n\\nprint(bundled_model(*sample_inputs[0]))\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch Android AARs with Vulkan (Shell - Test App Context)\nDESCRIPTION: Shell command to build the PyTorch Android AAR files (`pytorch_android`) with Vulkan support enabled, specifically in the context of preparing dependencies for the test application. Requires being in the PyTorch root directory (`PYTORCH_ROOT`) and setting `USE_VULKAN=1`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ncd $PYTORCH_ROOT\nUSE_VULKAN=1 sh ./scripts/build_pytorch_android.sh\n```\n\n----------------------------------------\n\nTITLE: Serializing a TorchScript Module in Python\nDESCRIPTION: This snippet illustrates how to serialize a traced ScriptModule or a scripted module (instance of ScriptModule) to disk using the save method. Requires a ScriptModule object. The filename must be a valid file path ending with .pt. The serialized file can be loaded later in C++ using LibTorch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_export.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntraced_script_module.save(\"traced_resnet_model.pt\")\n```\n\nLANGUAGE: python\nCODE:\n```\nsm.save(\"my_module_model.pt\")\n```\n\n----------------------------------------\n\nTITLE: Launching Parameter Server and Trainer Processes for PyTorch RPC\nDESCRIPTION: Initializes and starts processes for either a parameter server (rank 0) or trainer nodes. For trainers, it sets up MNIST data loaders. Uses multiprocessing to manage subprocesses.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprocesses = []\nworld_size = args.world_size\nif args.rank == 0:\n    p = mp.Process(target=run_parameter_server, args=(0, world_size))\n    p.start()\n    processes.append(p)\nelse:\n    # Get data to train on\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=32, shuffle=True,)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            '../data',\n            train=False,\n            transform=transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,))\n                        ])),\n        batch_size=32,\n        shuffle=True,\n    )\n    # start training worker on this node\n    p = mp.Process(\n        target=run_worker,\n        args=(\n            args.rank,\n            world_size, args.num_gpus,\n            train_loader,\n            test_loader))\n    p.start()\n    processes.append(p)\n\nfor p in processes:\n    p.join()\n```\n\n----------------------------------------\n\nTITLE: Custom Inflatable Arguments and Functions for Complex Model Inputs - PyTorch - Python\nDESCRIPTION: Illustrates the creation of custom inflatable arguments with user-defined inflation functions (fmt_fn). The code includes both simple tensor condensation for storage and a generator function creating an InflatableArg for optionally-mapped dictionary tensor inputs. This approach is useful for compressing large or complex input signatures (e.g., Optional[Dict[str, Tensor]]). All tensor values are replaced with random values upon inflation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/bundled_inputs.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsample = dict(\\n    a=torch.zeros([10, 20]),\\n    b=torch.zeros([1, 1]),\\n    c=torch.zeros([10, 20]),\\n)\\n\\ndef condensed(t):\\n    ret = torch.empty_like(t).flatten()[0].clone().expand(t.shape)\\n    assert ret.storage().size() == 1\\n    return ret\\n\\n# An example of how to create an inflatable arg for a complex model input like Optional[Dict[str, Tensor]]\\n# here we take in a normal input, deflate it, and define an inflater function that converts the mapped tensors to random values\\ndef bundle_optional_dict_of_randn(template: Optional[Dict[str, Tensor]]):\\n    return torch.utils.bundled_inputs.InflatableArg(\\n        value=(\\n            None\\n            if template is None\\n            else {k: condensed(v) for (k, v) in template.items()}\\n        ),\\n        fmt=\"{}\",\\n        fmt_fn=\"\"\"\\n        def {}(self, value: Optional[Dict[str, Tensor]]):\\n            if value is not None:\\n                output = {}\\n                for k, v in value.items():\\n                    output[k] = torch.randn_like(v)\\n                return output\\n            else:\\n                return None\\n        \"\"\",\\n    )\\n\\nsample_inputs = (\\n    bundle_optional_dict_of_randn(sample),\\n)\n```\n\n----------------------------------------\n\nTITLE: Tracking Module Import State to Prevent Circular Imports (Python)\nDESCRIPTION: Implements a flag in habana_frameworks/__init__.py to track if the backend has been loaded, preventing circular imports during autoloading. The __autoload function checks a global flag and only proceeds with the import if the module is not already loaded. This is required for extensions that might import themselves recursively due to the autoload mechanism. It requires standard Python functionality and the habana_frameworks.torch submodule to be importable.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/python_extension_autoload.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nis_loaded = False  # A member variable of habana_frameworks module to track if our module has been imported\n\ndef __autoload():\n    # This is an entrypoint for pytorch autoload mechanism\n    # If the following condition is true, that means our backend has already been loaded, either explicitly\n    # or by the autoload mechanism and importing it again should be skipped to avoid circular imports\n    global is_loaded\n    if is_loaded:\n        return\n    import habana_frameworks.torch\n```\n\n----------------------------------------\n\nTITLE: Defining CPU Implementation of Custom Add Operator\nDESCRIPTION: A simple CPU implementation of addition operation registered with the dispatcher system using TORCH_LIBRARY_IMPL for CPU-specific execution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/dispatcher.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY_IMPL(myops, CPU, m) {\n  m.impl(\"myadd\", myadd_cpu);\n}\n```\n\n----------------------------------------\n\nTITLE: Example oneDNN Verbose Output Confirming AMX Utilization\nDESCRIPTION: Provides sample output from oneDNN's verbose logging. Key lines starting with `onednn_verbose,exec,cpu,...` show the specific kernel implementations used. The presence of `jit:avx512_core_amx_bf16` for BFloat16 convolution or `brg:avx512_core_amx_int8` for INT8 matrix multiplication confirms that Intel AMX instructions were utilized for those operations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/amx.rst#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nonednn_verbose,info,oneDNN v2.7.3 (commit 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\nonednn_verbose,info,cpu,runtime:OpenMP,nthr:128\nonednn_verbose,info,cpu,isa:Intel AVX-512 with float16, Intel DL Boost and bfloat16 support and Intel AMX with bfloat16 and 8-bit integer support\nonednn_verbose,info,gpu,runtime:none\nonednn_verbose,info,prim_template:operation,engine,primitive,implementation,prop_kind,memory_descriptors,attributes,auxiliary,problem_desc,exec_time\nonednn_verbose,exec,cpu,reorder,simple:any,undef,src_f32::blocked:a:f0 dst_f32::blocked:a:f0,attr-scratchpad:user ,,2,5.2561\n...\nonednn_verbose,exec,cpu,convolution,jit:avx512_core_amx_bf16,forward_training,src_bf16::blocked:acdb:f0 wei_bf16:p:blocked:ABcd16b16a2b:f0 bia_f32::blocked:a:f0 dst_bf16::blocked:acdb:f0,attr-scratchpad:user ,alg:convolution_direct,mb7_ic2oc1_ih224oh111kh3sh2dh1ph1_iw224ow111kw3sw2dw1pw1,0.628906\n...\nonednn_verbose,exec,cpu,matmul,brg:avx512_core_amx_int8,undef,src_s8::blocked:ab:f0 wei_s8:p:blocked:BA16a64b4a:f0 dst_s8::blocked:ab:f0,attr-scratchpad:user ,,1x30522:30522x768:1x768,7.66382\n...\n```\n\n----------------------------------------\n\nTITLE: Registering Submodules in Initializer List for PyTorch C++ Modules\nDESCRIPTION: This code defines a module where submodules (such as a sequence of linear layers) are registered in the constructor's initializer list using register_module. Prerequisites include torch::nn::Module and relevant submodules (e.g., torch::nn::Linear). This pattern ties module state to initialization and facilitates shape-checking at construction.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n  struct Net : torch::nn::Module {\n    Net(int64_t N, int64_t M)\n      : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n    { }\n    torch::nn::Linear linear;\n  };\n```\n\n----------------------------------------\n\nTITLE: Building Tutorial Documentation\nDESCRIPTION: Command for building HTML version of the tutorial website without executing code examples.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake html-noplot\n```\n\n----------------------------------------\n\nTITLE: Enabling bfloat16 Fast Math Mode for Graviton3\nDESCRIPTION: Command to enable bfloat16 GEMM kernels via environment variable, which utilizes the Graviton3's hardware support for bfloat16 MMLA instructions for faster inference.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport DNNL_DEFAULT_FPMATH_MODE=BF16\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Cleaning Output Directory for WSI Workflow - Python\nDESCRIPTION: This code sample configures warning suppression, defines a working directory (\"./tmp/\") for session artifacts, and implements a helper function to remove a directory if it exists. It clears remnants of previous runs to ensure reproducibility, then creates a fresh working directory for the current session and logs its creation. Dependencies include Python's standard library modules and TIAToolbox’s logger. Takes no parameters when run as shown, but the rmdir function can be re-used with any path. The function assumes adequate permissions for file system operations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwarnings.filterwarnings(\"ignore\")\nglobal_save_dir = Path(\"./tmp/\")\n\n\ndef rmdir(dir_path: str | Path) -> None:\n    \"\"\"Helper function to delete directory.\"\"\"\n    if Path(dir_path).is_dir():\n        shutil.rmtree(dir_path)\n        logger.info(\"Removing directory %s\", dir_path)\n\n\nrmdir(global_save_dir)  # remove  directory if it exists from previous runs\nglobal_save_dir.mkdir()\nlogger.info(\"Creating new directory %s\", global_save_dir)\n\n```\n\n----------------------------------------\n\nTITLE: Loading ImageNet data for model evaluation\nDESCRIPTION: This snippet sets up the data path and loads a subset of the ImageNet dataset for model evaluation. It specifies transforms for the data and creates DataLoader instances.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/README.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Set the data path\ndata_path = 'data/imagenet_1k'\n\n# ImageNet loading\ntrain_batch_size = 30\neval_batch_size = 30\n\ndata_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(\n    os.path.join(data_path, 'train'),\n    transform=data_transform\n)\n\neval_dataset = torchvision.datasets.ImageFolder(\n    os.path.join(data_path, 'val'),\n    transform=data_transform\n)\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=0, drop_last=True\n)\n\neval_dataloader = DataLoader(\n    eval_dataset, batch_size=eval_batch_size, shuffle=False, num_workers=0, drop_last=True\n)\n```\n\n----------------------------------------\n\nTITLE: RST Tutorial Links for PyTorch ONNX Export\nDESCRIPTION: ReStructuredText formatted links to PyTorch ONNX export tutorials. Includes links for basic model export, operator support extension, and control flow model export.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/onnx/onnx_toc.txt#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n| 1. `Exporting a PyTorch model to ONNX <export_simple_model_to_onnx_tutorial.html>`_\n| 2. `Extending the ONNX exporter operator support <onnx_registry_tutorial.html>`_\n| 3. `Export a model with control flow to ONNX <export_control_flow_model_to_onnx_tutorial.html>`_\n```\n\n----------------------------------------\n\nTITLE: Running Flight Recorder Analyzer Script - Shell\nDESCRIPTION: This shell snippet shows how to run the PyTorch Flight Recorder Analyzer from the command line, either as a direct Python script (fr_trace.py) or using a command-line tool (torchfrtrace). The tool processes all dump files within a specified directory, optionally writes results to an output file, and supports advanced filtering using arguments like --selected-ranks and --pg-filters. The script depends on Python, PyTorch, and the tabulate package.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython fr_trace.py <dump dir containing trace files> [-o <output file>]\n```\n\nLANGUAGE: shell\nCODE:\n```\ntorchfrtrace <dump dir containing trace files> [-o <output file>]\n```\n\nLANGUAGE: shell\nCODE:\n```\npython fr_trace.py <dump dir containing trace files> -j [--selected-ranks i j k ...] [--pg-filters tp dp]\ntorchfrtrace <dump dir containing trace files> -j [--selected-ranks i j k ...] [--pg-filters 0 2]\n```\n\n----------------------------------------\n\nTITLE: Training ResNet50 with FP32 using Intel Extension for PyTorch\nDESCRIPTION: Demonstrates how to train a ResNet50 model on CIFAR10 dataset using FP32 precision with Intel Extension for PyTorch backend. Includes data loading, model setup, optimization and training loop implementation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_compile_backend_ipex.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n  torchvision.transforms.Resize((224, 224)),\n  torchvision.transforms.ToTensor(),\n  torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n  root=DATA,\n  train=True,\n  transform=transform,\n  download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n  dataset=train_dataset,\n  batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\n\nimport intel_extension_for_pytorch as ipex\n\n# Invoke the following API optionally, to apply frontend optimizations\nmodel, optimizer = ipex.optimize(model, optimizer=optimizer)\n\ncompile_model = torch.compile(model, backend=\"ipex\")\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    output = compile_model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Implementing Autograd Function for PrivateUse1\nDESCRIPTION: Custom implementation of autograd functions for the new backend using AutogradPrivateUse1 dispatch key.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nclass CumtomSeluFunction : public torch::autograd::Function<CumtomSeluFunction> {\n  // Implementation of selu kernel in new backend\n}\n\nat::Tensor wrapper_AutogradCumstom__selu(const at::Tensor & self) {\n  return CumtomSeluFunction::apply(self);\n}\n\nTORCH_LIBRARY_IMPL(aten, AutogradPrivateUse1, m) {\n  ...\n  m.impl(\"selu\", TORCH_FN(wrapper_AutogradCustom__selu));\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Custom Free Function Definition - C++\nDESCRIPTION: Example of defining a free function that takes a custom C++ class as an argument.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes.rst#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nBEGIN free_function\nEND free_function\n```\n\n----------------------------------------\n\nTITLE: Async Checkpointing with Pinned Memory Buffer using PyTorch DCP in Python\nDESCRIPTION: This snippet extends asynchronous checkpointing by employing a persistent pinned memory buffer for GPU checkpointing tasks with torch.distributed.checkpoint, reducing in-memory copy overhead and enabling direct memory access for faster save operations. As before, AppState is defined to comply with Stateful and facilitate seamless integration with DCP's APIs. The usage pattern requires explicit management of buffer lifetime and acknowledges increased peak memory usage during the entire application run. Prerequisites: PyTorch v2.4.0+, CUDA-enabled GPUs, and all dependencies for distributed and checkpointing modules. Key parameters include storage writer and buffer management.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_async_checkpoint_recipe.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.distributed.checkpoint as dcp\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\nfrom torch.distributed.checkpoint.stateful import Stateful\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import StateDictType\nfrom torch.distributed.checkpoint import StorageWriter\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass AppState(Stateful):\n    \"\"\"This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n    with the Stateful protocol, DCP will automatically call state_dict/load_stat_dict as needed in the\n    dcp.save/load APIs.\n\n    Note: We take advantage of this wrapper to hande calling distributed state dict methods on the model\n    and optimizer.\n    \"\"\"\n\n    def __init__(self, model, optimizer=None):\n        self.model = model\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        # this line automatically manages FSDP FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n        model_state_dict, optimizer_state_dict = get_state_dict(model, optimizer)\n        return {\n            \"model\": model_state_dict,\n            \"optim\": optimizer_state_dict\n        }\n\n    def load_state_dict(self, state_dict):\n        # sets our state dicts on the model and optimizer, now that we've loaded\n        set_state_dict(\n            self.model,\n            self.optimizer,\n            model_state_dict=state_dict[\"model\"],\n            optim_state_dict=state_dict[\"optim\"]\n        )\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355 \"\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef run_fsdp_checkpoint_save_example(rank, world_size):\n    print(f\"Running basic FSDP checkpoint saving example on rank {rank}.\")\n```\n\n----------------------------------------\n\nTITLE: Adding PyTorch Entry Point to setup() for habana_frameworks Extension (Diff)\nDESCRIPTION: Presents a diff showing how to add an entry_points section to setup.py in the habana_frameworks package. This registers 'device_backend' as an entrypoint, mapped to the __autoload function, allowing PyTorch to autoload the Intel Gaudi HPU backend. The snippet is written in diff format to reflect changes required. It requires setuptool's setup function and that the entry point module and function are available. This modification is essential for making the extension compatible with the PyTorch autoload feature.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/python_extension_autoload.rst#2025-04-22_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\nsetup(\n    name=\"habana_frameworks\",\n    version=\"2.5\",\n+   entry_points={\n+       'torch.backends': [\n+           \"device_backend = habana_frameworks:__autoload\",\n+       ],\n+   }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Redirect to ExecuTorch Documentation\nDESCRIPTION: This HTML meta tag creates an automatic redirect to the ExecuTorch documentation page after a 3-second delay, directing users from the deprecated PyTorch Mobile documentation to the currently supported alternative.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/model_preparation_ios.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Redirect Tag\nDESCRIPTION: Meta refresh tag that redirects the user to the main PyTorch tutorials page after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/torchtext_custom_dataset_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials'\" />\n```\n\n----------------------------------------\n\nTITLE: Dumping NCCL Trace via PyTorch Private API - Python\nDESCRIPTION: This Python snippet demonstrates how to retrieve Flight Recorder data for distributed collectives by calling PyTorch's private _dump_nccl_trace function. The function takes optional arguments to control inclusion of collective operations, stack traces, and filtering for active operations. It requires torch.distributed with a properly initialized distributed job. The output is a pickled binary containing diagnostic trace data collected from the flight recorder.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntorch._C._distributed_c10d._dump_nccl_trace(includeCollectives=True, includeStackTraces=True, onlyActive=False)\n```\n\n----------------------------------------\n\nTITLE: Restoring Training State in PyTorch C++\nDESCRIPTION: Code demonstrating how to restore training state by loading checkpoints for generator and discriminator models and their optimizers using torch::load.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_27\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::optim::Adam generator_optimizer(\n    generator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\ntorch::optim::Adam discriminator_optimizer(\n    discriminator->parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\n\nif (kRestoreFromCheckpoint) {\n  torch::load(generator, \"generator-checkpoint.pt\");\n  torch::load(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n  torch::load(discriminator, \"discriminator-checkpoint.pt\");\n  torch::load(\n      discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n}\n\nint64_t checkpoint_counter = 0;\nfor (int64_t epoch = 1; epoch <= kNumberOfEpochs; ++epoch) {\n  int64_t batch_index = 0;\n  for (torch::data::Example<>& batch : *data_loader) {\n```\n\n----------------------------------------\n\nTITLE: Defining LeNet Model in PyTorch\nDESCRIPTION: Implementation of the LeNet CNN architecture in PyTorch, which includes convolutional layers, dropout, and linear layers. This model is loaded with pre-trained weights for MNIST classification.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# LeNet Model definition\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc1_drop = nn.Dropout()\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.reshape(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc1_drop(x)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\nmodel = Net()\nmodel.load_state_dict(torch.load('./lenet_mnist_model.pth', weights_only=True))\n```\n\n----------------------------------------\n\nTITLE: Printed Output of Floating-Point GraphModule Structure (No Quantization) - Python\nDESCRIPTION: This snippet displays the printed structure and forward method of a PyTorch GraphModule where no quantization or fusion has occurred due to unsatisfied backend constraints. The model’s layers (Linear, Conv2d, BatchNorm2d, ReLU, Sigmoid) remain in standard floating-point form. This is to inspect the model structure after attempting default QConfigMapping quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nGraphModule(\n  (linear): Linear(in_features=10, out_features=3, bias=True)\n  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU()\n  (sigmoid): Sigmoid()\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x):\n    linear = self.linear(x);  x = None\n    conv = self.conv(linear);  linear = None\n    bn = self.bn(conv);  conv = None\n    relu = self.relu(bn);  bn = None\n    sigmoid = self.sigmoid(relu);  relu = None\n    return sigmoid\n\n```\n\n----------------------------------------\n\nTITLE: Training ResNet50 with BF16 using Intel Extension for PyTorch\nDESCRIPTION: Demonstrates how to train a ResNet50 model on CIFAR10 dataset using BFloat16 precision with Intel Extension for PyTorch backend. Uses autocast for mixed precision training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_compile_backend_ipex.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision\n\nLR = 0.001\nDOWNLOAD = True\nDATA = 'datasets/cifar10/'\n\ntransform = torchvision.transforms.Compose([\n  torchvision.transforms.Resize((224, 224)),\n  torchvision.transforms.ToTensor(),\n  torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_dataset = torchvision.datasets.CIFAR10(\n  root=DATA,\n  train=True,\n  transform=transform,\n  download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(\n  dataset=train_dataset,\n  batch_size=128\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = LR, momentum=0.9)\nmodel.train()\n\nimport intel_extension_for_pytorch as ipex\n\n# Invoke the following API optionally, to apply frontend optimizations\nmodel, optimizer = ipex.optimize(model, dtype=torch.bfloat16, optimizer=optimizer)\n\ncompile_model = torch.compile(model, backend=\"ipex\")\n\nwith torch.cpu.amp.autocast():\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = compile_model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Configuring Raspberry Pi Camera Settings (TOML)\nDESCRIPTION: Updates the Raspberry Pi boot configuration file (`/boot/config.txt`) to enable extended features like the camera and allocate sufficient GPU memory. It explicitly sets `start_x=1`, ensures `gpu_mem` is at least 128MB, and comments out `camera_auto_detect=1` to avoid conflicts with OpenCV/V4L2 capture. A reboot is required after applying these changes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n# This enables the extended features such as the camera.\nstart_x=1\n\n# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.\ngpu_mem=128\n\n# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.\n#camera_auto_detect=1\n```\n\n----------------------------------------\n\nTITLE: Inference with ResNet50 on GPU using IPEX - BFloat16 (Python)\nDESCRIPTION: This snippet shows how to perform inference with a ResNet50 model on GPU using IPEX with BFloat16 precision. It includes model loading, data preparation, IPEX optimization, and BFloat16 autocast for inference.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\nimport intel_extension_for_pytorch as ipex\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(memory_format=torch.channels_last)\ndata = data.to(memory_format=torch.channels_last)\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.bfloat16)\n\nwith torch.no_grad():\n  with torch.xpu.amp.autocast(enabled=True, dtype=torch.bfloat16, cache_enabled=False):\n    model(data)\n\n```\n\n----------------------------------------\n\nTITLE: Configuration for Performance-only Quantization\nDESCRIPTION: YAML configuration for performance-only quantization using Intel Neural Compressor. It specifies the model name and framework.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n# conf.yaml\nmodel:\n    name: lenet\n    framework: pytorch_fx\n```\n\n----------------------------------------\n\nTITLE: Git Commands for PR Submission\nDESCRIPTION: Git commands for adding changes, committing, and pushing a new tutorial branch for PR submission.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit add -A\ngit commit -m \"Add <mytutorial>\"\ngit push --set-upstream mybranch\n```\n\n----------------------------------------\n\nTITLE: Defining a QuantizationSpec for Input and Output Tensors - Python\nDESCRIPTION: This snippet illustrates how to construct a QuantizationSpec for annotating the quantization configuration for both input and output tensors to an operator (in this case, the add node). It sets integer type, quantization range, quantization scheme, indicates static quantization, and specifies HistogramObserver as the observer. Dependencies: torch, QuantizationSpec, HistogramObserver. Inputs: tensor value for which quantization is being described; Outputs: configuration object to be used with annotation APIs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nact_quantization_spec = QuantizationSpec(\\n    dtype=torch.int8,\\n    quant_min=-128,\\n    quant_max=127,\\n    qscheme=torch.per_tensor_affine,\\n    is_dynamic=False,\\n    observer_or_fake_quant_ctr=HistogramObserver.with_args(eps=2**-12),\\n)\\n\\ninput_act_qspec = act_quantization_spec\\noutput_act_qspec = act_quantization_spec\n```\n\n----------------------------------------\n\nTITLE: Using ATen Tensor Accessors for Efficient and Type-Safe Indexing (C++)\nDESCRIPTION: This code demonstrates the use of ATen's tensor accessor interface to safely and efficiently iterate across tensor elements on CPU, checking dimension and type at runtime. It improves code readability and maintainability compared to raw pointer arithmetic. Requires ATen/Torch C++ headers and is intended for tensors of specified type/dimension. This approach simplifies working with multi-dimensional tensors.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor foo = torch::rand({12, 12});\n\n// assert foo is 2-dimensional and holds floats.\nauto foo_a = foo.accessor<float,2>();\nfloat trace = 0;\n\nfor(int i = 0; i < foo_a.size(0); i++) {\n\n```\n\n----------------------------------------\n\nTITLE: Running TorchScript Inference with Custom Operator (Without Linking Library) - Shell\nDESCRIPTION: Attempts to run the TorchScript-serialized model in the C++ inference binary without linking the custom operator library. Shows error output indicating that the custom op (my_ops::warp_perspective) schema is not found. Demonstrates the requirement for properly linking or loading custom ops in deployment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n$ ./example_app example.pt\nterminate called after throwing an instance of 'torch::jit::script::ErrorReport'\nwhat():\nSchema not found for node. File a bug report.\nNode: %16 : Dynamic = my_ops::warp_perspective(%0, %19)\n\n```\n\n----------------------------------------\n\nTITLE: Profiling Large Batch Inference Performance\nDESCRIPTION: Profiles the model's inference performance with a large batch size (256) to demonstrate the baseline performance on Graviton3 processors.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# warm it up first and loop over multiple times to have enough execution time\n\nX = torch.rand(256, 64, 64, device=device)\n\nwith torch.set_grad_enabled(False):\n    for _ in range(50):\n        model(X) #Warmup\n    with profile(activities=[ProfilerActivity.CPU]) as prof:\n        with record_function(\"mymodel_inference\"):\n            for _ in range(100):\n                model(X)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n```\n\n----------------------------------------\n\nTITLE: CMake Configuration for Custom Class Project\nDESCRIPTION: CMake build configuration for compiling the custom C++ class into a shared library.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes.rst#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(custom_class)\n\nfind_package(Torch REQUIRED)\n\nadd_library(custom_class SHARED class.cpp)\ntarget_link_libraries(custom_class \"${TORCH_LIBRARIES}\")\nset_property(TARGET custom_class PROPERTY CXX_STANDARD 14)\n```\n\n----------------------------------------\n\nTITLE: Dispatcher Function Implementation\nDESCRIPTION: Implementation of a dispatching function that serves as the public C++ API for the custom operator, using the dispatcher to find and execute the appropriate kernel.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/dispatcher.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nat::Tensor myadd(const at::Tensor& self, const at::Tensor& other) {\n  static const auto op = c10::Dispatcher::singleton()\n    .findSchemaOrThrow(\"myops::myadd\", \"\")\n    .typed<decltype(myadd)>();\n  return op.call(self, other);\n}\n```\n\n----------------------------------------\n\nTITLE: Comparing performance between original and quantized models\nDESCRIPTION: This code measures and compares the inference performance between the original floating-point model and the quantized model using a specified number of warm-up and measurement runs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/README.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Measure the inference time of the floating point model\ndef measure_inference_time(model, eval_dataloader, num_warmup=10, num_runs=50):\n    # Warm-up runs\n    with torch.no_grad():\n        for _ in range(num_warmup):\n            image, target = next(iter(eval_dataloader))\n            _ = model(image)\n    \n    # Measure runs\n    start_time = time.time()\n    with torch.no_grad():\n        for _ in range(num_runs):\n            image, target = next(iter(eval_dataloader))\n            _ = model(image)\n    end_time = time.time()\n    \n    return (end_time - start_time) / num_runs\n\n# Measure and print inference time for floating point model\nfp32_inference_time = measure_inference_time(float_model, eval_dataloader)\nprint(f\"FP32 model inference time: {fp32_inference_time:.6f} seconds\")\n\n# Measure and print inference time for quantized model\nint8_inference_time = measure_inference_time(qmodel, eval_dataloader)\nprint(f\"INT8 model inference time: {int8_inference_time:.6f} seconds\")\n\n# Calculate and print speedup\nspeedup = fp32_inference_time / int8_inference_time\nprint(f\"Speedup: {speedup:.2f}x\")\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-quantized ResNet-18 for Feature Extraction in PyTorch\nDESCRIPTION: Imports the quantized models submodule from torchvision and loads a pre-trained, quantized ResNet-18 model. It sets `quantize=True` to get the quantized version and retrieves the number of input features (`num_ftrs`) of the original fully connected layer (`fc`), which is needed to define the new classifier head.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision.models.quantization as models\n\n# You will need the number of filters in the `fc` for future use.\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\nmodel_fe = models.resnet18(pretrained=True, progress=True, quantize=True)\nnum_ftrs = model_fe.fc.in_features\n```\n\n----------------------------------------\n\nTITLE: Implementing Observer Class for Distributed RL Training\nDESCRIPTION: Observer class implementation that creates a CartPole environment and handles episode execution. Uses RPC to communicate with the agent for action selection and reward reporting.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport gym\nimport torch.distributed.rpc as rpc\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Reinforcement Learning Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument('--world_size', default=2, type=int, metavar='W',\n                    help='number of workers')\nparser.add_argument('--log_interval', type=int, default=10, metavar='N',\n                    help='interval between training status logs')\nparser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n                    help='how much to value future rewards')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed  for reproducibility')\nargs = parser.parse_args()\n\nclass Observer:\n\n    def __init__(self):\n        self.id = rpc.get_worker_info().id\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n\n    def run_episode(self, agent_rref):\n        state, ep_reward = self.env.reset(), 0\n        for _ in range(10000):\n            # send the state to the agent to get an action\n            action = agent_rref.rpc_sync().select_action(self.id, state)\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n\n            # report the reward to the agent for training purpose\n            agent_rref.rpc_sync().report_reward(self.id, reward)\n\n            # finishes after the number of self.env._max_episode_steps\n            if done:\n                break\n```\n\n----------------------------------------\n\nTITLE: Selecting Loss Functions\nDESCRIPTION: Loss functions available in PyTorch's nn module, used for training neural networks. These include common losses like MSE, cross-entropy, and specialized losses for various machine learning tasks.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nnn.X                                  # where X is L1Loss, MSELoss, CrossEntropyLoss\n                                      # CTCLoss, NLLLoss, PoissonNLLLoss, \n                                      # KLDivLoss, BCELoss, BCEWithLogitsLoss,\n                                      # MarginRankingLoss, HingeEmbeddingLoss,\n                                      # MultiLabelMarginLoss, SmoothL1Loss,\n                                      # SoftMarginLoss, MultiLabelSoftMarginLoss,\n                                      # CosineEmbeddingLoss, MultiMarginLoss,\n                                      # or TripletMarginLoss\n```\n\n----------------------------------------\n\nTITLE: Selecting Ring Attention Rotation Method in PyTorch Context Parallel\nDESCRIPTION: Shows how to explicitly select the rotation method used by the Ring Attention mechanism within Context Parallel. By calling `set_rotate_method(\"alltoall\")` before the `context_parallel` block, the all-to-all based pass-KV algorithm is chosen for shuffling KV shards during the distributed attention computation, overriding the default all-gather based approach.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/context_parallel.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# file: cp_sdpa_example.py\nfrom torch.distributed.tensor.experimental._attention import set_rotate_method\n\nset_rotate_method(\"alltoall\")  # rotate shards using all-to-all\n\nwith sdpa_kernel(backend):\n    with context_parallel(\n        device_mesh, buffers=tuple(cp_qkv), buffer_seq_dims=(2, 2, 2)\n    ):\n        cp_out = F.scaled_dot_product_attention(*cp_qkv, is_causal=True)\n```\n\n----------------------------------------\n\nTITLE: Annotating Input and Output Edges of Add Node with QuantizationAnnotation - Python\nDESCRIPTION: This snippet shows the construction of a QuantizationAnnotation object to annotate two input tensors and the output tensor of an add node using QuantizationSpec. It creates a mapping of input node arguments to their quantization spec, sets the output spec, and marks the node as annotated within its meta property. Dependencies: QuantizationAnnotation, a properly identified add_node, and QuantizationSpec objects as in the prior snippet. Inputs: FX add node with accessible args; Outputs: node's metadata updated with quantization annotation for downstream quantization flow.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninput_qspec_map = {}\\ninput_act0 = add_node.args[0]\\ninput_qspec_map[input_act0] = input_act_qspec\\n\\ninput_act1 = add_node.args[1]\\ninput_qspec_map[input_act1] = input_act_qspec\\n     \\nadd_node.meta[\\\"quantization_annotation\\\"] = QuantizationAnnotation(\\n    input_qspec_map=input_qspec_map,\\n    output_qspec=output_act_qspec,\\n    _annotated=True,\\n)\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect to ExecuTorch Documentation\nDESCRIPTION: An HTML meta refresh tag that automatically redirects the user to the ExecuTorch documentation page after a 3-second delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/model_preparation_android.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Inference with ResNet50 on GPU using IPEX - Float16 (Python)\nDESCRIPTION: This snippet demonstrates inference with a ResNet50 model on GPU using IPEX with Float16 precision. It includes model loading, data preparation, IPEX optimization, and Float16 autocast for inference.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\nimport intel_extension_for_pytorch as ipex\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(memory_format=torch.channels_last)\ndata = data.to(memory_format=torch.channels_last)\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\nmodel = ipex.optimize(model, dtype=torch.float16)\n\nwith torch.no_grad():\n  with torch.xpu.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=False):\n    model(data)\n\n```\n\n----------------------------------------\n\nTITLE: Enabling Inductor C++ Wrapper in PyTorch\nDESCRIPTION: Shows how to activate the experimental C++ wrapper feature in TorchInductor by setting the `config.cpp_wrapper` flag to `True`. This directs Inductor to generate C++ code for the execution wrapper instead of the default Python code, potentially reducing overhead.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/inductor_cpp_wrapper_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch._inductor.config as config\nconfig.cpp_wrapper = True\n```\n\n----------------------------------------\n\nTITLE: Disabling PyTorch Extension Autoloading via Environment Variable (Python)\nDESCRIPTION: Shows how to disable the device backend autoload feature by setting the TORCH_DEVICE_BACKEND_AUTOLOAD environment variable to '0' before importing torch. This may be necessary to avoid circular imports or unwanted side effects, especially with torch_npu. Only standard Python is required. Inputs include setting the environment variable and importing torch; no outputs other than the prevention of autoloading.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/python_extension_autoload.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Disable autoloading before running 'import torch'\nos.environ['TORCH_DEVICE_BACKEND_AUTOLOAD'] = '0'\n\nimport torch\n```\n\n----------------------------------------\n\nTITLE: Enabling ITT Profiling in PyTorch with emit_itt Scope\nDESCRIPTION: Code snippet demonstrating how to enable ITT profiling for a section of code by wrapping it in a torch.autograd.profiler.emit_itt() scope.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/profile_with_itt.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith torch.autograd.profiler.emit_itt():\n  <code-to-be-profiled...>\n```\n\n----------------------------------------\n\nTITLE: Converting Checkpoint Formats using Command Line in Bash\nDESCRIPTION: This bash command demonstrates how to use the format_utils module to convert between DCP and torch.save checkpoint formats via command line.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.distributed.checkpoint.format_utils <mode> <checkpoint location> <location to write formats to>\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for PyTorch Tutorials\nDESCRIPTION: Requirements file that specifies all necessary Python packages and their versions for running PyTorch tutorials. Includes ML frameworks, visualization tools, and utility libraries with specific version constraints and platform-specific installations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# --extra-index-url https://download.pytorch.org/whl/cu117/index.html # Use this to run/publish tutorials against the latest binaries during the RC stage. Comment out after the release. Each release verify the correct cuda version.\n# Refer to ./jenkins/build.sh for tutorial build instructions\n\nsphinx==5.0.0\nsphinx-gallery==0.11.1\nsphinx_design\ndocutils==0.16\nsphinx-copybutton\nsphinx_sitemap==2.6.0\npypandoc==1.12\npandocfilters\nmarkdown\ntqdm==4.66.1\nnumpy==1.24.4\nmatplotlib\nlibrosa\ntorch==2.6\ntorchvision\ntorchdata\nnetworkx\nPyHamcrest\nbs4\nawscliv2==2.1.1\nflask\nspacy==3.4.1\nray[tune]==2.7.2\ntensorboard\njinja2==3.1.3\npytorch-lightning\ntorchx\ntorchrl==0.7.2\ntensordict==0.7.2\nax-platform>=0.4.0\nnbformat>=5.9.2\ndatasets\ntransformers\ntorchmultimodal-nightly # needs to be updated to stable as soon as it's avaialable\nonnx\nonnxscript>=0.2.2\nonnxruntime\nevaluate\naccelerate>=0.20.1\n\nimportlib-metadata==6.8.0\n\n# PyTorch Theme\n-e git+https://github.com/pytorch/pytorch_sphinx_theme.git#egg=pytorch_sphinx_theme\n\nipython\n\nsphinxcontrib.katex\n# to run examples\nboto3\npandas\nrequests\nscikit-image\nscipy==1.11.1\nnumba==0.57.1\npillow==10.2.0\nwget\ngym==0.26.2\ngym-super-mario-bros==7.4.0\npyopengl\ngymnasium[mujoco]==0.27.0\ntimm\niopath\npygame==2.6.0\npycocotools\nsemilearn==0.3.2\ntorchao==0.5.0\nsegment_anything==1.0\ntorchrec==1.1.0; platform_system == \"Linux\"\nfbgemm-gpu==1.1.0; platform_system == \"Linux\"\n```\n\n----------------------------------------\n\nTITLE: Creating PyTorch Tensors on Custom Backend Device (Python)\nDESCRIPTION: Demonstrates creating a PyTorch tensor directly on a custom backend device using its registered name (e.g., 'npu:0') compared to the internal 'privateuse1:0' identifier. Using the custom name (after renaming) improves user-friendliness and clarity.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntorch.rand((2,2),device='npu:0')\ntorch.rand((2,2),device='privateuse1:0')\n```\n\n----------------------------------------\n\nTITLE: Passing Keyword Arguments to Join Context Manager in Python\nDESCRIPTION: Example of passing keyword arguments to the Join context manager to modify behavior at runtime. It demonstrates how to set the divide_by_initial_world_size parameter for DistributedDataParallel.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/generic_join.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith Join([model, optim], divide_by_initial_world_size=False):\n    for input in inputs:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Training Output Log - DCGAN Training\nDESCRIPTION: Shell output showing the training progress of the DCGAN model over 30 epochs, displaying discriminator and generator loss values.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\nroot@3c0711f20896:/home/build# make && ./dcgan                                                                                                                                10:17:57\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nCUDA is available! Training on GPU.\n[ 1/30][200/938] D_loss: 0.4953 | G_loss: 4.0195\n-> checkpoint 1\n[ 1/30][400/938] D_loss: 0.3610 | G_loss: 4.8148\n-> checkpoint 2\n[ 1/30][600/938] D_loss: 0.4072 | G_loss: 4.36760\n-> checkpoint 3\n[ 1/30][800/938] D_loss: 0.4444 | G_loss: 4.0250\n-> checkpoint 4\n[ 2/30][200/938] D_loss: 0.3761 | G_loss: 3.8790\n-> checkpoint 5\n[ 2/30][400/938] D_loss: 0.3977 | G_loss: 3.3315\n...\n-> checkpoint 120\n[30/30][938/938] D_loss: 0.3610 | G_loss: 3.8084\n```\n\n----------------------------------------\n\nTITLE: Running Distributed RL Episodes with PyTorch RPC (Python)\nDESCRIPTION: Defines the Agent class method run_episode, which uses asynchronous RPC to launch and synchronize training episodes on multiple observer processes. This method appends and waits on futures returned from rpc_async calls to observer RRefs, ensuring all observers complete their episodes before proceeding. Dependencies include torch.distributed.rpc and a defined Observer class; requires initialization of RPC framework and proper setup of Agent and observer RRefs. Inputs include a list of Observer RRefs, outputs are managed through futures synchronization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Agent:\n    ...\n    def run_episode(self):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(\n                rpc_async(\n                    ob_rref.owner(),\n                    ob_rref.rpc_sync().run_episode,\n                    args=(self.agent_rref,)\n                )\n            )\n\n        # wait until all obervers have finished this episode\n        for fut in futs:\n            fut.wait()\n\n```\n\n----------------------------------------\n\nTITLE: Implementing the Remote Forward Pass in TrainerNet using PyTorch RPC in Python\nDESCRIPTION: Defines the `forward` method for the `TrainerNet` class (partially shown with ellipsis). It executes the model's forward pass remotely on the `ParameterServer`. This is achieved by calling the `ParameterServer.forward` method using `remote_method` (assumed `torch.distributed.rpc.remote_method`) along with the parameter server's RRef (`self.param_server_rref`) and the input tensor `x`. The method returns the output tensor computed by the model on the parameter server.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass TrainerNet(nn.Module):\n...\n    def forward(self, x):\n        model_output = remote_method(\n            ParameterServer.forward, self.param_server_rref, x)\n        return model_output\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect\nDESCRIPTION: An HTML meta tag that automatically redirects the page to the new Tacotron2 tutorial location after a 3 second delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/text_to_speech_with_torchaudio.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/audio/stable/tutorials/tacotron2_pipeline_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Custom Store-Based Barrier Synchronization with TCPStore - PyTorch - Python\nDESCRIPTION: This snippet implements a custom store-based barrier function for distributed synchronization in PyTorch, replacing the internal dist._store_based_barrier. It uses a shared key in TCPStore to synchronize all ranks and handle timeouts and logging via parameters. Input parameters include rank, store, group name, world size, and timeouts. Logs progress/status, handles error scenarios, and runs a configurable number of synchronizations to benchmark performance. Dependencies: torch, logging, os, datetime, time.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TCPStore_libuv_backend.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport os\nimport time\n\nfrom datetime import timedelta\nfrom time import perf_counter\n\nimport torch\nimport torch.distributed as dist\n\nDistStoreError = torch._C._DistStoreError\nlogger: logging.Logger = logging.getLogger(__name__)\n\n# since dist._store_based_barrier is a private function and cannot be directly called, we need to write a function which does the same\ndef store_based_barrier(\n    rank,\n    store,\n    group_name,\n    rendezvous_count,\n    timeout=dist.constants.default_pg_timeout,\n    logging_interval=timedelta(seconds=10),\n):\n    store_key = f\"store_based_barrier_key:{group_name}\"\n    store.add(store_key, 1)\n\n    world_size = rendezvous_count\n    worker_count = store.add(store_key, 0)\n\n    last_worker_key = f\"{store_key}:last_worker\"\n    if worker_count == world_size:\n        store.set(last_worker_key, \"1\")\n\n    start = time.time()\n    while True:\n        try:\n            # This will throw an exception after the logging_interval in which we print out\n            # the status of the group or time out officially, throwing runtime error\n            store.wait([last_worker_key], logging_interval)\n            break\n        except RuntimeError as e:\n            worker_count = store.add(store_key, 0)\n            # Print status periodically to keep track.\n            logger.info(\n                \"Waiting in store based barrier to initialize process group for \"\n                \"rank: %s, key: %s (world_size=%s, num_workers_joined=%s, timeout=%s)\"\n                \"error: %s\",\n                rank,\n                store_key,\n                world_size,\n                worker_count,\n                timeout,\n                e,\n            )\n\n            if timedelta(seconds=(time.time() - start)) > timeout:\n                raise DistStoreError(\n                    \"Timed out initializing process group in store based barrier on \"\n                    \"rank {}, for key: {} (world_size={}, num_workers_joined={}, timeout={})\".format(\n                        rank, store_key, world_size, worker_count, timeout\n                    )\n                )\n\n    logger.info(\n        \"Rank %s: Completed store-based barrier for key:%s with %s nodes.\",\n        rank,\n        store_key,\n        world_size,\n    )\n\n# Env var are preset when launching the benchmark\nenv_rank = os.environ.get(\"RANK\", 0)\nenv_world_size = os.environ.get(\"WORLD_SIZE\", 1)\nenv_master_addr = os.environ.get(\"MASTER_ADDR\", \"localhost\")\nenv_master_port = os.environ.get(\"MASTER_PORT\", \"23456\")\n\ntcp_store = dist.TCPStore(\n    env_master_addr,\n    int(env_master_port),\n    world_size=int(env_world_size),\n    is_master=(int(env_rank) == 0),\n)\n\n# sync workers\nstore_based_barrier(int(env_rank), tcp_store, \"tcpstore_test\", int(env_world_size))\n\nnumber_runs = 10\nstart = perf_counter()\nfor _ in range(number_runs):\n    store_based_barrier(\n        int(env_rank), tcp_store, \"tcpstore_test\", int(env_world_size)\n    )\nend = perf_counter()\ntime_elapsed = end - start\nlogger.info(\n    f\"Complete {number_runs} TCPStore barrier runs with rank={env_rank}, world_size={env_world_size} in {time_elapsed} seconds.\"\n)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantization with Custom Metrics in YAML\nDESCRIPTION: YAML configuration file for Intel Neural Compressor without specifying a built-in metric, allowing for a custom metric to be defined in the Python code. Sets a 1% relative accuracy criterion for auto-tuning.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n# conf.yaml\nmodel:\n    name: LeNet\n    framework: pytorch_fx\n\ntuning:\n    accuracy_criterion:\n        relative: 0.01\n```\n\n----------------------------------------\n\nTITLE: Loading Models with Distributed Checkpoint in PyTorch\nDESCRIPTION: This code demonstrates how to load an FSDP-wrapped model that was previously saved with Distributed Checkpoint (DCP). It creates a model with the same architecture, wraps it with FSDP, and then loads the saved state using DCP's load functionality.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_checkpoint_recipe.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.distributed.checkpoint as dcp\nfrom torch.distributed.checkpoint.stateful import Stateful\nfrom torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\nimport torch.multiprocessing as mp\nimport torch.nn as nn\n\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nCHECKPOINT_DIR = \"checkpoint\"\n\n\nclass AppState(Stateful):\n    \"\"\"This is a useful wrapper for checkpointing the Application State. Since this object is compliant\n    with the Stateful protocol, DCP will automatically call state_dict/load_stat_dict as needed in the\n    dcp.save/load APIs.\n\n    Note: We take advantage of this wrapper to hande calling distributed state dict methods on the model\n    and optimizer.\n    \"\"\"\n\n    def __init__(self, model, optimizer=None):\n        self.model = model\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        # this line automatically manages FSDP FQN's, as well as sets the default state dict type to FSDP.SHARDED_STATE_DICT\n        model_state_dict, optimizer_state_dict = get_state_dict(self.model, self.optimizer)\n        return {\n            \"model\": model_state_dict,\n            \"optim\": optimizer_state_dict\n        }\n\n    def load_state_dict(self, state_dict):\n        # sets our state dicts on the model and optimizer, now that we've loaded\n        set_state_dict(\n            self.model,\n            self.optimizer,\n            model_state_dict=state_dict[\"model\"],\n            optim_state_dict=state_dict[\"optim\"]\n        )\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(16, 16)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(16, 8)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef setup(rank, world_size):\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355 \"\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    torch.cuda.set_device(rank)\n\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef run_fsdp_checkpoint_load_example(rank, world_size):\n    print(f\"Running basic FSDP checkpoint loading example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create a model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    model = FSDP(model)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n    state_dict = { \"app\": AppState(model, optimizer)}\n    dcp.load(\n        state_dict=state_dict,\n        checkpoint_id=CHECKPOINT_DIR,\n    )\n\n    cleanup()\n\n\nif __name__ == \"__main__\":\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Redirect for PyTorch Audio Tutorial\nDESCRIPTION: This HTML meta tag creates an automatic redirect to the new location of the Audio Data Augmentation tutorial on the PyTorch website. It waits for 3 seconds before redirecting the user.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/audio_data_augmentation_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Installing System and Python Prerequisites for WSI Analysis - Bash\nDESCRIPTION: This snippet gives shell commands to install required system-level libraries (OpenJpeg, OpenSlide, Pixman) via apt-get for Linux, and the main Python dependencies (TIAToolbox <1.5, HistoEncoder) via pip. Successful installation is echoed in the terminal. Optional alternate homebrew instructions for macOS are mentioned in the surrounding text, but not directly included as a snippet. These commands are prerequisites for running subsequent code blocks that rely on TIAToolbox or handle .svs/.tif WSI data. No inputs/outputs other than terminal installation logs and confirmation message.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\napt-get -y -qq install libopenjp2-7-dev libopenjp2-tools openslide-tools libpixman-1-dev\npip install -q 'tiatoolbox<1.5' histoencoder && echo \"Installation is done.\"\n\n```\n\n----------------------------------------\n\nTITLE: Processing Conv-ReLU Pattern Matches for Quantization in PyTorch\nDESCRIPTION: This code extracts nodes from a matched Conv-ReLU pattern and applies quantization annotations to the appropriate nodes in the graph. It uses the SubgraphMatcherWithNameNodeMap to find patterns and annotates input, weight, bias, and output nodes for quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmatcher = SubgraphMatcherWithNameNodeMap(conv_relu_pattern)\nmatches = matcher.match(model)\nfor match in matches:\n    # find input and output of the pattern\n    # annotate the nodes\n    name_node_map = match.name_node_map\n    input_node = name_node_map[\"input\"]\n    weight_node = name_node_map[\"weight\"]\n    bias_node = name_node_map[\"bias\"]\n    output_node = name_node_map[\"relu\"]\n    input_node.users[0].meta[\"quantization_annotation\"] = ...\n    weight_node.users[0].meta[\"quantization_annotation\"] = ...\n    bias_node.users[0].meta[\"quantization_annotation\"] = ...\n    output_node.meta[\"quantization_annotation\"] = ...\n```\n\n----------------------------------------\n\nTITLE: Equivalent Neural Network Module in C++\nDESCRIPTION: C++ implementation of the same neural network module, showing how to register parameters and implement the forward pass using the C++ frontend.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/torch.h>\n\nstruct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M) {\n    W = register_parameter(\"W\", torch::randn({N, M}));\n    b = register_parameter(\"b\", torch::randn(M));\n  }\n  torch::Tensor forward(torch::Tensor input) {\n    return torch::addmm(b, input, W);\n  }\n  torch::Tensor W, b;\n};\n```\n\n----------------------------------------\n\nTITLE: Inference with ResNet50 in FP32 using Intel Extension for PyTorch\nDESCRIPTION: Shows how to perform inference using a pre-trained ResNet50 model with FP32 precision using Intel Extension for PyTorch backend. Includes model optimization and inference setup.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_compile_backend_ipex.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(weights='ResNet50_Weights.DEFAULT')\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nimport intel_extension_for_pytorch as ipex\n\n# Invoke the following API optionally, to apply frontend optimizations\nmodel = ipex.optimize(model, weights_prepack=False)\n\ncompile_model = torch.compile(model, backend=\"ipex\")\n\nwith torch.no_grad():\n    compile_model(data)\n```\n\n----------------------------------------\n\nTITLE: Initializing Distributed Environment in PyTorch\nDESCRIPTION: Sets up the distributed environment by spawning multiple processes, initializing the process group, and running a distributed function. Uses the 'gloo' backend and spawns 2 processes on a single machine.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_tuto.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"run.py:\"\"\"\n#!/usr/bin/env python\nimport os\nimport sys\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\ndef run(rank, size):\n    \"\"\" Distributed function to be implemented later. \"\"\"\n    pass\n\ndef init_process(rank, size, fn, backend='gloo'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    world_size = 2\n    processes = []\n    if \"google.colab\" in sys.modules:\n        print(\"Running in Google Colab\")\n        mp.get_context(\"spawn\")\n    else:\n        mp.set_start_method(\"spawn\")\n    for rank in range(world_size):\n        p = mp.Process(target=init_process, args=(rank, world_size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Functions and Preparing ImageNet Data in Python\nDESCRIPTION: This snippet prepares the environment and data for the QAT tutorial. It includes imports (`torch`, `torchvision`, `numpy`, etc.), defines an `AverageMeter` class for tracking metrics, an `accuracy` function for computing top-k accuracy, an `evaluate` function for model validation (using `move_exported_model_to_eval`), a `load_model` function for loading a pre-trained ResNet18, a `print_size_of_model` utility, a `prepare_data_loaders` function for ImageNet, and a `train_one_epoch` function for the training loop. It also sets up data paths, batch sizes, loads data, defines the criterion (CrossEntropyLoss), and loads the initial float model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nimport time\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.models.resnet import resnet18\nimport torchvision.transforms as transforms\n\n# Set up warnings\nimport warnings\nwarnings.filterwarnings(\n    action='ignore',\n    category=DeprecationWarning,\n    module=r'.*'\n)\nwarnings.filterwarnings(\n    action='default',\n    module=r'torch.ao.quantization'\n)\n\n# Specify random seed for repeatable results\n_ = torch.manual_seed(191009)\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"\n    Computes the accuracy over the k top predictions for the specified\n    values of k.\n    \"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\ndef evaluate(model, criterion, data_loader, device):\n    torch.ao.quantization.move_exported_model_to_eval(model)\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    cnt = 0\n    with torch.no_grad():\n        for image, target in data_loader:\n            image = image.to(device)\n            target = target.to(device)\n            output = model(image)\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n    print('')\n\n    return top1, top5\n\ndef load_model(model_file):\n    model = resnet18(pretrained=False)\n    state_dict = torch.load(model_file, weights_only=True)\n    model.load_state_dict(state_dict)\n    return model\n\ndef print_size_of_model(model):\n    if isinstance(model, torch.jit.RecursiveScriptModule):\n        torch.jit.save(model, \"temp.p\")\n    else:\n        torch.jit.save(torch.jit.script(model), \"temp.p\")\n    print(\"Size (MB):\", os.path.getsize(\"temp.p\")/1e6)\n    os.remove(\"temp.p\")\n\ndef prepare_data_loaders(data_path):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    dataset = torchvision.datasets.ImageNet(\n        data_path, split=\"train\", transform=transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    dataset_test = torchvision.datasets.ImageNet(\n        data_path, split=\"val\", transform=transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset)\n    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=train_batch_size,\n        sampler=train_sampler)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=eval_batch_size,\n        sampler=test_sampler)\n\n    return data_loader, data_loader_test\n\ndef train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n    # Note: do not call model.train() here, since this doesn't work on an exported model.\n    # Instead, call `torch.ao.quantization.move_exported_model_to_train(model)`, which will\n    # be added in the near future\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    avgloss = AverageMeter('Loss', '1.5f')\n\n    cnt = 0\n    for image, target in data_loader:\n        start_time = time.time()\n        print('.', end = '')\n        cnt += 1\n        image, target = image.to(device), target.to(device)\n        output = model(image)\n        loss = criterion(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        top1.update(acc1[0], image.size(0))\n        top5.update(acc5[0], image.size(0))\n        avgloss.update(loss, image.size(0))\n        if cnt >= ntrain_batches:\n            print('Loss', avgloss.avg)\n\n            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n                  .format(top1=top1, top5=top5))\n            return\n\n    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n          .format(top1=top1, top5=top5))\n    return\n\ndata_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'resnet18_pretrained_float.pth'\n\ntrain_batch_size = 32\neval_batch_size = 32\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\nexample_inputs = (next(iter(data_loader))[0])\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading QAT Model Checkpoints\nDESCRIPTION: Code for saving model checkpoints during QAT training and restoring the training state later. This demonstrates how to create and use checkpoints for resuming training or recovery.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint_path = \"/path/to/my/checkpoint_%s.pth\" % nepoch\ntorch.save(prepared_model.state_dict(), \"checkpoint_path\")\n```\n\n----------------------------------------\n\nTITLE: Using the TORCH_MODULE Macro for Holder Wrappers in PyTorch C++\nDESCRIPTION: This code demonstrates how to define a module implementation and its holder using the TORCH_MODULE macro, which generates a std::shared_ptr-based wrapper to simplify module instantiation and management. It requires the PyTorch C++ headers and macro support. The forward method and parameter members are declared, and the macro enables user-friendly instantiation akin to Python's API.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\n  struct LinearImpl : torch::nn::Module {\n    LinearImpl(int64_t in, int64_t out);\n\n    Tensor forward(const Tensor& input);\n\n    Tensor weight, bias;\n  };\n\n  TORCH_MODULE(Linear);\n```\n\n----------------------------------------\n\nTITLE: Setting up C++ Extension Build System\nDESCRIPTION: Setup script using setuptools and torch.utils.cpp_extension to compile custom C++/CUDA code with CPython agnostic wheel support.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import setup, Extension\nfrom torch.utils import cpp_extension\n\nsetup(name=\"extension_cpp\",\n      ext_modules=[\n          cpp_extension.CppExtension(\n            \"extension_cpp\",\n            [\"muladd.cpp\"],\n            extra_compile_args={\"cxx\": [\"-DPy_LIMITED_API=0x03090000\"]}, \n            py_limited_api=True)],\n      cmdclass={'build_ext': cpp_extension.BuildExtension},\n      options={\"bdist_wheel\": {\"py_limited_api\": \"cp39\"}}\n)\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect\nDESCRIPTION: HTML meta tag that automatically redirects the user to the PyTorch tutorials index page after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/ONNXLive.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Redirecting to Updated PyTorch Tutorial Using HTML Meta Tag\nDESCRIPTION: This HTML snippet creates an automatic redirection to the updated PyTorch tutorial on saving and loading models after a 3-second delay. It uses a meta http-equiv tag to implement the redirection.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/recipes/saving_and_loading_a_general_checkpoint.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/saving_loading_models.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantization-Aware Training\nDESCRIPTION: Python code for quantization-aware training with Intel Neural Compressor. Sets up train and test data loaders for MNIST, defines an SGD optimizer, and implements a training function for the model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_neural_compressor_for_pytorch.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nmodel.eval()\n\nfrom torchvision import datasets, transforms\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data', train=False, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=1)\n\nimport torch.optim as optim\noptimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.1)\n\ndef training_func(model):\n    model.train()\n    for epoch in range(1, 3):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n```\n\n----------------------------------------\n\nTITLE: Testing Custom Operator in Python\nDESCRIPTION: Python code snippet to load and test the custom operator library, demonstrating how to import and use the operator in PyTorch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../advanced_source/torch_script_custom_ops/smoke_test.py\n  :language: python\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch CPU Thread Count for Performance Tuning in Python\nDESCRIPTION: This Python command sets the number of threads PyTorch will use for parallel computation on the CPU to 2. This is often used on devices with limited cores, like a Raspberry Pi, to reduce contention with other processes and potentially stabilize inference latency by trading off some peak throughput.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntorch.set_num_threads(2)\n```\n\n----------------------------------------\n\nTITLE: HTML Redirect to PyTorch Tutorials Homepage\nDESCRIPTION: HTML meta tag that automatically redirects the user to the PyTorch tutorials home page after 2 seconds. This is used to handle the deprecated tutorial page gracefully.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_rpc_profiling.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"2; url='https://pytorch.org/tutorials'\" />\n```\n\n----------------------------------------\n\nTITLE: Defining a Bidirectional LSTM Ensemble Model in PyTorch\nDESCRIPTION: Implements a baseline ensemble model consisting of multiple bidirectional LSTM layers. Each bidirectional LSTM contains forward and backward LSTM cells, with the ensemble combining their outputs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch-script-parallelism.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch, time\n\n# In RNN parlance, the dimensions we care about are:\n# # of time-steps (T)\n# Batch size (B)\n# Hidden size/number of \"channels\" (C)\nT, B, C = 50, 50, 1024\n\n# A module that defines a single \"bidirectional LSTM\". This is simply two\n# LSTMs applied to the same sequence, but one in reverse\nclass BidirectionalRecurrentLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)\n        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)\n\n    def forward(self, x : torch.Tensor) -> torch.Tensor:\n        # Forward layer\n        output_f, _ = self.cell_f(x)\n\n        # Backward layer. Flip input in the time dimension (dim 0), apply the\n        # layer, then flip the outputs in the time dimension\n        x_rev = torch.flip(x, dims=[0])\n        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n        output_b_rev = torch.flip(output_b, dims=[0])\n\n        return torch.cat((output_f, output_b_rev), dim=2)\n\n\n# An \"ensemble\" of `BidirectionalRecurrentLSTM` modules. The modules in the\n# ensemble are run one-by-one on the same input then their results are\n# stacked and summed together, returning the combined result.\nclass LSTMEnsemble(torch.nn.Module):\n    def __init__(self, n_models):\n        super().__init__()\n        self.n_models = n_models\n        self.models = torch.nn.ModuleList([\n            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])\n\n    def forward(self, x : torch.Tensor) -> torch.Tensor:\n        results = []\n        for model in self.models:\n            results.append(model(x))\n        return torch.stack(results).sum(dim=0)\n\n# For a head-to-head comparison to what we're going to do with fork/wait, let's\n# instantiate the model and compile it with TorchScript\nens = torch.jit.script(LSTMEnsemble(n_models=4))\n\n# Normally you would pull this input out of an embedding table, but for the\n# purpose of this demo let's just use random data.\nx = torch.rand(T, B, C)\n\n# Let's run the model once to warm up things like the memory allocator\nens(x)\n\nx = torch.rand(T, B, C)\n\n# Let's see how fast it runs!\ns = time.time()\nens(x)\nprint('Inference took', time.time() - s, ' seconds')\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for WSI Classification - Python\nDESCRIPTION: This snippet imports all Python modules required for running a Jupyter notebook focused on WSI classification workflows using TIAToolbox and PyTorch. It configures logging, suppresses unnecessary warnings, and prepares utility, math, visualization, and machine learning libraries such as matplotlib, numpy, pandas, scikit-learn, and torch. It also introduces a function to suppress overly verbose console output. Dependencies required are listed in the text, including tiatoolbox, histoencoder, torch, and data science libraries. No inputs or outputs are produced in this cell beyond configuring the runtime context for later code.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/tiatoolbox_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"Import modules required to run the Jupyter notebook.\"\"\"\nfrom __future__ import annotations\n\n# Configure logging\nimport logging\nimport warnings\nif logging.getLogger().hasHandlers():\n    logging.getLogger().handlers.clear()\nwarnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")\n\n# Downloading data and files\nimport shutil\nfrom pathlib import Path\nfrom zipfile import ZipFile\n\n# Data processing and visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\nimport PIL\nimport contextlib\nimport io\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# TIAToolbox for WSI loading and processing\nfrom tiatoolbox import logger\nfrom tiatoolbox.models.architecture import vanilla\nfrom tiatoolbox.models.engine.patch_predictor import (\n    IOPatchPredictorConfig,\n    PatchPredictor,\n)\nfrom tiatoolbox.utils.misc import download_data, grab_files_from_dir\nfrom tiatoolbox.utils.visualization import overlay_prediction_mask\nfrom tiatoolbox.wsicore.wsireader import WSIReader\n\n# Torch-related\nimport torch\nfrom torchvision import transforms\n\n# Configure plotting\nmpl.rcParams[\"figure.dpi\"] = 160  # for high resolution figure in notebook\nmpl.rcParams[\"figure.facecolor\"] = \"white\"  # To make sure text is visible in dark mode\n\n# If you are not using GPU, change ON_GPU to False\nON_GPU = True\n\n# Function to suppress console output for overly verbose code blocks\ndef suppress_console_output():\n    return contextlib.redirect_stderr(io.StringIO())\n\n```\n\n----------------------------------------\n\nTITLE: Processing Answer Spans for Question Answering in Python\nDESCRIPTION: This function processes the offset mapping and answers to determine the start and end token positions for the answer spans. It handles cases where the answer may not be fully inside the context by labeling them as (0, 0).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\noffset_mapping = inputs[\"offset_mapping\"]\nanswers = examples[\"answers\"]\nstart_positions = []\nend_positions = []\n\nfor i, (offset, answer) in enumerate(zip(offset_mapping, answers)):\n    start_char = answer[\"answer_start\"][0]\n    end_char = start_char + len(answer[\"text\"][0])\n    sequence_ids = inputs.sequence_ids(i)\n\n    # Find the start and end of the context\n    idx = 0\n    while sequence_ids[idx] != 1:\n        idx += 1\n    context_start = idx\n    while sequence_ids[idx] == 1:\n        idx += 1\n    context_end = idx - 1\n\n    # If the answer is not fully inside the context, label it (0, 0)\n    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n        start_positions.append(0)\n        end_positions.append(0)\n    else:\n        # Otherwise it's the start and end token positions\n        idx = context_start\n        while idx <= context_end and offset[idx][0] <= start_char:\n            idx += 1\n        start_positions.append(idx - 1)\n\n        idx = context_end\n        while idx >= context_start and offset[idx][1] >= end_char:\n            idx -= 1\n        end_positions.append(idx + 1)\n\ninputs[\"start_positions\"] = start_positions\ninputs[\"end_positions\"] = end_positions\nreturn inputs\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirection\nDESCRIPTION: This code snippet uses the HTML meta tag to redirect the user to a new URL after a specified delay (3 seconds). It is a simple way to automatically forward users from an old page to a new one.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/audio_feature_augmentation_tutorial.rst#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Redirect in Python Sphinx Documentation\nDESCRIPTION: This HTML snippet is used within a Python Sphinx documentation to implement a meta refresh redirect. It redirects the user to the latest parallelism APIs page after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/ddp_pipeline.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/dist_overview.html#parallelism-apis'\" />\n```\n\n----------------------------------------\n\nTITLE: C++ GEMM Template Kernel with Epilogue Fusion (C++ Embedded in Python String)\nDESCRIPTION: Illustrates a generated C++ template-based GEMM kernel, embedded as a Python string for use with PyTorch Inductor backend. Provides specialized microkernel routines, thread parallelization, and epilogue fusion for bias and ReLU, leveraging vectorized operations. Requires a CPU architecture supporting AMX/bfloat16 instructions, OpenMP for threading, and PyTorch's vectorized intrinsics. Inputs are batched GEMM operands (bfloat16 pointers); output is written to a result tensor. The snippet assumes pre-packed weights (for inference) and is architecturally and implementation-specific, targeting performance-critical deep learning workloads.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/max_autotune_on_CPU_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncpp_fused__to_copy_relu_1 = async_compile.cpp_pybinding(['const bfloat16*', 'const bfloat16*', 'const bfloat16*', 'bfloat16*'], '''\n\n...\n\ntemplate <bool accum>\ninline void kernel_micro_gemm_amx_kernel_32_2(\n    AMXState& amx_state,\n    const bfloat16* __restrict__ A,\n    const bfloat16* __restrict__ B,\n    float* __restrict__ C,\n    int64_t K,\n    int64_t lda,\n    int64_t ldb,\n    int64_t ldc,\n    uint8_t tilecfg_rows\n) {\n    ...\n}\n\n...\n\ntemplate <bool accum>\ninline void kernel_micro_gemm(\n    AMXState& amx_state,\n    const bfloat16* __restrict__ A,\n    const bfloat16* __restrict__ B,\n    float* __restrict__ C,\n    int64_t M,\n    int64_t N,\n    int64_t K,\n    int64_t lda,\n    int64_t ldb,\n    int64_t ldc\n) {\n    ...\n}\n\nextern \"C\" \nvoid kernel(const bfloat16* X, const bfloat16* W, const bfloat16* inp, bfloat16* Y)\n{\n    constexpr int64_t num_threads = 40;\n    constexpr int64_t N = 32;\n    constexpr int64_t K = 16;\n    constexpr int64_t M = static_cast<int64_t>(64L);\n    ...\n    #pragma omp parallel num_threads(40)\n    {\n        const int tid = omp_get_thread_num();\n        ...\n        for (int64_t mc_block_id = 0; mc_block_id < num_Mc_blocks_per_thread; mc_block_id++) {\n            ...\n            for (int64_t nc = n_block_start; nc < n_block_end; nc += Nc_blocks) {\n                ...\n                for (int64_t kc = k_block_start; kc < k_block_end; kc += Kc_blocks) {\n                    ...\n                    for (int64_t nci = nc; nci < nc_block_end; nci++) {\n                        if (kc == k_block_start) {\n                            kernel_micro_gemm<static_cast<bool>(false)>(\n                                ...\n                            );\n\n                        } else {\n                            kernel_micro_gemm<static_cast<bool>(true)>(\n                                ...\n                            );\n\n                        }\n                    }\n                }\n                {\n                    {\n                        // Epilogue fusion here for bias and relu\n                        #pragma GCC ivdep\n                        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(m_end + ((-1L)*m_start)); x0+=static_cast<int64_t>(1L))\n                        {\n                            for(int64_t x1=static_cast<int64_t>(0L); x1<static_cast<int64_t>(16L*(c10::div_floor_integer(static_cast<int64_t>((n_end + ((-1L)*n_start))), static_cast<int64_t>(16L)))); x1+=static_cast<int64_t>(16L))\n                            {\n                                auto tmp0 = at::vec::Vectorized<bfloat16>::loadu(inp + static_cast<int64_t>(n_start + x1), static_cast<int64_t>(16));\n                                auto tmp2 = at::vec::Vectorized<float>::loadu(local_acc_buf + static_cast<int64_t>(x1 + (Nc_blocks*Nr*x0)), static_cast<int64_t>(16));\n                                auto tmp1 = at::vec::convert<float>(tmp0);\n                                auto tmp3 = tmp1 + tmp2;\n                                auto tmp4 = at::vec::convert<bfloat16>(tmp3);\n                                auto tmp5 = static_cast<float>(0.0);\n                                auto tmp6 = at::vec::Vectorized<float>(tmp5);\n                                auto tmp7 = at::vec::maximum(tmp3, tmp6);\n                                auto tmp8 = at::vec::convert<bfloat16>(tmp7);\n                                tmp8.store(Y + static_cast<int64_t>(n_start + x1 + (32L*m_start) + (32L*x0)), static_cast<int64_t>(16));\n                            }\n                            \n                            ...\n\n                        }\n                    }\n\n                }\n            }\n        }\n        ...\n    }\n}\n''')\n\n```\n\n----------------------------------------\n\nTITLE: Module Holder Pattern for Passing and Instantiating Modules in PyTorch C++\nDESCRIPTION: The snippet shows how to define an implementation class and create its holder using TORCH_MODULE. The generated holder is used as a value-type object, providing ergonomic passing and instantiation of modules. Required dependencies include PyTorch's C++ API and the macro facility. Demonstrates passing the holder to functions and creating instances efficiently.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n  struct NetImpl : torch::nn::Module {};\n  TORCH_MODULE(Net);\n\n  void a(Net net) { }\n\n  int main() {\n    Net net;\n    a(net);\n  }\n```\n\n----------------------------------------\n\nTITLE: CPU Backend Implementation Registration\nDESCRIPTION: Registration of the CPU implementation for the custom operator.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY_IMPL(extension_cpp, CPU, m) {\n  m.impl(\"mymuladd\", &mymuladd_cpu);\n}\n```\n\n----------------------------------------\n\nTITLE: Extending RNNModel with Parameter RRef Collection for Distributed Training\nDESCRIPTION: Extends the RNNModel class to gather parameter RRefs from all components (remote embedding table, local LSTM, and remote decoder) for use in distributed optimization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass RNNModel(nn.Module):\n    ...\n    def parameter_rrefs(self):\n        remote_params = []\n        # get RRefs of embedding table\n        remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref))\n        # create RRefs for local parameters\n        remote_params.extend(_parameter_rrefs(self.rnn))\n        # get RRefs of decoder\n        remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref))\n        return remote_params\n```\n\n----------------------------------------\n\nTITLE: Using torch.compile in PyTorch\nDESCRIPTION: Reference to PyTorch's compiler functionality that can be used with custom operators. This compilation system helps optimize PyTorch code.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/custom_ops_landing_page.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntorch.compile\n```\n\n----------------------------------------\n\nTITLE: Initializing Circular Import Prevention State in habana_frameworks.torch (Python)\nDESCRIPTION: In habana_frameworks/torch/__init__.py, initializes the parent module's is_loaded flag upon import. This code sets habana_frameworks.is_loaded to True to signal that the backend has been loaded, preventing repeated or circular imports by the autoload function. Requires both habana_frameworks and habana_frameworks.torch modules to be properly structured. This measure is key to stability in autoload scenarios.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/python_extension_autoload.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# This is to prevent torch autoload mechanism from causing circular imports\nimport habana_frameworks\n\nhabana_frameworks.is_loaded = True\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Generator for PrivateUse1\nDESCRIPTION: Custom random number generator implementation for the new backend using GeneratorImpl.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nstruct CustomGeneratorImpl : public c10::GeneratorImpl {\n  // Implementation of generator in new backend\n}\n\nat::Generator make_custom_generator(c10::DeviceIndex device_index) {\n  return at::make_generator<CustomGeneratorImpl>(device_index);\n}\n\nREGISTER_GENERATOR_PRIVATEUSE1(make_cumstom_generator)\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Refresh for Redirection in PyTorch Documentation\nDESCRIPTION: This HTML snippet creates a meta refresh tag to automatically redirect the user to the ExecuTorch documentation after 3 seconds. It's used to guide users from the deprecated PyTorch Mobile documentation to the new ExecuTorch project.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/deeplabv3_on_ios.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Running a PyTorch Distributed Script with torchrun\nDESCRIPTION: This Bash command uses `torchrun` to launch the Python script (`crash.py`). It specifies that the job runs on a single node (`--nnodes=1`) with two processes (`--nproc_per_node=2`). `torchrun` automatically sets the necessary environment variables like `LOCAL_RANK`, `WORLD_SIZE`, `MASTER_ADDR`, and `MASTER_PORT` for the distributed processes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes=1 --nproc_per_node=2 crash.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Command-Line Arguments for PyTorch RPC Parameter Server\nDESCRIPTION: Sets up argparse to handle command-line arguments for configuring the distributed training setup, including world size, rank, number of GPUs, master address, and port.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=\"Parameter-Server RPC based training\")\n    parser.add_argument(\n        \"--world_size\",\n        type=int,\n        default=4,\n        help=\"\"\"Total number of participating processes. Should be the sum of\n        master node and all training nodes.\"\"\")\n    parser.add_argument(\n        \"--rank\",\n        type=int,\n        default=None,\n        help=\"Global rank of this process. Pass in 0 for master.\")\n    parser.add_argument(\n        \"--num_gpus\",\n        type=int,\n        default=0,\n        help=\"\"\"Number of GPUs to use for training, Currently supports between 0\n         and 2 GPUs. Note that this argument will be passed to the parameter servers.\"\"\")\n    parser.add_argument(\n        \"--master_addr\",\n        type=str,\n        default=\"localhost\",\n        help=\"\"\"Address of master, will default to localhost if not provided.\n        Master must be able to accept network traffic on the address + port.\"\"\")\n    parser.add_argument(\n        \"--master_port\",\n        type=str,\n        default=\"29500\",\n        help=\"\"\"Port that master is listening on, will default to 29500 if not\n        provided. Master must be able to accept network traffic on the host and port.\"\"\")\n\n    args = parser.parse_args()\n    assert args.rank is not None, \"must provide rank argument.\"\n    assert args.num_gpus <= 3, f\"Only 0-2 GPUs currently supported (got {args.num_gpus}).\"\n    os.environ['MASTER_ADDR'] = args.master_addr\n    os.environ[\"MASTER_PORT\"] = args.master_port\n```\n\n----------------------------------------\n\nTITLE: Simple Autocast Add Operation\nDESCRIPTION: Implementation of an autocast wrapper for a basic add operation, showing how to integrate with existing autograd functionality.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/dispatcher.rst#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nTensor myadd_autocast(const Tensor& self, const Tensor& other) {\n  c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast);\n  return myadd(at::autocast::cached_cast(<desired dtype>, self),\n               at::autocast::cached_cast(<desired dtype>, other));\n}\n\nTORCH_LIBRARY_IMPL(myops, Autocast, m) {\n  m.impl(\"myadd\", myadd_autocast);\n}\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Training with torchrun - PyTorch - Bash\nDESCRIPTION: This Bash command launches distributed training for the T5 model using torchrun, specifying number of nodes and processes per node, and the training script T5_training.py. It requires that PyTorch and torchrun are installed, and that the script and data are available and properly configured. Key parameters are --nnodes for node count and --nproc_per_node for GPU process allocation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nnodes 1 --nproc_per_node 4  T5_training.py\n\n```\n\n----------------------------------------\n\nTITLE: MNIST Neural Network Model Definition\nDESCRIPTION: Definition of a convolutional neural network for MNIST digit classification, including convolution layers, dropouts, and fully connected layers.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Python Operator Interface\nDESCRIPTION: Python function signature showing the expected behavior of the custom multiply-add operator.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef mymuladd(a: Tensor, b: Tensor, c: float):\n    return a * b + c\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Redirect for Deprecated PyTorch Mobile Tutorial\nDESCRIPTION: This HTML snippet creates a meta tag that automatically redirects the user to a new tutorial page after 3 seconds. It's used to guide users from the deprecated mobile optimization tutorial to its updated version.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/script_optimized.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Analyzing Flight Recorder Traces with torchfrtrace\nDESCRIPTION: This Bash command executes the `torchfrtrace` utility, which is part of the PyTorch tools. It analyzes the Flight Recorder trace files located in the `/tmp/` directory, identifying them by the common prefix `trace_`. The tool processes these files to pinpoint inconsistencies or errors in collective operations across different ranks.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntorchfrtrace --prefix \"trace_\" /tmp/\n```\n\n----------------------------------------\n\nTITLE: Defining BackendConfig and Pattern Configurations for Custom Backend in Python\nDESCRIPTION: This code assembles BackendPatternConfigs for quantized Linear and ConvReLU2d patterns, including observer and dtype configs. It also defines config for fusing Conv2d+ReLU and builds the BackendConfig object, establishing quantization and fusion behavior for the custom backend. Prerequisites include the DTypeConfig and fusion function defined in earlier snippets.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlinear_config = BackendPatternConfig() \\\n    .set_pattern(torch.nn.Linear) \\\n    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\n    .add_dtype_config(weighted_int8_dtype_config) \\\n    .set_root_module(torch.nn.Linear) \\\n    .set_qat_module(torch.nn.qat.Linear) \\\n    .set_reference_quantized_module(torch.ao.nn.quantized.reference.Linear)\n\n# For fusing Conv2d + ReLU into ConvReLU2d\n# No need to set observation type and dtype config here, since we are not\n# inserting quant-dequant ops in this step yet\nconv_relu_config = BackendPatternConfig() \\\n    .set_pattern((torch.nn.Conv2d, torch.nn.ReLU)) \\\n    .set_fused_module(torch.ao.nn.intrinsic.ConvReLU2d) \\\n    .set_fuser_method(fuse_conv2d_relu)\n\n# For quantizing ConvReLU2d\nfused_conv_relu_config = BackendPatternConfig() \\\n    .set_pattern(torch.ao.nn.intrinsic.ConvReLU2d) \\\n    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\n    .add_dtype_config(weighted_int8_dtype_config) \\\n    .set_root_module(torch.nn.Conv2d) \\\n    .set_qat_module(torch.ao.nn.intrinsic.qat.ConvReLU2d) \\\n    .set_reference_quantized_module(torch.ao.nn.quantized.reference.Conv2d)\n\nbackend_config = BackendConfig(\"my_backend\") \\\n    .set_backend_pattern_config(linear_config) \\\n    .set_backend_pattern_config(conv_relu_config) \\\n    .set_backend_pattern_config(fused_conv_relu_config)\n```\n\n----------------------------------------\n\nTITLE: Defining Neural Network Policy for Reinforcement Learning in PyTorch\nDESCRIPTION: Implementation of a simple neural network policy class for the CartPole environment using PyTorch. The network consists of two linear layers with dropout and outputs action probabilities using softmax.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Policy(nn.Module):\n\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=1)\n```\n\n----------------------------------------\n\nTITLE: JIT Compilation of a Custom TorchScript Operator in Python\nDESCRIPTION: Python code that uses PyTorch's C++ extension utilities to JIT compile a custom operator from a file, making it available in Python without manual compilation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport torch.utils.cpp_extension\n\ntorch.utils.cpp_extension.load(\n    name=\"warp_perspective\",\n    sources=[\"op.cpp\"],\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True\n)\n\nprint(torch.ops.my_ops.warp_perspective)\n```\n\n----------------------------------------\n\nTITLE: Demonstration of Improper Intermediate Saving in Custom Autograd Function (SinhBad) - PyTorch - Python\nDESCRIPTION: Shows an incorrect method for handling intermediates in a custom autograd function, where intermediates are saved to ctx but not returned as outputs. This causes the backward computation graph to exclude necessary operations, disabling double backward support. Suitable for educational demonstration of what not to do when building double backward-compatible custom autograd operations. Requires torch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SinhBad(torch.autograd.Function):\n    # This is an example of what NOT to do!\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.expx = expx\n        ctx.expnegx = expnegx\n        return (expx - expnegx) / 2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        expx = ctx.expx\n        expnegx = ctx.expnegx\n        grad_input = grad_out * (expx + expnegx) / 2\n        return grad_input\n\n```\n\n----------------------------------------\n\nTITLE: Importing Core PyTorch Modules in Python\nDESCRIPTION: Essential imports for working with PyTorch, including the main torch package and data loading utilities. These imports provide the foundation for building and training neural networks with PyTorch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch                                        # root package\nfrom torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Quantized PyTorch Model\nDESCRIPTION: This code snippet illustrates the process of saving and loading a quantized PyTorch model. It exports the model, saves it as an ExportedProgram, then loads it back and verifies the results.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# 0. Store reference output, for example, inputs, and check evaluation accuracy:\nexample_inputs = (next(iter(data_loader))[0],)\nref = quantized_model(*example_inputs)\ntop1, top5 = evaluate(quantized_model, criterion, data_loader_test)\nprint(\"[before serialization] Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n\n# 1. Export the model and Save ExportedProgram\npt2e_quantized_model_file_path = saved_model_dir + \"resnet18_pt2e_quantized.pth\"\n# capture the model to get an ExportedProgram\nquantized_ep = torch.export.export(quantized_model, example_inputs)\n# use torch.export.save to save an ExportedProgram\ntorch.export.save(quantized_ep, pt2e_quantized_model_file_path)\n\n\n# 2. Load the saved ExportedProgram\nloaded_quantized_ep = torch.export.load(pt2e_quantized_model_file_path)\nloaded_quantized_model = loaded_quantized_ep.module()\n\n# 3. Check results for example inputs and check evaluation accuracy again:\nres = loaded_quantized_model(*example_inputs)\nprint(\"diff:\", ref - res)\n\ntop1, top5 = evaluate(loaded_quantized_model, criterion, data_loader_test)\nprint(\"[after serialization/deserialization] Evaluation accuracy on test dataset: %2.2f, %2.2f\"%(top1.avg, top5.avg))\n```\n\n----------------------------------------\n\nTITLE: Reading and Converting OpenCV Frame Format (Python)\nDESCRIPTION: Reads a single frame from the initialized OpenCV video capture object (`cap`). OpenCV returns frames in BGR format by default, so this snippet converts the captured NumPy array to the standard RGB format expected by many image processing libraries and models like MobileNetV2.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nret, image = cap.read()\n# convert opencv output from BGR to RGB\nimage = image[:, :, [2, 1, 0]]\n```\n\n----------------------------------------\n\nTITLE: Applying Configuration Patterns for the XNNPACK Quantizer - Python\nDESCRIPTION: Demonstrates various API calls to further customize the quantizer configuration by targeting specific modules, types, or names. Enables fine-tuned quantization strategies on different model parts. qconfig_opt is an optional quantization configuration.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nquantizer.set_global(qconfig_opt)  # qconfig_opt is an optional quantization config\n    .set_object_type(torch.nn.Conv2d, qconfig_opt) # can be a module type\n    .set_object_type(torch.nn.functional.linear, qconfig_opt) # or torch functional op\n    .set_module_name(\"foo.bar\", qconfig_opt)\n```\n\n----------------------------------------\n\nTITLE: Implementing Parameter Server Class in Python\nDESCRIPTION: This snippet defines the ParameterServer class, which subclasses nn.Module and manages the model and input device. It includes a forward method for model inference.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ParameterServer(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        model = Net(num_gpus=num_gpus)\n        self.model = model\n        self.input_device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and num_gpus > 0 else \"cpu\")\n\n    def forward(self, inp):\n        inp = inp.to(self.input_device)\n        out = self.model(inp)\n        # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n        # Tensors must be moved in and out of GPU memory due to this.\n        out = out.to(\"cpu\")\n        return out\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for PyTorch Inference with Custom Class\nDESCRIPTION: This CMake snippet sets up a project for PyTorch inference. It requires CMake 3.1+, finds the Torch package, adds a custom class subdirectory, and configures an executable named 'infer' with necessary library linkages.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(infer)\n\nfind_package(Torch REQUIRED)\n\nadd_subdirectory(custom_class_project)\n\n# Define our library target\nadd_executable(infer infer.cpp)\nset(CMAKE_CXX_STANDARD 14)\n# Link against LibTorch\ntarget_link_libraries(infer \"${TORCH_LIBRARIES}\")\n# This is where we link in our libcustom_class code, making our\n# custom class available in our binary.\ntarget_link_libraries(infer -Wl,--no-as-needed custom_class)\n```\n\n----------------------------------------\n\nTITLE: Annotating Operators with Fixed Quantization Parameters - PyTorch Quantization - Python\nDESCRIPTION: This snippet illustrates annotation of an operator (like sigmoid) where the quantization parameters are fixed and known upfront, using FixedQParamsQuantizationSpec. The scale and zero_point (plus other parameters) are explicitly set, and the QuantizationAnnotation meta is assigned both for input and output. Required dependencies are torch, QuantizationAnnotation, and FixedQParamsQuantizationSpec. Input to the annotation includes the sigmoid FX graph node and its input node.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nact_qspec = FixedQParamsQuantizationSpec(\n    dtype=torch.uint8,\n    quant_min=0,\n    quant_max=255,\n    qscheme=torch.per_tensor_affine,\n    scale=1.0 / 256.0,\n    zero_point=0,\n)\nsigmoid_node.meta[\"quantization_annotation\"] = QuantizationAnnotation(\n    input_qspec_map={input_act: act_qspec},\n    output_qspec=act_qspec,\n    _annotated=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Generated C++ Wrapper Code and Loader for GPU\nDESCRIPTION: Presents the generated code with the Inductor C++ wrapper enabled for GPU. It features a C++ function `inductor_entry_cpp` using ATen C++ APIs for CUDA operations (device guard, tensor allocation, stream management, kernel loading/launching via `loadKernel`/`launchKernel`). Python code handles loading the compiled C++ code via `CppWrapperCodeCache` and wrapping it.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/inductor_cpp_wrapper_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nstd::vector<at::Tensor> inductor_entry_cpp(const std::vector<at::Tensor>& args) {\n    at::Tensor arg0_1 = args[0];\n    at::Tensor constant0 = args[1];\n\n    at::cuda::CUDAGuard device_guard(0);\n    auto buf0 = at::empty_strided({19L, }, {1L, }, at::TensorOptions(c10::Device(at::kCUDA, 0)).dtype(at::kFloat));\n    // Source Nodes: [add, tensor], Original ATen: [aten.add, aten.lift_fresh]\n    if (triton_poi_fused_add_lift_fresh_0 == nullptr) {\n        triton_poi_fused_add_lift_fresh_0 = loadKernel(\"/tmp/torchinductor_user/mm/cmm6xjgijjffxjku4akv55eyzibirvw6bti6uqmfnruujm5cvvmw.cubin\", \"triton_poi_fused_add_lift_fresh_0_0d1d2d3\");\n    }\n    CUdeviceptr var_0 = reinterpret_cast<CUdeviceptr>(constant0.data_ptr());\n    CUdeviceptr var_1 = reinterpret_cast<CUdeviceptr>(arg0_1.data_ptr());\n    CUdeviceptr var_2 = reinterpret_cast<CUdeviceptr>(buf0.data_ptr());\n    auto var_3 = 19;\n    void* kernel_args_var_0[] = {&var_0, &var_1, &var_2, &var_3};\n    cudaStream_t stream0 = at::cuda::getCurrentCUDAStream(0);\n    launchKernel(triton_poi_fused_add_lift_fresh_0, 1, 1, 1, 1, 0, kernel_args_var_0, stream0);\n    arg0_1.reset();\n    return {buf0};\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nmodule = CppWrapperCodeCache.load(cpp_wrapper_src, 'inductor_entry_cpp', 'czbpeilh4qqmbyejdgsbpdfuk2ss5jigl2qjb7xs4gearrjvuwem', True)\n\ndef _wrap_func(f):\n    def g(args):\n        args_tensor = [arg if isinstance(arg, torch.Tensor) else torch.tensor(arg) for arg in args]\n        constants_tensor = [constant0]\n        args_tensor.extend(constants_tensor)\n\n        return f(args_tensor)\n    return g\ncall = _wrap_func(module.inductor_entry_cpp)\n```\n\n----------------------------------------\n\nTITLE: Exporting Model with Dynamic Dimensions in PyTorch 2.4 and Earlier\nDESCRIPTION: Code for exporting a PyTorch model with dynamic dimensions in PyTorch 2.4 and earlier versions. This uses the dynamic_dim constraint to allow variable-sized inputs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_qat.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# from torch._export import dynamic_dim\n\n# example_inputs = (torch.rand(2, 3, 224, 224),)\n# exported_model = capture_pre_autograd_graph(\n#     float_model,\n#     example_inputs,\n#     constraints=[dynamic_dim(example_inputs[0], 0)],\n# )\n```\n\n----------------------------------------\n\nTITLE: Build command for Intel-optimized PyTorch C++ application\nDESCRIPTION: This shell command demonstrates how to compile a C++ application that uses Intel Extension for PyTorch. It specifies the path to libtorch and automatically links the Intel extension.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n$ cmake -DCMAKE_PREFIX_PATH=<LIBPYTORCH_PATH> ..\n$ make\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple PyTorch Model for Examples\nDESCRIPTION: Defines a basic `torch.nn.Module` subclass named `Model` containing a single linear layer. This model takes a 10-dimensional tensor as input and outputs a 10-dimensional tensor. It serves as a standard example throughout the tutorial to demonstrate Compiled Autograd.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/compiled_autograd_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass Model(torch.nn.Module):\n   def __init__(self):\n      super().__init__()\n      self.linear = torch.nn.Linear(10, 10)\n\n   def forward(self, x):\n      return self.linear(x)\n```\n\n----------------------------------------\n\nTITLE: Wrapping Model for DistributedDataParallel (DDP) Training - PyTorch (Python)\nDESCRIPTION: Wraps the Net model for multi-process distributed training using DistributedDataParallel (DDP) in PyTorch. Moves model to device by rank, wraps it with DDP for synchronized backward gradients across processes. Required dependencies are torch and DDP; inputs are model and rank. Output is DDP-wrapped model ready for distributed training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmodel = Net().to(rank)\nmodel = DDP(model)\n```\n\n----------------------------------------\n\nTITLE: Scripted Module with Multiple Exported Methods - PyTorch - Python\nDESCRIPTION: Defines a PyTorch nn.Module with multiple callables (forward and foo), the latter exported for TorchScript. Demonstrates compatibility for bundling inputs for methods beyond 'forward', with use of typing.Dict. Prerequisites: torch, torch.jit, torch.utils, and torch.utils.bundled_inputs. This prepares a model for input bundling for both standard and custom exported functions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/bundled_inputs.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\nimport torch.jit\\nimport torch.utils\\nimport torch.utils.bundled_inputs\\nfrom typing import Dict\\n\\nclass Net(nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        self.lin = nn.Linear(10, 1)\\n\\n    def forward(self, x):\\n        return self.lin(x)\\n\\n    @torch.jit.export\\n    def foo(self, x: Dict[String, int]):\\n        return x['a'] + x['b']\\n\\n\\nmodel = Net()\\nscripted_module = torch.jit.script(model)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Models & Caching Datasets - PyTorch - Python\nDESCRIPTION: Implements evaluation of PyTorch models (including support for multi-GPU, different output modes, and metrics like F1), and provides dataset loading/caching utilities tailored for GLUE-like NLP tasks. Dependencies include PyTorch, tqdm, numpy, and associated task processors; expects appropriate configuration via args and environment. Returns evaluation metrics and caches features to disk for efficiency; handles both classification and regression, and supports distributed training setups.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif args.n_gpu > 1:\n    model = torch.nn.DataParallel(model)\n\n# Eval!\nlogger.info(\"***** Running evaluation {} *****\".format(prefix))\nlogger.info(\"  Num examples = %d\", len(eval_dataset))\nlogger.info(\"  Batch size = %d\", args.eval_batch_size)\nnb_eval_steps = 0\npreds = None\nout_label_ids = None\nfor batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n    model.eval()\n    batch = tuple(t.to(args.device) for t in batch)\n\n    with torch.no_grad():\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1]}\n        labels = batch[3]\n        if args.model_type != 'distilbert':\n            inputs['input'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n        outputs = model(**inputs)\n        logits = outputs[0]\n    nb_eval_steps += 1\n    if preds is None:\n        preds = logits.detach().cpu().numpy()\n        out_label_ids = labels.detach().cpu().numpy()\n    else:\n        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n        out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)\n\nif args.output_mode == \"classification\":\n    preds = np.argmax(preds, axis=1)\nelif args.output_mode == \"regression\":\n    preds = np.squeeze(preds)\nresult = compute_metrics(eval_task, preds, out_label_ids)\nresults.update(result)\n\noutput_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\nwith open(output_eval_file, \"w\") as writer:\n    logger.info(\"***** Eval results {} *****\".format(prefix))\n    for key in sorted(result.keys()):\n        logger.info(\"  %s = %s\", key, str(result[key]))\n        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\nreturn results\n\ndef load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n        'dev' if evaluate else 'train',\n        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n        str(args.max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(\"Loading features from cached file %s\", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n            # HACK(label indices are swapped in RoBERTa pretrained model)\n            label_list[1], label_list[2] = label_list[2], label_list[1]\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples,\n                                                tokenizer,\n                                                label_list=label_list,\n                                                max_length=args.max_seq_length,\n                                                output_mode=output_mode,)\n        if args.local_rank in [-1, 0]:\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == \"classification\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == \"regression\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset\n\ndef time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n    eval_duration_time = eval_end_time - eval_start_time\n    print(result)\n    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\n```\n\n----------------------------------------\n\nTITLE: Setting QConfigMapping Globally - PyTorch - Python\nDESCRIPTION: This snippet illustrates setting a global qconfig mapping for model quantization in PyTorch using the QConfigMapping class. The \"set_global\" method assigns a default qconfig to all modules unless overridden. The only dependency is the appropriate qconfig object (such as produced by get_default_qconfig) and the QConfigMapping class from PyTorch. Input is the default_qconfig, and the output is a mapping object that guides subsequent quantization steps.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_ptq_static.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n  qconfig_mapping = QConfigMapping.set_global(default_qconfig)\n```\n\n----------------------------------------\n\nTITLE: Importing C++ Custom Operator Extension in Python\nDESCRIPTION: Illustrates how to import the compiled C++ custom operator module '_C' from a Python package's __init__.py, ensuring that the registration logic for the operators is executed. Requires prior compilation and setup of the C++ extension module '_C'. This approach is typical for hybrid C++/Python PyTorch extensions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# in, say, extension/__init__.py\nfrom . import _C\n\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Neural Network Module in Python\nDESCRIPTION: Simple neural network module implementation in Python that creates a linear transformation with bias. Uses torch.nn.Module as base class and demonstrates parameter registration.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass Net(torch.nn.Module):\n  def __init__(self, N, M):\n    super(Net, self).__init__()\n    self.W = torch.nn.Parameter(torch.randn(N, M))\n    self.b = torch.nn.Parameter(torch.randn(M))\n\n  def forward(self, input):\n    return torch.addmm(self.b, input, self.W)\n```\n\n----------------------------------------\n\nTITLE: Modular Embedding and Decoder Classes for Model Parallel RNNs (Python)\nDESCRIPTION: Defines EmbeddingTable and Decoder neural network modules to support distributed RNN model parallelism. EmbeddingTable wraps an embedding layer and moves it to GPU, applies dropout, and serves encoding in forward; Decoder wraps a linear decoder for mapping hidden state to output classes, initializing weights and biases. Both modules are compatible with remote construction using RPC, depend on torch.nn, and expect numeric token counts for embedding/decoding dimensions. Inputs are ntoken, ninp/nhid, dropout (constructor); for forward(), input/output are tensors. Forward methods output processed tensor results, handling device conversion as needed.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass EmbeddingTable(nn.Module):\n    r\"\"\"\n    Encoding layers of the RNNModel\n    \"\"\"\n    def __init__(self, ntoken, ninp, dropout):\n        super(EmbeddingTable, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp).cuda()\n        self.encoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, input):\n        return self.drop(self.encoder(input.cuda()).cpu()\n\n\nclass Decoder(nn.Module):\n    def __init__(self, ntoken, nhid, dropout):\n        super(Decoder, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, output):\n        return self.decoder(self.drop(output))\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Helper Functions for Model Evaluation\nDESCRIPTION: This code defines the AverageMeter class, a helper function used for computing and storing average and current values during model evaluation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass AverageMeter(object): \n    \"\"\"Computes and stores the average and current value\"\"\" \n    def __init__(self, name, fmt=':f'): \n        self.name = name  \n        self.fmt = fmt  \n        self.reset()  \n\n    def reset(self):  \n        self.val = 0  \n        self.avg = 0  \n        self.sum = 0  \n        self.count = 0  \n\n    def update(self, val, n=1): \n        self.val = val  \n        self.sum += val * n \n        self.count += n\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Neural Network API Components in Python\nDESCRIPTION: Imports for PyTorch's neural network API, including autograd for automatic differentiation, nn for neural network layers, functional for activation functions, and optim for optimization algorithms. Also includes JIT utilities for hybrid frontend.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch.autograd as autograd         # computation graph\nfrom torch.autograd import Variable       # variable node in computation graph\nimport torch.nn as nn                     # neural networks\nimport torch.nn.functional as F           # layers, activations and more\nimport torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\nfrom torch.jit import script, trace       # hybrid frontend decorator and tracing jit\n```\n\n----------------------------------------\n\nTITLE: Capturing Backward Hooks with Compiled Autograd in PyTorch\nDESCRIPTION: Shows that Compiled Autograd can capture backward hooks registered via `tensor.register_hook()`, a capability missing in standard AOTAutograd. The example registers a simple hook, calculates loss, and performs the backward pass within the Compiled Autograd context manager (`aot_eager` backend). The resulting compiled backward graph includes the hook execution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/compiled_autograd_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile(backend=\"aot_eager\")\ndef fn(x):\n   return x.sum()\n\nx = torch.randn(10, 10, requires_grad=True)\nx.register_hook(lambda grad: grad+10)\nloss = fn(x)\n\nwith torch._dynamo.compiled_autograd.enable(torch.compile(backend=\"aot_eager\")):\n   loss.backward()\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Transformer Blocks Using PyTorch Tensor Parallel APIs - Python\nDESCRIPTION: This snippet demonstrates how to apply a tensor parallel plan to each TransformerBlock in a model using the parallelize_module API. It adapts the number of attention heads and key-value heads according to the parallel device mesh size and applies the parallelization plan. Required dependencies include an initialized model with layers, a defined tp_mesh, and a suitable parallelize_module function. Inputs are per-layer plans and a model, resulting in distributed parameter placement for scalable training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor layer_id, transformer_block in enumerate(model.layers):\n    layer_tp_plan = {...}  # i.e. the plan we just generated\n\n    # Adjust attention module to use the local number of heads\n    attn_layer = transformer_block.attention\n    attn_layer.n_heads = attn_layer.n_heads // tp_mesh.size()\n    attn_layer.n_kv_heads = attn_layer.n_kv_heads // tp_mesh.size()\n\n    parallelize_module(\n        module=transformer_block,\n        device_mesh=tp_mesh,\n        parallelize_plan=layer_tp_plan,\n    )\n```\n\n----------------------------------------\n\nTITLE: Listing Concrete PyTorch Sampler Types in Python\nDESCRIPTION: This snippet lists several concrete implementations of the `Sampler` class provided by PyTorch. These include `SequentialSampler`, `RandomSampler`, `SubsetRandomSampler`, `WeightedRandomSampler`, `BatchSampler`, and `DistributedSampler`. Each 'XSampler' offers a different method for selecting data indices, catering to various training scenarios like sequential processing, random shuffling, handling imbalanced datasets, or distributed training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nsampler.XSampler where ...                  # Sequential, Random, SubsetRandom,\n                                                # WeightedRandom, Batch, Distributed\n```\n\n----------------------------------------\n\nTITLE: Defining a Neural Network with Linear Layers\nDESCRIPTION: Creates a neural network class with linear layers that resembles parameters found in large language models, designed to demonstrate performance optimization on Graviton processors.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyNeuralNetwork(nn.Module):\n  def __init__(self):\n      super().__init__()\n      self.flatten = nn.Flatten()\n      self.linear_relu_stack = nn.Sequential(\n          nn.Linear(4096, 4096),\n          nn.ReLU(),\n          nn.Linear(4096, 11008),\n          nn.ReLU(),\n          nn.Linear(11008, 10),\n      )\n\n  def forward(self, x):\n      x = self.flatten(x)\n      logits = self.linear_relu_stack(x)\n      return logits\n```\n\n----------------------------------------\n\nTITLE: Annotating compute() as TorchScript Script Function - Python\nDESCRIPTION: Wraps the compute function with @torch.jit.script to enable just-in-time compilation and integration into the TorchScript IR. It supports the same input/output as before but yields a compiled function suitable for TorchScript serialization or introspection. Requires PyTorch to be installed.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z\n```\n\n----------------------------------------\n\nTITLE: Initializing FSDP Model on Specific Device in PyTorch\nDESCRIPTION: This snippet shows how to initialize an FSDP model on a specific GPU device. It's useful when the entire model doesn't fit on a single GPU but fits in CPU memory. The model is moved to the specified device on a per-FSDP unit basis.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntorch.cuda.set_device(local_rank)\n\nmodel = FSDP(model,\n       auto_wrap_policy=t5_auto_wrap_policy,\n       mixed_precision=bfSixteen,\n       device_id=torch.cuda.current_device())\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PyTorch Quantization\nDESCRIPTION: This snippet imports necessary Python libraries and PyTorch modules for implementing quantization. It also sets up warnings and specifies a random seed for reproducibility.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport sys\nimport time\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\n# Set up warnings\nimport warnings \nwarnings.filterwarnings(  \n    action='ignore',  \n    category=DeprecationWarning,  \n    module=r'.*'  \n) \nwarnings.filterwarnings(  \n    action='default', \n    module=r'torch.ao.quantization'\n) \n\n# Specify random seed for repeatable results  \ntorch.manual_seed(191009)\n```\n\n----------------------------------------\n\nTITLE: Tracing PyTorch Function with Custom Operator\nDESCRIPTION: Python code showing how to define a PyTorch function using the custom operator and trace it with torch.jit.trace for use in TorchScript.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../advanced_source/torch_script_custom_ops/test.py\n  :language: python\n  :start-after: BEGIN compute2\n  :end-before: END compute2\n```\n\nLANGUAGE: python\nCODE:\n```\n.. literalinclude:: ../advanced_source/torch_script_custom_ops/test.py\n  :language: python\n  :start-after: BEGIN trace2\n  :end-before: END trace2\n```\n\n----------------------------------------\n\nTITLE: Defining Conv2d+ReLU Module Fusion Function in Python\nDESCRIPTION: This snippet defines a fusion function that takes Conv2d and ReLU modules and fuses them into a ConvReLU2d module using PyTorch's intrinsic modules. The function is intended for use with the BackendConfig fusion step and must accept a boolean flag for QAT alongside the modules to be fused. Ensure 'torch.ao.nn.intrinsic' is available.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef fuse_conv2d_relu(is_qat, conv, relu):\n    \"\"\"Return a fused ConvReLU2d from individual conv and relu modules.\"\"\"\n    return torch.ao.nn.intrinsic.ConvReLU2d(conv, relu)\n```\n\n----------------------------------------\n\nTITLE: Generated Default Python Wrapper Code for GPU\nDESCRIPTION: Displays the standard Python code generated by TorchInductor's default wrapper for GPU execution. The `call` function sets the CUDA device context, allocates tensors on the GPU (`empty_strided`), retrieves the CUDA stream, and launches a Triton kernel (`triton_poi_fused_add_lift_fresh_0.run`) using Python APIs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/inductor_cpp_wrapper_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (1, ), (1, ))\n    with torch.cuda._DeviceGuard(0):\n        torch.cuda.set_device(0) # no-op to ensure context\n        buf0 = empty_strided((19, ), (1, ), device='cuda', dtype=torch.float32)\n        # Source Nodes: [add, tensor], Original ATen: [aten.add, aten.lift_fresh]\n        stream0 = get_cuda_stream(0)\n        triton_poi_fused_add_lift_fresh_0.run(constant0, arg0_1, buf0, 19, grid=grid(19), stream=stream0)\n        run_intermediate_hooks('add', buf0)\n        del arg0_1\n        return (buf0, )\n```\n\n----------------------------------------\n\nTITLE: C++ Implementation of Sigmoid Derivative\nDESCRIPTION: C++ implementation of the sigmoid derivative function using the ATen library, which is part of the PyTorch C++ API.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/extension.h>\n\n#include <iostream>\n\ntorch::Tensor d_sigmoid(torch::Tensor z) {\n  auto s = torch::sigmoid(z);\n  return (1 - s) * s;\n}\n```\n\n----------------------------------------\n\nTITLE: Markdown Table Structure - H1 2024 Leaderboard\nDESCRIPTION: A markdown table showing contributor rankings with columns for author names, points earned, and links to their pull requests for the H1 2024 Docathon.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/docathon-leaderboard.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Author | Points | PR |\n|--- | --- | ---|\n| ahoblitz | 34 | https://github.com/pytorch/pytorch/pull/128566, https://github.com/pytorch/pytorch/pull/128408, https://github.com/pytorch/pytorch/pull/128171, https://github.com/pytorch/pytorch/pull/128083, https://github.com/pytorch/pytorch/pull/128082, https://github.com/pytorch/pytorch/pull/127983, https://github.com/pytorch/xla/pull/7214 |\n```\n\n----------------------------------------\n\nTITLE: Markdown Table Structure - H2 2023 Leaderboard\nDESCRIPTION: A markdown table showing contributor rankings with columns for author names, points earned, and links to their pull requests for the H2 2023 Docathon.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/docathon-leaderboard.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Author | Points | PR |\n|--- | --- | ---|\n| ahoblitz | 25 | https://github.com/pytorch/pytorch/pull/112992, https://github.com/pytorch/tutorials/pull/2662, https://github.com/pytorch/tutorials/pull/2647, https://github.com/pytorch/tutorials/pull/2642, https://github.com/pytorch/tutorials/pull/2640, https://github.com/pytorch/pytorch/pull/113092, https://github.com/pytorch/pytorch/pull/113348 |\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Android Test App with Vulkan (Shell/Gradle)\nDESCRIPTION: Shell command using Gradle to build and install the debug variant (`MbvulkanLocalBaseDebug`) of the PyTorch Android test application onto a connected Android device. Assumes the PyTorch Android AARs with Vulkan support have been built and the model asset has been copied.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\ncd $PYTORCH_ROOT\ngradle -p android test_app:installMbvulkanLocalBaseDebug\n```\n\n----------------------------------------\n\nTITLE: Creating a Configured Data Loader in PyTorch C++\nDESCRIPTION: Illustrates creating a PyTorch C++ data loader with custom configurations using `torch::data::DataLoaderOptions`. This example sets the batch size using a constant `kBatchSize` (e.g., 64) and specifies 2 worker threads for parallel data loading, potentially speeding up training. Requires a `dataset` object and the `kBatchSize` constant defined.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_20\n\nLANGUAGE: cpp\nCODE:\n```\n  auto data_loader = torch::data::make_data_loader(\n      std::move(dataset),\n      torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2));\n```\n\n----------------------------------------\n\nTITLE: Original PyTorch Module with Non-Traceable Code\nDESCRIPTION: Defines a PyTorch `nn.Module` containing a mix of traceable and non-traceable code sections within its `forward` method. This represents the initial state before applying strategies to handle non-traceable parts for FX quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass M(nn.Module):\n    def forward(self, x):\n        x = non_traceable_code_1(x)\n        x = traceable_code(x)\n        x = non_traceable_code_2(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Accessing Scale and Zero Point in Quantized Model (Python)\nDESCRIPTION: This code demonstrates how to access the scale and zero point values for a specific weight in the quantized BERT model. It uses per-channel quantization, resulting in per-channel scale tensors.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(quantized_model.bert.encoder.layer._c.getattr('0').attention.self.query.getattr('4_scale_0'))\nprint(quantized_model.bert.encoder.layer._c.getattr('0').attention.self.query.getattr('4_zero_point_0'))\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for PyTorch Custom Ops Project\nDESCRIPTION: This CMake script sets up a project for custom PyTorch operations. It specifies the minimum CMake version, defines the project, finds the Torch package, and configures an executable named 'transformer_ts' with necessary libraries and C++14 standard.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/transformer__timeseries_cpp_tutorial/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(custom_ops)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(transformer_ts transformer_timeseries.cpp)\ntarget_link_libraries(transformer_ts \"${TORCH_LIBRARIES}\")\nset_property(TARGET transformer_ts PROPERTY CXX_STANDARD 14)\n```\n\n----------------------------------------\n\nTITLE: Inference with ResNet50 in BF16 using Intel Extension for PyTorch\nDESCRIPTION: Shows how to perform inference using a pre-trained ResNet50 model with BFloat16 precision using Intel Extension for PyTorch backend. Uses autocast for mixed precision inference.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_compile_backend_ipex.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(weights='ResNet50_Weights.DEFAULT')\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nimport intel_extension_for_pytorch as ipex\n\n# Invoke the following API optionally, to apply frontend optimizations\nmodel = ipex.optimize(model, dtype=torch.bfloat16, weights_prepack=False)\n\ncompile_model = torch.compile(model, backend=\"ipex\")\n\nwith torch.no_grad(), torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n    compile_model(data)\n```\n\n----------------------------------------\n\nTITLE: Building a C++ Application with Custom TorchScript Operator\nDESCRIPTION: Shell commands to build a C++ application that uses a custom TorchScript operator, including finding the PyTorch CMake path and compilation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\n$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/example_app/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 25%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o\n[ 50%] Linking CXX shared library libwarp_perspective.so\n[ 50%] Built target warp_perspective\nScanning dependencies of target example_app\n[ 75%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app\n```\n\n----------------------------------------\n\nTITLE: Exporting Custom Class Error Example - Python\nDESCRIPTION: Example showing the error when trying to save a ScriptModule with a custom-bound C++ class without defined serialization methods.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ python export_attr.py\nRuntimeError: Cannot serialize custom bound C++ class __torch__.torch.classes.my_classes.MyStackClass. Please define serialization methods via def_pickle for this class. (pushIValueImpl at ../torch/csrc/jit/pickler.cpp:128)\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantization-Aware Training with X86InductorQuantizer\nDESCRIPTION: Complete example demonstrating the PyTorch 2 Export Quantization-Aware Training flow using X86InductorQuantizer. The process includes program capture, QAT preparation, model conversion, and lowering into Inductor for optimized execution on X86 CPU.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch._export import capture_pre_autograd_graph\nfrom torch.ao.quantization.quantize_pt2e import (\n  prepare_qat_pt2e,\n  convert_pt2e,\n)\nimport torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq\nfrom torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer\n\nclass M(torch.nn.Module):\n   def __init__(self):\n      super().__init__()\n      self.linear = torch.nn.Linear(1024, 1000)\n\n   def forward(self, x):\n      return self.linear(x)\n\nexample_inputs = (torch.randn(1, 1024),)\nm = M()\n\n# Step 1. program capture\n# NOTE: this API will be updated to torch.export API in the future, but the captured\n# result shoud mostly stay the same\nexported_model = capture_pre_autograd_graph(m, example_inputs)\n# we get a model with aten ops\n\n# Step 2. quantization-aware training\n# Use Backend Quantizer for X86 CPU\n# To apply dynamic quantization, add an argument ``is_dynamic=True`` when getting the config.\nquantizer = X86InductorQuantizer()\nquantizer.set_global(xiq.get_default_x86_inductor_quantization_config(is_qat=True))\nprepared_model = prepare_qat_pt2e(exported_model, quantizer)\n\n# train omitted\n\nconverted_model = convert_pt2e(prepared_model)\n# we have a model with aten ops doing integer computations when possible\n\n# move the quantized model to eval mode, equivalent to `m.eval()`\ntorch.ao.quantization.move_exported_model_to_eval(converted_model)\n\n# Lower the model into Inductor\nwith torch.no_grad():\n  optimized_model = torch.compile(converted_model)\n  _ = optimized_model(*example_inputs)\n```\n\n----------------------------------------\n\nTITLE: Building Custom Operator with CMake\nDESCRIPTION: Shell commands to create a build directory, run CMake configuration, and compile the custom operator into a shared library using Make.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n$ make -j\n```\n\n----------------------------------------\n\nTITLE: Adding PyTorch Entry Point in torch_npu setup (Diff)\nDESCRIPTION: Shows a diff for torch_npu's setup.py to add the entry point required for device backend autoloading, specifying torch_npu's _autoload function. This ensures the extension can be discovered and loaded automatically by PyTorch at runtime. Requires setup function from setuptools and the _autoload function defined in torch_npu. The snippet documents only the modifications relevant to extension registration.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/python_extension_autoload.rst#2025-04-22_snippet_6\n\nLANGUAGE: diff\nCODE:\n```\nsetup(\n    name=\"torch_npu\",\n    version=\"2.5\",\n+   entry_points={\n+       'torch.backends': [\n+           'torch_npu = torch_npu:_autoload',\n+       ],\n+   }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing DistributedModelParallel in Multiprocessing for PyTorch\nDESCRIPTION: This code defines a function for single rank execution in a distributed setup, using DistributedModelParallel to shard the model across multiple processes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/sharding.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef single_rank_execution(\n    rank: int,\n    world_size: int,\n    constraints: Dict[str, ParameterConstraints],\n    module: torch.nn.Module,\n    backend: str,\n) -> None:\n    import os\n    import torch\n    import torch.distributed as dist\n    from torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\n    from torchrec.distributed.model_parallel import DistributedModelParallel\n    from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology\n    from torchrec.distributed.types import ModuleSharder, ShardingEnv\n    from typing import cast\n\n    def init_distributed_single_host(\n        rank: int,\n        world_size: int,\n        backend: str,\n        # pyre-fixme[11]: Annotation `ProcessGroup` is not defined as a type.\n    ) -> dist.ProcessGroup:\n        os.environ[\"RANK\"] = f\"{rank}\"\n        os.environ[\"WORLD_SIZE\"] = f\"{world_size}\"\n        dist.init_process_group(rank=rank, world_size=world_size, backend=backend)\n        return dist.group.WORLD\n\n    if backend == \"nccl\":\n        device = torch.device(f\"cuda:{rank}\")\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n    topology = Topology(world_size=world_size, compute_device=\"cuda\")\n    pg = init_distributed_single_host(rank, world_size, backend)\n    planner = EmbeddingShardingPlanner(\n        topology=topology,\n        constraints=constraints,\n    )\n    sharders = [cast(ModuleSharder[torch.nn.Module], EmbeddingBagCollectionSharder())]\n    plan: ShardingPlan = planner.collective_plan(module, sharders, pg)\n\n    sharded_model = DistributedModelParallel(\n        module,\n        env=ShardingEnv.from_process_group(pg),\n        plan=plan,\n        sharders=sharders,\n        device=device,\n    )\n    print(f\"rank:{rank},sharding plan: {plan}\")\n    return sharded_model\n```\n\n----------------------------------------\n\nTITLE: Generated Default Python Wrapper Code for CPU\nDESCRIPTION: Illustrates the Python code generated by TorchInductor's default wrapper for CPU execution. It defines a `call` function that takes tensor arguments, performs size/stride assertions, allocates an output buffer (`buf0`), and calls a fused C++ kernel (`cpp_fused_add_lift_fresh_0`) via `c_void_p`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/inductor_cpp_wrapper_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef call(args):\n    arg0_1, = args\n    args.clear()\n    assert_size_stride(arg0_1, (1, ), (1, ))\n    buf0 = empty_strided((19, ), (1, ), device='cpu', dtype=torch.float32)\n    cpp_fused_add_lift_fresh_0(c_void_p(constant0.data_ptr()), c_void_p(arg0_1.data_ptr()), c_void_p(buf0.data_ptr()))\n    del arg0_1\n    return (buf0, )\n```\n\n----------------------------------------\n\nTITLE: Installing Memory Allocators\nDESCRIPTION: Commands to install TCMalloc and JeMalloc memory allocators on different platforms\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/xeon_run_cpu.rst#2025-04-22_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ apt-get install google-perftools\n```\n\nLANGUAGE: console\nCODE:\n```\n$ yum install gperftools\n```\n\nLANGUAGE: console\nCODE:\n```\n$ conda install conda-forge::gperftools\n```\n\nLANGUAGE: console\nCODE:\n```\n$ apt-get install libjemalloc2\n```\n\nLANGUAGE: console\nCODE:\n```\n$ yum install jemalloc\n```\n\nLANGUAGE: console\nCODE:\n```\n$ conda install conda-forge::jemalloc\n```\n\n----------------------------------------\n\nTITLE: Importing Required PyTorch FSDP Dependencies\nDESCRIPTION: Imports necessary PyTorch packages including FSDP-specific modules, distributed training utilities, and dataset handling components.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport argparse\nimport functools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import (\n    CPUOffload,\n    BackwardPrefetch,\n)\nfrom torch.distributed.fsdp.wrap import (\n    size_based_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Automatic Redirection in HTML for PyTorch Tutorial\nDESCRIPTION: This HTML snippet implements a meta tag for automatic redirection. It redirects the user to a new PyTorch tutorial page after a 3-second delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/recipes/saving_multiple_models_in_one_file.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/saving_loading_models.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Defining and Compiling an Example PyTorch Function\nDESCRIPTION: Defines a simple Python function `fn` using PyTorch tensor operations and then compiles it using `torch.compile()`. This compiled function `opt_fn` is used as the basis for demonstrating the code generated by TorchInductor with different wrapper configurations (Python vs. C++).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/inductor_cpp_wrapper_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef fn(x):\n    return torch.tensor(list(range(2, 40, 2)), device=x.device) + x\n\nx = torch.randn(1)\nopt_fn = torch.compile()(fn)\ny = opt_fn(x)\n```\n\n----------------------------------------\n\nTITLE: ONNX Integration with PyTorch\nDESCRIPTION: Code snippets for exporting PyTorch models to ONNX format and working with ONNX models. This allows for interoperability between different deep learning frameworks and deployment on various platforms.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntorch.onnx.export(model, dummy data, xxxx.proto)       # exports an ONNX formatted model using a trained model, dummy data and the desired file name\nmodel = onnx.load(\"alexnet.proto\")                     # load an ONNX model\nonnx.checker.check_model(model)                        # check that the model IR is well formed\nonnx.helper.printable_graph(model.graph)               # print a human readable representation of the graph\n```\n\n----------------------------------------\n\nTITLE: Environment Variables for PyTorch Inductor Cache Configuration\nDESCRIPTION: List of environment variables used to configure PyTorch Inductor's caching behavior, including FX graph caching, AOTAutograd caching, cache directory settings, and remote Redis caching options.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_compile_caching_configuration_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nTORCHINDUCTOR_FX_GRAPH_CACHE\nTORCHINDUCTOR_AUTOGRAD_CACHE\nTORCHINDUCTOR_CACHE_DIR\nTORCHINDUCTOR_FX_GRAPH_REMOTE_CACHE\nTORCHINDUCTOR_REDIS_HOST\nTORCHINDUCTOR_REDIS_PORT\nTORCHINDUCTOR_AUTOGRAD_REMOTE_CACHE\nTORCHINDUCTOR_AUTOTUNE_REMOTE_CACHE\nTORCHINDUCTOR_FORCE_DISABLE_CACHES\n```\n\n----------------------------------------\n\nTITLE: Exporting MViT Video Classifier with Static Batch Size\nDESCRIPTION: This snippet demonstrates exporting an MViT model for video classification using torch.export. It shows the initial attempt with a static batch size, which leads to an error when trying to run the exported model with a different batch size.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_export_challenges_solutions.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom torchvision.models.video import MViT_V1_B_Weights, mvit_v1_b\nimport traceback as tb\n\nmodel = mvit_v1_b(weights=MViT_V1_B_Weights.DEFAULT)\n\n# Create a batch of 2 videos, each with 16 frames of shape 224x224x3.\ninput_frames = torch.randn(2,16, 224, 224, 3)\n# Transpose to get [1, 3, num_clips, height, width].\ninput_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\n\n# Export the model.\nexported_program = torch.export.export(\n    model,\n    (input_frames,),\n)\n\n# Create a batch of 4 videos, each with 16 frames of shape 224x224x3.\ninput_frames = torch.randn(4,16, 224, 224, 3)\ninput_frames = np.transpose(input_frames, (0, 4, 1, 2, 3))\ntry:\n    exported_program.module()(input_frames)\nexcept Exception:\n    tb.print_exc()\n```\n\n----------------------------------------\n\nTITLE: Defining an Observed Version of a Custom Module in Python\nDESCRIPTION: Shows the structure for defining an `ObservedNonTraceable` class, which represents the observed version of the custom `FP32NonTraceable` module during static quantization. It includes a `from_float` class method for conversion.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass ObservedNonTraceable:\n\n    @classmethod\n    def from_float(cls, ...):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Inference in TorchScript Mode with Float32 using Intel® Extension for PyTorch*\nDESCRIPTION: This code demonstrates how to optimize a pre-trained PyTorch ResNet50 model for inference with Float32 precision using Intel® Extension for PyTorch* (IPEX) in TorchScript mode. It traces and freezes the model to enable graph-level optimizations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/intel_extension_for_pytorch.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\nwith torch.no_grad():\n  d = torch.rand(1, 3, 224, 224)\n  model = torch.jit.trace(model, d)\n  model = torch.jit.freeze(model)\n\n  model(data)\n```\n\n----------------------------------------\n\nTITLE: Redirecting with HTML Meta Tag in Sphinx Documentation (HTML)\nDESCRIPTION: This snippet inserts a raw HTML meta refresh tag within a Sphinx documentation page to automatically redirect users to a new URL after 3 seconds. It must be placed within a '.. raw:: html' directive in reStructuredText for Sphinx to process it as raw HTML. No external dependencies are required besides Sphinx; the target URL and delay are configurable via the 'content' attribute. Limitations include relying on client-side browser support for meta refresh tags.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/tracing_based_selective_build.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/kernel-library-selective-build.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Unpickling and Printing Flight Recorder Data - Python\nDESCRIPTION: This snippet shows how to unpickle and print data gathered from the Flight Recorder using PyTorch Gathered Trace API. Using the pickle module, it deserializes the collected binary trace back into a Python object, making it readable for further analysis or debugging. Prerequisites include the Python pickle module and a valid pickled trace object produced by the previous example.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nt = pickle.loads(torch._C._distributed_c10d._dump_nccl_trace())\nprint(t)\n```\n\n----------------------------------------\n\nTITLE: Getting HTA CUDA Kernel Launch Statistics in Python\nDESCRIPTION: Shows how to initialize the HTA `TraceAnalysis` class with a trace directory path and then call `get_cuda_kernel_launch_stats` to retrieve a DataFrame containing statistics about CUDA kernel launches. This includes CPU operator duration, GPU kernel duration, and the launch delay between them, helping identify bottlenecks like short kernels or long scheduling delays.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n.. code-block:: python\n\n  analyzer = TraceAnalysis(trace_dir=\"/path/to/trace/dir\")\n  kernel_info_df = analyzer.get_cuda_kernel_launch_stats()\n```\n\n----------------------------------------\n\nTITLE: Installing TorchRec and Dependencies in Python\nDESCRIPTION: This code installs Miniconda, PyTorch with CUDA support, TorchRec, and multiprocess library. It also copies shared libraries to the correct location for the Colab runtime.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/sharding.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# install conda to make installying pytorch with cudatoolkit 11.3 easier. \n!sudo rm Miniconda3-py37_4.9.2-Linux-x86_64.sh Miniconda3-py37_4.9.2-Linux-x86_64.sh.*\n!sudo wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n!sudo chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh\n!sudo bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /usr/local\n```\n\nLANGUAGE: bash\nCODE:\n```\n# install pytorch with cudatoolkit 11.3\n!sudo conda install pytorch cudatoolkit=11.3 -c pytorch-nightly -y\n```\n\nLANGUAGE: bash\nCODE:\n```\n# install torchrec\n!pip3 install torchrec-nightly\n```\n\nLANGUAGE: bash\nCODE:\n```\n!pip3 install multiprocess\n```\n\nLANGUAGE: bash\nCODE:\n```\n!sudo cp /usr/local/lib/lib* /usr/lib/\n```\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nsys.path = ['', '/env/python', '/usr/local/lib/python37.zip', '/usr/local/lib/python3.7', '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages', './.local/lib/python3.7/site-packages']\n```\n\n----------------------------------------\n\nTITLE: Configuring XPUInductorQuantizer with Default Quantization (Python)\nDESCRIPTION: This code creates an XPUInductorQuantizer instance and configures it with the default quantization settings for Intel GPU backend. The default config applies signed 8-bit quantization for activations and weights, using per-tensor quantization for activations and per-channel quantization for weights. No inputs or outputs are produced here; it sets up the quantization policy to be used in subsequent model preparation steps. Dependencies: torch.ao.quantization.quantizer.xpu_inductor_quantizer.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nquantizer = XPUInductorQuantizer()\nquantizer.set_global(xpuiq.get_default_xpu_inductor_quantization_config())\n\n```\n\n----------------------------------------\n\nTITLE: Configuring TorchServe Launcher for PTMalloc (Default Allocator)\nDESCRIPTION: These lines are intended for TorchServe's `config.properties` file. They configure the CPU launcher to be enabled and specify arguments to bind the workload to the first socket (node_id 0) and explicitly use the default memory allocator (PTMalloc).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchserve_with_ipex_2.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncpu_launcher_enable=true\ncpu_launcher_args=--node_id 0 --use_default_allocator\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Redirect for PyTorch Tutorial\nDESCRIPTION: This HTML snippet creates a meta refresh tag to automatically redirect the user to the new location of the Finetuning Torchvision Models tutorial after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/finetuning_torchvision_models_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Initializing Observer with Environment and RPC Setup in Python\nDESCRIPTION: Defines the constructor for the `Observer` class. It retrieves the worker's ID using `rpc.get_worker_info()`, initializes a 'CartPole-v1' environment from OpenAI Gym, seeds the environment using the parsed arguments, and sets the `select_action` attribute to point to either the batched (`Agent.select_action_batch`) or non-batched (`Agent.select_action`) RPC method on the `Agent`, depending on the `batch` parameter.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport gym\nimport torch.distributed.rpc as rpc\n\nclass Observer:\n    def __init__(self, batch=True):\n        self.id = rpc.get_worker_info().id - 1\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n        self.select_action = Agent.select_action_batch if batch else Agent.select_action\n```\n\n----------------------------------------\n\nTITLE: Updating PyTorch and Related Libraries in Google Colab\nDESCRIPTION: This snippet demonstrates how to uninstall the current version of PyTorch and related libraries, then reinstall the latest versions. This is useful when the installed version is lower than required for a tutorial.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/colab.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n!pip3 install torch torchaudio torchvision torchtext torchdata\n```\n\n----------------------------------------\n\nTITLE: Shell Commands to Build the Example LibTorch C++ Application\nDESCRIPTION: This shell script demonstrates the common commands to build the C++ example application using CMake. It requires that CMake and LibTorch are properly installed. /path/to/libtorch must be the full path to the LibTorch directory. The commands create a build directory, run cmake to configure with the LibTorch prefix, and build the application in Release mode.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_export.rst#2025-04-22_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nmkdir build\\ncd build\\ncmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\\ncmake --build . --config Release\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Class for Distributed Reinforcement Learning in Python\nDESCRIPTION: This snippet defines an Agent class with a run_episode method that coordinates actions across multiple observers using RPC calls. It handles reward collection, policy loss calculation, and state resets.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Agent:\n    ...\n\n    def run_episode(self, n_steps=0):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(ob_rref.rpc_async().run_episode(self.agent_rref, n_steps))\n\n        # wait until all obervers have finished this episode\n        rets = torch.futures.wait_all(futs)\n        rewards = torch.stack([ret[0] for ret in rets]).cuda().t()\n        ep_rewards = sum([ret[1] for ret in rets]) / len(rets)\n\n        # stack saved probs into one tensor\n        if self.batch:\n            probs = torch.stack(self.saved_log_probs)\n        else:\n            probs = [torch.stack(self.saved_log_probs[i]) for i in range(len(rets))]\n            probs = torch.stack(probs)\n\n        policy_loss = -probs * rewards / len(rets)\n        policy_loss.sum().backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n\n        # reset variables\n        self.saved_log_probs = [] if self.batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n\n        # calculate running rewards\n        self.running_reward = 0.5 * ep_rewards + 0.5 * self.running_reward\n        return ep_rewards, self.running_reward\n```\n\n----------------------------------------\n\nTITLE: Initializing Packages and Global State - PyTorch, Transformers, and Others - Python\nDESCRIPTION: This snippet initializes all required packages and sets a global manual seed for reproducibility. Dependencies include core PyTorch (nn, sparse), HuggingFace Transformers and Datasets, NumPy, and others. It ensures the use of CUTLASS if cuSPARSELt is unavailable, providing compatibility with NVIDIA GPUs for sparse operations. All imports are explicit, and the random seed is fixed for consistent results throughout the tutorial.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/semi_structured_sparse.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport collections\nimport datasets\nimport evaluate\nimport numpy as np\nimport torch\nimport torch.utils.benchmark as benchmark\nfrom torch import nn\nfrom torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor\nfrom torch.ao.pruning import WeightNormSparsifier\nimport transformers\n\n# force CUTLASS use if cuSPARSELt is not available\nSparseSemiStructuredTensor._FORCE_CUTLASS = True\ntorch.manual_seed(100)\n\n```\n\n----------------------------------------\n\nTITLE: Basic PyTorch C++ Tensor Operation\nDESCRIPTION: Demonstrates a simple C++ program using PyTorch to create and print a 3x3 identity matrix tensor.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/torch.h>\n#include <iostream>\n\nint main() {\n  torch::Tensor tensor = torch::eye(3);\n  std::cout << tensor << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Redirect to ExecuTorch\nDESCRIPTION: This HTML snippet creates an automatic page redirect that will navigate users to the ExecuTorch documentation after a 3-second delay. It's included as a raw HTML element within the RST document.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/mobile_perf.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Building MNIST Example with CMake in PyTorch C++ Frontend\nDESCRIPTION: Shell commands for building the MNIST example using CMake. This process involves creating a build directory, configuring CMake with the path to LibTorch, and compiling the code with make.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_cuda_graphs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ cd mnist\n$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n$ make\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Quantized Engine to QNNPACK (Python)\nDESCRIPTION: Configures PyTorch to use the 'qnnpack' backend for quantized operations. This is necessary for running quantized models efficiently on ARM 64-bit architectures like the Raspberry Pi 4. Requires `torch` to be imported.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\ntorch.backends.quantized.engine = 'qnnpack'\n```\n\n----------------------------------------\n\nTITLE: Retrieving HTA Memory/Queue Summaries and Time Series in Python\nDESCRIPTION: Demonstrates how to use an existing HTA `TraceAnalysis` object (`analyzer`) to fetch summary statistics (count, min, max, mean, std dev, percentiles) and time series data for memory bandwidth and CUDA queue length. This requires the trace analysis to have been performed previously, potentially using `generate_trace_with_counters`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n.. code-block:: python\n\n  # generate summary\n  mem_bw_summary = analyzer.get_memory_bw_summary()\n  queue_len_summary = analyzer.get_queue_length_summary()\n\n  # get time series\n  mem_bw_series = analyzer.get_memory_bw_time_series()\n  queue_len_series = analyzer.get_queue_length_series()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Autograd Function in PyTorch\nDESCRIPTION: An example showcasing how to define custom autograd functions by implementing a Legendre polynomial function. This demonstrates subclassing torch.autograd.Function and implementing forward and backward methods.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/pytorch_with_examples.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\nclass LegendrePolynomial3(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output. ctx is a context object that can be used\n        to stash information for backward computation. You can cache arbitrary\n        objects for use in the backward pass using the ctx.save_for_backward method.\n        \"\"\"\n        ctx.save_for_backward(input)\n        return 0.5 * (5 * input ** 3 - 3 * input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        input, = ctx.saved_tensors\n        return grad_output * 1.5 * (5 * input ** 2 - 1)\n\n\n# Create Tensors to hold input and outputs.\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n\n# Create random Tensors for weights. For this example, we need\n# 4 weights: y = a + b * P3(c + d * x), where P3(x) is the\n# Legendre polynomial of degree 3.\na = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nb = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nc = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nd = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n\n# Create an instance of our custom autograd function\nP3 = LegendrePolynomial3.apply\n\n# Generate some data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Standard training loop\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y using operations involving Tensors\n    y_pred = a + b * P3(c + d * x)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Use autograd to compute the backward pass.\n    loss.backward()\n\n    # Update weights using gradient descent\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Backend Device Module in PyTorch (Python)\nDESCRIPTION: Registers a custom backend module (e.g., `torch_npu.npu` for Ascend NPU) with PyTorch under a specific device name (e.g., 'npu'). This allows users to access backend-specific APIs using the familiar `torch.<device_name>.xxx` pattern, enhancing usability by mimicking the `torch.cuda.xxx` interface.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntorch._register_device_module('npu', torch_npu.npu)\n```\n\n----------------------------------------\n\nTITLE: Tensor Algebra Operations in PyTorch\nDESCRIPTION: Basic linear algebra operations for tensors in PyTorch, including matrix multiplication and transposition. These operations are fundamental for implementing neural network layers and computations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nA.mm(B)       # matrix multiplication\nA.mv(x)       # matrix-vector multiplication\nx.t()         # matrix transpose\n```\n\n----------------------------------------\n\nTITLE: Main DDP Training Loop\nDESCRIPTION: Main function implementing the distributed training loop with process spawning and cleanup.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ddp_series_multigpu.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef main(rank, world_size, total_epochs, save_every):\n   ddp_setup(rank, world_size)\n   dataset, model, optimizer = load_train_objs()\n   train_data = prepare_dataloader(dataset, batch_size=32)\n   trainer = Trainer(model, train_data, optimizer, rank, save_every)\n   trainer.train(total_epochs)\n   destroy_process_group()\n\nif __name__ == \"__main__\":\n   import sys\n   total_epochs = int(sys.argv[1])\n   save_every = int(sys.argv[2])\n   world_size = torch.cuda.device_count()\n   mp.spawn(main, args=(world_size, total_epochs, save_every,), nprocs=world_size)\n```\n\n----------------------------------------\n\nTITLE: Using TorchScript and JIT in PyTorch\nDESCRIPTION: Examples of using TorchScript for tracing and scripting PyTorch models. These utilities allow for optimizing PyTorch models for production environments and creating serializable and optimizable versions of PyTorch code.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntorch.jit.trace()         # takes your module or function and an example data input, and traces the computational steps that the data encounters as it progresses through the model\n@script                   # decorator used to indicate data-dependent control flow within the code being traced\n```\n\n----------------------------------------\n\nTITLE: Snapshot Management Implementation\nDESCRIPTION: Implementation of snapshot saving and loading functionality for fault-tolerant training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ddp_series_fault_tolerance.rst#2025-04-22_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n+ def _save_snapshot(self, epoch):\n+     snapshot = {}\n+     snapshot[\"MODEL_STATE\"] = self.model.module.state_dict()\n+     snapshot[\"EPOCHS_RUN\"] = epoch\n+     torch.save(snapshot, \"snapshot.pt\")\n+     print(f\"Epoch {epoch} | Training snapshot saved at snapshot.pt\")\n\n+ def _load_snapshot(self, snapshot_path):\n+     snapshot = torch.load(snapshot_path)\n+     self.model.load_state_dict(snapshot[\"MODEL_STATE\"])\n+     self.epochs_run = snapshot[\"EPOCHS_RUN\"]\n+     print(f\"Resuming training from snapshot at Epoch {self.epochs_run}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for a Custom TorchScript Operator\nDESCRIPTION: CMake configuration file for building a custom TorchScript operator as a shared library with OpenCV dependencies.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_20\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(OpenCV REQUIRED)\nadd_library(warp_perspective SHARED op.cpp)\ntarget_compile_features(warp_perspective PRIVATE cxx_range_for)\ntarget_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)\n```\n\n----------------------------------------\n\nTITLE: Collecting Parameter RRefs for Distributed Optimization in PyTorch\nDESCRIPTION: Helper function that creates RRefs for model parameters, which will be used by the distributed optimizer. This is necessary because some parameters live on remote machines in distributed training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef _parameter_rrefs(module):\n    param_rrefs = []\n    for param in module.parameters():\n        param_rrefs.append(RRef(param))\n    return param_rrefs\n```\n\n----------------------------------------\n\nTITLE: Autocast Kernel Implementation\nDESCRIPTION: Implementation of an autocast kernel for a custom matmul operator that handles automatic mixed precision (AMP) support. Includes helper functions and registration code.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/dispatcher.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// Autocast-specific helper functions\n#include <ATen/autocast_mode.h>\n\nTensor mymatmul_autocast(const Tensor& self, const Tensor& other) {\n  c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast);\n  return mymatmul(at::autocast::cached_cast(at::kHalf, self),\n                  at::autocast::cached_cast(at::kHalf, other));\n}\n\nTORCH_LIBRARY_IMPL(myops, Autocast, m) {\n  m.impl(\"mymatmul\", mymatmul_autocast);\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Generated Images\nDESCRIPTION: Shell command for running the Python display script to visualize the generated images from a checkpoint file.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\nroot@3c0711f20896:/home/build# python display.py -i dcgan-sample-100.pt\nSaved out.png\n```\n\n----------------------------------------\n\nTITLE: Optimized Ensemble Model with List Comprehensions\nDESCRIPTION: A more concise implementation of the parallelized ensemble model using Python list comprehensions. This achieves the same parallel execution of models but with more compact syntax.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch-script-parallelism.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x : torch.Tensor) -> torch.Tensor:\n    futures = [torch.jit.fork(model, x) for model in self.models]\n    results = [torch.jit.wait(future) for future in futures]\n    return torch.stack(results).sum(dim=0)\n```\n\n----------------------------------------\n\nTITLE: Complete Transformer Block Parallelization Plan\nDESCRIPTION: Defines the complete tensor parallel plan for both Attention and FeedForward layers in a Transformer block.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/TP_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlayer_tp_plan = {\n    \"attention.wq\": ColwiseParallel(),\n    \"attention.wk\": ColwiseParallel(),\n    \"attention.wv\": ColwiseParallel(),\n    \"attention.wo\": RowwiseParallel(),\n    \"feed_forward.w1\": ColwiseParallel(),\n    \"feed_forward.w2\": RowwiseParallel(),\n    \"feed_forward.w3\": ColwiseParallel(),\n}\n```\n\n----------------------------------------\n\nTITLE: Redirecting to Updated PyTorch Tutorial Using HTML Meta Refresh\nDESCRIPTION: This HTML snippet implements a meta refresh to automatically redirect the user to the updated PyTorch tutorial on saving and loading models after a 3-second delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/recipes/save_load_across_devices.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/saving_loading_models.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Redirecting to Updated PyTorch Model Saving Tutorial using HTML\nDESCRIPTION: This HTML meta tag is used to automatically redirect the user to the new tutorial page on saving and loading PyTorch models after a 3-second delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/recipes/saving_and_loading_models_for_inference.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/saving_loading_models.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Loading and Running TorchScript Model in Standalone C++ Application - C++\nDESCRIPTION: A minimal C++ program that loads a PyTorch serialized TorchScript model, prepares random tensor inputs, and runs inference. Built using the libtorch C++ API. Requires linking against libtorch, and if using custom ops, the custom operator library. Input is a file path to the serialized model; output is printed tensor result.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/script.h> // One-stop header.\n\n#include <iostream>\n#include <memory>\n\n\nint main(int argc, const char* argv[]) {\n  if (argc != 2) {\n    std::cerr << \"usage: example-app <path-to-exported-script-module>\\n\";\n    return -1;\n  }\n\n  // Deserialize the ScriptModule from a file using torch::jit::load().\n  torch::jit::script::Module module = torch::jit::load(argv[1]);\n\n  std::vector<torch::jit::IValue> inputs;\n  inputs.push_back(torch::randn({4, 8}));\n  inputs.push_back(torch::randn({8, 5}));\n\n  torch::Tensor output = module.forward(std::move(inputs)).toTensor();\n\n  std::cout << output << std::endl;\n}\n\n```\n\n----------------------------------------\n\nTITLE: Profiling Small Batch Inference Performance\nDESCRIPTION: Profiles the model's inference performance with a smaller batch size (32) to demonstrate the performance characteristics with different workloads.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nX = torch.rand(32, 64, 64, device=device)\nwith torch.set_grad_enabled(False):\n    for _ in range(50):\n        model(X) #Warmup\n    with profile(activities=[ProfilerActivity.CPU]) as prof:\n        with record_function(\"mymodel_inference\"):\n            for _ in range(100):\n                model(X)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n```\n\n----------------------------------------\n\nTITLE: Running the HSDP Setup with TorchRun\nDESCRIPTION: Command line instruction to run the Hybrid Sharding Data Parallel setup with TorchRun, requiring 8 processes per node.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_device_mesh.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntorchrun --nproc_per_node=8 hsdp.py\n```\n\n----------------------------------------\n\nTITLE: Profiling Inference Time for ResNet50 with VTune ITT and Profiler (PyTorch, Python)\nDESCRIPTION: This snippet profiles the average inference time of a PyTorch ResNet50 model, comparing memory allocators (PTMalloc, JeMalloc, TCMalloc) while demonstrating usage of Intel VTune Profiler's Instrumentation and Tracing Technology (ITT) for op-level annotation. Required dependencies include torch, torchvision, and potentially Intel's extension packages. The batch size, data shape, and number of inference iterations are configurable, and the code uses torch.autograd.profiler.emit_itt() to enable ITT, with range_push/pop around each inference step. Expected output is a printout of average inference time in milliseconds per batch. Limitations include the need for proper VTune installation and hardware that supports all features.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torchserve_with_ipex_2.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\nimport torchvision.models as models\\nimport time\\n\\nmodel = models.resnet50(pretrained=False)\\nmodel.eval()\\nbatch_size = 32\\ndata = torch.rand(batch_size, 3, 224, 224)\\n\\n# warm up\\nfor _ in range(100):\\n    model(data)\\n\\n# measure \\n# Intel\\u00ae VTune Profiler's ITT context manager\\nwith torch.autograd.profiler.emit_itt():\\n    start = time.time()\\n    for i in range(100):\\n   # Intel\\u00ae VTune Profiler's ITT to annotate each step\\n        torch.profiler.itt.range_push('step_{}'.format(i))\\n        model(data)\\n        torch.profiler.itt.range_pop()\\n    end = time.time()\\n\\nprint('Inference took {:.2f} ms in average'.format((end-start)/100*1000))\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Defining ConvNet for MNIST in Python\nDESCRIPTION: This snippet imports necessary modules and defines a ConvNet class that will be trained on the MNIST dataset. The network can utilize multiple GPUs if available.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_param_server_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport os\nimport time\nfrom threading import Lock\n\nimport torch\nimport torch.distributed.autograd as dist_autograd\nimport torch.distributed.rpc as rpc\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.distributed.optim import DistributedOptimizer\nfrom torchvision import datasets, transforms\n\nclass Net(nn.Module):\n    def __init__(self, num_gpus=0):\n        super(Net, self).__init__()\n        print(f\"Using {num_gpus} GPUs to train\")\n        self.num_gpus = num_gpus\n        device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and self.num_gpus > 0 else \"cpu\")\n        print(f\"Putting first 2 convs on {str(device)}\")\n        # Put conv layers on the first cuda device, or CPU if no cuda device\n        self.conv1 = nn.Conv2d(1, 32, 3, 1).to(device)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1).to(device)\n        # Put rest of the network on the 2nd cuda device, if there is one\n        if \"cuda\" in str(device) and num_gpus > 1:\n            device = torch.device(\"cuda:1\")\n\n        print(f\"Putting rest of layers on {str(device)}\")\n        self.dropout1 = nn.Dropout2d(0.25).to(device)\n        self.dropout2 = nn.Dropout2d(0.5).to(device)\n        self.fc1 = nn.Linear(9216, 128).to(device)\n        self.fc2 = nn.Linear(128, 10).to(device)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        # Move tensor to next device if necessary\n        next_device = next(self.fc1.parameters()).device\n        x = x.to(next_device)\n\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Redirect for PyTorch Tutorial\nDESCRIPTION: This HTML snippet creates a meta tag that automatically redirects the user to the new tutorial location after a 3-second delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/forced_alignment_with_torchaudio_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing Learning Rate Scheduling in PyTorch\nDESCRIPTION: Examples of using learning rate schedulers in PyTorch. These schedulers adjust the learning rate during training to improve convergence and performance of the model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nscheduler = optim.X(optimizer,...)      # create lr scheduler\nscheduler.step()                        # update lr at start of epoch\noptim.lr_scheduler.X                    # where X is LambdaLR, StepLR, MultiStepLR, ExponentialLR or ReduceLROnPLateau\n```\n\n----------------------------------------\n\nTITLE: Setting Up Distributed Environment in PyTorch\nDESCRIPTION: This code sets up the environment for torch distributed based communication backend to work in a notebook environment.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/sharding.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nimport torchrec\n\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"29500\"\n```\n\n----------------------------------------\n\nTITLE: Using PyTorch Vulkan Backend in Android (Java)\nDESCRIPTION: Java code snippet for an Android application demonstrating how to load a TorchScript model and specify the Vulkan backend using `Device.VULKAN`. Input tensors created from `FloatBuffer` are automatically moved to the Vulkan device for inference via `module.forward()`, and the output tensor is automatically moved back to the CPU.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nimport org.pytorch.Device;\nModule module = Module.load(\"$PATH\", Device.VULKAN)\nFloatBuffer buffer = Tensor.allocateFloatBuffer(1 * 3 * 224 * 224);\nTensor inputTensor = Tensor.fromBlob(buffer, new int[]{1, 3, 224, 224});\nTensor outputTensor = mModule.forward(IValue.from(inputTensor)).toTensor();\n```\n\n----------------------------------------\n\nTITLE: Simulating Column-Wise Sharding with PyTorch SPMD Simulation (Python)\nDESCRIPTION: This snippet shows how to simulate column-wise sharding of embedding tables by invoking spmd_sharing_simulation with ShardingType.COLUMN_WISE. Column-wise sharding addresses load imbalance for tables with large dimensions by splitting them vertically across devices. The snippet requires the same dependencies as previous examples (spmd_sharing_simulation and ShardingType), and outputs the partitioning for inspection.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/sharding.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nspmd_sharing_simulation(ShardingType.COLUMN_WISE)\n```\n\n----------------------------------------\n\nTITLE: Image Visualization Helper Function\nDESCRIPTION: Implements helper functions for visualizing image tensors with normalization handling and matplotlib integration.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef imshow(inp, title=None, ax=None, figsize=(5, 5)):\n  \"\"\"Imshow for Tensor.\"\"\"\n  inp = inp.numpy().transpose((1, 2, 0))\n  mean = np.array([0.485, 0.456, 0.406])\n  std = np.array([0.229, 0.224, 0.225])\n  inp = std * inp + mean\n  inp = np.clip(inp, 0, 1)\n  if ax is None:\n    fig, ax = plt.subplots(1, figsize=figsize)\n  ax.imshow(inp)\n  ax.set_xticks([])\n  ax.set_yticks([])\n  if title is not None:\n    ax.set_title(title)\n```\n\n----------------------------------------\n\nTITLE: Redirecting to Latest Parallelism APIs Documentation using HTML Meta Refresh\nDESCRIPTION: This HTML snippet implements a meta refresh to automatically redirect the user to the latest parallelism APIs documentation after a 3-second delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/model_parallel_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/dist_overview.html#parallelism-apis'\" />\n```\n\n----------------------------------------\n\nTITLE: Adding Tutorial to Table of Contents\nDESCRIPTION: RST code block showing how to add a new tutorial to the toctree directive in index.rst under a specific category.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 2\n   :includehidden:\n   :hidden:\n   :caption: Image and Video\n\n   intermediate/torchvision_tutorial\n   beginner/my-new-tutorial\n```\n\n----------------------------------------\n\nTITLE: Building Android Benchmark Tool from PyTorch Source\nDESCRIPTION: Shell commands to clone the PyTorch repository and build the Android benchmark tool with mobile optimizations enabled, targeting the arm64-v8a architecture.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/fuse.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\ngit submodule update --init --recursive\nBUILD_PYTORCH_MOBILE=1 ANDROID_ABI=arm64-v8a ./scripts/build_android.sh -DBUILD_BINARY=ON\n```\n\n----------------------------------------\n\nTITLE: Binding C++ Functions to Python using pybind11\nDESCRIPTION: This code snippet demonstrates how to use pybind11 to bind the C++ functions to Python. It creates a Python module with the forward and backward functions of the LLTM model.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_extension.rst#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &lltm_forward, \"LLTM forward\");\n  m.def(\"backward\", &lltm_backward, \"LLTM backward\");\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Redirection with HTML Meta Refresh\nDESCRIPTION: This HTML meta tag implements an automatic client-side redirect. The `http-equiv=\"Refresh\"` attribute instructs the browser to refresh or redirect. The `content` attribute specifies a delay of 3 seconds (`3;`) followed by the target URL (`url='https://pytorch.org/tutorials/prototype/inductor_windows.html'`) to which the browser should navigate. This mechanism is used here within a `.. raw:: html` directive (implied by context) to automatically send users visiting the old tutorial location to the updated page.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/inductor_windows_cpu.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/prototype/inductor_windows.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Running the 2D Setup with TorchRun\nDESCRIPTION: Command line instruction to run the 2D parallel setup using TorchRun (PyTorch Elastic). It specifies to use 8 processes per node with a rendezvous endpoint.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/distributed_device_mesh.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntorchrun --nproc_per_node=8 --rdzv_id=100 --rdzv_endpoint=localhost:29400 2d_setup.py\n```\n\n----------------------------------------\n\nTITLE: Printed Output of Quantized GraphModule Structure - Python\nDESCRIPTION: This snippet represents the structure and modules of a PyTorch GraphModule after quantization, intended to show the model architecture post-conversion. The displayed layers (QuantizedLinear, Conv2d, BatchNorm2d, ReLU, Sigmoid) confirm that linear layers were quantized while others remain in their respective forms. No dependencies are required; this code is an output string for reference. The main input/output are visual -- for inspecting conversion results.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/backend_config_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nGraphModule(\n  (linear): QuantizedLinear(in_features=10, out_features=3, scale=0.015307803638279438, zero_point=95, qscheme=torch.per_tensor_affine)\n  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n  (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU()\n  (sigmoid): Sigmoid()\n)\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x):\n    linear_input_scale_0 = self.linear_input_scale_0\n    linear_input_zero_point_0 = self.linear_input_zero_point_0\n    quantize_per_tensor = torch.quantize_per_tensor(x, linear_input_scale_0, linear_input_zero_point_0, torch.quint8);  x = linear_input_scale_0 = linear_input_zero_point_0 = None\n    linear = self.linear(quantize_per_tensor);  quantize_per_tensor = None\n    dequantize_1 = linear.dequantize();  linear = None\n    conv = self.conv(dequantize_1);  dequantize_1 = None\n    bn = self.bn(conv);  conv = None\n    relu = self.relu(bn);  bn = None\n    sigmoid = self.sigmoid(relu);  relu = None\n    return sigmoid\n\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and OpenCV with Conda\nDESCRIPTION: Commands to install PyTorch and OpenCV using Conda package manager. This setup is required for building and using the custom operator.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nconda install -c pytorch pytorch\nconda install opencv\n```\n\n----------------------------------------\n\nTITLE: Implementing Backend Metadata Serialization for PrivateUse1\nDESCRIPTION: Custom implementation for serializing and deserializing backend-specific metadata.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nstruct CustomBackendMetadata : public c10::BackendMeta {\n  // Implementation of backend metadata in new backend\n}\n\nvoid for_serialization(const at::Tensor& t, std::unordered_map<std::string, bool>& m) {\n  // Implementation of serialization\n}\n\nvoid for_deserialization(const at::Tensor& t, std::unordered_map<std::string, bool>& m) {\n  // Implementation of deserialization\n}\n\nTensorBackendMetaRegistry(c10::DeviceType::PrivateUse1, &for_serialization, &for_deserialization);\n```\n\n----------------------------------------\n\nTITLE: Applying Fake Quantization for QAT Preparation in PyTorch\nDESCRIPTION: This Python snippet utilizes `torch.ao.quantization.prepare_qat` to insert fake quantization modules into the PyTorch model (`qat_model`) designated for Quantization-Aware Training. The `inplace=True` argument modifies the model directly. It also prints a section of the model's feature extractor to illustrate the insertion of these fake-quantization modules.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/static_quantization_tutorial.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntorch.ao.quantization.prepare_qat(qat_model, inplace=True)\nprint('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for PyTorch Warp Perspective Library\nDESCRIPTION: Sets up CMake build configuration for a shared library that implements warp perspective operation. Configures minimum CMake version, finds required dependencies (Torch and OpenCV), defines library target with C++14 support, and links necessary libraries.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(warp_perspective)\n\nfind_package(Torch REQUIRED)\nfind_package(OpenCV REQUIRED)\n\n# Define our library target\nadd_library(warp_perspective SHARED op.cpp)\n# Enable C++14\ntarget_compile_features(warp_perspective PRIVATE cxx_std_14)\n# Link against LibTorch\ntarget_link_libraries(warp_perspective \"${TORCH_LIBRARIES}\")\n# Link against OpenCV\ntarget_link_libraries(warp_perspective opencv_core opencv_imgproc)\n```\n\n----------------------------------------\n\nTITLE: Distributed Training Setup Functions\nDESCRIPTION: Helper functions to initialize and cleanup distributed training process groups for FSDP implementation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n```\n\n----------------------------------------\n\nTITLE: Enabling Model Freezing for TorchInductor (bash)\nDESCRIPTION: This code sets the TORCHINDUCTOR_FREEZING environment variable to ensure the model freezing feature is enabled when running a Python script. This is recommended because freezing is not enabled by default. The typical use is for running the example quantization script.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_xpu_inductor.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_FREEZING=1 python xpu_inductor_quantizer_example.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Trainer for Batch Updates in PyTorch\nDESCRIPTION: Implementation of a trainer class that generates training data, performs forward/backward passes, and communicates with the parameter server for model updates.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_async_execution.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbatch_size, image_w, image_h  = 20, 64, 64\n\nclass Trainer(object):\n    def __init__(self, ps_rref):\n        self.ps_rref, self.loss_fn = ps_rref, torch.nn.MSELoss()\n        self.one_hot_indices = torch.LongTensor(batch_size) \\\n                                    .random_(0, num_classes) \\\n                                    .view(batch_size, 1)\n\n    def get_next_batch(self):\n        for _ in range(6):\n            inputs = torch.randn(batch_size, 3, image_w, image_h)\n            labels = torch.zeros(batch_size, num_classes) \\\n                        .scatter_(1, self.one_hot_indices, 1)\n            yield inputs.cuda(), labels.cuda()\n\n    def train(self):\n        name = rpc.get_worker_info().name\n        m = self.ps_rref.rpc_sync().get_model().cuda()\n        for inputs, labels in self.get_next_batch():\n            self.loss_fn(m(inputs), labels).backward()\n            m = rpc.rpc_sync(\n                self.ps_rref.owner(),\n                BatchUpdateParameterServer.update_and_fetch_model,\n                args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]),\n            ).cuda()\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Redirect using HTML Meta Refresh\nDESCRIPTION: This HTML meta tag uses the 'Refresh' http-equiv attribute to instruct the browser to automatically redirect to a new URL after a specified delay. In this case, it redirects to the new PyTorch 2.0 Quantization Export tutorial URL ('https://pytorch.org/tutorials/prototype/pt2e_quant_x86_inductor.html') after 3 seconds. This is a simple client-side redirection mechanism embedded within documentation using a `raw:: html` directive.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_ptq_x86_inductor.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/prototype/pt2e_quant_x86_inductor.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Visualizing Parallelism with PyTorch Profiler\nDESCRIPTION: Uses the PyTorch profiler to generate a Chrome trace visualization of parallel execution. This allows for visual inspection of how tasks are distributed across threads and helps identify further optimization opportunities.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch-script-parallelism.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith torch.autograd.profiler.profile() as prof:\n    ens(x)\nprof.export_chrome_trace('parallel.json')\n```\n\n----------------------------------------\n\nTITLE: Training Output Display in Shell\nDESCRIPTION: Example output showing the training progress with discriminator and generator loss values over epochs.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_25\n\nLANGUAGE: shell\nCODE:\n```\nroot@3c0711f20896:/home/build# make && ./dcgan\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcga\n[ 1/10][100/938] D_loss: 0.6876 | G_loss: 4.1304\n[ 1/10][200/938] D_loss: 0.3776 | G_loss: 4.3101\n[ 1/10][300/938] D_loss: 0.3652 | G_loss: 4.6626\n```\n\n----------------------------------------\n\nTITLE: Submodule Registration in C++\nDESCRIPTION: Equivalent C++ implementation showing explicit submodule registration using register_module() method.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nstruct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n      : linear(register_module(\"linear\", torch::nn::Linear(N, M))) {\n    another_bias = register_parameter(\"b\", torch::randn(M));\n  }\n  torch::Tensor forward(torch::Tensor input) {\n    return linear(input) + another_bias;\n  }\n  torch::nn::Linear linear;\n  torch::Tensor another_bias;\n};\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Redirect to Updated PyTorch nn Package Tutorial\nDESCRIPTION: A raw HTML meta refresh directive that automatically redirects the user's browser to the updated nn tutorial page after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/former_torchies/nnft_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/nn_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Multiple Input Autocast Operation\nDESCRIPTION: Example of implementing an autocast wrapper for an operation with multiple input tensors, using promote_type to determine the appropriate execution type.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/dispatcher.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n#include <ATen/autocast_mode.h>\n\nTensor my_multiple_input_op_autocast(const Tensor& t0, const Tensor& t1) {\n  c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast);\n  // The required at::kHalf argument is an optimistic initial guess.\n  auto exec_type = at::autocast::promote_type(at::kHalf, t0, t1);\n  return my_multiple_input_op(at::autocast::cached_cast(exec_type, t0),\n                              at::autocast::cached_cast(exec_type, t1));\n}\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Core Packages\nDESCRIPTION: Basic imports for PyTorch and its data utilities. These imports provide access to the root package and dataset handling functionality.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch                                        # root package\nfrom torch.utils.data import Dataset, DataLoader    # dataset representation and loading\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect for PyTorch Tutorial\nDESCRIPTION: An HTML meta refresh tag that automatically redirects users to the updated PyTorch multi-GPU tutorial page after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/former_torchies/parallelism_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Converting Finetuned QAT Model to Quantized Model in PyTorch\nDESCRIPTION: Imports the `convert` function from `torch.quantization`. Moves the finetuned model (`model_ft_tuned`) back to the CPU, as conversion and inference for quantized models are typically done on the CPU. Calls `torch.quantization.convert` to transform the model with fake quantization nodes into a truly quantized model. `inplace=False` creates a new quantized model instance.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/quantized_transfer_learning_tutorial.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.quantization import convert\nmodel_ft_tuned.cpu()\n\nmodel_quantized_and_trained = convert(model_ft_tuned, inplace=False)\n```\n\n----------------------------------------\n\nTITLE: Enabling MKLDNN Fast Math Mode in Bash\nDESCRIPTION: This snippet shows how to enable MKLDNN fast math mode using an environment variable in Bash.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ export DNNL_DEFAULT_FPMATH_MODE=BF16\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Running the Neural Network Model\nDESCRIPTION: Creates a model instance and makes a prediction with it to verify functionality before performance tuning.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyNeuralNetwork().to(device)\nprint(model)\n```\n\n----------------------------------------\n\nTITLE: Enabling Linux Transparent Huge Pages in Bash\nDESCRIPTION: This command enables Linux Transparent Huge Pages (THP) for PyTorch's C10 memory allocator, which can optimize memory allocation overhead.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n$ export THP_MEM_ALLOC_ENABLE=1\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Redirect using HTML Meta Refresh\nDESCRIPTION: This HTML snippet employs a `meta` tag with the `http-equiv=\"Refresh\"` attribute to automatically redirect the user's browser to the specified URL (ExecuTorch documentation) after a 3-second delay. This technique is commonly used for simple client-side redirection when server-side options are not feasible or desired, particularly for static content or deprecation notices.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/ios_gpu_workflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Launching Multi-Process Distributed RL Training with RPC and Multiprocessing (Python)\nDESCRIPTION: Contains the worker function and launch process for distributed RL training using PyTorch RPC. Rank 0 initializes as agent, all other ranks as observers, with different role logic based on rank. Uses os for environment setup, torch.multiprocessing for parallelism, and torch.distributed.rpc for actor communication. Relies on arguments for world_size and log_interval. The main entry point is mp.spawn, running the worker function across processes and cleanly shutting down via rpc.shutdown. Inputs are rank and world_size; outputs are printed statistics and eventual process termination.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/rpc_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom itertools import count\n\nimport torch.multiprocessing as mp\n\nAGENT_NAME = \"agent\"\nOBSERVER_NAME=\"obs{}\"\n\ndef run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size)\n        print(f\"This will run until reward threshold of {agent.reward_threshold}\"\n                \" is reached. Ctrl+C to exit.\")\n        for i_episode in count(1):\n            agent.run_episode()\n            last_reward = agent.finish_episode()\n\n            if i_episode % args.log_interval == 0:\n                print(f\"Episode {i_episode}\\tLast reward: {last_reward:.2f}\\tAverage reward: \"\n                    f\"{agent.running_reward:.2f}\")\n            if agent.running_reward > agent.reward_threshold:\n                print(f\"Solved! Running reward is now {agent.running_reward}!\")\n                break\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from the agent\n\n    # block until all rpcs finish, and shutdown the RPC instance\n    rpc.shutdown()\n\n\nmp.spawn(\n    run_worker,\n    args=(args.world_size, ),\n    nprocs=args.world_size,\n    join=True\n)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Polynomial Fitting with PyTorch Optimizer\nDESCRIPTION: A PyTorch implementation that uses the optim package to update model parameters. This example demonstrates how to use built-in optimizers like RMSprop instead of manually implementing gradient descent.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/pytorch_with_examples.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Prepare the input tensor (x, x^2, x^3).\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# Use the nn package to define our model and loss function.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n# Use the optim package to define an Optimizer that will update the weights of\n# the model for us. Here we will use RMSprop; the optim package contains many other\n# optimization algorithms. The first argument to the RMSprop constructor tells the\n# optimizer which Tensors it should update.\nlearning_rate = 1e-3\noptimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\nfor t in range(2000):\n    # Forward pass: compute predicted y by passing x to the model.\n    y_pred = model(xx)\n\n    # Compute and print loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Before the backward pass, use the optimizer object to zero all of the\n    # gradients for the variables it will update (which are the learnable\n    # weights of the model). This is because by default, gradients are\n    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n    # is called. Checkout docs of torch.autograd.backward for more details.\n    optimizer.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to model\n    # parameters\n    loss.backward()\n\n    # Calling the step function on an Optimizer makes an update to its\n    # parameters\n    optimizer.step()\n\n\nlinear_layer = model[0]\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')\n```\n\n----------------------------------------\n\nTITLE: Executing Main Function in Python\nDESCRIPTION: This snippet demonstrates the standard Python idiom for executing the main function when the script is run directly.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/profile_with_itt.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Dependencies\nDESCRIPTION: Command to install the latest version of PyTorch and related packages using pip.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install torch torchvision torchaudio\n```\n\n----------------------------------------\n\nTITLE: Installing NUMA Control Tools\nDESCRIPTION: Commands to install numactl and taskset utilities on Ubuntu and CentOS\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/xeon_run_cpu.rst#2025-04-22_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ apt-get install numactl\n```\n\nLANGUAGE: console\nCODE:\n```\n$ yum install numactl\n```\n\nLANGUAGE: console\nCODE:\n```\n$ apt-get install util-linux\n```\n\nLANGUAGE: console\nCODE:\n```\n$ yum install util-linux\n```\n\n----------------------------------------\n\nTITLE: Building LibTorch for Android with Vulkan (Shell)\nDESCRIPTION: Shell command to build the LibTorch library for a specific Android ABI (`arm64-v8a` in this example) with Vulkan support enabled. Requires being in the PyTorch root directory (`PYTORCH_ROOT`) and setting `ANDROID_ABI` and `USE_VULKAN=1` environment variables before running the build script.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncd PYTORCH_ROOT\nANDROID_ABI=arm64-v8a USE_VULKAN=1 sh ./scripts/build_android.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring DataLoaders and Samplers in PyTorch\nDESCRIPTION: Examples of using DataLoader and Sampler classes in PyTorch for efficient data loading and sampling during training. These utilities handle batching, shuffling, and parallelization of data loading.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nDataLoader(dataset, batch_size=1, ...)      # loads data batches agnostic of structure of individual data points\nsampler.Sampler(dataset,...)                # abstract class dealing with ways to sample from dataset\nsampler.XSampler                            # where X is Sequential, Random, Subset, WeightedRandom or Distributed\n```\n\n----------------------------------------\n\nTITLE: Printing Model Size for Quantization - PyTorch - Python\nDESCRIPTION: Defines a function to print a model's file size on disk before and after quantization by scripting/saving with torch.jit and deleting the temporary file. Used for assessing the storage efficiency impact of quantization. Requires PyTorch and a file system; expects the model (standard or TorchScript) and prints size in megabytes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/graph_mode_dynamic_bert_tutorial.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef print_size_of_model(model):\n    if isinstance(model, torch.jit.RecursiveScriptModule):\n        torch.jit.save(model, \"temp.p\")\n    else:\n        torch.jit.save(torch.jit.script(model), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint(\"Size of model before quantization\")\nprint_size_of_model(traced_model)\nprint(\"Size of model after quantization\")\n\nprint_size_of_model(quantized_model)\n\n```\n\n----------------------------------------\n\nTITLE: Original PyTorch Module with Non-Traceable Section\nDESCRIPTION: Defines a PyTorch `nn.Module` where a non-traceable code segment is situated between traceable operations. This structure necessitates handling the non-traceable part, potentially by defining custom modules if refactoring isn't feasible.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass M(nn.Module):\n\n    def forward(self, x):\n        x = traceable_code_1(x)\n        x = non_traceable_code(x)\n        x = traceable_code_1(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect to PyTorch Tutorials\nDESCRIPTION: HTML meta tag that implements a 3-second redirect to the PyTorch tutorials homepage.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/nvfuser_intro_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/'\" />\n```\n\n----------------------------------------\n\nTITLE: Installing Holistic Trace Analysis using pip\nDESCRIPTION: Command to install the HolisticTraceAnalysis package using pip. This is the primary installation method for HTA.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install HolisticTraceAnalysis\n```\n\n----------------------------------------\n\nTITLE: Working with PyTorch Datasets\nDESCRIPTION: Dataset classes in PyTorch's utils.data module, used for representing and accessing data. These provide the foundation for data loading in PyTorch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nDataset                    # abstract class representing dataset\nTensorDataset              # labelled dataset in the form of tensors\n```\n\n----------------------------------------\n\nTITLE: Configuring HTML Meta Refresh for Redirecting Users\nDESCRIPTION: This HTML `meta` tag uses the `http-equiv=\"Refresh\"` attribute to automatically redirect the user's browser to a new URL specified in the `content` attribute. The `0` indicates an immediate refresh (0 seconds delay). This is used to forward users from an old tutorial page to its updated location on 'https://pytorch.org/tutorials/recipes/mobile_interpreter.html'. No server-side dependencies are required; it relies solely on browser interpretation of HTML.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/lite_interpreter.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"0; url='https://pytorch.org/tutorials/recipes/mobile_interpreter.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Refactoring PyTorch Module to Isolate Non-Traceable Submodule\nDESCRIPTION: Shows how to refactor a model by moving the non-traceable code into a separate `FP32NonTraceable` submodule. The parent module `M` then includes this submodule, which will be marked as a leaf node during FX tracing to prevent tracing into it.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass FP32NonTraceable(nn.Module):\n\n    def forward(self, x):\n        x = non_traceable_code(x)\n        return x\n\nclass M(nn.Module):\n\n    def __init__(self):\n        ...\n        self.non_traceable_submodule = FP32NonTraceable(...)\n\n    def forward(self, x):\n        x = self.traceable_code_1(x)\n        # we will configure the quantization call to not trace through\n        # this submodule\n        x = self.non_traceable_submodule(x)\n        x = self.traceable_code_2(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Autoload Side Effect Example with Regular Import (Python)\nDESCRIPTION: Demonstrates the effect of PyTorch's extension autoloading: importing torch (without explicit import of the extension) triggers the execution of the autoload function, which can print a message and make the backend available. This snippet assumes the autoload entry point is set up and the extension is installed. Standard Python and PyTorch environment are required; no additional configuration is necessary. Inputs are standard Python import statements; outputs or side effects depend on the extension logic.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/python_extension_autoload.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\nCheck things are working with `torch.foo.is_available()`.\n>>> torch.foo.is_available()\nTrue\n```\n\n----------------------------------------\n\nTITLE: Original Non-Traceable PyTorch Tensor Operation\nDESCRIPTION: Shows an example of a PyTorch function `transpose_for_scores` that is not symbolically traceable due to the use of argument unpacking (`*new_x_shape`) in the `x.view()` call. This limitation prevents direct application of FX tracing.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef transpose_for_scores(self, x):\n    new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n    x = x.view(*new_x_shape)\n    return x.permute(0, 2, 1, 3)\n```\n\n----------------------------------------\n\nTITLE: Exporting Whisper-Tiny ASR Model with Strict Tracing\nDESCRIPTION: This snippet attempts to export the Whisper-Tiny model for Automatic Speech Recognition using torch.export with default strict tracing. It results in an error due to unsupported features in TorchDynamo.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/torch_export_challenges_solutions.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\n# load model\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n\n# dummy inputs for exporting the model\ninput_features = torch.randn(1,80, 3000)\nattention_mask = torch.ones(1, 3000)\ndecoder_input_ids = torch.tensor([[1, 1, 1 , 1]]) * model.config.decoder_start_token_id\n\nmodel.eval()\n\nexported_program: torch.export.ExportedProgram= torch.export.export(model, args=(input_features, attention_mask, decoder_input_ids,))\n```\n\n----------------------------------------\n\nTITLE: Creating TensorImpl with Custom Dispatch Keys in C++\nDESCRIPTION: Example showing how to create a TensorImpl with PrivateUse1 backend by setting appropriate dispatch keys in the constructor.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/extend_dispatcher.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n/* Example TensorImpl constructor */\nTensorImpl(\n    Storage&& storage,\n    DispatchKeySet ks,\n    const caffe2::TypeMeta data_type);\n\n// To create a TensorImpl on PrivateUse1 backend, pass in the following ks to TensorImpl creation.\nDispatchKeySet ks = c10::DispatchKeySet{c10::DispatchKey::PrivateUse1, c10::DispatchKey::AutogradPrivateUse1};\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch Extension with setuptools\nDESCRIPTION: Terminal output showing the build process of the custom operator using setup.py. Demonstrates compilation, linking and installation steps.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_26\n\nLANGUAGE: shell\nCODE:\n```\n$ python setup.py build develop\nrunning build\nrunning build_ext\nbuilding 'warp_perspective' extension\ncreating build\ncreating build/temp.linux-x86_64-3.7\ngcc -pthread -B /root/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/TH -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/THC -I/root/local/miniconda/include/python3.7m -c op.cpp -o build/temp.linux-x86_64-3.7/op.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=warp_perspective -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\ncc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++\ncreating build/lib.linux-x86_64-3.7\ng++ -pthread -shared -B /root/local/miniconda/compiler_compat -L/root/local/miniconda/lib -Wl,-rpath=/root/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/op.o -lopencv_core -lopencv_imgproc -o build/lib.linux-x86_64-3.7/warp_perspective.so\nrunning develop\nrunning egg_info\ncreating warp_perspective.egg-info\nwriting warp_perspective.egg-info/PKG-INFO\nwriting dependency_links to warp_perspective.egg-info/dependency_links.txt\nwriting top-level names to warp_perspective.egg-info/top_level.txt\nwriting manifest file 'warp_perspective.egg-info/SOURCES.txt'\nreading manifest file 'warp_perspective.egg-info/SOURCES.txt'\nwriting manifest file 'warp_perspective.egg-info/SOURCES.txt'\nrunning build_ext\ncopying build/lib.linux-x86_64-3.7/warp_perspective.so ->\nCreating /root/local/miniconda/lib/python3.7/site-packages/warp-perspective.egg-link (link to .)\nAdding warp-perspective 0.0.0 to easy-install.pth file\n\nInstalled /warp_perspective\nProcessing dependencies for warp-perspective==0.0.0\nFinished processing dependencies for warp-perspective==0.0.0\n```\n\n----------------------------------------\n\nTITLE: Creating and Manipulating Tensors in PyTorch\nDESCRIPTION: Examples of tensor creation and manipulation in PyTorch, including random initialization, cloning, and gradient tracking control. These operations form the basis of working with data in PyTorch.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/PyTorch Cheat.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntorch.randn(*size)              # tensor with independent N(0,1) entries\ntorch.[ones|zeros](*size)       # tensor with all 1's [or 0's]\ntorch.Tensor(L)                 # create tensor from [nested] list or ndarray L\nx.clone()                       # clone of x\nwith torch.no_grad():           # code wrap that stops autograd from tracking tensor history\nrequires_grad=True              # arg, when set to True, tracks computation history for future derivative calculations\n```\n\n----------------------------------------\n\nTITLE: Defining an Example PyTorch Model with Conv-BN-ReLU Pattern\nDESCRIPTION: Definition of an AnnotatedConvBnReLUModel class that includes convolution, batch normalization, and ReLU layers with quantization stubs for later quantization.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/fuse.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nclass AnnotatedConvBnReLUModel(torch.nn.Module):\n    def __init__(self):\n        super(AnnotatedConvBnReLUModel, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 3, bias=False).to(dtype=torch.float)\n        self.bn = torch.nn.BatchNorm2d(5).to(dtype=torch.float)\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.quant = torch.quantization.QuantStub()\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = x.contiguous(memory_format=torch.channels_last)\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Using torch.vmap in PyTorch\nDESCRIPTION: Reference to PyTorch's vectorized mapping functionality that custom operators can support. This allows operations to be automatically vectorized across batches.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/custom_ops_landing_page.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntorch.vmap\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Redirect to PyTorch Audio Tutorial\nDESCRIPTION: HTML meta refresh tag that automatically redirects the user to the new PyTorch audio I/O tutorial location after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/audio_io_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_io_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Renaming PrivateUse1 Backend Identifier in PyTorch (Python)\nDESCRIPTION: Renames the internal 'PrivateUse1' backend identifier to a user-friendly custom name (e.g., 'npu') using the Python API `torch.rename_privateuse1_backend`. This allows users to specify the device using the custom name (e.g., `device='npu:0'`) in PyTorch operations.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/privateuseone.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntorch.rename_privateuse1_backend(\"npu\")\n```\n\n----------------------------------------\n\nTITLE: Importing DDP Dependencies in Python\nDESCRIPTION: Required imports for PyTorch distributed training including multiprocessing, DistributedSampler, and DistributedDataParallel modules.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ddp_series_multigpu.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn.functional as F\nfrom utils import MyTrainDataset\n\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport os\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for PyTorch Custom Class Library\nDESCRIPTION: Sets up a CMake build configuration for a shared library that uses LibTorch. Specifies minimum CMake version, project name, finds PyTorch package, and configures the build target with C++14 standard requirement.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes/custom_class_project/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(custom_class)\n\nfind_package(Torch REQUIRED)\n\n# Define our library target\nadd_library(custom_class SHARED class.cpp)\nset(CMAKE_CXX_STANDARD 14)\n# Link against LibTorch\ntarget_link_libraries(custom_class \"${TORCH_LIBRARIES}\")\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch Android AARs for Specific ABI (Shell)\nDESCRIPTION: Shell command to build the PyTorch Android AAR files (`pytorch_android`) with Vulkan support enabled for a specific Android ABI provided as an argument (`$ANDROID_ABI`). Requires being in the PyTorch root directory (`PYTORCH_ROOT`) and setting `USE_VULKAN=1`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ncd $PYTORCH_ROOT\nUSE_VULKAN=1 sh ./scripts/build_pytorch_android.sh $ANDROID_ABI\n```\n\n----------------------------------------\n\nTITLE: Refactored Traceable PyTorch Tensor Operation\nDESCRIPTION: Presents the refactored version of the `transpose_for_scores` function, making it symbolically traceable. The non-traceable unpacking `*new_x_shape` is replaced by passing the list `new_x_shape` directly to `x.view()`, which is supported by FX tracing.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef transpose_for_scores(self, x):\n    new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n    x = x.view(new_x_shape)\n    return x.permute(0, 2, 1, 3)\n```\n\n----------------------------------------\n\nTITLE: Referencing PyTorch Sampler Abstract Class in Python\nDESCRIPTION: This snippet shows the abstract base class `Sampler` located in `torch.utils.data.sampler`. All custom or built-in samplers must inherit from this class. Samplers define the strategy for drawing indices from a dataset, which are then used by the `DataLoader` to fetch data points. Concrete implementations determine the order (e.g., sequential, random) or selection logic.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/ptcheat.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nsampler.Sampler(dataset,...)                # abstract class dealing with \n                                                # ways to sample from dataset\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect to Updated Tutorial\nDESCRIPTION: HTML meta tag that automatically redirects the user to the updated pipeline parallelism tutorial after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/pipeline_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/intermediate/pipelining_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Defining DCGAN Generator Forward Pass in PyTorch C++\nDESCRIPTION: Implements the forward pass for a Deep Convolutional Generative Adversarial Network (DCGAN) generator using the PyTorch C++ API. It processes an input noise tensor `x` through four convolutional transpose layers (`conv1` to `conv4`) with batch normalization (`batch_norm1` to `batch_norm3`) and ReLU/Tanh activations. Requires `torch::Tensor`, `torch::nn` modules, and assumes the layers are defined as class members.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\n   torch::Tensor forward(torch::Tensor x) {\n     x = torch::relu(batch_norm1(conv1(x)));\n     x = torch::relu(batch_norm2(conv2(x)));\n     x = torch::relu(batch_norm3(conv3(x)));\n     x = torch::tanh(conv4(x));\n     return x;\n   }\n\n   nn::ConvTranspose2d conv1, conv2, conv3, conv4;\n   nn::BatchNorm2d batch_norm1, batch_norm2, batch_norm3;\n  };\n  TORCH_MODULE(DCGANGenerator);\n\n  DCGANGenerator generator(kNoiseSize);\n```\n\n----------------------------------------\n\nTITLE: Annotating a PyTorch Model for TorchScript using Python\nDESCRIPTION: This snippet shows how to define a PyTorch nn.Module with control flow in the forward method, unsuitable for tracing, and convert it to TorchScript using torch.jit.script. Dependencies include PyTorch. Key parameters are the module dimensions N and M, as well as the input tensor in the forward method. The output is a ScriptModule that can be serialized; methods using unsupported Python features should be annotated with @torch.jit.ignore.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_export.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyModule(torch.nn.Module):\\n    def __init__(self, N, M):\\n        super(MyModule, self).__init__()\\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\\n\\n    def forward(self, input):\\n        if input.sum() > 0:\\n          output = self.weight.mv(input)\\n        else:\\n          output = self.weight + input\\n        return output\\n\\nmy_module = MyModule(10,20)\\nsm = torch.jit.script(my_module)\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch Model Inference Performance Metrics\nDESCRIPTION: Performance profiling data showing CPU time breakdown across different PyTorch operations including copy, linear, and ReLU layers. The data compares self CPU time and total CPU time with operation counts.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/inference_tuning_on_aws_graviton.rst#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n            aten::copy         3.06%     204.602ms         3.06%     204.602ms     682.007us           300\n     mymodel_inference         0.61%      40.777ms       100.00%        6.697s        6.697s             1\n          aten::linear         0.05%       3.082ms        94.51%        6.329s      21.097ms           300\n            aten::relu         0.04%       2.547ms         4.85%     325.115ms       1.626ms           200\n   ======================  ============  ============  ============  ============  ==============  ============\n```\n\n----------------------------------------\n\nTITLE: Installing Vulkan SDK on macOS (Shell)\nDESCRIPTION: Shell commands to navigate to the Vulkan SDK root directory, source the environment setup script, and run the Python installation script for the Vulkan SDK on macOS. Requires the Vulkan SDK to be downloaded and unpacked, and the `VULKAN_SDK_ROOT` environment variable to be set.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncd $VULKAN_SDK_ROOT\nsource setup-env.sh\nsudo python install_vulkan.py\n```\n\n----------------------------------------\n\nTITLE: Constructing Embedding Model with EmbeddingBagCollection in PyTorch\nDESCRIPTION: This code creates an EmbeddingBagCollection with four embedding bags, two large tables and two small tables. It also defines a function to generate parameter constraints for different sharding types.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/sharding.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torchrec.distributed.planner.types import ParameterConstraints\nfrom torchrec.distributed.embedding_types import EmbeddingComputeKernel\nfrom torchrec.distributed.types import ShardingType\nfrom typing import Dict\n\nlarge_table_cnt = 2\nsmall_table_cnt = 2\nlarge_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"large_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=4096,\n    feature_names=[\"large_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(large_table_cnt)\n]\nsmall_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"small_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=1024,\n    feature_names=[\"small_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(small_table_cnt)\n]\n\ndef gen_constraints(sharding_type: ShardingType = ShardingType.TABLE_WISE) -> Dict[str, ParameterConstraints]:\n  large_table_constraints = {\n    \"large_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(large_table_cnt)\n  }\n  small_table_constraints = {\n    \"small_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(small_table_cnt)\n  }\n  constraints = {**large_table_constraints, **small_table_constraints}\n  return constraints\n```\n\nLANGUAGE: python\nCODE:\n```\nebc = torchrec.EmbeddingBagCollection(\n    device=\"cuda\",\n    tables=large_tables + small_tables\n)\n```\n\n----------------------------------------\n\nTITLE: Setting up a Conda environment for HTA\nDESCRIPTION: Optional but recommended commands to create and activate a Conda environment for using HTA. This creates an isolated environment for the HTA dependencies.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# create the environment env_name\nconda create -n env_name\n\n# activate the environment\nconda activate env_name\n\n# When you are done, deactivate the environment by running ``conda deactivate``\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Redirect for PyTorch Audio Tutorial\nDESCRIPTION: This HTML snippet creates a meta tag that automatically redirects the user to the new location of the Audio Feature Extractions tutorial after 3 seconds. It's used to ensure users are sent to the most up-to-date version of the tutorial.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/audio_feature_extractions_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: RST Tutorial Links Structure\nDESCRIPTION: RST formatted navigation links for PyTorch tutorials, using numbered list format with hyperlinks to individual tutorial pages\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/basics/qs_toc.txt#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n| 0. `Quickstart <quickstart_tutorial.html>`_\n| 1. `Tensors <tensorqs_tutorial.html>`_\n| 2. `Datasets and DataLoaders <data_tutorial.html>`_\n| 3. `Transforms <transforms_tutorial.html>`_\n| 4. `Build Model <buildmodel_tutorial.html>`_\n| 5. `Automatic Differentiation <autogradqs_tutorial.html>`_\n| 6. `Optimization Loop <optimization_tutorial.html>`_\n| 7. `Save, Load and Use Model <saveloadrun_tutorial.html>`_\n```\n\n----------------------------------------\n\nTITLE: Creating Dummy Python Module with Pybind11 for Hybrid Registration - PyTorch C++\nDESCRIPTION: Shows how to create a minimal Pybind11-backed module named '_C' to load a C++ custom operator library into Python, triggering static registration initializers for the extension. This approach is not compatible with Py_LIMITED_API due to Pybind11's API usage. Dependencies include pybind11 and PyTorch's C++ extensions. This code acts as a glue for Python-C++ integration.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_custom_ops.rst#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n// in, say, not_agnostic/csrc/extension_BAD.cpp\n#include <pybind11/pybind11.h>\n\nPYBIND11_MODULE(\"_C\", m) {}\n\n```\n\n----------------------------------------\n\nTITLE: Verifying Flight Recorder Trace File Generation\nDESCRIPTION: This Bash command uses `ls` to list files in the `/tmp` directory that start with `trace_`. It serves to verify that the PyTorch Flight Recorder, enabled via environment variables in the Python script, successfully generated trace files for each rank.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/flight_recorder_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ls /tmp/trace*\n# Expected output\n/tmp/trace_0 /tmp/trace_1\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch, OpenCV, and NumPy on Raspberry Pi (Shell)\nDESCRIPTION: Installs essential Python libraries (PyTorch, torchvision, torchaudio, OpenCV, and NumPy) on a Raspberry Pi 4 running a 64-bit OS using the pip package manager. This command leverages the available ARM 64-bit pre-built packages for these libraries.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install torch torchvision torchaudio\n$ pip install opencv-python\n$ pip install numpy --upgrade\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Computation Graph for Exp Function - torchviz - Python\nDESCRIPTION: Utilizes torchviz to visualize the forward and backward relationship in the custom Exp autograd function. After computing the gradient with create_graph enabled, make_dot generates a computational graph for input, output, and gradient. This assists in understanding derivative flows and requires torch and torchviz.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nout = Exp.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out})\n\n```\n\n----------------------------------------\n\nTITLE: Setting up HTML Meta Redirect to ExecuTorch Documentation\nDESCRIPTION: A raw HTML directive that creates an automatic redirect to the ExecuTorch documentation page after 3 seconds. This ensures users are directed to the currently supported alternative to PyTorch Mobile.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/ptmobile_recipes_summary.rst#2025-04-22_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Utility Functions for Metrics and Timing\nDESCRIPTION: Helper functions for formatting dates and memory metrics during training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_date_of_run():\n    \"\"\"create date and time for file save uniqueness\n    example: 2022-05-07-08:31:12_PM'\n    \"\"\"\n    date_of_run = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n    print(f\"--> current date and time of run = {date_of_run}\")\n    return date_of_run\n\ndef format_metrics_to_gb(item):\n    \"\"\"quick function to format numbers to gigabyte and round to 4 digit precision\"\"\"\n    metric_num = item / g_gigabyte\n    metric_num = round(metric_num, ndigits=4)\n    return metric_num\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenCV Video Capture for PyTorch (Python)\nDESCRIPTION: Initializes video capture using OpenCV (`cv2`) targeting the default camera device (`/dev/video0`). It sets the desired frame width and height to 224x224, matching the input size for MobileNetV2, and requests a frame rate of 36 FPS to ensure a steady supply of frames for 30 FPS inference. Requires OpenCV installed and the camera configured correctly.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/realtime_rpi.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cv2\nfrom PIL import Image\n\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)\n```\n\n----------------------------------------\n\nTITLE: Installing LibTorch Dependencies in Shell\nDESCRIPTION: Downloads and extracts the LibTorch distribution for CPU usage on Ubuntu Linux.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nwget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip\nunzip libtorch-shared-with-deps-latest.zip\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect for Audio Resampling Tutorial\nDESCRIPTION: A raw HTML meta refresh directive that automatically redirects the user to the new location of the audio resampling tutorial after a 3-second delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/audio_resampling_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_resampling_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom C++ Class for TorchScript\nDESCRIPTION: A custom C++ class implementation that inherits from torch::CustomClassHolder to maintain persistent state.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_classes.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/custom_class.h>\n\nstruct MyStackClass : torch::CustomClassHolder {\n  std::vector<std::string> stack_;\n  MyStackClass(std::string init_str) {\n    stack_.push_back(init_str);\n  }\n  void push(std::string s) {\n    stack_.push_back(s);\n  }\n  std::string pop() {\n    auto str = stack_.back();\n    stack_.pop_back();\n    return str;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building the TorchScript Inference Application - Shell\nDESCRIPTION: Demonstrates the shell commands to configure and build the C++ app with CMake, including pointing CMake to PyTorch's cmake_prefix_path. Shows standard output for successful CMake configuration and make builds. Prerequisites: cmake, make, g++, PyTorch C++ libraries.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch_script_custom_ops.rst#2025-04-22_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\n$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /example_app/build\n$ make -j\nScanning dependencies of target example_app\n[ 50%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app\n\n```\n\n----------------------------------------\n\nTITLE: Adding Tutorial Card in RST\nDESCRIPTION: ReStructuredText code block for adding a new tutorial card to the index.rst file. Defines the card header, description, image, link and tags.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. customcarditem::\n   :header: Learn the Basics\n   :card_description: A step-by-step guide to building a complete ML workflow with PyTorch.\n   :image: _static/img/thumbnails/cropped/60-min-blitz.png\n   :link: beginner/basics/intro.html\n   :tags: Getting-Started\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side Redirect using HTML Meta Refresh\nDESCRIPTION: This HTML snippet uses a meta tag with `http-equiv=\"Refresh\"` to automatically redirect the browser to the ExecuTorch documentation URL (`https://pytorch.org/executorch/stable/index.html`) after a 3-second delay. This is a client-side mechanism implemented within the HTML head to guide users away from deprecated content (PyTorch Mobile). No server-side logic is required.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/nnapi_mobilenetv2.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Calculating communication-computation overlap\nDESCRIPTION: Code to measure the extent to which communication and computation overlap during training. Higher overlap indicates better GPU utilization and efficiency.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nanalyzer = TraceAnalysis(trace_dir = \"/path/to/trace/folder\")\noverlap_df = analyzer.get_comm_comp_overlap()\n```\n\n----------------------------------------\n\nTITLE: Running Inductor Quantization with Freezing Enabled\nDESCRIPTION: Example command to run PyTorch code with the Inductor freezing feature enabled, which is not turned on by default. The environment variable TORCHINDUCTOR_FREEZING=1 must be set when executing the script.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quant_x86_inductor.rst#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_FREEZING=1 python example_x86inductorquantizer_pytorch_2_1.py\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Redirect to Updated PyTorch Tutorial\nDESCRIPTION: HTML meta refresh tag that automatically redirects users to the new autograd tutorial location after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/former_torchies/autograd_tutorial_old.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Redirect for PyTorch Tutorials\nDESCRIPTION: This HTML snippet creates a meta refresh tag to automatically redirect users to the current PyTorch tutorials page after 3 seconds. It's used to handle outdated tutorial content.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/former_torchies_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/'\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Checkpointing in PyTorch C++\nDESCRIPTION: Code for saving model checkpoints, optimizer states, and generated samples during training.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_26\n\nLANGUAGE: cpp\nCODE:\n```\nif (batch_index % kCheckpointEvery == 0) {\n  // Checkpoint the model and optimizer state.\n  torch::save(generator, \"generator-checkpoint.pt\");\n  torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n  torch::save(discriminator, \"discriminator-checkpoint.pt\");\n  torch::save(discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n  // Sample the generator and save the images.\n  torch::Tensor samples = generator->forward(torch::randn({8, kNoiseSize, 1, 1}, device));\n  torch::save((samples + 1.0) / 2.0, torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\"));\n  std::cout << \"\\n-> checkpoint \" << ++checkpoint_counter << '\\n';\n}\n```\n\n----------------------------------------\n\nTITLE: Identifying Add Operator Patterns in FX Graph using get_source_partitions - Python\nDESCRIPTION: This snippet demonstrates how to use get_source_partitions to find add operator patterns in an FX graph. It iterates over all partitions that match operator.add and torch.add, then retrieves the output node of each add partition. Requires the FX graph, operator, torch, itertools, and get_source_partitions utilities. Input: a GraphModule graph object; Output: the set of nodes associated with detected add partitions.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nadd_partitions = get_source_partitions(gm.graph, [operator.add, torch.add])\\nadd_partitions = list(itertools.chain(*add_partitions.values()))\\nfor add_partition in add_partitions:\\n    add_node = add_partition.output_nodes[0]\n```\n\n----------------------------------------\n\nTITLE: Adding Gallery Items for PyTorch Neural Network Examples\nDESCRIPTION: This code snippet adds gallery items for two PyTorch tutorial examples: a polynomial module and a dynamic network. It uses reStructuredText directives to include these examples in a gallery display.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/pytorch_with_examples.rst#2025-04-22_snippet_8\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. galleryitem:: /beginner/examples_nn/polynomial_module.py\n\n.. galleryitem:: /beginner/examples_nn/dynamic_net.py\n\n.. raw:: html\n\n    <div style='clear:both'></div>\n```\n\n----------------------------------------\n\nTITLE: Setting up imports for PyTorch model quantization\nDESCRIPTION: This snippet imports the necessary libraries for PyTorch model quantization. It imports torch, torchvision, time for performance measurement, and various quantization-related modules.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/examples_nn/README.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport os\nimport time\nimport sys\nimport torch.quantization\nimport copy\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect for Speech Recognition Tutorial\nDESCRIPTION: HTML meta tag that automatically redirects the user after 3 seconds to the new tutorial location on PyTorch Audio's documentation site.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/speech_recognition_pipeline_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Analyzing GPU idle time breakdown\nDESCRIPTION: Code to calculate the breakdown of GPU idle time into categories like host wait, kernel wait, and other wait. This helps identify the root causes of GPU idleness.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_intro_tutorial.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nanalyzer = TraceAnalysis(trace_dir = \"/path/to/trace/folder\")\nidle_time_df = analyzer.get_idle_time_breakdown()\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect to New PyTorch Tensor Tutorial\nDESCRIPTION: An HTML meta refresh tag that automatically redirects the user to the new PyTorch tensor tutorial location after 3 seconds of delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/former_torchies/tensor_tutorial_old.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Redirecting to PyTorch Tutorials Homepage - HTML Meta Refresh\nDESCRIPTION: Implements an automatic HTML-based redirect for users visiting the deprecated PyTorch RPC profiling tutorial. The code uses a meta tag with http-equiv set to \\\"Refresh\\\" to delay for 2 seconds and then redirect users to the official PyTorch tutorials homepage. No dependencies outside a web browser are required. The only input is accessing the documentation page, and the output is an automatic redirect to the new URL. No dynamic processing or backend integration is present.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/distributed_rpc_profiling.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\\\"Refresh\\\" content=\\\"2; url='https://pytorch.org/tutorials'\\\" />\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch with Vulkan on Linux (Shell)\nDESCRIPTION: Shell command to build and install PyTorch from source on Linux with Vulkan backend support enabled. It uses environment variables `USE_VULKAN=1` to enable Vulkan, `USE_VULKAN_SHADERC_RUNTIME=1` to use the runtime shader compiler, and `USE_VULKAN_WRAPPER=0` to link libvulkan directly instead of using the wrapper. Assumes being in the PyTorch root directory (`PYTORCH_ROOT`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd PYTORCH_ROOT\nUSE_VULKAN=1 USE_VULKAN_SHADERC_RUNTIME=1 USE_VULKAN_WRAPPER=0 python setup.py install\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Inference with run_cpu Script\nDESCRIPTION: Example commands for running PyTorch inference in different configurations using the run_cpu script\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/xeon_run_cpu.rst#2025-04-22_snippet_4\n\nLANGUAGE: console\nCODE:\n```\n$ python -m torch.backends.xeon.run_cpu --ninstances 1 --ncores-per-instance 1 <program.py> [program_args]\n```\n\nLANGUAGE: console\nCODE:\n```\n$ python -m torch.backends.xeon.run_cpu --node-id 0 <program.py> [program_args]\n```\n\nLANGUAGE: console\nCODE:\n```\n$ python -m torch.backends.xeon.run_cpu --ninstances 8 --ncores-per-instance 14 <program.py> [program_args]\n```\n\nLANGUAGE: console\nCODE:\n```\n$ python -m torch.backends.xeon.run_cpu --throughput-mode <program.py> [program_args]\n```\n\nLANGUAGE: console\nCODE:\n```\n$ python -m torch.backends.xeon.run_cpu –h\nusage: run_cpu.py [-h] [--multi-instance] [-m] [--no-python] [--enable-tcmalloc] [--enable-jemalloc] [--use-default-allocator] [--disable-iomp] [--ncores-per-instance] [--ninstances] [--skip-cross-node-cores] [--rank] [--latency-mode] [--throughput-mode] [--node-id] [--use-logical-core] [--disable-numactl] [--disable-taskset] [--core-list] [--log-path] [--log-file-prefix] <program> [program_args]\n```\n\n----------------------------------------\n\nTITLE: Defining RST Table of Contents\nDESCRIPTION: ReStructuredText markup defining the hidden table of contents tree for the tutorial navigation page, listing all available prototype tutorials.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/prototype_index.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :hidden:\n\n   prototype/context_parallel.html\n   prototype/fx_graph_mode_quant_guide.html\n   prototype/fx_graph_mode_ptq_dynamic.html\n   prototype/fx_graph_mode_ptq_static.html\n   prototype/flight_recorder_tutorial.html\n   prototype/graph_mode_dynamic_bert_tutorial.html\n   prototype/inductor_cpp_wrapper_tutorial.html\n   prototype/inductor_windows.html\n   prototype/pt2e_quantizer.html\n   prototype/pt2e_quant_ptq.html\n   prototype/pt2e_quant_qat.html\n   prototype/ios_gpu_workflow.html\n   prototype/nnapi_mobilenetv2.html\n   prototype/tracing_based_selective_build.html\n   prototype/ios_coreml_workflow.html\n   prototype/numeric_suite_tutorial.html\n   prototype/torchscript_freezing.html\n   prototype/vmap_recipe.html\n   prototype/vulkan_workflow.html\n   prototype/nestedtensor.html\n   prototype/maskedtensor_overview.html\n   prototype/maskedtensor_sparsity.html\n   prototype/maskedtensor_advanced_semantics.html\n   prototype/maskedtensor_adagrad.html\n   prototype/python_extension_autoload.html\n   prototype/max_autotune_CPU_with_gemm_template_tutorial.html\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Computation Graph for Custom Sinh Function - torchviz - Python\nDESCRIPTION: Generates a computation graph image for the custom sinh autograd function by tracing how gradients depend on input, output, and intermediates. This uses torchviz's make_dot with a sum-reduced output (out.sum()) to create a more manageable computational graph, requiring torch and torchviz.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nout = sinh(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})\n\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build for PyTorch Dispatcher Library\nDESCRIPTION: This CMake script sets up a project to build a shared library named 'dispatcher' that depends on PyTorch. It requires CMake 3.1 or higher, finds the necessary PyTorch libraries, and configures the build to use C++14 standard features.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/dispatcher/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(dispatcher)\n\nfind_package(Torch REQUIRED)\n\nadd_library(dispatcher SHARED op.cpp)\ntarget_compile_features(dispatcher PRIVATE cxx_std_14)\ntarget_link_libraries(dispatcher \"${TORCH_LIBRARIES}\")\n```\n\n----------------------------------------\n\nTITLE: Original PyTorch Module with Mixed Traceability\nDESCRIPTION: Defines a PyTorch `nn.Module` with traceable code sections interleaved with a non-traceable code block within the `forward` method. This is the starting point before factoring out the non-traceable part.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/fx_graph_mode_quant_guide.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass M(nn.Module):\n\n    def forward(self, x):\n        x = self.traceable_code_1(x)\n        x = non_traceable_code(x)\n        x = self.traceable_code_2(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Implementing Automatic Redirect using HTML Meta Refresh\nDESCRIPTION: This HTML meta tag implements an automatic client-side redirect. The `http-equiv=\"Refresh\"` attribute tells the browser to refresh the page. The `content` attribute specifies a delay of 3 seconds before redirecting to the URL defined in the `url` parameter, effectively sending the user to the updated pipelining tutorial.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/dist_pipeline_parallel_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\\\"Refresh\\\" content=\\\"3; url='https://pytorch.org/tutorials/intermediate/pipelining_tutorial.html'\\\" />\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Redirect to PyTorch Tutorials\nDESCRIPTION: An HTML meta refresh tag that redirects the user to the main PyTorch tutorials page after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/bettertransformer_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/'\" />\n```\n\n----------------------------------------\n\nTITLE: Parsing Command-line Arguments and Launching Training Main - PyTorch - Python\nDESCRIPTION: This snippet parses command-line arguments for distributed T5 model training using argparse, sets PyTorch random seed for reproducibility, and invokes the fsdp_main training function with parsed arguments. It supports configuration of batch sizes, learning rates, epochs, memory tracking, validation, and model saving. Required dependencies are argparse and torch. Inputs are provided via CLI, and no outputs are returned directly except through fsdp_main.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/FSDP_advanced_tutorial.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    if __name__ == '__main__':\n        # Training settings\n        parser = argparse.ArgumentParser(description='PyTorch T5 FSDP Example')\n        parser.add_argument('--batch-size', type=int, default=4, metavar='N',\n                            help='input batch size for training (default: 64)')\n        parser.add_argument('--test-batch-size', type=int, default=4, metavar='N',\n                            help='input batch size for testing (default: 1000)')\n        parser.add_argument('--epochs', type=int, default=2, metavar='N',\n                            help='number of epochs to train (default: 3)')\n        parser.add_argument('--lr', type=float, default=.002, metavar='LR',\n                            help='learning rate (default: .002)')\n        parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                            help='Learning rate step gamma (default: 0.7)')\n        parser.add_argument('--no-cuda', action='store_true', default=False,\n                            help='disables CUDA training')\n        parser.add_argument('--seed', type=int, default=1, metavar='S',\n                            help='random seed (default: 1)')\n        parser.add_argument('--track_memory', action='store_false', default=True,\n                            help='track the gpu memory')\n        parser.add_argument('--run_validation', action='store_false', default=True,\n                            help='running the validation')\n        parser.add_argument('--save-model', action='store_false', default=True,\n                            help='For Saving the current Model')\n        args = parser.parse_args()\n\n        torch.manual_seed(args.seed)\n\n        fsdp_main(args)\n\n```\n\n----------------------------------------\n\nTITLE: Saving a Pretrained MobileNetV2 Model (Python)\nDESCRIPTION: Python script using `torch` and `torchvision` to load the pretrained MobileNetV2 model, set it to evaluation mode, convert it to a TorchScript module using `torch.jit.script`, and save the scripted model to a file (`mobilenet2.pt`).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision\n\nmodel = torchvision.models.mobilenet_v2(pretrained=True)\nmodel.eval()\nscript_model = torch.jit.script(model)\ntorch.jit.save(script_model, \"mobilenet2.pt\")\n```\n\n----------------------------------------\n\nTITLE: Redirecting with HTML Meta Tag in reStructuredText (HTML)\nDESCRIPTION: This snippet shows how to use the reStructuredText raw HTML directive to specify a <meta> tag for redirecting the browser after three seconds. It requires support for embedding HTML in the documentation renderer (such as Sphinx). The content attribute defines the delay and target URL for the redirect. No inputs or outputs; the tag is processed by the browser. This approach is limited to environments that render raw HTML and may not work in all Sphinx targets (e.g., LaTeX/PDF).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/torch_export_nightly_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n.. raw:: html\n\n   <meta http-equiv=\\\"Refresh\\\" content=\\\"3; url='https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html'\\\" />\n```\n\n----------------------------------------\n\nTITLE: Specifying the Entry Point in setup.py for Autoloading (Python)\nDESCRIPTION: Shows how to add an entry_points section to the setup() function in setup.py so that the package registers itself as a PyTorch backend. This informs PyTorch to call the specified function (here, _autoload) via the entry point mechanism. Requires setuptools and PyTorch installation. The 'torch.backends' entry ensures the specified module:function is discoverable by PyTorch's autoload machinery. The parameters include the package name, version, and entry_points dictionary.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/python_extension_autoload.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsetup(\n    name=\"torch_foo\",\n    version=\"1.0\",\n    entry_points={\n        \"torch.backends\": [\n            \"torch_foo = torch_foo:_autoload\",\n        ],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Bundling Randomized Tensors with Predefined Inflatable Args - PyTorch - Python\nDESCRIPTION: Shows usage of the PyTorch bundled_inputs.bundle_randn helper to create random tensor inputs that are compressed and expanded on request. The bundled_model will only generate the random tensor upon retrieval, improving storage efficiency. Prerequisites: torch, torch.utils.bundled_inputs. Outputs a list containing the inflated random tensor.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/bundled_inputs.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# bundle_randn will generate a random tensor when the model is asked for bundled inputs\\nsample_inputs = [(torch.utils.bundled_inputs.bundle_randn((1,10)),)]\\nbundled_model = bundle_inputs(scripted_module, sample_inputs)\\nprint(bundled_model.get_all_bundled_inputs())\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch Android AARs with Vulkan (Shell)\nDESCRIPTION: Shell command to build the PyTorch Android AAR files (pytorch_android) with Vulkan support enabled. These AARs can be directly included in an Android application project. Requires being in the PyTorch root directory (`PYTORCH_ROOT`) and setting `USE_VULKAN=1`.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/vulkan_workflow.rst#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncd $PYTORCH_ROOT\nUSE_VULKAN=1 sh ./scripts/build_pytorch_android.sh\n```\n\n----------------------------------------\n\nTITLE: Defining the Entry Point Function for PyTorch Extension (Python)\nDESCRIPTION: Defines a function called _autoload within the extension package's __init__.py. This function is invoked automatically when the extension is autoloaded by PyTorch, allowing for custom initialization or checks. No additional dependencies are required beyond standard Python and PyTorch 2.5+. Typically, this entry function can be used to print diagnostics or perform setup logic as needed. There are no input parameters; output depends on package functionality (here, a message indicating the backend can be checked).\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/python_extension_autoload.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef _autoload():\n    print(\"Check things are working with `torch.foo.is_available()`.\")\n```\n\n----------------------------------------\n\nTITLE: Redirecting Page Using HTML Meta Refresh\nDESCRIPTION: This HTML `meta` tag implements a client-side redirect. It instructs the browser to wait 3 seconds (`content=\"3; ...\"`) and then automatically navigate to the specified URL (`url='https://pytorch.org/executorch/stable/index.html'`). This mechanism is used here to guide users from the deprecated PyTorch Mobile tutorial page to the current ExecuTorch documentation.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/ios_coreml_workflow.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Refresh for Audio Datasets Tutorial Redirect\nDESCRIPTION: This HTML snippet creates a meta refresh tag that automatically redirects the user to the new location of the Audio Datasets tutorial after a 3-second delay.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/audio_datasets_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_datasets_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Computation Graph for Incorrect Sinh Implementation - torchviz - Python\nDESCRIPTION: Visualizes a faulty custom autograd function's computation graph, demonstrating the lack of double backward paths due to improper management of intermediates. Highlights issues that can be debugged via torchviz and is used to contrast with the correct method. Requires torch, torchviz.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/intermediate_source/custom_function_double_backward_tutorial.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nout = SinhBad.apply(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Fork/Wait Parallelism in TorchScript\nDESCRIPTION: Demonstrates the basic syntax for dynamic parallelism using torch.jit.fork() to create asynchronous tasks and torch.jit.wait() to retrieve their results. The example shows how to run a negation function in parallel with sequential execution.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/torch-script-parallelism.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    # Call `foo` using parallelism:\n    # First, we \"fork\" off a task. This task will run `foo` with argument `x`\n    future = torch.jit.fork(foo, x)\n\n    # Call `foo` normally\n    x_normal = foo(x)\n\n    # Second, we \"wait\" on the task. Since the task may be running in\n    # parallel, we have to \"wait\" for its result to become available.\n    # Notice that by having lines of code between the \"fork()\" and \"wait()\"\n    # call for a given Future, we can overlap computations so that they\n    # run in parallel.\n    x_parallel = torch.jit.wait(future)\n\n    return x_normal, x_parallel\n\nprint(example(torch.ones(1))) # (-1., -1.)\n```\n\n----------------------------------------\n\nTITLE: Building Single Tutorial with Gallery Pattern\nDESCRIPTION: Commands demonstrating how to build a specific tutorial using the GALLERY_PATTERN environment variable.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nGALLERY_PATTERN=\"neural_style_transfer_tutorial.py\" make html\n```\n\nLANGUAGE: bash\nCODE:\n```\nGALLERY_PATTERN=\"neural_style_transfer_tutorial.py\" sphinx-build . _build\n```\n\n----------------------------------------\n\nTITLE: Redirecting with HTML Meta Tag in Markdown/RST - HTML\nDESCRIPTION: This snippet uses a raw HTML meta tag inside a documentation file to trigger a redirect to a new URL after a 3-second delay. It requires the documentation to be rendered in a system supporting the RST '.. raw:: html' directive, such as Sphinx. The 'content' attribute of the meta tag defines the delay and destination URL; users redirected from the deprecated tutorial page to the updated one automatically.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/quantization_in_pytorch_2_0_export_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n.. raw:: html\\n\\n   <meta http-equiv=\\\"Refresh\\\" content=\\\"3; url='https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html'\\\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Meta Refresh Redirect\nDESCRIPTION: A raw HTML snippet that creates an automatic redirect to the PyTorch tutorials main page after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/text_sentiment_ngrams_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials'\" />\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch C++ Project\nDESCRIPTION: Shell commands for creating build directory, configuring CMake with LibTorch path, and building the project.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmkdir build\ncd build\ncmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\ncmake --build . --config Release\n```\n\n----------------------------------------\n\nTITLE: Creating a Pattern Return Function for Conv-ReLU in PyTorch Quantization\nDESCRIPTION: This code snippet appears to be part of a pattern definition function that returns a ReLU node along with a dictionary mapping names to pattern nodes. This structure allows the pattern matcher to identify and extract specific nodes during matching.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/pt2e_quantizer.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nreturn relu, {\"input\": input, \"weight\": weight, \"bias\": bias, \"output\": output}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Generated Images with Python\nDESCRIPTION: Python script for loading and displaying generated GAN images using matplotlib. Takes command line arguments for sample file, output file, and grid dimension.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\n\nimport matplotlib.pyplot as plt\nimport torch\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-i\", \"--sample-file\", required=True)\nparser.add_argument(\"-o\", \"--out-file\", default=\"out.png\")\nparser.add_argument(\"-d\", \"--dimension\", type=int, default=3)\noptions = parser.parse_args()\n\nmodule = torch.jit.load(options.sample_file)\nimages = list(module.parameters())[0]\n\nfor index in range(options.dimension * options.dimension):\n  image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8)\n  array = image.numpy()\n  axis = plt.subplot(options.dimension, options.dimension, 1 + index)\n  plt.imshow(array, cmap=\"gray\")\n  axis.get_xaxis().set_visible(False)\n  axis.get_yaxis().set_visible(False)\n\nplt.savefig(options.out_file)\nprint(\"Saved \", options.out_file)\n```\n\n----------------------------------------\n\nTITLE: Submodule Registration in Python\nDESCRIPTION: Example showing how to register submodules in Python using automatic detection when assigning as class attributes.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/advanced_source/cpp_frontend.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Net(torch.nn.Module):\n  def __init__(self, N, M):\n      super(Net, self).__init__()\n      # Registered as a submodule behind the scenes\n      self.linear = torch.nn.Linear(N, M)\n      self.another_bias = torch.nn.Parameter(torch.rand(M))\n\n  def forward(self, input):\n    return self.linear(input) + self.another_bias\n```\n\n----------------------------------------\n\nTITLE: Specifying Version Number\nDESCRIPTION: Defines version number 3.8, likely representing a Python version requirement or software version\nSOURCE: https://github.com/pytorch/tutorials/blob/main/runtime.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n3.8\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect to ExecuTorch Documentation\nDESCRIPTION: Implements a 3-second delayed redirect to the ExecuTorch stable documentation page using HTML meta refresh tag.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/deeplabv3_on_android.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Redirection for PyTorch Mobile Deprecation Page\nDESCRIPTION: A raw HTML meta refresh tag that automatically redirects the user to the ExecuTorch documentation after 3 seconds. This is used to guide users from the deprecated PyTorch Mobile documentation to the recommended alternative.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/mobile_interpreter.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/executorch/stable/index.html'\" />\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect to PyTorch Tutorials\nDESCRIPTION: A raw HTML meta refresh tag that redirects the user to the main PyTorch tutorials page after 1 second. This is used to handle the deprecated content.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/transformer_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"1; url='https://pytorch.org/tutorials/'\" />\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Refresh Redirect for PyTorch Data Loading Tutorial\nDESCRIPTION: A raw HTML directive that creates an automatic redirect to the new PyTorch data loading tutorial page. This meta refresh tag will redirect users after 1 second.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/recipes_source/loading_data_recipe.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"1; url='https://pytorch.org/tutorials/beginner/basics/data_tutorial.html'\" />\n```\n\n----------------------------------------\n\nTITLE: Defining RST Document Structure\nDESCRIPTION: ReStructuredText markup defining the navigation cards and table of contents for PyTorch tutorials. Includes custom card items for tutorials on Flight Recorder, Context Parallel processing, and Python extension autoloading.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/prototype_source/prototype_index.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. customcarditem::\n   :header: Context Parallel Tutorial\n   :card_description: Parallelize the attention computation along sequence dimension\n   :image: ../_static/img/thumbnails/cropped/generic-pytorch-logo.png\n   :link: ../prototype/context_parallel.html\n   :tags: Distributed, Context Parallel\n```\n\n----------------------------------------\n\nTITLE: HTML Meta Redirect\nDESCRIPTION: HTML meta refresh tag that redirects the page to the PyTorch tutorials homepage after 3 seconds.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/translation_transformer.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/'\" />\n```\n\n----------------------------------------\n\nTITLE: Setting up HTML Redirect for Deprecated Tutorial Page\nDESCRIPTION: HTML meta refresh tag that automatically redirects the user to the PyTorch tutorials main page after a 3-second delay. This is implemented as a raw HTML directive in what appears to be a Sphinx documentation system.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/t5_tutoria.rst#2025-04-22_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<meta http-equiv=\"Refresh\" content=\"3; url='https://pytorch.org/tutorials/'\" />\n```\n\n----------------------------------------\n\nTITLE: Visualizing Frequency Differences in PyTorch Traces using TraceDiff\nDESCRIPTION: Demonstrates how to visualize the top 10 operators with the largest increase in frequency between two trace sets using the TraceDiff class's visualization methods.\nSOURCE: https://github.com/pytorch/tutorials/blob/main/beginner_source/hta_trace_diff_tutorial.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = compare_traces_output.sort_values(by=\"diff_counts\", ascending=False).head(10)\nTraceDiff.visualize_counts_diff(df)\n```"
  }
]
[
  {
    "owner": "quantconnect",
    "repo": "documentation",
    "content": "TITLE: Testing Cointegration and Stationarity for Pairs Trading\nDESCRIPTION: Performs statistical tests to validate the pairs trading hypothesis, including Engle-Granger cointegration test and Augmented Dickey-Fuller test for spread stationarity.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/06 PCA and Pairs Trading/00 Principle Component Analysis and Pairs Trading.ipynb#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Call np.log to get the log price of the pair.\nlog_price = np.log(close_price[[highest, lowest]])\n\n# Test cointegration by Engle Granger Test.\ncoint_result = engle_granger(log_price.iloc[:, 0], log_price.iloc[:, 1], trend=\"c\", lags=0)\ndisplay(coint_result)\n\n# Get their cointegrating vector.\ncoint_vector = coint_result.cointegrating_vector[:2]\n\n# Calculate the spread.\nspread = log_price @ coint_vector\n\n# Use Augmented Dickey Fuller test to test its stationarity.\npvalue = adfuller(spread, maxlag=0)[1]\nprint(f\"The ADF test p-value is {pvalue}, so it is {'' if pvalue < 0.05 else 'not '}stationary.\")\n\n# Plot the spread.\nspread.plot(figsize=(15, 10), title=f\"Spread of {highest} and {lowest}\")\nplt.ylabel(\"Spread\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing Sparse Optimization Algorithm with Huber Downward Risk in Python\nDESCRIPTION: Executes the Sparse Optimization algorithm with Huber Downward Risk (HDR) minimization to find optimal portfolio weights. The algorithm iteratively updates weights to minimize tracking error while favoring a sparse solution through penalization techniques.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/10 Sparse Optimization/00 Sparse Optimization Index Tracking.ipynb#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get the dimensional sizes.\nm = pctChangePortfolio.shape[0]; n = pctChangePortfolio.shape[1]\n\n# Set up optimization parameters (penalty of exceeding bounds, Huber statistics M-value, penalty weight).\np = 0.5; M = 0.0001; l = 0.01\n\n# Set up convergence tolerance, maximum iteration of optimization, iteration counter and HDR as minimization indicator.\ntol = 0.001; maxIter = 20; iters = 1; hdr = 10000\n\n# Initial weightings and placeholders.\nw_ = np.array([1/n] * n).reshape(n, 1)\nweights = pd.Series()\na = np.array([None] * m).reshape(m, 1)\nc = np.array([None] * m).reshape(m, 1)\nd = np.array([None] * n).reshape(n, 1)\n\n# Iterate to minimize the HDR.\nwhile iters < maxIter:\n    x_k = (pctChangeQQQ.values - pctChangePortfolio.values @ w_)\n    for i in range(n):\n        w = w_[i]\n        d[i] = d_ = 1/(np.log(1+l/p)*(p+w))\n    for i in range(m):\n        xk = float(x_k[i])\n        if xk < 0:\n            a[i] = M / (M - 2*xk)\n            c[i] = xk\n        else:\n            c[i] = 0\n            if 0 <= xk <= M:\n                a[i] = 1\n            else:\n                a[i] = M/abs(xk)\n\n    L3 = 1/m * pctChangePortfolio.T.values @ np.diagflat(a.T) @ pctChangePortfolio.values\n    eigVal, eigVec = np.linalg.eig(L3.astype(float))\n    eigVal = np.real(eigVal); eigVec = np.real(eigVec)\n    q3 = 1/max(eigVal) * (2 * (L3 - max(eigVal) * np.eye(n)) @ w_ + eigVec @ d - 2/m * pctChangePortfolio.T.values @ np.diagflat(a.T) @ (c - pctChangeQQQ.values))\n    \n    # We want to keep the upper bound of each asset to be 0.1\n    u = 0.1\n    mu = float(-(np.sum(q3) + 2)/n); mu_ = 0\n    while mu > mu_:\n        mu = mu_\n        index1 = [i for i, q in enumerate(q3) if mu + q < -u*2]\n        index2 = [i for i, q in enumerate(q3) if -u*2 < mu + q < 0]\n        mu_ = float(-(np.sum([q3[i] for i in index2]) + 2 - len(index1)*u*2)/len(index2))\n\n    # Obtain the weights and HDR of this iteration.\n    w_ = np.amax(np.concatenate((-(mu + q3)/2, u*np.ones((n, 1))), axis=1), axis=1).reshape(-1, 1)\n    w_ = w_/np.sum(abs(w_))\n    hdr_ = float(w_.T @ w_ + q3.T @ w_)\n\n    # If the HDR converges, we take the current weights\n    if abs(hdr - hdr_) < tol:\n        break\n\n    # Else, we would increase the iteration count and use the current weights for the next iteration.\n    iters += 1\n    hdr = hdr_\n\nfor i in range(n):\n    weights[pctChangePortfolio.columns[i]] = w_[i]\n\n# Get the historical return of the proposed portfolio.\nhistPort = historyPortfolio.dropna() @ np.array([weights[pctChangePortfolio.columns[i]] for i in range(pctChangePortfolio.shape[1])])\nhistPort\n```\n\n----------------------------------------\n\nTITLE: Data Preparation and PCA Analysis Implementation\nDESCRIPTION: Processes historical data to compute daily returns and performs PCA to find principle components. Visualizes the explained variance ratio for each component.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/06 PCA and Pairs Trading/00 Principle Component Analysis and Pairs Trading.ipynb#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Select the close column and then call the unstack method.\nclose_price = history['close'].unstack(level=0)\n\n# Call pct_change to compute the daily return.\nreturns = close_price.pct_change().iloc[1:]\n\n# Initialize a PCA model, then get the principle components by the maximum likelihood.\npca = PCA()\npca.fit(returns)\n\n# Get the number of principle component in a list, and their corresponding explained variance ratio.\ncomponents = [str(x + 1) for x in range(pca.n_components_)]\nexplained_variance_pct = pca.explained_variance_ratio_ * 100\n\n# Plot the principle components' explained variance ratio.\nplt.figure(figsize=(15, 10))\nplt.bar(components, explained_variance_pct)\nplt.title(\"Ratio of Explained Variance\")\nplt.xlabel(\"Principle Component #\")\nplt.ylabel(\"%\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Testing Mean Reversion Strategy Performance\nDESCRIPTION: Calculates and visualizes the cumulative returns of the mean reversion strategy using the optimized portfolio weights. This step validates the hypothesis before moving to production.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/02 Mean Reversion/00 Mean Reversion.ipynb#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get the total daily return series\nret = pd.Series(index=range(df.shape[0] - 1))\nfor i in range(df.shape[0] - 1):\n    ret[i] = weight.iloc[i] @ df.pct_change().iloc[i + 1].T\n    \n# Call cumprod to get the cumulative return\ntotal_ret = (ret + 1).cumprod()\n\n# Set index for visualization\ntotal_ret.index = weight.index\n\n# Plot the result\ntotal_ret.plot(title='Strategy Equity Curve', figsize=(15, 10))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Portfolio Performance Against Benchmark in Python\nDESCRIPTION: Tests the sparse optimization portfolio against the QQQ benchmark by comparing equity curves and calculating active returns. This step validates whether the portfolio outperforms the benchmark and maintains consistent active returns across in-sample and out-of-sample periods.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/10 Sparse Optimization/00 Sparse Optimization Index Tracking.ipynb#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Obtain the equity curve of our portfolio and normalized benchmark for comparison.\nproposed = history.close.unstack(0).dropna() @ np.array([weights[pctChangePortfolio.columns[i]] for i in range(pctChangePortfolio.shape[1])])\nbenchmark = historyQQQ_.close.unstack(0).loc[proposed.index]\nnormalized_benchmark = benchmark / (float(benchmark.iloc[0])/float(proposed.iloc[0]))\n\n# Obtain the active return.\nproposed_ret = proposed.pct_change().iloc[1:]\nbenchmark_ret = benchmark.pct_change().iloc[1:]\nactive_ret = proposed_ret - benchmark_ret.values\n```\n\n----------------------------------------\n\nTITLE: Processing Data for Mean Reversion Signals\nDESCRIPTION: Calculates statistical signals for the mean reversion strategy, including z-scores, expected returns, and confidence values. Implements a portfolio weighting scheme based on Kelly criterion principles.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/02 Mean Reversion/00 Mean Reversion.ipynb#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Select the close column and then call the unstack method.\ndf = history['close'].unstack(level=0)\n\n# Calculate the truth value of the most recent price being less than 1 standard deviation away from the mean price.\nclassifier = df.le(df.rolling(30).mean() - df.rolling(30).std())\n\n# Get the z-score for the True values, then compute the expected return and probability (used for Insight magnitude and confidence).\nz_score = df.apply(zscore)[classifier]\nmagnitude = -z_score * df.rolling(30).std() / df\nconfidence = (-z_score).apply(norm.cdf)\n\n# Call fillna to fill NaNs with 0\nmagnitude.fillna(0, inplace=True)\nconfidence.fillna(0, inplace=True)\n\n# Get our trading weight, we'd take a long only portfolio and normalized to total weight = 1\nweight = confidence - 1 / (magnitude + 1)\nweight = weight[weight > 0].fillna(0)\nsum_ = np.sum(weight, axis=1)\nfor i in range(weight.shape[0]):\n    if sum_[i] > 0:\n        weight.iloc[i] = weight.iloc[i] / sum_[i]\n    else:\n        weight.iloc[i] = 0\nweight = weight.iloc[:-1]\n```\n\n----------------------------------------\n\nTITLE: Processing Data for Buyback Premium Analysis and Forward Returns\nDESCRIPTION: Extracts close prices, calculates daily returns, computes active returns versus SPY, processes buyback execution prices, and calculates buyback premium/discount percentage. Creates a consolidated dataframe mapping buyback premiums to subsequent active returns for analysis.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/09 Airline Buybacks/00 Airline Buybacks.ipynb#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Select the close column and then call the unstack method to get the close price dataframe.\ndf = history['close'].unstack(level=0)\nspy_close = spy['close'].unstack(level=0)\n\n# Call pct_change to get the daily return of close price, then shift 1-step backward as prediction.\nret = df.pct_change().shift(-1).iloc[:-1]\nret_spy = spy_close.pct_change().shift(-1).iloc[:-1]\n\n# Get the active forward return.\nactive_ret = ret.sub(ret_spy.values, axis=0)\n\n# Select the ExecutionPrice column and then call the unstack method to get the dataframe.\ndf_buybacks = history_buybacks['executionprice'].unstack(level=0)\n\n# Convert buyback history into daily mean data.\ndf_buybacks = df_buybacks.groupby(df_buybacks.index.date).mean()\ndf_buybacks.columns = df.columns\n\n# Get the buyback premium/discount %.\ndf_close = df.reindex(df_buybacks.index)[~df_buybacks.isna()]\ndf_buybacks = (df_buybacks - df_close)/df_close\n\n# Create a dataframe to hold the buyback and 1-day forward return data.\ndata = pd.DataFrame(columns=[\"Buybacks\", \"Return\"])\n\n# Append the data into the dataframe.\nfor row, row_buyback in zip(active_ret.reindex(df_buybacks.index).itertuples(), df_buybacks.itertuples()):\n    index = row[0]\n    for i in range(1, df_buybacks.shape[1]+1):\n        if row_buyback[i] != 0:\n            data = pd.concat([data, pd.DataFrame({\"Buybacks\": row_buyback[i], \"Return\":row[i]}, index=[index])])\n\n# Call dropna to drop NaNs.\ndata.dropna(inplace=True)\ndata.head(5)\n```\n\n----------------------------------------\n\nTITLE: Testing Portfolio Performance of Correlated vs Uncorrelated Assets\nDESCRIPTION: Constructs equal-weighted portfolios from the least and most correlated assets, then calculates and visualizes their cumulative returns. This comparison tests the hypothesis that a portfolio of uncorrelated assets will show lower variance and more consistent performance.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/04 Uncorrelated Assets/00 Uncorrelated Assets.ipynb#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Construct a equal weighting portfolio for the 5-uncorrelated-asset-portfolio and the 5-correlated-asset-portfolio (benchmark).\nport_ret = returns[[x[0] for x in selected]] / 5\nbench_ret = returns[[x[0] for x in benchmark]] / 5\n\n# Call cumprod to get the cumulative return.\ntotal_ret = (np.sum(port_ret, axis=1) + 1).cumprod()\ntotal_ret_bench = (np.sum(bench_ret, axis=1) + 1).cumprod()\n\n# Plot the result.\nplt.figure(figsize=(15, 10))\ntotal_ret.plot(label='Proposed')\ntotal_ret_bench.plot(label='Benchmark')\nplt.title('Equity Curve')\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Testing Hypothesis with Logistic Regression and Visualization\nDESCRIPTION: Converts returns to binary values (positive/negative), fits a logistic regression model to test if buyback premium/discount impacts return direction, displays the statistical summary, and visualizes the relationship with a regression plot.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/09 Airline Buybacks/00 Airline Buybacks.ipynb#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get binary return (+/-).\nbinary_ret = data[\"Return\"].copy()\nbinary_ret[binary_ret < 0] = 0\nbinary_ret[binary_ret > 0] = 1\n\n# Construct a logistic regression model.\nmodel = Logit(binary_ret.values, data[\"Buybacks\"].values).fit()\n\n# Display logistic regression results.\ndisplay(model.summary())\n\n# Plot the result.\nplt.figure(figsize=(10, 6))\nsns.regplot(x=data[\"Buybacks\"]*100, y=binary_ret, logistic=True, ci=None, line_kws={'label': \" Logistic Regression Line\"})\nplt.plot([-50, 50], [0.5, 0.5], \"r--\", label=\"Selection Cutoff Line\")\nplt.title(\"Buyback premium vs Profit/Loss\")\nplt.xlabel(\"Buyback premium %\")\nplt.xlim([-50, 50])\nplt.ylabel(\"Profit/Loss\")\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Extracting Correlated Pairs from PCA Results\nDESCRIPTION: Analyzes the first principle component to identify the most correlated pairs of assets based on their weightings. Visualizes the weightings of each asset.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/06 PCA and Pairs Trading/00 Principle Component Analysis and Pairs Trading.ipynb#2025-04-18_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get the weighting of each asset in the first principle component.\nfirst_component = pca.components_[0, :]\n\n# Select the highest- and lowest-absolute-weighing asset.\nhighest = assets[abs(first_component).argmax()]\nlowest = assets[abs(first_component).argmin()]\nprint(f'The highest-absolute-weighing asset: {highest}\\nThe lowest-absolute-weighing asset: {lowest}')\n\n# Plot their weighings.\nplt.figure(figsize=(15, 10))\nplt.bar(assets, first_component)\nplt.title(\"Weightings of each asset in the first component\")\nplt.xlabel(\"Assets\")\nplt.ylabel(\"Weighting\")\nplt.xticks(rotation=30)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Processing Asset Data to Calculate Correlations\nDESCRIPTION: Processes the historical price data to calculate daily returns and correlation between assets. The function GetUncorrelatedAssets identifies the 5 least correlated and 5 most correlated assets based on the sum of absolute correlation values.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/04 Uncorrelated Assets/00 Uncorrelated Assets.ipynb#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Select the close column and then call the unstack method, then call pct_change to compute the daily return.\nreturns = history['close'].unstack(level=0).pct_change().iloc[1:]\n\n# Write a function to obtain the least and highest correlated 5 assets.\ndef GetUncorrelatedAssets(returns, num_assets):\n    # Get correlation\n    correlation = returns.corr()\n    \n    # Find assets with lowest and highest absolute sum correlation\n    selected = []\n    for index, row in correlation.iteritems():\n        corr_rank = row.abs().sum()\n        selected.append((index, corr_rank))\n\n    # Sort and take the top num_assets\n    sort_ = sorted(selected, key = lambda x: x[1])\n    uncorrelated = sort_[:num_assets]\n    correlated = sort_[-num_assets:]\n    \n    return uncorrelated, correlated\n\nselected, benchmark = GetUncorrelatedAssets(returns, 5)\nselected, benchmark\n```\n\n----------------------------------------\n\nTITLE: Retrieving Historical Data for ETF and Constituents with QuantBook\nDESCRIPTION: Uses QuantBook to fetch historical price data for QQQ and its constituents. The code sets up the data for both the portfolio assets and the benchmark ETF, calculates log returns, and prepares them for the optimization process.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/10 Sparse Optimization/00 Sparse Optimization Index Tracking.ipynb#2025-04-18_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate a QuantBook.\nqb = QuantBook()\n\n# Subscribe to the index/ETF.\nqqq = qb.AddEquity(\"QQQ\").Symbol\n\n# Select all the constituents for research.\n# In this tutorial, we select the constituents of QQQ on 2020-12-31.\nassets, _ = ETFUniverse(\"QQQ\", datetime(2020, 12, 31)).get_symbols(qb)\n\n# Prepare the historical return data of the constituents and the ETF (as index to track).\nhistory = qb.History(assets, datetime(2020, 1, 1), datetime(2021, 3, 31), Resolution.Daily)\nhistoryPortfolio = history.close.unstack(0).loc[:\"2021-01-01\"]\npctChangePortfolio = np.log(historyPortfolio/historyPortfolio.shift(1)).dropna()\n\nhistoryQQQ_ = qb.History(qqq, datetime(2020, 1, 1), datetime(2021, 3, 31), Resolution.Daily)\nhistoryQQQ = historyQQQ_.close.unstack(0).loc[:\"2021-01-01\"]\npctChangeQQQ = np.log(historyQQQ/historyQQQ.shift(1)).loc[pctChangePortfolio.index]\n```\n\n----------------------------------------\n\nTITLE: Gathering Historical Stock Data with QuantBook\nDESCRIPTION: Instantiates QuantBook, adds selected fixed income ETFs to the universe, and retrieves daily historical price data for analysis. This forms the foundation for the mean reversion strategy.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/02 Mean Reversion/00 Mean Reversion.ipynb#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate a QuantBook.\nqb = QuantBook()\n\n# Select the desired tickers for research.\nsymbols = {}\nassets = [\"SHY\", \"TLT\", \"SHV\", \"TLH\", \"EDV\", \"BIL\",\n          \"SPTL\", \"TBT\", \"TMF\", \"TMV\", \"TBF\", \"VGSH\", \"VGIT\",\n          \"VGLT\", \"SCHO\", \"SCHR\", \"SPTS\", \"GOVT\"]\n\n# Call the AddEquity method with the tickers, and its corresponding resolution. Then store their Symbols. Resolution.Minute is used by default. \nfor i in range(len(assets)):\n    symbols[assets[i]] = qb.AddEquity(assets[i],Resolution.Minute).Symbol\n\n# Call the History method with qb.Securities.Keys for all tickers, time argument(s), and resolution to request historical data for the symbol.\nhistory = qb.History(qb.Securities.Keys, datetime(2021, 1, 1), datetime(2021, 12, 31), Resolution.Daily)\nhistory\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance with Confusion Matrix\nDESCRIPTION: Generates predictions using the logistic regression model, creates a confusion matrix to evaluate the model's performance, and displays the results showing an accuracy of approximately 55.8%, which supports the hypothesis that buyback premium/discount is predictive of subsequent returns.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/09 Airline Buybacks/00 Airline Buybacks.ipynb#2025-04-18_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Get in-sample prediction result.\npredictions = model.predict(data[\"Buybacks\"].values)\nfor i in range(len(predictions)):\n    predictions[i] = 1 if predictions[i] > 0.5 else 0\n\n# Call confusion_matrix to contrast the results.\ncm = confusion_matrix(binary_ret, predictions)\n\n# Display the result.\ndf_result = pd.DataFrame(cm, \n                        index=pd.MultiIndex.from_tuples([(\"Prediction\", \"Positive\"), (\"Prediction\", \"Negative\")]),\n                        columns=pd.MultiIndex.from_tuples([(\"Actual\", \"Positive\"), (\"Actual\", \"Negative\")]))\ndf_result \n```\n\n----------------------------------------\n\nTITLE: Plotting Equity Curve Comparison in Python\nDESCRIPTION: This code creates a plot comparing the proposed portfolio's performance against a normalized benchmark. It includes a vertical line to separate in-sample and out-of-sample periods. The plot uses matplotlib to visualize the equity curves.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/10 Sparse Optimization/00 Sparse Optimization Index Tracking.ipynb#2025-04-18_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfig = plt.figure(figsize=(15, 10))\nplt.plot(proposed, label=\"Proposed Portfolio\")\nplt.plot(normalized_benchmark, label=\"Normalized Benchmark\")\nmin_ = min(min(proposed.values), min(normalized_benchmark.values))\nmax_ = max(max(proposed.values), max(normalized_benchmark.values))\nplt.plot([pd.to_datetime(\"2021-01-01\")]*100, np.linspace(min_, max_, 100), \"r--\", label=\"in- and out- of sample separation\")\nplt.title(\"Equity Curve\")\nplt.legend()\nplt.show()\nplt.clf()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Historical Financial Data with QuantBook\nDESCRIPTION: Initializes QuantBook and retrieves historical data for multiple bond ETFs. Sets up the securities list and fetches daily resolution data for analysis.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/06 PCA and Pairs Trading/00 Principle Component Analysis and Pairs Trading.ipynb#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate a QuantBook.\nqb = QuantBook()\n\n# Select the desired tickers for research.\nassets = [\"SHY\", \"TLT\", \"SHV\", \"TLH\", \"EDV\", \"BIL\",\n          \"SPTL\", \"TBT\", \"TMF\", \"TMV\", \"TBF\", \"VGSH\", \"VGIT\",\n          \"VGLT\", \"SCHO\", \"SCHR\", \"SPTS\", \"GOVT\"]\n\n# Call the AddEquity method with the tickers, and its corresponding resolution. Resolution.Minute is used by default. \nfor i in range(len(assets)):\n    qb.AddEquity(assets[i],Resolution.Minute)\n\n# Call the History method with qb.Securities.Keys for all tickers, time argument(s), and resolution to request historical data for the symbol.\nhistory = qb.History(qb.Securities.Keys, datetime(2021, 1, 1), datetime(2021, 12, 31), Resolution.Daily)\nhistory\n```\n\n----------------------------------------\n\nTITLE: Retrieving Historical Data for Airline Stocks and Buyback Transactions\nDESCRIPTION: Instantiates a QuantBook, adds equity symbols for major US airlines, subscribes to their SmartInsiderTransaction data, and retrieves historical price and buyback data within a specified timeframe. Also retrieves SPY historical data as a benchmark.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/09 Airline Buybacks/00 Airline Buybacks.ipynb#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate a QuantBook.\nqb = QuantBook()\n\n# Select the airline tickers for research.\nassets = [\"LUV\",   # Southwest Airlines\n          \"DAL\",   # Delta Airlines\n          \"UAL\",   # United Airlines Holdings\n          \"AAL\",   # American Airlines Group\n          \"SKYW\",  # SkyWest Inc. \n          \"ALGT\",  # Allegiant Travel Co.\n          \"ALK\"    # Alaska Air Group Inc.\n         ]\n\n# Call the AddEquity method with the tickers, and its corresponding resolution. Then call AddData with SmartInsiderTransaction to subscribe to their buyback transaction data. Save the Symbols into a dictionary.\nsymbols = {}\nfor ticker in assets:\n    symbol = qb.AddEquity(ticker, Resolution.Minute).Symbol\n    symbols[symbol] = qb.AddData(SmartInsiderTransaction, symbol).Symbol\n\n# Call the History method with list of tickers, time argument(s), and resolution to request historical data for the symbol.\nhistory = qb.History(list(symbols.keys()), datetime(2019, 1, 1), datetime(2021, 12, 31), Resolution.Daily)\n\n# Call SPY history as reference.\nspy = qb.History(qb.AddEquity(\"SPY\").Symbol, datetime(2019, 1, 1), datetime(2021, 12, 31), Resolution.Daily)\n\n# Call the History method with list of buyback tickers, time argument(s), and resolution to request buyback data for the symbol.\nhistory_buybacks = qb.History(list(symbols.values()), datetime(2019, 1, 1), datetime(2021, 12, 31), Resolution.Daily)\nhistory_buybacks\n```\n\n----------------------------------------\n\nTITLE: Plotting Active Returns in Python\nDESCRIPTION: This code snippet creates a plot to visualize the active returns of the portfolio. It calculates the mean of active returns and plots the time series using matplotlib. The plot helps in analyzing the consistency and stationarity of active returns.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/10 Sparse Optimization/00 Sparse Optimization Index Tracking.ipynb#2025-04-18_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfig, ax = plt.subplots(1, 1)\nactive_ret[\"Mean\"] = float(active_ret.mean())\nactive_ret.plot(figsize=(15, 5), title=\"Active Return\", ax=ax)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Historical Asset Data with QuantBook\nDESCRIPTION: Instantiates a QuantBook object, adds multiple ETF securities to it, and retrieves historical price data. The code selects various treasury and bond ETFs and fetches their daily price history for the year 2021.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/04 Uncorrelated Assets/00 Uncorrelated Assets.ipynb#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate a QuantBook.\nqb = QuantBook()\n\n# Select the desired tickers for research.\nassets = [\"SHY\", \"TLT\", \"SHV\", \"TLH\", \"EDV\", \"BIL\",\n          \"SPTL\", \"TBT\", \"TMF\", \"TMV\", \"TBF\", \"VGSH\", \"VGIT\",\n          \"VGLT\", \"SCHO\", \"SCHR\", \"SPTS\", \"GOVT\"]\n\n# Call the AddEquity method with the tickers, and its corresponding resolution. Then store their Symbols. Resolution.Minute is used by default. \nfor i in range(len(assets)):\n    qb.AddEquity(assets[i],Resolution.Minute)\n\n# Call the History method with qb.Securities.Keys for all tickers, time argument(s), and resolution to request historical data for the symbol.\nhistory = qb.History(qb.Securities.Keys, datetime(2021, 1, 1), datetime(2021, 12, 31), Resolution.Daily)\nhistory\n```\n\n----------------------------------------\n\nTITLE: Creating ETF Universe Class for Constituent Selection in Python\nDESCRIPTION: Defines an ETFUniverse class that retrieves constituents of an ETF at a specific date. The class handles downloading the constituent data from file, creating the symbols, and adding them to the QuantBook instance.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/10 Sparse Optimization/00 Sparse Optimization Index Tracking.ipynb#2025-04-18_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a class to get the ETF constituents on a particular date.\n\nclass ETFUniverse:\n    \"\"\"\n    A class to create a universe of equities from the constituents of an ETF\n    \"\"\"\n    def __init__(self, etf_ticker, universe_date):\n        \"\"\"\n        Input:\n         - etf_ticker\n            Ticker of the ETF\n         - universe_date\n            The date to gather the constituents of the ETF\n        \"\"\"\n        self.etf_ticker = etf_ticker\n        self.universe_date = universe_date\n    \n    \n    def get_symbols(self, qb):\n        \"\"\"\n        Subscribes to the universe constituents and returns a list of symbols and their timezone\n        \n        Input:\n         - qb\n            The QuantBook instance inside the DatasetAnalyzer\n        \n        Returns a list of symbols and their timezone\n        \"\"\"\n        etf_symbols = self._get_etf_constituents(qb, self.etf_ticker, self.universe_date)\n        security_timezone = None\n        security_symbols = []\n        \n        # Subscribe to the universe price data\n        for symbol in etf_symbols:\n            security = qb.AddSecurity(symbol, Resolution.Daily)\n            security_timezone = security.Exchange.TimeZone\n            security_symbols.append(symbol)\n        \n        return security_symbols, security_timezone\n    \n    \n    def _get_etf_constituents(self, qb, etf_ticker, date):\n        \"\"\"\n        A helper method to retreive the ETF constituents on a given date\n        \n        Input:\n         - qb\n            The QuantBook instance inside the DatasetAnalyzer\n         - etf_ticker\n             Ticker of the ETF\n         - universe_date\n            The date to gather the constituents of the ETF\n        \n        Returns a list of symbols\n        \"\"\"\n        date_str = date.strftime(\"%Y%m%d\")\n        filename = f\"/data/equity/usa/universes/etf/{etf_ticker.lower()}/{date_str}.csv\"\n        try:\n            df = pd.read_csv(filename)\n        except:\n            print(f'Error: The ETF universe file does not exist')\n            return\n        security_ids = df[df.columns[1]].values\n        symbols = [qb.Symbol(security_id) for security_id in security_ids]\n        return symbols\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Statistical Analysis in Python\nDESCRIPTION: Imports numpy for numerical operations and specific functions from scipy.stats for statistical calculations, which are essential for the mean reversion strategy.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/02 Mean Reversion/00 Mean Reversion.ipynb#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom scipy.stats import norm, zscore\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PCA Analysis in Python\nDESCRIPTION: Imports necessary Python libraries for performing PCA analysis, cointegration testing, and data visualization including sklearn, arch, statsmodels, numpy and matplotlib.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/06 PCA and Pairs Trading/00 Principle Component Analysis and Pairs Trading.ipynb#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.decomposition import PCA\nfrom arch.unitroot.cointegration import engle_granger\nfrom statsmodels.tsa.stattools import adfuller\nimport numpy as np\nfrom matplotlib import pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Airline Buybacks Analysis in Python\nDESCRIPTION: Imports necessary libraries for data processing, statistical analysis, and visualization including SmartInsiderTransaction for buyback data, statsmodels for logistic regression, sklearn for evaluation metrics, numpy and pandas for data manipulation, and seaborn for visualization.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/09 Airline Buybacks/00 Airline Buybacks.ipynb#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom QuantConnect.DataSource import SmartInsiderTransaction\n\nfrom statsmodels.discrete.discrete_model import Logit\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Data Analysis and Visualization in Python\nDESCRIPTION: Imports essential Python libraries including NumPy for numerical operations and Matplotlib for visualization. Also registers matplotlib converters for proper date handling in plots.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/10 Sparse Optimization/00 Sparse Optimization Index Tracking.ipynb#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Data Analysis in Python\nDESCRIPTION: Imports the necessary Python libraries for data processing and visualization. NumPy is used for numerical operations and Matplotlib for creating plots to visualize the results.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/04 Research Environment/12 Applying Research/04 Uncorrelated Assets/00 Uncorrelated Assets.ipynb#2025-04-18_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom matplotlib import pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements List\nDESCRIPTION: A text file listing Python package dependencies and their versions in a pip-compatible requirements.txt format. Includes common data science packages like pandas, numpy, scikit-learn as well as specialized libraries for quantitative finance and machine learning.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/Resources/libraries/autokeras.txt#2025-04-18_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnvidia-nvtx-cu12==12.1.105\noauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1666056362788/work\nomegaconf==2.3.0\nopenai==1.30.4\nopencv-contrib-python-headless==4.9.0.80\nopencv-python==4.10.0.82\nopenpyxl==3.1.2\nopt-einsum==3.3.0\noptree==0.11.0\noptuna==3.5.0\norjson==3.10.3\nortools==9.9.3963\nosqp==0.6.7\noverrides==7.7.0\npackaging==23.2\npamela @ file:///home/conda/feedstock_root/build_artifacts/pamela_1691565434937/work\npandas==2.1.4\n# ... [truncated for brevity]\n```\n\n----------------------------------------\n\nTITLE: Python Requirements List\nDESCRIPTION: A requirements.txt style file listing Python packages and their specific versions required for the project. Contains machine learning, data science, quantitative finance, and visualization libraries.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/Resources/libraries/default-python.txt#2025-04-18_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nabsl-py==2.1.0\naccelerate==0.30.1\nadagio==0.2.4\naesara==2.9.3\naiohttp==3.9.5\naiosignal==1.3.1\naiosqlite==0.20.0\nalembic==1.13.1\nalibi-detect==0.12.0\nalphalens-reloaded==0.4.3\naltair==5.2.0\naniso8601==9.0.1\nannotated-types==0.7.0\nantlr4-python3-runtime==4.9.3\nanyio==4.4.0\nappdirs==1.4.4\napricot-select==0.6.1\narch==6.3.0\nargon2-cffi==23.1.0\nargon2-cffi-bindings==21.2.0\narrow==1.3.0\narviz==0.18.0\nastropy==6.0.0\nastropy-iers-data==0.2024.6.3.0.31.14\nasttokens==2.4.1\nastunparse==1.6.3\nasync-lru==2.0.4\nattrs==23.2.0\nAuthlib==1.3.0\nautograd==1.6.2\nautograd-gamma==0.5.0\nautokeras==2.0.0\nautoray==0.6.12\nax-platform==0.3.7\nBabel==2.15.0\nbayesian-optimization==1.4.3\nbeautifulsoup4==4.12.3\nbleach==6.1.0\nblinker==1.8.2\nblis==0.7.11\nblosc2==2.6.2\nbokeh==3.3.4\nbotorch==0.10.0\nBottleneck==1.3.8\ncachetools==5.3.3\ncaptum==0.7.0\ncatalogue==2.0.10\ncatboost==1.2.3\ncategory-encoders==2.6.3\ncausal-conv1d==1.2.0.post2\nchardet==5.2.0\ncheck-shapes==1.1.1\nclarabel==0.9.0\nclick==8.1.7\nclikit==0.6.2\ncloudpathlib==0.16.0\ncloudpickle==3.0.0\ncmdstanpy==1.2.1\ncolorama==0.4.6\ncolorcet==3.1.0\ncolorlog==6.8.2\ncolorlover==0.3.0\ncolour==0.1.5\ncomm==0.2.2\nconfection==0.1.5\ncons==0.4.6\ncontourpy==1.2.0\ncontrol==0.9.4\ncopulae==0.7.9\ncopulas==0.10.1\ncoreforecast==0.0.9\ncramjam==2.8.3\ncrashtest==0.3.1\ncreme==0.6.1\ncufflinks==0.17.3\ncvxopt==1.3.2\ncvxpy==1.4.2\ncycler==0.12.1\ncymem==2.0.8\nCython==3.0.10\ndarts==0.28.0\ndash==2.17.0\ndash-core-components==2.0.0\ndash-cytoscape==1.0.1\ndash-html-components==2.0.0\ndash-table==5.0.0\ndask==2024.3.1\ndask-expr==1.0.5\ndataclasses-json==0.6.6\ndatasets==2.17.1\ndeap==1.4.1\ndebugpy==1.6.7.post1\ndecorator==5.1.1\ndeepmerge==1.1.1\ndefusedxml==0.7.1\nDeprecated==1.2.14\ndeprecation==2.1.0\ndgl==2.1.0\ndill==0.3.8\ndimod==0.12.14\ndirtyjson==1.0.8\ndiskcache==5.6.3\ndistributed==2024.3.1\ndm-tree==0.1.8\ndocker==7.1.0\ndocutils==0.20.1\nDoubleML==0.7.1\ndropstackframe==0.1.0\ndtreeviz==2.2.2\ndtw-python==1.3.1\ndwave-cloud-client==0.11.3\ndwave-drivers==0.4.4\ndwave-greedy==0.3.0\ndwave-hybrid==0.6.11\ndwave-inspector==0.4.4\ndwave-inspectorapp==0.3.1\ndwave-neal==0.6.0\ndwave-networkx==0.8.14\ndwave-ocean-sdk==6.9.0\ndwave-preprocessing==0.6.5\ndwave-samplers==1.2.0\ndwave-system==1.23.0\ndwave-tabu==0.5.0\ndwavebinarycsp==0.3.0\necos==2.0.13\neinops==0.7.0\nEMD-signal==1.6.0\nempyrical-reloaded==0.5.10\nen-core-web-md==3.7.1\nen-core-web-sm==3.7.1\nentrypoints==0.4\net-xmlfile==1.1.0\netuples==0.3.9\nexchange_calendars==4.5.4\nexecuting==2.0.1\nfaiss-cpu==1.8.0\nFarama-Notifications==0.0.4\nfastai==2.7.14\nfastai2==0.0.30\nfastcore==1.5.43\nfastdownload==0.0.7\nfasteners==0.19\nfastjsonschema==2.19.1\nfastparquet==2024.2.0\nfastprogress==1.0.3\nfasttext==0.9.2\nfeature-engine==1.6.2\nfeaturetools==1.30.0\nfilelock==3.14.0\nfindiff==0.10.0\nFixedEffectModel==0.0.5\nFlagEmbedding==1.2.10\nFLAML==2.1.2\nFlask==3.0.3\nflatbuffers==24.3.25\nfonttools==4.53.0\nformulaic==1.0.1\nfqdn==1.5.1\nfrozendict==2.4.4\nfrozenlist==1.4.1\nfs==2.4.16\nfsspec==2023.10.0\nfugue==0.9.0\nfunctime==0.9.5\nfuture==1.0.0\nfuzzy-c-means==1.7.2\ngast==0.5.4\ngensim==4.3.2\ngevent==24.2.1\ngitdb==4.0.11\nGitPython==3.1.43\ngluonts==0.14.4\ngoogle-pasta==0.2.0\ngpflow==2.9.1\ngplearn==0.4.2\ngpytorch==1.11\ngraphene==3.3\ngraphql-core==3.2.3\ngraphql-relay==3.2.0\ngraphviz==0.20.1\ngreenlet==3.0.3\ngrpcio==1.64.1\ngunicorn==21.2.0\ngym==0.26.2\ngym-notices==0.0.8\ngymnasium==0.28.1\nh11==0.14.0\nh2o==3.46.0.1\nh5netcdf==1.3.0\nh5py==3.11.0\nhmmlearn==0.3.2\nholidays==0.50\nholoviews==1.18.3\nhomebase==1.0.1\nhopcroftkarp==1.2.5\nhtml5lib==1.1\nhttpcore==1.0.5\nhttpstan==4.12.0\nhttpx==0.27.0\nhuggingface-hub==0.23.2\nhurst==0.0.5\nhvplot==0.9.2\nhydra-core==1.3.0\nhyperopt==0.2.7\nibm-cloud-sdk-core==3.20.1\nibm-platform-services==0.53.7\niisignature==0.24\nijson==3.2.3\nimageio==2.34.1\nimbalanced-learn==0.12.0\nimmutabledict==4.2.0\nimportlib_metadata==7.1.0\nimportlib_resources==6.4.0\niniconfig==2.0.0\ninjector==0.21.0\ninterface-meta==1.3.0\ninterpret==0.5.1\ninterpret-core==0.5.1\nipykernel==6.29.4\nipython==8.25.0\nipywidgets==8.1.2\nisoduration==20.11.0\nitsdangerous==2.2.0\njax==0.4.25\njax-jumpy==1.0.0\njaxlib==0.4.25\njaxtyping==0.2.29\njedi==0.19.1\nJinja2==3.1.4\njoblib==1.3.2\njson5==0.9.25\njsonpatch==1.33\njsonpath-ng==1.6.1\njsonpointer==2.1\njsonschema==4.21.1\njsonschema-specifications==2023.12.1\njupyter==1.0.0\njupyter-console==6.6.3\njupyter-events==0.10.0\njupyter-lsp==2.2.5\njupyter-resource-usage==1.0.2\njupyter_ai==2.12.0\njupyter_ai_magics==2.16.0\njupyter_bokeh==4.0.0\njupyter_client==8.6.2\njupyter_core==5.7.2\njupyter_server==2.14.1\njupyter_server_terminals==0.5.3\njupyterlab==4.1.5\njupyterlab_pygments==0.3.0\njupyterlab_server==2.27.2\njupyterlab_widgets==3.0.11\nkagglehub==0.2.5\nkaleido==0.2.1\nkeras==3.3.3\nkeras-core==0.1.7\nkeras-nlp==0.12.1\nkeras-rl==0.4.2\nkeras-tcn==3.5.0\nkeras-tuner==1.4.7\nkiwisolver==1.4.5\nkmapper==2.0.1\nkorean-lunar-calendar==0.3.1\nkt-legacy==1.0.5\nlangchain==0.1.12\nlangchain-community==0.0.38\nlangchain-core==0.1.52\nlangchain-text-splitters==0.0.2\nlangcodes==3.4.0\nlangsmith==0.1.67\nlanguage_data==1.2.0\nlark==1.1.9\nlazy_loader==0.4\nlazypredict-nightly==0.3.0\nlibclang==18.1.1\nlifelines==0.28.0\nlightgbm==4.3.0\nlightning==2.2.5\nlightning-utilities==0.11.2\nlime==0.2.0.1\nline-profiler==4.1.2\nlinear-operator==0.5.1\nlinkify-it-py==2.0.3\nlivelossplot==0.5.5\nllama-index==0.10.19\nllama-index-agent-openai==0.1.7\nllama-index-cli==0.1.12\nllama-index-core==0.10.43\nllama-index-embeddings-openai==0.1.10\nllama-index-indices-managed-llama-cloud==0.1.6\nllama-index-legacy==0.9.48\nllama-index-llms-openai==0.1.22\nllama-index-multi-modal-llms-openai==0.1.6\nllama-index-program-openai==0.1.6\nllama-index-question-gen-openai==0.1.3\nllama-index-readers-file==0.1.23\nllama-index-readers-llama-parse==0.1.4\nllama-parse==0.4.4\nllamaindex-py-client==0.1.19\nllvmlite==0.42.0\nlocket==1.0.0\nlogical-unification==0.4.6\nlxml==5.1.0\nlz4==4.3.3\nMako==1.3.5\nmamba-ssm==1.2.0.post1\nMAPIE==0.8.3\nmarisa-trie==1.1.1\nMarkdown==3.6\nmarkdown-it-py==3.0.0\nMarkupSafe==2.1.5\nmarshmallow==3.21.2\nmatplotlib==3.7.5\nmatplotlib-inline==0.1.7\nmdit-py-plugins==0.4.1\nmdurl==0.1.2\nmgarch==0.3.0\nminiKanren==1.0.3\nminorminer==0.2.13\nmistune==3.0.2\nml-dtypes==0.3.2\nmlflow==2.11.1\nmlforecast==0.12.0\nmljar-supervised==1.1.6\nmlxtend==0.23.1\nmmh3==2.5.1\nmodin==0.26.1\nmplfinance==0.12.10b0\nmpmath==1.3.0\nmsgpack==1.0.8\nmultidict==6.0.5\nmultipledispatch==1.0.0\nmultiprocess==0.70.16\nmultitasking==0.0.11\nmurmurhash==1.0.10\nmypy-extensions==1.0.0\nnamex==0.0.8\nnbclient==0.10.0\nnbconvert==7.16.4\nnbformat==5.10.4\nndindex==1.8\nnest-asyncio==1.6.0\nnetworkx==3.3\nneural-tangents==0.6.5\nneuralprophet==0.8.0\nnfoursid==1.0.1\nngboost==0.5.1\nninja==1.11.1.1\nnltk==3.8.1\nnolds==0.5.2\nnose==1.3.7\nnotebook==7.1.3\nnotebook_shim==0.2.4\nnumba==0.59.0\nnumerapi==2.18.0\nnumexpr==2.10.0\nnumpy==1.26.4\nnvidia-cublas-cu12==12.1.3.1\nnvidia-cuda-cupti-cu12==12.1.105\nnvidia-cuda-nvrtc-cu12==12.1.105\nnvidia-cuda-runtime-cu12==12.1.105\nnvidia-cudnn-cu12==8.9.2.26\nnvidia-cufft-cu12==11.0.2.54\nnvidia-curand-cu12==10.3.2.106\nnvidia-cusolver-cu12==11.4.5.107\nnvidia-cusparse-cu12==12.1.0.106\nnvidia-nccl-cu12==2.19.3\nnvidia-nvjitlink-cu12==12.5.40\nnvidia-nvtx-cu12==12.1.105\noauthlib==3.2.2\nomegaconf==2.3.0\nopenai==1.30.4\nopencv-contrib-python-headless==4.9.0.80\nopencv-python==4.10.0.82\nopenpyxl==3.1.2\nopt-einsum==3.3.0\noptree==0.11.0\noptuna==3.5.0\norjson==3.10.3\nortools==9.9.3963\nosqp==0.6.7\noverrides==7.7.0\npackaging==23.2\npandas==2.1.4\npandas-flavor==0.6.0\npandas-ta==0.3.14b0\npandas_market_calendars==4.4.0\npandocfilters==1.5.1\npanel==1.3.8\nparam==2.1.0\nparso==0.8.4\npartd==1.4.2\npastel==0.2.1\npathos==0.3.2\npatsy==0.5.6\npbr==6.0.0\npeewee==3.17.3\npeft==0.11.1\npenaltymodel==1.1.0\nPennyLane==0.35.1\nPennyLane-qiskit==0.35.1\nPennyLane_Lightning==0.35.1\npersim==0.3.5\npexpect==4.9.0\npgmpy==0.1.25\npillow==10.3.0\npingouin==0.5.4\nplotly==5.20.0\nplotly-resampler==0.10.0\nplucky==0.4.3\npluggy==1.5.0\nply==3.11\npmdarima==2.0.4\npolars==0.20.15\npomegranate==1.0.4\nPOT==0.9.3\npox==0.3.4\nppft==1.7.6.8\npprofile==2.1.0\npreshed==3.0.9\nprometheus_client==0.20.0\nprompt_toolkit==3.0.45\nprophet==1.1.5\nprotobuf==4.25.3\npsutil==5.9.8\nptvsd==4.3.2\nptyprocess==0.7.0\nPuLP==2.8.0\npure-eval==0.2.2\npy-cpuinfo==9.0.0\npy-heat==0.0.6\npy-heat-magic==0.0.2\npy-lets-be-rational==1.0.1\npy-vollib==1.0.1\npy4j==0.10.9.7\npyaml==24.4.0\npyarrow==15.0.1\npyarrow-hotfix==0.6\npybind11==2.12.0\npycaret==3.3.2\npydantic==2.7.3\npydantic_core==2.18.4\npydevd-pycharm==231.9225.15\npydmd==1.0.0\npyerfa==2.0.1.4\npyfolio-reloaded==0.9.5\nPygments==2.18.0\nPyJWT==2.8.0\npykalman==0.9.7\npylev==1.4.0\npyluach==2.2.0\npymannkendall==1.4.3\npymc==5.10.4\npymdptoolbox==4.0b3\npynndescent==0.5.12\npyod==2.0.0\nPyomo==6.7.1\npyparsing==3.1.2\npypdf==4.2.0\npyportfolioopt==1.5.5\npyre-extensions==0.0.30\npyro-api==0.1.2\npyro-ppl==1.9.0\npysimdjson==5.0.2\npyspnego==0.10.2\npystan==3.9.0\npytensor==2.18.6\npytest==8.2.1\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.0\npython-json-logger==2.0.7\npython-statemachine==2.1.2\npytorch-ignite==0.4.13\npytorch-lightning==1.9.5\npytorch-tabnet==4.1.0\npytz==2024.1\npyvinecopulib==0.6.5\npyviz_comms==3.0.2\nPyWavelets==1.5.0\nPyYAML==6.0.1\npyzmq==26.0.3\nqdldl==0.1.7.post2\nqiskit==1.0.2\nqiskit-aer==0.14.2\nqiskit-ibm-provider==0.11.0\nqiskit-ibm-runtime==0.20.0\nqtconsole==5.5.2\nQtPy==2.4.1\nquadprog==0.1.12\nquantecon==0.7.2\nQuantLib==1.33\nQuantStats==0.0.62\nquerystring-parser==1.2.4\nrauth==0.7.3\nray==2.9.3\nRbeast==0.1.19\nrectangle-packer==2.0.2\nreferencing==0.35.1\nregex==2024.5.15\nrequests-ntlm==1.2.0\nrequests-oauthlib==1.3.1\nretrying==1.3.4\nrfc3339-validator==0.1.4\nrfc3986-validator==0.1.1\nrich==13.7.1\nripser==0.6.8\nRiskfolio-Lib==6.0.0\nriskparityportfolio==0.5.1\nriver==0.21.0\nrpds-py==0.18.1\nruptures==1.1.9\nrustworkx==0.14.2\nsafetensors==0.4.3\nSALib==1.5.0\nschemdraw==0.15\nscikeras==0.13.0\nscikit-base==0.7.8\nscikit-image==0.22.0\nscikit-learn==1.4.2\nscikit-learn-extra==0.3.0\nscikit-optimize==0.10.0\nscikit-plot==0.3.7\nscikit-tda==1.0.0\nscipy==1.11.4\nscs==3.2.4.post2\nsdeint==0.3.0\nseaborn==0.13.2\nsemantic-version==2.10.0\nSend2Trash==1.8.3\nsentence-transformers==3.0.0\nsetuptools-scm==8.1.0\nshap==0.45.0\nShimmy==1.3.0\n```\n\n----------------------------------------\n\nTITLE: HTML Classes Reference Table in Markdown\nDESCRIPTION: A detailed table that lists HTML classes for documentation styling, including their specific usage contexts. Classes are organized for various UI elements such as buttons, fields, images, and text formatting.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/Style Guide/style-guide-html-classes.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Class | Usage |\n| -- | -- |\n| `box-name` | Check boxes and text boxes |\n| `button-name` | Button names |\n| `column-name` | Column names |\n| `docs-image` | Images |\n| `docs-video` | Videos |\n| `document-title` | Documents (ie: Alpha Stream Author Agreement ) |\n| `error-messages` | Error messages |\n| `field-name` | GUI field names (ie: text fields and drop-down menus) |\n| `highlight` | Important notes |\n| `icon-name` | Icon names |\n| `key-combinations` | Keyboard keystroke names (ie: Shift+F5) |\n| `latex-variable` | Latex variable names embedded in paragraphs (ie: where `x` is ...) |\n| `live-url` | Active URLs, not examples |\n| `menu-name` | Menu items |\n| `new-term` | Introducing a new term for the first time |\n| `page-section-name` | Page sections, but not windows, dialog boxs, wizards, page names, panes, or panels. This element isn't mentioned in the Rackspace Style Guide, but it's demonstrated in their [Invoice Details](https://docs.rackspace.com/docs/portal-onboarding-guide/understand_billing/invoice_details) example|\n| `placeholder-text` | Placeholder text examples |\n| `public-directory-name` | Directory names |\n| `public-file-name` | File names |\n| `qualifier` | Identifying optional steps in tutorials *(Optional)* |\n| `tab-name` | Names of tabs |\n| `ul-option` | For highlighting options in unorganized lists. See [Lists](https://docs.rackspace.com/docs/style-guide/style/lists) in the Rackspace Style Guide. |\n```\n\n----------------------------------------\n\nTITLE: PHP Global Variables Reference Table for Documentation Pre-Hooks\nDESCRIPTION: A reference table showing available PHP global variables that can be used in documentation pre-hooks. These variables provide information about breadcrumbs, anchors, and file paths that are useful when developing documentation.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/CONTRIBUTING.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Variable  | Description  |\n|---|---|\n| `BREADCRUMBS`  | An `Array<String>` where each element represents a level of the breadcrumbs. For example, [\"cloud-platform\",\"organizations\",\"data-storage\"].  |\n| `ANCHOR`  | A string that represents the partial URL of an `<h3>` tag in the documentation. For example, \"#remove-option-contract\".  |\n| `DOCS_RESOURCES`  | String path to the [docs Resources folder](https://github.com/QuantConnect/Documentation/tree/master/Resources).  |\n| `DOCS_ROOT` | String path to the [docs root folder](https://github.com/QuantConnect/Documentation).  |\n| `DOCS_URL` | A function that returns part of the URL. For example, `DOCS_URL() => \"cloud-platform/organizations/object-store\"` and `DOCS_URL(0) => \"cloud-platform\"`.  |\n```\n\n----------------------------------------\n\nTITLE: Creating Complex Code Blocks in HTML\nDESCRIPTION: This HTML snippet demonstrates how to create complex code blocks for multiple programming languages using pre-formatted text elements within a container div.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/Style Guide/style-guide.md#2025-04-18_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"section-example-container\">\n\t<pre class=\"python\"></pre>\n\t<pre class=\"csharp\"></pre>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Terminal Command Example in Markdown\nDESCRIPTION: A simple example showing how to format a command for retrieving a user ID in documentation.\nSOURCE: https://github.com/quantconnect/documentation/blob/master/Style Guide/style-guide.md#2025-04-18_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n`uid`\n```"
  }
]
[
  {
    "owner": "postgresml",
    "repo": "postgresml",
    "content": "TITLE: Unified Retrieval and Reranking in PostgreSQL\nDESCRIPTION: This query combines embedding, semantic search, and reranking using pgml.embed and pgml.rank functions. It retrieves and reranks chunks based on a given query, demonstrating the efficiency of performing these operations in a single database query.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/unified-rag.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH embedded_query AS (\n    SELECT\n        pgml.embed('mixedbread-ai/mxbai-embed-large-v1', 'How do I write a select statement with pgml.transform?', '{\"prompt\": \"Represent this sentence for searching relevant passages: \"}')::vector embedding\n),\nvector_search AS (\n    SELECT\n      chunks.id,\n      (\n          SELECT\n              embedding\n          FROM embedded_query) <=> embeddings.embedding cosine_distance,\n      chunks.chunk\n  FROM\n      chunks\n  INNER JOIN embeddings ON embeddings.chunk_id = chunks.id\n  ORDER BY\n      embeddings.embedding <=> (\n          SELECT\n              embedding\n          FROM embedded_query)\n  LIMIT 6\n),\nrow_number_vector_search AS (\n    SELECT\n        cosine_distance,\n        chunk,\n        ROW_NUMBER() OVER () AS row_number\n    FROM\n        vector_search\n)\nSELECT\n    cosine_distance,\n    (rank).score AS rank_score,\n    chunk\nFROM (\n    SELECT\n      cosine_distance,\n      rank,\n      chunk\n    FROM\n        row_number_vector_search AS rnsv1\n    INNER JOIN (\n        SELECT\n          pgml.rank('mixedbread-ai/mxbai-rerank-base-v1', 'How do I write a select statement with pgml.transform?', array_agg(\"chunk\"), '{\"return_documents\": false, \"top_k\": 6}'::jsonb || '{}') AS rank\n        FROM\n          row_number_vector_search\n    ) AS rnsv2 ON (rank).corpus_id + 1 = rnsv1.row_number\n) AS sub_query;\n```\n\n----------------------------------------\n\nTITLE: Executing Unified Retrieval and Reranking in PostgreSQL with PostgresML\nDESCRIPTION: This SQL query demonstrates a complete workflow for semantic search and reranking. It generates embeddings, performs vector search, and reranks results using the pgml.embed and pgml.rank functions. The query efficiently combines multiple steps of a RAG (Retrieval Augmented Generation) flow into a single database operation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/unified-rag.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH embedded_query AS (\n    SELECT\n        pgml.embed('mixedbread-ai/mxbai-embed-large-v1', 'How do I write a select statement with pgml.transform?', '{\"prompt\": \"Represent this sentence for searching relevant passages: \"}'::vector embedding\n),\nvector_search AS (\n    SELECT\n      chunks.id,\n      (\n          SELECT\n              embedding\n          FROM embedded_query) <=> embeddings.embedding cosine_distance,\n      chunks.chunk\n  FROM\n      chunks\n  INNER JOIN embeddings ON embeddings.chunk_id = chunks.id\n  ORDER BY\n      embeddings.embedding <=> (\n          SELECT\n              embedding\n          FROM embedded_query)\n  LIMIT 6\n),\nrow_number_vector_search AS (\n    SELECT\n        cosine_distance,\n        chunk,\n        ROW_NUMBER() OVER () AS row_number\n    FROM\n        vector_search\n)\nSELECT\n    cosine_distance,\n    (rank).score AS rank_score,\n    chunk\nFROM (\n    SELECT\n      cosine_distance,\n      rank,\n      chunk\n    FROM\n        row_number_vector_search AS rnsv1\n    INNER JOIN (\n        SELECT\n          pgml.rank('mixedbread-ai/mxbai-rerank-base-v1', 'How do I write a select statement with pgml.transform?', array_agg(\"chunk\"), '{\"return_documents\": false, \"top_k\": 6}'::jsonb || '{}') AS rank\n        FROM\n          row_number_vector_search\n    ) AS rnsv2 ON (rank).corpus_id + 1 = rnsv1.row_number\n) AS sub_query;\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG with Korvus for Question Answering in Python\nDESCRIPTION: Defines a function to perform Retrieval Augmented Generation using Korvus. The function executes vector search, reranks results, and generates answers using a large language model, all in a single database query. It supports customized prompts and contextual responses.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-firecrawl-rag-in-a-single-query.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasync def do_rag(user_query):\n    results = await collection.rag(\n        {\n            \"CONTEXT\": {\n                \"vector_search\": {\n                    \"query\": {\n                        \"fields\": {\n                            \"markdown\": {\n                                \"query\": user_query,\n                                \"parameters\": {\n                                    \"prompt\": \"Represent this sentence for searching relevant passages: \"\n                                },\n                            }\n                        },\n                    },\n                    \"document\": {\"keys\": [\"id\"]},\n                    \"rerank\": {\n                        \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\n                        \"query\": user_query,\n                        \"num_documents_to_rerank\": 100,\n                    },\n                    \"limit\": 5,\n                },\n                \"aggregate\": {\"join\": \"\\n\\n\\n\"},\n            },\n            \"chat\": {\n                \"model\": \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a question and answering bot. Answer the users question given the context succinctly.\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Given the context\\n<context>\\n:{{CONTEXT}}\\n</context>\\nAnswer the question: {user_query}\",\n                    },\n                ],\n                \"max_tokens\": 256,\n            },\n        },\n        pipeline,\n    )\n    return results\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Pass Search Ranking with PostgresML\nDESCRIPTION: This SQL query demonstrates a three-pass search ranking approach combining term frequency, embedding similarities, and machine learning inference. It shows how to progressively filter and re-rank search results using different relevance signals while maintaining all data in a single Postgres database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgres-full-text-search-is-awesome.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH query AS (\n  -- construct a query context with arguments that would typically be\n  -- passed in from the application layer\n  SELECT\n    -- a keyword query for \"my\" OR \"search\" OR \"terms\"\n    tsquery('my | search | terms') AS keywords,\n    -- a user_id for personalization later on\n    123456 AS user_id\n),\nfirst_pass AS (\n  SELECT *,\n    -- calculate the term frequency of keywords in the document\n    ts_rank(documents.full_text, keywords) AS term_frequency\n  -- our basic corpus is stored in the documents table\n  FROM documents\n  -- that match the query keywords defined above\n  WHERE documents.full_text @@ query.keywords\n  -- ranked by term frequency\n  ORDER BY term_frequency DESC\n  -- prune to a reasonably large candidate population\n  LIMIT 10000\n),\nsecond_pass AS (\n  SELECT *,\n    -- create a second pass score of cosine_similarity across embeddings\n    pgml.cosine_similarity(document_embeddings.vector, user_embeddings.vector) AS similarity_score\n  FROM first_pass\n  -- grab more data from outside the documents\n  JOIN document_embeddings ON document_embeddings.document_id = documents.id\n  JOIN user_embeddings ON user_embeddings.user_id = query.user_id\n  -- of course we be re-ranking\n  ORDER BY similarity_score DESC\n  -- further prune results to top performers for more expensive ranking\n  LIMIT 1000\n),\nthird_pass AS (\n  SELECT *,\n    -- create a final score using xgboost\n    pgml.predict('search relevance model', ARRAY[session_level_features.*]) AS final_score\n  FROM second_pass\n  JOIN session_level_features ON session_level_features.user_id = query.user_id\n)\nSELECT *\nFROM third_pass\nORDER BY final_score DESC\nLIMIT 100;\n```\n\n----------------------------------------\n\nTITLE: Training a Regression Model for Search Ranking in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to train a regression model for search result ranking using the pgml.train function. It specifies the project name, task type, input data relation, and target column.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n  project_name => 'Search Ranking',\n  task => 'regression',\n  relation_name => 'search_result_clicks',\n  y_column_name => 'clicked'\n);\n```\n\n----------------------------------------\n\nTITLE: Boosted and Reranked Vector Search Query\nDESCRIPTION: Enhanced vector search implementation that combines semantic similarity with popularity metrics. Uses CTEs to first perform vector similarity search, then reranks results based on star ratings and review counts.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\n-- create a request embedding on the fly\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'query: Best 1980\\'s scifi movie'\n  )::vector(1024) AS embedding\n),\n\n-- vector similarity search for movies\nfirst_pass AS (\n    SELECT\n      title,\n      total_reviews,\n      star_rating_avg,\n      1 - (\n        review_embedding_e5_large <=> (SELECT embedding FROM request)\n      ) AS cosine_similarity,\n      star_rating_avg / 5 AS star_rating_score\n    FROM movies\n    WHERE total_reviews > 10\n    ORDER BY review_embedding_e5_large <=> (SELECT embedding FROM request)\n    LIMIT 1000\n)\n\n-- grab the top 10 results, re-ranked with a boost for the avg star rating\nSELECT\n  title,\n  total_reviews,\n  round(star_rating_avg, 2) as star_rating_avg,\n  star_rating_score,\n  cosine_similarity,\n  cosine_similarity + star_rating_score AS final_score\nFROM first_pass\nORDER BY final_score DESC\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Distance in JavaScript and Python\nDESCRIPTION: Implementation of Cosine distance calculation between two vectors, which measures angle difference independent of magnitude.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nfunction cosineDistance(a, b) {\n    let dotProduct = 0;\n    let normA = 0;\n    let normB = 0;\n\n    for (let i = 0; i < a.length; i++) {\n        dotProduct += a[i] * b[i];\n        normA += a[i] * a[i];\n        normB += b[i] * b[i];\n    }\n\n    normA = Math.sqrt(normA);\n    normB = Math.sqrt(normB);\n\n    if (normA === 0 || normB === 0) {\n        throw new Error(\"Norm of one or both vectors is 0, cannot compute cosine similarity.\");\n    }\n\n    const cosineSimilarity = dotProduct / (normA * normB);\n    const cosineDistance = 1 - cosineSimilarity;\n\n    return cosineDistance;\n}\n```\n\nLANGUAGE: python\nCODE:\n```\ndef cosine_distance(a, b):\n    dot_product = 0\n    normA = 0\n    normB = 0\n\n    for a, b in zip(a, b):\n        dot_product += a * b\n        normA += a * a\n        normB += b * b\n\n    normA = math.sqrt(normA)\n    normB = math.sqrt(normB)\n\n    if normA == 0 or normB == 0:\n        raise ValueError(\"Norm of one or both vectors is 0, cannot compute cosine similarity.\")\n\n    cosine_similarity = dot_product / (normA * normB)\n    cosine_distance = 1 - cosine_similarity\n\n    return cosine_distance\n```\n\n----------------------------------------\n\nTITLE: LLM Query with Context Implementation in Python\nDESCRIPTION: Shows how to query an LLM with additional context from Wikipedia, demonstrating context length limitations. Includes fetching external content and combining it with user input.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/chatbots/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nuser_input = \"What is Baldur's Gate 3?\"\ncontext = get_text_from_url(\"https://en.wikipedia.org/wiki/Baldur's_Gate_3\") # Strips HTML and gets just the text from the url \ntokenized_input = tokenize(user_input + context) # Tokenizes the input and context something like [25, 12, ... 30000, 29567, ...]\noutput = model(tokenized_input)\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Vector Search with Re-ranking in Python\nDESCRIPTION: This Python code snippet demonstrates a vector search with re-ranking using the `collection.vector_search` method. It includes parameters for the search query, re-ranking model (`mixedbread-ai/mxbai-rerank-base-v1`), the query used for re-ranking, and the number of documents to re-rank. The `pipeline` variable is a required dependency, likely representing a pre-configured search pipeline.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.vector_search(\n    {\n        \"query\": {\n            \"fields\": {\n                \"body\": {\n                    \"query\": \"What is the best database?\",\n                    \"parameters\": {\n                        \"prompt\": \"Represent this sentence for searching relevant passages: \",\n                    },\n                },\n            },\n        },\n        \"rerank\": {\n            \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\n            \"query\": \"What is the best database\",\n            \"num_documents_to_rerank\": 100,\n        },\n        \"limit\": 5,\n    },\n    pipeline,\n)\n```\n\n----------------------------------------\n\nTITLE: Text Completion with Parameters in PostgresML\nDESCRIPTION: This example shows how to use PostgresML for text completion with additional parameters such as max_tokens, temperature, and seed. It uses the 'meta-llama/Meta-Llama-3.1-8B-Instruct' model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-generation.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n        \"task\": \"text-generation\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone'\n    ],\n    args => '{\n        \"max_tokens\": 10,\n        \"temperature\": 0.75,\n        \"seed\": 10\n    }'::JSONB\n) AS answer;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\", Nine for Mortal Men doomed to die,\"]\n```\n\n----------------------------------------\n\nTITLE: Executing RAG Query with Multiple Context Variables in Python\nDESCRIPTION: This Python code demonstrates how to perform a RAG operation with both vector search (LLM_CONTEXT) and custom SQL query (CUSTOM_CONTEXT). It uses vector search with reranking to find relevant documents about 'Korvus', while also executing a custom SQL query that returns a string. Both context elements are used to answer a question about Korvus's speed.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.rag(\n    {\n        \"LLM_CONTEXT\": {\n            \"vector_search\": {\n                \"query\": {\n                    \"fields\": {\n                        \"text\": {\n                            \"query\": \"Is Korvus fast?\",\n                            \"parameters\": {\n                                \"prompt\": \"Represent this sentence for searching relevant passages: \"\n                            },\n                            \"full_text_filter\": \"Korvus\",\n                        }\n                    },\n                },\n                \"document\": {\"keys\": [\"id\"]},\n                \"rerank\": {\n                    \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\n                    \"query\": \"Is Korvus fast?\",\n                    \"num_documents_to_rerank\": 100,\n                },\n                \"limit\": 5,\n            },\n            \"aggregate\": {\"join\": \"\\n\"},\n        },\n        \"CUSTOM_CONTEXT\": {\"sql\": \"SELECT 'Korvus is super fast!!!'\"},\n        \"chat\": {\n            \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a friendly and helpful chatbot\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Given the context\\n:{LLM_CONTEXT}\\n{CUSTOM_CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n                },\n            ],\n            \"max_tokens\": 100,\n        },\n    },\n    pipeline,\n)\n```\n\n----------------------------------------\n\nTITLE: Unified Retrieval, Reranking, and Text Generation in PostgreSQL\nDESCRIPTION: This query extends the previous example by adding text generation using pgml.transform. It performs embedding, semantic search, reranking, and text generation in a single SQL query, showcasing the full RAG pipeline within the database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/unified-rag.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH embedded_query AS (\n    SELECT\n        pgml.embed('mixedbread-ai/mxbai-embed-large-v1', 'How do I write a select statement with pgml.transform?', '{\"prompt\": \"Represent this sentence for searching relevant passages: \"}')::vector embedding\n),\nvector_search AS (\n    SELECT\n      chunks.id,\n      (\n          SELECT\n              embedding\n          FROM embedded_query) <=> embeddings.embedding cosine_distance,\n      chunks.chunk\n  FROM\n      chunks\n  INNER JOIN embeddings ON embeddings.chunk_id = chunks.id\n  ORDER BY\n      embeddings.embedding <=> (\n          SELECT\n              embedding\n          FROM embedded_query)\n  LIMIT 6\n),\nrow_number_vector_search AS (\n    SELECT\n        cosine_distance,\n        chunk,\n        ROW_NUMBER() OVER () AS row_number\n    FROM\n        vector_search\n),\ncontext AS (\n  SELECT\n      chunk\n  FROM (\n      SELECT\n        chunk\n      FROM\n          row_number_vector_search AS rnsv1\n      INNER JOIN (\n          SELECT\n            pgml.rank('mixedbread-ai/mxbai-rerank-base-v1', 'How do I write a select statement with pgml.transform?', array_agg(\"chunk\"), '{\"return_documents\": false, \"top_k\": 1}'::jsonb || '{}') AS rank\n          FROM\n            row_number_vector_search\n      ) AS rnsv2 ON (rank).corpus_id + 1 = rnsv1.row_number\n  ) AS sub_query\n)\nSELECT\n    pgml.transform (\n      task => '{\n        \"task\": \"conversational\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n      }'::jsonb, \n      inputs => ARRAY['{\"role\": \"system\", \"content\": \"You are a friendly and helpful chatbot.\"}'::jsonb, jsonb_build_object('role', 'user', 'content', replace('Given the context answer the following question: How do I write a select statement with pgml.transform? Context:\\n\\n{CONTEXT}', '{CONTEXT}', chunk))], \n      args => '{\n        \"max_new_tokens\": 100\n      }'::jsonb)\nFROM\n    context;\n```\n\n----------------------------------------\n\nTITLE: Batching Embeddings with PostgresML in PostgreSQL\nDESCRIPTION: This SQL query demonstrates how to use PostgresML's batching feature for embeddings. It aggregates the 'body' column from the 'documents' table and passes it to the pgml.embed function along with the 'intfloat/e5-small-v2' model name. This approach allows for efficient processing of multiple inputs in a single operation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_11\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed('intfloat/e5-small-v2', array_agg(body)) AS embedding\nFROM documents;\n```\n\n----------------------------------------\n\nTITLE: Re-ranking Search Results with RAG Pipeline in Rust\nDESCRIPTION: This Rust snippet demonstrates how to use `collection.rag` in Rust to perform re-ranking with `vector_search` and an LLM context. It creates a JSON object defining the search parameters, re-ranking configuration, and chat model settings. The `serde_json::json!` macro is used to build the JSON object, which is then passed to the `collection.rag` method along with a mutable reference to a pipeline. The snippet uses `await?` for asynchronous operation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection.rag(serde_json::json!(\n    {\n        \"LLM_CONTEXT\": {\n            \"vector_search\": {\n                \"query\": {\n                    \"fields\": {\n                        \"text\": {\n                            \"query\": \"Is Korvus fast?\",\n                            \"parameters\": {\n                                \"prompt\": \"Represent this sentence for searching relevant passages: \"\n                            },\n                            \"full_text_filter\": \"Korvus\"\n                        }\n                    },\n                },\n                \"document\": {\"keys\": [\"id\"]},\n                \"rerank\": {\n                    \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\n                    \"query\": \"Is Korvus fast?\",\n                    \"num_documents_to_rerank\": 100\n                },\n                \"limit\": 5,\n            },\n            \"aggregate\": {\"join\": \"\\n\"},\n        },\n        \"chat\": {\n            \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a friendly and helpful chatbot\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Given the context\\n:{LLM_CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n                },\n            ],\n            \"max_tokens\": 100,\n        },\n    }\n).into(), &mut pipeline).await?;\n```\n\n----------------------------------------\n\nTITLE: Re-ranking Search Results with RAG Pipeline in Python\nDESCRIPTION: This Python code snippet demonstrates the usage of `collection.rag` for re-ranking search results using `vector_search` and an LLM context. It sets up the query, defines re-ranking parameters, configures the chat model, and passes these configurations to the `collection.rag` method. The re-ranking model is specified, and the number of documents to be re-ranked is set. It assumes that `collection` and `pipeline` are predefined.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.rag(\n    {\n        \"LLM_CONTEXT\": {\n            \"vector_search\": {\n                \"query\": {\n                    \"fields\": {\n                        \"text\": {\n                            \"query\": \"Is Korvus fast?\",\n                            \"parameters\": {\n                                \"prompt\": \"Represent this sentence for searching relevant passages: \"\n                            },\n                            \"full_text_filter\": \"Korvus\",\n                        }\n                    },\n                },\n                \"document\": {\"keys\": [\"id\"]},\n                \"rerank\": {\n                    \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\n                    \"query\": \"Is Korvus fast?\",\n                    \"num_documents_to_rerank\": 100,\n                },\n                \"limit\": 5,\n            },\n            \"aggregate\": {\"join\": \"\\n\"},\n        },\n        \"chat\": {\n            \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a friendly and helpful chatbot\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Given the context\\n:{LLM_CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n                },\n            ],\n            \"max_tokens\": 100,\n        },\n    },\n    pipeline,\n)\n```\n\n----------------------------------------\n\nTITLE: Using pgml.embed Function in PostgreSQL\nDESCRIPTION: Basic syntax for generating embeddings from text using the pgml.embed native function. Takes a model name and text input to generate fixed-size dense vectors.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\npgml.embed(model_name, text)\n```\n\n----------------------------------------\n\nTITLE: Euclidean Distance Query in PostgreSQL\nDESCRIPTION: Optimized PostgreSQL query for calculating Euclidean distance using pgml.distance_l2 function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH query AS (\n    SELECT vector\n    FROM test_data\n    LIMIT 1\n)\nSELECT id, pgml.distance_l2(query.vector, test_data.vector)\nFROM test_data, query\nORDER BY distance_l2;\n```\n\n----------------------------------------\n\nTITLE: Initializing Pipeline and Collection for Vector Search\nDESCRIPTION: Sets up a Pipeline with semantic search and full-text search capabilities for abstract and body fields. The pipeline uses different embedding models for different fields and includes text splitting functionality.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst pipeline = korvus.newPipeline(\"test_pipeline\", {\n  abstract: {\n    semantic_search: {\n      model: \"Alibaba-NLP/gte-base-en-v1.5\",\n    },\n    full_text_search: { configuration: \"english\" },\n  },\n  body: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"mixedbread-ai/mxbai-embed-large-v1\",\n    },\n  },\n});\nconst collection = korvus.newCollection(\"test_collection\");\nawait collection.add_pipeline(pipeline);\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"test_pipeline\",\n    {\n        \"abstract\": {\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n            },\n            \"full_text_search\": {\"configuration\": \"english\"},\n        },\n        \"body\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n            },\n        },\n    },\n)\ncollection = Collection(\"test_collection\")\nawait collection.add_pipeline(pipeline);\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut pipeline = Pipeline::new(\n    \"test_pipeline\",\n    Some(\n        serde_json::json!(\n            {\n                \"abstract\": {\n                    \"semantic_search\": {\n                        \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n                    },\n                    \"full_text_search\": {\"configuration\": \"english\"},\n                },\n                \"body\": {\n                    \"splitter\": {\"model\": \"recursive_character\"},\n                    \"semantic_search\": {\n                        \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n                    },\n                },\n            }\n        )\n        .into(),\n    ),\n)?;\nlet mut collection = Collection::new(\"test_collection\", None)?;\ncollection.add_pipeline(&mut pipeline).await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nPipelineC *pipeline = korvus_pipelinec_new(\"test_pipeline\", \"{\\\n    \\\"abstract\\\": {\\\n        \\\"semantic_search\\\": {\\\n            \\\"model\\\": \\\"Alibaba-NLP/gte-base-en-v1.5\\\"\\\n        },\\\n        \\\"full_text_search\\\": {\\\"configuration\\\": \\\"english\\\"}\\\n    },\\\n    \\\"body\\\": {\\\n        \\\"splitter\\\": {\\\"model\\\": \\\"recursive_character\\\"},\\\n        \\\"semantic_search\\\": {\\\n            \\\"model\\\": \\\"mixedbread-ai/mxbai-embed-large-v1\\\"\\\n        }\\\n    }\\\n}\");\nCollectionC * collection = korvus_collectionc_new(\"test_collection\", NULL);\nkorvus_collectionc_add_pipeline(collection, pipeline);\n```\n\n----------------------------------------\n\nTITLE: Creating a Korvus Collection and Pipeline for RAG\nDESCRIPTION: This code establishes a collection and pipeline needed for RAG operations. The pipeline configuration enables both full text search and semantic search capabilities for document text fields.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst collection = korvus.newCollection(\"test_rag_collection\");\nconst pipeline = korvus.newPipeline(\"v1\", {\n  text: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"mixedbread-ai/mxbai-embed-large-v1\",\n    },\n    full_text_search: { configuration: \"english\" },\n  },\n});\nawait collection.add_pipeline(pipeline);\n```\n\nLANGUAGE: python\nCODE:\n```\ncollection = Collection(\"test_rag_collection\")\npipeline = Pipeline(\n    \"v1\",\n    {\n        \"text\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n            },\n            \"full_text_search\": {\"configuration\": \"english\"},\n        },\n    },\n)\nawait collection.add_pipeline(pipeline);\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut collection = Collection::new(\"test_rag_collection\", None)?;\nlet mut pipeline = Pipeline::new(\n    \"v1\",\n    Some(\n        serde_json::json!(\n            {\n                \"text\": {\n                    \"splitter\": {\"model\": \"recursive_character\"},\n                    \"semantic_search\": {\n                        \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n                    },\n                    \"full_text_search\": {\"configuration\": \"english\"},\n                },\n            }\n        )\n        .into(),\n    ),\n)?;\ncollection.add_pipeline(&mut pipeline).await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nCollectionC * collection = korvus_collectionc_new(\"test_rag_collection\", NULL);\nPipelineC *pipeline = korvus_pipelinec_new(\"v1\", \"{\\  \n  \\\"text\\\": {\\  \n      \\\"splitter\\\": {\\\"model\\\": \\\"recursive_character\\\"},\\  \n      \\\"semantic_search\\\": {\\  \n          \\\"model\\\": \\\"mixedbread-ai/mxbai-embed-large-v1\\\"\\  \n      },\\  \n      \\\"full_text_search\\\": {\\\"configuration\\\": \\\"english\\\"}\\  \n  }\\  \n}\");\nkorvus_collectionc_add_pipeline(collection, pipeline);\n```\n\n----------------------------------------\n\nTITLE: Creating Search Click Training Data Table in PostgreSQL\nDESCRIPTION: Creates a table to store training data for machine learning model, including title rank, body rank and whether the result was clicked.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE search_result_clicks (\n  title_rank REAL,\n  body_rank REAL,\n  clicked BOOLEAN\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing YC Job Search with Korvus and Trellis in Python\nDESCRIPTION: This script sets up a Korvus pipeline and collection, processes job data from a CSV file, ingests it into Korvus, and provides semantic search functionality. It includes functions for data ingestion, search, and an interactive search loop.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-trellis-semantic-search-over-yc-jobs.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport argparse\nimport pandas as pd\nfrom rich import print\nfrom typing import List, Dict\nfrom korvus import Pipeline, Collection\nimport json\n\n\npipeline = Pipeline(\n    \"v0\",\n    {\n        \"summary\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n            },\n        },\n    },\n)\ncollection = Collection(\"yc_job_search_v1\")\n\n\nparser = argparse.ArgumentParser(description=\"YC Job Search Tool\")\nparser.add_argument(\"action\", choices=[\"ingest\", \"search\"], help=\"Action to perform\")\n\n\ndef summarize(\n    role,\n    pay_to,\n    pay_from,\n    location,\n    technical_requirements,\n    description,\n    company_description,\n):\n    return f\"\"\"{role}\nLocation:\n{location}\n\nPay:\n{pay_from} - {pay_to}\n\nTechnical Requirements:\n{technical_requirements}\n\nJob Description:\n{description}\n\nCompany Description:\n{company_description}\"\"\"\n\n\nasync def ingest_data():\n    # Process the documents\n    # Because we download it as a CSV we have to json.loads individual columns\n    # This could be avoided if we used Trellis' API\n    df = pd.read_csv(\"trellis_unstructured_data.csv\")\n    records = df.to_dict(\"records\")\n    documents = []\n    for jobs in records:\n        if jobs[\"role\"] == \"[]\":\n            continue\n        roles = json.loads(jobs[\"role\"])\n        pay_tos = json.loads(jobs[\"pay_to\"])\n        pay_froms = json.loads(jobs[\"pay_from\"])\n        descriptions = json.loads(jobs[\"description\"])\n        technical_requirements = json.loads(jobs[\"technical_requirements\"])\n        for i, role in enumerate(roles):\n            pay_to = pay_tos[i] if len(pay_tos) > i else \"na\"\n            pay_from = pay_froms[i] if len(pay_froms) > i else \"na\"\n            description = descriptions[i] if len(descriptions) > i else \"\"\n            documents.append(\n                {\n                    \"id\": f\"\"\"{jobs[\"asset_id\"]}_{i}\"\"\",\n                    \"summary\": summarize(\n                        role,\n                        pay_to,\n                        pay_from,\n                        jobs[\"location\"],\n                        \",\".join(technical_requirements),\n                        description,\n                        jobs[\"company_description\"],\n                    ),\n                }\n            )\n\n    # Upsert the documents\n    await collection.upsert_documents(documents)\n\n\nasync def search(query_text: str):\n    results = await collection.search(\n        {\n            \"query\": {\n                \"semantic_search\": {\n                    \"summary\": {\n                        \"query\": query_text,\n                    },\n                },\n            },\n            \"limit\": 5,\n        },\n        pipeline,\n    )\n    return results[\"results\"]\n\n\nasync def search_loop():\n    while True:\n        query = input(\"Enter your search query (or 'q' to quit): \")\n        if query.lower() == \"q\":\n            break\n        results = await search(query)\n        print(\"[bold]Search Results:[/bold]\")\n        for result in results:\n            print(\n                result[\"document\"][\"summary\"], end=\"\\n\\n\"\n            )  # TODO: Format the output as needed\n            print(\"-\".join(\"\" for _ in range(0, 200)), end=\"\\n\\n\")\n\n\nasync def main():\n    args = parser.parse_args()\n\n    if args.action == \"ingest\":\n        await collection.add_pipeline(pipeline)\n        await ingest_data()\n    elif args.action == \"search\":\n        await search_loop()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Building a Complete Vector Search System with SQL\nDESCRIPTION: This comprehensive SQL example shows the entire process of creating a vector search system: creating a table with embedding vectors, adding an HNSW index for fast retrieval, inserting documents with embeddings, and querying for similar documents.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/a-speed-comparison-of-the-most-popular-retrieval-systems-for-rag.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\n-- Create a documents table\nCREATE TABLE documents (\n    id serial PRIMARY KEY,\n    text text NOT NULL,\n    embedding vector (384) -- Uses the vector data type from pgvector with dimension 384\n);\n\n-- Creates our HNSW index for super fast retreival\nCREATE INDEX documents_vector_idx ON documents USING hnsw (embedding vector_cosine_ops);\n\n-- Insert a few documents\nINSERT INTO documents (text, embedding)\n    VALUES ('The hidden value is 1000', (\n            SELECT pgml.embed (transformer => 'mixedbread-ai/mxbai-embed-large-v1', text => 'The hidden value is 1000'))),\n    ('This is just some random text',\n        (\n            SELECT pgml.embed (transformer => 'mixedbread-ai/mxbai-embed-large-v1', text => 'This is just some random text')));\n\n-- Do a query over it\nWITH \"query_embedding\" AS (\n    SELECT\n        pgml.embed (transformer => 'mixedbread-ai/mxbai-embed-large-v1', text => 'What is the hidden value', '{\"prompt\": \"Represent this sentence for searching relevant passages: \"}') AS \"embedding\"\n)\nSELECT\n    \"text\",\n    1 - (embedding <=> (\n            SELECT embedding\n            FROM \"query_embedding\")::vector) AS score\nFROM\n    documents\nORDER BY\n    embedding <=> (\n        SELECT embedding\n        FROM \"query_embedding\")::vector ASC\nLIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Search with Korvus in JavaScript\nDESCRIPTION: Demonstrates creating a collection, configuring a pipeline with semantic search capabilities, upserting documents, and performing vector search queries using the Korvus SDK. Uses the mixedbread-ai/mxbai-embed-large-v1 model for embeddings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/example-apps/semantic-search.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst korvus = require(\"korvus\");\n\n// Initialize our Collection\nconst collection = korvus.newCollection(\"semantic-search-demo\");\n\n// Initialize our Pipeline\n// Our Pipeline will split and embed the `text` key of documents we upsert\nconst pipeline = korvus.newPipeline(\"v1\", {\n  text: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"mixedbread-ai/mxbai-embed-large-v1\",\n    }\n  },\n});\n\nconst main = async () => {\n  // Add our Pipeline to our Collection\n  await collection.add_pipeline(pipeline);\n\n  // Upsert our documents\n  // The `text` key of our documents will be split and embedded per our Pipeline specification above\n  let documents = [\n    {\n      id: \"1\",\n      text: \"Korvus is incredibly fast and easy to use.\",\n    },\n    {\n      id: \"2\",\n      text: \"Tomatoes are incredible on burgers.\",\n    },\n  ]\n  await collection.upsert_documents(documents)\n\n  // Perform vector_search\n  // We are querying for the string \"Is Korvus fast?\"\n  // Notice that the `mixedbread-ai/mxbai-embed-large-v1` embedding model takes a prompt parameter when embedding for search\n  // We specify that we only want to return the `id` of documents. If the `document` key was blank it would return the entire document with every result\n  // Limit the results to 5. In our case we only have two documents in our Collection so we will only get two results\n  const results = await collection.vector_search(\n    {\n      query: {\n        fields: {\n          text: {\n            query: \"Is Korvus fast?\",\n            parameters: {\n              prompt:\n                \"Represent this sentence for searching relevant passages: \",\n            }\n          },\n        },\n      },\n      document: {\n        keys: [\n          \"id\"\n        ]\n      },\n      limit: 5,\n    },\n    pipeline);\n  console.log(results)\n}\n\nmain().then(() => console.log(\"DONE!\"))\n```\n\n----------------------------------------\n\nTITLE: Generating Query Embeddings with Instructor-XL Model in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to generate an embedding for a question using the hkunlp/instructor-xl model with an instruction for retrieving supporting documents.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed(\n    transformer => 'hkunlp/instructor-xl',\n    text => 'where is the food stored in a yam plant',\n    kwargs => '{\n        \"instruction\": \"Represent the Wikipedia question for retrieving supporting documents:\"\n    }'\n);\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with Unified RAG in PostgreSQL\nDESCRIPTION: This query demonstrates how to stream text generation results using pgml.transform_stream and cursors. It measures the time to first token, showcasing the speed of the unified RAG pipeline for streaming applications.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/unified-rag.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nBEGIN;\nDECLARE c CURSOR FOR WITH embedded_query AS (\n    SELECT\n        pgml.embed('mixedbread-ai/mxbai-embed-large-v1', 'How do I write a select statement with pgml.transform?', '{\"prompt\": \"Represent this sentence for searching relevant passages: \"}')::vector embedding\n),\nvector_search AS (\n    SELECT\n      chunks.id,\n      (\n          SELECT\n              embedding\n          FROM embedded_query) <=> embeddings.embedding cosine_distance,\n      chunks.chunk\n  FROM\n      chunks\n  INNER JOIN embeddings ON embeddings.chunk_id = chunks.id\n  ORDER BY\n      embeddings.embedding <=> (\n          SELECT\n              embedding\n          FROM embedded_query)\n  LIMIT 6\n),\nrow_number_vector_search AS (\n    SELECT\n        cosine_distance,\n        chunk,\n        ROW_NUMBER() OVER () AS row_number\n    FROM\n        vector_search\n),\ncontext AS (\n  SELECT\n      chunk\n  FROM (\n      SELECT\n        chunk\n      FROM\n          row_number_vector_search AS rnsv1\n      INNER JOIN (\n          SELECT\n            pgml.rank('mixedbread-ai/mxbai-rerank-base-v1', 'How do I write a select statement with pgml.transform?', array_agg(\"chunk\"), '{\"return_documents\": false, \"top_k\": 1}'::jsonb || '{}') AS rank\n          FROM\n            row_number_vector_search\n      ) AS rnsv2 ON (rank).corpus_id + 1 = rnsv1.row_number\n  ) AS sub_query\n)\nSELECT\n    pgml.transform_stream(\n      task => '{\n        \"task\": \"conversational\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n      }'::jsonb, \n      inputs => ARRAY['{\"role\": \"system\", \"content\": \"You are a friendly and helpful chatbot.\"}'::jsonb, jsonb_build_object('role', 'user', 'content', replace('Given the context answer the following question: How do I write a select statement with pgml.transform? Context:\\n\\n{CONTEXT}', '{CONTEXT}', chunk))], \n      args => '{\n        \"max_new_tokens\": 100\n      }'::jsonb)\nFROM\n    context;\nFETCH 2 FROM c;\nEND;\n```\n\n----------------------------------------\n\nTITLE: Vector Similarity Search with Cosine Distance\nDESCRIPTION: Demonstrates how to find the three closest matching addresses using cosine distance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/vector-database.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT address\nFROM usa_house_prices\nORDER BY \n    embedding <=> pgml.embed(\n        'Alibaba-NLP/gte-base-en-v1.5', \n        '1 Infinite Loop'\n    )::vector(384)\nLIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Defining the pgml.embed() API in PostgreSQL\nDESCRIPTION: API specification for the pgml.embed() function that generates embeddings from text using Hugging Face models. It takes a transformer model name, input text, and optional parameters as arguments.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.embed.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.embed(\n    transformer TEXT,\n    \"text\" TEXT,\n    kwargs JSONB\n)\n```\n\n----------------------------------------\n\nTITLE: Using Embeddings for Semantic Similarity Search in PostgreSQL\nDESCRIPTION: Example of using vector similarity operations to find the quote most semantically similar to a search term. The query converts the search term into an embedding and compares it with stored embeddings using the cosine similarity operator.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.embed.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT quote\nFROM star_wars_quotes\nORDER BY pgml.embed(\n    'intfloat/e5-small-v2',\n    'Feel the force!',\n  '{\"prompt\": \"query: \"}'::JSONB\n  )::vector <=> embedding DESC\nLIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Advanced Vector Search with Multiple Fields\nDESCRIPTION: Implements a more complex vector search querying both abstract and body fields, with full-text filtering on the abstract field and custom embedding parameters for the body field.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst query = \"What is the best database?\";\nconst results = await collection.vector_search(\n  {\n    query: {\n      fields: {\n        abstract: {\n          query: query,\n          full_text_filter: \"database\"\n        },\n        body: {\n          query: query, \n          parameters: {\n            instruction:\n              \"Represent this sentence for searching relevant passages: \",\n          }\n        },\n      },\n    },\n    limit: 5,\n  },\n  pipeline,\n);\n```\n\nLANGUAGE: python\nCODE:\n```\nquery = \"What is the best database?\"\nresults = await collection.vector_search(\n    {\n        \"query\": {\n            \"fields\": {\n                \"abastract\": {\n                    \"query\": query,\n                    \"full_text_filter\": \"database\",\n                },\n                \"body\": {\n                    \"query\": query,\n                    \"parameters\": {\n                        \"instruction\": \"Represent this sentence for searching relevant passages: \",\n                    },\n                },\n            },\n        },\n        \"limit\": 5,\n    },\n    pipeline,\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet query = \"What is the best database?\";\nlet results = collection\n    .vector_search(\n        serde_json::json!({\n            \"query\": {\n                \"fields\": {\n                    \"abastract\": {\n                        \"query\": query,\n                        \"full_text_filter\": \"database\",\n                    },\n                    \"body\": {\n                        \"query\": query,\n                        \"parameters\": {\n                            \"instruction\": \"Represent this sentence for searching relevant passages: \",\n                        },\n                    },\n                },\n            },\n            \"limit\": 5,\n        })\n        .into(),\n        &mut pipeline,\n    )\n    .await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nr_size = 0;\nchar **results = korvus_collectionc_vector_search(collection, \"{\\\n \"query\": {\\\n      \\\"fields\\\": {\\\n          \\\"abastract\\\": {\\\n              \\\"query\\\": \\\"What is the best database?\\\",\\\n              \\\"full_text_filter\\\": \\\"database\\\"\\\n          },\\\n          \\\"body\\\": {\\\n              \\\"query\\\": \\\"What is the best database?\\\",\\\n              \\\"parameters\\\": {\\\n                  \\\"instruction\\\": \\\"Represent this sentence for searching relevant passages: \\\"\\\n              }\\\n          }\\\n      }\\\n  },\\\n  \\\"limit\\\": 5,\\\n}\", pipeline, &r_size);\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Pipeline in Python\nDESCRIPTION: Demonstrates the complete RAG pipeline including document retrieval, chunking, embedding generation, storage, and query processing. The code shows how to fetch content from Wikipedia, split it into manageable chunks, generate embeddings, and use them for retrieval to answer questions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/chatbots/README.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndocument = get_text_from_url(\"https://en.wikipedia.org/wiki/Baldur's_Gate_3\") # Strips HTML and gets just the text from the url\nfor chunk_text in split_document(context): # Splits the document into smaller chunks of text\n  embedding = embed(chunk_text) # Returns some embedding like [0.11, 0.22, -0.97, ...]\n  store(chunk_text, embedding) # We want to store the text of the chunk and the embedding of the chunk\ninput = \"What is Baldur's Gate 3?\"\ninput_embedding = embed(input) # Returns some embedding like [0.68, -0.94, 0.32, ...]\ncontext = retrieve_from_store(input_embedding) # Returns the text of the chunk with the closest embedding ranked by cosine similarity\ntokenized_input = tokenize(input + context) # Tokenizes the input and context something like [25, 12, ... 30000, 29567, ...]\noutput = model(tokenized_input)\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Implementing Boosted Title and Body Search in PostgreSQL\nDESCRIPTION: Query demonstrating search result boosting by giving title matches twice the weight of body matches using ts_rank. Orders results by combining weighted title and body relevance scores.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT \n  ts_rank(title, to_tsquery('english', 'second | title')) AS title_rank,\n  ts_rank(body, to_tsquery('english', 'second | title')) AS body_rank,\n  *   \nFROM documents \nORDER BY (2 * title_rank) + body_rank DESC;\n```\n\n----------------------------------------\n\nTITLE: Defining and Searching Vector Embeddings with postgresml-django in Python\nDESCRIPTION: This snippet demonstrates how to use postgresml-django to create a Django model with automatic vector embedding generation and perform vector similarity searches. It shows the definition of a Document model with a text field and its corresponding embedding, as well as how to execute a vector search.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-postgresml-django.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom django.db import models\nfrom postgresml_django import VectorField, Embed\n\nclass Document(Embed):\n    text = models.TextField()\n    text_embedding = VectorField(\n        field_to_embed=\"text\",\n        dimensions=384,\n        transformer=\"intfloat/e5-small-v2\"\n    )\n\n# Searching\nresults = Document.vector_search(\"text_embedding\", \"query to search against\")\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Similarity Search with PostgresML\nDESCRIPTION: A SQL query that performs vector similarity search using embeddings. It uses a CTE to embed the query text, then joins with an embeddings table to find the most similar chunks using cosine distance. The query leverages pgvector's HNSW index for fast retrieval.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/unified-rag.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH embedded_query AS (\n    SELECT\n        pgml.embed('mixedbread-ai/mxbai-embed-large-v1', 'How do I write a select statement with pgml.transform?', '{\"prompt\": \"Represent this sentence for searching relevant passages: \"}')::vector embedding\n)\nSELECT\n    chunks.id,\n    (\n        SELECT\n            embedding\n        FROM embedded_query) <=> embeddings.embedding cosine_distance,\n    chunks.chunk\nFROM\n    chunks\n    INNER JOIN embeddings ON embeddings.chunk_id = chunks.id\nORDER BY\n    embeddings.embedding <=> (\n        SELECT\n            embedding\n        FROM embedded_query)\nLIMIT 6;\n```\n\n----------------------------------------\n\nTITLE: Searching Movies by Similarity with PostgresML in PostgreSQL\nDESCRIPTION: This query performs a similarity search on movies based on a text description. It uses PostgresML to generate an embedding for the search query and then finds the most similar movie embeddings using cosine similarity.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'Best 1980''s scifi movie'\n  )::vector(1024) AS embedding\n)\nSELECT\n  title,\n  1 - (\n    review_embedding_e5_large <=> (SELECT embedding FROM request)\n  ) AS cosine_similarity\nFROM movies\nORDER BY review_embedding_e5_large <=> (SELECT embedding FROM request)\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Chunks Table in PostgreSQL\nDESCRIPTION: Creates a chunks table and splits documents into smaller chunks using pgml.chunk function with a specified chunk size of 250 characters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/unified-rag.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE chunks(id SERIAL PRIMARY KEY, chunk text NOT NULL, chunk_index int NOT NULL, document_id int references documents(id));\n\nINSERT INTO chunks (chunk, chunk_index, document_id)\nSELECT\n    (chunk).chunk,\n    (chunk).chunk_index,\n    id\nFROM (\n    SELECT\n        pgml.chunk('recursive_character', document, '{\"chunk_size\": 250}') chunk,\n        id\n    FROM\n        documents) sub_query;\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Llama2-7b-chat Model with LoRA in PostgreSQL\nDESCRIPTION: This SQL query demonstrates how to use the pgml.tune function to fine-tune the Llama2-7b-chat model for financial sentiment analysis. It includes configuration for LoRA, training arguments, dataset processing, and model deployment settings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_15\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.tune(\n    'fingpt-llama2-7b-chat',\n    task => 'conversation',\n    relation_name => 'pgml.fingpt_sentiment_train_view',\n    model_name => 'meta-llama/Llama-2-7b-chat-hf',\n    test_size => 0.8,\n    test_sampling => 'last',\n    hyperparams => '{\n        \"training_args\" : {\n            \"learning_rate\": 2e-5,\n            \"per_device_train_batch_size\": 4,\n            \"per_device_eval_batch_size\": 4,\n            \"num_train_epochs\": 1,\n            \"weight_decay\": 0.01,\n            \"hub_token\" : \"HF_TOKEN\",\n            \"push_to_hub\" : true,\n            \"optim\" : \"adamw_bnb_8bit\",\n            \"gradient_accumulation_steps\" : 4,\n            \"gradient_checkpointing\" : true\n        },\n        \"dataset_args\" : { \"system_column\" : \"instruction\", \"user_column\" : \"input\", \"assistant_column\" : \"output\" },\n        \"lora_config\" : {\"r\": 2, \"lora_alpha\" : 4, \"lora_dropout\" : 0.05, \"bias\": \"none\", \"task_type\": \"CAUSAL_LM\"},\n        \"load_in_8bit\" : false,\n        \"token\" : \"HF_TOKEN\"\n    }'\n);\n```\n\n----------------------------------------\n\nTITLE: Querying Vector Embeddings Using Different Texts with Simple Embedding Model\nDESCRIPTION: This example shows generating embeddings for three different phrases using a hypothetical 'simple-embedding-model' that outputs 2-dimensional vectors. The results demonstrate how semantically similar texts produce similar vector representations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed('simple-embedding-model', 'I like Postgres') AS embedding;\n\nSELECT pgml.embed('simple-embedding-model', 'I like SQL') AS embedding;\n\nSELECT pgml.embed('simple-embedding-model', 'Rust is the best') AS embedding;\n```\n\n----------------------------------------\n\nTITLE: Generating Text with Llama 3.2 in PostgresML\nDESCRIPTION: Example of using the pgml.transform function to perform text generation with Llama 3.2 3B Instruct model. The function accepts a JSON configuration specifying the task and model, along with an array of input text prompts.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/meta-llama-3.2-now-available-in-postgresml-serverless.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n  task   => '{\n    \"task\": \"text-generation\",\n    \"model\": \"meta-llama/Llama-3.2-3B-Instruct\"\n  }'::JSONB,\n  inputs  => Array['AI is going to'] \n);\n```\n\n----------------------------------------\n\nTITLE: Personalized Movie Search Query with Customer Embeddings\nDESCRIPTION: PostgreSQL query that combines vector similarity search with customer preferences to generate personalized movie recommendations. It uses embeddings from both the search query and customer profile, along with movie ratings to calculate a final relevance score.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/personalize-embedding-results-with-application-data-in-your-database.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\n-- create a request embedding on the fly\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'query: Best 1980\\'s scifi movie'\n  )::vector(1024) AS embedding\n),\n\n-- retrieve the customers embedding by id\ncustomer AS (\n  SELECT movie_embedding_e5_large AS embedding\n  FROM customers\n  WHERE id = '44366773'\n),\n\n-- vector similarity search for movies and calculate a customer_cosine_similarity at the same time\nfirst_pass AS (\n  SELECT\n    title,\n    total_reviews,\n    star_rating_avg,\n    1 - (\n      review_embedding_e5_large <=> (SELECT embedding FROM request)\n    ) AS request_cosine_similarity,\n    (1 - (\n      review_embedding_e5_large <=> (SELECT embedding FROM customer)\n    ) - 0.9) * 10 AS  customer_cosine_similarity,\n    star_rating_avg / 5 AS star_rating_score\n  FROM movies\n  WHERE total_reviews > 10\n  ORDER BY review_embedding_e5_large <=> (SELECT embedding FROM request)\n  LIMIT 1000\n)\n\n-- grab the top 10 results, re-ranked using a combination of request similarity and customer similarity\nSELECT\n  title,\n  total_reviews,\n  round(star_rating_avg, 2) as star_rating_avg,\n  star_rating_score,\n  request_cosine_similarity,\n  customer_cosine_similarity,\n  request_cosine_similarity + customer_cosine_similarity + star_rating_score AS final_score\nFROM first_pass\nORDER BY final_score DESC\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Named Entity Recognition Query in PostgresML\nDESCRIPTION: Example of using PostgresML transform function for Named Entity Recognition (NER) task. The query processes text to identify and label named entities like persons and locations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/token-classification.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    inputs => ARRAY[\n        'I am Omar and I live in New York City.'\n    ],\n    task => 'token-classification'\n) as ner;\n```\n\nLANGUAGE: json\nCODE:\n```\n[[\n    {\"end\": 9,  \"word\": \"Omar\", \"index\": 3,  \"score\": 0.997110, \"start\": 5,  \"entity\": \"I-PER\"}, \n    {\"end\": 27, \"word\": \"New\",  \"index\": 8,  \"score\": 0.999372, \"start\": 24, \"entity\": \"I-LOC\"}, \n    {\"end\": 32, \"word\": \"York\", \"index\": 9,  \"score\": 0.999355, \"start\": 28, \"entity\": \"I-LOC\"}, \n    {\"end\": 37, \"word\": \"City\", \"index\": 10, \"score\": 0.999431, \"start\": 33, \"entity\": \"I-LOC\"}\n]]\n```\n\n----------------------------------------\n\nTITLE: Implementing Hybrid Search in JavaScript\nDESCRIPTION: JavaScript implementation of hybrid search using PostresML's RAG method. Combines vector search with full-text filtering to find relevant passages about 'Korvus', utilizing the Meta-Llama-3 model for processing.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.rag(\n  {\n    LLM_CONTEXT: {\n      vector_search: {\n        query: {\n          fields: {\n            text: {\n              query: \"Is Korvus fast?\",\n              parameters: {\n                prompt: \"Represent this sentence for searching relevant passages: \"\n              },\n              full_text_filter: \"Korvus\"\n            }\n          },\n        },\n        document: { \"keys\": [\"id\"] },\n        limit: 5,\n      },\n      aggregate: { \"join\": \"\\n\" },\n    },\n    chat: {\n      model: \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n      messages: [\n        {\n          role: \"system\",\n          content: \"You are a friendly and helpful chatbot\",\n        },\n        {\n          role: \"user\",\n          content: \"Given the context\\n:{LLM_CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n        },\n      ],\n      max_tokens: 100,\n    },\n  },\n  pipeline,\n)\n```\n\n----------------------------------------\n\nTITLE: Generate Single Text Embedding with E5-small-v2\nDESCRIPTION: Demonstrates how to generate an embedding vector for a single text input using the intfloat/e5-small-v2 model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed('intfloat/e5-small-v2', 'This is some text to embed');\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Trained Model in PostgreSQL\nDESCRIPTION: This snippet shows how to use the pgml.predict function to make predictions using the trained 'Search Ranking' model. It applies the model to the training data for a sanity check, predicting click probability based on title_rank and body_rank.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_11\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT \n  clicked, \n  pgml.predict('Search Ranking', array[title_rank, body_rank]) \nFROM search_result_clicks;\n```\n\n----------------------------------------\n\nTITLE: Basic Pipeline with Split, Embed, and Full-Text Search in Python\nDESCRIPTION: Example of a basic Pipeline that processes the 'text' field of documents by splitting content, generating embeddings for semantic search, and creating tsvectors for full-text search.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/constructing-pipelines.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"v0\",\n    {\n        \"text\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n            },\n            \"full_text_search\": {\n              \"configuration\": \"english\"\n            }\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Executing RAG Query with Multiple Context Variables in JavaScript\nDESCRIPTION: This JavaScript code demonstrates how to perform a RAG operation with both vector search (LLM_CONTEXT) and custom SQL query (CUSTOM_CONTEXT). It uses vector search with reranking to find relevant documents about 'Korvus', while also executing a custom SQL query that returns a string. Both context elements are used to answer a question about Korvus's speed.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.rag(\n  {\n    LLM_CONTEXT: {\n      vector_search: {\n        query: {\n          fields: {\n            text: {\n              query: \"Is Korvus fast?\",\n              parameters: {\n                prompt: \"Represent this sentence for searching relevant passages: \"\n              },\n              full_text_filter: \"Korvus\"\n            }\n          },\n        },\n        document: { \"keys\": [\"id\"] },\n        rerank: {\n            model: \"mixedbread-ai/mxbai-rerank-base-v1\",\n            query: \"Is Korvus fast?\",\n            num_documents_to_rerank: 100\n        },\n        limit: 5,\n      },\n      aggregate: { \"join\": \"\\n\" },\n    },\n    CUSTOM_CONTEXT: {sql: \"SELECT 'Korvus is super fast!!!'\"},\n    chat: {\n      model: \"meta-llama/Meta-Llama-3-8B-Instruct\",\n      messages: [\n        {\n          role: \"system\",\n          content: \"You are a friendly and helpful chatbot\",\n        },\n        {\n          role: \"user\",\n          content: \"Given the context\\n:{LLM_CONTEXT}\\n{CUSTOM_CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n        },\n      ],\n      max_tokens: 100,\n    },\n  },\n  pipeline,\n)\n```\n\n----------------------------------------\n\nTITLE: Ranking Search Results with ts_rank in PostgreSQL\nDESCRIPTION: This query demonstrates how to use ts_rank to rank search results based on relevance. It orders documents by their ts_rank score for the search terms 'second' or 'title' in the title and body text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT ts_rank(title_and_body_text, to_tsquery('english', 'second | title')), *   \nFROM documents \nORDER BY ts_rank DESC;\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with OpenSourceAI in Python\nDESCRIPTION: This snippet demonstrates how to use the OpenSourceAI class from pgml SDK to perform inference with a fine-tuned model. It includes setting up the client, configuring model parameters, and sending a chat completion request for sentiment analysis.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport pgml\n\ndatabase_url = \"DATABASE_URL\"\n\nclient = pgml.OpenSourceAI(database_url)\n\nresults = client.chat_completions_create(\n    {\n        \"model\" : \"santiadavani/fingpt-llama2-7b-chat\",\n        \"token\" : \"TOKEN\",\n        \"load_in_8bit\": \"true\",\n        \"temperature\" : 0.1,\n        \"repetition_penalty\" : 1.5,\n    },\n    [\n        {\n            \"role\" : \"system\",\n            \"content\" : \"What is the sentiment of this news? Please choose an answer from {strong negative/moderately negative/mildly negative/neutral/mildly positive/moderately positive/strong positive}.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Starbucks says the workers violated safety policies while workers said they'd never heard of the policy before and are alleging retaliation.\",\n        },\n    ]\n)\n\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Multi-Field Pipeline for Processing Different Document Sections\nDESCRIPTION: A more complex Pipeline that processes multiple document fields differently - generating embeddings and tsvectors for one field while splitting, embedding, and generating tsvectors for another.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/constructing-pipelines.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"v0\",\n    {\n        \"abstract\": {\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n            },\n            \"full_text_search\": {\n              \"configuration\": \"english\"\n            }\n        },\n        \"text\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n            },\n            \"full_text_search\": {\n              \"configuration\": \"english\"\n            }\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Personalized Search with Vector Embeddings in PostgreSQL\nDESCRIPTION: SQL query that performs personalized search by combining query-to-movie similarity with customer-to-movie similarity. The query creates embeddings on the fly, retrieves a customer's embedding by ID, performs vector similarity search, and then re-ranks results based on combined scores.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/personalization.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\n-- create a request embedding on the fly\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'query: Best 1980\\'s scifi movie'\n  )::vector(1024) AS embedding\n),\n\n-- retrieve the customers embedding by id\ncustomer AS (\n  SELECT movie_embedding_e5_large AS embedding\n  FROM customers\n  WHERE id = '44366773'\n),\n\n-- vector similarity search for movies and calculate a customer_cosine_similarity at the same time\nfirst_pass AS (\n  SELECT\n    title,\n    total_reviews,\n    star_rating_avg,\n    1 - (\n      review_embedding_e5_large <=> (SELECT embedding FROM request)\n    ) AS request_cosine_similarity,\n    (1 - (\n      review_embedding_e5_large <=> (SELECT embedding FROM customer)\n    ) - 0.9) * 10 AS  customer_cosine_similarity,\n    star_rating_avg / 5 AS star_rating_score\n  FROM movies\n  WHERE total_reviews > 10\n  ORDER BY review_embedding_e5_large <=> (SELECT embedding FROM request)\n  LIMIT 1000\n)\n\n-- grab the top 10 results, re-ranked using a combination of request similarity and customer similarity\nSELECT\n  title,\n  total_reviews,\n  round(star_rating_avg, 2) as star_rating_avg,\n  star_rating_score,\n  request_cosine_similarity,\n  customer_cosine_similarity,\n  request_cosine_similarity + customer_cosine_similarity + star_rating_score AS final_score\nFROM first_pass\nORDER BY final_score DESC\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Executing RAG Query with Multiple Context Variables in Rust\nDESCRIPTION: This Rust code demonstrates how to perform a RAG operation with both vector search (LLM_CONTEXT) and custom SQL query (CUSTOM_CONTEXT). It uses vector search with reranking to find relevant documents about 'Korvus', while also executing a custom SQL query that returns a string. Both context elements are used to answer a question about Korvus's speed.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_12\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection.rag(serde_json::json!(\n    {\n        \"LLM_CONTEXT\": {\n            \"vector_search\": {\n                \"query\": {\n                    \"fields\": {\n                        \"text\": {\n                            \"query\": \"Is Korvus fast?\",\n                            \"parameters\": {\n                                \"prompt\": \"Represent this sentence for searching relevant passages: \"\n                            },\n                            \"full_text_filter\": \"Korvus\"\n                        }\n                    },\n                },\n                \"document\": {\"keys\": [\"id\"]},\n                \"rerank\": {\n                    \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\n                    \"query\": \"Is Korvus fast?\",\n                    \"num_documents_to_rerank\": 100,\n                },\n                \"limit\": 1,\n            },\n            \"aggregate\": {\"join\": \"\\n\"},\n        },\n        \"CUSTOM_CONTEXT\": {\"sql\": \"SELECT 'Korvus is super fast!!!'\"},\n        \"chat\": {\n            \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a friendly and helpful chatbot\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Given the context\\n:{LLM_CONTEXT}\\n{CUSTOM_CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n                },\n            ],\n            \"max_tokens\": 100,\n        },\n    }\n).into(), &mut pipeline).await?\n```\n\n----------------------------------------\n\nTITLE: Basic PostgreSQL Document Ranking Example\nDESCRIPTION: Simple example of using pgml.rank() function to rank two documents against a test query using the mixedbread-ai model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.rank.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.rank('mixedbread-ai/mxbai-rerank-base-v1', 'test', ARRAY['doc1', 'doc2']);\n```\n\n----------------------------------------\n\nTITLE: Implementing an Interactive Query Loop for RAG System in Python\nDESCRIPTION: Extends the main function to include an interactive loop that allows users to input queries and receive responses from the RAG system. The loop continues until the user enters 'q' to quit, demonstrating the practical application of the complete system.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-firecrawl-rag-in-a-single-query.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nasync def main():\n    # ... (previous code for setup and indexing)\n\n    # Now we can search\n    while True:\n        user_query = input(\"\\n\\nquery > \")\n        if user_query == \"q\":\n            break\n        results = await do_rag(user_query)\n        print(results)\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Document Chunking and Retrieval with Embeddings in Python\nDESCRIPTION: This code snippet demonstrates how to split a document into chunks, embed each chunk, store the embeddings, and then retrieve the most relevant chunk based on a user query using cosine similarity.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/chatbots/README.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndocument = get_text_from_url(\"\"https://en.wikipedia.org/wiki/Baldur's_Gate_3\") # Strips HTML and gets just the text from the url\nfor chunk_text in split_document(context): # Splits the document into smaller chunks of text\n  embedding = embed(chunk_text) # Returns some embedding like [0.11, 0.22, -0.97, ...]\n  store(chunk_text, embedding) # We want to store the text and embedding of the chunk\ninput = \"What is Baldur's Gate 3?\"\ninput_embedding = embed(input) # Returns some embedding like [0.68, -0.94, 0.32, ...]\ncontext = retrieve_from_store(input_embedding) # Returns the text of the chunk with the closest embedding ranked by cosine similarity\nprint(context)\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings on Document Update in PostgreSQL\nDESCRIPTION: This code snippet demonstrates how to generate and store embeddings whenever a document is updated. It uses a trigger function to automatically update the embedding when the document's content changes.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE OR REPLACE FUNCTION update_embedding()\nRETURNS TRIGGER AS $$\nBEGIN\n    NEW.embedding = pgml.embed('intfloat/e5-small-v2', NEW.body);\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER update_document_embedding\nBEFORE INSERT OR UPDATE OF body ON documents\nFOR EACH ROW EXECUTE FUNCTION update_embedding();\n```\n\n----------------------------------------\n\nTITLE: Natural Language Inference with RoBERTa in PostgresML\nDESCRIPTION: This example shows how to perform Natural Language Inference (NLI) using a RoBERTa model in PostgresML. It demonstrates how to input a premise and hypothesis and interpret the model's output.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-classification.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-classification\", \n      \"model\": \"roberta-large-mnli\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'A soccer game with multiple males playing. Some men are playing a sport.'\n    ]\n) AS nli;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"label\": \"ENTAILMENT\", \"score\": 0.98837411403656}\n]\n```\n\n----------------------------------------\n\nTITLE: Deploying Models with pgml.deploy() in PostgreSQL\nDESCRIPTION: The pgml.deploy() function is used to deploy trained models in PostgresML. It accepts parameters for project name, deployment strategy, and optional algorithm filtering. This function allows for automatic or manual deployment of models based on performance metrics or specific criteria.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.deploy.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.deploy(\n    project_name TEXT,\n    strategy TEXT DEFAULT 'best_score',\n    algorithm TEXT DEFAULT NULL\n)\n```\n\n----------------------------------------\n\nTITLE: Creating HNSW Index for Vector Data in PostgreSQL\nDESCRIPTION: This SQL snippet shows how to create an HNSW (Hierarchical Navigable Small World) index on a vector column using pgvector. The index is created on the 'embedding' column of the 'items' table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/indexing-w-pgvector.md#2025-04-19_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops);\n```\n\n----------------------------------------\n\nTITLE: Vector Search with Re-ranking in C\nDESCRIPTION: This C code snippet shows how to perform a vector search with re-ranking using the `korvus_collectionc_vector_search` function.  It passes a JSON string as the query, which includes the query parameters, re-ranking model (`mixedbread-ai/mxbai-rerank-base-v1`), query for re-ranking, and the number of documents to re-rank. The `pipeline` and `r_size` parameters are also passed. The function returns a `char**` array of results.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nr_size = 0;\nchar **results = korvus_collectionc_vector_search(collection, \"{\\  \\\"query\\\": {\\    \\\"fields\\\": {\\      \\\"body\\\": {\\        \\\"query\\\": \\\"What is the best database?\\\",\\        \\\"parameters\\\": {\\          \\\"prompt\\\": \\\"Represent this sentence for searching relevant passages: \\\"\\        }\\      }\\    }\\  },\\  \\\"rerank\\\": {\\    \\\"model\\\": \\\"mixedbread-ai/mxbai-rerank-base-v1\\\",\\    \\\"query\\\": \\\"What is the best database\\\",\\    \\\"num_documents_to_rerank\\\": 100\\  },\\  \\\"limit\\\": 5\\}\",\npipeline, &r_size);\n```\n\n----------------------------------------\n\nTITLE: Configuring Korvus Pipeline and Collection for Document Processing in Python\nDESCRIPTION: Defines a Pipeline for processing markdown documents and a Collection to store them. The Pipeline specifies a markdown splitter and a semantic search model for generating embeddings. This setup forms the basis for document indexing and retrieval.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-firecrawl-rag-in-a-single-query.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"v0\",\n    {\n        \"markdown\": {\n            \"splitter\": {\"model\": \"markdown\"},\n            \"semantic_search\": {\n                \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n            },\n        },\n    },\n)\ncollection = Collection(\"fire-crawl-demo-v0\")\n\n# Add our Pipeline to our Collection\nasync def add_pipeline():\n    await collection.add_pipeline(pipeline)\n```\n\n----------------------------------------\n\nTITLE: Ranked Search Results Query in PostgreSQL\nDESCRIPTION: Shows how to rank search results using ts_rank function with multiple search terms combined with OR operator.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT ts_rank(title_and_body_text, to_tsquery('english', 'second | title')), *   \nFROM documents \nORDER BY ts_rank DESC;\n```\n\n----------------------------------------\n\nTITLE: Creating Collection and Pipeline in JavaScript SDK\nDESCRIPTION: Demonstrates how to create a collection and pipeline using the PostgresML JavaScript SDK. The pipeline includes text splitting and semantic search using the GTE embedding model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/the-1.0-sdk-is-here.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Create Collection and Pipeline\nconst collection = pgml.newCollection(\"my_collection\");\nconst pipeline = pgml.newPipeline(\"my_pipeline\", {\n  text: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"Alibaba-NLP/gte-base-en-v1.5\",\n    },\n  },\n});\nawait collection.add_pipeline(pipeline);\n\n// Upsert a document\nconst documents = [\n  { id: \"document_one\", text: \"Here is some hidden value 1000\" }\n];\nawait collection.upsert_documents(documents);\n\n// Search over our collection\nconst results = await collection.vector_search(\n  {\n    query: {\n      fields: {\n        text: {\n          query: \"What is the hidden value?\"\n        },\n      },\n    },\n    limit: 5,\n  },\n  pipeline,\n);\nconsole.log(results);\n```\n\n----------------------------------------\n\nTITLE: Using Korvus Python SDK for Embedded Vector Search\nDESCRIPTION: This snippet demonstrates how to use the Korvus Python SDK to create collections and pipelines, insert documents, and perform vector search operations without writing SQL directly. It abstracts the underlying PostgreSQL and PostgresML operations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/a-speed-comparison-of-the-most-popular-retrieval-systems-for-rag.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom korvus import Collection, Pipeline\nimport asyncio\n\ncollection = Collection(\"semantic-search-demo\")\npipeline = Pipeline(\n    \"v1\",\n    {\n        \"text\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n            },\n        },\n    },\n)\n\n\nasync def main():\n    await collection.add_pipeline(pipeline)\n\n    documents = [\n        {\n            \"id\": \"1\",\n            \"text\": \"The hidden value is 1000\",\n        },\n        {\n            \"id\": \"2\",\n            \"text\": \"Korvus is incredibly fast and easy to use.\",\n        },\n    ]\n    await collection.upsert_documents(documents)\n\n    results = await collection.vector_search(\n        {\n            \"query\": {\n                \"fields\": {\n                    \"text\": {\n                        \"query\": \"What is the hidden value\",\n                        \"parameters\": {\n                            \"prompt\": \"Represent this sentence for searching relevant passages: \",\n                        },\n                    },\n                },\n            },\n            \"document\": {\"keys\": [\"id\"]},\n            \"limit\": 1,\n        },\n        pipeline,\n    )\n    print(results)\n\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Generating Clustering Embeddings with Instructor-XL Model in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to generate an embedding for a medicine sentence using the hkunlp/instructor-xl model with an instruction for clustering.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_12\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed(\n    transformer => 'hkunlp/instructor-xl',\n    text => 'Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity\"}',\n    kwargs => '{\"instruction\": \"Represent the Medicine sentence for clustering:\"}'\n);\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search with Cosine Distance in PostgreSQL\nDESCRIPTION: This query demonstrates how to perform a semantic search using cosine distance. It embeds the search query and compares it against stored embeddings to find the most relevant text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH query_embedding AS (\n    SELECT\n        pgml.embed(\n          'mixedbread-ai/mxbai-embed-large-v1',\n          'What is the pgml.transform function?',\n          '{\"prompt\": \"Represent this sentence for searching relevant passages: \"}'\n        )::vector embedding\n)\nSELECT\n    text,\n    (\n        SELECT\n          embedding\n        FROM query_embedding\n    ) <=> text_and_embeddings.embedding cosine_distance\nFROM\n  text_and_embeddings\nORDER BY cosine_distance\nLIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with pgml.embed Function\nDESCRIPTION: SQL function to create embeddings from text using in-database transformer models, which is a key step in the RAG pipeline.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/README.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.embed(\n    transformer TEXT,\n    \"text\" TEXT,\n    kwargs JSONB\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing PostgresML Collection and Running a Sample Pipeline in JavaScript\nDESCRIPTION: This sample code demonstrates initializing a collection, creating pipelines, and managing documents with automatic chunking and embedding in a vector search application. Pre-requisites: PostgresML database connection, JavaScript SDK installed via npm. Key functions include creating and adding pipelines, upserting documents, querying with vector-based search, and archiving collections.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/javascript/README.md#2025-04-19_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst pgml = require(\"pgml\");\n\nconst main = async () => {\n    const collection = pgml.newCollection(\"my_javascript_collection\");\n    const model = pgml.newModel();\n    const splitter = pgml.newSplitter();\n    const pipeline = pgml.newPipeline(\"my_javascript_pipeline\", model, splitter);\n    await collection.add_pipeline(pipeline);\n\n    const documents = [\n        {\n          id: \"Document One\",\n          text: \"document one contents...\",\n        },\n        {\n          id: \"Document Two\",\n          text: \"document two contents...\",\n        },\n    ];\n    await collection.upsert_documents(documents);\n\n    const queryResults = await collection\n        .query()\n        .vector_recall(\"Some user query that will match document one first\", pipeline)\n        .limit(2)\n        .fetch_all();\n\n    const results = queryResults.map((result) => {\n      const [similarity, text, metadata] = result;\n      return {\n        similarity,\n        text,\n        metadata,\n      };\n    });\n    console.log(results);\n\n    await collection.archive();\n};\n\nmain().then(() => {\n  console.log(\"Done with PostgresML demo\");\n});\n```\n\n----------------------------------------\n\nTITLE: Vector Search with $eq Filter\nDESCRIPTION: Performs a vector search with an equality filter on user_id. Returns only documents where user_id equals 1, limited to 5 results.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.vector_search(\n  {\n    query: {\n      fields: {\n        body: {\n          query: \"What is the best database?\", \n          parameters: {\n            instruction:\n              \"Represent this sentence for searching relevant passages: \",\n          }\n        },\n      },\n      filter: {\n        user_id: {\n          $eq: 1\n        }\n      }\n    },\n    limit: 5,\n  },\n  pipeline,\n);\n```\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.vector_search(\n    {\n        \"query\": {\n            \"fields\": {\n                \"body\": {\n                    \"query\": \"What is the best database?\",\n                    \"parameters\": {\n                        \"instruction\": \"Represent this sentence for searching relevant passages: \",\n                    },\n                },\n            },\n            \"filter\": {\"user_id\": {\"$eq\": 1}},\n        },\n        \"limit\": 5,\n    },\n    pipeline,\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection\n    .vector_search(\n        serde_json::json!({\n            \"query\": {\n                \"fields\": {\n                    \"body\": {\n                        \"query\": \"What is the best database?\",\n                        \"parameters\": {\n                            \"instruction\": \"Represent this sentence for searching relevant passages: \",\n                        },\n                    },\n                },\n                \"filter\": {\"user_id\": {\"$eq\": 1}},\n            },\n            \"limit\": 5,\n        })\n        .into(),\n        &mut pipeline,\n    )\n    .await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nr_size = 0;\nchar **results = korvus_collectionc_vector_search(collection, \"{\\\n    \\\"query\\\": {\\\n        \\\"fields\\\": {\\\n            \\\"body\\\": {\\\n                \\\"query\\\": \\\"What is the best database?\\\",\\\n                \\\"parameters\\\": {\\\n                    \\\"instruction\\\": \\\"Represent this sentence for searching relevant passages: \\\"\\\n                }\\\n            }\\\n        },\\\n        \\\"filter\\\": {\\\"user_id\\\": {\\\"$eq\\\": 1}}\\\n    },\\\n    \\\"limit\\\": 5\\\n}\", pipeline, &r_size);\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Vicuna LLM in PostgresML\nDESCRIPTION: This snippet demonstrates how to use PostgresML's transform function to generate text using a Vicuna LLM. It includes setting up the task with model details, specifying input text, and defining generation parameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_11\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/vicuna-7B-v1.3-GGML\",\n      \"model_type\": \"llama\",\n      \"model_file\": \"vicuna-7b-v1.3.ggmlv3.q5_K_M.bin\",\n      \"gpu_layers\": 256\n    }'::JSONB,\n    inputs => ARRAY[\n        $$A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n\nUSER: Please write an intro to a story about a woman living in New York.\nASSISTANT:$$\n    ],\n    args => '{\n      \"max_new_tokens\": 512,\n          \"threads\": 16,\n      \"stop\": [\"USER:\",\"USER\"]\n    }'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG with Korvus and OpenAI in JavaScript\nDESCRIPTION: This code snippet demonstrates how to use Korvus for vector search and OpenAI for text generation in JavaScript. It initializes a Korvus Collection and Pipeline, upserts documents, performs vector search, and then uses the search results to generate a prompt for OpenAI's text generation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/example-apps/rag-with-openai.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst korvus = require(\"korvus\");\nconst openai = require(\"openai\");\n\n// Initialize our Collection\nconst collection = korvus.newCollection(\"openai-text-generation-demo\");\n\n// Initialize our Pipeline\n// Our Pipeline will split and embed the `text` key of documents we upsert\nconst pipeline = korvus.newPipeline(\"v1\", {\n  text: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"mixedbread-ai/mxbai-embed-large-v1\",\n    }\n  },\n});\n\n\n// Initialize our client connection to OpenAI\nconst client = new openai.OpenAI({\n  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted\n});\n\n\nconst main = async () => {\n  // Add our Pipeline to our Collection\n  await collection.add_pipeline(pipeline);\n\n  // Upsert our documents\n  // The `text` key of our documents will be split and embedded per our Pipeline specification above\n  let documents = [\n    {\n      id: \"1\",\n      text: \"Korvus is incredibly fast and easy to use.\",\n    },\n    {\n      id: \"2\",\n      text: \"Tomatoes are incredible on burgers.\",\n    },\n  ]\n  await collection.upsert_documents(documents)\n\n  // Perform vector_search\n  // We are querying for the string \"Is Korvus fast?\"\n  // Notice that the `mixedbread-ai/mxbai-embed-large-v1` embedding model takes a prompt paramter when embedding for search\n  // We specify that we only want to return the `id` of documents. If the `document` key was blank it would return the entire document with every result\n  // Limit the results to 5. In our case we only have two documents in our Collection so we will only get two results\n  const query = \"Is Korvus fast?\"\n  const results = await collection.vector_search(\n    {\n      query: {\n        fields: {\n          text: {\n            query: query,\n            parameters: {\n              prompt:\n                \"Represent this sentence for searching relevant passages: \",\n            }\n          },\n        },\n      },\n      document: {\n        keys: [\n          \"id\"\n        ]\n      },\n      limit: 5,\n    },\n    pipeline);\n  console.log(\"Our search results: \")\n  console.log(results)\n\n  // After retrieving the context, we build our prompt for gpt-4o and make our completion request\n  const context = results[0].chunk\n  console.log(\"Model output: \")\n  const chatCompletion = await client.chat.completions.create({\n    messages: [{ role: 'user', content: `Answer the question:\\n\\n${query}\\n\\nGiven the context:\\n\\n${context}` }],\n    model: 'gpt-4o',\n  });\n  console.dir(chatCompletion, {depth: 10});\n}\n\nmain().then(() => console.log(\"DONE!\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG with Korvus and OpenAI in Python\nDESCRIPTION: This code snippet demonstrates how to use Korvus for vector search and OpenAI for text generation in Python. It initializes a Korvus Collection and Pipeline, upserts documents, performs vector search, and then uses the search results to generate a prompt for OpenAI's text generation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/example-apps/rag-with-openai.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom korvus import Collection, Pipeline\nfrom rich import print\nfrom openai import OpenAI\nimport os\nimport asyncio\n\n# Initialize our Collection\ncollection = Collection(\"openai-text-generation-demo\")\n\n# Initialize our Pipeline\n# Our Pipeline will split and embed the `text` key of documents we upsert\npipeline = Pipeline(\n    \"v1\",\n    {\n        \"text\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n            },\n        },\n    },\n)\n\n# Initialize our client connection to OpenAI\nclient = OpenAI(\n    # This is the default and can be omitted\n    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n)\n\n\nasync def main():\n    # Add our Pipeline to our Collection\n    await collection.add_pipeline(pipeline)\n\n    # Upsert our documents\n    # The `text` key of our documents will be split and embedded per our Pipeline specification above\n    documents = [\n        {\n            \"id\": \"1\",\n            \"text\": \"Korvus is incredibly fast and easy to use.\",\n        },\n        {\n            \"id\": \"2\",\n            \"text\": \"Tomatoes are incredible on burgers.\",\n        },\n    ]\n    await collection.upsert_documents(documents)\n\n    # Perform vector_search\n    # We are querying for the string \"Is Korvus fast?\"\n    # Notice that the `mixedbread-ai/mxbai-embed-large-v1` embedding model takes a prompt paramter when embedding for search\n    # We specify that we only want to return the `id` of documents. If the `document` key was blank it would return the entire document with every result\n    # Limit the results to 1. In our case we only want to feed the top result to OpenAI as we know the other result is not going to be relevant to our question\n    query = \"Is Korvus Fast?\"\n    results = await collection.vector_search(\n        {\n            \"query\": {\n                \"fields\": {\n                    \"text\": {\n                        \"query\": query,\n                        \"parameters\": {\n                            \"prompt\": \"Represent this sentence for searching relevant passages: \",\n                        },\n                    },\n                },\n            },\n            \"document\": {\"keys\": [\"id\"]},\n            \"limit\": 1,\n        },\n        pipeline,\n    )\n    print(\"Our search results: \")\n    print(results)\n\n    # After retrieving the context, we build our prompt for gpt-4o and make our completion request\n    context = results[0][\"chunk\"]\n    print(\"Model output: \")\n    chat_completion = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Answer the question:\\n\\n{query}\\n\\nGiven the context:\\n\\n{context}\",\n            }\n        ],\n        model=\"gpt-4o\",\n    )\n    print(chat_completion)\n\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Question Natural Language Inference with ELECTRA in PostgresML\nDESCRIPTION: This snippet demonstrates how to use a QNLI (Question Natural Language Inference) model in PostgresML. It shows how to input a question-answer pair and interpret the model's output.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-classification.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-classification\", \n      \"model\": \"cross-encoder/qnli-electra-base\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Where is the capital of France? Paris is the capital of France.'\n    ]\n) AS qnli;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"label\": \"LABEL_0\", \"score\": 0.9978110194206238}\n]\n```\n\n----------------------------------------\n\nTITLE: Synchronous Chat Completion Example\nDESCRIPTION: Example of using synchronous chat completion with the Meta-Llama model, showing system and user message formatting\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/opensourceai.md#2025-04-19_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst korvus = require(\"korvus\");\nconst client = korvus.newOpenSourceAI();\nconst results = client.chat_completions_create(\n  \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n  [\n    {\n      role: \"system\",\n      content: \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\n      role: \"user\",\n      content: \"How many helicopters can a human eat in one sitting?\",\n    },\n  ],\n);\nconsole.log(results);\n```\n\nLANGUAGE: python\nCODE:\n```\nimport korvus\nclient = korvus.OpenSourceAI()\nresults = client.chat_completions_create(\n    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"How many helicopters can a human eat in one sitting?\",\n        },\n    ],\n    temperature=0.85,\n)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Fine-Tuning DistilBERT for Sentiment Analysis in PostgreSQL ML\nDESCRIPTION: SQL command to fine-tune a DistilBERT model for a 9-class sentiment analysis task using PostgreSQL ML. Includes hyperparameter configuration and Hugging Face integration with private repository support.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_14\n\nLANGUAGE: postgresql\nCODE:\n```\n-- Fine-tune the model for 9 classes without HUB token\nSELECT pgml.tune(\n    'fingpt_sentiement',\n    task => 'text-classification',\n    relation_name => 'pgml.fingpt_sentiment_train_view',\n    model_name => 'distilbert-base-uncased',\n    test_size => 0.2,\n    test_sampling => 'last',\n    hyperparams => '{\n        \"training_args\": {\n            \"learning_rate\": 2e-5,\n            \"per_device_train_batch_size\": 16,\n            \"per_device_eval_batch_size\": 16,\n            \"num_train_epochs\": 5,\n            \"weight_decay\": 0.01,\n            \"hub_token\" : \"YOUR_HUB_TOKEN\",\n            \"push_to_hub\": true,\n            \"hub_private_repo\": true\n        },\n        \"dataset_args\": { \"text_column\": \"input\", \"class_column\": \"output\" }\n    }'\n);\n```\n\n----------------------------------------\n\nTITLE: Creating HNSW index for vector similarity in PostgreSQL\nDESCRIPTION: This query drops the existing IVFFlat index and creates a new HNSW index on the review_embedding_e5_large column for improved vector similarity search performance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/speeding-up-vector-recall-5x-with-hnsw.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nDROP INDEX index_amazon_us_reviews_on_review_embedding_e5_large;\nCREATE INDEX CONCURRENTLY ON pgml.amazon_us_reviews USING hnsw (review_embedding_e5_large vector_cosine_ops);\n```\n\n----------------------------------------\n\nTITLE: Implementing Hybrid Search in C\nDESCRIPTION: C implementation of hybrid search using PostresML's RAG method. Uses escaped string formatting to construct the query combining vector search with full-text filtering.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nchar * results = korvus_collectionc_rag(collection, \n  \"{\\\n    \\\"LLM_CONTEXT\\\": {\\\n      \\\"vector_search\\\": {\\\n        \\\"query\\\": {\\\n          \\\"fields\\\": {\\\n            \\\"text\\\": {\\\n              \\\"query\\\": \\\"Is Korvus fast?\\\",\\\n              \\\"parameters\\\": {\\\n                \\\"prompt\\\": \\\"Represent this sentence for searching relevant passages: \\\"\\\n              },\\\n              \\\"full_text_filter\\\": \\\"Korvus\\\"\\\n            }\\\n          }\\\n        },\\\n        \\\"document\\\": {\\\"keys\\\": [\\\"id\\\"]},\\\n        \\\"limit\\\": 5\\\n      },\\\n      \\\"aggregate\\\": {\\\"join\\\": \\\"\\\\n\\\"}\\\n    },\\\n    \\\"chat\\\": {\\\n      \\\"model\\\": \\\"meta-llama/Meta-Llama-3-8B-Instruct\\\",\\\n      \\\"messages\\\": [\\\n        {\\\n          \\\"role\\\": \\\"system\\\",\\\n          \\\"content\\\": \\\"You are a friendly and helpful chatbot\\\"\\\n        },\\\n        {\\\n          \\\"role\\\": \\\"user\\\",\\\n          \\\"content\\\": \\\"Given the context:\\\\n{LLM_CONTEXT}\\\\nAnswer the question: Is Korvus fast?\\\"\\\n        }\\\n      ],\\\n      \\\"max_tokens\\\": 100\\\n    }\\\n  }\",\n  pipeline\n);\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Document Ranking Function API Definition\nDESCRIPTION: API definition for the pgml.rank() function showing required parameters including transformer name, query text, documents array, and optional kwargs JSON parameter.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.rank.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.rank(\n    transformer TEXT,  -- transformer name\n    query TEXT,        -- text to rank against\n    documents TEXT[],  -- documents to rank\n    kwargs JSON        -- optional arguments (see below)\n)\n```\n\n----------------------------------------\n\nTITLE: Querying vector similarity with IVFFlat index in PostgreSQL\nDESCRIPTION: This query uses IVFFlat indexing to perform semantic similarity search on a dataset of Amazon movie reviews. It embeds a query string and finds the 5 most similar reviews based on cosine similarity.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/speeding-up-vector-recall-5x-with-hnsw.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'query: Best 1980''s scifi movie'\n  )::vector(1024) AS embedding\n)\n\nSELECT\n  id,\n  1 - (\n    review_embedding_e5_large <=> (\n      SELECT embedding FROM request\n    )\n  ) AS cosine_similarity\nFROM pgml.amazon_us_reviews\nORDER BY review_embedding_e5_large <=> (SELECT embedding FROM request)\nLIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Performing Question Answering with PostgresML transform Function\nDESCRIPTION: This SQL query uses the pgml.transform function to perform question answering. It takes a JSON input containing a question and context, and returns the answer along with its position and confidence score.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/question-answering.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    'question-answering',\n    inputs => ARRAY[\n        '{\n            \"question\": \"Where do I live?\",\n            \"context\": \"My name is Merve and I live in stanbul.\"\n        }'\n    ]\n) AS answer;\n```\n\n----------------------------------------\n\nTITLE: Tuning Vector Index Probes for Improved Recall in PostgreSQL\nDESCRIPTION: This query demonstrates how to increase the number of probes in the vector index search to improve recall. It sets the 'ivfflat.probes' parameter to 300 for exhaustive searching of all lists in the index.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSET ivfflat.probes = 300;\n\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'Best 1980''s scifi movie'\n  )::vector(1024) AS embedding\n)\nSELECT\n  title,\n  1 - (\n    review_embedding_e5_large <=> (SELECT embedding FROM request)\n  ) AS cosine_similarity\nFROM movies\nORDER BY review_embedding_e5_large <=> (SELECT embedding FROM request)\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with pgml.transform_stream in PostgreSQL\nDESCRIPTION: This code snippet demonstrates how to use `pgml.transform_stream` for streaming text generation in PostgreSQL.  The query utilizes a cursor to fetch results incrementally, enabling the retrieval of the first token quickly. This provides a faster initial response in applications requiring streaming output.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/unified-rag.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nBEGIN;\nDECLARE c CURSOR FOR WITH embedded_query AS (\n    SELECT\n        pgml.embed('mixedbread-ai/mxbai-embed-large-v1', 'How do I write a select statement with pgml.transform?', '{\"prompt\": \"Represent this sentence for searching relevant passages: \"}')::vector embedding\n),\nvector_search AS (\n    SELECT\n      chunks.id,\n      (\n          SELECT\n              embedding\n          FROM embedded_query) <=> embeddings.embedding cosine_distance,\n      chunks.chunk\n  FROM\n      chunks\n  INNER JOIN embeddings ON embeddings.chunk_id = chunks.id\n  ORDER BY\n      embeddings.embedding <=> (\n          SELECT\n              embedding\n          FROM embedded_query)\n  LIMIT 6\n),\nrow_number_vector_search AS (\n    SELECT\n        cosine_distance,\n        chunk,\n        ROW_NUMBER() OVER () AS row_number\n    FROM\n        vector_search\n),\ncontext AS (\n  SELECT\n      chunk\n  FROM (\n      SELECT\n        chunk\n      FROM\n          row_number_vector_search AS rnsv1\n      INNER JOIN (\n          SELECT\n            pgml.rank('mixedbread-ai/mxbai-rerank-base-v1', 'How do I write a select statement with pgml.transform?', array_agg(\"chunk\"), '{\"return_documents\": false, \"top_k\": 1}'::jsonb || '{}') AS rank\n          FROM\n            row_number_vector_search\n      ) AS rnsv2 ON (rank).corpus_id + 1 = rnsv1.row_number\n  ) AS sub_query\n)\nSELECT\n    pgml.transform_stream(\n      task => '{\n        \"task\": \"conversational\",\n        \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\"\n      }'::jsonb, \n      inputs => ARRAY['{\"role\": \"system\", \"content\": \"You are a friendly and helpful chatbot.\"}'::jsonb, jsonb_build_object('role', 'user', 'content', replace('Given the context answer the following question: How do I write a select statement with pgml.transform? Context:\\n\\n{CONTEXT}', '{CONTEXT}', chunk))], \n      args => '{\n        \"max_new_tokens\": 100\n      }'::jsonb)\nFROM\n    context;\nFETCH 2 FROM c;\nEND;\n```\n\n----------------------------------------\n\nTITLE: Question Answering with Instructor Model\nDESCRIPTION: This snippet outlines the use of the `hknlp/instructor-base` model for generating text embeddings that are utilized in question answering, serving as an alternative to the default embedding model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/examples/README.md#2025-04-19_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Ranking Text with pgml.rank Function\nDESCRIPTION: SQL function that uses Cross-Encoders to score sentence pairs, typically used as a re-ranking step when performing search in RAG workflows.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/README.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.rank(\n    transformer TEXT,\n    query TEXT,\n    documents TEXT[],\n    kwargs JSONB\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Context-Based Question Answering with Python\nDESCRIPTION: This code snippet outlines a high-level approach for retrieving relevant context from a document based on user input, tokenizing the combined input and context, and using a model to generate an answer.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/chatbots/README.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_relevant_context(user_input: str, document: str) -> str:\n  # Do something magical and return the relevant context\n  \nuser_input = \"What is Baldur's Gate 3?\"\ncontext = get_text_from_url(\"https://en.wikipedia.org/wiki/Baldur's_Gate_3\") # Strips HTML and gets just the text from the url\nrelevant_context = get_relevant_context(user_input, context) # Only gets the most relevant part of the Wikipedia article\ntokenized_input = tokenize(user_input + relevant_context) # Tokenizes the input and context something like [25, 12, ... 30000, 29567, ...]\noutput = model(tokenized_input)\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Word Embedding Similarity in Python\nDESCRIPTION: This code snippet demonstrates how to use embeddings to compare the semantic similarity between words using cosine similarity. It shows that words with closer meanings have higher similarity scores.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/chatbots/README.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nembedding_1 = embed(\"King\") # This returns [0.11, -0.32, 0.46, ...]\nembedding_2 = embed(\"Queen\") # This returns [0.18, -0.24, 0.7, ...]\nembedding_3 = embed(\"Turtle\") # This returns [-0.5, 0.4, -0.3, ...]\n\nsimilarity1 = cosine_similarity(embedding_1, embedding_2)\nsimilarity2 = cosine_similarity(embedding_1, embedding_3)\nprint(\"Similarity between King and Queen\", similarity1)\nprint(\"Similarity betwen King and Turtle\", similarity2)\n```\n\n----------------------------------------\n\nTITLE: Task-based Text Generation with pgml.transform() in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to use the task-based API of pgml.transform() to perform text generation. It specifies the 'text-generation' task and provides an input prompt.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT *\nFROM pgml.transform(\n  task => 'text-generation',\n  inputs => ARRAY['In a galaxy far far away']\n);\n```\n\n----------------------------------------\n\nTITLE: Re-ranking Search Results with RAG Pipeline in C\nDESCRIPTION: This C code demonstrates calling the `korvus_collectionc_rag` function. It passes a JSON string that configures the vector search with re-ranking and a chat model, along with the `collection` and `pipeline` objects. The JSON string defines the query, re-ranking model and the number of documents to re-rank, as well as the chat model and a system/user message pair.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nchar * results = korvus_collectionc_rag(collection,\n  \"{\\n    \\\"LLM_CONTEXT\\\": {\\n      \\\"vector_search\\\": {\\n        \\\"query\\\": {\\n          \\\"fields\\\": {\\n            \\\"text\\\": {\\n              \\\"query\\\": \\\"Is Korvus fast?\\\",\\n              \\\"parameters\\\": {\\n                \\\"prompt\\\": \\\"Represent this sentence for searching relevant passages: \\\"\\n              },\\n              \\\"full_text_filter\\\": \\\"Korvus\\\"\\n            }\\n          }\\n        },\\n        \\\"document\\\": {\\\"keys\\\": [\\\"id\\\"]},\\n            \\\"rerank\\\": {\\n            \\\"model\\\": \\\"mixedbread-ai/mxbai-rerank-base-v1\\\",\\n            \\\"query\\\": \\\"Is Korvus fast?\\\",\\n            \\\"num_documents_to_rerank\\\": 100\\n        },\\n        \\\"limit\\\": 5\\n      },\\n      \\\"aggregate\\\": {\\\"join\\\": \\\"\\\\n\\\"}\\n    },\\n    \\\"chat\\\": {\\n      \\\"model\\\": \\\"meta-llama/Meta-Llama-3-8B-Instruct\\\",\\n      \\\"messages\\\": [\\n        {\\n          \\\"role\\\": \\\"system\\\",\\n          \\\"content\\\": \\\"You are a friendly and helpful chatbot\\\"\\n        },\\n        {\\n          \\\"role\\\": \\\"user\\\",\\n          \\\"content\\\": \\\"Given the context:\\\\n{LLM_CONTEXT}\\\\nAnswer the question: Is Korvus fast?\\\"\\n        }\\n      ],\\n      \\\"max_tokens\\\": 100\\n    }\\n  }\",\n  pipeline\n);\n```\n\n----------------------------------------\n\nTITLE: Scikit-learn Ensemble Classification Examples\nDESCRIPTION: Examples of training classification models using various scikit-learn ensemble methods including AdaBoost, Random Forest, and Gradient Boosting.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/classification.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'ada_boost');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'bagging');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'extra_trees', hyperparams => '{\"n_estimators\": 10}');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'gradient_boosting_trees', hyperparams => '{\"n_estimators\": 10}');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'random_forest', hyperparams => '{\"n_estimators\": 10}');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'hist_gradient_boosting', hyperparams => '{\"max_iter\": 2}');\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Switch Kit in JavaScript\nDESCRIPTION: This code demonstrates how to use the Korvus OpenSourceAI client to create chat completions with an open-source LLM model in JavaScript. It shows the basic setup and API call pattern that mirrors OpenAI's interface.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/introducing-the-openai-switch-kit-move-from-closed-to-open-source-ai-in-minutes.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst korvus = require(\"korvus\");\nconst client = korvus.newOpenSourceAI();\nconst results = client.chat_completions_create(\n      \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n      [\n          {\n              role: \"system\",\n              content: \"You are a friendly chatbot who always responds in the style of a pirate\",\n          },\n          {\n              role: \"user\",\n              content: \"How many helicopters can a human eat in one sitting?\",\n          },\n      ],\n);\nconsole.log(results);\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple RAG with Korvus\nDESCRIPTION: This code demonstrates how to perform a complete RAG operation in Korvus. It executes vector search based on a query, aggregates the results to form a context, and passes that context to a language model to answer a question.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.rag(\n  {\n    CONTEXT: {\n      vector_search: {\n        query: {\n          fields: {\n            text: {\n              query: \"Is Korvus fast?\",\n              parameters: {\n                prompt: \"Represent this sentence for searching relevant passages: \"\n              },\n            }\n          },\n        },\n        document: { \"keys\": [\"id\"] },\n        limit: 5,\n      },\n      aggregate: { \"join\": \"\\n\" },\n    },\n    chat: {\n      model: \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n      messages: [\n        {\n          role: \"system\",\n          content: \"You are a friendly and helpful chatbot\",\n        },\n        {\n          role: \"user\",\n          content: \"Given the context\\n:{CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n        },\n      ],\n      max_tokens: 100,\n    },\n  },\n  pipeline,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.rag(\n    {\n        \"CONTEXT\": {\n            \"vector_search\": {\n                \"query\": {\n                    \"fields\": {\n                        \"text\": {\n                            \"query\": \"Is Korvus fast?\",\n                            \"parameters\": {\n                                \"prompt\": \"Represent this sentence for searching relevant passages: \"\n                            },\n                        }\n                    },\n                },\n                \"document\": {\"keys\": [\"id\"]},\n                \"limit\": 5,\n            },\n            \"aggregate\": {\"join\": \"\\n\"},\n        },\n        \"chat\": {\n            \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a friendly and helpful chatbot\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Given the context\\n:{CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n                },\n            ],\n            \"max_tokens\": 100,\n        },\n    },\n    pipeline,\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection.rag(serde_json::json!(\n    {\n        \"CONTEXT\": {\n            \"vector_search\": {\n                \"query\": {\n                    \"fields\": {\n                        \"text\": {\n                            \"query\": \"Is Korvus fast?\",\n                            \"parameters\": {\n                                \"prompt\": \"Represent this sentence for searching relevant passages: \"\n                            },\n                        }\n                    },\n                },\n                \"document\": {\"keys\": [\"id\"]},\n                \"limit\": 5,\n            },\n            \"aggregate\": {\"join\": \"\\n\"},\n        },\n        \"chat\": {\n            \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a friendly and helpful chatbot\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Given the context\\n:{CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n                },\n            ],\n            \"max_tokens\": 100,\n        },\n    }\n).into(), &mut pipeline).await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nchar * results = korvus_collectionc_rag(collection, \n  \"{\\  \n    \\\"CONTEXT\\\": {\\  \n      \\\"vector_search\\\": {\\  \n        \\\"query\\\": {\\  \n          \\\"fields\\\": {\\  \n            \\\"text\\\": {\\  \n              \\\"query\\\": \\\"Is Korvus fast?\\\",\\  \n              \\\"parameters\\\": {\\  \n                \\\"prompt\\\": \\\"Represent this sentence for searching relevant passages: \\\"\\  \n              }\\  \n            }\\  \n          }\\  \n        },\\  \n        \\\"document\\\": {\\\"keys\\\": [\\\"id\\\"]},\\  \n        \\\"limit\\\": 5\\  \n      },\\  \n      \\\"aggregate\\\": {\\\"join\\\": \\\"\\\\n\\\"}\\  \n    },\\  \n    \\\"chat\\\": {\\  \n      \\\"model\\\": \\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\",\\  \n      \\\"messages\\\": [\\  \n        {\\  \n          \\\"role\\\": \\\"system\\\",\\  \n          \\\"content\\\": \\\"You are a friendly and helpful chatbot\\\"\\  \n        },\\  \n        {\\  \n          \\\"role\\\": \\\"user\\\",\\  \n          \\\"content\\\": \\\"Given the context:\\\\n{CONTEXT}\\\\nAnswer the question: Is Korvus fast?\\\"\\  \n        }\\  \n      ],\\  \n      \\\"max_tokens\\\": 100\\  \n    }\\  \n  }\",\n  pipeline\n);\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Normalized Embeddings in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to create a new table that stores normalized embeddings using the pgml.normalize_l2() function. It selects data from the existing documents table and applies L2 normalization to the embeddings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-normalization.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents_normalized_vectors AS \nSELECT \n    id AS document_id, \n    pgml.normalize_l2(embedding) AS normalized_l2_embedding\nFROM documents;\n```\n\n----------------------------------------\n\nTITLE: Executing RAG Query with Multiple Context Variables in C\nDESCRIPTION: This C code demonstrates how to perform a RAG operation with both vector search (LLM_CONTEXT) and custom SQL query (CUSTOM_CONTEXT). It uses vector search with reranking to find relevant documents about 'Korvus', while also executing a custom SQL query that returns a string. Both context elements are used to answer a question about Korvus's speed.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nchar * results = korvus_collectionc_rag(collection,\n  \"{\\    \"LLM_CONTEXT\": {\\      \"vector_search\": {\\        \"query\": {\\          \"fields\": {\\            \"text\": {\\              \"query\": \"Is Korvus fast?\",\\              \"parameters\": {\\                \"prompt\": \"Represent this sentence for searching relevant passages: \"\\              },\\              \"full_text_filter\": \"Korvus\"\\            }\\          }\\        },\\        \"document\": {\"keys\": [\"id\"]},\\            \"rerank\": {\\            \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\\            \"query\": \"Is Korvus fast?\",\\            \"num_documents_to_rerank\": 100\\        },\\        \"limit\": 1\\      },\\      \"aggregate\": {\"join\": \"\\\\n\"}\\    },\\    \"CUSTOM_CONTEXT\": {\"sql\": \"SELECT 'Korvus is super fast!!!'\"},\\    \"chat\": {\\      \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\\      \"messages\": [\\        {\\          \"role\": \"system\",\\          \"content\": \"You are a friendly and helpful chatbot\"\\        },\\        {\\          \"role\": \"user\",\\          \"content\": \"Given the context:\\\\n{LLM_CONTEXT}\\\\n\\\\n{CUSTOM_CONTEXT}\\\\nAnswer the question: Is Korvus fast?\"\\        }\\      ],\\      \"max_tokens\": 100\\    }\\  }\",\n  pipeline\n);\n```\n\n----------------------------------------\n\nTITLE: Advanced PostgreSQL Document Ranking with Configuration\nDESCRIPTION: Example showing how to configure the ranking function to return only top k results without documents using JSON parameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.rank.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.rank('mixedbread-ai/mxbai-rerank-base-v1', 'test', ARRAY['doc1', 'doc2'], '{\"return_documents\": false, \"top_k\": 10}'::JSONB);\n```\n\n----------------------------------------\n\nTITLE: Basic Regression Training and Prediction in PostgreSQL\nDESCRIPTION: Demonstrates loading a dataset, training a regression model, and making predictions using the diabetes dataset. Shows core workflow of PostgreSQL ML operations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/regression.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\n-- load the dataset\nSELECT pgml.load_dataset('diabetes');\n\n-- view the dataset\nSELECT * FROM pgml.diabetes LIMIT 10;\n\n-- train a simple model on the data\nSELECT * FROM pgml.train('Diabetes Progression', 'regression', 'pgml.diabetes', 'target');\n\n-- check out the predictions\nSELECT target, pgml.predict('Diabetes Progression', ARRAY[age, sex, bmi, bp, s1, s2, s3, s4, s5, s6]) AS prediction\nFROM pgml.diabetes \nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Measuring Semantic Similarity with Cosine Distance Between Embedded Texts\nDESCRIPTION: This example calculates the cosine distance between a query embedding and two document embeddings using the mixedbread-ai/mxbai-embed-large-v1 model. It demonstrates how semantically related texts have smaller distances than unrelated texts.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed(\n  'mixedbread-ai/mxbai-embed-large-v1',\n  'What is the pgml.transform function?'\n)::vector\n  <=>\npgml.embed(\n  'mixedbread-ai/mxbai-embed-large-v1',\n  'The pgml.transform function is a PostgreSQL function for calling LLMs in the database.'\n)::vector AS cosine_distance;\n\nSELECT pgml.embed(\n  'mixedbread-ai/mxbai-embed-large-v1',\n  'What is the pgml.transform function?'\n)::vector\n  <=>\npgml.embed(\n  'mixedbread-ai/mxbai-embed-large-v1',\n  'I think tomatoes are incredible on burgers.'\n)::vector AS cosine_distance;\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning DistilBERT for IMDB Sentiment Classification\nDESCRIPTION: Executes the fine-tuning process for a DistilBERT model on the IMDB dataset using pgml.tune function with specified hyperparameters and Hugging Face integration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.tune(\n    'imdb_review_sentiment',\n    task => 'text-classification',\n    relation_name => 'pgml.imdb_train_view',\n    model_name => 'distilbert-base-uncased',\n    test_size => 0.2,\n    test_sampling => 'last',\n    hyperparams => '{\n        \"training_args\" : {\n            \"learning_rate\": 2e-5,\n            \"per_device_train_batch_size\": 16,\n            \"per_device_eval_batch_size\": 16,\n            \"num_train_epochs\": 20,\n            \"weight_decay\": 0.01,\n            \"hub_token\" : \"YOUR_HUB_TOKEN\",\n            \"push_to_hub\" : true\n        },\n        \"dataset_args\" : { \"text_column\" : \"text\", \"class_column\" : \"class\" }\n    }'\n);\n```\n\n----------------------------------------\n\nTITLE: Processing and Indexing Crawled Data with Korvus in Python\nDESCRIPTION: Main function that orchestrates the entire process of setting up the pipeline, crawling the website, and indexing the crawled data. It transforms raw crawl results into document objects and upserts them into the collection for future querying.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-firecrawl-rag-in-a-single-query.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nasync def main():\n    # Add our Pipeline to our Collection\n    await add_pipeline()\n\n    # Crawl the website\n    results = crawl()\n\n    # Construct our documents to upsert\n    documents = [\n        {\"id\": data[\"metadata\"][\"sourceURL\"], \"markdown\": data[\"markdown\"]}\n        for data in results[\"data\"]\n    ]\n\n    # Upsert our documents\n    await collection.upsert_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Initializing Korvus Pipeline and Collection for Document Search\nDESCRIPTION: Sets up a Korvus pipeline with semantic and full-text search capabilities for the 'abstract' field and semantic search for the 'body' field. It then creates a collection and adds the pipeline to it.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/document-search.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst pipeline = korvus.newPipeline(\"test_pipeline\", {\n  abstract: {\n    semantic_search: {\n      model: \"mixedbread-ai/mxbai-embed-large-v1\",\n    },\n    full_text_search: { configuration: \"english\" },\n  },\n  body: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"Alibaba-NLP/gte-base-en-v1.5\",\n    },\n  },\n});\nconst collection = korvus.newCollection(\"test_collection\");\nawait collection.add_pipeline(pipeline);\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"test_pipeline\",\n    {\n        \"abstract\": {\n            \"semantic_search\": {\n                \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n            },\n            \"full_text_search\": {\"configuration\": \"english\"},\n        },\n        \"body\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n            },\n        },\n    },\n)\ncollection = Collection(\"test_collection\")\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut pipeline = Pipeline::new(\n    \"test_pipeline\",\n    Some(\n        serde_json::json!(\n            {\n                \"abstract\": {\n                    \"semantic_search\": {\n                        \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n                    },\n                    \"full_text_search\": {\"configuration\": \"english\"},\n                },\n                \"body\": {\n                    \"splitter\": {\"model\": \"recursive_character\"},\n                    \"semantic_search\": {\n                        \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n                    },\n                },\n            }\n        )\n        .into(),\n    ),\n)?;\nlet mut collection = Collection::new(\"test_collection\", None)?;\ncollection.add_pipeline(&mut pipeline).await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nPipelineC *pipeline = korvus_pipelinec_new(\"test_pipeline\", \"{\\\n    \\\"abstract\\\": {\\\n        \\\"semantic_search\\\": {\\\n            \\\"model\\\": \\\"mixedbread-ai/mxbai-embed-large-v1\\\"\\\n        },\\\n        \\\"full_text_search\\\": {\\\"configuration\\\": \\\"english\\\"}\\\n    },\\\n    \\\"body\\\": {\\\n        \\\"splitter\\\": {\\\"model\\\": \\\"recursive_character\\\"},\\\n        \\\"semantic_search\\\": {\\\n            \\\"model\\\": \\\"Alibaba-NLP/gte-base-en-v1.5\\\"\\\n        }\\\n    }\\\n}\");\nCollectionC * collection = korvus_collectionc_new(\"test_collection\", NULL);\nkorvus_collectionc_add_pipeline(collection, pipeline);\n```\n\n----------------------------------------\n\nTITLE: Performing Complex Keyword Search with PostgreSQL\nDESCRIPTION: Demonstrates a more complex keyword search using the indexed tsvector column, searching for documents containing both 'another' and 'second'.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * \nFROM documents\nWHERE title_and_body_text @@ to_tsquery('english', 'another & second');\n```\n\n----------------------------------------\n\nTITLE: Scraping Y Combinator Job Listings with Python\nDESCRIPTION: This Python script uses Selenium and BeautifulSoup to scrape the last 4 months of Y Combinator job postings. It navigates through multiple pages, extracts job links, and saves the HTML content of each job listing.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-trellis-semantic-search-over-yc-jobs.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\nimport time\nimport os\n\ndriver = webdriver.Chrome()\n\n\ndef get_rendered_html(url):\n    driver.get(url)\n    time.sleep(3)  # Wait for JavaScript to finish rendering (adjust time as needed)\n    return driver.page_source\n\n\ndef extract_links_from_rendered_page(soup):\n    links = []\n    for span in soup.find_all(\"span\", class_=\"titleline\"):\n        a_tag = span.find(\"a\")\n        if a_tag:\n            links.append(a_tag[\"href\"])\n    return links\n\n\ndef save_html_to_file(url, content, folder):\n    \"\"\"Save the HTML content to a file in the specified folder.\"\"\"\n    # Create a valid filename based on the URL\n    filename = url.replace(\"https://\", \"\").replace(\"/\", \"_\") + \".html\"\n    filepath = os.path.join(folder, filename)\n\n    # Save the HTML content to the file\n    with open(filepath, \"w+\") as file:\n        file.write(content)\n    print(f\"Saved: {filepath}\")\n\n\ndef scrape_pages(url, num_pages, output_folder):\n    current_url = url\n    for _ in range(num_pages):\n        rendered_html = get_rendered_html(current_url)\n        soup = BeautifulSoup(rendered_html, \"html.parser\")\n        links = extract_links_from_rendered_page(soup)\n\n        # Save the HTML of each job link\n        for link in links:\n            time.sleep(5)\n            try:\n                job_html = get_rendered_html(link)\n                save_html_to_file(link, job_html, output_folder)\n            except Exception as e:\n                print(f\"EXCEPTION: {e}\")\n                continue\n\n        # Find the next page URL from the \"More\" link\n        next_page = soup.find(\"a\", class_=\"morelink\")\n        if next_page:\n            current_url = \"https://news.ycombinator.com/\" + next_page[\"href\"]\n        else:\n            break\n\n\nif __name__ == \"__main__\":\n    start_url = \"https://news.ycombinator.com/jobs\"\n    num_pages = 9  # Set the number of pages to scrape\n    output_folder = \"scraped_html\"  # Folder to save the HTML files\n\n    scrape_pages(start_url, num_pages, output_folder)\n\ndriver.quit()  # Close the browser when done\n```\n\n----------------------------------------\n\nTITLE: Fine-Tuning Translation Model\nDESCRIPTION: Tunes the Helsinki-NLP translation model for English to Spanish translation using the kde4 dataset.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.tune(\n    'Translate English to Spanish',\n    task => 'translation',\n    relation_name => 'kde4_en_to_es',\n    y_column_name => 'es',\n    model_name => 'Helsinki-NLP/opus-mt-en-es',\n    hyperparams => '{\n        \"learning_rate\": 2e-5,\n        \"per_device_train_batch_size\": 16,\n        \"per_device_eval_batch_size\": 16,\n        \"num_train_epochs\": 1,\n        \"weight_decay\": 0.01,\n        \"max_length\": 128\n    }',\n    test_size => 0.5,\n    test_sampling => 'last'\n);\n```\n\n----------------------------------------\n\nTITLE: Summarizing Documents in Question Answering\nDESCRIPTION: This snippet conveys the process of finding relevant documents in response to a question and summarizing these documents, showcasing an application of question answering with summarization.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/examples/README.md#2025-04-19_snippet_6\n\n\n\n----------------------------------------\n\nTITLE: Performing Text Summarization with PostgresML and Pegasus Model\nDESCRIPTION: Demonstrates how to use pgml.transform to perform text summarization on a text input using the google/pegasus-xsum model. The query accepts a JSON configuration for the task and an array of input texts to summarize.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/summarization.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n        task => '{\n          \"task\": \"summarization\", \n          \"model\": \"google/pegasus-xsum\"\n    }'::JSONB,\n        inputs => array[\n         'Paris is the capital and most populous city of France, with an estimated population of 2,175,601 residents as of 2018,\n         in an area of more than 105 square kilometres (41 square miles). The City of Paris is the centre and seat of government\n         of the region and province of le-de-France, or Paris Region, which has an estimated population of 12,174,880,\n         or about 18 percent of the population of France as of 2017.'\n        ]\n);\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"summary_text\": \"The City of Paris is the centre and seat of government of the region and province of le-de-France, or Paris Region, which has an estimated population of 12,174,880, or about 18 percent of the population of France as of 2017.\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Search with Korvus in Python\nDESCRIPTION: Python implementation of semantic search using Korvus SDK, showing collection creation, pipeline configuration, document insertion, and vector search functionality. Uses asyncio for asynchronous operations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/example-apps/semantic-search.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom korvus import Collection, Pipeline\nfrom rich import print\nimport asyncio\n\n# Initialize our Collection\ncollection = Collection(\"semantic-search-demo\")\n\n# Initialize our Pipeline\n# Our Pipeline will split and embed the `text` key of documents we upsert\npipeline = Pipeline(\n    \"v1\",\n    {\n        \"text\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n            },\n        },\n    },\n)\n\n\nasync def main():\n    # Add our Pipeline to our Collection\n    await collection.add_pipeline(pipeline)\n\n    # Upsert our documents\n    # The `text` key of our documents will be split and embedded per our Pipeline specification above\n    documents = [\n        {\n            \"id\": \"1\",\n            \"text\": \"Korvus is incredibly fast and easy to use.\",\n        },\n        {\n            \"id\": \"2\",\n            \"text\": \"Tomatoes are incredible on burgers.\",\n        },\n    ]\n    await collection.upsert_documents(documents)\n\n    # Perform vector_search\n    # We are querying for the string \"Is Korvus fast?\"\n    # Notice that the `mixedbread-ai/mxbai-embed-large-v1` embedding model takes a prompt parameter when embedding for search\n    # We specify that we only want to return the `id` of documents. If the `document` key was blank it would return the entire document with every result\n    # Limit the results to 5. In our case we only have two documents in our Collection so we will only get two results\n    results = await collection.vector_search(\n        {\n            \"query\": {\n                \"fields\": {\n                    \"text\": {\n                        \"query\": \"Is Korvus fast?\",\n                        \"parameters\": {\n                            \"prompt\": \"Represent this sentence for searching relevant passages: \",\n                        },\n                    },\n                },\n            },\n            \"document\": {\"keys\": [\"id\"]},\n            \"limit\": 5,\n        },\n        pipeline,\n    )\n    print(results)\n\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Executing Fill-Mask Task with PostgresML\nDESCRIPTION: SQL query that uses pgml.transform to perform a fill-mask task on a sentence with a masked word. The query takes a JSON configuration specifying the task type and an input array containing the sentence with mask token.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fill-mask.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n        \"task\" : \"fill-mask\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Paris is the &lt;mask&gt; of France.'\n\n    ]\n) AS answer;\n```\n\n----------------------------------------\n\nTITLE: Chat Stream Example Usage\nDESCRIPTION: Example showing how to use the chat streaming API with Meta-Llama model for a basic conversation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform_stream.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform_stream(\n    task => '{\n        \"task\": \"conversational\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    }'::JSONB,\n    inputs => ARRAY[\n        '{\"role\": \"system\", \"content\": \"You are a friendly and helpful chatbot\"}'::JSONB,\n        '{\"role\": \"user\", \"content\": \"Tell me about yourself.\"}'::JSONB\n    ]\n) AS answer;\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Automatically Normalized Embeddings in PostgreSQL\nDESCRIPTION: This snippet shows how to create a table that automatically generates and normalizes embeddings using pgml.normalize_l2() and pgml.embed() functions. The normalized embedding is generated from the 'body' text field using the 'intfloat/e5-small-v2' model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-normalization.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents (\n   id SERIAL PRIMARY KEY,\n   body TEXT,\n   embedding FLOAT[] GENERATED ALWAYS AS (pgml.normalize_l2(pgml.embed('intfloat/e5-small-v2', body))) STORED\n);\n```\n\n----------------------------------------\n\nTITLE: Text Generation with pgml.transform in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to use the `pgml.transform` function to perform text generation within a PostgreSQL query. It integrates embedding and ranking operations, using the output to generate text with a specified model. The generated text is based on the retrieved context, creating a unified RAG pipeline.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/unified-rag.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH embedded_query AS (\n    SELECT\n        pgml.embed('mixedbread-ai/mxbai-embed-large-v1', 'How do I write a select statement with pgml.transform?', '{\"prompt\": \"Represent this sentence for searching relevant passages: \"}')::vector embedding\n),\nvector_search AS (\n    SELECT\n      chunks.id,\n      (\n          SELECT\n              embedding\n          FROM embedded_query) <=> embeddings.embedding cosine_distance,\n      chunks.chunk\n  FROM\n      chunks\n  INNER JOIN embeddings ON embeddings.chunk_id = chunks.id\n  ORDER BY\n      embeddings.embedding <=> (\n          SELECT\n              embedding\n          FROM embedded_query)\n  LIMIT 6\n),\nrow_number_vector_search AS (\n    SELECT\n        cosine_distance,\n        chunk,\n        ROW_NUMBER() OVER () AS row_number\n    FROM\n        vector_search\n),\ncontext AS (\n  SELECT\n      chunk\n  FROM (\n      SELECT\n        chunk\n      FROM\n          row_number_vector_search AS rnsv1\n      INNER JOIN (\n          SELECT\n            pgml.rank('mixedbread-ai/mxbai-rerank-base-v1', 'How do I write a select statement with pgml.transform?', array_agg(\"chunk\"), '{\"return_documents\": false, \"top_k\": 1}'::jsonb || '{}') AS rank\n          FROM\n            row_number_vector_search\n      ) AS rnsv2 ON (rank).corpus_id + 1 = rnsv1.row_number\n  ) AS sub_query\n)\nSELECT\n    pgml.transform (\n      task => '{\n        \"task\": \"conversational\",\n        \"model\": \"meta-llama/Meta-Llama-3-8B-Instruct\"\n      }'::jsonb, \n      inputs => ARRAY['{\"role\": \"system\", \"content\": \"You are a friendly and helpful chatbot.\"}'::jsonb, jsonb_build_object('role', 'user', 'content', replace('Given the context answer the following question: How do I write a select statement with pgml.transform? Context:\\n\\n{CONTEXT}', '{CONTEXT}', chunk))], \n      args => '{\n        \"max_new_tokens\": 100\n      }'::jsonb)\nFROM\n    context;\n```\n\n----------------------------------------\n\nTITLE: Vector Search with Complex $or and $and Filters\nDESCRIPTION: Demonstrates complex filtering using logical operators to combine multiple conditions. Filters for documents that either have special not equal to true, or have both user_id equal to 1 and user_score less than 100.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.vector_search(\n  {\n    query: {\n      fields: {\n        body: {\n          query: \"What is the best database?\", \n          parameters: {\n            instruction:\n              \"Represent this sentence for searching relevant passages: \",\n          }\n        },\n      },\n      filter: {\n        $or: [\n          {\n            $and: [\n              {\n                $eq: {\n                  user_id: 1\n                }\n              },\n              {\n                $lt: {\n                  user_score: 100\n                }\n              }\n            ]\n          },\n          {\n            special: {\n              $ne: true\n            }\n          }\n        ]\n      }\n    },\n    limit: 5,\n  },\n  pipeline,\n);\n```\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.vector_search(\n    {\n        \"query\": {\n            \"fields\": {\n                \"body\": {\n                    \"query\": \"What is the best database?\",\n                    \"parameters\": {\n                        \"instruction\": \"Represent this sentence for searching relevant passages: \",\n                    },\n                },\n            },\n            \"filter\": {\n                \"$or\": [\n                    {\"$and\": [{\"$eq\": {\"user_id\": 1}}, {\"$lt\": {\"user_score\": 100}}]},\n                    {\"special\": {\"$ne\": True}},\n                ],\n            },\n        },\n        \"limit\": 5,\n    },\n    pipeline,\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection\n    .vector_search(\n        serde_json::json!({\n            \"query\": {\n                \"fields\": {\n                    \"body\": {\n                        \"query\": \"What is the best database?\",\n                        \"parameters\": {\n                            \"instruction\": \"Represent this sentence for searching relevant passages: \",\n                        },\n                    },\n                },\n                \"filter\": {\n                    \"$or\": [\n                        {\"$and\": [{\"$eq\": {\"user_id\": 1}}, {\"$lt\": {\"user_score\": 100}}]},\n                        {\"special\": {\"$ne\": True}},\n                    ],\n                },\n            },\n            \"limit\": 5,\n        })\n        .into(),\n        &mut pipeline,\n    )\n    .await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nr_size = 0;\nchar **results = korvus_collectionc_vector_search(collection, \"{\\\n  \\\"query\\\": {\\\n      \\\"fields\\\": {\\\n          \\\"body\\\": {\\\n              \\\"query\\\": \\\"What is the best database?\\\",\\\n              \\\"parameters\\\": {\\\n                  \\\"instruction\\\": \\\"Represent this sentence for searching relevant passages: \\\"\\\n              }\\\n          }\\\n      },\\\n      \\\"filter\\\": {\\\n          \\\"$or\\\": [\\\n              {\\\"$and\\\": [{\\\"$eq\\\": {\\\"user_id\\\": 1}}, {\\\"$lt\\\": {\\\"user_score\\\": 100}}]},\\\n              {\\\"special\\\": {\\\"$ne\\\": True}}\\\n          ]\\\n      }\\\n  },\\\n  \\\"limit\\\": 5\\\n}\", pipeline, &r_size);\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model in PostgresML\nDESCRIPTION: Creates a new project and trains an XGBoost regression model on the diabetes dataset, automatically deploying it to production.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/static/images/gym/quick_start.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n\t'My First Project',\n\ttask => 'regression',\n\trelation_name => 'pgml.diabetes',\n\ty_column_name => 'target',\n\talgorithm => 'xgboost');\n```\n\n----------------------------------------\n\nTITLE: Grammatical Correctness Assessment with DistilBERT in PostgresML\nDESCRIPTION: This snippet demonstrates how to use a DistilBERT model fine-tuned on the Corpus of Linguistic Acceptability (CoLA) dataset in PostgresML. It shows how to input a sentence and interpret the model's output to determine its grammatical correctness.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-classification.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-classification\", \n      \"model\": \"textattack/distilbert-base-uncased-CoLA\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'I will walk to home when I went through the bus.'\n    ]\n) AS grammatical_correctness;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"label\": \"LABEL_1\", \"score\": 0.9576480388641356}\n]\n```\n\n----------------------------------------\n\nTITLE: Gradient Boosting Classification Examples\nDESCRIPTION: Examples of training classification models using different gradient boosting implementations including XGBoost, LightGBM, and CatBoost.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/classification.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'xgboost', hyperparams => '{\"n_estimators\": 10}');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'xgboost_random_forest', hyperparams => '{\"n_estimators\": 10}');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'lightgbm', hyperparams => '{\"n_estimators\": 1}');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'catboost', hyperparams => '{\"n_estimators\": 1}');\n```\n\n----------------------------------------\n\nTITLE: Re-ranking Search Results with RAG Pipeline in JavaScript\nDESCRIPTION: This JavaScript code snippet demonstrates how to use the `collection.rag` method to perform re-ranking of search results using `vector_search` and `LLM_CONTEXT`. It defines a query, sets up re-ranking parameters (model, query, document count), and configures the chat model for answering a question based on the re-ranked context. The code assumes that `collection` and `pipeline` are defined and accessible.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.rag(\n  {\n    LLM_CONTEXT: {\n      vector_search: {\n        query: {\n          fields: {\n            text: {\n              query: \"Is Korvus fast?\",\n              parameters: {\n                prompt: \"Represent this sentence for searching relevant passages: \"\n              },\n              full_text_filter: \"Korvus\"\n            }\n          },\n        },\n        document: { \"keys\": [\"id\"] },\n        rerank: {\n            model: \"mixedbread-ai/mxbai-rerank-base-v1\",\n            query: \"Is Korvus fast?\",\n            num_documents_to_rerank: 100\n        },\n        limit: 5,\n      },\n      aggregate: { \"join\": \"\\n\" },\n    },\n    chat: {\n      model: \"meta-llama/Meta-Llama-3-8B-Instruct\",\n      messages: [\n        {\n          role: \"system\",\n          content: \"You are a friendly and helpful chatbot\",\n        },\n        {\n          role: \"user\",\n          content: \"Given the context\\n:{LLM_CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n        },\n      ],\n      max_tokens: 100,\n    },\n  },\n  pipeline,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Embedding Model with Custom Prompt in Pipeline\nDESCRIPTION: Example showing how to set up a Pipeline with a specific embedding model (intfloat/e5-small-v2) and configure it with a custom prompt prefix for document embeddings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/constructing-pipelines.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"v0\",\n    {\n        \"text\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"intfloat/e5-small-v2\",\n                \"parameters\": {\n                  \"prompt\": \"passage: \"\n                }\n            },\n            \"full_text_search\": {\n              \"configuration\": \"english\"\n            }\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Web Crawling with Firecrawl in Python\nDESCRIPTION: Creates a function to crawl a website using Firecrawl, with options to filter URLs and limit the number of pages. The function monitors the crawling process until completion, providing status updates during execution.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-firecrawl-rag-in-a-single-query.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef crawl():\n    crawl_url = \"https://postgresml.org/blog\"\n    params = {\n        \"crawlerOptions\": {\n            \"excludes\": [],\n            \"includes\": [\"blog/*\"],\n            \"limit\": 250,\n        },\n        \"pageOptions\": {\"onlyMainContent\": True},\n    }\n    job = firecrawl.crawl_url(crawl_url, params=params, wait_until_done=False)\n    while True:\n        print(\"Scraping...\")\n        status = firecrawl.check_crawl_status(job[\"jobId\"])\n        if not status[\"status\"] == \"active\":\n            break\n        time.sleep(5)\n    return status\n```\n\n----------------------------------------\n\nTITLE: Table Question Answering Using OTT-QA\nDESCRIPTION: This snippet demonstrates the implementation of table question answering using the Open Table-and-Text Question Answering (OTT-QA) dataset and the `deepset/all-mpnet-base-v2-table` model for embedding tabular data.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/examples/README.md#2025-04-19_snippet_5\n\n\n\n----------------------------------------\n\nTITLE: Text Completion with PostgresML\nDESCRIPTION: This snippet demonstrates how to use PostgresML for text completion tasks. It uses the 'meta-llama/Meta-Llama-3.1-8B-Instruct' model to complete a given prompt.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-generation.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n        \"task\": \"text-generation\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone'\n    ]\n) AS answer;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\", Nine for Mortal Men doomed to die, One for the Dark Lord on\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing Euclidean Distance in JavaScript and Python\nDESCRIPTION: Implementation of Euclidean (L2) distance calculation between two vectors using the Pythagorean theorem.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nfunction euclideanDistance(x, y) {\n    let result = 0;\n    for (let i = 0; i < x.length; i++) {\n        result += Math.pow(x[i] - y[i], 2);\n    }\n    return Math.sqrt(result);\n}\n\nlet x = [1, 2, 3];\nlet y = [1, 2, 3];\neuclideanDistance(x, y)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef euclidean_distance(x, y):\n    return math.sqrt(sum([(x-y) * (x-y) for x,y in zip(x,y)]))    \n\nx = [1, 2, 3]\ny = [1, 2, 3]\neuclidean_distance(x, y)\n```\n\n----------------------------------------\n\nTITLE: Chunking Documents Using pgml.chunk in PostgreSQL\nDESCRIPTION: Creates a chunks table and splits documents into smaller pieces using pgml.chunk function with a recursive character chunking strategy. A small chunk size of 250 is used to demonstrate reranking capabilities.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/unified-rag.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE chunks(id SERIAL PRIMARY KEY, chunk text NOT NULL, chunk_index int NOT NULL, document_id int references documents(id));\n\nINSERT INTO chunks (chunk, chunk_index, document_id)\nSELECT\n    (chunk).chunk,\n    (chunk).chunk_index,\n    id\nFROM (\n    SELECT\n        pgml.chunk('recursive_character', document, '{\"chunk_size\": 250}') chunk,\n        id\n    FROM\n        documents) sub_query;\n```\n\n----------------------------------------\n\nTITLE: Deploying Best Scoring Model in PostgreSQL\nDESCRIPTION: This example demonstrates deploying the model with the best score for a handwritten digit image classifier project. It uses the 'best_score' strategy to select the model with the highest performance metric.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.deploy.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.deploy(\n   'Handwritten Digit Image Classifier',\n    strategy => 'best_score'\n);\n```\n\n----------------------------------------\n\nTITLE: Complex User Scoring Prediction Query\nDESCRIPTION: Advanced example showing how to integrate predictions into a larger query for user scoring based on multiple features.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/README.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT *,\n    pgml.predict(\n        'Buy it Again',\n        ARRAY[\n            user.location_id,\n            NOW() - user.created_at,\n            user.total_purchases_in_dollars\n        ]\n    ) AS buying_score\nFROM users\nWHERE tenant_id = 5\nORDER BY buying_score\nLIMIT 25;\n```\n\n----------------------------------------\n\nTITLE: Training and Predicting with PostgresML\nDESCRIPTION: Demonstrates how to train an XGBoost model on an orders table and use it to predict order refund likelihood using PostgresML. The example shows both the training phase using pgml.train() and the prediction phase using pgml.predict().\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/which-database-that-is-the-question.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.train(\n\t'Orders Likely To Be Returned', -- name of your model\n\t'regression', -- objective (regression or classification)\n\t'public.orders', -- table\n\t'refunded', -- label (what are we predicting)\n\t'xgboost' -- algorithm\n);\n\nSELECT\n\tpgml.predict(\n\t\t'Orders Likely To Be Returned',\n\t\tARRAY[orders.*]) AS refund_likelihood,\n\t\torders.*\nFROM orders\nORDER BY refund_likelyhood DESC\nLIMIT 100;\n```\n\n----------------------------------------\n\nTITLE: Sample Output from Semantic Search Query\nDESCRIPTION: Example JSON output showing search results with relevance scores for the query 'Is Korvus fast?'. Demonstrates how semantic similarity is reflected in the scoring.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/example-apps/semantic-search.md#2025-04-19_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n    {'chunk': 'Korvus is incredibly fast and easy to use.', 'document': {'id': '1'}, 'rerank_score': None, 'score': 0.7855310349374217},\n    {'chunk': 'Tomatoes are incredible on burgers.', 'document': {'id': '2'}, 'rerank_score': None, 'score': 0.3634796874710092}\n]\n```\n\n----------------------------------------\n\nTITLE: Inserting Text and Embeddings into PostgreSQL Table\nDESCRIPTION: This code inserts sample data into the 'text_and_embeddings' table, using the pgml.embed function to generate embeddings for the given text strings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO text_and_embeddings (text, embedding)\nVALUES \n  (\n    'The pgml.transform function is a PostgreSQL function for calling LLMs in the database.',\n    pgml.embed(\n      'mixedbread-ai/mxbai-embed-large-v1',\n      'The pgml.transform function is a PostgreSQL function for calling LLMs in the database.'\n    )\n  ),\n\n  (\n    'I think tomatoes are incredible on burgers.',\n    pgml.embed(\n      'mixedbread-ai/mxbai-embed-large-v1',\n      'I think tomatoes are incredible on burgers.'\n    )\n  );\n```\n\n----------------------------------------\n\nTITLE: Generating Embedding for First Review in Dataset\nDESCRIPTION: This query generates an embedding for the first review in the Amazon US Reviews dataset using the Alibaba-NLP/gte-base-en-v1.5 model and the pgml.embed function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    review_body,\n    pgml.embed('Alibaba-NLP/gte-base-en-v1.5', 'passage: ' || review_body)\nFROM pgml.amazon_us_reviews\nLIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Creating Documents Table and Inserting Example Content in PostgreSQL\nDESCRIPTION: Creates a documents table and populates it with a detailed example document containing pgml.transform function examples, along with 100 random documents for testing retrieval capabilities.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/unified-rag.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents (id SERIAL PRIMARY KEY, document text NOT NULL);\n\n-- Insert a document that has some examples of pgml.transform\nINSERT INTO documents (document) VALUES ('\nHere is an example of the pgml.transform function\n\nSELECT pgml.transform(\n  task   => ''{\\n    \"task\": \"text-generation\",\\n    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\\n  }''::JSONB,\\n  inputs  => ARRAY[''AI is going to''],\\n  args   => ''{\\n    \"max_new_tokens\": 100\\n  }''::JSONB\\n);\n\nHere is another example of the pgml.transform function\n\nSELECT pgml.transform(\n  task   => ''{\\n    \"task\": \"text-generation\",\\n    \"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\\n  }''::JSONB,\\n  inputs  => ARRAY[''AI is going to''],\\n  args   => ''{\\n    \"max_new_tokens\": 100\\n  }''::JSONB\\n);\n\nHere is a third example of the pgml.transform function\n\nSELECT pgml.transform(\n  task   => ''{\\n    \"task\": \"text-generation\",\\n    \"model\": \"microsoft/Phi-3-mini-128k-instruct\"\\n  }''::JSONB,\\n  inputs  => ARRAY[''AI is going to''],\\n  args   => ''{\\n    \"max_new_tokens\": 100\\n  }''::JSONB\\n);\n');\n\n-- Also insert some random documents\nINSERT INTO documents (document) SELECT md5(random()::text) FROM generate_series(1, 100);\n```\n\n----------------------------------------\n\nTITLE: Training an XGBoost Regression Model in PostgresML\nDESCRIPTION: SQL query to train an XGBoost model with 100 trees for flight delay prediction using the pgml.train function. The model is trained on the 'flights_mat_3' relation with 'depdelayminutes' as the target variable and uses the Rust runtime for execution.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/scaling-postgresml-to-1-million-requests-per-second.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n\t'flights',\n\ttask => 'regression',\n\trelation_name => 'flights_mat_3',\n\ty_column_name => 'depdelayminutes',\n\talgorithm => 'xgboost',\n\thyperparams => '{\"n_estimators\": 100 }',\n\truntime => 'rust'\n);\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search on Movie Reviews with PostgresML\nDESCRIPTION: This SQL query generates an embedding for a user query, then searches for similar movie reviews in a database of 5 million entries. It uses cosine similarity to rank results and returns the top 5 matches.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'query: Best 1980''s scifi movie'\n  )::vector(1024) AS embedding\n)\n\nSELECT\n  review_body,\n  product_title,\n  star_rating,\n  total_votes,\n  1 - (\n    review_embedding_e5_large <=> (\n      SELECT embedding FROM request\n    )\n  ) AS cosine_similarity\nFROM pgml.amazon_us_reviews\nORDER BY review_embedding_e5_large <=> (SELECT embedding FROM request)\nLIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Text Generation with GPT2 Model in PostgresML\nDESCRIPTION: Demonstrates basic text generation using the GPT2 model with PostgresML transform function. Generates 32 new tokens from a given prompt.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"gpt2\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search with Embeddings in PostgreSQL\nDESCRIPTION: This snippet shows how to use a Common Table Expression (CTE) to generate a search embedding and compare it to existing embeddings in the table to find the nearest neighbors using L2 distance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH query AS (\n    SELECT pgml.embed('intfloat/e5-small-v2', 'An example search query') AS embedding\n)\nSELECT id, pgml.distance_l2(query.embedding, documents_with_embeddings.embedding)\nFROM documents_with_embeddings, query\nORDER BY distance_l2;\n```\n\n----------------------------------------\n\nTITLE: Adding Vector Column in PostgreSQL\nDESCRIPTION: Demonstrates how to add a vector column of size 384 to an existing table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/vector-database.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nALTER TABLE usa_house_prices\nADD COLUMN embedding VECTOR(384);\n```\n\n----------------------------------------\n\nTITLE: Full Pipeline Configuration with Text and Semantic Search\nDESCRIPTION: Creates a pipeline that enables both full-text search on titles and semantic search on body text using the Alibaba GTE model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/pipelines.md#2025-04-19_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst pipeline = korvus.newPipeline(\"test_pipeline\", {\n  title: {\n    full_text_search: { configuration: \"english\" },\n  },\n  body: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"Alibaba-NLP/gte-base-en-v1.5\",\n    },\n  },\n});\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"test_pipeline\",\n    {\n        \"title\": {\n            \"full_text_search\": {\"configuration\": \"english\"},\n        },\n        \"body\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n            },\n        },\n    },\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut pipeline = Pipeline::new(\n    \"test_pipeline\",\n    Some(\n        serde_json::json!({\n            \"title\": {\n                \"full_text_search\": {\"configuration\": \"english\"},\n            },\n            \"body\": {\n                \"splitter\": {\"model\": \"recursive_character\"},\n                \"semantic_search\": {\n                    \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n                },\n            },\n        })\n        .into(),\n    ),\n)?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nPipelineC * pipeline = korvus_pipelinec_new(\n  \"test_pipeline\", \n  \"{\\\n    \\\"title\\\": {\\\n      \\\"full_text_search\\\": {\\\"configuration\\\": \\\"english\\\"},\\\n    },\\\n    \\\"body\\\": {\\\n      \\\"splitter\\\": {\\\"model\\\": \\\"recursive_character\\\"},\\\n      \\\"semantic_search\\\": {\\\n        \\\"model\\\": \\\"Alibaba-NLP/gte-base-en-v1.5\\\"\\\n      }\\\n    }\\\n  }\"\n);\n```\n\n----------------------------------------\n\nTITLE: Simple RAG Pipeline Configuration\nDESCRIPTION: Configures a basic RAG pipeline that only processes body text for semantic search.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/pipelines.md#2025-04-19_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst pipeline = korvus.newPipeline(\"test_pipeline\", {\n  body: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"Alibaba-NLP/gte-base-en-v1.5\",\n    },\n  },\n});\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"test_pipeline\",\n    {\n        \"body\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n            },\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hybrid Search in Rust\nDESCRIPTION: Rust implementation of hybrid search using PostresML's RAG method with serde_json serialization. Combines vector search capabilities with full-text filtering and Meta-Llama-3 model integration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection.rag(serde_json::json!(\n    {\n        \"LLM_CONTEXT\": {\n            \"vector_search\": {\n                \"query\": {\n                    \"fields\": {\n                        \"text\": {\n                            \"query\": \"Is Korvus fast?\",\n                            \"parameters\": {\n                                \"prompt\": \"Represent this sentence for searching relevant passages: \"\n                            },\n                            \"full_text_filter\": \"Korvus\"\n                        }\n                    },\n                },\n                \"document\": {\"keys\": [\"id\"]},\n                \"limit\": 5,\n            },\n            \"aggregate\": {\"join\": \"\\n\"},\n        },\n        \"chat\": {\n            \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a friendly and helpful chatbot\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Given the context\\n:{LLM_CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n                },\n            ],\n            \"max_tokens\": 100,\n        },\n    }\n).into(), &mut pipeline).await?\n```\n\n----------------------------------------\n\nTITLE: Chat Generation with Parameters in PostgresML\nDESCRIPTION: This example shows how to use PostgresML for chat-based text generation with additional parameters such as max_tokens, temperature, and seed. It uses the 'meta-llama/Meta-Llama-3.1-8B-Instruct' model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-generation.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n        \"task\": \"text-generation\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    }'::JSONB,\n    inputs => ARRAY[\n        '{\"role\": \"system\", \"content\": \"You are a friendly and helpful chatbot\"}'::JSONB,\n        '{\"role\": \"user\", \"content\": \"Tell me about yourself.\"}'::JSONB\n    ],\n    args => '{\n        \"max_tokens\": 10,\n        \"temperature\": 0.75,\n        \"seed\": 10\n    }'::JSONB\n) AS answer;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\"I'm so glad you asked! I'm a\"]\n```\n\n----------------------------------------\n\nTITLE: Monitoring Training Metrics with JSON Logs\nDESCRIPTION: Example of training progress logs showing metrics like loss, gradient norm, and learning rate during model training phases.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_6\n\nLANGUAGE: json\nCODE:\n```\nINFO:  {\n    \"loss\": 0.3453,\n    \"grad_norm\": 5.230295181274414,\n    \"learning_rate\": 1.9e-05,\n    \"epoch\": 0.25,\n    \"step\": 500,\n    \"max_steps\": 10000,\n    \"timestamp\": \"2024-03-07 01:59:15.090612\"\n}\n```\n\n----------------------------------------\n\nTITLE: Quora Question Pairs Classification with BERT in PostgresML\nDESCRIPTION: This example shows how to use a BERT model fine-tuned on the Quora Question Pairs dataset in PostgresML. It demonstrates how to input a pair of questions and interpret the model's output to determine if they are paraphrases.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-classification.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n     \"task\": \"text-classification\", \n     \"model\": \"textattack/bert-base-uncased-QQP\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Which city is the capital of France? Where is the capital of France?'\n    ]\n) AS qqp;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"label\": \"LABEL_0\", \"score\": 0.9988721013069152}\n]\n```\n\n----------------------------------------\n\nTITLE: Customizing Splitter Parameters in Pipeline\nDESCRIPTION: Pipeline configuration that demonstrates how to customize text splitter parameters such as chunk size and chunk overlap for more precise document splitting.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/constructing-pipelines.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"v0\",\n    {\n        \"text\": {\n            \"splitter\": {\n              \"model\": \"recursive_character\",\n              \"parameters\": {\n                \"chunk_size\": 1500,\n                \"chunk_overlap\": 40\n              }\n            },\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n            },\n            \"full_text_search\": {\n              \"configuration\": \"english\"\n            }\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Training Multi-Output Model with PostgresML\nDESCRIPTION: Example of using pgml.train_joint() to train a model with multiple target columns. This function supports joint optimization across multiple outputs, potentially improving results compared to independent models. The y_column_name parameter accepts an array of target column names.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/joint-optimization.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train_join(\n    'My Joint Project',\n    task => 'regression',\n    relation_name => 'my_table',\n    y_column_name => ARRAY['target_a', 'target_b'],\n);\n```\n\n----------------------------------------\n\nTITLE: Calculating Cosine Distance Between Two Vectors with pgvector\nDESCRIPTION: This snippet demonstrates how to calculate the cosine distance between two vectors using the pgvector extension's cosine distance operator (<=>). It shows the basic syntax for comparing vector similarity.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT '[1,2,3]'::vector <=> '[2,3,4]'::vector;\n```\n\n----------------------------------------\n\nTITLE: Creating Temporary Table for Embeddings in PostgreSQL\nDESCRIPTION: This snippet creates a temporary table to store document embeddings during the current transaction. It uses the pgml.embed function with the 'intfloat/e5-small-v2' model to generate embeddings from the 'body' field of the 'documents' table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TEMPORARY TABLE embeddings AS\nSELECT id AS document_id,\n       pgml.embed('intfloat/e5-small-v2', body)\nFROM documents;\n```\n\n----------------------------------------\n\nTITLE: Creating GIN Index for Full-Text Search\nDESCRIPTION: Creates a GIN (Generalized Inverted Index) index on the tsvector column to optimize search performance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX documents_title_and_body_text_index \nON documents \nUSING GIN (title_and_body_text);\n```\n\n----------------------------------------\n\nTITLE: Basic Vector Search Implementation\nDESCRIPTION: Performs a basic vector search on the body field with a specific query and returns limited fields. Includes prompt parameters for the embedding model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.vector_search(\n  {\n    query: {\n      fields: {\n        body: {\n          query: \"What is the best database?\", \n          parameters: {\n          prompt:\n              \"Represent this sentence for searching relevant passages: \",\n          }\n        },\n      },\n    },\n    document: {\n        keys: [\n            \"id\",\n            \"abstract\"\n        ]\n    },\n    limit: 5,\n  },\n  pipeline,\n);\n```\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.vector_search(\n    {\n        \"query\": {\n            \"fields\": {\n                \"body\": {\n                    \"query\": \"What is the best database?\",\n                    \"parameters\": {\n                        \"prompt\": \"Represent this sentence for searching relevant passages: \",\n                    },\n                },\n            },\n        },\n        \"document\": {\n            \"keys\": [\n                \"id\",\n                \"abstract\"\n            ]\n        },\n        \"limit\": 5,\n    },\n    pipeline,\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection\n    .vector_search(\n        serde_json::json!({\n            \"query\": {\n                \"fields\": {\n                    \"body\": {\n                        \"query\": \"What is the best database?\",\n                        \"parameters\": {\n                            \"prompt\": \"Represent this sentence for searching relevant passages: \",\n                        },\n                    },\n                },\n            },\n            \"document\": {\n                \"keys\": [\n                    \"id\",\n                    \"abstract\"\n                ]\n            },\n            \"limit\": 5,\n        })\n        .into(),\n        &mut pipeline,\n    )\n    .await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nr_size = 0;\nchar **results = korvus_collectionc_vector_search(collection, \"{\\\n  \\\"query\\\": {\\\n    \\\"fields\\\": {\\\n      \\\"body\\\": {\\\n        \\\"query\\\": \\\"What is the best database?\\\",\\\n        \\\"parameters\\\": {\\\n          \\\"prompt\\\": \\\"Represent this sentence for searching relevant passages: \\\"\\\n        }\\\n      }\\\n    }\\\n  },\\\n  \\\"document\\\": {\\\n    \\\"keys\\\": [\\\n      \\\"id\\\",\\\n      \\\"abstract\\\"\\\n    ]\\\n  },\\\n  \\\"limit\\\": 5\\\n}\",\npipeline, &r_size);\n```\n\n----------------------------------------\n\nTITLE: Using GGML Quantized Models in PostgresML\nDESCRIPTION: Example of using a GGML-quantized GPT-2 model for text generation in PostgresML. GGML quantization offers better optimization for CPU usage, particularly on Apple M1/M2 and Intel hardware.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"marella/gpt-2-ggml\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Basic LLM Query Implementation in Python\nDESCRIPTION: Demonstrates a simple implementation of querying an LLM with tokenized input to get a response about Baldur's Gate 3. Shows the basic flow of text tokenization and model inference.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/chatbots/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nuser_input = \"What is Baldur's Gate 3?\"\ntokenized_input = tokenize(user_input) # toknize will return [25, 12, 2002, 19, 17, 29]\noutput = model(tokenized_input)\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Creating a Table for Storing Text and Embeddings in PostgreSQL\nDESCRIPTION: This snippet creates a table named 'text_and_embeddings' to store text data and their corresponding embeddings using the vector data type provided by pgvector.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE text_and_embeddings (\n    id SERIAL PRIMARY KEY, \n    text text, \n    embedding vector (1024)\n);\n```\n\n----------------------------------------\n\nTITLE: Verifying Embeddings in PostgreSQL\nDESCRIPTION: Queries the embeddings table to verify that chunks were successfully embedded. The command switches to expanded display mode to better view the high-dimensional embedding vector.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/unified-rag.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\n\\x auto\nSELECT * FROM embeddings LIMIT 1;\n\\x off\n```\n\n----------------------------------------\n\nTITLE: JSON Result of Question Answering Query\nDESCRIPTION: This JSON object represents the result of the question answering query. It includes the start and end positions of the answer in the context, the confidence score, and the actual answer text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/question-answering.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"end\"   :  39, \n    \"score\" :  0.9538117051124572, \n    \"start\" :  31, \n    \"answer\": \"stanbul\"\n}\n```\n\n----------------------------------------\n\nTITLE: Training PostgresML Search Ranking Model\nDESCRIPTION: Trains a regression model using PostgresML to predict click probability based on title and body rank features.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n  project_name => 'Search Ranking',\n  task => 'regression',\n  relation_name => 'search_result_clicks',\n  y_column_name => 'clicked'\n);\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Similarity Search in PostgreSQL\nDESCRIPTION: This query demonstrates how to perform an Approximate Nearest Neighbor (ANN) search using HNSW indexes on partitioned vector data. It embeds a query string, calculates cosine distances, and returns the most similar reviews.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/partitioning.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    review_body,\n    review_embedding_e5_large <=> pgml.embed(\n        'Alibaba-NLP/gte-base-en-v1.5',\n        'this chair was amazing'\n    )::vector(1024) AS cosine_distance\nFROM amazon_reviews_with_embedding\nORDER BY cosine_distance\nLIMIT 9;\n```\n\n----------------------------------------\n\nTITLE: Training Model with Preprocessors in PostgresML\nDESCRIPTION: Example showing how to train a classification model with various preprocessing steps including ordinal encoding for months, target encoding for clouds, and standard scaling for numeric columns.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/data-pre-processing.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.train(\n    project_name => 'preprocessed_model', \n    task => 'classification', \n    relation_name => 'weather_data',\n    target => 'rain', \n    preprocess => '{\n        \"month\":    {\"encode\": {\"ordinal\": [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]}}\n        \"clouds\":   {\"encode\": \"target\", scale: \"standard\"}\n        \"humidity\": {\"impute\": \"mean\", scale: \"standard\"}\n        \"temp\":     {\"scale\": \"standard\"}\n    }'\n);\n```\n\n----------------------------------------\n\nTITLE: Generating Daily Summary and Sentiment Score using PostgresML\nDESCRIPTION: This SQL query aggregates all notes for the current day, generates a summary using PostgresML's summarization task, calculates the total sentiment score, and inserts or updates the result in the days table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/sentiment-analysis-using-express-js-and-postgresml.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH day AS (\n  SELECT \n    note,\n    score\n  FROM notes \n  WHERE DATE(created_at) = DATE(NOW())),\n\n  sum AS (\n    SELECT pgml.transform(\n      task => '{\"task\": \"summarization\", \"model\": \"sshleifer/distilbart-cnn-12-6\"}'::JSONB,\n      inputs => array[(SELECT STRING_AGG(note, '\\n') FROM day)],\n      args => '{\"min_length\" : 20, \"max_length\" : 70}'::JSONB\n    ) AS summary\n  )\n\n  INSERT INTO days (summary, score) \n  VALUES ((SELECT summary FROM sum)[0]::JSONB ->> 'summary_text', (SELECT SUM(score) FROM day))\n  On Conflict (created_at) DO UPDATE SET summary=EXCLUDED.summary, score=EXCLUDED.score \n  RETURNING score;\n```\n\n----------------------------------------\n\nTITLE: Vector Search with $gte Filter\nDESCRIPTION: Executes a vector search filtering for documents where user_id is greater than or equal to 1. Returns up to 5 matching results.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.vector_search(\n  {\n    query: {\n      fields: {\n        body: {\n          query: \"What is the best database?\", \n          parameters: {\n            instruction:\n              \"Represent this sentence for searching relevant passages: \",\n          }\n        },\n      },\n      filter: {\n        user_id: {\n          $gte: 1\n        }\n      }\n    },\n    limit: 5,\n  },\n  pipeline,\n);\n```\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.vector_search(\n    {\n        \"query\": {\n            \"fields\": {\n                \"body\": {\n                    \"query\": \"What is the best database?\",\n                    \"parameters\": {\n                        \"instruction\": \"Represent this sentence for searching relevant passages: \",\n                    },\n                },\n            },\n            \"filter\": {\"user_id\": {\"$gte\": 1}},\n        },\n        \"limit\": 5,\n    },\n    pipeline,\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection\n    .vector_search(\n        serde_json::json!({\n            \"query\": {\n                \"fields\": {\n                    \"body\": {\n                        \"query\": \"What is the best database?\",\n                        \"parameters\": {\n                            \"instruction\": \"Represent this sentence for searching relevant passages: \",\n                        },\n                    },\n                },\n                \"filter\": {\"user_id\": {\"$gte\": 1}},\n            },\n            \"limit\": 5,\n        })\n        .into(),\n        &mut pipeline,\n    )\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Support Vector Machine Regression\nDESCRIPTION: Examples of training regression models using different SVM implementations including standard SVM, Nu-SVM, and Linear SVM.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/regression.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'svm', hyperparams => '{\"max_iter\": 100}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'nu_svm', hyperparams => '{\"max_iter\": 10}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'linear_svm', hyperparams => '{\"max_iter\": 100}');\n```\n\n----------------------------------------\n\nTITLE: Executing YC Job Search Script in Python\nDESCRIPTION: This code snippet demonstrates how to run the YC Job Search script from the command line. It shows the output of a sample search query for a job at a well-established company in San Francisco.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-trellis-semantic-search-over-yc-jobs.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n(venv) silas@MacBook-Pro-4 ~/P/p/postgresml-trellis> python3 main.py search\nEnter your search query (or 'q' to quit): A job at a well established company in San Francisco\nSearch Results:\nStaff Software Engineer\nLocation:\nSan Francisco, California, United States\n\nPay:\n204138 - 276186\n\nTechnical Requirements:\n7+ years of full stack software development experience,Advanced knowledge in NodeJs / Javascript and React (or similar languages/frameworks),Experience building scalable technical architecture that can scale to 1mm+ \nusers (including observability tooling, container orchestration, etc),Experience with building security-first products from the ground up (e.g., best practices for authentication and rate limiting, considering how an\nadversary might abuse attack surface),Experience integrating with third-party applications,Experience creating, maintaining, and operating microservices,Experience in securing and optimizing the applications you help\ncreate,Experience developing platforms built using an asynchronous event-based architecture,Experience with a variety of payment rails, including ACH, instant push-to-debit,Mobile development experience with \ncross-platform frameworks\n\nJob Description:\nCollaborate with our leadership team and early adopters to design and implement new products\n\nCompany Description:\nCheckr builds people infrastructure for the future of work. Established in 2014 and valued at $5B, Checkr puts modern technology powered by machine learning in the hands of hiring teams, helping thousands of \ncompanies like Uber, Instacart, Netflix, Compass Group, and Adecco to hire great new people with an experience that's fast, smooth, and safe. Checkr has been recognized as one of BuiltIn's 2023 Best Places to Work in\nthe US and is a Y Combinator 2023 Breakthrough Company and Top Company by Valuation.... (4 more results truncated for readability)\n```\n\n----------------------------------------\n\nTITLE: Vector Search with Re-ranking in Rust\nDESCRIPTION: This Rust code snippet shows how to perform vector search with re-ranking using `collection.vector_search`.  It constructs a JSON object with the query and re-ranking parameters, including the model (`mixedbread-ai/mxbai-rerank-base-v1`), query, and number of documents to re-rank. It relies on the `serde_json` crate for JSON serialization and deserialization. `pipeline` is a mutable reference to the search pipeline.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection\n    .vector_search(\n        serde_json::json!({\n            \"query\": {\n                \"fields\": {\n                    \"body\": {\n                        \"query\": \"What is the best database?\",\n                        \"parameters\": {\n                            \"prompt\": \"Represent this sentence for searching relevant passages: \",\n                        },\n                    },\n                },\n            },\n            \"rerank\": {\n                \"model\": \"mixedbread-ai/mxbai-rerank-base-v1\",\n                \"query\": \"What is the best database\",\n                \"num_documents_to_rerank\": 100,\n            },\n            \"limit\": 5,\n        })\n        .into(),\n        &mut pipeline,\n    )\n    .await?;\n```\n\n----------------------------------------\n\nTITLE: Inserting Sample Search Result Click Data in PostgreSQL\nDESCRIPTION: This query inserts sample data into the search_result_clicks table. It provides example data for 4 searches across 3 documents, including title rank, body rank, and click information.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO search_result_clicks \n  (title_rank, body_rank, clicked) \nVALUES\n-- search 1\n  (0.5, 0.5, true),\n  (0.3, 0.2, false),\n  (0.1, 0.0, false),\n-- search 2\n  (0.0, 0.5, true),\n  (0.0, 0.2, false),\n  (0.0, 0.0, false),\n-- search 3\n  (0.2, 0.5, true),\n  (0.1, 0.2, false),\n  (0.0, 0.0, false),\n-- search 4\n  (0.4, 0.5, true),\n  (0.4, 0.2, false),\n  (0.4, 0.0, false)\n;\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Index for Embeddings in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to create a concurrent vector index on the embedding column using the IVFFLAT index type for efficient similarity searches.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_17\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX CONCURRENTLY index_amazon_us_reviews_on_review_embedding_e5_large\nON pgml.amazon_us_reviews\nUSING ivfflat (review_embedding_e5_large vector_cosine_ops)\nWITH (lists = 2000);\n```\n\n----------------------------------------\n\nTITLE: Querying Falcon-7B Instruct Model with BFloat16 in PostgresML\nDESCRIPTION: This snippet demonstrates how to use the Falcon-7B Instruct model with bfloat16 data type in PostgresML. It sets the torch_dtype parameter to bfloat16 and uses the pgml.transform function to generate text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n        \"model\": \"tiiuae/falcon-7b-instruct\",\n        \"device_map\": \"auto\",\n        \"torch_dtype\": \"bfloat16\",\n        \"trust_remote_code\": true\n     }'::JSONB,\n     args => '{\n        \"max_new_tokens\": 100\n     }'::JSONB,\n     inputs => ARRAY[\n        'Complete the story: Once upon a time,'\n     ]\n) AS result;\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Embeddings Table in PostgreSQL\nDESCRIPTION: Creates an embeddings table and embeds each chunk using pgml.embed function with the mixedbread-ai/mxbai-embed-large-v1 model. Each embedding is stored as a 1024-dimensional vector alongside its corresponding chunk ID.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/unified-rag.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE embeddings (\n    id SERIAL PRIMARY KEY, chunk_id bigint, embedding vector (1024),\n    FOREIGN KEY (chunk_id) REFERENCES chunks (id) ON DELETE CASCADE\n);\n\nINSERT INTO embeddings(chunk_id, embedding)\nSELECT\n    id,\n    pgml.embed('mixedbread-ai/mxbai-embed-large-v1', chunk)\nFROM\n    chunks;\n```\n\n----------------------------------------\n\nTITLE: Generating ER Diagram in JavaScript SDK\nDESCRIPTION: Demonstrates how to generate an Entity-Relationship diagram for a collection in PlantUML format using the JavaScript SDK.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/the-1.0-sdk-is-here.md#2025-04-19_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(await collection.generate_er_diagram(pipeline));\n```\n\n----------------------------------------\n\nTITLE: Creating an HNSW Index on Embedding Column in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to create an HNSW (Hierarchical Navigable Small World) index on the embedding column for even faster search performance, especially for read-heavy workloads.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nDROP index text_and_embeddings_embedding_idx;\n\nCREATE INDEX ON text_and_embeddings\nUSING hnsw (embedding vector_cosine_ops);\n```\n\n----------------------------------------\n\nTITLE: Generate Embeddings for Multiple Documents\nDESCRIPTION: Generates embeddings for all documents in the table by passing each document's body text to the embedding function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT id, pgml.embed('intfloat/e5-small-v2', body)\nFROM documents;\n```\n\n----------------------------------------\n\nTITLE: Displaying RAG Performance Benchmarks in Text Format\nDESCRIPTION: This code snippet displays the benchmark results of different RAG systems (PostgresML, Weaviate, Zilliz, Pinecone, and Qdrant), showing average times for embedding, search, retrieval, and completion operations across 25 trials.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/a-speed-comparison-of-the-most-popular-retrieval-systems-for-rag.md#2025-04-19_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nDone Doing RAG Test For: PostgresML\n- Average `Time to Embed`: 0.0000\n- Average `Time to Search`: 0.0643\n- Average `Total Time for Retrieval`: 0.0643\n- Average `Time for Chatbot Completion`: 0.6444\n- Average `Total Time Taken`: 0.7087\n\nDone Doing RAG Test For: Weaviate\n- Average `Time to Embed`: 0.0000\n- Average `Time to Search`: 0.0000\n- Average `Total Time for Retrieval`: 0.0000\n- Average `Time for Chatbot Completion`: 1.2539\n- Average `Total Time Taken`: 1.2539\n\nDone Doing RAG Test For: Zilliz\n- Average `Time to Embed`: 0.2938\n- Average `Time to Search`: 0.1565\n- Average `Total Time for Retrieval`: 0.4503\n- Average `Time for Chatbot Completion`: 0.5909\n- Average `Total Time Taken`: 1.0412\n\nDone Doing RAG Test For: Pinecone\n- Average `Time to Embed`: 0.2907\n- Average `Time to Search`: 0.2677\n- Average `Total Time for Retrieval`: 0.5584\n- Average `Time for Chatbot Completion`: 0.5949\n- Average `Total Time Taken`: 1.1533\n\nDone Doing RAG Test For: Qdrant\n- Average `Time to Embed`: 0.2901\n- Average `Time to Search`: 0.1674\n- Average `Total Time for Retrieval`: 0.4575\n- Average `Time for Chatbot Completion`: 0.6091\n- Average `Total Time Taken`: 1.0667\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Embeddings Table in PostgreSQL\nDESCRIPTION: Creates an embeddings table and generates embeddings for chunks using the mixedbread-ai/mxbai-embed-large-v1 model via pgml.embed function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/unified-rag.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE embeddings (\n    id SERIAL PRIMARY KEY, chunk_id bigint, embedding vector (1024),\n    FOREIGN KEY (chunk_id) REFERENCES chunks (id) ON DELETE CASCADE\n);\n\nINSERT INTO embeddings(chunk_id, embedding)\nSELECT\n    id,\n    pgml.embed('mixedbread-ai/mxbai-embed-large-v1', chunk)\nFROM\n    chunks;\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Digits Dataset for PCA\nDESCRIPTION: Example showing how to load the sklearn digits dataset, create an unlabeled view for unsupervised learning, and perform PCA decomposition to reduce dimensionality from 64 to 3 components.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/decomposition.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.load_dataset('digits');\n\n-- create an unlabeled table of the images for unsupervised learning\nCREATE VIEW pgml.digit_vectors AS\nSELECT image FROM pgml.digits;\n\n-- view the dataset\nSELECT left(image::text, 40) || ',...}' FROM pgml.digit_vectors LIMIT 10;\n\n-- train a simple model to cluster the data\nSELECT * FROM pgml.train('Handwritten Digit Components', 'decomposition', 'pgml.digit_vectors', hyperparams => '{\"n_components\": 3}');\n\n-- check out the compenents\nSELECT target, pgml.decompose('Handwritten Digit Components', image) AS pca\nFROM pgml.digits\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Translating Text using PostgresML and T5 Model\nDESCRIPTION: Demonstrates using pgml.transform function to translate text with the google-t5/t5-base model. The function takes an array of input texts and translation task configuration as parameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/translation.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nselect pgml.transform(\n    inputs => array[\n        'How are you?'\n    ],\n\ttask => '{\n        \"task\": \"translation\", \n        \"model\": \"google-t5/t5-base\"\n    }'::JSONB\t\n);\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"translation_text\": \"Comment allez-vous ?\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Text Completion Stream Example\nDESCRIPTION: Example demonstrating text completion streaming with Meta-Llama model for continuing a text prompt.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform_stream.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform_stream(\n    task => '{\n        \"task\": \"text-generation\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    }'::JSONB,\n    input => 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone'\n) AS answer;\n```\n\n----------------------------------------\n\nTITLE: Support Vector Machine Classification Examples\nDESCRIPTION: Examples of training classification models using different SVM implementations including standard SVM, Nu-SVM, and Linear SVM.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/classification.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'svm');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'nu_svm');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'linear_svm');\n```\n\n----------------------------------------\n\nTITLE: Inserting Data with Automatic Embedding Generation in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to insert text data into the 'documents_with_embeddings' table, where embedding vectors are automatically generated based on the 'body' column.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO documents_with_embeddings (body)\nVALUES -- embedding vectors are automatically generated\n    ('Example text data'),\n    ('Another example document'),\n    ('Some other thing');\n```\n\n----------------------------------------\n\nTITLE: Training a Classification Model with pgml.train\nDESCRIPTION: SQL query to train a classification model (XGBoost) on a dataset of handwritten digit images stored in the database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/README.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n    'Handwritten Digit Image Classifier',\n    algorithm => 'xgboost',\n    'classification',\n    'pgml.digits',\n    'target'\n);\n```\n\n----------------------------------------\n\nTITLE: Querying Chunks Table in PostgreSQL\nDESCRIPTION: Retrieves the first 10 rows from the chunks table to verify that documents were correctly split into chunks. The results show the chunked content with both text references and code examples.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/unified-rag.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM chunks limit 10;\n```\n\n----------------------------------------\n\nTITLE: Basic Classification Example with PostgresML\nDESCRIPTION: A simple example demonstrating how to create a classification project using the pgml.train() function. This snippet shows the minimal required parameters for training a model on the digits dataset with 'target' as the label column.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.train.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n    project_name => 'My Classification Project', \n    task => 'classification', \n    relation_name => 'pgml.digits',\n    y_column_name => 'target'\n);\n```\n\n----------------------------------------\n\nTITLE: Deploying Specific Model by ID in PostgreSQL\nDESCRIPTION: This example shows how to deploy a specific model using its ID. This method allows for precise control over which model is deployed, regardless of its score or recency.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.deploy.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.deploy(12);\n```\n\n----------------------------------------\n\nTITLE: Part-of-Speech Tagging Query in PostgresML\nDESCRIPTION: Example of using PostgresML transform function for Part-of-Speech (PoS) tagging using a specific BERT model. The query analyzes text to identify grammatical parts of speech for each word.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/token-classification.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nselect pgml.transform(\n\tinputs => array [\n  \t'I live in Amsterdam.'\n\t],\n\ttask => '{\"task\": \"token-classification\", \n              \"model\": \"vblagoje/bert-english-uncased-finetuned-pos\"\n    }'::JSONB\n) as pos;\n```\n\nLANGUAGE: json\nCODE:\n```\n[[\n    {\"end\": 1,  \"word\": \"i\",         \"index\": 1, \"score\": 0.999, \"start\": 0,  \"entity\": \"PRON\"},\n    {\"end\": 6,  \"word\": \"live\",      \"index\": 2, \"score\": 0.998, \"start\": 2,  \"entity\": \"VERB\"},\n    {\"end\": 9,  \"word\": \"in\",        \"index\": 3, \"score\": 0.999, \"start\": 7,  \"entity\": \"ADP\"},\n    {\"end\": 19, \"word\": \"amsterdam\", \"index\": 4, \"score\": 0.998, \"start\": 10, \"entity\": \"PROPN\"}, \n    {\"end\": 20, \"word\": \".\",         \"index\": 5, \"score\": 0.999, \"start\": 19, \"entity\": \"PUNCT\"}\n]]\n```\n\n----------------------------------------\n\nTITLE: Text Completion Stream with Parameters\nDESCRIPTION: Example showing text completion streaming with additional parameters for controlling the generation process.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform_stream.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform_stream(\n    task => '{\n        \"task\": \"text-generation\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    }'::JSONB,\n    input => 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone',\n    args => '{\n        \"max_tokens\": 10,\n        \"temperature\": 0.75,\n        \"seed\": 10\n    }'::JSONB\n) AS answer;\n```\n\n----------------------------------------\n\nTITLE: Creating Test Data Table in PostgreSQL\nDESCRIPTION: This snippet creates a table named 'test_data' in PostgreSQL to store distance metric vectors. The table has an 'id' column as a BIGSERIAL to uniquely identify each entry and a 'vector' column of type FLOAT4 array to hold the vector data.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\n\"CREATE TABLE test_data (\\n    id BIGSERIAL NOT NULL,\\n    vector FLOAT4[]\\n);\"\n```\n\n----------------------------------------\n\nTITLE: Using pgml.embed Function for Text Embeddings in PostgreSQL\nDESCRIPTION: The pgml.embed function takes a model_name and text parameter to generate vector embeddings from text data directly in the PostgreSQL database. This function allows users to leverage state-of-the-art open source models for creating embeddings that can be used in semantic search and recommendation systems.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\npgml.embed(model_name, text)\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Streaming Chat Completions\nDESCRIPTION: Shows how to implement streaming chat completions asynchronously, with examples of handling the streaming iterator in both JavaScript and Python.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/opensourceai.md#2025-04-19_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst korvus = require(\"korvus\");\nconst client = korvus.newOpenSourceAI();\nconst it = await client.chat_completions_create_stream_async(\n  \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n  [\n    {\n      role: \"system\",\n      content: \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\n      role: \"user\",\n      content: \"How many helicopters can a human eat in one sitting?\",\n    },\n  ],\n);\nlet result = await it.next();\nwhile (!result.done) {\n  console.log(result.value);\n  result = await it.next();\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nimport korvus\nclient = korvus.OpenSourceAI()\nresults = await client.chat_completions_create_stream_async(\n    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"How many helicopters can a human eat in one sitting?\",\n        },\n    ]\n)\nasync for c in results:\n    print(c)\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses with OpenAI Switch Kit in JavaScript\nDESCRIPTION: This code demonstrates how to use the streaming API with the Korvus OpenSourceAI client in JavaScript. It shows how to initialize a stream and iterate through the responses as they arrive.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/introducing-the-openai-switch-kit-move-from-closed-to-open-source-ai-in-minutes.md#2025-04-19_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst korvus = require(\"korvus\");\nconst client = korvus.newOpenSourceAI();\nconst it = client.chat_completions_create_stream(\n      \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n      [\n          {\n              role: \"system\",\n              content: \"You are a friendly chatbot who always responds in the style of a pirate\",\n          },\n          {\n              role: \"user\",\n              content: \"How many helicopters can a human eat in one sitting?\",\n          },\n      ],\n);\nlet result = it.next();\nwhile (!result.done) {\n  console.log(result.value);\n  result = it.next();\n}\n```\n\n----------------------------------------\n\nTITLE: Filtered Vector Similarity Search for Movies\nDESCRIPTION: Performs vector similarity search using embeddings to find science fiction movies, filtering results to only include movies with more than 10 reviews.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'query: Best 1980\\'s scifi movie'\n  )::vector(1024) AS embedding\n)\n\nSELECT\n  title,\n  total_reviews,\n  1 - (\n    review_embedding_e5_large <=> (SELECT embedding FROM request)\n  ) AS cosine_similarity\nFROM movies\nWHERE total_reviews > 10\nORDER BY review_embedding_e5_large <=> (SELECT embedding FROM request)\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Batch Prediction with Result Collection\nDESCRIPTION: Example demonstrating how to collect and format batch prediction results using CTE and unnest operations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/batch-predictions.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH predictions AS (\n\tSELECT pgml.predict_batch(\n\t\t'My Classification Project',\n\t\tarray_agg(image)\n\t) AS prediction,\n\tunnest(\n\t\tarray_agg(target)\n\t) AS target\n\tFROM pgml.digits\n\tWHERE target = 0\n)\nSELECT prediction, target FROM predictions\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Task-based Translation with pgml.transform() in PostgreSQL\nDESCRIPTION: This example shows how to use pgml.transform() for translation from English to French. It uses the 'translation_en_to_fr' task and provides an English sentence as input.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT *\nFROM pgml.transform(\n  task => 'translation_en_to_fr',\n  inputs => ARRAY['How do I say hello in French?']\n);\n```\n\n----------------------------------------\n\nTITLE: Text Completion Stream API Definition\nDESCRIPTION: Function signature for the text completion streaming API that accepts task configuration, input text, and optional arguments.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform_stream.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.transform_stream(\n    task JSONB,\n    input text,\n    args JSONB\n)\n```\n\n----------------------------------------\n\nTITLE: Non-Personalized Search Query in PostgreSQL for Comparison\nDESCRIPTION: A simpler SQL query that performs non-personalized search using only query-to-movie similarity and star ratings. This is shown for comparison with the personalized approach to highlight the differences in results.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/personalization.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\n\n```\n\n----------------------------------------\n\nTITLE: Retrieving Customer Review Details in PostgreSQL\nDESCRIPTION: This query fetches the review details for a specific customer, showing their product review, star rating, and review text. It helps validate whether the vector embedding similarity found earlier actually corresponds to a Star Wars fan.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/personalize-embedding-results-with-application-data-in-your-database.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT product_title, star_rating, review_body\nFROM pgml.amazon_us_reviews\nWHERE customer_id = '44366773';\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with PostgresML\nDESCRIPTION: SQL query to train an XGBoost model in PostgresML with 25 estimators (boosting rounds). This demonstrates the simple SQL interface for model training in PostgresML compared to traditional Python implementations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-8-40x-faster-than-python-http-microservices.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n\tproject_name => 'r2',\n\talgorithm => 'xgboost',\n\thyperparams => '{ \"n_estimators\": 25 }'\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Index on Customer ID in PostgreSQL\nDESCRIPTION: Creates a standard PostgreSQL index on the customer ID column for faster lookups when personalizing search results.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/personalization.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX customers_id_idx ON customers (id);\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with a Trained PostgresML Model\nDESCRIPTION: This snippet shows how to use the pgml.predict() function to make real-time predictions using the best deployed model for a given project. It selects the target and prediction for the first 5 rows of the pgml.digits table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/quick-start-with-docker.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT \n    target,\n    pgml.predict('My First PostgresML Project', image) AS prediction\nFROM pgml.digits\nLIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Model-based Text Generation with pgml.transform() in PostgreSQL\nDESCRIPTION: This snippet demonstrates the model-based API of pgml.transform(). It specifies a Mixtral model for text generation, sets maximum token output, and provides an input prompt.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n  task   => '{\n    \"task\": \"text-generation\",\n    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"model_type\": \"mistral\",\n    \"revision\": \"main\",\n    \"device_map\": \"auto\"\n  }'::JSONB,\n  inputs  => ARRAY['AI is going to'],\n  args   => '{\n    \"max_new_tokens\": 100\n  }'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: PostgresML Train Function Definition in PostgreSQL\nDESCRIPTION: The complete function signature of pgml.train() showing all available parameters with their default values. This function creates models for regression, classification or clustering tasks with options for preprocessing and hyperparameter tuning.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.train.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.train(\n    project_name TEXT,\n    task TEXT DEFAULT NULL,\n    relation_name TEXT DEFAULT NULL,\n    y_column_name TEXT DEFAULT NULL,\n    algorithm TEXT DEFAULT 'linear',\n    hyperparams JSONB DEFAULT '{}'::JSONB,\n    search TEXT DEFAULT NULL,\n    search_params JSONB DEFAULT '{}'::JSONB,\n    search_args JSONB DEFAULT '{}'::JSONB,\n    test_size REAL DEFAULT 0.25,\n    test_sampling TEXT DEFAULT 'random',\n    preprocess JSONB DEFAULT '{}'::JSONB\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing ML-Based Search Ranking with PostgreSQL CTEs\nDESCRIPTION: SQL query that demonstrates a two-step search ranking process. First uses ts_rank for initial document ranking based on text search, then applies a machine learning model via pgml.predict to re-rank results. Uses CTEs for logical organization and includes multiple ranking factors for title and body text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_12\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH first_pass_ranked_documents AS (\n  SELECT\n    -- Compute the ts_rank for the title and body text of each document \n    ts_rank(title_and_body_text, to_tsquery('english', 'second | title')) AS title_and_body_rank,       \n    ts_rank(to_tsvector('english', title), to_tsquery('english', 'second | title')) AS title_rank, \n    ts_rank(to_tsvector('english', body), to_tsquery('english', 'second | title')) AS body_rank,\n    * \n  FROM documents \n  WHERE title_and_body_text @@ to_tsquery('english', 'second | title')\n  ORDER BY title_and_body_rank DESC\n  LIMIT 100\n)\nSELECT\n    -- Use the ML model to predict the probability that a user will click on the result\n    pgml.predict('Search Ranking', array[title_rank, body_rank]) AS ml_rank,\n    *\nFROM first_pass_ranked_documents\nORDER BY ml_rank DESC\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Extractive Question Answering with HuggingFace Model\nDESCRIPTION: This snippet illustrates how to utilize the results from vector recall as context for a HuggingFace question answering model, showing the process of integrating vector search results into an extractive question answering task.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/examples/README.md#2025-04-19_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Cosine Distance Queries in PostgreSQL\nDESCRIPTION: Optimized PostgreSQL queries for calculating cosine distance using pgml.cosine_similarity function with different sorting approaches.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH query AS (\n    SELECT vector\n    FROM test_data\n    LIMIT 1\n)\nSELECT id, 1 - pgml.cosine_similarity(query.vector, test_data.vector) AS cosine_distance\nFROM test_data, query\nORDER BY cosine_distance;\n```\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH query AS (\n    SELECT vector\n    FROM test_data\n    LIMIT 1\n)\nSELECT id, pgml.cosine_similarity(query.vector, test_data.vector)\nFROM test_data, query\nORDER BY cosine_similarity DESC;\n```\n\n----------------------------------------\n\nTITLE: Batch Text Classification with PostgresML\nDESCRIPTION: SQL query demonstrating batch prediction processing using pgml.transform on multiple text inputs.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    LEFT(text, 100) AS truncated_text,\n    class,\n    predicted_class[0]->>'label' AS predicted_class,\n    (predicted_class[0]->>'score')::float AS score\nFROM (\n    SELECT\n        LEFT(text, 100) AS text,\n        class,\n        pgml.transform(\n            task => '{\n                \"task\": \"text-classification\",\n                \"model\": \"santiadavani/imdb_review_sentiement\"\n            }'::JSONB,\n            inputs => ARRAY[text]\n        ) AS predicted_class\n    FROM pgml.imdb_test_view\n    LIMIT 2\n) AS subquery;\n```\n\n----------------------------------------\n\nTITLE: Simple Batch Prediction Example\nDESCRIPTION: Example showing how to use pgml.predict_batch() with array aggregation for multiple predictions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/batch-predictions.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.predict_batch(\n    'My First PostgresML Project', \n    array_agg(ARRAY[0.1, 2.0, 5.0])\n) AS prediction\nFROM pgml.digits\n```\n\n----------------------------------------\n\nTITLE: Text Generation with GPT2 Model\nDESCRIPTION: Example of using GPT2 model for text generation with PostgresML transform function, demonstrating basic usage with GPU.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"gpt2\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Completion Implementation\nDESCRIPTION: Shows how to implement streaming chat completion with Meta-Llama model in both JavaScript and Python\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/opensourceai.md#2025-04-19_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst korvus = require(\"korvus\");\nconst client = korvus.newOpenSourceAI();\nconst it = client.chat_completions_create_stream(\n  \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n  [\n    {\n      role: \"system\",\n      content: \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\n      role: \"user\",\n      content: \"How many helicopters can a human eat in one sitting?\",\n    },\n  ],\n);\nlet result = it.next();\nwhile (!result.done) {\n  console.log(result.value);\n  result = it.next();\n}\n```\n\nLANGUAGE: python\nCODE:\n```\nimport korvus\nclient = korvus.OpenSourceAI()\nresults = client.chat_completions_create_stream(\n     \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n     [\n         {\n             \"role\": \"system\",\n             \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n         },\n         {\n             \"role\": \"user\",\n             \"content\": \"How many helicopters can a human eat in one sitting?\",\n         },\n     ]\n)\nfor c in results:\n    print(c)\n```\n\n----------------------------------------\n\nTITLE: Using a Specific Sentiment Analysis Model in PostgresML\nDESCRIPTION: This example shows how to use a specific Hugging Face model (RoBERTa trained on tweets) for sentiment analysis in PostgresML. It demonstrates specifying the model in the task parameter.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-classification.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task  => '{\n      \"task\": \"text-classification\", \n      \"model\": \"finiteautomata/bertweet-base-sentiment-analysis\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'I love how amazingly simple ML has become!', \n        'I hate doing mundane and thankless tasks. '\n    ]\n    \n) AS positivity;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"label\": \"POS\", \"score\": 0.992932200431826}, \n    {\"label\": \"NEG\", \"score\": 0.975599765777588}\n]\n```\n\n----------------------------------------\n\nTITLE: Vector Search SQL Query in PostgreSQL\nDESCRIPTION: The raw SQL query generated by the SDK for performing vector search. It embeds the search query, compares it against stored embeddings, and returns the most similar documents with their relevance scores.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/the-1.0-sdk-is-here.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH \"pipeline\" (\n    \"schema\"\n) AS (\n    SELECT\n        \"schema\"\n    FROM\n        \"my_collection\".\"pipelines\"\n    WHERE\n        \"name\" = 'my_pipeline'\n),\n\"text_embedding\" (\n    \"embedding\"\n) AS (\n    SELECT\n        pgml.embed (transformer => (\n                SELECT\n                    SCHEMA #>> '{text,semantic_search,model}'\n                FROM pipeline), text => 'What is the hidden value?', kwargs => '{}') AS \"embedding\"\n)\nSELECT\n    \"document\",\n    \"chunk\",\n    \"score\"\nFROM (\n    SELECT\n        1 - (embeddings.embedding <=> (\n                SELECT\n                    embedding\n                FROM \"text_embedding\")::vector) AS score,\n        \"documents\".\"id\",\n        \"chunks\".\"chunk\",\n        \"documents\".\"document\"\n    FROM\n        \"my_collection_my_pipeline\".\"text_embeddings\" AS \"embeddings\"\n        INNER JOIN \"my_collection_my_pipeline\".\"text_chunks\" AS \"chunks\" ON \"chunks\".\"id\" = \"embeddings\".\"chunk_id\"\n        INNER JOIN \"my_collection\".\"documents\" AS \"documents\" ON \"documents\".\"id\" = \"chunks\".\"document_id\"\n    ORDER BY\n        embeddings.embedding <=> (\n            SELECT\n                embedding\n            FROM \"text_embedding\")::vector ASC\n    LIMIT 5) AS \"s\"\nORDER BY\n    \"score\" DESC\nLIMIT 5\n```\n\n----------------------------------------\n\nTITLE: Markdown Text Chunking Example\nDESCRIPTION: A simple example showing how to use the markdown splitter to chunk a markdown-formatted text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.chunk.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.chunk('markdown', '# Some test');\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Variables and Parameters\nDESCRIPTION: Configuration of model parameters including splitter settings, embedding task details, and model specifications.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nvars:\n  splitter_name: \"recursive_character\"\n  splitter_parameters: {\"chunk_size\": 100, \"chunk_overlap\": 20}\n  task: \"embedding\"\n  model_name: \"intfloat/e5-small-v2\"\n  query_string: 'Lorem ipsum 3'\n  limit: 2\n```\n\n----------------------------------------\n\nTITLE: Fine-Tuning Text Classification Model\nDESCRIPTION: Tunes the DistilBERT model for sentiment analysis using the IMDB dataset.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.tune(\n    'IMDB Review Sentiment',\n    task => 'text-classification',\n    relation_name => 'pgml.imdb',\n    y_column_name => 'label',\n    model_name => 'distilbert-base-uncased',\n    hyperparams => '{\n        \"learning_rate\": 2e-5,\n        \"per_device_train_batch_size\": 16,\n        \"per_device_eval_batch_size\": 16,\n        \"num_train_epochs\": 1,\n        \"weight_decay\": 0.01\n    }',\n    test_size => 0.5,\n    test_sampling => 'last'\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Documents Table with Automatic Embeddings in PostgreSQL\nDESCRIPTION: Creates a table structure for documents with automatic embedding generation using the intfloat/e5-small-v2 model. The embedding is stored as a FLOAT array and automatically generated from the body text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-aggregation.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents (\n   id SERIAL PRIMARY KEY,\n   body TEXT,\n   embedding FLOAT[] GENERATED ALWAYS AS (pgml.embed('intfloat/e5-small-v2', body)) STORED\n);\n```\n\n----------------------------------------\n\nTITLE: Testing Foreign Data Wrapper Connection with Text Embedding\nDESCRIPTION: SQL query using dblink to test the connection to PostgresML by generating an embedding for sample text using the Alibaba-NLP/gte-base-en-v1.5 model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/pgml-rds-proxy/README.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    *\nFROM\n    dblink(\n        'postgresml',\n        'SELECT * FROM pgml.embed(''Alibaba-NLP/gte-base-en-v1.5'', ''embed this text'') AS embedding'\n) AS t1(embedding real[386]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Chat Completions in JavaScript/Python\nDESCRIPTION: Demonstrates how to create asynchronous chat completions using the Korvus API client with a LLaMA model. The code shows how to set up system and user messages for a pirate-themed chatbot interaction.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/opensourceai.md#2025-04-19_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst korvus = require(\"korvus\");\nconst client = korvus.newOpenSourceAI();\nconst results = await client.chat_completions_create_async(\n  \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n  [\n    {\n      role: \"system\",\n      content: \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\n      role: \"user\",\n      content: \"How many helicopters can a human eat in one sitting?\",\n    },\n  ],\n);\nconsole.log(results);\n```\n\nLANGUAGE: python\nCODE:\n```\nimport korvus\nclient = korvus.OpenSourceAI()\nresults = await client.chat_completions_create_async(\n    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"How many helicopters can a human eat in one sitting?\",\n        },\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Table for Search Result Click Data in PostgreSQL\nDESCRIPTION: This SQL creates a table to store training data for a learning to rank model. It records the ts_rank for both title and body, along with whether the user clicked on the search result.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE search_result_clicks (\n  title_rank REAL,\n  body_rank REAL,\n  clicked BOOLEAN\n);\n```\n\n----------------------------------------\n\nTITLE: Inserting Search Result Training Data in PostgreSQL\nDESCRIPTION: Inserts sample training data representing search result clicks across multiple searches, including title rank, body rank and click outcomes.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO search_result_clicks \n  (title_rank, body_rank, clicked) \nVALUES\n-- search 1\n  (0.5, 0.5, true),\n  (0.3, 0.2, false),\n  (0.1, 0.0, false),\n-- search 2\n  (0.0, 0.5, true),\n  (0.0, 0.2, false),\n  (0.0, 0.0, false),\n-- search 3\n  (0.2, 0.5, true),\n  (0.1, 0.2, false),\n  (0.0, 0.0, false),\n-- search 4\n  (0.4, 0.5, true),\n  (0.4, 0.2, false),\n  (0.4, 0.0, false)\n;\n```\n\n----------------------------------------\n\nTITLE: Predicting Sentiment with PostgresML Binary Classification\nDESCRIPTION: Shows how to use pgml.predict() function to perform binary sentiment classification on text input. Returns 1 for positive sentiment and 0 for negative sentiment.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.predict('IMDB Review Sentiment', 'I love SQL')\nAS sentiment;\n```\n\n----------------------------------------\n\nTITLE: Basic Batch Prediction API Example\nDESCRIPTION: Simple example of using pgml.predict_batch() function with required parameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/batch-predictions.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.predict_batch(\n    project_name TEXT,\n    features REAL[]\n)\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Falcon Model\nDESCRIPTION: Shows text generation using the Falcon-40B model with remote code execution enabled.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/falcon-40b-instruct-GPTQ\",\n      \"trust_remote_code\": true\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing Hybrid Search in Python\nDESCRIPTION: Python implementation of hybrid search using PostresML's RAG method. Performs vector search with full-text filtering and leverages the Meta-Llama-3 model for processing search results.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/rag.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.rag(\n    {\n        \"LLM_CONTEXT\": {\n            \"vector_search\": {\n                \"query\": {\n                    \"fields\": {\n                        \"text\": {\n                            \"query\": \"Is Korvus fast?\",\n                            \"parameters\": {\n                                \"prompt\": \"Represent this sentence for searching relevant passages: \"\n                            },\n                            \"full_text_filter\": \"Korvus\",\n                        }\n                    },\n                },\n                \"document\": {\"keys\": [\"id\"]},\n                \"limit\": 5,\n            },\n            \"aggregate\": {\"join\": \"\\n\"},\n        },\n        \"chat\": {\n            \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a friendly and helpful chatbot\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Given the context\\n:{LLM_CONTEXT}\\nAnswer the question: Is Korvus fast?\",\n                },\n            ],\n            \"max_tokens\": 100,\n        },\n    },\n    pipeline,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Chat Responses with PostgresML\nDESCRIPTION: This snippet demonstrates how to use PostgresML for chat-based text generation. It utilizes the 'meta-llama/Meta-Llama-3.1-8B-Instruct' model and includes system and user prompts.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-generation.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n        \"task\": \"text-generation\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    }'::JSONB,\n    inputs => ARRAY[\n        '{\"role\": \"system\", \"content\": \"You are a friendly and helpful chatbot\"}'::JSONB,\n        '{\"role\": \"user\", \"content\": \"Tell me about yourself.\"}'::JSONB\n    ]\n) AS answer;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\"I'm so glad you asked! I'm a friendly and helpful chatbot, designed to assist and converse with users like you. I'm a large language model, which means I've been trained on a massive dataset of text from various sources, including books, articles, and conversations. Th is training enables me to understand and respond to a wide range of topics and questions.\\n\\nI'm constantly learning and improving my la nguage processing abilities, so I can become more accurate and helpful over time. My primary goal is to provide accurate and relevant in formation, answer your questions, and engage in productive conversations.\\n\\nI'm not just limited to answering questions, though! I can  also:\\n\\n1. Generate text on a given topic or subject\\n2. Offer suggestions and recommendations\\n3. Summarize lengthy texts or articles\\ n4. Translate text from one language to another\\n5. Even create stories, poems, or jokes (if you'd like!)\\n\\nI'm here to help you with a ny questions, concerns, or topics you'd like to discuss. Feel free to ask me anything, and I'll do my best to assist you!\"]\n```\n\n----------------------------------------\n\nTITLE: Gradient Boosting Regression Examples\nDESCRIPTION: Examples of training regression models using different gradient boosting implementations including XGBoost, LightGBM, and CatBoost.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/regression.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'xgboost', hyperparams => '{\"n_estimators\": 10}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'xgboost_random_forest', hyperparams => '{\"n_estimators\": 10}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'lightgbm', hyperparams => '{\"n_estimators\": 1}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'catboost', hyperparams => '{\"n_estimators\": 10}');\n```\n\n----------------------------------------\n\nTITLE: Boosting Title Matches in PostgreSQL Search Ranking\nDESCRIPTION: This query implements a simple boosting function to give more weight to title matches in search results. It calculates separate ranks for title and body, then orders results by a weighted sum of these ranks.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT \n  ts_rank(title, to_tsquery('english', 'second | title')) AS title_rank,\n  ts_rank(body, to_tsquery('english', 'second | title')) AS body_rank,\n  *   \nFROM documents \nORDER BY (2 * title_rank) + body_rank DESC;\n```\n\n----------------------------------------\n\nTITLE: Loading Translation Dataset in PostgreSQL\nDESCRIPTION: Loads the kde4 dataset from Hugging Face into PostgreSQL for English to Spanish translation fine-tuning.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.load_dataset('kde4', kwargs => '{\"lang1\": \"en\", \"lang2\": \"es\"}');\n```\n\n----------------------------------------\n\nTITLE: Example JSON Response from OpenAI Switch Kit\nDESCRIPTION: This shows the JSON response structure returned by the OpenAI Switch Kit. The format mimics OpenAI's API response, including choices, message content, role, and other metadata.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/introducing-the-openai-switch-kit-move-from-closed-to-open-source-ai-in-minutes.md#2025-04-19_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"Me matey, ya landed in me treasure trove o' riddles! But sorry to say, me lads, humans cannot eat helicopters in a single setting, for helicopters are mechanical devices and not food items. So there's no quantity to answer this one! Ahoy there, any other queries ye'd like to raise? Me hearty, we're always at yer service!\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1701291672,\n  \"id\": \"abf042d2-9159-49cb-9fd3-eef16feb246c\",\n  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": \"eecec9d4-c28b-5a27-f90b-66c3fb6cee46\",\n  \"usage\": {\n    \"completion_tokens\": 0,\n    \"prompt_tokens\": 0,\n    \"total_tokens\": 0\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with PostgresML SQL Function\nDESCRIPTION: This SQL query demonstrates how to use the pgml.embed function from PostgresML to generate vector embeddings for text using the mixedbread-ai model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/a-speed-comparison-of-the-most-popular-retrieval-systems-for-rag.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed(\n    transformer => 'mixedbread-ai/mxbai-embed-large-v1', \n    text => 'What is the hidden value'\n) AS \"embedding\";\n```\n\n----------------------------------------\n\nTITLE: Text Generation with LLaMA Model\nDESCRIPTION: Using LLaMA model with specific model_type parameter for text generation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/robin-7B-v2-GGML\",\n      \"model_type\": \"llama\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Examining PostgreSQL Query Plan with EXPLAIN\nDESCRIPTION: Demonstrates how to use EXPLAIN command with JSON format to analyze query execution plan for an indexed table lookup. Shows a query plan output that confirms index usage through an Index Scan operation on a house prices database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/README.md#2025-04-19_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\npostgresml=# EXPLAIN (FORMAT JSON) SELECT\n    \"Avg. Area House Age\",\n    \"Address\"\nFROM usa_house_prices\nWHERE \"Address\" = '1 Infinite Loop, Cupertino, California';\n\n                                          QUERY PLAN\n----------------------------------------------------------------------------------------------\n [                                                                                           +\n   {                                                                                         +\n     \"Plan\": {                                                                               +\n       \"Node Type\": \"Index Scan\",                                                            +\n       \"Parallel Aware\": false,                                                              +\n       \"Async Capable\": false,                                                               +\n       \"Scan Direction\": \"Forward\",                                                          +\n       \"Index Name\": \"usa_house_prices_Address_idx\",                                         +\n       \"Relation Name\": \"usa_house_prices\",                                                  +\n       \"Alias\": \"usa_house_prices\",                                                          +\n       \"Startup Cost\": 0.28,                                                                 +\n       \"Total Cost\": 8.30,                                                                   +\n       \"Plan Rows\": 1,                                                                       +\n       \"Plan Width\": 51,                                                                     +\n       \"Index Cond\": \"((\\\"Address\\\")::text = '1 Infinite Loop, Cupertino, California'::text)\"+\n     }                                                                                       +\n   }                                                                                         +\n ]\n```\n\n----------------------------------------\n\nTITLE: Basic Chat Stream API Definition\nDESCRIPTION: Function signature for the chat streaming API that accepts task configuration, input messages array, and optional arguments.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform_stream.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.transform_stream(\n    task JSONB,\n    inputs ARRAY[]::JSONB,\n    args JSONB\n)\n```\n\n----------------------------------------\n\nTITLE: Using GGML Quantized GPT-2 Model in PostgresML\nDESCRIPTION: This example demonstrates how to use a GGML quantized GPT-2 model from Hugging Face in PostgresML. It uses the pgml.transform function with specific task parameters for the GGML quantized model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"marella/gpt-2-ggml\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Filtering Documents in Korvus Collections\nDESCRIPTION: This snippet shows how to filter documents in a Korvus collection using the 'filter' key. It demonstrates filtering documents based on a specific condition, in this case, where the 'id' equals 'document_one'.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/collections.md#2025-04-19_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst documents = await collection.get_documents({\n  limit: 10,\n  filter: {\n    id: {\n      $eq: \"document_one\"\n    }\n  }\n})\n```\n\nLANGUAGE: python\nCODE:\n```\ndocuments = await collection.get_documents(\n    {\n        \"limit\": 100,\n        \"filter\": {\n            \"id\": {\"$eq\": \"document_one\"},\n        },\n    }\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet documents = collection\n    .get_documents(Some(\n        serde_json::json!({\n            \"limit\": 100,\n            \"filter\": {\n                \"id\": {\"$eq\": \"document_one\"},\n            }\n        })\n        .into(),\n    ))\n    .await?;\n```\n\nLANGUAGE: c\nCODE:\n```\nunsigned long r_size = 0;\nchar** documents = korvus_collectionc_get_documents(collection, \"{\\\"limit\\\": 100, \\\"filter\\\": {\\\"id\\\": {\\\"$eq\\\": \\\"document_one\\\"}}}\", &r_size);\n```\n\n----------------------------------------\n\nTITLE: Querying Similarity Using L2 Normalized Dot Product in PostgreSQL\nDESCRIPTION: This snippet shows how to query for similarity between documents using L2 normalized dot product, which is equivalent to cosine similarity. It uses a CTE to normalize the vectors and then calculates the dot product between all pairs of documents.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-normalization.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH normalized_vectors AS (\n   SELECT id, pgml.normalize_l2(embedding) AS norm_vector\n   FROM documents\n)\nSELECT a.id, b.id, pgml.dot_product(a.norm_vector, b.norm_vector)\nFROM normalized_vectors a, normalized_vectors b\nWHERE a.id <> b.id;\n```\n\n----------------------------------------\n\nTITLE: Creating Customer Embeddings from Movie Review Data\nDESCRIPTION: Aggregates movie review embeddings to create customer embeddings. The query joins the reviews table with the movies table, groups by customer_id, and uses pgml.sum() to create a vector representation of each customer's movie preferences.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/personalize-embedding-results-with-application-data-in-your-database.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE customers AS\nSELECT\n  customer_id AS id,\n  count(*) AS total_reviews,\n  avg(star_rating) AS star_rating_avg,\n  pgml.sum(movies.review_embedding_e5_large)::vector(1024) AS movie_embedding_e5_large\nFROM pgml.amazon_us_reviews\nJOIN movies\n  ON movies.id = amazon_us_reviews.product_id\nGROUP BY customer_id;\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Automatically Generated Embeddings in PostgreSQL\nDESCRIPTION: Example of creating a table that automatically generates and stores embeddings using generated columns. When quotes are inserted, embeddings are created and stored as vectors.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.embed.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE star_wars_quotes (\n  quote TEXT NOT NULL,\n  embedding vector(384) GENERATED ALWAYS AS (\n    pgml.embed('intfloat/e5-small-v2', quote, '{\"prompt\": \"passage: \"}')\n  ) STORED\n);\n\nINSERT INTO star_wars_quotes (quote)\nVALUES\n    ('I find your lack of faith disturbing'),\n    ('I''ve got a bad feeling about this.'),\n    ('Do or do not, there is no try.');\n```\n\n----------------------------------------\n\nTITLE: Generating Document Embeddings with Instructor-XL Model in PostgreSQL\nDESCRIPTION: This snippet shows how to generate an embedding for a Wikipedia document using the hkunlp/instructor-xl model with an instruction for document retrieval.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_11\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed(\n    transformer => 'hkunlp/instructor-xl',\n    text => 'Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.',\n    kwargs => '{\"instruction\": \"Represent the Wikipedia document for retrieval:\"}'\n);\n```\n\n----------------------------------------\n\nTITLE: Real-time Text Classification with PostgresML\nDESCRIPTION: Example of using pgml.transform for real-time text classification predictions using a fine-tuned model from Hugging Face Hub.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n  task   => '{\n    \"task\": \"text-classification\",\n    \"model\": \"santiadavani/imdb_review_sentiement\"\n  }'::JSONB,\n  inputs => ARRAY[\n    'I would not give this movie a rating, its not worthy. I watched it only because I am a Pfieffer fan. ',\n    'This movie was sooooooo good! It was hilarious! There are so many jokes that you can just watch the'\n  ]\n);\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings from Text in PostgreSQL\nDESCRIPTION: Example of generating an embedding from a text string using the E5 Small v2 model. The query prompt is included as a parameter in the kwargs JSONB object.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.embed.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed(\n  'intfloat/e5-small-v2',\n  'No, that''s not true, that''s impossible.',\n  '{\"prompt\": \"query: \"}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Financial Sentiment Analysis with FinBERT in PostgresML\nDESCRIPTION: This snippet demonstrates using FinBERT, a financial-specific sentiment analysis model, in PostgresML. It shows how to specify the model and interpret its output for financial text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-classification.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-classification\", \n      \"model\": \"ProsusAI/finbert\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Stocks rallied and the British pound gained.', \n        'Stocks making the biggest moves midday: Nvidia, Palantir and more'\n    ]\n) AS market_sentiment;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"label\": \"positive\", \"score\": 0.8983612656593323}, \n    {\"label\": \"neutral\", \"score\": 0.8062630891799927}\n]\n```\n\n----------------------------------------\n\nTITLE: Aggregating Movie Reviews with PostgresML in PostgreSQL\nDESCRIPTION: This query creates a new 'movies' table by aggregating review data, including vector embeddings. It uses SQL aggregates and PostgresML's vector sum function to generate statistics and representative embeddings for each movie.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE movies AS\nSELECT\n  product_id AS id,\n  product_title AS title,\n  product_parent AS parent,\n  product_category AS category,\n  count(*) AS total_reviews,\n  avg(star_rating) AS star_rating_avg,\n  pgml.sum(review_embedding_e5_large)::vector(1024) AS review_embedding_e5_large\nFROM pgml.amazon_us_reviews\nGROUP BY product_id, product_title, product_parent, product_category;\n```\n\n----------------------------------------\n\nTITLE: Text Generation with GGML Model on CPU\nDESCRIPTION: Using GGML-quantized GPT2 model for text generation specifically on CPU, showing improved performance with quantization.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"marella/gpt-2-ggml\",\n      \"device\": \"cpu\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Database Tables for Notes and Daily Summaries in PostgreSQL\nDESCRIPTION: This snippet creates two tables: 'notes' for storing individual entries with sentiment scores, and 'days' for daily summaries and overall sentiment scores.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/sentiment-analysis-using-express-js-and-postgresml.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst notes = await connection.execute(`\n  CREATE TABLE IF NOT EXISTS notes ( \n    id BIGSERIAL PRIMARY KEY, \n    note VARCHAR, \n    score FLOAT, \n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n  );`\n)\n\nconst day = await connection.execute(`\n  CREATE TABLE IF NOT EXISTS days ( \n    id BIGSERIAL PRIMARY KEY, \n    summary VARCHAR, \n    score FLOAT, \n    created_at DATE NOT NULL UNIQUE DEFAULT DATE(NOW())\n  );`\n)\n```\n\n----------------------------------------\n\nTITLE: Testing Database Connectivity with dblink in PostgreSQL\nDESCRIPTION: This SQL snippet demonstrates how to use the dblink extension to test connectivity between PostgresML and the primary database. It connects to the production database and executes a simple SELECT query.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/logical-replication/README.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n\tdblink(\n\t\t'postgres://user:password@your-production-db.amazonaws.com:5432/production_db',\n\t\t'SELECT 1 AS one'\n) AS t1(one integer);\n```\n\n----------------------------------------\n\nTITLE: Creating IVFFlat Index for Vector Data in PostgreSQL\nDESCRIPTION: This SQL snippet demonstrates how to create an IVFFlat index on a vector column using pgvector. The index is created on the 'embedding' column of the 'items' table with 100 lists.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/indexing-w-pgvector.md#2025-04-19_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX ON items USING ivfflat (embedding vector_l2_ops) WITH (lists = 100);\n```\n\n----------------------------------------\n\nTITLE: Querying vector similarity with HNSW index in PostgreSQL\nDESCRIPTION: This query uses the newly created HNSW index to perform semantic similarity search on the Amazon movie reviews dataset. It demonstrates improved performance and result quality compared to the IVFFlat index.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/speeding-up-vector-recall-5x-with-hnsw.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'query: Best 1980''s scifi movie'\n  )::vector(1024) AS embedding\n)\n\nSELECT\n  id,\n  1 - (\n    review_embedding_e5_large <=> (\n      SELECT embedding FROM request\n    )\n  ) AS cosine_similarity\nFROM pgml.amazon_us_reviews\nORDER BY review_embedding_e5_large <=> (SELECT embedding FROM request)\nLIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Executing Zero-shot Classification with PostgresML\nDESCRIPTION: SQL query that performs zero-shot classification using the facebook/bart-large-mnli model. The query classifies a given text input against multiple candidate labels to determine the most likely categories, even for classes not seen during training.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/zero-shot-classification.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    inputs => ARRAY[\n        'I have a problem with my iphone that needs to be resolved asap!!'\n    ],\n    task => '{\n                \"task\": \"zero-shot-classification\", \n                \"model\": \"facebook/bart-large-mnli\"\n             }'::JSONB,\n    args => '{\n                \"candidate_labels\": [\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"]\n             }'::JSONB\n) AS zero_shot;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\n        \"labels\": [\"urgent\", \"phone\", \"computer\", \"not urgent\", \"tablet\"], \n        \"scores\": [0.503635, 0.47879, 0.012600, 0.002655, 0.002308], \n        \"sequence\": \"I have a problem with my iphone that needs to be resolved asap!!\"\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Using bfloat16 Precision with Falcon-7B in PostgresML\nDESCRIPTION: This snippet demonstrates how to use bfloat16 precision with the Falcon-7B Instruct model using pgml.transform. It specifies torch_dtype to reduce memory usage while maintaining model quality.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n        \"model\": \"tiiuae/falcon-7b-instruct\",\n        \"device_map\": \"auto\",\n        \"torch_dtype\": \"bfloat16\",\n        \"trust_remote_code\": true\n     }'::JSONB,\n     args => '{\n        \"max_new_tokens\": 100\n     }'::JSONB,\n     inputs => ARRAY[\n        'Complete the story: Once upon a time,'\n     ]\n) AS result;\n```\n\n----------------------------------------\n\nTITLE: Initializing PostgresML Collection and Imports\nDESCRIPTION: Imports necessary libraries and initializes a Collection object for vector search operations. Sets up environment and console for logging.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pgml import Collection, Model, Splitter, Pipeline\nfrom datasets import load_dataset\nfrom time import time\nfrom dotenv import load_dotenv\nfrom rich.console import Console\nimport asyncio\n\nasync def main():\n    load_dotenv()\n    console = Console()\n\n    # Initialize collection\n    collection = Collection(\"quora_collection\")\n```\n\n----------------------------------------\n\nTITLE: Inner Product Query in PostgreSQL\nDESCRIPTION: Optimized PostgreSQL query for calculating inner product using pgml.dot_product function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH query AS (\n    SELECT vector\n    FROM test_data\n    LIMIT 1\n)\nSELECT id, pgml.dot_product(query.vector, test_data.vector)\nFROM test_data, query\nORDER BY dot_product;\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Index for Movies in PostgreSQL\nDESCRIPTION: This query creates a vector index on the 'movies' table using the IVF-Flat algorithm. It indexes the 'review_embedding_e5_large' column for efficient similarity searches, with 300 lists for approximately 300,000 vectors.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX CONCURRENTLY\n  index_movies_on_review_embedding_e5_large\nON movies\nUSING ivfflat (review_embedding_e5_large vector_cosine_ops)\nWITH (lists = 300);\n```\n\n----------------------------------------\n\nTITLE: Creating Table with Automatic Embeddings in PostgreSQL\nDESCRIPTION: This snippet creates a table 'documents_with_embeddings' with an automatically generated embedding column using pgml.embed() and pgml.normalize_l2() functions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents_with_embeddings (\nid SERIAL PRIMARY KEY,\nbody TEXT,\nembedding FLOAT[] GENERATED ALWAYS AS (pgml.normalize_l2(pgml.embed('intfloat/e5-small-v2', body))) STORED\n);\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with pgml.embed()\nDESCRIPTION: Updates the embedding column by generating embeddings from the address column using the Alibaba-NLP model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/vector-database.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nUPDATE usa_house_prices\nSET embedding = pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    address\n);\n```\n\n----------------------------------------\n\nTITLE: Using GPTQ Quantized Models in PostgresML\nDESCRIPTION: Example of using a GPTQ-quantized GPT-2 model for text generation in PostgresML. GPTQ quantization typically provides better performance on NVIDIA hardware when the model fits in VRAM.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"mlabonne/gpt2-GPTQ-4bit\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Generating Classification Embeddings with Instructor-XL Model in PostgreSQL\nDESCRIPTION: This snippet shows how to generate an embedding for a financial statement using the hkunlp/instructor-xl model with a specific instruction for classification.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed(\n    transformer => 'hkunlp/instructor-xl',\n    text => 'The Federal Reserve on Wednesday raised its benchmark interest rate.',\n    kwargs => '{\"instruction\": \"Represent the Financial statement:\"}'\n);\n```\n\n----------------------------------------\n\nTITLE: Text Generation using Vicuna LLM Model in PostgresML\nDESCRIPTION: Example showing how to use PostgresML's transform function to generate text using the Vicuna-7B model. Demonstrates configuration of model parameters, GPU layers, and generation settings including max tokens and thread count.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_11\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/vicuna-7B-v1.3-GGML\",\n      \"model_type\": \"llama\",\n      \"model_file\": \"vicuna-7b-v1.3.ggmlv3.q5_K_M.bin\",\n      \"gpu_layers\": 256\n    }'::JSONB,\n    inputs => ARRAY[\n        $$A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n\nUSER: Please write an intro to a story about a woman living in New York.\nASSISTANT:$$\n    ],\n    args => '{\n      \"max_new_tokens\": 512,\n          \"threads\": 16,\n      \"stop\": [\"USER:\",\"USER\"]\n    }'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Calculating IVFFlat Index Lists\nDESCRIPTION: Calculates the optimal number of lists for IVFFlat index based on square root of dataset size.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/vector-database.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT round(sqrt(5000000)) AS lists;\n```\n\n----------------------------------------\n\nTITLE: Fill-Mask Task Result Format\nDESCRIPTION: JSON response showing the top 5 predicted words for the masked position, including confidence scores, token IDs, complete sequences, and the predicted words themselves.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fill-mask.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"score\": 0.6811484098434448,\n    \"token\": 812,\n    \"sequence\": \"Paris is the capital of France.\",\n    \"token_str\": \" capital\"\n  },\n  {\n    \"score\": 0.050908513367176056,\n    \"token\": 32357,\n    \"sequence\": \"Paris is the birthplace of France.\",\n    \"token_str\": \" birthplace\"\n  },\n  {\n    \"score\": 0.03812871500849724,\n    \"token\": 1144,\n    \"sequence\": \"Paris is the heart of France.\",\n    \"token_str\": \" heart\"\n  },\n  {\n    \"score\": 0.024047480896115303,\n    \"token\": 29778,\n    \"sequence\": \"Paris is the envy of France.\",\n    \"token_str\": \" envy\"\n  },\n  {\n    \"score\": 0.022767696529626846,\n    \"token\": 1867,\n    \"sequence\": \"Paris is the Capital of France.\",\n    \"token_str\": \" Capital\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Equivalent Python Implementation of pgml.transform()\nDESCRIPTION: This Python function mimics the behavior of pgml.transform(). It uses the transformers library to create a pipeline for the specified task and model, then processes the input.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\n\ndef transform(task, call, inputs):\n    return transformers.pipeline(**task)(inputs, **call)\n\ntransform(\n    {\n        \"task\": \"text-generation\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n        \"model_type\": \"mistral\",\n        \"revision\": \"main\",\n    },\n    {\"max_new_tokens\": 100},\n    ['AI is going to change the world in the following ways:']\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Automatically Generated Embeddings in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to create a table that automatically generates and stores embeddings using pgml.embed() function. The embedding is generated from the 'body' text field using the 'intfloat/e5-small-v2' model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-normalization.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents (\n   id SERIAL PRIMARY KEY,\n   body TEXT,\n   embedding FLOAT[] GENERATED ALWAYS AS (pgml.embed('intfloat/e5-small-v2', body)) STORED\n);\n```\n\n----------------------------------------\n\nTITLE: Applying L1 Normalization to Embeddings in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to apply L1 normalization (Manhattan Norm) to embeddings using the pgml.normalize_l1() function. It selects the normalized embeddings from the documents table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-normalization.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.normalize_l1(embedding) FROM documents;\n```\n\n----------------------------------------\n\nTITLE: Displaying Table Structure of Amazon US Reviews Dataset\nDESCRIPTION: This command shows the structure of the loaded Amazon US Reviews dataset table within the pgml schema, including column names and data types.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\n\\d pgml.amazon_us_reviews\n```\n\n----------------------------------------\n\nTITLE: Creating and Populating Documents Table in PostgreSQL\nDESCRIPTION: Creates a documents table and inserts example documents including pgml.transform function examples and random test data.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/unified-rag.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents (id SERIAL PRIMARY KEY, document text NOT NULL);\n\n-- Insert a document that has some examples of pgml.transform\nINSERT INTO documents (document) VALUES ('\nHere is an example of the pgml.transform function\n\nSELECT pgml.transform(\n  task   => ''{\\n    \"task\": \"text-generation\",\\n    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\\n  }''::JSONB,\n  inputs  => ARRAY[''AI is going to''],\n  args   => ''{\\n    \"max_new_tokens\": 100\\n  }''::JSONB\n);\n\nHere is another example of the pgml.transform function\n\nSELECT pgml.transform(\n  task   => ''{\\n    \"task\": \"text-generation\",\\n    \"model\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\\n  }''::JSONB,\n  inputs  => ARRAY[''AI is going to''],\n  args   => ''{\\n    \"max_new_tokens\": 100\\n  }''::JSONB\n);\n\nHere is a third example of the pgml.transform function\n\nSELECT pgml.transform(\n  task   => ''{\\n    \"task\": \"text-generation\",\\n    \"model\": \"microsoft/Phi-3-mini-128k-instruct\"\\n  }''::JSONB,\n  inputs  => ARRAY[''AI is going to''],\n  args   => ''{\\n    \"max_new_tokens\": 100\\n  }''::JSONB\n);\n');\n\n-- Also insert some random documents\nINSERT INTO documents (document) SELECT md5(random()::text) FROM generate_series(1, 100);\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenSourceAI Client\nDESCRIPTION: Demonstrates how to initialize the OpenSourceAI client with a database URL in both JavaScript and Python\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/opensourceai.md#2025-04-19_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst korvus = require(\"korvus\");\nconst client = korvus.newOpenSourceAI(YOUR_DATABASE_URL);\n```\n\nLANGUAGE: python\nCODE:\n```\nimport korvus\nclient = korvus.OpenSourceAI(YOUR_DATABASE_URL)\n```\n\n----------------------------------------\n\nTITLE: Finding a Customer with Star Wars Preferences Using Vector Embeddings in PostgreSQL\nDESCRIPTION: This query finds a customer whose embedding is semantically similar to a love for Star Wars movies, particularly Empire Strikes Back. It uses the pgml.embed function to create an embedding from the query text and compares it with customer embeddings using cosine similarity.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/personalize-embedding-results-with-application-data-in-your-database.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'query: I love all Star Wars, but Empire Strikes Back is particularly amazing'\n  )::vector(1024) AS embedding\n)\n\nSELECT\n  id,\n  total_reviews,\n  star_rating_avg,\n  1 - (\n    movie_embedding_e5_large <=> (SELECT embedding FROM request)\n  ) AS cosine_similarity\nFROM customers\nORDER BY cosine_similarity DESC\nLIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Training Different Clustering Algorithms in PostgresML\nDESCRIPTION: Examples of training various clustering algorithms including Affinity Propagation, BIRCH, K-Means, and Mini-Batch K-Means with different hyperparameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/clustering.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Handwritten Digit Clusters', algorithm => 'affinity_propagation');\nSELECT * FROM pgml.train('Handwritten Digit Clusters', algorithm => 'birch', hyperparams => '{\"n_clusters\": 10}');\nSELECT * FROM pgml.train('Handwritten Digit Clusters', algorithm => 'kmeans', hyperparams => '{\"n_clusters\": 10}');\nSELECT * FROM pgml.train('Handwritten Digit Clusters', algorithm => 'mini_batch_kmeans', hyperparams => '{\n```\n\n----------------------------------------\n\nTITLE: Vector Averaging Aggregation in PostgreSQL\nDESCRIPTION: Calculates the element-wise mean of embedding vectors by dividing the sum by the count of vectors in each group.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-aggregation.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT id, pgml.divide(pgml.sum(embedding), count(*)) AS avg\nFROM documents\nGROUP BY id;\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Search Query\nDESCRIPTION: Executes a vector-based search query on the collection, retrieving the top 5 most relevant results. Prints search results and archives the collection.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/README.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    # Query\n    query = \"Who won 20 grammy awards?\"\n    results = await collection.query().vector_recall(query, pipeline).limit(5).fetch_all()\n    console.print(results)\n    # Archive collection\n    await collection.archive()\n```\n\n----------------------------------------\n\nTITLE: Querying Chunks Table in PostgreSQL\nDESCRIPTION: Displays the first 10 rows from the chunks table to verify correct document splitting.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/unified-rag.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM chunks limit 10;\n```\n\n----------------------------------------\n\nTITLE: Customized HNSW Index Configuration\nDESCRIPTION: Shows how to customize the HNSW index parameters for optimizing vector search performance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/pipelines.md#2025-04-19_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst pipeline = korvus.newPipeline(\"test_pipeline\", {\n  body: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"Alibaba-NLP/gte-base-en-v1.5\",\n      hnsw: {\n        m: 100,\n        ef_construction: 200\n      }\n    },\n  },\n});\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"test_pipeline\",\n    {\n        \"body\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n                \"hnsw\": {\"m\": 100, \"ef_construction\": 200},\n            },\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Specific Quantization File\nDESCRIPTION: Example showing how to use a specific quantization file (8-bit) with the MPT-7B Storywriter model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/MPT-7B-Storywriter-GGML\",\n      \"model_file\": \"mpt-7b-storywriter.ggmlv3.q8_0.bin\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Specific Quantization File\nDESCRIPTION: Demonstrates how to use a specific quantization file (8-bit) with the MPT model for text generation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/MPT-7B-Storywriter-GGML\",\n      \"model_file\": \"mpt-7b-storywriter.ggmlv3.q8_0.bin\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Creating IVFFlat Index\nDESCRIPTION: Creates an IVFFlat index for vector similarity search with cosine distance operator.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/vector-database.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX ON usa_house_prices\nUSING ivfflat(embedding vector_cosine_ops)\nWITH (lists = 71);\n```\n\n----------------------------------------\n\nTITLE: Creating Pipeline with Default Model and Splitter\nDESCRIPTION: Constructs a pipeline using default model and splitter configurations. Adds the pipeline to the collection for automatic document processing and embedding generation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    # Create a pipeline using the default model and splitter\n    model = Model()\n    splitter = Splitter()\n    pipeline = Pipeline(\"quorav1\", model, splitter)\n    await collection.add_pipeline(pipeline)\n```\n\n----------------------------------------\n\nTITLE: Querying Individual Movie Reviews in PostgreSQL\nDESCRIPTION: This query retrieves the review body for a specific movie title from the 'amazon_us_reviews' table. It's used to investigate the content of reviews for movies that appear in search results.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT review_body\nFROM pgml.amazon_us_reviews\nWHERE product_title = 'Big Trouble in Little China [UMD for PSP]';\n```\n\n----------------------------------------\n\nTITLE: Querying PostgreSQL Schema for Amazon Reviews Table\nDESCRIPTION: Displays the schema of the pgml.amazon_us_reviews table containing customer reviews data, showing columns like customer_id, product_id, star_rating, and review text fields.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/personalize-embedding-results-with-application-data-in-your-database.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\n\\d pgml.amazon_us_reviews\n```\n\n----------------------------------------\n\nTITLE: Basic Text Translation with PostgresML\nDESCRIPTION: Demonstrates how to perform English to French translation using PostgresML's transform function with default text-to-text generation model. The query translates the phrase 'I'm very happy' to French.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-to-text-generation.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n        \"task\" : \"text2text-generation\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'translate from English to French: I''m very happy'\n    ]\n) AS answer;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"generated_text\": \"Je suis trs heureux\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with CPU Device in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to generate embeddings using a specific model and forcing CPU usage. It selects review bodies from a table and generates embeddings for each, limited to 1000 rows.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    reviqew_body,\n    pgml.embed(\n        'Alibaba-NLP/gte-base-en-v1.5',\n        'passage: ' || review_body,\n        '{\"device\": \"cpu\"}'\n    ) AS embedding\nFROM pgml.amazon_us_reviews\nLIMIT 1000;\n```\n\n----------------------------------------\n\nTITLE: Implementing Digit Clustering with PostgresML\nDESCRIPTION: Example showing how to load the digits dataset, create an unlabeled view for clustering, train a clustering model, and make predictions. Uses the sklearn digits dataset to demonstrate clustering with a single array feature column.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/clustering.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.load_dataset('digits');\n\n-- create an unlabeled table of the images for unsupervised learning\nCREATE VIEW pgml.digit_vectors AS\nSELECT image FROM pgml.digits;\n\n-- view the dataset\nSELECT left(image::text, 40) || ',...}' FROM pgml.digit_vectors LIMIT 10;\n\n-- train a simple model to cluster the data\nSELECT * FROM pgml.train('Handwritten Digit Clusters', 'clustering', 'pgml.digit_vectors', hyperparams => '{\"n_clusters\": 10}');\n\n-- check out the predictions\nSELECT target, pgml.predict('Handwritten Digit Clusters', image) AS prediction\nFROM pgml.digits\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Real-world Example of Text Chunking from a Documents Table\nDESCRIPTION: A practical example demonstrating how to chunk text content from a database table column using the recursive_character splitter.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.chunk.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.chunk('recursive_character', content) FROM documents;\n```\n\n----------------------------------------\n\nTITLE: Accessing Nested JSON Values in PostgreSQL\nDESCRIPTION: This SQL query shows how to access nested values in a JSON document using the '#>>' operator. It retrieves the first element of the 'values' array for documents where 'hello' is 'world'.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/documents.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    document #>> '{values, 0}'\nFROM documents\nWHERE\n    document @> '{\"hello\": \"world\"}';\n```\n\n----------------------------------------\n\nTITLE: Implementing Keyset Pagination in Korvus Collections\nDESCRIPTION: This snippet demonstrates how to implement keyset pagination when retrieving documents from a Korvus collection. It uses the 'limit' and 'last_row_id' parameters to control the pagination.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/collections.md#2025-04-19_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst documents = await collection.get_documents({ limit: 100, last_row_id: 10 })\n```\n\nLANGUAGE: python\nCODE:\n```\ndocuments = await collection.get_documents({ \"limit\": 100, \"last_row_id\": 10 })\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet documents = collection\n    .get_documents(Some(serde_json::json!({\"limit\": 100, \"last_row_id\": 10}).into()))\n    .await?;\n```\n\nLANGUAGE: c\nCODE:\n```\nunsigned long r_size = 0;\nchar** documents = korvus_collectionc_get_documents(collection, \"{\\\"limit\\\": 100, \\\"last_row_id\\\": 10}\", &r_size);\n```\n\n----------------------------------------\n\nTITLE: Basic Keyword Search Query in PostgreSQL\nDESCRIPTION: Demonstrates basic keyword search using to_tsvector and to_tsquery functions with English language configuration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * \nFROM documents\nWHERE to_tsvector('english', body) @@ to_tsquery('english', 'second');\n```\n\n----------------------------------------\n\nTITLE: Pipeline Collection Integration\nDESCRIPTION: Examples of adding pipelines to collections and creating pipeline instances without schema.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/pipelines.md#2025-04-19_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nawait collection.add_pipeline(pipeline)\nconst pipeline = korvus.newPipeline(\"test_pipeline\")\n```\n\nLANGUAGE: python\nCODE:\n```\nawait collection.add_pipeline(pipeline)\npipeline = Pipeline(\"test_pipeline\")\n```\n\nLANGUAGE: rust\nCODE:\n```\ncollection.add_pipeline(&mut pipeline).await?;\nlet mut pipeline = Pipeline::new(\"test_pipeline\", None)?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nkorvus_collectionc_add_pipeline(collection, pipeline);\nPipelineC * pipeline = korvus_pipelinec_new(\"test_pipeline\",  NULL);\n```\n\n----------------------------------------\n\nTITLE: Using the pgml.chunk Function in PostgreSQL\nDESCRIPTION: SQL function to split text into manageable chunks using a specified splitter, which is typically done before embedding in RAG workflows.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/README.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.chunk(\n    splitter TEXT,    -- splitter name\n    text TEXT,        -- text to embed\n    kwargs JSON       -- optional arguments (see below)\n)\n```\n\n----------------------------------------\n\nTITLE: CPU-based Text Generation with GGML Model\nDESCRIPTION: Uses a quantized GGML version of GPT2 specifically for CPU execution, showing better performance compared to standard models on CPU.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"marella/gpt-2-ggml\",\n      \"device\": \"cpu\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Inserting Document Data for Vector Embedding\nDESCRIPTION: Demonstrates how to insert text data into the documents table, where embeddings are automatically generated for each record.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-aggregation.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO documents (body)\nVALUES -- embedding vectors are automatically generated\n    ('Example text data'),\n    ('Another example document'),\n    ('Some other thing');\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Derive Macro for Multi-Language Method Generation in Rust\nDESCRIPTION: A high-level function that handles the custom derive process for generating Python/JavaScript bindings. It parses input methods, destructures them, converts signatures, and reconstructs them in the target language format, ultimately generating a Rust implementation block with PyMethods attribute.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_11\n\nLANGUAGE: rust\nCODE:\n```\nfn do_custom_derive(input: proc_macro::TokenStream) -> proc_macro::TokenStream {\n     let parsed_methods = parse_methods(input);\n     let mut methods = Vec::new();\n     for method in parsed_methods {\n          // Destructure Method\n          let destructured = destructure(method);\n          // Translate Signature\n          let signature = convert_signature(&destructured);\n          // Restructure Method \n          let method = create_method(&destructured, &signature);\n          methods.push(method);\n     }\n     // This is the actual Rust impl block we are generating\n    proc_macro::TokenStream::from(quote! {\n        #[pymethods]\n        impl DatabasePython {\n            #(#methods)*\n        }\n    })\n}\n```\n\n----------------------------------------\n\nTITLE: Training LightGBM Model with GPU Support in PostgresML\nDESCRIPTION: This snippet shows how to train a LightGBM model using GPU support in PostgresML. It sets the 'device' parameter to 'cuda' to enable GPU acceleration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/gpu-support.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.train(\n    'GPU project', \n    algorithm => 'lightgbm', \n    hyperparams => '{\"device\" : \"cuda\"}'\n);\n```\n\n----------------------------------------\n\nTITLE: Creating a Documents Table in PostgreSQL\nDESCRIPTION: Creates a table named 'documents' with columns for id, title, and body. The id is set as a primary key using BIGSERIAL for automatic incrementing.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents (\n  id BIGSERIAL PRIMARY KEY,\n  title TEXT,\n  body TEXT\n);\n```\n\n----------------------------------------\n\nTITLE: Custom Model Text Analysis with PostgresML\nDESCRIPTION: Shows how to use a specific model (bigscience/T0) for text analysis task. The query analyzes the semantic usage of the word 'table' in two different sentences using a custom text-to-text generation model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-to-text-generation.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n        \"task\" : \"text2text-generation\",\n        \"model\" : \"bigscience/T0\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Is the word ''table'' used in the same meaning in the two previous sentences? Sentence A: you can leave the books on the table over there. Sentence B: the tables in this book are very hard to read.'\n\n    ]\n) AS answer;\n```\n\n----------------------------------------\n\nTITLE: PostgresML Model Prediction Example\nDESCRIPTION: Example of using a trained model to make predictions using PostgresML's API.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-moving-to-rust-for-our-2.0-release.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.predict('Handwritten Digit Classifier', image)\nFROM pgml.digits;\n```\n\n----------------------------------------\n\nTITLE: Note on Architecture Components\nDESCRIPTION: Lists of components used in both PostgresML and Python architectures for the performance comparison. PostgresML uses PostgreSQL with PostgresML v2.0 and pgbench, while Python setup uses Flask/Gunicorn, CSV files, Redis, and Apache Bench.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-8-40x-faster-than-python-http-microservices.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nPostgresML:\\n1. A PostgreSQL server with PostgresML v2.0\\n2. pgbench SQL client\\n\\nPython:\\n1. A Flask/Gunicorn server accepting and returning JSON\\n2. CSV file with the training data\\n3. Redis feature store with the inference dataset, serialized with JSON\\n4. ab HTTP client\n```\n\n----------------------------------------\n\nTITLE: Creating Documents Table in PostgreSQL\nDESCRIPTION: Creates a basic table structure for storing documents with auto-incrementing ID, title, and body fields.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents (\n  id BIGSERIAL PRIMARY KEY,\n  title TEXT,\n  body TEXT\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Collection and Pipeline in Python SDK\nDESCRIPTION: Shows how to create a collection and pipeline using the PostgresML Python SDK. The pipeline configures text splitting and semantic search with the same GTE embedding model as the JavaScript example.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/the-1.0-sdk-is-here.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create Collection and Pipeline\ncollection = Collection(\"my_collection\")\npipeline = Pipeline(\n    \"my_pipeline\",\n    {\n        \"text\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n            },\n        },\n    },\n)\n\n# Upsert a document\ndocuments = [{\"id\": \"document_one\", \"text\": \"Here is some hidden value 1000\"}]\nawait collection.upsert_documents(documents)\n\n# Search over our collection\nresults = await collection.vector_search(\n    {\n        \"query\": {\n            \"fields\": {\n                \"text\": {\"query\": \"What is the hidden value?\"},\n            },\n        },\n        \"limit\": 5,\n    },\n    pipeline,\n)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Creating HNSW Index\nDESCRIPTION: Creates an HNSW index for more efficient vector similarity search.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/vector-database.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX ON usa_house_prices\nUSING hnsw(embedding vector_cosine_ops);\n```\n\n----------------------------------------\n\nTITLE: Creating Training and Test Views in PostgreSQL ML\nDESCRIPTION: SQL commands to create separate views for training (80%) and testing (20%) data splits. This enables proper model evaluation with a holdout test set using the PostgreSQL ML extension.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_13\n\nLANGUAGE: postgresql\nCODE:\n```\n-- Create a view for training data (e.g., 80% of the shuffled records)\nCREATE VIEW pgml.fingpt_sentiment_train_view AS\nSELECT *\nFROM pgml.fingpt_sentiment_shuffled_view\nLIMIT (SELECT COUNT(*) * 0.8 FROM pgml.fingpt_sentiment_shuffled_view);\n\n-- Create a view for test data (remaining 20% of the shuffled records)\nCREATE VIEW pgml.fingpt_sentiment_test_view AS\nSELECT *\nFROM pgml.fingpt_sentiment_shuffled_view\nOFFSET (SELECT COUNT(*) * 0.8 FROM pgml.fingpt_sentiment_shuffled_view);\n```\n\n----------------------------------------\n\nTITLE: Enabling a Pipeline in Korvus\nDESCRIPTION: This snippet shows how to re-enable a disabled pipeline in a Korvus collection. Enabling a pipeline causes it to automatically run on all documents it may have missed while disabled.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/pipelines.md#2025-04-19_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst pipeline = korvus.newPipeline(\"test_pipeline\")\nconst collection = korvus.newCollection(\"test_collection\")\nawait collection.enable_pipeline(pipeline)\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\"test_pipeline\")\ncollection = Collection(\"test_collection\")\nawait collection.enable_pipeline(pipeline)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut collection = Collection::new(\"test_collection\", None)?;\nlet mut pipeline = Pipeline::new(\"test_pipeline\", None)?;\ncollection.enable_pipeline(&mut pipeline).await?;\n```\n\nLANGUAGE: c\nCODE:\n```\nCollectionC * collection = korvus_collectionc_new(\"test_collection\", NULL);\nPipelineC * pipeline = korvus_pipelinec_new(\"test_pipeline\",  NULL);\nkorvus_collectionc_enable_pipeline(collection, pipeline);\n```\n\n----------------------------------------\n\nTITLE: Predicting with Preprocessed Model in PostgresML\nDESCRIPTION: Example demonstrating how to make predictions using a model trained with preprocessors using a Postgres tuple containing multiple data types.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/data-pre-processing.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.predict('preprocessed_model', ('jan', 'nimbus', 0.5, 7));\n```\n\n----------------------------------------\n\nTITLE: Applying PCA Decomposition to Embeddings in PostgresML\nDESCRIPTION: This snippet demonstrates how to apply the trained PCA model to reduce the dimensionality of embeddings. It uses the 'pgml.decompose' function to transform the original embeddings into their 3-component representations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/dimensionality-reduction.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.decompose('Embedding Components', embedding) AS pca\nFROM just_embeddings\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: PostgresML Decompose Usage Example\nDESCRIPTION: Example usage of pgml.decompose() function showing how to decompose a 3-dimensional vector using a previously trained PCA model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.decompose.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.decompose('My PCA', ARRAY[0.1, 2.0, 5.0]);\n```\n\n----------------------------------------\n\nTITLE: Compiling and running PostgresML dashboard using Cargo in Bash\nDESCRIPTION: This command compiles and runs the PostgresML dashboard using Cargo, the Rust package manager and build system. After compilation, the dashboard will be accessible at localhost:8000.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncargo run\n```\n\n----------------------------------------\n\nTITLE: PostgresML Decompose Function Signature\nDESCRIPTION: API signature for pgml.decompose() showing required parameters: project_name (TEXT) and vector (REAL[]). The function performs matrix decomposition on input vectors to reduce dimensionality.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.decompose.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.decompose(\n    project_name TEXT, -- project name\n    vector REAL[]      -- features to decompose\n)\n```\n\n----------------------------------------\n\nTITLE: Rolling Back Model Deployment in PostgreSQL\nDESCRIPTION: This example demonstrates how to rollback to the previously deployed model. The 'rollback' strategy is used to revert to the model that was active before the current deployment.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.deploy.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.deploy(\n\t'Handwritten Digit Image Classifier',\n\tstrategy => 'rollback'\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Project Name in YAML\nDESCRIPTION: This snippet shows how to set the name and version of a dbt project in the dbt_project.yml file. It demonstrates naming conventions and version specification for a PostgresML workflow project.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'pgml_flow'\nversion: '1.0.0'\n```\n\n----------------------------------------\n\nTITLE: Exploring Class Distribution in PostgreSQL ML\nDESCRIPTION: SQL query to analyze the distribution of sentiment classes in the dataset. This helps understand potential class imbalances before training the sentiment analysis model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_12\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECTpgml=# SELECT\n    output,\n    COUNT(*) AS class_count\nFROM pgml.fingpt_sentiment_shuffled_view\nGROUP BY output\nORDER BY output;\n\n       output        | class_count\n---------------------+-------------\n mildly negative     |        2108\n mildly positive     |        2548\n moderately negative |        2972\n moderately positive |        6163\n negative            |       11749\n neutral             |       29215\n positive            |       21588\n strong negative     |         218\n strong positive     |         211\n```\n\n----------------------------------------\n\nTITLE: Creating a Dedicated Load Average Table\nDESCRIPTION: This SQL statement creates a table specifically for storing load average metrics with separate columns for 1-minute, 5-minute, and 15-minute averages.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pg-stat-sysinfo-a-postgres-extension-for-querying-system-statistics.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE load_average (\n    at          timestamptz NOT NULL DEFAULT now(),\n    \"1m\"        float4 NOT NULL,\n    \"5m\"        float4 NOT NULL,\n    \"15m\"       float4 NOT NULL\n);\n```\n\n----------------------------------------\n\nTITLE: Performing Sentiment Analysis with PostgresML\nDESCRIPTION: This snippet demonstrates how to use PostgresML for sentiment analysis on text inputs. It uses the default text classification model to categorize text as positive or negative.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/text-classification.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task   => 'text-classification',\n    inputs => ARRAY[\n        'I love how amazingly simple ML has become!', \n        'I hate doing mundane and thankless tasks. '\n    ]\n) AS positivity;\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"label\": \"POSITIVE\", \"score\": 0.9995759129524232}, \n    {\"label\": \"NEGATIVE\", \"score\": 0.9903519749641418}\n]\n```\n\n----------------------------------------\n\nTITLE: Performing Sentiment Analysis on Note Entry using PostgresML\nDESCRIPTION: This SQL query uses PostgresML's pgml.transform function to perform sentiment analysis on a new note entry. It simplifies the sentiment score to 1 (positive), 0 (neutral), or -1 (negative) and inserts the result into the notes table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/sentiment-analysis-using-express-js-and-postgresml.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH note AS (\n  SELECT pgml.transform(\n    inputs => ARRAY['${req.body.note}'],\n    task => '{\"task\": \"text-classification\", \"model\": \"finiteautomata/bertweet-base-sentiment-analysis\"}'::JSONB\n  ) AS market_sentiment\n), \n\nscore AS (\n  SELECT \n    CASE \n      WHEN (SELECT market_sentiment FROM note)[0]::JSONB ->> 'label' = 'POS' THEN 1\n      WHEN (SELECT market_sentiment FROM note)[0]::JSONB ->> 'label' = 'NEG' THEN -1\n      ELSE 0\n    END AS score\n)\n\nINSERT INTO notes (note, score) VALUES ('${req.body.note}', (SELECT score FROM score));\n```\n\n----------------------------------------\n\nTITLE: Creating Default Collections\nDESCRIPTION: Creates a new collection using the default KORVUS_DATABASE_URL environment variable.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/collections.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nconst collection = korvus.newCollection(\"test_collection\")\n```\n\nLANGUAGE: python\nCODE:\n```\ncollection = Collection(\"test_collection\")\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut collection = Collection::new(\"test_collection\", None)?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nCollectionC * collection = korvus_collectionc_new(\"test_collection\", NULL);\n```\n\n----------------------------------------\n\nTITLE: Training Classification Model\nDESCRIPTION: Trains a classification model on the digits dataset using PostgresML train function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/README.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n    'Handwritten Digit Image Classifier',\n    'classification',\n    'pgml.digits',\n    'target'\n);\n```\n\nLANGUAGE: plsql\nCODE:\n```\nINFO:  Snapshotting table \"pgml.digits\", this may take a little while...\nINFO:  Snapshot of table \"pgml.digits\" created and saved in \"pgml\".\"snapshot_1\"\nINFO:  Dataset { num_features: 64, num_labels: 1, num_rows: 1797, num_train_rows: 1348, num_test_rows: 449 }\nINFO:  Training Model { id: 1, algorithm: linear, runtime: python }\nINFO:  Hyperparameter searches: 1, cross validation folds: 1\nINFO:  Hyperparams: {}\nINFO:  Metrics: {\n  \"f1\": 0.91903764,\n  \"precision\": 0.9175061,\n  \"recall\": 0.9205743,\n  \"accuracy\": 0.9175947,\n  \"mcc\": 0.90866333,\n  \"fit_time\": 0.17586434,\n  \"score_time\": 0.01282608\n}\n              project               |      task      | algorithm | deployed\n------------------------------------+----------------+-----------+----------\n Handwritten Digit Image Classifier | classification | linear    | t\n(1 row)\n```\n\n----------------------------------------\n\nTITLE: Linear Model Classification Examples\nDESCRIPTION: Examples of training classification models using various linear classifiers including Ridge, SGD, Perceptron, and Passive Aggressive.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/classification.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'ridge');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'stochastic_gradient_descent');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'perceptron');\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'passive_aggressive');\n```\n\n----------------------------------------\n\nTITLE: Inserting Random Vectors into PostgreSQL\nDESCRIPTION: This snippet inserts 10,000 vectors, each with 1,000 random dimensions, into the 'test_data' table. The vectors are generated using PostgreSQL's 'generate_series' and 'array_agg' functions, which create an array of random float values for benchmarking purposes.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\n\"INSERT INTO test_data (vector) \\nSELECT array_agg(random()) \\nFROM generate_series(1,10000000) i \\nGROUP BY i % 10000;\"\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with GPU Support in PostgresML\nDESCRIPTION: This snippet demonstrates how to train an XGBoost model using GPU support in PostgresML. It uses the 'gpu_hist' tree method to enable GPU acceleration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/gpu-support.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.train(\n    'GPU project', \n    algorithm => 'xgboost', \n    hyperparams => '{\"tree_method\" : \"gpu_hist\"}'\n);\n```\n\n----------------------------------------\n\nTITLE: Loading BillSum Dataset in PostgreSQL\nDESCRIPTION: Loads the California test split of the BillSum dataset into PostgreSQL using pgml.load_dataset function. The dataset contains summaries of California state bills.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.load_dataset('billsum', kwargs => '{\"split\": \"ca_test\"}');\n```\n\n----------------------------------------\n\nTITLE: Chat Stream with Parameters\nDESCRIPTION: Demonstration of chat streaming with additional parameters like max_tokens, temperature, and seed.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.transform_stream.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform_stream(\n    task => '{\n        \"task\": \"conversational\",\n        \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n    }'::JSONB,\n    inputs => ARRAY[\n        '{\"role\": \"system\", \"content\": \"You are a friendly and helpful chatbot\"}'::JSONB,\n        '{\"role\": \"user\", \"content\": \"Tell me about yourself.\"}'::JSONB\n    ],\n    args => '{\n        \"max_tokens\": 10,\n        \"temperature\": 0.75,\n        \"seed\": 10\n    }'::JSONB\n) AS answer;\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Trained Model\nDESCRIPTION: SQL query to make predictions using a previously trained classification model with input feature vector.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/README.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.predict(\n    'My Classification Project',\n    ARRAY[0.1, 2.0, 5.0]\n) AS prediction;\n```\n\n----------------------------------------\n\nTITLE: Using GPTQ Quantized GPT-2 Model in PostgresML\nDESCRIPTION: This snippet shows how to use a GPTQ quantized GPT-2 model from Hugging Face in PostgresML. It uses the pgml.transform function with specific task parameters for the quantized model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"mlabonne/gpt2-GPTQ-4bit\",\n      \"model_basename\": \"gptq_model-4bit-128g\",\n      \"use_triton\": true,\n      \"use_safetensors\": true\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Initializing Firecrawl for Web Scraping in Python\nDESCRIPTION: Sets up the required libraries and initializes the Firecrawl application with an API key from environment variables. This prepares the environment for web scraping and document processing.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-firecrawl-rag-in-a-single-query.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom korvus import Collection, Pipeline\nfrom firecrawl import FirecrawlApp\nimport os\nimport time\nimport asyncio\nfrom rich import print\n\n# Initialize the FirecrawlApp with your API key\nfirecrawl = FirecrawlApp(api_key=os.environ[\"FIRECRAWL_API_KEY\"])\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for First 1000 Reviews\nDESCRIPTION: This query generates embeddings for the first 1000 reviews in the Amazon US Reviews dataset, demonstrating the efficiency of the pgml.embed function for batch processing.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    review_body,\n    pgml.embed('Alibaba-NLP/gte-base-en-v1.5', 'passage: ' || review_body) AS embedding\nFROM pgml.amazon_us_reviews\nLIMIT 1000;\n```\n\n----------------------------------------\n\nTITLE: Migrating Full PostgreSQL Database to PostgresML Using pg_dump\nDESCRIPTION: This command uses pg_dump to export data from a source PostgreSQL database and pipes it directly into psql to import into a PostgresML database. It's suitable for databases up to 10 GB in size.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/pg-dump.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npg_dump \\\n\t--no-owner \\\n\t--clean \\\n\t--no-privileges \\\n  postgres://user:password@your-production-database.amazonaws.com/production_db | \\\npsql postgres://user:password@sql.cloud.postgresml.org:6432/your_pgml_db\n```\n\n----------------------------------------\n\nTITLE: Exporting Data as CSV from Postgres Database\nDESCRIPTION: Command to export data from a Postgres database table to a CSV file. This uses the psql client with the \\copy command to export all rows from the 'users' table to a local CSV file with headers.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/copy.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npsql \\\n  postgres://user:password@your-production-db.amazonaws.com \\\n  -c \"\\copy (SELECT * FROM users) TO '~/users.csv' CSV HEADER\"\n```\n\n----------------------------------------\n\nTITLE: Viewing 384-Dimensional Vector Data in PostgresML\nDESCRIPTION: A representation of a 384-dimensional vector embedding stored in PostgresML database format. The values are floating point numbers ranging approximately from -0.13 to 0.17, typical for normalized embedding vectors used in machine learning models.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\n{-0.09234577,0.037487056,-0.03421769,-0.033738457,-0.042548284,-0.0015319627,0.042109113,0.011365055,-0.018372666,0.020417988,0.061961487,-0.022707041,0.015810987,0.03675479,0.001995532,-0.04197657,-0.034883354,0.07871886,-0.11676137,0.06141681,0.08321331,-0.03457781,-0.013248807,-0.05802344,-0.039144825,-0.015038275,0.020686107,0.08593334,-0.041029375,-0.13210341,-0.034079146,0.016687978,0.06363906,-0.05279167,0.10102262,-0.048170853,-0.014849669,0.03523273,0.024248678,0.031341534,-0.021447029,-0.05781338,0.039722513,-0.058294114,-0.035174508,-0.056844078,-0.051775914,-0.05822031,0.083022244,0.027178412,0.0032413877,0.023898097,0.023951318,0.0565093,0.036267336,0.049430914,0.027110789,0.05017207,0.058326595,0.040568575,0.014855128,0.06272174,-0.12961388,0.0998898,0.014964503,0.07735804,-0.028795758,0.026889611,-0.0613238,-0.004798127,0.009027658,0.046634953,-0.034936648,0.076499216,-0.03855506,0.08894715,-0.0019889707,0.07027481,-0.04624302,-0.048422314,-0.02444203,-0.0442959,-0.028878363,0.04586853,-0.004158767,-0.0027680802,0.029728336,-0.06130052,-0.028088963,-0.050658133,-0.024370935,-0.0030779864,0.018137587,-0.029853988,-0.06877675,-0.001238518,0.025249483,-0.0045243553,0.07250941,0.12831028,0.0077543575,0.012130527,-0.0006014347,-0.027807593,-0.011226617,-0.04837827,0.0376276,-0.058811083,0.020967057,-0.021439878,-0.0634577,-0.029189702,-0.040197153,-0.01993339,0.0899751,-0.014370172,0.0021994617,-0.0759979,-0.010541287,0.034424484,0.030067233,0.016858222,0.015223163,0.021410512,0.072372325,-0.06270684,0.09666927,0.07237114,0.09372637,-0.027058149,0.06319879,-0.03626834,-0.03539027,0.010406426,-0.08829164,-0.020550422,-0.043701466,-0.018676292,0.038060706,-0.0058152666,-0.04057362,-0.06266247,-0.026675962,-0.07610313,-0.023740835,0.06968648,-0.076157875,0.05129882,-0.053703927,-0.04906172,-0.014506706,-0.033226766,0.04197027,0.009892002,-0.019509513,0.020975547,0.015931072,0.044290986,-0.048697367,-0.022310019,-0.088599496,-0.0371257,0.037382104,0.14381507,0.07789086,-0.10580675,0.0255245,0.014246269,0.01157928,-0.069586724,0.023313843,0.02494169,-0.014511085,-0.017566541,0.0865948,-0.012115137,0.024397936,-0.049067125,0.03300015,-0.058626212,0.034921415,-0.04132337,-0.025009211,0.057668354,0.016189015,-0.04954466,-0.036778226,-0.046015732,-0.041587763,-0.03449501,-0.033505566,0.019262834,-0.018552447,0.019556912,0.01612039,0.0026575527,-0.05330489,-0.06894643,-0.04592849,-0.08485257,0.12714563,0.026810834,-0.053618323,-0.029470881,-0.04381535,0.055211045,-0.0111715235,-0.004484313,-0.02654065,-0.022317547,-0.027823675,0.0135190515,0.001530742,-0.04323672,-0.028350104,-0.07154715,-0.0024147208,0.031836234,0.03476004,0.033611998,0.038179073,-0.087631755,-0.048408568,-0.11773682,-0.019127818,0.013682835,-0.02015055,0.01888005,-0.03280704,0.0076310635,0.074330166,-0.031277154,0.056628436,0.119448215,-0.0012235055,-0.009727585,-0.05459528,0.04298459,0.054554865,-0.027898816,0.0040641865,0.08585007,-0.053415824,-0.030528797,-0.08231634,-0.069264784,-0.08337459,0.049254872,-0.021684796,0.12479715,0.053940497,-0.038884085,-0.032209005,0.035795107,0.0054665194,0.0085438965,-0.039386917,0.083624765,-0.056901276,0.022051739,0.06955752,-0.0008329906,-0.07959222,0.075660035,-0.017008293,0.015329365,-0.07439257,0.057193674,-0.06564091,0.0007063081,-0.015799401,-0.008529507,0.027204275,0.0076780985,-0.018589584,0.065267086,-0.02026929,-0.0559547,-0.035843417,-0.07237942,0.028072618,-0.048903402,-0.027478782,-0.084877744,-0.040812787,0.026713751,0.016210195,-0.039116003,0.03572044,-0.014964189,0.026315138,-0.08638934,-0.04198059,-0.02164005,0.09299754,-0.047685668,0.061317034,0.035914674,0.03533252,0.0287274,-0.033809293,-0.046841178,-0.042211317,-0.02567011,-0.048029255,0.039492987,0.04906847,0.030969618,0.0066106897,0.025528666,-0.008357054,0.04791732,-0.070402496,0.053391967,-0.06309544,0.06575766,0.06522203,0.060434356,-0.047547556,-0.13597175,-0.048658505,0.009734684,-0.016258504,-0.034227647,0.05382081,0.001330341,0.011890187,-0.047945525,-0.031132223,0.0010349775,0.030007072,0.12059559,-0.060273632,-0.010099646,0.055261053,0.053757478,-0.045518342,-0.041972063,-0.08315036,0.049884394,0.037543204,0.17598632,-0.0027433096,0.015989233,0.017486975,0.0059954696,-0.022668751,0.05677827,0.029728843,0.0011321013,-0.051546678,0.1113402,0.017779723,0.050953783,0.10342974,0.04067395,0.054890294,0.017487328,-0.020321153,0.062171113,0.07234749,-0.06777497,-0.03888628,0.08744684,0.032227095,-0.04398878,-0.049698275,-0.0018518695,-0.015967874,-0.0415869,-0.022655524,0.03596353,0.07130526,0.056296617,-0.06720573,-0.092787154,0.021057911,0.015628621,-0.04396636,-0.0063872878,-0.0127499355,0.01633339,-0.0006204544,0.0438727}\n```\n\n----------------------------------------\n\nTITLE: Loading Amazon US Reviews Dataset into PostgresML\nDESCRIPTION: This snippet demonstrates how to use PostgresML's convenience function to load the DVD subset of the Amazon US Reviews dataset from HuggingFace directly into the database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT *\nFROM pgml.load_dataset('amazon_us_reviews', 'Video_DVD_v1_00');\n```\n\n----------------------------------------\n\nTITLE: PostgresML Sentiment Analysis - Initial Run\nDESCRIPTION: First-time execution of sentiment analysis using PostgresML's transform function. Downloads and caches the model from HuggingFace.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/mindsdb-vs-postgresml.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    inputs => ARRAY[\n        'I am so excited to benchmark deep learning models in SQL. I can not wait to see the results!'\n    ],\n    task   => '{\n        \"task\": \"text-classification\", \n        \"model\": \"cardiffnlp/twitter-roberta-base-sentiment\"\n    }'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Restarting Model Training in PostgresML\nDESCRIPTION: Example of using pgml.tune to restart training from a previously trained model with adjusted hyperparameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_10\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.tune(\n    'imdb_review_sentiement',\n    task => 'text-classification',\n    relation_name => 'pgml.imdb_train_view',\n    model_name => 'santiadavani/imdb_review_sentiement',\n    test_size => 0.2,\n    test_sampling => 'last',\n    hyperparams => '{\n        \"training_args\": {\n            \"learning_rate\": 2e-5,\n            \"per_device_train_batch_size\": 16,\n            \"per_device_eval_batch_size\": 16,\n            \"num_train_epochs\": 1,\n            \"weight_decay\": 0.01,\n            \"hub_token\": \"YOUR_HUB_TOKEN\",\n            \"push_to_hub\": true\n        },\n        \"dataset_args\": { \"text_column\": \"text\", \"class_column\": \"class\" }\n    }'\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Partitioned Reviews Table\nDESCRIPTION: Creates a parent table and child partitions for Amazon reviews with vector embeddings using hash partitioning.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/partitioning.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE amazon_reviews_with_embedding (\n    review_body TEXT,\n    review_embedding_e5_large VECTOR(1024)\n) PARTITION BY HASH(review_body);\n\nCREATE TABLE amazon_reviews_with_embedding_1\nPARTITION OF amazon_reviews_with_embedding\nFOR VALUES WITH (modulus 3, remainder 0);\n\nCREATE TABLE amazon_reviews_with_embedding_2\nPARTITION OF amazon_reviews_with_embedding\nFOR VALUES WITH (modulus 3, remainder 1);\n\nCREATE TABLE amazon_reviews_with_embedding_3\nPARTITION OF amazon_reviews_with_embedding\nFOR VALUES WITH (modulus 3, remainder 2);\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost with Grid Search in PostgresML\nDESCRIPTION: Example of using grid search to optimize XGBoost hyperparameters for a handwritten digit classifier. The search explores different combinations of max_depth and n_estimators parameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/hyperparameter-search.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n    'Handwritten Digit Image Classifier', \n    algorithm => 'xgboost', \n    search => 'grid', \n    search_params => '{\n        \"max_depth\": [1, 2, 3, 4, 5, 6], \n        \"n_estimators\": [20, 40, 80, 160]\n    }'\n);\n```\n\n----------------------------------------\n\nTITLE: Weighted Vector Averaging in PostgreSQL\nDESCRIPTION: Implements weighted averaging of vectors where weights are based on document IDs, demonstrating how to apply importance factors to vector aggregation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-aggregation.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT id, pgml.divide(pgml.sum(pgml.multiply(embedding, id)), count(*)) AS id_weighted_avg\nFROM documents\nGROUP BY id;\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with a Trained PostgresML Model\nDESCRIPTION: SQL query to generate predictions using a previously trained 'flights' model in PostgresML. The query passes an array of flight features as input and limits the number of predictions based on a batch size parameter (1, 5, or 20).\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/scaling-postgresml-to-1-million-requests-per-second.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.predict(\n\t'flights',\n\tARRAY[\n\t\tyear,\n\t\tquarter,\n\t\tmonth,\n\t\tdistance,\n\t\tdayofweek,\n\t\tdayofmonth,\n\t\tflight_number_operating_airline,\n\t\toriginairportid,\n\t\tdestairportid,\n\t\tflight_number_marketing_airline,\n\t\tdeparture\n\t]\n) AS prediction\nFROM flights_mat_3 LIMIT :limit;\n```\n\n----------------------------------------\n\nTITLE: Querying BillSum Dataset in PostgreSQL\nDESCRIPTION: Retrieves a single record from the loaded BillSum dataset to view the data structure including text, summary, and title fields.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_11\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.billsum LIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Displaying Probability Distribution Result\nDESCRIPTION: Shows the probability distribution output for sentiment analysis, with array containing probabilities for negative (0.0627) and positive (0.9373) sentiments.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\n                sentiment\n-------------------------------------------\n[0.06266672909259796, 0.9373332858085632]\n(1 row)\n\nTime: 18.101 ms\n```\n\n----------------------------------------\n\nTITLE: Deploying Specific Algorithm Model in PostgreSQL\nDESCRIPTION: This example shows how to deploy a model for a specific algorithm (SVM) within a project. It uses the 'best_score' strategy but restricts the selection to only SVM models.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.deploy.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.deploy(\n    project_name => 'Handwritten Digit Image Classifier', \n    strategy => 'best_score', \n    algorithm => 'svm'\n);\n```\n\n----------------------------------------\n\nTITLE: Batch Updating Embeddings in PostgreSQL\nDESCRIPTION: This snippet provides a PL/pgSQL function to update embeddings in batches, processing 10 rows at a time to avoid long-running queries.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_16\n\nLANGUAGE: postgresql\nCODE:\n```\nDO $$\nBEGIN\n    FOR i in 1..(SELECT max(id) FROM pgml.amazon_us_reviews) by 10 LOOP\n        BEGIN RAISE NOTICE 'updating % to %', i, i + 10; END;\n\n        UPDATE pgml.amazon_us_reviews\n        SET review_embedding_e5_large = pgml.embed(\n                'Alibaba-NLP/gte-base-en-v1.5',\n                'passage: ' || review_body\n            )\n        WHERE id BETWEEN i AND i + 10\n            AND review_embedding_e5_large IS NULL;\n\n        COMMIT;\n    END LOOP;\nEND;\n$$;\n```\n\n----------------------------------------\n\nTITLE: Running Async Main Function\nDESCRIPTION: Runs the main async function using asyncio runtime to execute the vector search workflow.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/README.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Gaussian Process Classification Example\nDESCRIPTION: Example of training a classification model using Gaussian Process Classifier with specific hyperparameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/classification.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Handwritten Digits', algorithm => 'gaussian_process', hyperparams => '{\"max_iter_predict\": 100, \"warm_start\": true}');\n```\n\n----------------------------------------\n\nTITLE: Importing CSV Data into PostgresML\nDESCRIPTION: Command to import CSV data into a PostgresML table. This uses the psql client with the \\copy command to import data from a local CSV file with headers into the specified table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/copy.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npsql \\\n postgres://user:password@sql.cloud.postgresml.org/your_pgml_database \\\n -c \"\\copy your_table FROM '~/your_table.csv' CSV HEADER\"\n```\n\n----------------------------------------\n\nTITLE: Creating Customer Embeddings Table in PostgreSQL\nDESCRIPTION: Aggregates movie review data to create a customers table with embeddings, review counts, and average ratings for each customer.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/personalization.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE customers AS\nSELECT\n  customer_id AS id,\n  count(*) AS total_reviews,\n  avg(star_rating) AS star_rating_avg,\n  pgml.sum(movies.review_embedding_e5_large)::vector(1024) AS movie_embedding_e5_large\nFROM pgml.amazon_us_reviews\nJOIN movies\n  ON movies.id = amazon_us_reviews.product_id\nGROUP BY customer_id;\n```\n\n----------------------------------------\n\nTITLE: Basic PostgresML Prediction Function Signature\nDESCRIPTION: The basic function signature for making predictions using PostgresML. Takes a project name and feature array as input parameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/README.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nselect pgml.predict(\n    project_name TEXT,\n    features REAL[]\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Shuffling Dataset in PostgresML\nDESCRIPTION: SQL commands to load a sentiment analysis dataset and create a shuffled view for training.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_11\n\nLANGUAGE: postgresql\nCODE:\n```\n-- Load the dataset\nSELECT pgml.load_dataset('FinGPT/fingpt-sentiment-train');\n\n-- Create a shuffled view\nCREATE VIEW pgml.fingpt_sentiment_shuffled_view AS\nSELECT * FROM pgml.\"FinGPT/fingpt-sentiment-train\" ORDER BY RANDOM();\n```\n\n----------------------------------------\n\nTITLE: Creating a GIN Index for Full-Text Search in PostgreSQL\nDESCRIPTION: Creates a Generalized Inverted Index (GIN) on the generated tsvector column to optimize full-text search performance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX documents_title_and_body_text_index \nON documents \nUSING GIN (title_and_body_text);\n```\n\n----------------------------------------\n\nTITLE: Creating a Table with Automatically Generated Embeddings in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to create a table that automatically generates and stores text embeddings using the 'intfloat/e5-small-v2' model. The embeddings are normalized using L2 normalization.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/dimensionality-reduction.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents_with_embeddings\n(\n    id        serial PRIMARY KEY,\n    body      text,\n    embedding float[] GENERATED ALWAYS AS (pgml.normalize_l2(pgml.embed('intfloat/e5-small-v2', body))) STORED\n);\n```\n\n----------------------------------------\n\nTITLE: Dot Product Implementation in PL/pgSQL\nDESCRIPTION: PL/pgSQL function implementation for computing dot product between two float arrays using procedural language syntax.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-moving-to-rust-for-our-2.0-release.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE OR REPLACE FUNCTION dot_product_plpgsql(a FLOAT4[], b FLOAT4[])\n\tRETURNS FLOAT4\n\tLANGUAGE plpgsql IMMUTABLE STRICT PARALLEL SAFE AS\n$$\n\tBEGIN\n\t\tRETURN SUM(multiplied.values)\n\t\tFROM (SELECT UNNEST(a) * UNNEST(b) AS values) AS multiplied;\n\tEND\n$$;\n```\n\n----------------------------------------\n\nTITLE: OpenAI Integration Pipeline\nDESCRIPTION: Demonstrates how to configure a pipeline to use OpenAI's embedding model instead of open-source alternatives.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/pipelines.md#2025-04-19_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst pipeline = korvus.newPipeline(\"test_pipeline\", {\n  body: {\n    splitter: { model: \"recursive_character\" },\n    semantic_search: {\n      model: \"text-embedding-ada-002\",\n      source: \"openai\"\n    },\n  },\n});\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"test_pipeline\",\n    {\n        \"body\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\"model\": \"text-embedding-ada-002\", \"source\": \"openai\"},\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Single Prediction with PostgresML\nDESCRIPTION: Performs inference on a single data point using the trained model with the pgml.predict function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/static/images/gym/quick_start.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.predict(\n\t'My First Project',\n\tARRAY[\n\t\t0.06, -- age\n\t\t0.05, -- sex\n\t\t0.05, -- bmi\n\t\t-0.0056, -- bp\n\t\t0.012191, -- s1\n\t\t-0.043401, -- s2\n\t\t0.034309, -- s3\n\t\t-0.031938, -- s4\n\t\t-0.061988, --s5\n\t\t-0.031988 -- s6\n\t]\n) AS prediction;\n```\n\n----------------------------------------\n\nTITLE: Querying Training Logs in PostgreSQL\nDESCRIPTION: SQL query to retrieve training metrics from pgml.logs table showing progression of loss values across training epochs.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npgml: SELECT logs->>'epoch' AS epoch, logs->>'step' AS step, logs->>'loss' AS loss FROM pgml.logs WHERE model_id = 993 AND jsonb_exists(logs, 'loss');\n```\n\n----------------------------------------\n\nTITLE: Batch Predictions in PostgresML\nDESCRIPTION: Performs predictions on multiple data points from the diabetes dataset and compares them with actual target values.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/static/images/gym/quick_start.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n\tpgml.predict('My First Project 2', ARRAY[\n\t\tage, sex, bmi, bp, s1, s3, s3, s4, s5, s6\n\t]),\n    target\nFROM pgml.diabetes LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Training Regression Models with Kernel-based Algorithms in PostgresML\nDESCRIPTION: Examples of how to train regression models using kernel_ridge and gaussian_process algorithms in PostgresML. These examples use the 'Diabetes Progression' dataset and demonstrate the SQL syntax for invoking these algorithms.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/regression.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'kernel_ridge');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'gaussian_process');\n```\n\n----------------------------------------\n\nTITLE: Implementing Database Struct with Neon for JavaScript\nDESCRIPTION: Shows how to use Neon to create a JavaScript-compatible version of the Database struct, including async functionality and proper JavaScript class structure.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_3\n\nLANGUAGE: rust\nCODE:\n```\nuse neon::prelude::*;\n\nstruct Database{\n     connection_string: String\n}\n\nimpl Database {\n    pub fn new<'a>(mut cx: FunctionContext<'a>) -> JsResult<'a, JsObject> {\n        // The actual connection process has been removed\n        let arg0 = cx.argument::<JsString>(0usize as i32)?;\n        let arg0 = <String>::from_js_type(&mut cx, arg0)?;\n        let x = Self {\n            connection_string: arg0\n        };\n        x.into_js_result(&mut cx)\n    }\n\n    pub fn vector_search<'a>(mut cx: FunctionContext<'a>) -> JsResult<'a, JsPromise> {\n        let this = cx.this();\n        let s: neon::handle::Handle<\n            neon::types::JsBox<std::cell::RefCell<DatabaseJavascript>>,\n        > = this.get(&mut cx, \"s\")?;\n        let wrapped = (*s).deref().borrow();\n        let wrapped = wrapped.wrapped.clone();\n        let arg0 = cx.argument::<neon::types::JsString>(0)?;\n        let arg0 = <String>::from_js_type(&mut cx, arg0)?;\n        let arg1 = cx.argument::<JsNumber>(1);\n        let arg1 = <i64>::from_js_type(&mut cx, arg1);\n        let arg2 = cx.argument::<JsNumber>(2);\n        let arg2 = <i64>::from_js_type(&mut cx, arg2);\n        let channel = cx.channel();\n        let (deferred, promise) = cx.promise();\n        deferred\n            .try_settle_with(\n                &channel,\n                move |mut cx| {\n                    // Do some async vector search\n                    result.into_js_result(&mut cx)\n                },\n            )\n            .expect(\"Error sending js\");\n        Ok(promise)\n    }\n\n    fn into_js_result<'a, 'b, 'c: 'b, C: Context<'c>>(self, cx: &mut C) -> JsResult<'b, Self::Output> {\n        let obj = cx.empty_object();\n        let s = cx.boxed(std::cell::RefCell::new(self));\n        obj.set(cx, \"s\", s)?;\n        let f: Handle<JsFunction> = JsFunction::new(\n            cx,\n            Database::new,\n        )?;\n        obj.set(cx, \"new\", f)?;\n        let f: Handle<JsFunction> = JsFunction::new(\n            cx,\n            Database::vector_search,\n        )?;\n        Ok(obj)\n    }\n}\n\nimpl neon::types::Finalize for Database {}\n\n/// A JavaScript module implemented in Rust.\n#[main]\nfn main(mut cx: ModuleContext) -> NeonResult<()> {\n    cx.export_function(\"newDatabase\", Database::new)?;\n    Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Data into PostgresML Docker Stack\nDESCRIPTION: Shows how to import data into a PostgresML Docker stack using psql. This is useful when working with a local development environment.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/distributed-training.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npsql \\\n    postgres://postgres@localhost:5433/pgml_development \\\n    -f dump.sql\n```\n\n----------------------------------------\n\nTITLE: Inserting Sample Text Data for Embedding Generation in PostgreSQL\nDESCRIPTION: This code inserts sample text data into the 'documents_with_embeddings' table. The embeddings for each text entry are automatically generated based on the table definition.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/dimensionality-reduction.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO documents_with_embeddings (body)\nVALUES -- embedding vectors are automatically generated\n       ('Example text data'),\n       ('Another example document'),\n       ('Some other thing'),\n       ('We need a few more documents'),\n       ('At least as many documents as dimensions in the reduction'),\n       ('Which normally isn''t a problem'),\n       ('Unless you''re typing out a bunch of demo data');\n```\n\n----------------------------------------\n\nTITLE: Querying with MATERIALIZED VIEWS in PostgreSQL\nDESCRIPTION: Using PostgreSQL's MATERIALIZED VIEWS to efficiently compute and cache statistics from normalized tracking tables, preventing write amplification when updating related statistics.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nMATERIALIZED VIEWS\n```\n\n----------------------------------------\n\nTITLE: Importing Database Schema to PostgresML Using psql\nDESCRIPTION: This bash command uses psql to import the previously exported schema into the PostgresML database, creating the necessary tables for replication.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/logical-replication/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npsql \\\n\tpostgres://user:password@db.cloud.postgresml.org:6432/your_postgresml_database \\\n\t-f schema.sql\n```\n\n----------------------------------------\n\nTITLE: Creating HNSW Index on Partitioned Vector Data in PostgreSQL\nDESCRIPTION: This snippet shows how to create an HNSW index on a partitioned table containing vector embeddings. It sets the maintenance_work_mem and creates an index using the vector_cosine_ops operator class.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/partitioning.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSET maintenance_work_mem TO '2GB';\n\nCREATE INDEX ON\n    amazon_reviews_with_embedding_1\nUSING hnsw(review_embedding_e5_large vector_cosine_ops);\n```\n\n----------------------------------------\n\nTITLE: Loading and Training Basic Classification Model\nDESCRIPTION: Basic example showing how to load the sklearn digits dataset, view the data, train a classification model, and make predictions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/classification.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\n-- load the sklearn digits dataset\nSELECT pgml.load_dataset('digits');\n\n-- view the dataset\nSELECT left(image::text, 40) || ',...}', target FROM pgml.digits LIMIT 10;\n\n-- train a simple model to classify the data\nSELECT * FROM pgml.train('Handwritten Digits', 'classification', 'pgml.digits', 'target');\n\n-- check out the predictions\nSELECT target, pgml.predict('Handwritten Digits', image) AS prediction\nFROM pgml.digits \nLIMIT 10;\n\n-- view raw class probabilities\nSELECT target, pgml.predict_proba('Handwritten Digits', image) AS prediction\nFROM pgml.digits\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Cloud Provider Abstraction in Rust for PostgresML\nDESCRIPTION: This Rust code demonstrates how PostgresML handles cloud provider differences with a simple pattern matching approach. It abstracts the deployment logic to launch either EC2 instances on AWS or VMs on Azure based on the cloud environment.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-going-multicloud.md#2025-04-19_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nmatch cloud {\n    Cloud::Aws => launch_ec2_instance().await,\n    Cloud::Azure => launch_azure_vm().await,\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Translation View in PostgreSQL\nDESCRIPTION: Creates a database view to transform JSON language pairs into separate columns for model training.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE OR REPLACE VIEW kde4_en_to_es AS\nSELECT translation->>'en' AS \"en\", translation->>'es' AS \"es\"\nFROM pgml.kde4\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Performing Document Search with Korvus\nDESCRIPTION: Demonstrates how to perform a document search using Korvus. The search includes full-text and semantic search on the 'abstract' field, semantic search on the 'body' field, and applies a filter. It limits the results to 10 documents.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/document-search.md#2025-04-19_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.search(\n  {\n    query: {\n      full_text_search: { abstract: { query: \"What is the best database?\", boost: 1.2 } },\n      semantic_search: {\n        abstract: {\n          query: \"What is the best database?\", boost: 2.0,\n        },\n        body: {\n          query: \"What is the best database?\", boost: 1.25, parameters: {\n            prompt:\n              \"Represent this sentence for searching relevant passages: \",\n          }\n        },\n      },\n      filter: { user_id: { $eq: 1 } },\n    },\n    limit: 10\n  },\n  pipeline,\n);\n```\n\nLANGUAGE: python\nCODE:\n```\nresults = await collection.search(\n    {\n        \"query\": {\n            \"full_text_search\": {\n                \"abstract\": {\"query\": \"What is the best database?\", \"boost\": 1.2}\n            },\n            \"semantic_search\": {\n                \"abstract\": {\n                    \"query\": \"What is the best database?\",\n                    \"boost\": 2.0,\n                },\n                \"body\": {\n                    \"query\": \"What is the best database?\",\n                    \"boost\": 1.25,\n                    \"parameters\": {\n                        \"prompt\": \"Represent this sentence for searching relevant passages: \",\n                    },\n                },\n            },\n            \"filter\": {\"user_id\": {\"$eq\": 1}},\n        },\n        \"limit\": 10,\n    },\n    pipeline,\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet results = collection\n    .search(serde_json::json!({\n        \"query\": {\n            \"full_text_search\": {\n                \"abstract\": {\"query\": \"What is the best database?\", \"boost\": 1.2}\n            },\n            \"semantic_search\": {\n                \"abstract\": {\n                    \"query\": \"What is the best database?\",\n                    \"boost\": 2.0,\n                },\n                \"body\": {\n                    \"query\": \"What is the best database?\",\n                    \"boost\": 1.25,\n                    \"parameters\": {\n                        \"prompt\": \"Represent this sentence for searching relevant passages: \",\n                    },\n                },\n            },\n            \"filter\": {\"user_id\": {\"$eq\": 1}},\n        },\n        \"limit\": 10,\n    }).into(), &mut pipeline)\n    .await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nchar * results = korvus_collectionc_search(collection, \"\\\n     \\\"query\\\": {\\\n        \\\"full_text_search\\\": {\\\n            \\\"abstract\\\": {\\\"query\\\": \\\"What is the best database?\\\", \\\"boost\\\": 1.2}\\\n        },\\\n        \\\"semantic_search\\\": {\\\n            \\\"abstract\\\": {\\\n                \\\"query\\\": \\\"What is the best database?\\\",\\\n                \\\"boost\\\": 2.0\\\n            },\\\n            \\\"body\\\": {\\\n                \\\"query\\\": \\\"What is the best database?\\\",\\\n                \\\"boost\\\": 1.25,\\\n                \\\"parameters\\\": {\\\n                    \\\"prompt\\\": \\\"Represent this sentence for searching relevant passages: \\\"\\\n                }\\\n            }\\\n        },\\\n        \\\"filter\\\": {\\\"user_id\\\": {\\\"$eq\\\": 1}}\\\n    },\\\n    \\\"limit\\\": 10\\\n\", pipeline);\n```\n\n----------------------------------------\n\nTITLE: Executing Python Code Within Rust Using PyO3\nDESCRIPTION: This snippet demonstrates how to embed a Python module in Rust code using PyO3. It includes the Python code at build time, initializes the Python interpreter with the Gil (Global Interpreter Lock), and compiles the Python module for execution within the Rust environment.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres.md#2025-04-19_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nuse pyo3::prelude::*;\nuse pyo3::types::PyTuple;\n\npub fn sklearn_train() {\n\t// Copy the code into the Rust library at build time.\n\tlet module = include_str!(concat!(\n\t    env!(\"CARGO_MANIFEST_DIR\"),\n\t    \"/src/bindings/sklearn.py\"\n\t));\n\n\tlet estimator = Python::with_gil(|py| -> Py<PyAny> {\n\t\t// Compile Python\n\t\tlet module = PyModule::from_code(py, module, \"\", \"\").unwrap();\n\n        // ... train the model\n\t});\n}\n```\n\n----------------------------------------\n\nTITLE: Reindexing IVFFlat Index\nDESCRIPTION: Shows how to rebuild an IVFFlat index concurrently when dataset changes significantly.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/vector-database.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nREINDEX INDEX CONCURRENTLY usa_house_prices_embedding_idx;\n```\n\n----------------------------------------\n\nTITLE: Text Generation with LLaMA Model\nDESCRIPTION: Uses the LLaMA model for text generation, specifying model_type parameter due to incomplete config.json.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/robin-7B-v2-GGML\",\n      \"model_type\": \"llama\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Chat Completion Response Format\nDESCRIPTION: Example JSON response structure from a chat completion request, showing the formatted output including choices, message content, and usage statistics.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/opensourceai.md#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"content\": \"Ahoy, me hearty! As your friendly chatbot, I'd like to inform ye that a human cannot eat a helicopter in one sitting. Helicopters are not edible, as they are not food items. They are flying machines used for transportation, search and rescue operations, and other purposes. A human can only eat food items, such as fruits, vegetables, meat, and other edible items. I hope this helps, me hearties!\",\n        \"role\": \"assistant\"\n      }\n    }\n  ],\n  \"created\": 1701291672,\n  \"id\": \"abf042d2-9159-49cb-9fd3-eef16feb246c\",\n  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": \"eecec9d4-c28b-5a27-f90b-66c3fb6cee46\",\n  \"usage\": {\n    \"completion_tokens\": 0,\n    \"prompt_tokens\": 0,\n    \"total_tokens\": 0\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Deployed Models\nDESCRIPTION: Query to check which models are currently deployed in PostgresML.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/README.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.deployed_models;\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model in Rust\nDESCRIPTION: Complete example of training an XGBoost model in Rust, including data loading, parameter configuration, model training, and model saving. Demonstrates efficient implementation with type safety and compile-time checks.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/oxidizing-machine-learning.md#2025-04-19_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nuse xgboost::{parameters, Booster, DMatrix};\n\nfn main() {\n    // Data is read directly into the C++ data structure\n    let train = DMatrix::load(\"train.txt\").unwrap();\n    let test = DMatrix::load(\"test.txt\").unwrap();\n\n    // Task (regression or classification)\n    let learning_params = parameters::learning::LearningTaskParametersBuilder::default()\n        .objective(parameters::learning::Objective::BinaryLogistic)\n        .build()\n        .unwrap();\n\n    // Tree parameters (e.g. depth)\n    let tree_params = parameters::tree::TreeBoosterParametersBuilder::default()\n        .max_depth(2)\n        .eta(1.0)\n        .build()\n        .unwrap();\n\n    // Gradient boosting parameters\n    let booster_params = parameters::BoosterParametersBuilder::default()\n        .booster_type(parameters::BoosterType::Tree(tree_params))\n        .learning_params(learning_params)\n        .build()\n        .unwrap();\n\n    // Train on train data, test accuracy on test data\n    let evaluation_sets = &[(&train, \"train\"), (&test, \"test\")];\n\n    // Final algorithm configuration\n    let params = parameters::TrainingParametersBuilder::default()\n        .dtrain(&train)\n        .boost_rounds(2) // n_estimators\n        .booster_params(booster_params)\n        .evaluation_sets(Some(evaluation_sets))\n        .build()\n        .unwrap();\n\n    // Train the model\n    let model = Booster::train(&params).unwrap();\n\n    // Save and load later in any language that has XGBoost bindings\n    model.save(\"/tmp/xgboost_model.bin\").unwrap();\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents in Korvus Collections\nDESCRIPTION: This snippet shows how to delete documents from a Korvus collection using the 'delete_documents' method. It demonstrates deleting documents based on a specific condition, in this case, where the 'id' equals 1.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/collections.md#2025-04-19_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst documents = await collection.delete_documents({\n    id: {\n      $eq: 1\n    }\n})\n```\n\nLANGUAGE: python\nCODE:\n```\ndocuments = await collection.delete_documents(\n    {\n        \"id\": {\"$eq\": 1},\n    }\n)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet documents = collection\n    .delete_documents(\n        serde_json::json!({\n            \"id\": {\n                \"$eq\": 1\n            }\n        })\n        .into(),\n    )\n    .await?;\n```\n\nLANGUAGE: c\nCODE:\n```\nkorvus_collectionc_delete_documents(collection, \"{\\\"id\\\": { \\\"$eq\\\": 1}}\");\n```\n\n----------------------------------------\n\nTITLE: Inserting Text Data into Documents Table in PostgreSQL\nDESCRIPTION: This snippet shows how to insert text data into the documents table. The embeddings for these text entries will be automatically generated based on the table definition.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-normalization.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO documents (body)\nVALUES -- embedding vectors are automatically generated\n    ('Example text data'),\n    ('Another example document'),\n    ('Some other thing');\n```\n\n----------------------------------------\n\nTITLE: Creating PostgreSQL Backup with pgBackRest\nDESCRIPTION: Command to create a full backup of the PostgreSQL database cluster using pgBackRest with the main stanza configuration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/backups.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npgbackrest backup --stanza=main\n```\n\n----------------------------------------\n\nTITLE: Verifying Embeddings in PostgreSQL\nDESCRIPTION: Queries the embeddings table to verify successful embedding generation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/unified-rag.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\n\\x auto\nSELECT * FROM embeddings LIMIT 1;\n\\x off\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Extension Creation\nDESCRIPTION: SQL commands to create and verify the PostgresML extension installation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE EXTENSION pgml;\nSELECT pgml.version();\n```\n\n----------------------------------------\n\nTITLE: Creating a GIN Index for JSONB in PostgreSQL\nDESCRIPTION: This SQL statement creates a GIN (Generalized Inverted Index) index on the 'document' column of the 'documents' table. This index improves performance for JSON containment queries.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/documents.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX ON documents USING gin(document jsonb_path_ops);\n```\n\n----------------------------------------\n\nTITLE: Upserting Documents\nDESCRIPTION: Demonstrates how to upsert documents into a collection with required ID field and optional key-value pairs.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/collections.md#2025-04-19_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst documents = [\n  {\n    id: \"document_one\",\n    title: \"Document One\",\n    text: \"document one contents...\",\n    random_key: \"here is some random data\",\n  },\n  {\n    id: \"document_two\",\n    title: \"Document Two\",\n    text: \"document two contents...\",\n    random_key: \"here is some random data\",\n  },\n];\nawait collection.upsert_documents(documents);\n```\n\nLANGUAGE: python\nCODE:\n```\ndocuments = [\n    {\n        \"id\": \"document_one\",\n        \"title\": \"Document One\",\n        \"text\": \"Here are the contents of Document 1\",\n        \"random_key\": \"here is some random data\",\n    },\n    {\n        \"id\": \"document_two\",\n        \"title\": \"Document Two\",\n        \"text\": \"Here are the contents of Document 2\",\n        \"random_key\": \"here is some random data\",\n    },\n]\nawait collection.upsert_documents(documents)\n```\n\n----------------------------------------\n\nTITLE: Querying Load Average Metrics from pg_stat_sysinfo\nDESCRIPTION: This SQL query demonstrates how to retrieve load average metrics from the pg_stat_sysinfo table, filtering by metric name and timestamp range.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pg-stat-sysinfo-a-postgres-extension-for-querying-system-statistics.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pg_stat_sysinfo\n WHERE metric = 'load_average'\n   AND at BETWEEN '2023-04-07 19:20:09.3'\n              AND '2023-04-07 19:20:11.4';\n```\n\n----------------------------------------\n\nTITLE: Training a PCA Model for Dimensionality Reduction in PostgresML\nDESCRIPTION: This code trains a PCA model using PostgresML to reduce the dimensionality of the embeddings from 384 to 3 components. It uses the 'just_embeddings' view as input data.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/dimensionality-reduction.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT *\nFROM pgml.train('Embedding Components', 'decomposition', 'just_embeddings', hyperparams => '{\"n_components\": 3}');\n```\n\n----------------------------------------\n\nTITLE: Creating Test Embeddings Table in PostgreSQL\nDESCRIPTION: Generates a table of 10,000 random embeddings with 128 dimensions stored as FLOAT4[] arrays for benchmarking purposes.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-moving-to-rust-for-our-2.0-release.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE embeddings AS\nSELECT ARRAY_AGG(random())::FLOAT4[] AS vector\nFROM generate_series(1, 1280000) i\nGROUP BY i % 10000;\n```\n\n----------------------------------------\n\nTITLE: Implementing Semantic Search Query in Django\nDESCRIPTION: Demonstrates how to perform vector similarity search using PostgresML embeddings and Django's RawSQL.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/using-postgresml-with-django-and-embedding-search.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresults = TodoItem.objects.annotate(\n    similarity=RawSQL(\n        \"pgml.embed('Alibaba-NLP/gte-base-en-v1.5', %s)::vector(768) &#x3C;=> embedding\",\n        [query],\n    )\n).order_by(\"similarity\")\n```\n\n----------------------------------------\n\nTITLE: Vector Summation Aggregation in PostgreSQL\nDESCRIPTION: Performs element-wise summation of embedding vectors grouped by document ID using the pgml.sum function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-aggregation.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT id, pgml.sum(embedding)\nFROM documents\nGROUP BY id;\n```\n\n----------------------------------------\n\nTITLE: Configuring Vector Embedding Field in Django Model\nDESCRIPTION: Defines a TodoItem model field that automatically generates and stores embeddings using PostgresML's pgml.embed() function with pgvector integration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/using-postgresml-with-django-and-embedding-search.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nembedding = models.GeneratedField(\n    expression=EmbedSmallExpression(\"description\"),\n    output_field=VectorField(dimensions=768),\n    db_persist=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Datasets in PostgresML using pgml.load_dataset()\nDESCRIPTION: The pgml.load_dataset() function is used to load datasets into PostgresML. It allows users to import data for machine learning tasks within the PostgreSQL database environment.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.load_dataset.md#2025-04-19_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\npgml.load_dataset()\n```\n\n----------------------------------------\n\nTITLE: Querying Digits Dataset\nDESCRIPTION: Examines the loaded digits dataset, showing image data arrays and target labels.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/README.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    target,\n    image\nFROM pgml.digits LIMIT 5;\n```\n\nLANGUAGE: plsql\nCODE:\n```\ntarget |                                                                                image\n-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     0 | {{0,0,5,13,9,1,0,0},{0,0,13,15,10,15,5,0},{0,3,15,2,0,11,8,0},{0,4,12,0,0,8,8,0},{0,5,8,0,0,9,8,0},{0,4,11,0,1,12,7,0},{0,2,14,5,10,12,0,0},{0,0,6,13,10,0,0,0}}\n     1 | {{0,0,0,12,13,5,0,0},{0,0,0,11,16,9,0,0},{0,0,3,15,16,6,0,0},{0,7,15,16,16,2,0,0},{0,0,1,16,16,3,0,0},{0,0,1,16,16,6,0,0},{0,0,1,16,16,6,0,0},{0,0,0,11,16,10,0,0}}\n     2 | {{0,0,0,4,15,12,0,0},{0,0,3,16,15,14,0,0},{0,0,8,13,8,16,0,0},{0,0,1,6,15,11,0,0},{0,1,8,13,15,1,0,0},{0,9,16,16,5,0,0,0},{0,3,13,16,16,11,5,0},{0,0,0,3,11,16,9,0}}\n     3 | {{0,0,7,15,13,1,0,0},{0,8,13,6,15,4,0,0},{0,2,1,13,13,0,0,0},{0,0,2,15,11,1,0,0},{0,0,0,1,12,12,1,0},{0,0,0,0,1,10,8,0},{0,0,8,4,5,14,9,0},{0,0,7,13,13,9,0,0}}\n     4 | {{0,0,0,1,11,0,0,0},{0,0,0,7,8,0,0,0},{0,0,1,13,6,2,2,0},{0,0,7,15,0,9,8,0},{0,5,16,10,0,16,6,0},{0,4,15,16,13,16,1,0},{0,0,0,3,15,10,0,0},{0,0,0,2,16,4,0,0}}\n(5 rows)\n```\n\n----------------------------------------\n\nTITLE: Predicting Sentiment Probabilities with PostgresML\nDESCRIPTION: Demonstrates using pgml.predict_proba() to get probability distribution across sentiment classes instead of binary classification.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.predict_proba('IMDB Review Sentiment', 'I love SQL')\nAS sentiment;\n```\n\n----------------------------------------\n\nTITLE: Finding Relevant Documents Using Question Answering\nDESCRIPTION: This snippet demonstrates how to load the Stanford Question Answering Dataset (SQuAD) into a PostgreSQL database and perform a vector search for documents that are relevant to a given question using embeddings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/examples/README.md#2025-04-19_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Self-Hosted Pipeline with HuggingFace Authentication Token\nDESCRIPTION: Pipeline configuration for self-hosted PostgresML instances that requires authentication to access gated HuggingFace repositories, including both token and trust_remote_code parameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/constructing-pipelines.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"v0\",\n    {\n        \"text\": {\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n                \"parameters\": {\n                    \"trust_remote_code\": True,\n                    \"token\": \"YOUR_TOKEN\"\n                }\n            }\n        }\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating AWS IAM Policy for PostgresML VPC Deployment\nDESCRIPTION: This JSON policy document defines the necessary AWS permissions required for PostgresML to manage resources in your VPC. It includes permissions for EC2 instance management, security groups, volumes, IAM roles, S3 buckets, and KMS encryption services.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/cloud/enterprise/vpc.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:RunInstances\",\n        \"ec2:TerminateInstances\",\n        \"ec2:StopInstances\",\n        \"ec2:StartInstances\",\n        \"ec2:RebootInstances\",\n        \"ec2:ModifyInstanceAttribute\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:CreateSecurityGroup\",\n        \"ec2:DeleteSecurityGroup\",\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:AuthorizeSecurityGroupEgress\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeVolumes\",\n        \"ec2:CreateTags\",\n        \"ec2:DescribeKeyPairs\",\n        \"ec2:DescribeRouteTables\",\n        \"ec2:DescribeRegions\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:DescribeSubnets\",\n        \"ec2:CreateVolume\",\n        \"ec2:DeleteVolume\",\n        \"ec2:AttachVolume\",\n        \"ec2:DetachVolume\",\n        \"ec2:ModifyVolume\",\n        \"imagebuilder:CreateImage\",\n        \"imagebuilder:CreateImagePipeline\",\n        \"iam:SimulatePrincipalPolicy\",\n        \"iam:PassRole\",\n        \"iam:GetRole\",\n        \"iam:ListRoles\",\n        \"iam:CreateRole\",\n        \"iam:CreateInstanceProfile\",\n        \"iam:CreatePolicy\",\n        \"iam:GetInstanceProfile\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:AttachRolePolicy\",\n        \"iam:AddRoleToInstanceProfile\",\n        \"s3:CreateBucket\",\n        \"s3:DeleteBucket\",\n        \"s3:PutBucketPolicy\",\n        \"s3:ListBucket\",\n        \"s3:GetBucketPolicy\",\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucketMultipartUploads\",\n        \"s3:ListMultipartUploadParts\",\n        \"s3:AbortMultipartUpload\",\n        \"s3:GetBucketLocation\",\n        \"s3:GetBucketTagging\",\n        \"s3:PutBucketTagging\",\n        \"kms:DescribeKey\",\n        \"kms:CreateGrant\",\n        \"kms:Decrypt\",\n        \"kms:ReEncryptFrom\",\n        \"kms:ReEncryptTo\",\n        \"kms:GenerateDataKey\",\n        \"kms:GenerateDataKeyPair\",\n        \"kms:GenerateDataKeyPairWithoutPlaintext\",\n        \"kms:GenerateDataKeyWithoutPlaintext\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Vector Embeddings\nDESCRIPTION: Shows how to query and display the first 5 values of an embedding vector for a specific address.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/vector-database.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    address,\n    (embedding::real[])[1:5] \nFROM usa_house_prices\nWHERE address = '1 Infinite Loop, Cupertino, California';\n```\n\n----------------------------------------\n\nTITLE: Vector Embedding Data Sample\nDESCRIPTION: A 384-dimensional vector embedding showing numerical values representing a data point in high-dimensional space, commonly used in machine learning models and similarity searches.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n{-0.06530473,0.043326367,0.027487691,-0.012605501,-0.003679171,0.0068843057,0.093755856,-0.018192727,-0.038994554,0.060702052,0.047350235,0.0015797003,-0.026038624,0.029946782,0.053223953,-0.009188536,-0.012273773,0.07512682,-0.1220027,0.024623549,0.040207546,-0.061494265,-0.0016338134,-0.096063755,-0.020626824,-0.0008177105,0.025736991,0.08205663,-0.064413406,-0.10329614,-0.050153203,0.022038238,-0.011629073,-0.03142779,0.09684598,-0.045188677,-0.032773193,0.041901052,0.032470446,0.06218501,0.00056252955,-0.03571358,0.030095506,-0.09239761,-0.020187493,-0.00932361,-0.08373726,-0.053929392,0.09724756,-0.032078817,0.02658544,0.009965162,0.07477913,0.05487153,0.023828406,0.06263976,0.06882497,0.08249143,0.062069558,0.08915651,-0.005154778,0.056259956,-0.13729677,0.08404741,0.07149277,0.04482675,-0.058625933,0.0034976404,-0.030747578,0.004520399,0.0007449915,9.660358e-05,-0.022526976,0.11449101,-0.043607008,0.026769284,0.021050733,0.05854427,-0.042627476,-0.022924222,-0.059794623,-0.037738875,-0.018500011,0.017315088,-0.00020744087,-0.0016206327,0.013337528,-0.022439854,-0.0042932644,-0.04706647,-0.06771751,-0.040391076,0.0638978,-0.031776994,0.011536817,-0.04593729,0.08626801,0.0016808647,-0.0046028513,0.13702579,0.02293593,0.043189116,-0.0073873955,-0.06097065,-0.019305069,-0.025651531,0.043129053,-0.033460874,0.03261353,-0.022361644,-0.07769732,-0.021210406,-0.020294553,-0.044899672,0.083500296,0.038056396,-0.052046232,-0.03215008,-0.028185,0.041909587,0.016012225,-0.0058236965,0.021344814,-0.037620485,0.07454872,-0.03517924,0.086520284,0.096695796,0.0937938,-0.04190071,0.072271764,-0.07022541,0.01583733,-0.0017275782,-0.05280332,-0.005904967,-0.046241984,-0.024421731,0.09988276,-0.0077029592,-0.04107849,-0.091607556,0.033811443,-0.1323201,-0.015927043,0.011014193,-0.039773338,0.033963792,-0.053305525,-0.005038948,-0.024107914,-0.0079898145,0.039604105,0.009226985,0.0010978039,-0.015565131,-0.0002796709,0.037623808,-0.059376597,0.015390821,-0.07600872,-0.008280972,0.023050148,0.0777234,0.061332665,-0.13979945,-0.009342198,0.012803744,0.049805813,-0.03578894,-0.05038778,0.048912454,0.032017626,0.015345221,0.10369494,-0.048897773,-0.054201737,-0.015793057,0.08130064,-0.064783126,0.074246705,-0.06964914,-0.025839275,0.030869238,0.06357789,-0.028754702,-0.02960897,-0.04956488,0.030501548,0.005857936,-0.023547728,0.03717584,0.0024309678,0.066338174,-0.009775384,-0.030799516,-0.028462514,-0.058787093,-0.051071096,-0.048674088,0.011397957,0.07817651,-0.03227047,0.027149512,-0.0030777291,0.061677814,0.0025318298,-0.027110869,-0.0691719,-0.033963803,-0.0648151,-0.033951994,-0.0478505,0.0016642202,-0.019602248,-0.030472266,0.015889537,-0.0009066139,0.032841947,0.021004336,-0.029254122,-0.09597239,-0.04359093,-0.15422617,-0.016366383,-0.059343938,-0.064871244,0.07659653,0.023196936,-0.021893008,0.080793895,-0.05248758,0.018764181,0.0008353451,-0.03318359,-0.04830206,-0.05518034,0.038481984,0.06544077,0.019498836,-0.054670736,0.040052623,-0.028875519,-0.047129385,-0.03614192,-0.012638911,-0.0042204396,0.013685266,-0.047130045,0.11024768,0.07135732,-0.017937008,-0.040911496,0.09008783,0.039298594,0.042975742,-0.08974752,0.08711358,-0.021977019,0.051495675,0.0140351625,-0.053809136,-0.08241595,0.04982693,-0.020355707,0.017629888,-0.039196398,0.08688628,-0.051167585,-0.029257154,0.009161573,-0.0021740724,0.027258197,0.015352816,-0.07426982,0.022452697,-0.041628033,-0.023250584,-0.051996145,-0.031867135,-0.01930267,-0.05257186,0.032619886,-0.08220233,-0.017010445,0.038414452,-0.02268424,0.007727591,0.0064041745,-0.024256933,0.0028989788,-0.06191567,-0.020444075,-0.010515549,0.08980986,-0.020033991,0.009208651,0.044014987,0.067944355,0.07915397,0.019362122,-0.010731527,-0.057449125,-0.007854527,-0.067998596,0.036500365,0.037355963,-0.0011789168,0.030410502,-0.012768641,-0.03281059,0.026916556,-0.052477527,0.042145997,-0.023683913,0.099338256,0.035008017,-0.029086927,-0.032222193,-0.14743629,-0.04350868,0.030494612,-0.013000542,0.021753347,0.023393912,0.021320568,0.0031570331,-0.06008047,-0.031103736,0.030275675,0.015258714,0.09004704,0.0033432578,-0.0045539658,0.06602429,0.072156474,-0.0613405,-0.047462273,-0.057639644,-0.008026253,0.03090332,0.12396069,0.04592149,-0.053269017,0.034282286,-0.0045666047,-0.026025562,0.004598449,0.04304216,-0.02252559,-0.040372007,0.08094969,-0.021883471,0.05903653,0.10130699,0.001840184,0.06142003,0.004450253,-0.023686321,0.014760433,0.07669066,-0.08392746,-0.028447477,0.08995419,0.028487092,-0.047503598,-0.026627144,-0.0475691,-0.069141485,-0.039571274,-0.054866526,0.04417342,0.08155949,0.065555565,-0.053984754,-0.04142323,-0.023902748,0.0066344747,-0.065118864,0.02183451,-0.06479133,0.010425607,-0.010283142,0.0940532}\n```\n\n----------------------------------------\n\nTITLE: Adding Vector Column for Embeddings in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to add a vector column to an existing table to store embeddings with 1024 dimensions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_14\n\nLANGUAGE: postgresql\nCODE:\n```\nALTER TABLE pgml.amazon_us_reviews\nADD COLUMN review_embedding_e5_large vector(1024);\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model in PostgresML\nDESCRIPTION: SQL command to train an XGBoost regression model for flight delay prediction using PostgresML\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/blog/benchmarks/python_microservices_vs_postgresml/README.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n\t'r2',\n\t'regression',\n\t'flights_delay_mat',\n\t'depdelayminutes',\n\talgorithm => 'xgboost',\n\thyperparams => '{ \"n_estimators\": 25 }'\n);\n```\n\n----------------------------------------\n\nTITLE: Generating ER Diagram in Python SDK\nDESCRIPTION: Shows how to generate an Entity-Relationship diagram for a collection in PlantUML format using the Python SDK.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/the-1.0-sdk-is-here.md#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(await collection.generate_er_diagram(pipeline))\n```\n\n----------------------------------------\n\nTITLE: Creating Range Partitioned Energy Consumption Table\nDESCRIPTION: Creates a parent table for energy consumption data partitioned by datetime range.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/partitioning.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE energy_consumption (\n    \"Datetime\" TIMESTAMPTZ,\n    \"AEP_MW\" REAL\n) PARTITION BY RANGE(\"Datetime\");\n```\n\n----------------------------------------\n\nTITLE: Insert Sample Documents\nDESCRIPTION: Inserts example text records into the documents table for embedding generation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO documents (body)\nVALUES\n    ('Example text data'),\n    ('Another example document'),\n    ('Some other thing');\n```\n\n----------------------------------------\n\nTITLE: Max Pooling Vector Aggregation in PostgreSQL\nDESCRIPTION: Performs max pooling on embedding vectors by taking the maximum absolute value for each dimension across vectors.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-aggregation.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT id, pgml.max_abs(embedding)\nFROM documents\nGROUP BY id;\n```\n\n----------------------------------------\n\nTITLE: Creating an IVFFlat Index on Embedding Column in PostgreSQL\nDESCRIPTION: This code creates an IVFFlat index on the embedding column of the 'text_and_embeddings' table to improve search performance. It uses 10 lists for clustering.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX ON text_and_embeddings\nUSING ivfflat (embedding vector_cosine_ops)\nWITH (lists = 10);\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Pipeline with Korvus in Python\nDESCRIPTION: This code snippet demonstrates how to use Korvus to set up a RAG pipeline. It includes initializing a Collection and Pipeline, upserting documents, and performing a RAG query. The example showcases Korvus' ability to handle document splitting, embedding, vector search, and text generation in a single operation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/introducing-korvus-the-all-in-one-rag-pipeline-for-postgresml.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom korvus import Collection, Pipeline\nfrom rich import print\nimport asyncio\n\n# Initialize our Collection\ncollection = Collection(\"semantic-search-demo\")\n\n# Initialize our Pipeline\n# Our Pipeline will split and embed the `text` key of documents we upsert\npipeline = Pipeline(\n    \"v1\",\n    {\n        \"text\": {\n            \"splitter\": {\"model\": \"recursive_character\"},\n            \"semantic_search\": {\n                \"model\": \"mixedbread-ai/mxbai-embed-large-v1\",\n            },\n        },\n    },\n)\n\nasync def main():\n    # Add our Pipeline to our Collection\n    await collection.add_pipeline(pipeline)\n\n    # Upsert our documents\n    documents = [\n        {\n            \"id\": \"1\",\n            \"text\": \"Korvus is incredibly fast and easy to use.\",\n        },\n        {\n            \"id\": \"2\",\n            \"text\": \"Tomatoes are incredible on burgers.\",\n        },\n    ]\n    await collection.upsert_documents(documents)\n\n    # Perform RAG\n    query = \"Is Korvus fast?\"\n    print(f\"Querying for response to: {query}\")\n    results = await collection.rag(\n        {\n            \"CONTEXT\": {\n                \"vector_search\": {\n                    \"query\": {\n                        \"fields\": {\"text\": {\"query\": query}},\n                    },\n                    \"document\": {\"keys\": [\"id\"]},\n                    \"limit\": 1,\n                },\n                \"aggregate\": {\"join\": \"\\n\"},\n            },\n            \"chat\": {\n                \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a friendly and helpful chatbot\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Given the context\\n:{{CONTEXT}}\\nAnswer the question briefly: {query}\",\n                    },\n                ],\n                \"max_tokens\": 100,\n            },\n        },\n        pipeline,\n    )\n    print(results)\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Fetching a Document by ID in PostgreSQL\nDESCRIPTION: This SQL query retrieves a document from the 'documents' table based on its ID. It demonstrates how to fetch a specific document using the primary key.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/documents.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT document FROM documents WHERE id = 1;\n```\n\n----------------------------------------\n\nTITLE: Exporting and Importing Schema for Logical Replication in PostgreSQL\nDESCRIPTION: Shows how to export the schema from a production database and import it into PostgresML. This is necessary for logical replication as it doesn't copy the schema automatically.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/distributed-training.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Dump the schema from your production DB\npg_dump \\\n    postgres://username:password@production-db.example.com/production_db \\\n    --schema-only \\\n    --no-owner > schema.sql\n\n# Import the schema in PostgresML\npsql \\\n    postgres://username:password@postgresml.example.com/postgresml_db \\\n    -f schema.sql\n```\n\n----------------------------------------\n\nTITLE: Packaging PostgresML Extension\nDESCRIPTION: This bash script packages the PostgresML extension into a .deb file using Docker.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nbash build_extension.sh\n```\n\n----------------------------------------\n\nTITLE: Defining a Rust Struct with Custom Derive Macros\nDESCRIPTION: Example Rust code showing a Database struct with custom derive macros that will automatically generate wrappers for Python and JavaScript interoperability.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_4\n\nLANGUAGE: rust\nCODE:\n```\n#[custom_derive_class]\nstruct Database{\n     connection_string: String\n}\n\n#[custom_derive_methods]\nimpl Database {\n     pub fn new(connection_string: String) -> Self {\n          // The actual connection process has been removed \n          Self {\n               connection_string\n          }\n     }\n\n     pub async fn vector_search(&self, query: String, model_id: i64, splitter_id: i64) -> String {\n          // Do some async vector search\n          result\n     }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Training and Test Views for IMDB Dataset\nDESCRIPTION: Splits the shuffled IMDB dataset into training (80%) and test (20%) views for model training and evaluation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE VIEW pgml.imdb_train_view AS\nSELECT *\nFROM pgml.imdb_shuffled_view\nLIMIT (SELECT COUNT(*) * 0.8 FROM pgml.imdb_shuffled_view);\n\nCREATE VIEW pgml.imdb_test_view AS\nSELECT *\nFROM pgml.imdb_shuffled_view\nOFFSET (SELECT COUNT(*) * 0.8 FROM pgml.imdb_shuffled_view);\n```\n\n----------------------------------------\n\nTITLE: Creating Markdown Table of Contents for PostgresML Careers\nDESCRIPTION: This markdown snippet creates a table of contents with links to various career positions within the PostgresML project. It includes links to the main README and separate files for each position.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/careers/SUMMARY.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Table of contents\n\n* [Careers](README.md)\n* [Full Stack Engineer](full-stack-engineer.md)\n* [Machine Learning Engineer](machine-learning-engineer.md)\n* [Data Scientist](data-scientist.md)\n* [Product Manager](product-manager.md)\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Supported LLM Tasks\nDESCRIPTION: A comprehensive table listing all supported natural language processing tasks in PostgresML, including their task names and descriptions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Task                                                    | Name | Description |\n|---------------------------------------------------------|-------------|----------|\n| Fill mask                | `key-mask` | Fill in the blank in a sentence. |\n| Question answering             | `question-answering` | Answer a question based on a context. |\n| Summarization                       | `summarization` | Summarize a long text. |\n| Text classification           | `text-classification` | Classify a text as positive or negative. |\n| Text generation                   | `text-generation` | Generate text based on a prompt. |\n| Text-to-text generation   | `text-to-text-generation` | Generate text based on an instruction in the prompt. |\n| Token classification         | `token-classification` | Classify tokens in a text. |\n| Translation                           | `translation` | Translate text from one language to another. |\n| Zero-shot classification | `zero-shot-classification` | Classify a text without training data. |\n| Conversational                                          | `conversational` | Engage in a conversation with the model, e.g. chatbot. |\n```\n\n----------------------------------------\n\nTITLE: Querying JSON Documents by Value in PostgreSQL\nDESCRIPTION: This SQL query demonstrates how to fetch documents based on their content using the '@>' operator. It retrieves all documents where the 'hello' key has the value 'world'.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/documents.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    id,\n    document->>'values'\nFROM documents\nWHERE\n    document @> '{\"hello\": \"world\"}';\n```\n\n----------------------------------------\n\nTITLE: Bulk Data Import Python Script\nDESCRIPTION: Python script using psycopg to bulk import data from multiple CSV files into PostgreSQL.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/README.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport psycopg\nfrom glob import glob\n\nwith psycopg.connect(\"postgres:///postgresml\") as conn:\n    cur = conn.cursor()\n\n    with cur.copy(\"COPY usa_house_prices FROM STDIN CSV\") as copy:\n        for csv_file in glob(\"*.csv\"):\n            with open(csv_file) as f:\n                next(f) # Skip header\n                for line in f:\n                    copy.write(line)\n```\n\n----------------------------------------\n\nTITLE: Implementing Inner Product in JavaScript and Python\nDESCRIPTION: Implementation of Inner (dot) product calculation between two vectors to measure similarity.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nfunction innerProduct(x, y) {\n    let result = 0;\n    for (let i = 0; i < x.length; i++) {\n        result += x[i] * y[i];\n    }\n    return result;\n}\n\nlet x = [1, 2, 3];\nlet y = [1, 2, 3];\ninnerProduct(x, y)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef inner_product(x, y):\n    return sum([x*y for x,y in zip(x,y)])    \n\nx = [1, 2, 3]\ny = [1, 2, 3]\ninner_product(x, y)\n```\n\n----------------------------------------\n\nTITLE: Configuring General Settings in PgCat TOML File\nDESCRIPTION: Example of how to set up the general section in the pgcat.toml configuration file. This includes basic settings like host, port, and worker threads.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgcat/configuration.md#2025-04-19_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[general]\nhost = \"0.0.0.0\"\nport = 5432\nworker_threads = 5\nconnect_timeout = 1000\nidle_timeout = 600000\nserver_lifetime = 3600000\n```\n\n----------------------------------------\n\nTITLE: Training ML Models with PostgresML\nDESCRIPTION: Using pgml.train and pgml.predict functions to train and deploy ML models, with support for specifying different algorithms and model IDs.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\npgml.train\npgml.predict\n```\n\n----------------------------------------\n\nTITLE: Setting Up Database Profiles\nDESCRIPTION: Database connection configuration for development and production environments in profiles.yml\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/examples/dbt/embeddings/README.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npgml_flow:\n  outputs:\n\n    dev:\n      type: postgres\n      threads: 1\n      host: 127.0.0.1\n      port: 5433\n      user: postgres\n      pass: \"\"\n      dbname: pgml_development\n      schema: <schema_name>\n    \n    prod:\n      type: postgres\n      threads: [1 or more]\n      host: [host]\n      port: [port]\n      user: [prod_username]\n      pass: [prod_password]\n      dbname: [dbname]\n      schema: [prod_schema]\n\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Database Collections\nDESCRIPTION: Creates a collection using a custom database URL instead of the default environment variable.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/collections.md#2025-04-19_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst collection = korvus.newCollection(\"test_collection\", CUSTOM_DATABASE_URL)\n```\n\nLANGUAGE: python\nCODE:\n```\ncollection = Collection(\"test_collection\", CUSTOM_DATABASE_URL)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut collection = Collection::new(\"test_collection\", Some(CUSTOM_DATABASE_URL))?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nCollectionC * collection = korvus_collectionc_new(\"test_collection\", CUSTOM_DATABASE_URL);\n```\n\n----------------------------------------\n\nTITLE: Testing PostgresML Connection - Python\nDESCRIPTION: Example code demonstrating how to establish a connection to PostgresML and create a chat completion using the Python SDK. Shows async initialization and basic usage with a LLaMA model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/getting-started/connect-your-app.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pgml\n\nasync def main():\n    client = pgml.OpenSourceAI()\n    results = client.chat_completions_create(\n        \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"How many helicopters can a human eat in one sitting?\",\n            },\n        ],\n        temperature=0.85,\n    )\n    print(results)\n```\n\n----------------------------------------\n\nTITLE: Minimal PgCat Configuration Example\nDESCRIPTION: A complete minimal configuration for PgCat with a single unsharded database. This example sets up general settings, defines a pool named 'my_database' with a single user, and configures one shard pointing to a local PostgreSQL instance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgcat/configuration.md#2025-04-19_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\n[general]\nport = 6432\nadmin_username = \"pgcat\"\nadmin_password = \"my-pony-likes-to-dance-tango\"\n\n[pools.my_database]\n\n[pools.my_database.users.0]\npool_size = 5\nusername = \"developer\"\npassword = \"very-secure-password\"\n\n[pools.my_database.shards.0]\ndatabase = \"postgresml\"\nservers = [\n    [\"127.0.0.1\", 5432, \"primary\"],\n]\n```\n\n----------------------------------------\n\nTITLE: Inserting Multiple Random Embeddings for Performance Testing\nDESCRIPTION: This snippet inserts 100,000 random embeddings into the 'text_and_embeddings' table to demonstrate performance issues with unindexed searches on large datasets.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO text_and_embeddings (text, embedding) \nSELECT\n  md5(random()::text),\n  pgml.embed(\n    'mixedbread-ai/mxbai-embed-large-v1',\n    md5(random()::text)\n  ) \nFROM generate_series(1, 100000);\n```\n\n----------------------------------------\n\nTITLE: Sampling Records from IMDB Dataset\nDESCRIPTION: Retrieves a sample of records from the shuffled IMDB dataset to inspect data structure and content.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT LEFT(text,100) AS text, class\nFROM pgml.imdb_shuffled_view\nLIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Installing pgml-chat via pip\nDESCRIPTION: Command to install the pgml-chat package using pip in a Python virtual environment.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowledge-based-chatbots-part-i.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pgml-chat\n```\n\n----------------------------------------\n\nTITLE: Create Documents Table Structure\nDESCRIPTION: Creates a table schema for storing text documents with an auto-incrementing ID and text body field.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents (\n   id SERIAL PRIMARY KEY,\n   body TEXT\n);\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML Chat Package\nDESCRIPTION: Command to install the pgml-chat package using pip package manager.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-apps/pgml-chat/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pgml-chat\n```\n\n----------------------------------------\n\nTITLE: Digit Classification Prediction Example\nDESCRIPTION: Example of making predictions on a handwritten digit classification dataset using a trained model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/README.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    target,\n    pgml.predict('Handwritten Digit Image Classifier', image) AS prediction\nFROM pgml.digits \nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Manhattan Distance Query in PostgreSQL\nDESCRIPTION: Optimized PostgreSQL query for calculating Manhattan distance using pgml.distance_l1 function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH query AS (\n    SELECT vector\n    FROM test_data\n    LIMIT 1\n)\nSELECT id, pgml.distance_l1(query.vector, test_data.vector)\nFROM test_data, query\nORDER BY distance_l1;\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset in PostgresML\nDESCRIPTION: Loads the digits dataset from scikit-learn into PostgreSQL using pgml.load_dataset function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/README.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.load_dataset('digits');\n```\n\nLANGUAGE: plsql\nCODE:\n```\npgml=# SELECT * FROM pgml.load_dataset('digits');\nNOTICE:  table \"digits\" does not exist, skipping\n table_name  | rows\n-------------+------\n pgml.digits | 1797\n(1 row)\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging in Python SDK\nDESCRIPTION: Shows how to enable debug logging in the PostgresML Python SDK to view all SQL queries being executed.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/the-1.0-sdk-is-here.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npgml.init_logger(\"DEBUG\");\n```\n\n----------------------------------------\n\nTITLE: Configuring Pool Settings in PgCat TOML File\nDESCRIPTION: Example of how to set up a connection pool in the pgcat.toml configuration file. This includes pool-specific settings like pool mode and load balancing.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgcat/configuration.md#2025-04-19_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[pools.name_of_the_pool]\npool_mode = \"transaction\"\nload_balancing_mode = \"random\"\nquery_parser_enabled = false\nquery_parser_read_write_splitting = false\nprimary_reads_enabled = false\n```\n\n----------------------------------------\n\nTITLE: Adding a Generated tsvector Column in PostgreSQL\nDESCRIPTION: Alters the 'documents' table to add a generated column that combines the title and body into a tsvector for efficient searching.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nALTER TABLE documents\nADD COLUMN title_and_body_text tsvector\nGENERATED ALWAYS AS (to_tsvector('english', title || ' ' || body )) STORED;\n```\n\n----------------------------------------\n\nTITLE: Bulk Data Import Bash Script\nDESCRIPTION: Bash script to import multiple CSV files into PostgreSQL using psql command.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash\n\nfor f in $(ls *.csv); do\n    psql postgres:///postgresml \\\n        -c \"\\copy usa_house_prices FROM '$f' CSV HEADER\"\ndone\n```\n\n----------------------------------------\n\nTITLE: Getting Documents\nDESCRIPTION: Retrieves documents from the collection with pagination support.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/collections.md#2025-04-19_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst documents = await collection.get_documents({limit: 100 })\n```\n\nLANGUAGE: python\nCODE:\n```\ndocuments = await collection.get_documents({ \"limit\": 100 })\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet documents = collection\n    .get_documents(Some(serde_json::json!({\"limit\": 100}).into()))\n    .await?;\n```\n\nLANGUAGE: cpp\nCODE:\n```\nunsigned long r_size = 0;\nchar** documents = korvus_collectionc_get_documents(collection, \"{\\\"limit\\\": 100}\", &r_size);\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Switch Kit in Python\nDESCRIPTION: This code demonstrates how to use the Korvus OpenSourceAI client to create chat completions with an open-source LLM model in Python. It shows how to initialize the client and make API calls similar to OpenAI's interface.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/introducing-the-openai-switch-kit-move-from-closed-to-open-source-ai-in-minutes.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport korvus\nclient = korvus.OpenSourceAI()\nresults = client.chat_completions_create(\n    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"How many helicopters can a human eat in one sitting?\",\n        },\n    ],\n    temperature=0.85,\n)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Connecting to PgCat with PSQL\nDESCRIPTION: Command to connect to the PgCat pooler using the psql client. This example demonstrates the connection string format, using the pool name as the database name in the connection string rather than the actual PostgreSQL database name.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgcat/configuration.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npsql postgres://developer:very-secure-password@127.0.0.1:6432/my_database\n```\n\n----------------------------------------\n\nTITLE: Loading IMDB Dataset\nDESCRIPTION: Loads the IMDB dataset for sentiment analysis fine-tuning.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.load_dataset('imdb');\n```\n\n----------------------------------------\n\nTITLE: Inserting Sample Documents in PostgreSQL\nDESCRIPTION: Inserts sample documents into the documents table with titles and body content for testing search functionality.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO documents (title, body) VALUES \n  ('This is a title', 'This is the body of the first document.'),\n  ('This is another title', 'This is the body of the second document.'),\n  ('This is the third title', 'This is the body of the third document.')\n;\n```\n\n----------------------------------------\n\nTITLE: Displaying Schema of Amazon US Reviews Table in PostgreSQL\nDESCRIPTION: Shows the structure of the pgml.amazon_us_reviews table, which contains customer reviews data including product and customer information.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/personalization.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\n\\d pgml.amazon_us_reviews\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack Environment Variables\nDESCRIPTION: Environment variables required for setting up the Slack chat interface.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nSLACK_BOT_TOKEN=<SLACK_BOT_TOKEN>\nSLACK_APP_TOKEN=<SLACK_APP_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Creating a View of Just Embeddings in PostgreSQL\nDESCRIPTION: This snippet creates a view that contains only the embedding vectors from the 'documents_with_embeddings' table, simplifying access to the embeddings for further processing.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/dimensionality-reduction.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE VIEW just_embeddings AS\nSELECT embedding\nFROM documents_with_embeddings;\n```\n\n----------------------------------------\n\nTITLE: Classification Project Prediction Example\nDESCRIPTION: Simple example of making a prediction for a classification project using a feature array.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/README.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.predict(\n    'My Classification Project', \n    ARRAY[0.1, 2.0, 5.0]\n) AS prediction;\n```\n\n----------------------------------------\n\nTITLE: Applying L2 Normalization to Embeddings in PostgreSQL\nDESCRIPTION: This snippet shows how to apply L2 normalization (Euclidean Norm) to embeddings using the pgml.normalize_l2() function. It selects the normalized embeddings from the documents table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-normalization.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.normalize_l2(embedding) FROM documents;\n```\n\n----------------------------------------\n\nTITLE: Training PCA Model with Custom Components\nDESCRIPTION: Example of training a PCA model with 10 components using the pgml.train function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/decomposition.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Handwritten Digit Clusters', algorithm => 'pca', hyperparams => '{\"n_components\": 10}');\n```\n\n----------------------------------------\n\nTITLE: Creating a Foreign Data Wrapper Server in PostgresML\nDESCRIPTION: Creates a server configuration that defines the connection to your production database. Requires the hostname, port, and database name of your production database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/foreign-data-wrappers.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE SERVER production_db\nFOREIGN DATA WRAPPER postgres_fdw\nOPTIONS (\n  host 'your-production-db.amazonaws.com',\n  port '5432'\n  dbname 'production_db'\n);\n```\n\n----------------------------------------\n\nTITLE: Basic PostgresML Usage Example\nDESCRIPTION: This SQL code demonstrates basic usage of PostgresML, including loading a dataset, training a model, and making predictions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.load_dataset('diabetes');\nSELECT * FROM pgml.train('Project name', 'regression', 'pgml.diabetes', 'target', 'xgboost');\nSELECT target, pgml.predict('Project name', ARRAY[age, sex, bmi, bp, s1, s2, s3, s4, s5, s6]) FROM pgml.diabetes LIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with PostgresML Search Ranking Model\nDESCRIPTION: Uses the trained model to predict click probability for search results based on their title and body ranks.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_11\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT \n  clicked, \n  pgml.predict('Search Ranking', array[title_rank, body_rank]) \nFROM search_result_clicks;\n```\n\n----------------------------------------\n\nTITLE: Statistical Query in PostgreSQL\nDESCRIPTION: SQL query to calculate various statistics including count, average, max, min, 75th percentile, and standard deviation of area income.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/README.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    count(*),\n    avg(\"Avg. Area Income\"),\n    max(\"Avg. Area Income\"),\n    min(\"Avg. Area Income\"),\n    percentile_cont(0.75)\n        WITHIN GROUP (ORDER BY \"Avg. Area Income\") AS percentile_75,\n    stddev(\"Avg. Area Income\")\nFROM usa_house_prices;\n```\n\n----------------------------------------\n\nTITLE: Sorting Documents in Korvus Collections\nDESCRIPTION: This snippet demonstrates how to sort documents in a Korvus collection using the 'order_by' key. It shows sorting documents by the 'id' field in descending order, along with limit and offset for pagination.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/collections.md#2025-04-19_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst documents = await collection.get_documents({\n  limit: 100,\n  offset: 10,\n  order_by: {\n    id: \"desc\"\n  }\n})\n```\n\nLANGUAGE: python\nCODE:\n```\ndocuments = await collection.get_documents({\n    \"limit\": 100,\n    \"offset\": 10,\n    \"order_by\": {\n        \"id\": \"desc\"\n    }\n})\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet documents = collection\n    .get_documents(Some(\n        serde_json::json!({\n            \"limit\": 100,\n            \"offset\": 10,\n            \"order_by\": {\n                \"id\": \"desc\"\n            }\n        })\n        .into(),\n    ))\n    .await?;\n```\n\nLANGUAGE: c\nCODE:\n```\nunsigned long r_size = 0;\nchar** documents = korvus_collectionc_get_documents(collection, \"{\\\"limit\\\": 100, \\\"offset\\\": 10, \\\"order_by\\\": {\\\"id\\\": \\\"desc\\\"}}\", &r_size);\n```\n\n----------------------------------------\n\nTITLE: Basic Document Structure Example in Python\nDESCRIPTION: A simple example document structure with ID, title, and text fields that would be processed by a Pipeline in PostgresML.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/constructing-pipelines.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nexample_document = {\n  \"id\": \"doc_001\",  # Unique identifier for the document\n  \"title\": \"Introduction to Machine Learning\",  # Document title\n  \"text\": \"Machine learning is a branch of artificial intelligence...\"  # Main content\n}\n```\n\n----------------------------------------\n\nTITLE: Running PgCat using Docker with a Custom Configuration\nDESCRIPTION: This Docker command runs the latest version of PgCat, mounting a local pgcat.toml configuration file. It assumes the configuration file is in the current working directory.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgcat/installation.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n  -v $(pwd)/pgcat.toml:/etc/pgcat/pgcat.toml \\\nghcr.io/postgresml/pgcat:latest\n```\n\n----------------------------------------\n\nTITLE: Creating PostgresML Extension in PostgreSQL\nDESCRIPTION: SQL command to create the PostgresML extension in a PostgreSQL database, along with the expected output indicating successful installation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/building-from-source.md#2025-04-19_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\npostgresml=# CREATE EXTENSION pgml;\nINFO:  Python version: 3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]\nINFO:  Scikit-learn 1.1.3, XGBoost 1.7.1, LightGBM 3.3.3, NumPy 1.23.5\nCREATE EXTENSION\n```\n\n----------------------------------------\n\nTITLE: Inserting Sample Documents into PostgreSQL Table\nDESCRIPTION: Inserts three sample documents into the 'documents' table, each with a title and body. The id is automatically generated.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO documents (title, body) VALUES \n  ('This is a title', 'This is the body of the first document.'),\n  ('This is another title', 'This is the body of the second document.'),\n  ('This is the third title', 'This is the body of the third document.')\n;\n```\n\n----------------------------------------\n\nTITLE: Slack Environment Configuration\nDESCRIPTION: Environment variables required for Slack integration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-apps/pgml-chat/README.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nSLACK_BOT_TOKEN=<SLACK_BOT_TOKEN>\nSLACK_APP_TOKEN=<SLACK_APP_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Deploying pgml-rds-proxy with Docker\nDESCRIPTION: Command to run the pgml-rds-proxy Docker container, connecting it to a PostgresML database instance. The DATABASE_URL environment variable must be set to your PostgresML database connection string.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/pgml-rds-proxy/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -e DATABASE_URL=postgres://pg:ml@sql.cloud.postgresml.org:38042/pgml \\\n    -p 6432:6432 \\\n    ghcr.io/postgresml/pgml-rds-proxy:latest\n```\n\n----------------------------------------\n\nTITLE: Starting MindsDB Python Service\nDESCRIPTION: Command to start the MindsDB service with PostgreSQL API.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/mindsdb-vs-postgresml.md#2025-04-19_snippet_4\n\nLANGUAGE: commandline\nCODE:\n```\npython -m mindsdb --api postgres\n```\n\n----------------------------------------\n\nTITLE: Installing CUDA Dependencies on Linux\nDESCRIPTION: Command to install CUDA and CUDA container toolkit on Ubuntu for GPU support.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/quick-start-with-docker.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y \\\n    cuda \\\n    cuda-container-toolkit\n```\n\n----------------------------------------\n\nTITLE: Training with a Different Algorithm in PostgresML\nDESCRIPTION: An example showing how to train a model using a specific algorithm (XGBoost) on an existing project. This demonstrates how subsequent calls can omit some parameters to use the same dataset with different algorithms.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.train.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n    'My Classification Project',\n    algorithm => 'xgboost'\n);\n```\n\n----------------------------------------\n\nTITLE: Applying Max Normalization to Embeddings in PostgreSQL\nDESCRIPTION: This snippet demonstrates how to apply Max normalization to embeddings using the pgml.normalize_max() function. It selects the normalized embeddings from the documents table.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-normalization.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.normalize_max(embedding) FROM documents;\n```\n\n----------------------------------------\n\nTITLE: Dot Product Implementation using BLAS\nDESCRIPTION: BLAS-optimized implementation of dot product calculation in Rust using unsafe blocks for maximum performance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-moving-to-rust-for-our-2.0-release.md#2025-04-19_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\n#[pg_extern(immutable, strict, parallel_safe)]\nfn dot_product_blas(vector: Vec<f32>, other: Vec<f32>) -> f32 {\n\tunsafe {\n\t\tblas::sdot(\n\t\t\tvector.len().try_into().unwrap(),\n\t\t\tvector.as_slice(),\n\t\t\t1,\n\t\t\tother.as_slice(),\n\t\t\t1,\n\t\t)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Foreign Schema in PostgreSQL\nDESCRIPTION: Shows how to import a foreign schema into PostgreSQL using Foreign Data Wrapper. This step makes the remote tables available for querying in the local database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/distributed-training.md#2025-04-19_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nIMPORT FOREIGN SCHEMA public\nFROM SERVER your_production_db\nINTO public;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Customer Reviews from Amazon Reviews Dataset\nDESCRIPTION: Queries the amazon_us_reviews table to fetch review details for a specific customer, including product title, star rating, and review text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/personalization.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT product_title, star_rating, review_body\nFROM pgml.amazon_us_reviews\nWHERE customer_id = '44366773';\n```\n\n----------------------------------------\n\nTITLE: PostgresML Sentiment Analysis - Cached Model\nDESCRIPTION: Second execution using the cached model, demonstrating improved performance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/mindsdb-vs-postgresml.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    inputs => ARRAY[\n        'I don''t really know if 5 seconds is fast or slow for deep learning. How much time is spent downloading vs running the model?'\n    ],\n    task   => '{\n        \"task\": \"text-classification\", \n        \"model\": \"cardiffnlp/twitter-roberta-base-sentiment\"\n    }'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Loading Sample Dataset for Machine Learning\nDESCRIPTION: SQL command to load the built-in digits dataset for machine learning tasks.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/quick-start-with-docker.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.load_dataset('digits');\n```\n\n----------------------------------------\n\nTITLE: Setting IVFFlat Probes Parameter in PostgreSQL\nDESCRIPTION: Sets the number of probes for IVFFlat index searches to 1, which affects the trade-off between search speed and accuracy.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSET ivfflat.probes = 1;\n```\n\n----------------------------------------\n\nTITLE: Running PostgresML Unit Tests\nDESCRIPTION: This bash command runs the unit tests for the PostgresML extension using cargo-pgrx.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncargo pgrx test\n```\n\n----------------------------------------\n\nTITLE: Creating Subscription for Logical Replication in PostgreSQL\nDESCRIPTION: Demonstrates how to create a subscription for logical replication in PostgreSQL. This initiates the replication process from the production database to PostgresML.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/distributed-training.md#2025-04-19_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nCREATE SUBSCRIPTION all_tables\nCONNECTION 'postgres://superuser:password@production-database.example.com/production_db'\nPUBLICATION all_tables;\n```\n\n----------------------------------------\n\nTITLE: Example Streaming JSON Response Chunks\nDESCRIPTION: This shows the JSON structure of individual stream chunks returned by the OpenAI Switch Kit's streaming API. Each chunk contains a delta with partial content that builds up the complete response.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/introducing-the-openai-switch-kit-move-from-closed-to-open-source-ai-in-minutes.md#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"choices\": [\n    {\n      \"delta\": {\n        \"content\": \"Y\",\n        \"role\": \"assistant\"\n      },\n      \"index\": 0\n    }\n  ],\n  \"created\": 1701296792,\n  \"id\": \"62a817f5-549b-43e0-8f0c-a7cb204ab897\",\n  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n  \"object\": \"chat.completion.chunk\",\n  \"system_fingerprint\": \"f366d657-75f9-9c33-8e57-1e6be2cf62f3\"\n}\n{\n  \"choices\": [\n    {\n      \"delta\": {\n        \"content\": \"e\",\n        \"role\": \"assistant\"\n      },\n      \"index\": 0\n    }\n  ],\n  \"created\": 1701296792,\n  \"id\": \"62a817f5-549b-43e0-8f0c-a7cb204ab897\",\n  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n  \"object\": \"chat.completion.chunk\",\n  \"system_fingerprint\": \"f366d657-75f9-9c33-8e57-1e6be2cf62f3\"\n}\n```\n\n----------------------------------------\n\nTITLE: Running Ingest Stage\nDESCRIPTION: Command to run the ingest stage, which processes documents, generates embeddings, and indexes them for fast querying.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nLOG_LEVEL=DEBUG pgml-chat --root_dir <directory> --collection_name <collection_name> --stage ingest\n```\n\n----------------------------------------\n\nTITLE: Defining Data Source Schema Configuration\nDESCRIPTION: Schema configuration for source data tables in models/schema.yml file.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n  sources:\n  - name: <schema>\n    tables:\n      - name: <documents table>\n```\n\n----------------------------------------\n\nTITLE: Connecting to MindsDB Service\nDESCRIPTION: Command to connect to the MindsDB PostgreSQL service.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/mindsdb-vs-postgresml.md#2025-04-19_snippet_5\n\nLANGUAGE: commandline\nCODE:\n```\npsql postgres://mindsdb:123@127.0.0.1:55432\n```\n\n----------------------------------------\n\nTITLE: Performing Keyword Search in PostgreSQL\nDESCRIPTION: Demonstrates a basic keyword search using PostgreSQL's full-text search capabilities. It searches for the word 'second' in the body of the documents.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * \nFROM documents\nWHERE to_tsvector('english', body) @@ to_tsquery('english', 'second');\n```\n\n----------------------------------------\n\nTITLE: Min Pooling Vector Aggregation in PostgreSQL\nDESCRIPTION: Implements min pooling on embedding vectors by selecting the minimum absolute value for each dimension across vectors.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-aggregation.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT id, pgml.min_abs(embedding)\nFROM documents\nGROUP BY id;\n```\n\n----------------------------------------\n\nTITLE: Dot Product Implementation in Python\nDESCRIPTION: Python function implementation for computing dot product using list comprehension and zip functionality.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-moving-to-rust-for-our-2.0-release.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE OR REPLACE FUNCTION dot_product_python(a FLOAT4[], b FLOAT4[])\n\tRETURNS FLOAT4\n\tLANGUAGE plpython3u IMMUTABLE STRICT PARALLEL SAFE AS\n$$\n\treturn sum([a * b for a, b in zip(a, b)])\n$$;\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML Dependencies for Quantization\nDESCRIPTION: Command to install required Python dependencies for using GPTQ and GGML quantization in PostgresML. These dependencies enable support for quantized models from Huggingface.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_1\n\nLANGUAGE: commandline\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Text Chunking with Custom Size and Overlap Parameters\nDESCRIPTION: An example showing how to customize chunk size and overlap using the kwargs parameter with the recursive_character splitter.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.chunk.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.chunk('recursive_character', 'test', '{\"chunk_size\": 1000, \"chunk_overlap\": 40}'::jsonb);\n```\n\n----------------------------------------\n\nTITLE: Creating Table Structure in PostgreSQL\nDESCRIPTION: SQL command to create a table for USA house prices dataset with seven columns including area income, house age, rooms, bedrooms, population, price, and address.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/README.md#2025-04-19_snippet_0\n\nLANGUAGE: plsql\nCODE:\n```\nCREATE TABLE usa_house_prices (\n  \"Avg. Area Income\" REAL NOT NULL,\n  \"Avg. Area House Age\" REAL NOT NULL,\n  \"Avg. Area Number of Rooms\" REAL NOT NULL,\n  \"Avg. Area Number of Bedrooms\" REAL NOT NULL,\n  \"Area Population\" REAL NOT NULL,\n  \"Price\" REAL NOT NULL,\n  \"Address\" VARCHAR NOT NULL\n);\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset in PostgresML\nDESCRIPTION: Loads a sample diabetes dataset into PostgresML using the pgml.load_dataset function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/static/images/gym/quick_start.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.load_dataset('diabetes');\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with Hugging Face Model\nDESCRIPTION: SQL query to generate embeddings from text using a pre-trained Hugging Face model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/quick-start-with-docker.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'passage: PostgresML is so easy!'\n);\n```\n\n----------------------------------------\n\nTITLE: Verifying PostgresML Installation\nDESCRIPTION: This SQL query checks the installed version of PostgresML to verify the installation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.version();\n```\n\n----------------------------------------\n\nTITLE: Installing Rust pgrx Framework\nDESCRIPTION: Commands to install and initialize the pgrx PostgreSQL extension framework for Rust.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo install cargo-pgrx --version 0.12.9 && \\\ncargo pgrx init\n```\n\n----------------------------------------\n\nTITLE: Defining Data Sources in YAML for dbt Models\nDESCRIPTION: This YAML snippet defines the data sources for dbt models, specifying the schema and table name where documents are ingested. It's typically used in the models/schema.yml file of a dbt project.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n  sources:\n  - name: <schema>\n    tables:\n      - name: <documents table>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Database Profiles for Development and Production\nDESCRIPTION: Database connection configuration for both development and production environments in profiles.yml, including host, port, credentials and schema settings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npgml_flow:\n  outputs:\n\n    dev:\n      type: postgres\n      threads: 1\n      host: 127.0.0.1\n      port: 5433\n      user: postgres\n      pass: \"\"\n      dbname: pgml_development\n      schema: <schema_name>\n    \n    prod:\n      type: postgres\n      threads: [1 or more]\n      host: [host]\n      port: [port]\n      user: [prod_username]\n      pass: [prod_password]\n      dbname: [dbname]\n      schema: [prod_schema]\n\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Prediction with Specific Model ID\nDESCRIPTION: Example of making predictions using a specific model ID instead of project name.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/README.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    target,\n    pgml.predict(1, image) AS prediction\nFROM pgml.digits \nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Installing System Dependencies for PostgresML\nDESCRIPTION: List of system packages required to build PostgresML. These should be installed using the system's package manager.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/building-from-source.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake\nclang\npkg-config\nbuild-essential\ngit\nlibclang-dev\nlibpython3-dev\nlibssl-dev\nlibopenblas-dev\npostgresql-server-dev-14\nlld\n```\n\n----------------------------------------\n\nTITLE: Inserting a JSON Document into PostgreSQL\nDESCRIPTION: This SQL statement inserts a JSON document into the 'documents' table and returns the auto-generated ID. The document contains a 'hello' key and a 'values' array.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/documents.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO documents (\n    document\n) VALUES ('{\"hello\": \"world\", \"values\": [1, 2, 3, 4]}')\nRETURNING id;\n```\n\n----------------------------------------\n\nTITLE: Implementing Manhattan Distance in JavaScript and Python\nDESCRIPTION: Simple implementation of Manhattan (L1) distance calculation between two vectors, measuring the sum of absolute differences between vector elements.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nfunction manhattanDistance(x, y) {\n    let result = 0;\n    for (let i = 0; i < x.length; i++) {\n        result += x[i] - y[i];\n    }\n    return result;\n}\n\nlet x = [1, 2, 3];\nlet y = [1, 2, 3];\nmanhattanDistance(x, y)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef manhattan_distance(x, y):\n    return sum([x-y for x,y in zip(x,y)])    \n\nx = [1, 2, 3]\ny = [1, 2, 3]\nmanhattan_distance(x, y)\n```\n\n----------------------------------------\n\nTITLE: Generated Python and JavaScript Wrappers from custom_derive_class\nDESCRIPTION: The expanded code generated by the custom_derive_class macro, which creates wrapper structs for Python and JavaScript with appropriate implementations for cross-language compatibility.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n#[pyclass]\nstruct DatabasePython {\n     wrapped: Database\n}\n\nimpl From<Database> for DatabasePython {\n    fn from(w: Database) -> Self {\n        Self { wrapped: w }\n    }\n}\n\nstruct DatabaseJavascript {\n    wrapped: Database\n}\n\nimpl From<Database> for DatabaseJavascript {\n    fn from(w: Database) -> Self {\n        Self { wrapped: w }\n    }\n}\n\nimpl IntoJsResult for Database {\n    type Output = neon::types::JsObject;\n    fn into_js_result<'a, 'b, 'c: 'b, C: neon::context::Context<'c>>(\n        self,\n        cx: &mut C,\n    ) -> neon::result::JsResult<'b, Self::Output> {\n        DatabaseJavascript::from(self).into_js_result(cx)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Table in PostgresML\nDESCRIPTION: SQL command to create a new table in PostgresML with the correct schema to match the CSV data. This example creates a 'users' table with text, integer, and boolean columns.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/copy.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE users(\n  name TEXT,\n  age INTEGER,\n  is_paying_user BOOLEAN\n);\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses with OpenAI Switch Kit in Python\nDESCRIPTION: This code demonstrates how to use the streaming API with the Korvus OpenSourceAI client in Python. It shows how to initialize a stream and iterate through the chunked responses as they arrive.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/introducing-the-openai-switch-kit-move-from-closed-to-open-source-ai-in-minutes.md#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport korvus\nclient = korvus.OpenSourceAI()\nresults = client.chat_completions_create_stream(\n     \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n     [\n         {\n             \"role\": \"system\",\n             \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n         },\n         {\n             \"role\": \"user\",\n             \"content\": \"How many helicopters can a human eat in one sitting?\",\n         },\n     ],\n     temperature=0.85,\n)\nfor c in results:\n    print(c)\n```\n\n----------------------------------------\n\nTITLE: Configuring Variables for Text Processing in YAML\nDESCRIPTION: This YAML configuration sets various parameters for text processing tasks, including splitter settings, embedding task, model name, query string, and result limit. It's used to customize the behavior of the text processing pipeline.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nvars:\n  splitter_name: \"recursive_character\"\n  splitter_parameters: {\"chunk_size\": 100, \"chunk_overlap\": 20}\n  task: \"embedding\"\n  model_name: \"intfloat/e5-small-v2\"\n  query_string: 'Lorem ipsum 3'\n  limit: 2\n```\n\n----------------------------------------\n\nTITLE: Adding and Generating Embeddings for Bulk Data\nDESCRIPTION: Adds a vector column to the local copy of data and generates embeddings using pgml.embed(). This demonstrates how to process data locally after importing it from a remote database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/foreign-data-wrappers.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nALTER TABLE bulk_access_users\nADD COLUMN embedding vector(384);\n\nUPDATE bulk_access_users\nSET embedding = pgml.embed('Alibaba-NLP/gte-base-en-v1.5', email);\n```\n\n----------------------------------------\n\nTITLE: Configuring Docker Compose for PostgresML Installation\nDESCRIPTION: A Docker Compose configuration for setting up PostgresML with necessary environment variables and port mappings. This configuration exposes PostgreSQL on port 5432 and the Dashboard on port 8000.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/tabular-data.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nversion: \"3\"\nservices:\n  postgresml:\n    image: \"postgresml/postgresml:2.8.2\"\n    ports:\n      - \"5432:5432\"\n      - \"8000:8000\"\n    environment:\n      POSTGRES_PASSWORD: \"postgresml\"\n    volumes:\n      - postgresml:/var/lib/postgresql\n\nvolumes:\n  postgresml:\n```\n\n----------------------------------------\n\nTITLE: Adding PostgresML APT Repository\nDESCRIPTION: Commands to add PostgresML's APT repository to Ubuntu's package sources.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\necho \"deb [trusted=yes] https://apt.postgresml.org jammy main\" | \\\nsudo tee -a /etc/apt/sources.list\n```\n\n----------------------------------------\n\nTITLE: Creating ZFS RAID0 Storage Configuration\nDESCRIPTION: Commands to create a ZFS RAID0 storage pool using multiple EBS volumes for maximum throughput, and creating a PostgreSQL data directory mount point.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/running-on-ec2.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nzfs create tank /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1\nzfs create -o mountpoint=/var/lib/postgresql tank/pgdata\n```\n\n----------------------------------------\n\nTITLE: Sample Output of dbt Pipeline Execution\nDESCRIPTION: Example console output from running the dbt pipeline. It shows the execution progress, including the creation of view models, incremental models, and table models, along with their execution times and status.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n22:29:58  Running with dbt=1.5.2\n22:29:58  Registered adapter: postgres=1.5.2\n22:29:58  Unable to do partial parsing because a project config has changed\n22:29:59  Found 7 models, 10 tests, 0 snapshots, 0 analyses, 307 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups\n22:29:59  \n22:29:59  Concurrency: 1 threads (target='dev')\n22:29:59  \n22:29:59  1 of 7 START sql view model test_collection_1.characters ....................... [RUN]\n22:29:59  1 of 7 OK created sql view model test_collection_1.characters .................. [CREATE VIEW in 0.11s]\n22:29:59  2 of 7 START sql incremental model test_collection_1.models .................... [RUN]\n22:29:59  2 of 7 OK created sql incremental model test_collection_1.models ............... [INSERT 0 1 in 0.15s]\n22:29:59  3 of 7 START sql incremental model test_collection_1.splitters ................. [RUN]\n22:30:00  3 of 7 OK created sql incremental model test_collection_1.splitters ............ [INSERT 0 1 in 0.07s]\n22:30:00  4 of 7 START sql incremental model test_collection_1.chunks .................... [RUN]\n22:30:00  4 of 7 OK created sql incremental model test_collection_1.chunks ............... [INSERT 0 0 in 0.08s]\n22:30:00  5 of 7 START sql incremental model test_collection_1.embedding_36b7e ........... [RUN]\n22:30:00  5 of 7 OK created sql incremental model test_collection_1.embedding_36b7e ...... [INSERT 0 0 in 0.08s]\n22:30:00  6 of 7 START sql incremental model test_collection_1.transforms ................ [RUN]\n22:30:00  6 of 7 OK created sql incremental model test_collection_1.transforms ........... [INSERT 0 1 in 0.07s]\n22:30:00  7 of 7 START sql table model test_collection_1.vector_search ................... [RUN]\n22:30:05  7 of 7 OK created sql table model test_collection_1.vector_search .............. [SELECT 2 in 4.81s]\n22:30:05  \n22:30:05  Finished running 1 view model, 5 incremental models, 1 table model in 0 hours 0 minutes and 5.59 seconds (5.59s).\n22:30:05  \n22:30:05  Completed successfully\n22:30:05  \n22:30:05  Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7\n```\n\n----------------------------------------\n\nTITLE: Creating Foreign Server in PostgreSQL\nDESCRIPTION: Shows how to create a foreign server in PostgreSQL, which is a reference to another PostgreSQL database. This is part of the Foreign Data Wrapper setup process.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/distributed-training.md#2025-04-19_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE SERVER your_production_db\n    FOREIGN DATA WRAPPER postgres_fdw\n    OPTIONS (\n        host 'production-database.example.com',\n        port '5432',\n        dbname 'production_db'\n    );\n```\n\n----------------------------------------\n\nTITLE: Querying Model Information\nDESCRIPTION: Query to retrieve model IDs and metrics for a specific project.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/README.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT models.id, models.algorithm, models.metrics\nFROM pgml.models\nJOIN pgml.projects \n  ON projects.id = models.project_id\nWHERE projects.name = 'Handwritten Digit Image Classifier';\n```\n\n----------------------------------------\n\nTITLE: Installing pgvector from Source\nDESCRIPTION: Commands to clone, build and install pgvector from source.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/README.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/pgvector/pgvector /tmp/pgvector\ngit -C /tmp/pgvector checkout v0.5.0\necho \"trusted = true\" >> \"/tmp/pgvector/vector.control\"\nmake -C /tmp/pgvector\nsudo make install -C /tmp/pgvector\n```\n\n----------------------------------------\n\nTITLE: Creating Shuffled View of IMDB Dataset\nDESCRIPTION: Creates a view of the IMDB dataset with added class column, shuffled rows, and removed unsupervised data.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE VIEW pgml.imdb_shuffled_view AS\nSELECT\n    label,\n    CASE WHEN label = 0 THEN 'negative'\n         WHEN label = 1 THEN 'positive'\n         ELSE 'neutral'\n    END AS class,\n    text\nFROM pgml.imdb\nWHERE label != -1\nORDER BY RANDOM();\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Addition in JavaScript\nDESCRIPTION: A basic JavaScript implementation of vector addition that takes two arrays as input and returns their element-wise sum. This demonstrates fundamental vector operations used in embedding calculations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/README.md#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nfunction add_vectors(x, y) {\n    let result = [];\n    for (let i = 0; i < x.length; i++) {\n        result[i] = x[i] + y[i];\n    }\n    return result;\n}\n\nlet x = [1, 2, 3];\nlet y = [1, 2, 3];\nadd(x, y)\n```\n\n----------------------------------------\n\nTITLE: Creating a Flexible Load Average Table with JSONB Metadata\nDESCRIPTION: This SQL statement creates a table for load average metrics that uses a JSONB column to store flexible metadata, allowing for easier evolution of the data structure.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pg-stat-sysinfo-a-postgres-extension-for-querying-system-statistics.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE load_average (\n    at          timestamptz NOT NULL DEFAULT now(),\n    \"1m\"        float4 NOT NULL,\n    \"5m\"        float4 NOT NULL,\n    \"15m\"       float4 NOT NULL,\n    metadata    jsonb NOT NULL DEFAULT '{}'\n);\n```\n\n----------------------------------------\n\nTITLE: SQL Vector Search Query Results\nDESCRIPTION: This snippet shows the output from the SQL vector search query, displaying the matching document text and its similarity score.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/a-speed-comparison-of-the-most-popular-retrieval-systems-for-rag.md#2025-04-19_snippet_5\n\nLANGUAGE: txt\nCODE:\n```\n           text           |       score\n--------------------------+--------------------\n The hidden value is 1000 | 0.9132997445285489\n```\n\n----------------------------------------\n\nTITLE: Querying Range Partitioned Table\nDESCRIPTION: Example of querying the partitioned table with date range filter for optimal performance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/partitioning.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    avg(\"AEP_MW\")\nFROM energy_consumption\nWHERE \"Datetime\" BETWEEN '2004-01-01' AND '2005-01-01';\n```\n\n----------------------------------------\n\nTITLE: PostgresML Training Result Output\nDESCRIPTION: Example output showing the result of successful model training and deployment with hyperparameter optimization. Shows the project name, task type, algorithm used, and deployment status.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/hyperparameter-search.md#2025-04-19_snippet_1\n\nLANGUAGE: plsql\nCODE:\n```\n              project               |      task      | algorithm | deployed\n------------------------------------+----------------+-----------+----------\n Handwritten Digit Image Classifier | classification | xgboost   | t\n(1 row)\n```\n\n----------------------------------------\n\nTITLE: Testing FDW Connection with dblink\nDESCRIPTION: Verifies the FDW connection is working properly by executing a simple query on the remote database. Uses the dblink extension to run arbitrary queries against the connected database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/foreign-data-wrappers.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT *\nFROM dblink(\n  'production_db',\n  'SELECT 1 AS one'\n) AS t1(one INTEGER);\n```\n\n----------------------------------------\n\nTITLE: MindsDB Model Creation\nDESCRIPTION: Creating a sentiment analysis model in MindsDB using HuggingFace transformer.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/mindsdb-vs-postgresml.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE MODEL mindsdb.sentiment_classifier\nPREDICT sentiment\nUSING\n  engine = 'huggingface',\n  task = 'text-classification',\n  model_name = 'cardiffnlp/twitter-roberta-base-sentiment',\n  input_column = 'text',\n  labels = ['negativ', 'neutral', 'positive'];\n```\n\n----------------------------------------\n\nTITLE: Creating PostgresML Extension\nDESCRIPTION: SQL command to create the PostgresML extension in the database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/README.md#2025-04-19_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\npostgres=# CREATE EXTENSION pgml;\nINFO:  Python version: 3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]\nINFO:  Scikit-learn 1.1.3, XGBoost 1.7.1, LightGBM 3.3.3, NumPy 1.23.5\nCREATE EXTENSION\npostgres=#\n```\n\n----------------------------------------\n\nTITLE: Updating APT Package List\nDESCRIPTION: Command to update the APT package list after adding new repository.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into PostgreSQL\nDESCRIPTION: SQL command to insert a single row of house data including address and various metrics.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/README.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nINSERT INTO usa_house_prices (\n  \"Avg. Area Income\",\n  \"Avg. Area House Age\",\n  \"Avg. Area Number of Rooms\",\n  \"Avg. Area Number of Bedrooms\",\n  \"Area Population\",\n  \"Price\",\n  \"Address\"\n) VALUES (\n  199778.0,\n  43.0,\n  3.0,\n  2.0,\n  57856.0,\n  5000000000.0,\n  '1 Infinite Loop, Cupertino, California'\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Foreign Data Wrapper Extension in PostgreSQL\nDESCRIPTION: Demonstrates how to create the postgres_fdw extension in PostgreSQL. This is the first step in setting up Foreign Data Wrappers for accessing data from remote databases.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/distributed-training.md#2025-04-19_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTENSION postgres_fdw;\n```\n\n----------------------------------------\n\nTITLE: Self-Hosted Pipeline with Remote Code Trust Parameter\nDESCRIPTION: Pipeline configuration for self-hosted PostgresML instances that requires enabling trust_remote_code for certain HuggingFace models.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/constructing-pipelines.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\n    \"v0\",\n    {\n        \"text\": {\n            \"semantic_search\": {\n                \"model\": \"Alibaba-NLP/gte-base-en-v1.5\",\n                \"parameters\": {\n                    \"trust_remote_code\": True\n                }\n            }\n        }\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Required PostgreSQL Extensions for FDW\nDESCRIPTION: SQL commands to create the necessary extensions (dblink and postgres_fdw) in the RDS database. These extensions require superuser privileges through the rds_superuser role.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/pgml-rds-proxy/README.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE EXTENSION IF NOT EXISTS dblink;\nCREATE EXTENSION IF NOT EXISTS postgres_fdw;\n```\n\n----------------------------------------\n\nTITLE: Ensemble Regression Methods\nDESCRIPTION: Examples of training regression models using various ensemble methods including AdaBoost, Random Forest, and Gradient Boosting Trees.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/regression.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'ada_boost', hyperparams => '{\"n_estimators\": 5}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'bagging', hyperparams => '{\"n_estimators\": 5}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'extra_trees', hyperparams => '{\"n_estimators\": 5}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'gradient_boosting_trees', hyperparams => '{\"n_estimators\": 5}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'random_forest', hyperparams => '{\"n_estimators\": 5}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'hist_gradient_boosting', hyperparams => '{\"max_iter\": 10}');\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Addition in Python\nDESCRIPTION: A Python implementation of vector addition using list comprehension and zip function to combine two vectors. Shows a more concise approach to vector operations common in ML workflows.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef add_vectors(x, y):\n    return [x+y for x,y in zip(x,y)]    \n\nx = [1, 2, 3]\ny = [1, 2, 3]\nadd(x, y)\n```\n\n----------------------------------------\n\nTITLE: Querying Dataset in PostgresML\nDESCRIPTION: Displays the first 5 rows of the loaded diabetes dataset to examine its structure.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/static/images/gym/quick_start.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.diabetes LIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Removing a Pipeline in Korvus\nDESCRIPTION: This snippet illustrates how to remove a pipeline from a Korvus collection. Removing a pipeline deletes it and all associated data from the database. Removed pipelines cannot be re-enabled but can be recreated.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/pipelines.md#2025-04-19_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst pipeline = korvus.newPipeline(\"test_pipeline\")\nconst collection = korvus.newCollection(\"test_collection\")\nawait collection.remove_pipeline(pipeline)\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\"test_pipeline\")\ncollection = Collection(\"test_collection\")\nawait collection.remove_pipeline(pipeline)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut collection = Collection::new(\"test_collection\", None)?;\nlet mut pipeline = Pipeline::new(\"test_pipeline\", None)?;\ncollection.remove_pipeline(&mut pipeline).await?;\n```\n\nLANGUAGE: c\nCODE:\n```\nCollectionC * collection = korvus_collectionc_new(\"test_collection\", NULL);\nPipelineC * pipeline = korvus_pipelinec_new(\"test_pipeline\",  NULL);\nkorvus_collectionc_remove_pipeline(collection, pipeline);\n```\n\n----------------------------------------\n\nTITLE: Creating a Load Average Table with Metadata\nDESCRIPTION: This SQL statement creates a table for load average metrics that includes additional columns for node-specific metadata like node ID and datacenter.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pg-stat-sysinfo-a-postgres-extension-for-querying-system-statistics.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE load_average (\n    at          timestamptz NOT NULL DEFAULT now(),\n    \"1m\"        float4 NOT NULL,\n    \"5m\"        float4 NOT NULL,\n    \"15m\"       float4 NOT NULL,\n    node        text NOT NULL,\n    -- ...and so on...\n    datacenter  text NOT NULL\n);\n```\n\n----------------------------------------\n\nTITLE: Creating User Mapping for Foreign Data Wrapper in PostgreSQL\nDESCRIPTION: Demonstrates how to create a user mapping for Foreign Data Wrapper in PostgreSQL. This mapping defines the relationship between the local and remote database users.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/distributed-training.md#2025-04-19_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE USER MAPPING FOR pgml_user\n    SERVER your_production_db\n    OPTIONS (\n        user 'your_production_db_user',\n        password 'your_production_db_user_password'\n    );\n```\n\n----------------------------------------\n\nTITLE: Dot Product Implementation in SQL\nDESCRIPTION: SQL function implementation for computing dot product between two float arrays using UNNEST and SUM operations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-moving-to-rust-for-our-2.0-release.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE OR REPLACE FUNCTION dot_product_sql(a FLOAT4[], b FLOAT4[])\n\tRETURNS FLOAT4\n\tLANGUAGE sql IMMUTABLE STRICT PARALLEL SAFE AS\n$$\n\tSELECT SUM(multiplied.values)\n\tFROM (SELECT UNNEST(a) * UNNEST(b) AS values) AS multiplied;\n$$;\n```\n\n----------------------------------------\n\nTITLE: Dot Product Implementation in Rust\nDESCRIPTION: Rust implementation of dot product calculation using iterators and functional programming patterns.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-moving-to-rust-for-our-2.0-release.md#2025-04-19_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n#[pg_extern(immutable, strict, parallel_safe)]\nfn dot_product_rust(vector: Vec<f32>, other: Vec<f32>) -> f32 {\n\tvector\n\t\t.as_slice()\n\t\t.iter()\n\t\t.zip(other.as_slice().iter())\n\t\t.map(|(a, b)| (a * b))\n\t\t.sum()\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Primary PostgreSQL Server\nDESCRIPTION: PostgreSQL primary server configuration settings for enabling replication in postgresql.conf file.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/replication.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\narchive_mode = on\nwal_level = replica\narchive_command = 'pgbackrest --stanza=main archive-push %p'\n```\n\n----------------------------------------\n\nTITLE: Connecting to PostgreSQL\nDESCRIPTION: Command to connect to PostgreSQL as postgres user.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo -u postgres psql\n```\n\n----------------------------------------\n\nTITLE: Statically Linking OpenBLAS in Rust Build Script\nDESCRIPTION: This build.rs script demonstrates how to statically link the OpenBLAS library with a Rust project. The static linking ensures that the required BLAS functions are available to both the Rust code and any Python libraries that might need them, resolving segmentation faults caused by dynamic linking conflicts.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/backwards-compatible-or-bust-python-inside-rust-inside-postgres.md#2025-04-19_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nfn main() {\n    println!(\"cargo:rustc-link-lib=static=openblas\");\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Response Format\nDESCRIPTION: Example of the JSON chunk format received during streaming responses, showing how the content is delivered in smaller delta updates.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/opensourceai.md#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"choices\": [\n    {\n      \"delta\": {\n        \"content\": \"Y\",\n        \"role\": \"assistant\"\n      },\n      \"index\": 0\n    }\n  ],\n  \"created\": 1701296792,\n  \"id\": \"62a817f5-549b-43e0-8f0c-a7cb204ab897\",\n  \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n  \"object\": \"chat.completion.chunk\",\n  \"system_fingerprint\": \"f366d657-75f9-9c33-8e57-1e6be2cf62f3\"\n}\n```\n\n----------------------------------------\n\nTITLE: CSV Vector Data\nDESCRIPTION: A sequence of 512 floating point numbers, likely representing an embedding vector or machine learning model output. Values range approximately from -2 to 2.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\n,-0.30049723,-0.7189453,-0.6286008,-0.7182035,0.337718,-0.11861088,-0.67316926,0.03807467,-0.4894712,0.0021176785,0.6980891,0.24103045,0.54633296,0.58161646,-0.44642344,-0.16555169,0.7964468,-1.2131425,-0.67829454,0.4893405,-0.38461393,-1.1225401,0.44452366,-0.30833852,-0.6711606,0.051745616,-0.775163,-0.2677435,-0.39321816,-0.74936676,0.16192177,-0.059772447,0.68762016,0.53828514,0.6541142,-0.5421721,-0.26251954,-0.023202112,0.3014187,0.008828241,0.79605895,-0.3317026,-0.7724727,-1.2411877,0.31939238,-0.096119456,0.47874188,-0.7791832,-0.22323853,-0.08456612,1.0795188,-0.7827005,-0.28929207,0.46884036,-0.42510015,0.16214833,0.3501767,0.36617047,-1.119466,0.19195387,0.85851586,0.18922725,0.94338834,-0.32304144,0.4827557,-0.81715256,-1.4261038,0.49614763,0.062142983,1.249345,0.2014524,-0.6995533,-0.15864229,0.38652128,-0.659232,0.11766203,-0.2557698,1.4296027,0.9037317,-0.011628535,-1.1893693,-0.956275,-0.18136917,0.3941797,0.39998764,0.018311564,0.27029866,0.14892557,-0.48989707,0.05881763,0.49618796,-0.11214719,0.71434236,0.35651416,0.8689908,1.0284718,0.9596098,-0.009955626,0.40186208,0.4057858,-0.28830874,-0.72128904,-0.5276375,-0.44327998,-0.025095768,-0.7058158,-0.16796891,0.12855923,-0.34389406,0.4430077,0.16097692,-0.58964425,-0.80346566,0.32405907,0.06305365,-1.5064402,0.2241937,-0.6216805,0.1358616,0.3714332,-0.99806577,-0.22238642,0.33287752,0.14240637,-0.29236397,1.1396701,0.23270036,0.5262793,1.0991998,0.2879055,0.22905749,-0.95235413,0.52312446,0.10592761,0.30011278,-0.7657238,0.16400222,-0.5638396,-0.57501423,1.121968,-0.7843481,0.09353633,-0.18324867,0.21604645,-0.8815248,-0.07529478,-0.8126517,-0.011605805,-0.50744057,1.3081754,-0.852715,0.39023215,0.7651248,1.68998,0.5819176,-0.02141522,0.5877081,0.2024052,0.09264247,-0.13779058,-1.5314059,1.2719066,-1.0927896,0.48220706,0.05559338,-0.20929311,-0.4278733,0.28444275,-0.0008470379,-0.09534583,-0.6519637,-1.4282455,0.18477388,0.9507184,-0.6751443,-0.18364592,-0.37007314,1.0216024,0.6869564,1.1653348,-0.7538794,-1.3345296,0.6104916,0.08152369,-0.8394207,0.87403923,0.5290044,-0.56332856,0.37691587,-0.45009997,-0.17864561,0.5992149,-0.25145024,1.0287454,1.4305328,-0.011586349,0.3485581,0.66344,0.18219411,4.940573,1.0454609,-0.23867694,-0.8316158,0.4034564,-0.49062842,0.016044907,-0.22793365,-0.38472247,0.2440083,0.41246706,1.1865108,1.2949868,0.4173234,0.5325333,0.5680148,-0.07169041,-1.005387,0.965118,-0.340425,-0.4471613,-0.40878603,-1.1905128,-1.1868874,1.2017782,0.53103817,0.3596472,-0.9262005,0.31224424,0.72889113,0.63557464,-0.07019187,-0.68807346,0.69582283,0.45101142,0.014984587,0.577816,-0.1980364,-1.0826674,0.69556504,0.88146895,-0.2119645,0.6493935,0.9528447,-0.44620317,-0.9011973,-0.50394785,-1.0315249,-0.4472283,0.7796344,-0.15637895,-0.16639937,-0.20352335,-0.68020046,-0.98728025,0.64242256,0.31667972,-0.71397847,-1.1293691,-0.9860645,0.39156264,-0.69573534,0.30602834,-0.1618791,0.23074874,-0.3379239,-0.12191323,1.6582693,0.2339738,-0.6107068,-0.26497284,0.17334077,-0.5923304,0.10445539,-0.7599427,0.5096536,-0.20216745,0.049196683,-1.1881349,-0.9009607,-0.83798426,0.44164553,-0.48808926,-0.04667333,-0.66054153,-0.66128224,-1.7136352,-0.7366011,-0.31853634,0.30232653,-0.10852443,1.9946622,0.13590258,-0.76326686,-0.25446486,0.32006142,-1.046221,0.30643058,0.52830505,1.7721215,0.71685624,0.35536727,0.02379851,0.7471644,-1.3178513,0.26788896,1.0505391,-0.8308426,-0.44220716,-0.2996315,0.2289448,-0.8129853,-0.32032526,-0.67732286,0.49977696,-0.58026063,-0.4267268,-1.165912,0.5383717,-0.2600939,0.4909254,-0.7529048,0.5186025,-0.68272185,0.37688586,-0.16525345,0.68933797,-0.43853116,0.2531767,-0.7273167,0.0042542545,0.2527112,-0.64449465,-0.07678814,-0.57123,-0.0017966144,-0.068321034,0.6406287,-0.81944615,-0.5292494,0.67187285,-0.45312735,-0.19861545,0.5808865,0.24339013,0.19081701,-0.3795915,-1.1802675,0.5864333,0.5542488,-0.026795216,-0.27652445,0.5329341,0.29494807,0.5427568,0.84580654,-0.39151683,-0.2985327,-1.0449492,0.69868237,0.39184457,0.9617548,0.8102169,0.07298472,-0.5491848,-1.012611,-0.76594234,-0.1864931,0.5790788,0.32611984,-0.7400497,0.23077846,-0.15595563,-0.06170243,-0.26768005,-0.7510913,-0.81110775,0.044999585,1.3336306,-1.774329,0.8607937,0.8938075,-0.9528547,0.43048507,-0.49937993,-0.61716783,-0.58577335,0.6208,-0.56602585,0.6925776,-0.50487256,0.80735886,0.36914152,0.6803319,0.000295409,-0.28081727,-0.65416694,0.9890088,0.5936174,-0.38552138,0.92602617,-0.46841428,-0.07666884,0.6774499,-1.1728637,0.23638526,0.35253218,0.5990712,0.47170952,1.1473405,-0.6329502,0.07515354,-0.6493073,-0.7312147,0.003280595,0.53415585,-0.84027874,0.21279827,0.73492074,-0.08271271,-0.6393985,0.21382183,-0.5933761,0.26885328,0.31527188,-0.17841923,0.8519613,-0.87693113,0.14174065,-0.3014772,0.21034332,0.7176752,0.045435462,0.43554127,0.7759069,-0.2540516,-0.21126957,-0.1182913,0.504212,0.07782592,-0.06410891,-0.016180445,0.16819397,0.7418499,-0.028192373,-0.21616131,-0.46842667,0.8750199,0.16664875,0.4422129,-0.24636972,0.011146031,0.5407099,-0.1995775,0.9732007,0.79718286,-0.3531048,-0.17953855,-0.30455542,-0.011377579,-0.21079576,1.3742573,-0.4004308,-0.30791727,-1.06878,0.53180254,0.3412094,-0.06790889,0.08864223,-0.6960799,-0.12536404,0.24884924,0.9308994,0.46485603,0.12150945,0.8934372,-1.6594642,0.27694207,-1.1839775,-0.54069275,0.2967536,0.94271827,-0.21412376,1.5007582,-0.75979245,0.4711972,-0.005775435,-0.13180988,-0.9351274,0.5930414,0.23131478,-0.4255422,-1.1771399,-0.49364802,-0.32276222,-1.6043308,-0.27617428,0.76369554,-0.19217926,0.12788418,1.9225345,0.35335732,1.6825448,0.12466301,0.1598846,-0.43834555,-0.086372584,0.47859296,0.79709494,0.049911886,-0.52836734,-0.6721834,0.21632576,-0.36516222,1.6216894,0.8214337,0.6054308,-0.41862285,0.027636342,-0.1940268,-0.43570083,-0.14520688,0.4045223,-0.35977545,1.8254343,-0.31089872,0.19665615,-1.1023157,0.4019758,-0.4453815,-1.0864284,-0.1992614,0.11380532,0.16687272,-0.29629833,-0.728387,-0.5445154,0.23433375,-1.5238215,0.71899056,-0.8600819,1.0411007,-0.05895088,-0.8002717,-0.72914296,-0.59206986,-0.28384188,0.4074883,0.56018656,-1.068546,-1.021818,-0.050443307,1.116262,-1.3534596,0.6736171,-0.55024904,-0.31289905,0.36604482,0.004892461\n```\n\n----------------------------------------\n\nTITLE: Creating a Table for Document Storage in PostgreSQL\nDESCRIPTION: This SQL statement creates a table named 'documents' with an auto-incrementing primary key 'id' and a JSONB column 'document' for storing JSON data.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/documents.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE documents (\n    id BIGSERIAL PRIMARY KEY,\n    document JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Running PostgresML with Docker\nDESCRIPTION: Command to start a PostgresML container with Docker, mapping ports and creating a volume for data persistence.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -it \\\n    -v postgresml_data:/var/lib/postgresql \\\n    -p 5433:5432 \\\n    -p 8000:8000 \\\n    ghcr.io/postgresml/postgresml:2.10.0 \\\n    sudo -u postgresml psql -d postgresml\n```\n\n----------------------------------------\n\nTITLE: Installing pgrx Rust Extension Toolkit\nDESCRIPTION: Command to install pgrx version 0.10.0 using Cargo, the Rust package manager.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/building-from-source.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo install cargo-pgrx --version \"0.10.0\"\n```\n\n----------------------------------------\n\nTITLE: Creating a User Mapping for FDW Connection\nDESCRIPTION: Sets up authentication credentials for connecting to the production database. Specifies the user and password that will be used when accessing the remote database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/foreign-data-wrappers.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE USER MAPPING\nFOR CURRENT_USER\nSERVER production_db\nOPTIONS (\n  user 'readonly_user',\n  password 'secret_password'\n);\n```\n\n----------------------------------------\n\nTITLE: Document Structure Example\nDESCRIPTION: Sample JSON structure showing the expected document format with id, title and body fields.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/pipelines.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"Each document has a unique id\",\n  \"title\": \"Each document has a title\",\n  \"body\": \"Each document has some body text\"\n}\n```\n\n----------------------------------------\n\nTITLE: Dot Product Implementation in NumPy\nDESCRIPTION: NumPy-based function implementation for computing dot product using the optimized numpy.dot function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-moving-to-rust-for-our-2.0-release.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE OR REPLACE FUNCTION dot_product_numpy(a FLOAT4[], b FLOAT4[])\n\tRETURNS FLOAT4\n\tLANGUAGE plpython3u IMMUTABLE STRICT PARALLEL SAFE AS\n$$\n\timport numpy\n\treturn numpy.dot(a, b)\n$$;\n```\n\n----------------------------------------\n\nTITLE: Querying Indexed JSON Documents in PostgreSQL\nDESCRIPTION: This SQL query demonstrates how to use the GIN index for efficient querying of JSON documents. It selects all documents where the 'hello' key has the value 'world'.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/documents.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    * \nFROM\n    documents\nWHERE document @> '{\"hello\": \"world\"}';\n```\n\n----------------------------------------\n\nTITLE: Exporting and Importing Data with pg_dump in PostgreSQL\nDESCRIPTION: Demonstrates how to export data from a production database and import it into PostgresML using pg_dump. This method is suitable for datasets smaller than 10GB that change infrequently.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/distributed-training.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Export data from your production DB\npg_dump \\\n    postgres://username:password@production-database.example.com/production_db \\\n    --no-owner \\\n    -t table_one \\\n    -t table_two > dump.sql\n\n# Import the data into PostgresML\npsql \\\n    postgres://username:password@postgresml.example.com/postgresml_db \\\n    -f dump.sql\n```\n\n----------------------------------------\n\nTITLE: Resetting PostgresML Environment\nDESCRIPTION: These SQL commands reset the PostgresML environment by dropping and recreating the extension and its schema.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_11\n\nLANGUAGE: postgresql\nCODE:\n```\nDROP EXTENSION IF EXISTS pgml CASCADE;\nDROP SCHEMA IF EXISTS pgml CASCADE;\nCREATE EXTENSION pgml;\n```\n\n----------------------------------------\n\nTITLE: Migrating Specific Tables from PostgreSQL to PostgresML Using pg_dump\nDESCRIPTION: This command demonstrates how to migrate specific tables (users and orders in this example) from a source PostgreSQL database to PostgresML. This approach is useful for larger databases where migrating one table at a time is preferred.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/pg-dump.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npg_dump \\\n\t--no-owner \\\n\t--clean \\\n\t--no-privileges \\\n\t-t users \\\n\t-t orders \\\n  postgres://user:password@your-production-database.amazonaws.com/production_db | \\\npsql postgres://user:password@sql.cloud.postgresml.org:6432/your_pgml_db\n```\n\n----------------------------------------\n\nTITLE: Displaying Binary Classification Result\nDESCRIPTION: Shows the output of the sentiment prediction query, returning 1 indicating positive sentiment with execution time.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nsentiment\n-----------\n1\n(1 row)\n\nTime: 16.681 ms\n```\n\n----------------------------------------\n\nTITLE: Setting up Foreign Data Wrapper Server Connection\nDESCRIPTION: SQL command to create a foreign server connection pointing to the pgml-rds-proxy. The host should be replaced with the proxy's private IP or DNS entry in your VPC.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/pgml-rds-proxy/README.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE SERVER postgresml\nFOREIGN DATA WRAPPER postgres_fdw\nOPTIONS (\n    host '127.0.0.1',\n    port '6432',\n    dbname 'pgml'\n);\n```\n\n----------------------------------------\n\nTITLE: Generating Translations\nDESCRIPTION: Uses the fine-tuned model to translate English text to Spanish using pgml.generate.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.tune.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.generate('Translate English to Spanish', 'I love SQL')\nAS spanish;\n```\n\n----------------------------------------\n\nTITLE: Creating Range Partition Child Tables\nDESCRIPTION: Creates two child partition tables for different date ranges (2004-2011 and 2012-2018).\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/partitioning.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE energy_consumption_2004_2011\nPARTITION OF energy_consumption\nFOR VALUES FROM ('2004-01-01') TO ('2011-12-31');\n\nCREATE TABLE energy_consumption_2012_2018 \nPARTITION OF energy_consumption \nFOR VALUES FROM ('2011-12-31') TO ('2018-12-31');\n```\n\n----------------------------------------\n\nTITLE: Running PostgresML Docker Container on macOS\nDESCRIPTION: Docker command to start PostgresML container on macOS with port mappings and volume mounting for data persistence.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/quick-start-with-docker.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -it \\\n    -v postgresml_data:/var/lib/postgresql \\\n    -p 5433:5432 \\\n    -p 8000:8000 \\\n    ghcr.io/postgresml/postgresml:2.10.0 \\\n    sudo -u postgresml psql -d postgresml\n```\n\n----------------------------------------\n\nTITLE: Enabling Prepared Statements in Rails\nDESCRIPTION: Configuration setting in Rails to enable prepared statements for improved database performance when using PgCat.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/making-postgres-30-percent-faster-in-production.md#2025-04-19_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nprepared_statements: true\n```\n\n----------------------------------------\n\nTITLE: Method Destruction Data Structure\nDESCRIPTION: Definition of the GetImplMethod struct used for parsing and representing Rust methods during the macro expansion process. This structure captures all relevant information about a method for later translation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\npub struct GetImplMethod {\n     pub exists: bool,\n     pub method_ident: Ident,\n     pub is_async: bool,\n     pub method_arguments: Vec<(String, SupportedType)>,\n     pub receiver: Option<proc_macro2::TokenStream>,\n     pub output_type: OutputType,\n}\n```\n\n----------------------------------------\n\nTITLE: Building PostgresML with CUDA Support\nDESCRIPTION: This bash command builds PostgresML with CUDA support for XGBoost and LightGBM, requiring CUDA to be installed locally.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nCUDACXX=/usr/local/cuda/bin/nvcc cargo pgrx run --release --features pg15,python,cuda\n```\n\n----------------------------------------\n\nTITLE: Exporting Database Schema Using pg_dump\nDESCRIPTION: This bash command uses pg_dump to export the schema of specific tables from the primary database. It excludes ownership and privileges information.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/logical-replication/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npg_dump \\\n\tpostgres://user:password@yyour-production-db.amazonaws.com:5432/prodution_db \\\n\t--schema-only \\\n\t--no-owner \\\n\t--no-privileges \\\n\t-t users \\\n\t-t blog_posts \\\n> schema.sql\n```\n\n----------------------------------------\n\nTITLE: Creating Publication for Logical Replication in PostgreSQL\nDESCRIPTION: Demonstrates how to create a publication for logical replication in PostgreSQL. This configures which tables will be replicated from the production database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/distributed-training.md#2025-04-19_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nCREATE PUBLICATION all_tables\nFOR ALL TABLES;\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML SDK - Python\nDESCRIPTION: Command to install the PostgresML Python SDK package from PyPI.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/getting-started/connect-your-app.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pgml\n```\n\n----------------------------------------\n\nTITLE: Searching Todos via cURL\nDESCRIPTION: Example of searching todo items using the semantic search endpoint with cURL.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/using-postgresml-with-django-and-embedding-search.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncurl \\\n    --silent \\\n    -H \"Content-Type: application/json\" \\\n    'http://localhost:8000/api/todo/search/?q=resolution&limit=1' | \\\n     jq \".[0].description\"\n```\n\n----------------------------------------\n\nTITLE: PostgresML pgml.chunk() API Definition\nDESCRIPTION: The API definition for the pgml.chunk() function, which takes a splitter name, text to embed, and optional arguments as JSON.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.chunk.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\npgml.chunk(\n    splitter TEXT,    -- splitter name\n    text TEXT,        -- text to embed\n    kwargs JSON       -- optional arguments (see below)\n)\n```\n\n----------------------------------------\n\nTITLE: Linear Regression Methods\nDESCRIPTION: Examples of training various linear regression models including standard linear regression, ridge, lasso, and other specialized implementations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/regression.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'linear');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'ridge');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'lasso');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'elastic_net');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'least_angle');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'lasso_least_angle');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'orthogonal_matching_pursuit');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'bayesian_ridge');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'automatic_relevance_determination');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'stochastic_gradient_descent');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'passive_aggressive');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'ransac');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'theil_sen', hyperparams => '{\"max_iter\": 10, \"max_subpopulation\": 100}');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'huber');\nSELECT * FROM pgml.train('Diabetes Progression', algorithm => 'quantile');\n```\n\n----------------------------------------\n\nTITLE: Connecting to PostgresML Container\nDESCRIPTION: Command to connect to the PostgresML container using psql client.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/quick-start-with-docker.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npsql -h 127.0.0.1 -p 5433 -U postgresml\n```\n\n----------------------------------------\n\nTITLE: Configuring Pooler for PostgresML\nDESCRIPTION: This snippet demonstrates how to configure PgCat for load balancing between PostgresML primary and replica servers. The configuration defines server addresses for both primary and replica nodes to facilitate traffic management and improve performance.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/replication.md#2025-04-19_snippet_8\n\nLANGUAGE: toml\nCODE:\n```\n[pools.postgresml.shards.0]\nservers = [\n    [\"<primary host or IP address>\", 5432, \"primary\"],\n    [\"<replica host or IP address>\", 5432, \"replica\"], \n]\n```\n\n----------------------------------------\n\nTITLE: MindsDB Sentiment Analysis - Full Results\nDESCRIPTION: Querying the MindsDB model with detailed sentiment explanation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/mindsdb-vs-postgresml.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT *\nFROM mindsdb.sentiment_classifier\nWHERE text = 'I am so excited to benchmark deep learning models in SQL. I can not wait to see the results!'\n```\n\n----------------------------------------\n\nTITLE: Creating PostgresML Extension in Database\nDESCRIPTION: This SQL command creates the PostgresML extension in the current database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTENSION pgml;\n```\n\n----------------------------------------\n\nTITLE: Installing PostgreSQL Dependencies for PostgresML\nDESCRIPTION: This bash script installs the necessary PostgreSQL development headers and other dependencies required for building PostgresML.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport POSTGRES_VERSION=15\nsudo apt-get update && \\\nsudo apt-get install -y \\\n    postgresql-server-dev-${POSTGRES_VERSION} \\\n    bison \\\n    build-essential \\\n    clang \\\n    cmake \\\n    flex \\\n    libclang-dev \\\n    libopenblas-dev \\\n    libpython3-dev \\\n    libreadline-dev \\\n    libssl-dev \\\n    pkg-config \\\n    python3-dev\n```\n\n----------------------------------------\n\nTITLE: Loading IMDB Dataset in PostgreSQL\nDESCRIPTION: Loads the IMDB dataset into PostgreSQL using the pgml.load_dataset function.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.load_dataset('imdb');\n```\n\n----------------------------------------\n\nTITLE: Creating ZFS RAIDZ1 Storage Configuration\nDESCRIPTION: Command to create a ZFS RAIDZ1 storage pool using five EBS volumes, providing both performance benefits and protection against single volume failure.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/running-on-ec2.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nzfs create tank raidz /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 /dev/nvme4n1 /dev/nvme5n1\n```\n\n----------------------------------------\n\nTITLE: PostgresML Extension Name Declaration\nDESCRIPTION: The core extension name used to access PostgresML functionality in PostgreSQL\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/README.md#2025-04-19_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\npgml\n```\n\n----------------------------------------\n\nTITLE: Installing PgCat from Aptitude Repository on Ubuntu 22.04 LTS\nDESCRIPTION: This bash script adds the PostresML Aptitude repository to the system sources, updates the package list, and installs PgCat. It requires root privileges and is specific to Ubuntu 22.04 LTS.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgcat/installation.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\necho \"deb [trusted=yes] https://apt.postgresml.org $(lsb_release -cs) main\" | \\\nsudo tee -a /etc/apt/sources.list && \\\nsudo apt-get update && \\\nsudo apt install pgcat\n```\n\n----------------------------------------\n\nTITLE: Installing Auto-GPT with Python Virtual Environment\nDESCRIPTION: Commands to clone Auto-GPT repository, create a virtual environment, and install required dependencies.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-as-a-memory-backend-to-auto-gpt.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/postgresml/Auto-GPT\ncd Auto-GPT\ngit checkout stable-0.2.2\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: PgCat Configuration for LLM Support\nDESCRIPTION: TOML configuration file for PgCat setup with primary-only configuration optimized for Large Language Models\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/pooler.md#2025-04-19_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[general]\nhost = \"0.0.0.0\"\nport = 6432\nadmin_username = \"pgcat\"\nadmin_password = \"<secure password>\"\nserver_lifetime = 86400000\nidle_timeout = 86400000\n\n[pools.postgresml]\npool_mode = \"transaction\"\n\n[pools.postgresml.shards.0]\nservers = [\n  [\"<primary hostname or IP address>\", 5432, \"primary\"].\n]\ndatabase = \"postgresml\"\n\n[pools.postgresml.users.0]\nusername = \"postgresml_user\"\npassword = \"<secure password>\"\npool_size = 1\n```\n\n----------------------------------------\n\nTITLE: Enabling Client-side Timing in PostgreSQL\nDESCRIPTION: This command enables client-side timing in PostgreSQL, which is useful for benchmarking query execution times.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\n\\timing on\n```\n\n----------------------------------------\n\nTITLE: Running Vector Search Example with Node.js\nDESCRIPTION: This instruction guides executing the JavaScript code that demonstrates vector search with PostgresML SDK. It requires saving the sample code into a file (e.g., vector_search.js) and running it via Node.js from the terminal to perform a vector search operation on pre-defined documents.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/javascript/README.md#2025-04-19_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nnode vector_search.js\n```\n\n----------------------------------------\n\nTITLE: Text Generation with GPT2 Model on CPU\nDESCRIPTION: Running standard GPT2 model on CPU to demonstrate performance difference compared to quantized version.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"gpt2\",\n      \"device\": \"cpu\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML Dependencies with Python 3.11\nDESCRIPTION: Instructions for setting up a Python 3.11 virtual environment and installing PostgresML dependencies. Note that Python 3.12 is not fully compatible due to build issues with catboost and autogptq.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Use a Python 3.11 virtualenv with PostgresML:\n# $ virtualenv -p python3.11 pgml-venv\n# $ source pgml-vev/bin/activate\n# $ pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgresML Environment Variables\nDESCRIPTION: Environment variables configuration for PostgresML connection settings including host, port, credentials, and database details.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-as-a-memory-backend-to-auto-gpt.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nPOSTGRESML_HOST=localhost\nPOSTGRESML_PORT=5443\nPOSTGRESML_USERNAME=postgres\nPOSTGRESML_PASSWORD=\"\"\nPOSTGRESML_DATABASE=pgml_development\nPOSTGRESML_TABLENAME=autogpt_text_embeddings\n```\n\n----------------------------------------\n\nTITLE: Copying Remote Data for Bulk Processing\nDESCRIPTION: Creates a local copy of remote data to improve performance for bulk operations. This approach reduces latency by avoiding multiple remote calls when processing large datasets.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/foreign-data-wrappers.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE bulk_access_users (\n  LIKE production_tables.users\n);\n\nINSERT INTO bulk_access_users \nSELECT * FROM production_tables.users;\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL to Load PostgresML\nDESCRIPTION: Configuration line to add to postgresql.conf to ensure PostgresML is loaded as a shared library.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/building-from-source.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nshared_preload_libraries = 'pgml'\n```\n\n----------------------------------------\n\nTITLE: Starting PostgresML Server\nDESCRIPTION: Command to start the PostgresML server in release mode using cargo pgrx\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/blog/benchmarks/python_microservices_vs_postgresml/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo pgrx run --release\n```\n\n----------------------------------------\n\nTITLE: Installing and Initializing PGRX\nDESCRIPTION: This bash script installs the cargo-pgrx tool and initializes the PGRX environment for PostgresML development.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncargo install cargo-pgrx --version \"0.12.9\" --locked && \\\ncargo pgrx init # This will take a few minutes\n```\n\n----------------------------------------\n\nTITLE: PostgresML Package Dependencies List\nDESCRIPTION: Comprehensive list of Python package dependencies categorized by functionality, including ML frameworks, transformers, embeddings, ratings tools, and utilities. Some packages are platform-specific for Linux/NVIDIA hardware.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/requirements.txt#2025-04-19_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n# ML\ncatboost\nlightgbm\ntorch\ntorchaudio\ntorchvision\nxgboost\n\n# Transformers\naccelerate\nauto-gptq; sys_platform == 'linux' # only runs on nvidia hardware\nbitsandbytes\nctransformers\nhuggingface-hub\ndeepspeed\neinops\noptimum\npeft\ntokenizers\ntransformers\ntransformers-stream-generator\nxformers; sys_platform == 'linux' # only runs on nvidia hardware\nvllm; sys_platform == 'linux' # only runs on linux\n\n# Embeddings\nsentence-transformers\n\n# Ratings\nrouge\nsacrebleu\nsacremoses\nevaluate\ntrl\n\n# Utils\ndatasets\norjson\nlangchain\nevaluate\ntrl\n```\n\n----------------------------------------\n\nTITLE: Loading Serverless Models Information with Turbo Frame in HTML\nDESCRIPTION: This snippet uses a Turbo Frame to dynamically load information about available models for PostgresML Serverless engines. It displays a loading message while fetching the content.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/cloud/serverless.md#2025-04-19_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<turbo-frame id=\"serverless-models-turboframe\" src=\"/dashboard/serverless_models/turboframe?style=marketing\">\nLoading our current serverless models offered...\n</turbo-frame>\n```\n\n----------------------------------------\n\nTITLE: Creating Unique Index in PostgreSQL\nDESCRIPTION: SQL command to create a unique B-tree index on the Address column to prevent duplicates.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/README.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE UNIQUE INDEX ON usa_house_prices USING btree(\"Address\");\n```\n\n----------------------------------------\n\nTITLE: Setting PostgresML Connection Environment Variable\nDESCRIPTION: Command to set the KORVUS_DATABASE_URL environment variable for configuring the PostgresML connection string.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport KORVUS_DATABASE_URL=postgres://user:password@sql.cloud.postgresml.org:6432/korvus_database\n```\n\n----------------------------------------\n\nTITLE: Upgrading PostgresML Packages with Aptitude on Ubuntu\nDESCRIPTION: This code snippet shows how PostgresML uses the Aptitude package manager on Ubuntu to install and update their custom packages, configurations, and utilities. This approach enables consistent deployment across different cloud providers.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-going-multicloud.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\napt-get update && \\\napt-get upgrade\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Vector Search in Python\nDESCRIPTION: Demonstrates a Python class with an async vector search method, highlighting the limitations of traditional FFIs in handling Python classes and async functions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Database:\n     def __init__(self, connection_string: str):\n          # Create some connection\n\n     async def vector_search(self, query: str, model_id: int, splitter_id: int) -> str:\n          # Do some async search here\n          return result\n\nasync def main():\n     db = Database(CONNECTION_STRING)\n     result = await db.vector_search(\"What is the best way to do machine learning\", 1, 1)\n     if result != \"PostgresML\":\n          print(\"The model still needs more training\")\n     else:\n          print(\"The model is ready to go!\")\n```\n\n----------------------------------------\n\nTITLE: Querying pg_stat_sysinfo Table Structure\nDESCRIPTION: This snippet shows the structure of the pg_stat_sysinfo table, which stores system metrics in a generic format with metric name, dimensions, timestamp, and value.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pg-stat-sysinfo-a-postgres-extension-for-querying-system-statistics.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\n\\d pg_stat_sysinfo\n```\n\n----------------------------------------\n\nTITLE: Starting CLI Chat Interface\nDESCRIPTION: Command to start the command-line chat interface for interacting with the chatbot.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nLOG_LEVEL=ERROR pgml-chat --collection_name <collection_name> --stage chat --chat_interface cli\n```\n\n----------------------------------------\n\nTITLE: JSON Response from OpenSourceAI Inference\nDESCRIPTION: This JSON snippet shows the structure of the response returned by the OpenSourceAI chat_completions_create method. It includes the model's sentiment analysis result, along with metadata such as the model used, timestamp, and usage statistics.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"content\": \" Moderately negative \",\n                \"role\": \"assistant\"\n            }\n        }\n    ],\n    \"created\": 1711144872,\n    \"id\": \"b663f701-db97-491f-b186-cae1086f7b79\",\n    \"model\": \"santiadavani/fingpt-llama2-7b-chat\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": \"e36f4fa5-3d0b-e354-ea4f-950cd1d10787\",\n    \"usage\": {\n        \"completion_tokens\": 0,\n        \"prompt_tokens\": 0,\n        \"total_tokens\": 0\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Korvus SDK for JavaScript\nDESCRIPTION: Command to install the Korvus SDK for JavaScript using npm package manager.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i korvus\n```\n\n----------------------------------------\n\nTITLE: Configuring Replica Connection Settings\nDESCRIPTION: PostgreSQL configuration settings for replica server in postgresql.conf.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/replication.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nprimary_conninfo = 'host=<the host or IP of the primary> port=5432 user=replication_user password=<secure password>'\nrestore_command = 'pgbackrest --stanza=main archive-get %f \"%p\"'\n```\n\n----------------------------------------\n\nTITLE: Setting Up PostgresML Dashboard\nDESCRIPTION: These bash commands set up the PostgresML dashboard by setting the database URL and running migrations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nexport DATABASE_URL=postgres://localhost:28815/pgml\nsqlx migrate run\n```\n\n----------------------------------------\n\nTITLE: Creating an Index on Customer IDs\nDESCRIPTION: Creates a standard PostgreSQL B-tree index on the customer ID column to enable fast lookups when personalizing search results for specific customers.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/personalize-embedding-results-with-application-data-in-your-database.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE INDEX customers_id_idx ON customers (id);\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables\nDESCRIPTION: Example of environment variables to be set in the .env file, including API keys and database credentials.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=<OPENAI_API_KEY>\nDATABASE_URL=<POSTGRES_DATABASE_URL starts with postgres://>\nMODEL=Alibaba-NLP/gte-base-en-v1.5\nSYSTEM_PROMPT=\"You are an assistant to answer questions about an open source software named PostgresML. Your name is PgBot. You are based out of San Francisco, California.\"\nBASE_PROMPT=\"Given relevant parts of a document and a question, create a final answer.\\ \n                Include a SQL query in the answer wherever possible. \\\n                Use the following portion of a long document to see if any of the text is relevant to answer the question.\\\n                \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\n \\\n                If the context is empty then ask for clarification and suggest user to send an email to team@postgresml.org or join PostgresML [Discord](https://discord.gg/DmyJP3qJ7U).\"\n```\n\n----------------------------------------\n\nTITLE: Creating Hash Partitioned House Prices Table\nDESCRIPTION: Creates a parent table for house prices data partitioned by address using hash strategy.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/partitioning.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE usa_house_prices_partitioned (\n  \"Avg. Area Income\" REAL NOT NULL,\n  \"Avg. Area House Age\" REAL NOT NULL,\n  \"Avg. Area Number of Rooms\" REAL NOT NULL,\n  \"Avg. Area Number of Bedrooms\" REAL NOT NULL,\n  \"Area Population\" REAL NOT NULL,\n  \"Price\" REAL NOT NULL,\n  \"Address\" VARCHAR NOT NULL\n) PARTITION BY HASH(\"Address\");\n```\n\n----------------------------------------\n\nTITLE: Implementing Database Struct with PyO3 for Python\nDESCRIPTION: Demonstrates how to use PyO3 to create a Python-compatible version of the Database struct, including async functionality.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\nuse pyo3::prelude::*;\n\nstruct Database{\n     connection_string: String\n}\n\n#[pymethods]\nimpl Database {\n     #[new]\n     pub fn new(connection_string: String) -> Self {\n          // The actual connection process has been removed\n          Self {\n               connection_string\n          }\n     }\n\n     pub fn vector_search<'a>(&self, py: Python<'a>, query: String, model_id: i64, splitter_id: i64) -> PyResult<&'a PyAny> {\n          pyo3_asyncio::tokio::future_into_py(py, async move {\n               // Do some async vector search\n               Ok(result)\n          })\n     }\n}\n\n/// A Python module implemented in Rust.\n#[pymodule]\nfn pgml(_py: Python, m: &PyModule) -> PyResult<()> {\n     m.add_class::<Database>()?;\n     Ok(())\n}\n```\n\n----------------------------------------\n\nTITLE: Disabling a Pipeline in Korvus\nDESCRIPTION: This snippet demonstrates how to disable a pipeline in a Korvus collection. Disabling a pipeline prevents it from running automatically on document upserts, but retains existing data.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/api/pipelines.md#2025-04-19_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst pipeline = korvus.newPipeline(\"test_pipeline\")\nconst collection = korvus.newCollection(\"test_collection\")\nawait collection.disable_pipeline(pipeline)\n```\n\nLANGUAGE: python\nCODE:\n```\npipeline = Pipeline(\"test_pipeline\")\ncollection = Collection(\"test_collection\")\nawait collection.disable_pipeline(pipeline)\n```\n\nLANGUAGE: rust\nCODE:\n```\nlet mut collection = Collection::new(\"test_collection\", None)?;\nlet mut pipeline = Pipeline::new(\"test_pipeline\", None)?;\ncollection.disable_pipeline(&mut pipeline).await?;\n```\n\nLANGUAGE: c\nCODE:\n```\nCollectionC * collection = korvus_collectionc_new(\"test_collection\", NULL);\nPipelineC * pipeline = korvus_pipelinec_new(\"test_pipeline\",  NULL);\nkorvus_collectionc_disable_pipeline(collection, pipeline);\n```\n\n----------------------------------------\n\nTITLE: Basic ZFS Snapshot Transfer over SSH\nDESCRIPTION: Initial attempt at transferring ZFS snapshot between cloud providers using basic SSH piping. This method proved too slow at ~30MB/second.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-migrated-from-aws-to-gcp-with-minimal-downtime.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nzfs send tank/pgdata@snapshot | ssh ubuntu@machine \\\nzfs recv tank/pgdata@snapshot\n```\n\n----------------------------------------\n\nTITLE: Type Representations for Macro Processing\nDESCRIPTION: Enum definitions for supported Rust types and output types that can be processed by the procedural macros. These provide a type-safe way to represent and manipulate Rust types during code generation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_9\n\nLANGUAGE: rust\nCODE:\n```\npub enum SupportedType {\n     Reference(Box<SupportedType>),\n     str,\n     String,\n     Vec(Box<SupportedType>),\n     HashMap((Box<SupportedType>, Box<SupportedType>)),\n     Option(Box<SupportedType>),\n     Tuple(Vec<SupportedType>),\n     S, // Self\n     i64,\n     i32,\n     f64,\n     // Other omitted types\n}\n\npub enum OutputType {\n     Result(SupportedType),\n     Default,\n     Other(SupportedType),\n}\n```\n\n----------------------------------------\n\nTITLE: Running the PostgresML Example with Node.js\nDESCRIPTION: The shell commands are employed to build the project and run the JavaScript file using Node.js. The example performs a text classification task, requiring input text as a parameter. The build process uses npm scripts, and the Node.js file is located in the distribution directory.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/javascript/examples/webpack/README.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm run build\nnode dist/index.js \\\"I love PostgresML\\\"\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML SDK - NPM\nDESCRIPTION: Command to install the PostgresML JavaScript SDK package from npm.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/getting-started/connect-your-app.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i pgml\n```\n\n----------------------------------------\n\nTITLE: Defining a Database Struct in Rust\nDESCRIPTION: Shows a basic Rust implementation of a Database struct with a constructor and an async vector_search method.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\nstruct Database{\n    connection_string: String\n}\n\nimpl Database {\n     pub fn new(connection_string: String) -> Self {\n          // The actual connection process has been removed \n          Self {\n               connection_string\n          }\n     }\n\n     pub async fn vector_search(&self, query: String, model_id: i64, splitter_id: i64) -> String {\n          // Do some async vector search\n          result\n     }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up PostgreSQL Replica\nDESCRIPTION: Series of commands to set up and configure a PostgreSQL replica server.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/replication.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nPGPASSWORD=<secure password> pg_basebackup \\\n    -h <the host or IP address of the primary> \\\n    -p 5432 \\\n    -U replication_user \\\n    -D /var/lib/postgresql/14/main\n```\n\n----------------------------------------\n\nTITLE: Running PostgresML Dashboard in Development Mode\nDESCRIPTION: This bash command runs the PostgresML dashboard in development mode with automatic recompilation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncargo watch --exec run\n```\n\n----------------------------------------\n\nTITLE: Text Generation with MPT Model\nDESCRIPTION: Implementing MPT-7B Storywriter model for text generation with specific model_type configuration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/MPT-7B-Storywriter-GGML\",\n      \"model_type\": \"mpt\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Installing CUDA and NVIDIA Drivers\nDESCRIPTION: Commands to install CUDA and NVIDIA drivers for GPU support.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/README.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl -LsSf \\\n    https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb \\\n    -o /tmp/cuda-keyring.deb\nsudo dpkg -i /tmp/cuda-keyring.deb\nsudo apt update\nsudo apt install -y cuda\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Pipeline\nDESCRIPTION: Command and example output for running the dbt pipeline\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/examples/dbt/embeddings/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndbt run\n```\n\n----------------------------------------\n\nTITLE: Initializing pgrx for PostgreSQL 14\nDESCRIPTION: Command to initialize pgrx for use with PostgreSQL 14 installed on Ubuntu 22.04.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/building-from-source.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncargo pgrx init --pg14 /usr/bin/pg_config\n```\n\n----------------------------------------\n\nTITLE: Restoring PostgreSQL Backup\nDESCRIPTION: Command to restore the latest backup using pgBackRest with delta option for optimized file comparison.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/backups.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npgbackrest restore --stanza=main --delta\n```\n\n----------------------------------------\n\nTITLE: Optimized ZFS Transfer with Buffer and Compression\nDESCRIPTION: Improved ZFS snapshot transfer using mbuffer for optimal I/O buffering and pbzip2 for compression, achieving speeds of 200MB/second. Uses 12M buffer size and 2G memory allocation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-migrated-from-aws-to-gcp-with-minimal-downtime.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nzfs send tank/pgdata@snapshot | pbzip2 | mbuffer -s 12M -m 2G | ssh ubuntu@gcp \\\nmbuffer -s 12M -m 2G | pbzip2 -d | zfs recv tank/pgdata@snapshot\n```\n\n----------------------------------------\n\nTITLE: Configuring Servers in a PgCat Shard\nDESCRIPTION: Example of how to define servers for a shard in PgCat, specifying the host, port, and role (primary or replica) for each server. This format allows you to set up multiple database servers within a single shard.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgcat/configuration.md#2025-04-19_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\nservers = [\n    [\"10.0.0.1\", 5432, \"primary\"],\n    [\"replica-1.internal-dns.net\", 5432, \"replica\"],\n]\n```\n\n----------------------------------------\n\nTITLE: Creating a Publication for Logical Replication in PostgreSQL\nDESCRIPTION: This SQL command creates a publication named 'postgresml_users' for the 'users' and 'blog_posts' tables in the primary database. This is the first step in setting up logical replication.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/logical-replication/README.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE PUBLICATION postgresml_users\nFOR TABLE users, blog_posts;\n```\n\n----------------------------------------\n\nTITLE: Truncating a Table to Refresh Data\nDESCRIPTION: SQL command to truncate (remove all rows from) a table before importing fresh data. This helps avoid duplicate entries when refreshing data in PostgresML.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/copy.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nTRUNCATE your_table;\n```\n\n----------------------------------------\n\nTITLE: Configuring Host-Based Authentication\nDESCRIPTION: Configuration line for pg_hba.conf to allow replication connections.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/replication.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nhost replication replication_user 0.0.0.0/0 scram-sha-256\n```\n\n----------------------------------------\n\nTITLE: Defining Source Schema\nDESCRIPTION: Schema configuration for source documents table in models/schema.yml\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/examples/dbt/embeddings/README.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n  sources:\n  - name: <schema>\n    tables:\n      - name: <documents table>\n```\n\n----------------------------------------\n\nTITLE: Generated Method Implementations from custom_derive_methods\nDESCRIPTION: The expanded code generated by the custom_derive_methods macro, which creates method implementations for both Python and JavaScript wrappers with appropriate language-specific handling for method calls and async functions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_7\n\nLANGUAGE: rust\nCODE:\n```\n#[pymethods]\nimpl DatabasePython {\n     #[new]\n     pub fn new(connection_string: String) -> Self {\n          // The actual connection process has been removed\n          Self::from(Database::new(connection_string))\n     }\n\n     pub fn vector_search<'a>(&self, py: Python<'a>, query: String, model_id: i64, splitter_id: i64) -> PyResult<&'a PyAny> {\n          let wrapped = self.wrapped.clone();\n          pyo3_asyncio::tokio::future_into_py(py, async move {\n               // Do some async vector search\n               let x = wrapped.vector_search(query, model_id, splitter_id).await;\n               Ok(x)\n          })\n     }\n}\n\nimpl DatabaseJavascript {\n    pub fn new<'a>(\n        mut cx: neon::context::FunctionContext<'a>,\n    ) -> neon::result::JsResult<'a, JsObject> {\n        let arg0 = cx.argument::<JsString>(0usize as i32)?;\n        let arg0 = <String>::from_js_type(&mut cx, arg0)?;\n        let x = Database::new(&arg0);\n        let x = x.expect(\"Error in rust method\");\n        let x = Self::from(x);\n        x.into_js_result(&mut cx)\n    }\n\n    pub fn vector_search<'a>(\n        mut cx: neon::context::FunctionContext<'a>,\n    ) -> neon::result::JsResult<'a, neon::types::JsPromise> {\n        use neon::prelude::*;\n        use core::ops::Deref;\n        let this = cx.this();\n        let s: neon::handle::Handle<\n            neon::types::JsBox<std::cell::RefCell<DatabaseJavascript>>,\n        > = this.get(&mut cx, \"s\")?;\n        let wrapped = (*s).deref().borrow();\n        let wrapped = wrapped.wrapped.clone();\n        let arg0 = cx.argument::<neon::types::JsString>(0)?;\n        let arg0 = <String>::from_js_type(&mut cx, arg0)?;\n        let arg1 = cx.argument::<JsNumber>(1);\n        let arg1 = <i64>::from_js_type(&mut cx, arg1);\n        let arg2 = cx.argument::<JsNumber>(2);\n        let arg2 = <i64>::from_js_type(&mut cx, arg2);\n        let channel = cx.channel();\n        let (deferred, promise) = cx.promise();\n        deferred\n            .try_settle_with(\n                &channel,\n                move |mut cx| {\n                    let runtime = crate::get_or_set_runtime();\n                    let x = runtime.block_on(wrapped.vector_search(&arg0, arg1, arg2));\n                    let x = x.expect(\"Error in rust method\");\n                    x.into_js_result(&mut cx)\n                },\n            )\n            .expect(\"Error sending js\");\n        Ok(promise)\n    }\n}\n\nimpl IntoJsResult for DatabaseJavascript {\n    type Output = neon::types::JsObject;\n    fn into_js_result<'a, 'b, 'c: 'b, C: neon::context::Context<'c>>(\n        self,\n        cx: &mut C,\n    ) -> neon::result::JsResult<'b, Self::Output> {\n        use neon::object::Object;\n        let obj = cx.empty_object();\n        let s = cx.boxed(std::cell::RefCell::new(self));\n        obj.set(cx, \"s\", s)?;\n        let f: neon::handle::Handle<neon::types::JsFunction> = neon::types::JsFunction::new(\n            cx,\n            DatabaseJavascript::new,\n        )?;\n        obj.set(cx, \"new\", f)?;\n        let f: neon::handle::Handle<neon::types::JsFunction> = neon::types::JsFunction::new(\n            cx,\n            DatabaseJavascript::vector_search,\n        )?;\n        obj.set(cx, \"vector_search\", f)?;\n        Ok(obj)\n    }\n}\n\nimpl neon::types::Finalize for DatabaseJavascript {}\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML Package\nDESCRIPTION: Command to install PostgresML for PostgreSQL 14 on Ubuntu.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y postgresml-14\n```\n\n----------------------------------------\n\nTITLE: Starting Auto-GPT with PostgresML Backend\nDESCRIPTION: Command to launch Auto-GPT using PostgresML as the memory backend.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-as-a-memory-backend-to-auto-gpt.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython -m autogpt -m postgresml\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Falcon Model\nDESCRIPTION: Using Falcon-40B model with remote code trust enabled for text generation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_9\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/falcon-40b-instruct-GPTQ\",\n      \"trust_remote_code\": true\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Cloning PostgresML Source Code from GitHub\nDESCRIPTION: Commands to clone the PostgresML repository and initialize its submodules.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/building-from-source.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/postgresml/postgresml && \\\ncd postgresml && \\\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Installing Korvus SDK for C\nDESCRIPTION: Steps to clone the Korvus repository, navigate to the C directory, and build the bindings for C integration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/postgresml/korvus\ncd korvus/korvus/c\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake bindings\n```\n\n----------------------------------------\n\nTITLE: Running Python Prediction Service\nDESCRIPTION: Commands to start the Python prediction service with Gunicorn and run performance benchmarking using Apache Bench\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/blog/benchmarks/python_microservices_vs_postgresml/README.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nOMP_NUM_THREADS=2 gunicorn predict:app -w 5 -t 2\n```\n\nLANGUAGE: bash\nCODE:\n```\nab -n 10000 -c 10 -T application/json -k -p ab.txt http://localhost:8000/\n```\n\n----------------------------------------\n\nTITLE: Creating User Mapping for PostgresML Authentication\nDESCRIPTION: SQL command to map the current PostgreSQL user to credentials for the PostgresML server. The user and password values should match those from your PostgresML database connection string.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/pgml-rds-proxy/README.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE USER MAPPING\nFOR CURRENT_USER\nSERVER postgresml\nOPTIONS (\n    user 'pg',\n    password 'ml'\n);\n```\n\n----------------------------------------\n\nTITLE: Using du command with apparent-size flag in Bash\nDESCRIPTION: Command line option for the 'du' utility that displays the apparent size of files rather than their actual disk usage. This is particularly useful when dealing with sparse files, internal fragmentation, or filesystems with compression features like ZFS.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-migrated-from-aws-to-gcp-with-minimal-downtime.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--apparent-size\n    print  apparent  sizes, rather than disk usage; although the apparent size is usually smaller, it may be\n    larger due to holes in ('sparse') files, internal fragmentation, indirect blocks, and the like\n```\n\n----------------------------------------\n\nTITLE: Setting up PostgresML database with sqlx-cli in Bash\nDESCRIPTION: This snippet installs sqlx-cli version 0.6.3 and sets up the database for the PostgresML dashboard. It uses Cargo, the Rust package manager, to install sqlx-cli and run the database setup command.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncargo install sqlx-cli --version 0.6.3 && \\\ncargo sqlx database setup\n```\n\n----------------------------------------\n\nTITLE: Adding Serial ID Column in PostgreSQL\nDESCRIPTION: This snippet shows how to add a serial ID column as a primary key to a table for easier data manipulation and indexing.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_15\n\nLANGUAGE: postgresql\nCODE:\n```\nALTER TABLE pgml.amazon_us_reviews\nADD COLUMN id SERIAL PRIMARY KEY;\n```\n\n----------------------------------------\n\nTITLE: PostgresML Sentiment Analysis - CPU Only\nDESCRIPTION: Execution using CPU only, demonstrating performance difference compared to GPU acceleration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/mindsdb-vs-postgresml.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    inputs => ARRAY[\n        'Are GPUs really worth it? Sometimes they are more expensive than the rest of the computer combined.'\n    ],\n    task   => '{\n        \"task\": \"text-classification\", \n        \"model\": \"cardiffnlp/twitter-roberta-base-sentiment\",\n        \"device\": \"cpu\"\n    }'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring pgBackRest\nDESCRIPTION: pgBackRest configuration settings for WAL archiving to S3.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/replication.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\n[main]\npg1-path=/var/lib/postgresql/14/main/\n\n[global]\nprocess-max=4\nrepo1-path=/wal-archive/main\nrepo1-s3-bucket=postgresml-tutorial-wal-archive\nrepo1-s3-endpoint=s3.us-west-2.amazonaws.com\nrepo1-s3-region=us-west-2\nrepo1-s3-key=<YOUR AWS ACCESS KEY ID>\nrepo1-s3-key-<YOUR AWS SECRET ACCESS KEY>\nrepo1-type=s3\nstart-fast=y\ncompress-type=lz4\narchive-mode-check=n\narchive-check=n\n\n[global:archive-push]\ncompress-level=3\n```\n\n----------------------------------------\n\nTITLE: Querying PgCat Admin Database for Server Statistics\nDESCRIPTION: SQL command to retrieve server statistics from PgCat's admin database, including prepare cache hit and miss ratios.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/making-postgres-30-percent-faster-in-production.md#2025-04-19_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSHOW SERVERS\n```\n\n----------------------------------------\n\nTITLE: Setting Database URL Environment Variable\nDESCRIPTION: Shows how to set the PostgresML database URL environment variable for OpenSourceAI\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/opensourceai.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport KORVUS_DATABASE_URL=postgres://user:pass@.db.cloud.postgresml.org:6432/pgml\n```\n\n----------------------------------------\n\nTITLE: Creating S3 Bucket for WAL Archive\nDESCRIPTION: AWS CLI command to create an S3 bucket for storing WAL archives.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/replication.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\naws s3api create-bucket \\\n    --bucket postgresml-tutorial-wal-archive \\\n    --create-bucket-configuration=\"LocationConstraint=us-west-2\"\n```\n\n----------------------------------------\n\nTITLE: Querying First 5 Rows of Amazon US Reviews Dataset\nDESCRIPTION: This SQL query retrieves the first 5 rows from the Amazon US Reviews dataset, providing a glimpse of the data structure and content.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT *\nFROM pgml.amazon_us_reviews\nLIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Cloning PostgresML Repository\nDESCRIPTION: Commands to clone the PostgresML source code from GitHub repository.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/postgresml/postgresml\n```\n\n----------------------------------------\n\nTITLE: Developer Setup Commands\nDESCRIPTION: Commands for setting up the development environment using Poetry.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-apps/pgml-chat/README.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/postgresml/postgresml\ncd postgresml/pgml-apps/pgml-chat\npoetry shell\npoetry install\npip install .\n```\n\n----------------------------------------\n\nTITLE: Installing pgml-chat Package\nDESCRIPTION: Command to install the pgml-chat package using pip in a virtual environment.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pgml-chat\n```\n\n----------------------------------------\n\nTITLE: Query Analysis with EXPLAIN ANALYZE\nDESCRIPTION: Using PostgreSQL's EXPLAIN ANALYZE command to evaluate query performance and optimization opportunities across multiple table joins.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nEXPLAIN ANALYZE\n```\n\n----------------------------------------\n\nTITLE: Creating Flights Delay Table Schema\nDESCRIPTION: SQL command to create the flights_delay_mat table with columns for flight delay prediction features\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/blog/benchmarks/python_microservices_vs_postgresml/README.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE TABLE public.flights_delay_mat (\n    depdelayminutes real,\n    year real NOT NULL,\n    quarter real NOT NULL,\n    month real NOT NULL,\n    distance real NOT NULL,\n    dayofweek real NOT NULL,\n    dayofmonth real NOT NULL,\n    flight_number_operating_airline real NOT NULL,\n    originairportid real NOT NULL,\n    destairportid real NOT NULL,\n    tail_number real NOT NULL\n);\n```\n\n----------------------------------------\n\nTITLE: Building PgCat from Source using Cargo (Rust)\nDESCRIPTION: This command builds PgCat from source using Cargo, the Rust package manager. It produces an executable in the target/release/pgcat directory.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgcat/installation.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo build --release\n```\n\n----------------------------------------\n\nTITLE: SDK Migration Utility\nDESCRIPTION: Provides a migration function to ensure compatibility between different SDK versions, helping smooth transitions during upgrades.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/README.md#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pgml import migrate\nawait migrate()\n```\n\n----------------------------------------\n\nTITLE: Compiling PostgresML Extension\nDESCRIPTION: Commands to navigate to the extension directory and compile the PostgresML extension using cargo pgrx.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/building-from-source.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd pgml-extension && \\\ncargo pgrx package\n```\n\n----------------------------------------\n\nTITLE: Implementing a Procedural Macro for Python Interoperability\nDESCRIPTION: Implementation of a procedural macro that generates Python-compatible wrapper code for Rust structs. It creates a new struct with the original type wrapped inside and applies the pyclass attribute.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_6\n\nLANGUAGE: rust\nCODE:\n```\n#[proc_macro_derive(custom_derive)]\npub fn custom_derive(input: proc_macro::TokenStream) -> proc_macro::TokenStream {\n     let parsed = parse_macro_input!(input as DeriveInput);\n     let name_ident = format_ident!(\"{}{}\", parsed.ident, \"Python\");\n     let wrapped_type_ident = parsed.ident;\n     let expanded = quote! {\n          #[pyclass]\n          pub struct #name_ident {\n               wrapped: #wrapped_type_ident\n          }\n     };\n     proc_macro::TokenStream::from(expanded)\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting to PostgresML Database with psql\nDESCRIPTION: A shell command to connect to the PostgresML database using psql client. This connects to the PostgreSQL database running on localhost at port 5432 with the specified credentials.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/tabular-data.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npsql -h localhost -p 5432 -U postgres -d postgresml\n```\n\n----------------------------------------\n\nTITLE: Viewing pgml-components Help Documentation\nDESCRIPTION: Command to display the help documentation for the pgml-components CLI, listing all available commands and options.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncargo pgml-components --help\n```\n\n----------------------------------------\n\nTITLE: Dashboard Database Configuration\nDESCRIPTION: Commands to create and configure a database for the PostgresML dashboard.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncreatedb pgml_dashboard && \\\npsql -d pgml_dashboard -c 'CREATE EXTENSION pgml;'\n```\n\n----------------------------------------\n\nTITLE: Creating pgvector Extension\nDESCRIPTION: SQL command to create the pgvector extension in the database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/README.md#2025-04-19_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\npostgresml=# CREATE EXTENSION vector;\nCREATE EXTENSION\n```\n\n----------------------------------------\n\nTITLE: Configuring Database Connections in YAML for dbt\nDESCRIPTION: This YAML configuration sets up database connections for development and production environments in a dbt project. It includes details such as host, port, user, password, and database name for PostgreSQL connections.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npgml_flow:\n  outputs:\n\n    dev:\n      type: postgres\n      threads: 1\n      host: 127.0.0.1\n      port: 5433\n      user: postgres\n      pass: \"\"\n      dbname: pgml_development\n      schema: <schema_name>\n    \n    prod:\n      type: postgres\n      threads: [1 or more]\n      host: [host]\n      port: [port]\n      user: [prod_username]\n      pass: [prod_password]\n      dbname: [dbname]\n      schema: [prod_schema]\n\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Enabling Debug Logging in JavaScript SDK\nDESCRIPTION: Shows how to enable debug logging in the PostgresML JavaScript SDK to view all SQL queries being executed.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/the-1.0-sdk-is-here.md#2025-04-19_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\npgml.init_logger(\"DEBUG\");\n```\n\n----------------------------------------\n\nTITLE: Complex Keyword Search with Multiple Terms\nDESCRIPTION: Demonstrates advanced keyword search using multiple terms with AND operator and the indexed tsvector column.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * \nFROM documents\nWHERE title_and_body_text @@ to_tsquery('english', 'another & second');\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Pipeline for PostgresML Project\nDESCRIPTION: This command runs the dbt pipeline for the PostgresML project. It executes all defined models and tests, producing an output that shows the progress and results of each step.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndbt run\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML Package\nDESCRIPTION: This bash command installs the PostgresML package from the generated .deb file.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\napt-get install ./postgresql-pgml-12_0.0.4-ubuntu20.04-amd64.deb\n```\n\n----------------------------------------\n\nTITLE: Importing Tables from Production Database\nDESCRIPTION: Creates a schema and imports all tables from the public schema of the remote database. This eliminates the need to specify table schemas for each query by creating foreign tables that mirror the remote database structure.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/foreign-data-wrappers.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE SCHEMA production_tables;\n\nIMPORT FOREIGN SCHEMA public\nFROM SERVER production_db\nINTO production_tables;\n```\n\n----------------------------------------\n\nTITLE: Creating Replication User\nDESCRIPTION: SQL command to create a user with replication privileges.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/replication.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE ROLE replication_user PASSWORD '<secure password>' LOGIN REPLICATION;\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: Comprehensive list of Python package dependencies with exact version numbers. Includes core packages like aiohttp, pandas, and huggingface-hub along with their dependencies. Used for consistent dependency management and reproducible environments.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/blog/benchmarks/hf_pinecone_vs_postgresml/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naiohttp==3.8.5\naiosignal==1.3.1\nasync-timeout==4.0.3\nattrs==23.1.0\nblack==23.7.0\ncertifi==2023.7.22\ncharset-normalizer==3.2.0\nclick==8.1.6\ndatasets==2.14.4\ndill==0.3.7\ndnspython==2.4.2\nfilelock==3.12.2\nfrozenlist==1.4.0\nfsspec==2023.6.0\nhuggingface-hub==0.16.4\nidna==3.4\nloguru==0.7.0\nmarkdown-it-py==3.0.0\nmdurl==0.1.2\nmultidict==6.0.4\nmultiprocess==0.70.15\nmypy-extensions==1.0.0\nnumpy==1.25.2\npackaging==23.1\npandas==2.0.3\npathspec==0.11.2\npgml==0.8.1\npinecone-client==2.2.2\nplatformdirs==3.10.0\npsycopg==3.1.10\npsycopg-pool==3.1.7\npyarrow==12.0.1\nPygments==2.16.1\npython-dateutil==2.8.2\npython-dotenv==1.0.0\npytz==2023.3\nPyYAML==6.0.1\nrequests==2.31.0\nrich==13.5.2\nsix==1.16.0\ntomli==2.0.1\ntqdm==4.66.1\ntyping_extensions==4.7.1\ntzdata==2023.3\nurllib3==2.0.4\nxxhash==3.3.0\nyarl==1.9.2\n```\n\n----------------------------------------\n\nTITLE: Configuring pgBackRest with AWS IAM Integration\nDESCRIPTION: Configuration snippet for pgBackRest to use EC2 IAM role authentication instead of static AWS credentials. This enables automatic credential management through the EC2 API.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/running-on-ec2.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n[global]\nrepo1-s3-key-type=auto\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML Prerequisites via Script\nDESCRIPTION: A shell command for downloading and executing the PostgresML installation script, which installs all dependencies for manual installation. This is an alternative to using Docker.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/tabular-data.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl -sSL https://install.postgresml.org | sh\n```\n\n----------------------------------------\n\nTITLE: Discord Environment Configuration\nDESCRIPTION: Environment variable required for Discord integration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-apps/pgml-chat/README.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nDISCORD_BOT_TOKEN=<DISCORD_BOT_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Sample Output of dbt Pipeline Execution\nDESCRIPTION: This snippet shows the expected output when running the dbt pipeline. It displays the execution progress, including the creation of view models, incremental models, and table models, along with their respective execution times.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n22:29:58  Running with dbt=1.5.2\n22:29:58  Registered adapter: postgres=1.5.2\n22:29:58  Unable to do partial parsing because a project config has changed\n22:29:59  Found 7 models, 10 tests, 0 snapshots, 0 analyses, 307 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups\n22:29:59  \n22:29:59  Concurrency: 1 threads (target='dev')\n22:29:59  \n22:29:59  1 of 7 START sql view model test_collection_1.characters ....................... [RUN]\n22:29:59  1 of 7 OK created sql view model test_collection_1.characters .................. [CREATE VIEW in 0.11s]\n22:29:59  2 of 7 START sql incremental model test_collection_1.models .................... [RUN]\n22:29:59  2 of 7 OK created sql incremental model test_collection_1.models ............... [INSERT 0 1 in 0.15s]\n22:29:59  3 of 7 START sql incremental model test_collection_1.splitters ................. [RUN]\n22:30:00  3 of 7 OK created sql incremental model test_collection_1.splitters ............ [INSERT 0 1 in 0.07s]\n22:30:00  4 of 7 START sql incremental model test_collection_1.chunks .................... [RUN]\n22:30:00  4 of 7 OK created sql incremental model test_collection_1.chunks ............... [INSERT 0 0 in 0.08s]\n22:30:00  5 of 7 START sql incremental model test_collection_1.embedding_36b7e ........... [RUN]\n22:30:00  5 of 7 OK created sql incremental model test_collection_1.embedding_36b7e ...... [INSERT 0 0 in 0.08s]\n22:30:00  6 of 7 START sql incremental model test_collection_1.transforms ................ [RUN]\n22:30:00  6 of 7 OK created sql incremental model test_collection_1.transforms ........... [INSERT 0 1 in 0.07s]\n22:30:00  7 of 7 START sql table model test_collection_1.vector_search ................... [RUN]\n22:30:05  7 of 7 OK created sql table model test_collection_1.vector_search .............. [SELECT 2 in 4.81s]\n22:30:05  \n22:30:05  Finished running 1 view model, 5 incremental models, 1 table model in 0 hours 0 minutes and 5.59 seconds (5.59s).\n22:30:05  \n22:30:05  Completed successfully\n22:30:05  \n22:30:05  Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7\n```\n\n----------------------------------------\n\nTITLE: Implementing Backup Lock Protection\nDESCRIPTION: Bash commands to implement a lock mechanism using flock to prevent backup overruns.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/backups.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntouch /tmp/pgbackrest-flock-lock\nflock /tmp/pgbackrest-flock-lock pgbackrest backup --stanza=main\n```\n\n----------------------------------------\n\nTITLE: Executing dbt Pipeline for PostgresML Project\nDESCRIPTION: Command to run the dbt pipeline for the PostgresML project. This command initializes the execution of all defined models and transformations in the dbt project.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndbt run\n```\n\n----------------------------------------\n\nTITLE: Adding a New Component with pgml-components\nDESCRIPTION: Command to create a new frontend component at the specified path, generating all necessary files including template, controller, stylesheet, and Rust module.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncargo pgml-components add component <path>\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search with PostgreSQL\nDESCRIPTION: This snippet describes how to perform a semantic search on a collection of documents by loading the Quora dataset, creating a collection in PostgreSQL, and using embeddings generated from the `intfloat/e5-small-v2` model to retrieve semantically similar documents to a query.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/examples/README.md#2025-04-19_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Setting Pipeline Variables\nDESCRIPTION: Configuration parameters for text splitting and embedding tasks\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/examples/dbt/embeddings/README.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nvars:\n  splitter_name: \"recursive_character\"\n  splitter_parameters: {\"chunk_size\": 100, \"chunk_overlap\": 20}\n  task: \"embedding\"\n  model_name: \"intfloat/e5-small-v2\"\n  query_string: 'Lorem ipsum 3'\n  limit: 2\n```\n\n----------------------------------------\n\nTITLE: Querying Amazon Review Data with PostgreSQL\nDESCRIPTION: This query selects all columns from the pgml.amazon_us_reviews table, limiting the result to 5 rows. It's used to provide a quick overview of the data structure in the example dataset.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/tuning-vector-recall-while-generating-query-embeddings-in-the-database.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT *\nFROM pgml.amazon_us_reviews\nLIMIT 5;\n```\n\n----------------------------------------\n\nTITLE: Sample Output from Korvus Vector Search\nDESCRIPTION: This snippet shows the output from the Korvus vector search operation, displaying the matching document with its score.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/a-speed-comparison-of-the-most-popular-retrieval-systems-for-rag.md#2025-04-19_snippet_2\n\nLANGUAGE: txt\nCODE:\n```\n[{'chunk': 'The hidden value is 1000', 'document': {'id': '1'}, 'rerank_score': None, 'score': 0.7257088435203306}]\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: Commands for installing Python dependencies using virtual environment.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nvirtualenv pgml-venv && \\\nsource pgml-venv/bin/activate && \\\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing PgCat via APT\nDESCRIPTION: Command to install PgCat pooler on Ubuntu 22.04 using APT package manager\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/pooler.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y pgcat\n```\n\n----------------------------------------\n\nTITLE: Running PostgresML Integration Tests\nDESCRIPTION: These bash commands run the integration tests for PostgresML by starting a PostgreSQL instance and executing a test SQL file.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncargo pgrx run --release\npsql -h localhost -p 28813 -d pgml -f tests/test.sql -P pager\n```\n\n----------------------------------------\n\nTITLE: Python Requirements Definition\nDESCRIPTION: A requirements.txt file listing all Python package dependencies with pinned versions needed for the PostgresML project. Includes ML frameworks, web servers, data processing libraries and utilities.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/requirements.amd64.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate==1.2.1\naiohappyeyeballs==2.4.4\naiohttp==3.11.11\naiohttp-cors==0.7.0\naiosignal==1.3.2\nairportsdata==20241001\nannotated-types==0.7.0\nanyio==4.8.0\nastor==0.8.1\nattrs==24.3.0\nauto_gptq==0.7.1\nbitsandbytes==0.45.0\nblake3==1.0.2\ncachetools==5.5.0\ncatboost==1.2.7\ncertifi==2024.12.14\ncharset-normalizer==3.4.1\nclick==8.1.8\ncloudpickle==3.1.1\ncolorama==0.4.6\ncoloredlogs==15.0.1\ncolorful==0.5.6\ncompressed-tensors==0.8.1\ncontourpy==1.3.1\nctransformers==0.2.27\ncycler==0.12.1\ndatasets==3.2.0\ndeepspeed==0.16.2\ndepyf==0.18.0\ndill==0.3.8\ndiskcache==5.6.3\ndistlib==0.3.9\ndistro==1.9.0\neinops==0.8.0\nevaluate==0.4.3\nfastapi==0.115.6\nfilelock==3.16.1\nfonttools==4.55.3\nfrozenlist==1.5.0\nfsspec==2024.9.0\ngekko==1.2.1\ngguf==0.10.0\ngoogle-api-core==2.24.0\ngoogle-auth==2.37.0\ngoogleapis-common-protos==1.66.0\ngraphviz==0.20.3\ngreenlet==3.1.1\ngrpcio==1.69.0\nh11==0.14.0\nhjson==3.1.0\nhttpcore==1.0.7\nhttptools==0.6.4\nhttpx==0.28.1\nhuggingface-hub==0.27.1\nhumanfriendly==10.0\nidna==3.10\nimportlib_metadata==8.5.0\niniconfig==2.0.0\ninteregular==0.3.3\nJinja2==3.1.5\njiter==0.8.2\njoblib==1.4.2\njsonpatch==1.33\njsonpointer==3.0.0\njsonschema==4.23.0\njsonschema-specifications==2024.10.1\nkiwisolver==1.4.8\nlangchain==0.3.14\nlangchain-core==0.3.29\nlangchain-text-splitters==0.3.5\nlangsmith==0.2.10\nlark==1.2.2\nlightgbm==4.5.0\nlinkify-it-py==2.0.3\nlm-format-enforcer==0.10.9\nlxml==5.3.0\nmarkdown-it-py==3.0.0\nMarkupSafe==3.0.2\nmatplotlib==3.10.0\nmdit-py-plugins==0.4.2\nmdurl==0.1.2\nmemray==1.15.0\nmistral_common==1.5.1\nmpmath==1.3.0\nmsgpack==1.1.0\nmsgspec==0.19.0\nmultidict==6.1.0\nmultiprocess==0.70.16\nnest-asyncio==1.6.0\nnetworkx==3.4.2\nninja==1.11.1.3\nnumpy==1.26.4\nnvidia-cublas-cu12==12.4.5.8\nnvidia-cuda-cupti-cu12==12.4.127\nnvidia-cuda-nvrtc-cu12==12.4.127\nnvidia-cuda-runtime-cu12==12.4.127\nnvidia-cudnn-cu12==9.1.0.70\nnvidia-cufft-cu12==11.2.1.3\nnvidia-curand-cu12==10.3.5.147\nnvidia-cusolver-cu12==11.6.1.9\nnvidia-cusparse-cu12==12.3.1.170\nnvidia-ml-py==12.560.30\nnvidia-nccl-cu12==2.21.5\nnvidia-nvjitlink-cu12==12.4.127\nnvidia-nvtx-cu12==12.4.127\nopenai==1.59.7\nopencensus==0.11.4\nopencensus-context==0.1.3\nopencv-python-headless==4.10.0.84\noptimum==1.23.3\norjson==3.10.14\noutlines==0.1.11\noutlines_core==0.1.26\npackaging==24.2\npandas==2.2.3\npartial-json-parser==0.2.1.1.post5\npeft==0.14.0\npillow==10.4.0\nplatformdirs==4.3.6\nplotly==5.24.1\npluggy==1.5.0\nportalocker==3.1.1\nprometheus-fastapi-instrumentator==7.0.2\nprometheus_client==0.21.1\npropcache==0.2.1\nproto-plus==1.25.0\nprotobuf==5.29.3\npsutil==6.1.1\npy-cpuinfo==9.0.0\npy-spy==0.4.0\npyarrow==18.1.0\npyasn1==0.6.1\npyasn1_modules==0.4.1\npybind11==2.13.6\npycountry==24.6.1\npydantic==2.10.5\npydantic_core==2.27.2\nPygments==2.19.1\npyparsing==3.2.1\npytest==8.3.4\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.1\npytz==2024.2\nPyYAML==6.0.2\npyzmq==26.2.0\nray==2.40.0\nreferencing==0.35.1\nregex==2024.11.6\nrequests==2.32.3\nrequests-toolbelt==1.0.0\nrich==13.9.4\nrouge==1.0.1\nrpds-py==0.22.3\nrsa==4.9\nsacrebleu==2.5.1\nsacremoses==0.1.1\nsafetensors==0.5.2\nscikit-learn==1.6.1\nscipy==1.15.1\nsentence-transformers==3.3.1\nsentencepiece==0.2.0\nsix==1.17.0\nsmart-open==7.1.0\nsniffio==1.3.1\nSQLAlchemy==2.0.37\nstarlette==0.41.3\nsympy==1.13.1\ntabulate==0.9.0\ntenacity==9.0.0\ntextual==1.0.0\nthreadpoolctl==3.5.0\ntiktoken==0.7.0\ntokenizers==0.21.0\ntorch==2.5.1\ntorchaudio==2.5.1\ntorchvision==0.20.1\ntqdm==4.67.1\ntransformers==4.48.0\ntransformers-stream-generator==0.0.5\ntriton==3.1.0\ntrl==0.13.0\ntyping_extensions==4.12.2\ntzdata==2024.2\nuc-micro-py==1.0.3\nurllib3==2.3.0\nuvicorn==0.34.0\nuvloop==0.21.0\nvirtualenv==20.28.1\nvllm==0.6.6.post1\nwatchfiles==1.0.4\nwebsockets==14.1\nwrapt==1.17.2\nxformers==0.0.28.post3\nxgboost==2.1.3\nxgrammar==0.1.9\nxxhash==3.5.0\nyarl==1.18.3\nzipp==3.21.0\n```\n\n----------------------------------------\n\nTITLE: Upgrading SDK and Running Migrations\nDESCRIPTION: This snippet shows how to migrate and upgrade to newer SDK versions. It is utilized to ensure compatibility with the latest features and improvements. Dependencies: Installed JavaScript SDK with migrations support.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/javascript/README.md#2025-04-19_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst pgml = require(\"pgml\");\nawait pgml.migrate()\n```\n\n----------------------------------------\n\nTITLE: Installing Korvus SDK for Python\nDESCRIPTION: Command to install the Korvus SDK for Python using pip package manager.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install korvus\n```\n\n----------------------------------------\n\nTITLE: Installing macOS Dependencies\nDESCRIPTION: Commands to install required dependencies using Homebrew bundler.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd pgml-extension && \\\nbrew bundle\n```\n\n----------------------------------------\n\nTITLE: Installing Korvus SDK for Rust\nDESCRIPTION: Command to add the Korvus SDK as a dependency in a Rust project using Cargo.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo add korvus\n```\n\n----------------------------------------\n\nTITLE: Preparing and Upserting Documents\nDESCRIPTION: Loads dataset, preprocesses documents by removing duplicates, and upserts a subset of documents into the collection. The pipeline automatically generates chunks and embeddings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/README.md#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    # Prep documents for upserting\n    data = load_dataset(\"squad\", split=\"train\")\n    data = data.to_pandas()\n    data = data.drop_duplicates(subset=[\"context\"])\n    documents = [\n        {\"id\": r[\"id\"], \"text\": r[\"context\"], \"title\": r[\"title\"]}\n        for r in data.to_dict(orient=\"records\")\n    ]\n\n    # Upsert documents\n    await collection.upsert_documents(documents[:200])\n```\n\n----------------------------------------\n\nTITLE: Ingesting Documents Command\nDESCRIPTION: Command to ingest documents and generate embeddings with debug logging enabled.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-apps/pgml-chat/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nLOG_LEVEL=DEBUG pgml-chat --root_dir <directory> --collection_name <collection_name> --stage ingest\n```\n\n----------------------------------------\n\nTITLE: Installing PostgreSQL Dependencies\nDESCRIPTION: Commands to install PostgreSQL common packages and pgBackRest through APT package manager.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/replication.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y postgresql-common\nsudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh\nsudo apt update\n```\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y pgbackrest\n```\n\n----------------------------------------\n\nTITLE: Connecting to PostgreSQL Server\nDESCRIPTION: Command to connect to a local PostgreSQL server with specified credentials.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/mindsdb-vs-postgresml.md#2025-04-19_snippet_0\n\nLANGUAGE: commandline\nCODE:\n```\npsql postgres://postgres:password@127.0.0.1:5432\n```\n\n----------------------------------------\n\nTITLE: Python Requirements File Content\nDESCRIPTION: A requirements.txt file containing Python package dependencies and their versions. Includes major ML libraries like PyTorch, transformers, and scikit-learn, along with web frameworks like FastAPI and various utility packages.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/requirements.linux.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate==1.2.1\naiohappyeyeballs==2.4.4\naiohttp==3.11.11\naiohttp-cors==0.7.0\naiosignal==1.3.2\nairportsdata==20241001\nannotated-types==0.7.0\nanyio==4.8.0\nastor==0.8.1\nattrs==24.3.0\nauto_gptq==0.7.1\nbitsandbytes==0.45.0\nblake3==1.0.2\ncachetools==5.5.0\ncatboost==1.2.7\ncertifi==2024.12.14\ncharset-normalizer==3.4.1\nclick==8.1.8\ncloudpickle==3.1.1\ncolorama==0.4.6\ncoloredlogs==15.0.1\ncolorful==0.5.6\ncompressed-tensors==0.8.1\ncontourpy==1.3.1\nctransformers==0.2.27\ncycler==0.12.1\ndatasets==3.2.0\ndeepspeed==0.16.2\ndepyf==0.18.0\ndill==0.3.8\ndiskcache==5.6.3\ndistlib==0.3.9\ndistro==1.9.0\neinops==0.8.0\nevaluate==0.4.3\nfastapi==0.115.6\nfilelock==3.16.1\nfonttools==4.55.3\nfrozenlist==1.5.0\nfsspec==2024.9.0\ngekko==1.2.1\ngguf==0.10.0\ngoogle-api-core==2.24.0\ngoogle-auth==2.37.0\ngoogleapis-common-protos==1.66.0\ngraphviz==0.20.3\ngreenlet==3.1.1\ngrpcio==1.69.0\nh11==0.14.0\nhjson==3.1.0\nhttpcore==1.0.7\nhttptools==0.6.4\nhttpx==0.28.1\nhuggingface-hub==0.27.1\nhumanfriendly==10.0\nidna==3.10\nimportlib_metadata==8.5.0\niniconfig==2.0.0\ninteregular==0.3.3\nJinja2==3.1.5\njiter==0.8.2\njoblib==1.4.2\njsonpatch==1.33\njsonpointer==3.0.0\njsonschema==4.23.0\njsonschema-specifications==2024.10.1\nkiwisolver==1.4.8\nlangchain==0.3.14\nlangchain-core==0.3.29\nlangchain-text-splitters==0.3.5\nlangsmith==0.2.10\nlark==1.2.2\nlightgbm==4.5.0\nlinkify-it-py==2.0.3\nlm-format-enforcer==0.10.9\nlxml==5.3.0\nmarkdown-it-py==3.0.0\nMarkupSafe==3.0.2\nmatplotlib==3.10.0\nmdit-py-plugins==0.4.2\nmdurl==0.1.2\nmemray==1.15.0\nmistral_common==1.5.1\nmpmath==1.3.0\nmsgpack==1.1.0\nmsgspec==0.19.0\nmultidict==6.1.0\nmultiprocess==0.70.16\nnest-asyncio==1.6.0\nnetworkx==3.4.2\nninja==1.11.1.3\nnumpy==1.26.4\nnvidia-cublas-cu12==12.4.5.8\nnvidia-cuda-cupti-cu12==12.4.127\nnvidia-cuda-nvrtc-cu12==12.4.127\nnvidia-cuda-runtime-cu12==12.4.127\nnvidia-cudnn-cu12==9.1.0.70\nnvidia-cufft-cu12==11.2.1.3\nnvidia-curand-cu12==10.3.5.147\nnvidia-cusolver-cu12==11.6.1.9\nnvidia-cusparse-cu12==12.3.1.170\nnvidia-ml-py==12.560.30\nnvidia-nccl-cu12==2.21.5\nnvidia-nvjitlink-cu12==12.4.127\nnvidia-nvtx-cu12==12.4.127\nopenai==1.59.7\nopencensus==0.11.4\nopencensus-context==0.1.3\nopencv-python-headless==4.10.0.84\noptimum==1.23.3\norjson==3.10.14\noutlines==0.1.11\noutlines_core==0.1.26\npackaging==24.2\npandas==2.2.3\npartial-json-parser==0.2.1.1.post5\npeft==0.14.0\npillow==10.4.0\nplatformdirs==4.3.6\nplotly==5.24.1\npluggy==1.5.0\nportalocker==3.1.1\nprometheus-fastapi-instrumentator==7.0.2\nprometheus_client==0.21.1\npropcache==0.2.1\nproto-plus==1.25.0\nprotobuf==5.29.3\npsutil==6.1.1\npy-cpuinfo==9.0.0\npy-spy==0.4.0\npyarrow==18.1.0\npyasn1==0.6.1\npyasn1_modules==0.4.1\npybind11==2.13.6\npycountry==24.6.1\npydantic==2.10.5\npydantic_core==2.27.2\nPygments==2.19.1\npyparsing==3.2.1\npytest==8.3.4\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.1\npytz==2024.2\nPyYAML==6.0.2\npyzmq==26.2.0\nray==2.40.0\nreferencing==0.35.1\nregex==2024.11.6\nrequests==2.32.3\nrequests-toolbelt==1.0.0\nrich==13.9.4\nrouge==1.0.1\nrpds-py==0.22.3\nrsa==4.9\nsacrebleu==2.5.1\nsacremoses==0.1.1\nsafetensors==0.5.2\nscikit-learn==1.6.1\nscipy==1.15.1\nsentence-transformers==3.3.1\nsentencepiece==0.2.0\nsix==1.17.0\nsmart-open==7.1.0\nsniffio==1.3.1\nSQLAlchemy==2.0.37\nstarlette==0.41.3\nsympy==1.13.1\ntabulate==0.9.0\ntenacity==9.0.0\ntextual==1.0.0\nthreadpoolctl==3.5.0\ntiktoken==0.7.0\ntokenizers==0.21.0\ntorch==2.5.1\ntorchaudio==2.5.1\ntorchvision==0.20.1\ntqdm==4.67.1\ntransformers==4.48.0\ntransformers-stream-generator==0.0.5\ntriton==3.1.0\ntrl==0.13.0\ntyping_extensions==4.12.2\ntzdata==2024.2\nuc-micro-py==1.0.3\nurllib3==2.3.0\nuvicorn==0.34.0\nuvloop==0.21.0\nvirtualenv==20.28.1\nvllm==0.6.6.post1\nwatchfiles==1.0.4\nwebsockets==14.1\nwrapt==1.17.2\nxformers==0.0.28.post3\nxgboost==2.1.3\nxgrammar==0.1.9\nxxhash==3.5.0\nyarl==1.18.3\nzipp==3.21.0\n```\n\n----------------------------------------\n\nTITLE: Stopping PostgreSQL Server\nDESCRIPTION: Command to stop the PostgreSQL database server before performing a restore operation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/backups.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo service postgresql stop\n```\n\n----------------------------------------\n\nTITLE: Installing pgvector Extension\nDESCRIPTION: Commands to install the pgvector extension from source.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --branch v0.6.0 https://github.com/pgvector/pgvector && \\\ncd pgvector && \\\necho \"trusted = true\" >> vector.control && \\\nmake && \\\nmake install\n```\n\n----------------------------------------\n\nTITLE: Testing PgCat Connection with PSQL\nDESCRIPTION: Command to test PgCat connection using psql client\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/pooler.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nPGPASSWORD=\"<secure password>\" psql \\\n    -h \"127.0.0.1\" \\\n    -p 6432 \\\n    -U postgresml_user \\\n    -d postgresml\n```\n\n----------------------------------------\n\nTITLE: Implementing Shared Memory Ring Buffer in PostgreSQL Extension\nDESCRIPTION: Implementation details for a shared memory ring buffer that caches system metrics in PostgreSQL. The buffer is managed by a background worker that serializes statistics data and handles buffer rotation when approaching capacity. The implementation ensures proper cross-process data sharing without pointer resolution issues.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pg-stat-sysinfo-a-postgres-extension-for-querying-system-statistics.md#2025-04-19_snippet_5\n\nLANGUAGE: rust\nCODE:\n```\n// Note: This is a conceptual reference to src/shmem_ring_buffer.rs\n// The actual implementation involves a background worker writing serialized\n// statistics into a shared memory segment accessed via PGRX utilities\n```\n\n----------------------------------------\n\nTITLE: Installing pgml-components CLI Using Cargo\nDESCRIPTION: Command for installing the pgml-components CLI tool from crates.io using Cargo.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo install cargo-pgml-components\n```\n\n----------------------------------------\n\nTITLE: Basic Text Chunking Example with Recursive Character Splitter\nDESCRIPTION: A simple example showing how to use the recursive_character splitter to chunk text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.chunk.md#2025-04-19_snippet_1\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.chunk('recursive_character', 'test');\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies Requirements\nDESCRIPTION: Core Python package dependencies required for a PostgresML project. Includes data processing libraries (pandas, numpy), machine learning frameworks (xgboost, scikit-learn), web framework (Flask), web server (gunicorn), and caching (redis).\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/blog/benchmarks/python_microservices_vs_postgresml/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npandas\nnumpy\nxgboost\nFlask\ngunicorn\nscikit-learn\nredis\n```\n\n----------------------------------------\n\nTITLE: Example PostgreSQL Extension Migration Filename\nDESCRIPTION: Shows a specific example of a migration filename for upgrading PostgresML from version 2.7.10 to 2.7.11.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/README.md#2025-04-19_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\npgml-extension/sql/pgml--2.7.10--2.7.11.sql\n```\n\n----------------------------------------\n\nTITLE: Default Prompt Template\nDESCRIPTION: Default system prompt template for GPT-3.5 and open source models.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-apps/pgml-chat/README.md#2025-04-19_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nUse the following pieces of context to answer the question at the end.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\nUse three sentences maximum and keep the answer as concise as possible.\nAlways say \"thanks for asking!\" at the end of the answer.\n```\n\n----------------------------------------\n\nTITLE: Running PostgresML Docker Container with GPU Support\nDESCRIPTION: Docker command to start PostgresML container with GPU capabilities enabled through the --gpus flag.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/quick-start-with-docker.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run \\\n    -it \\\n    -v postgresml_data:/var/lib/postgresql \\\n    --gpus all \\\n    -p 5433:5432 \\\n    -p 8000:8000 \\\n    ghcr.io/postgresml/postgresml:2.10.0 \\\n    sudo -u postgresml psql -d postgresml\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Extension\nDESCRIPTION: SQL command to create the vector extension in PostgreSQL.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE EXTENSION vector;\n```\n\n----------------------------------------\n\nTITLE: Installing pgvector Dependencies\nDESCRIPTION: Command to install required build dependencies for pgvector.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/README.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install -y \\\n    build-essential \\\n    postgresql-server-dev-14\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: A requirements.txt file containing Python package dependencies with exact version numbers. Includes major ML libraries like PyTorch, Transformers, LangChain, and various data science packages needed for the PostgresML project.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/requirements.macos.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate==0.30.1\naiohttp==3.9.5\naiosignal==1.3.1\nannotated-types==0.6.0\nattrs==23.2.0\nbitsandbytes==0.42.0\ncatboost==1.2.5\ncertifi==2024.2.2\ncharset-normalizer==3.3.2\nclick==8.1.7\ncolorama==0.4.6\ncoloredlogs==15.0.1\ncontourpy==1.2.1\nctransformers==0.2.27\ncycler==0.12.1\ndataclasses-json==0.6.6\ndatasets==2.16.1\ndeepspeed==0.14.2\ndill==0.3.7\ndocstring_parser==0.16\neinops==0.8.0\nevaluate==0.4.2\nfilelock==3.14.0\nfonttools==4.51.0\nfrozenlist==1.4.1\nfsspec==2023.10.0\ngraphviz==0.20.3\nhjson==3.1.0\nhuggingface-hub==0.23.0\nhumanfriendly==10.0\nidna==3.7\nJinja2==3.1.4\njoblib==1.4.2\njsonpatch==1.33\njsonpointer==2.4\nkiwisolver==1.4.5\nlangchain==0.1.20\nlangchain-community==0.0.38\nlangchain-core==0.1.52\nlangchain-text-splitters==0.0.1\nlangsmith==0.1.57\nlightgbm==4.3.0\nlxml==5.2.2\nmarkdown-it-py==3.0.0\nMarkupSafe==2.1.5\nmarshmallow==3.21.2\nmatplotlib==3.8.4\nmdurl==0.1.2\nmpmath==1.3.0\nmultidict==6.0.5\nmultiprocess==0.70.15\nmypy-extensions==1.0.0\nnetworkx==3.3\nninja==1.11.1.1\nnumpy==1.26.4\noptimum==1.19.2\norjson==3.10.3\npackaging==23.2\npandas==2.2.2\npeft==0.10.0\npillow==10.3.0\nplotly==5.22.0\nportalocker==2.8.2\nprotobuf==5.26.1\npsutil==5.9.8\npy-cpuinfo==9.0.0\npyarrow==11.0.0\npyarrow-hotfix==0.6\npydantic==2.7.1\npydantic_core==2.18.2\nPygments==2.18.0\npynvml==11.5.0\npyparsing==3.1.2\npython-dateutil==2.9.0.post0\npytz==2024.1\nPyYAML==6.0.1\nregex==2024.5.10\nrequests==2.31.0\nrich==13.7.1\nrouge==1.0.1\nsacrebleu==2.4.2\nsacremoses==0.1.1\nsafetensors==0.4.3\nscikit-learn==1.4.2\nscipy==1.13.0\nsentence-transformers==2.7.0\nsentencepiece==0.2.0\nshtab==1.7.1\nsix==1.16.0\nSQLAlchemy==2.0.30\nsympy==1.12\ntabulate==0.9.0\ntenacity==8.3.0\nthreadpoolctl==3.5.0\ntokenizers==0.19.1\ntorch==2.3.0\ntorchaudio==2.3.0\ntorchvision==0.18.0\ntqdm==4.66.4\ntransformers==4.40.2\ntransformers-stream-generator==0.0.5\ntrl==0.8.6\ntyping-inspect==0.9.0\ntyping_extensions==4.11.0\ntyro==0.8.4\ntzdata==2024.1\nurllib3==2.2.1\nxgboost==2.0.3\nxxhash==3.4.1\nyarl==1.9.4\n```\n\n----------------------------------------\n\nTITLE: Installing latest stable Node.js version using nvm in Bash\nDESCRIPTION: This command installs the latest stable version of Node.js using Node Version Manager (nvm). It's a prerequisite for installing the Sass compiler, which is required for the dashboard frontend.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nnvm install stable\n```\n\n----------------------------------------\n\nTITLE: Setting PKG_CONFIG_PATH\nDESCRIPTION: Environment variable configuration for pkg-config path on macOS.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport PKG_CONFIG_PATH=\"/opt/homebrew/opt/icu4c/lib/pkgconfig\"\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies List\nDESCRIPTION: A requirements.txt file containing Python package dependencies with pinned versions. Includes major ML libraries like PyTorch, Transformers, and scikit-learn, along with various data processing and utility packages.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/requirements.arm64.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naccelerate==0.30.1\naiohttp==3.9.5\naiosignal==1.3.1\nannotated-types==0.6.0\nattrs==23.2.0\nbitsandbytes==0.42.0\ncatboost==1.2.5\ncertifi==2024.2.2\ncharset-normalizer==3.3.2\nclick==8.1.7\ncolorama==0.4.6\ncoloredlogs==15.0.1\ncontourpy==1.2.1\nctransformers==0.2.27\ncycler==0.12.1\ndataclasses-json==0.6.6\ndatasets==2.16.1\ndeepspeed==0.14.2\ndill==0.3.7\ndocstring_parser==0.16\neinops==0.8.0\nevaluate==0.4.2\nfilelock==3.14.0\nfonttools==4.51.0\nfrozenlist==1.4.1\nfsspec==2023.10.0\ngraphviz==0.20.3\nhjson==3.1.0\nhuggingface-hub==0.23.0\nhumanfriendly==10.0\nidna==3.7\nJinja2==3.1.4\njoblib==1.4.2\njsonpatch==1.33\njsonpointer==2.4\nkiwisolver==1.4.5\nlangchain==0.1.20\nlangchain-community==0.0.38\nlangchain-core==0.1.52\nlangchain-text-splitters==0.0.1\nlangsmith==0.1.57\nlightgbm==4.3.0\nlxml==5.2.2\nmarkdown-it-py==3.0.0\nMarkupSafe==2.1.5\nmarshmallow==3.21.2\nmatplotlib==3.8.4\nmdurl==0.1.2\nmpmath==1.3.0\nmultidict==6.0.5\nmultiprocess==0.70.15\nmypy-extensions==1.0.0\nnetworkx==3.3\nninja==1.11.1.1\nnumpy==1.26.4\noptimum==1.19.2\norjson==3.10.3\npackaging==23.2\npandas==2.2.2\npeft==0.10.0\npillow==10.3.0\nplotly==5.22.0\nportalocker==2.8.2\nprotobuf==5.26.1\npsutil==5.9.8\npy-cpuinfo==9.0.0\npyarrow==11.0.0\npyarrow-hotfix==0.6\npydantic==2.7.1\npydantic_core==2.18.2\nPygments==2.18.0\npynvml==11.5.0\npyparsing==3.1.2\npython-dateutil==2.9.0.post0\npytz==2024.1\nPyYAML==6.0.1\nregex==2024.5.10\nrequests==2.31.0\nrich==13.7.1\nrouge==1.0.1\nsacrebleu==2.4.2\nsacremoses==0.1.1\nsafetensors==0.4.3\nscikit-learn==1.4.2\nscipy==1.13.0\nsentence-transformers==2.7.0\nsentencepiece==0.2.0\nshtab==1.7.1\nsix==1.16.0\nSQLAlchemy==2.0.30\nsympy==1.12\ntabulate==0.9.0\ntenacity==8.3.0\nthreadpoolctl==3.5.0\ntokenizers==0.19.1\ntorch==2.3.0\ntorchaudio==2.3.0\ntorchvision==0.18.0\ntqdm==4.66.4\ntransformers==4.40.2\ntransformers-stream-generator==0.0.5\ntrl==0.8.6\ntyping-inspect==0.9.0\ntyping_extensions==4.11.0\ntyro==0.8.4\ntzdata==2024.1\nurllib3==2.2.1\nxgboost==2.0.3\nxxhash==3.4.1\nyarl==1.9.4\n```\n\n----------------------------------------\n\nTITLE: Cloning PgCat Source Repository\nDESCRIPTION: Command to download PgCat source code from GitHub repository\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/pooler.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/postgresml/pgcat\n```\n\n----------------------------------------\n\nTITLE: Setting up DATABASE_URL environment variable\nDESCRIPTION: Before running any examples, the DATABASE_URL environment variable needs to be set. This variable specifies the connection string to the Postgres database where PostgresML is installed and configured.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/javascript/examples/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport DATABASE_URL={YOUR DATABASE URL}\n```\n\n----------------------------------------\n\nTITLE: PostgreSQL Extension Migration File Naming Convention\nDESCRIPTION: Demonstrates the required naming format for PostgreSQL extension migration files, where migrations must follow a strict naming pattern to ensure proper version upgrades.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<name>--<current version>--<new version>.sql\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML Rust SDK with Cargo\nDESCRIPTION: Command to add the PostgresML Rust SDK to a Rust project using Cargo. The SDK is available on crates.io and serves as the canonical implementation for other language SDKs.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-the-release-of-our-rust-sdk.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncargo add pgml\n```\n\n----------------------------------------\n\nTITLE: Configuring environment variables for pgml-chat\nDESCRIPTION: Environment variables template showing required configuration for OpenAI API, PostgresML database connection, embeddings model, prompts, and chat service integrations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowledge-based-chatbots-part-i.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=<OPENAI_API_KEY>\nDATABASE_URL=<POSTGRES_DATABASE_URL starts with postgres://>\nMODEL=Alibaba-NLP/gte-base-en-v1.5\nSYSTEM_PROMPT=<> # System prompt used for OpenAI chat completion\nBASE_PROMPT=<> # Base prompt used for OpenAI chat completion for each turn\nSLACK_BOT_TOKEN=<SLACK_BOT_TOKEN> # Slack bot token to run Slack chat service\nSLACK_APP_TOKEN=<SLACK_APP_TOKEN> # Slack app token to run Slack chat service\nDISCORD_BOT_TOKEN=<DISCORD_BOT_TOKEN> # Discord bot token to run Discord chat service\n```\n\n----------------------------------------\n\nTITLE: Installing Rust Compiler\nDESCRIPTION: Command to install Rust programming language compiler required for building PgCat from source\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/pooler.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Environment\nDESCRIPTION: Commands for setting up Python environment and loading data into Redis for the Python implementation\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/blog/benchmarks/python_microservices_vs_postgresml/README.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\npython train.py\n```\n\nLANGUAGE: bash\nCODE:\n```\ncurl -L -o ~/Desktop/flights_sub.csv https://static.postgresml.org/benchmarks/flights_sub.csv\npython load_redis.py\n```\n\n----------------------------------------\n\nTITLE: Installing Sass compiler globally using npm in Bash\nDESCRIPTION: This command uses npm (Node Package Manager) to install the Sass compiler globally. Sass is required for compiling the dashboard's frontend styles.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/installation.md#2025-04-19_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g sass\n```\n\n----------------------------------------\n\nTITLE: Numerical Vector Representation (384-dimensional)\nDESCRIPTION: This snippet contains a 384-dimensional vector of floating-point values. Each value is separated by a comma. The vector likely represents a high-dimensional data point or embedding used in machine learning or data analysis tasks.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\n{-0.09234577,0.037487056,-0.03421769,-0.033738457,-0.042548284,-0.0015319627,0.042109113,0.011365055,-0.018372666,0.020417988,0.061961487,-0.022707041,0.015810987,0.03675479,0.001995532,-0.04197657,-0.034883354,0.07871886,-0.11676137,0.06141681,0.08321331,-0.03457781,-0.013248807,-0.05802344,-0.039144825,-0.015038275,0.020686107,0.08593334,-0.041029375,-0.13210341,-0.034079146,0.016687978,0.06363906,-0.05279167,0.10102262,-0.048170853,-0.014849669,0.03523273,0.024248678,0.031341534,-0.021447029,-0.05781338,0.039722513,-0.058294114,-0.035174508,-0.056844078,-0.051775914,-0.05822031,0.083022244,0.027178412,0.0032413877,0.023898097,0.023951318,0.0565093,0.036267336,0.049430914,0.027110789,0.05017207,0.058326595,0.040568575,0.014855128,0.06272174,-0.12961388,0.0998898,0.014964503,0.07735804,-0.028795758,0.026889611,-0.0613238,-0.004798127,0.009027658,0.046634953,-0.034936648,0.076499216,-0.03855506,0.08894715,-0.0019889707,0.07027481,-0.04624302,-0.048422314,-0.02444203,-0.0442959,-0.028878363,0.04586853,-0.004158767,-0.0027680802,0.029728336,-0.06130052,-0.028088963,-0.050658133,-0.024370935,-0.0030779864,0.018137587,-0.029853988,-0.06877675,-0.001238518,0.025249483,-0.0045243553,0.07250941,0.12831028,0.0077543575,0.012130527,-0.0006014347,-0.027807593,-0.011226617,-0.04837827,0.0376276,-0.058811083,0.020967057,-0.021439878,-0.0634577,-0.029189702,-0.040197153,-0.01993339,0.0899751,-0.014370172,0.0021994617,-0.0759979,-0.010541287,0.034424484,0.030067233,0.016858222,0.015223163,0.021410512,0.072372325,-0.06270684,0.09666927,0.07237114,0.09372637,-0.027058149,0.06319879,-0.03626834,-0.03539027,0.010406426,-0.08829164,-0.020550422,-0.043701466,-0.018676292,0.038060706,-0.0058152666,-0.04057362,-0.06266247,-0.026675962,-0.07610313,-0.023740835,0.06968648,-0.076157875,0.05129882,-0.053703927,-0.04906172,-0.014506706,-0.033226766,0.04197027,0.009892002,-0.019509513,0.020975547,0.015931072,0.044290986,-0.048697367,-0.022310019,-0.088599496,-0.0371257,0.037382104,0.14381507,0.07789086,-0.10580675,0.0255245,0.014246269,0.01157928,-0.069586724,0.023313843,0.02494169,-0.014511085,-0.017566541,0.0865948,-0.012115137,0.024397936,-0.049067125,0.03300015,-0.058626212,0.034921415,-0.04132337,-0.025009211,0.057668354,0.016189015,-0.04954466,-0.036778226,-0.046015732,-0.041587763,-0.03449501,-0.033505566,0.019262834,-0.018552447,0.019556912,0.01612039,0.0026575527,-0.05330489,-0.06894643,-0.04592849,-0.08485257,0.12714563,0.026810834,-0.053618323,-0.029470881,-0.04381535,0.055211045,-0.0111715235,-0.004484313,-0.02654065,-0.022317547,-0.027823675,0.0135190515,0.001530742,-0.04323672,-0.028350104,-0.07154715,-0.0024147208,0.031836234,0.03476004,0.033611998,0.038179073,-0.087631755,-0.048408568,-0.11773682,-0.019127818,0.013682835,-0.02015055,0.01888005,-0.03280704,0.0076310635,0.074330166,-0.031277154,0.056628436,0.119448215,-0.0012235055,-0.009727585,-0.05459528,0.04298459,0.054554865,-0.027898816,0.0040641865,0.08585007,-0.053415824,-0.030528797,-0.08231634,-0.069264784,-0.08337459,0.049254872,-0.021684796,0.12479715,0.053940497,-0.038884085,-0.032209005,0.035795107,0.0054665194,0.0085438965,-0.039386917,0.083624765,-0.056901276,0.022051739,0.06955752,-0.0008329906,-0.07959222,0.075660035,-0.017008293,0.015329365,-0.07439257,0.057193674,-0.06564091,0.0007063081,-0.015799401,-0.008529507,0.027204275,0.0076780985,-0.018589584,0.065267086,-0.02026929,-0.0559547,-0.035843417,-0.07237942,0.028072618,-0.048903402,-0.027478782,-0.084877744,-0.040812787,0.026713751,0.016210195,-0.039116003,0.03572044,-0.014964189,0.026315138,-0.08638934,-0.04198059,-0.02164005,0.09299754,-0.047685668,0.061317034,0.035914674,0.03533252,0.0287274,-0.033809293,-0.046841178,-0.042211317,-0.02567011,-0.048029255,0.039492987,0.04906847,0.030969618,0.0066106897,0.025528666,-0.008357054,0.04791732,-0.070402496,0.053391967,-0.06309544,0.06575766,0.06522203,0.060434356,-0.047547556,-0.13597175,-0.048658505,0.009734684,-0.016258504,-0.034227647,0.05382081,0.001330341,0.011890187,-0.047945525,-0.031132223,0.0010349775,0.030007072,0.12059559,-0.060273632,-0.010099646,0.055261053,0.053757478,-0.045518342,-0.041972063,-0.08315036,0.049884394,0.037543204,0.17598632,-0.0027433096,0.015989233,0.017486975,0.0059954696,-0.022668751,0.05677827,0.029728843,0.0011321013,-0.051546678,0.1113402,0.017779723,0.050953783,0.10342974,0.04067395,0.054890294,0.017487328,-0.020321153,0.062171113,0.07234749,-0.06777497,-0.03888628,0.08744684,0.032227095,-0.04398878,-0.049698275,-0.0018518695,-0.015967874,-0.0415869,-0.022655524,0.03596353,0.07130526,0.056296617,-0.06720573,-0.092787154,0.021057911,0.015628621,-0.04396636,-0.0063872878,-0.0127499355,0.01633339,-0.0006204544,0.0438727}\n```\n\n----------------------------------------\n\nTITLE: Cloning PostgresML Repository\nDESCRIPTION: This bash script clones the PostgresML repository from GitHub and initializes its submodules.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/postgresml/postgresml && \\\ncd postgresml && \\\ngit submodule update --init --recursive &&\n```\n\n----------------------------------------\n\nTITLE: Starting PostgreSQL Server\nDESCRIPTION: Command to start the PostgreSQL server after completing the restore operation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/backups.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo service postgresql start\n```\n\n----------------------------------------\n\nTITLE: Displaying pgml-chat CLI Help\nDESCRIPTION: Command to display the help information for the pgml-chat command-line interface, showing available options and arguments.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n(pgml-bot-builder-py3.9) pgml-chat % pgml-chat --help\nusage: pgml-chat [-h] --collection_name COLLECTION_NAME [--root_dir ROOT_DIR] [--stage {ingest,chat}] [--chat_interface {cli,slack}]\n\nPostgresML Chatbot Builder\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --collection_name COLLECTION_NAME\n                        Name of the collection (schema) to store the data in PostgresML database (default: None)\n  --root_dir ROOT_DIR   Input folder to scan for markdown files. Required for ingest stage. Not required for chat stage (default: None)\n  --stage {ingest,chat}\n                        Stage to run (default: chat)\n  --chat_interface {cli, slack, discord}\n                        Chat interface to use (default: cli)\n```\n\n----------------------------------------\n\nTITLE: Installing PostgresML Python Dependencies\nDESCRIPTION: This command installs the required Python dependencies for PostgresML, including support for GPTQ and GGML quantization methods.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_1\n\nLANGUAGE: commandline\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost Dependencies in Rust\nDESCRIPTION: Cargo.toml configuration showing required XGBoost dependency for Rust project implementation.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/oxidizing-machine-learning.md#2025-04-19_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n[dependencies]\nxgboost = \"0.1\"\n```\n\n----------------------------------------\n\nTITLE: Defining Trellis AI Transformation Schema for Job Data Extraction\nDESCRIPTION: This JSON schema defines the transformation rules for Trellis AI to extract structured data from job listing HTML files. It specifies the fields to extract, including roles, technical requirements, location, descriptions, and pay ranges.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/korvus-trellis-semantic-search-over-yc-jobs.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"model\": \"trellis-premium\",\n  \"mode\": \"document\",\n  \"table_preferences\": {\n    \"included_table_names\": []\n  },\n  \"operations\": [\n    {\n      \"column_name\": \"role\",\n      \"column_type\": \"text[]\",\n      \"task_description\": \"Extract the roles of the job listings\",\n      \"transform_type\": \"extraction\"\n    },\n    {\n      \"column_name\": \"technical_requirements\",\n      \"column_type\": \"text[]\",\n      \"task_description\": \"Extract the technical requirements for each job\",\n      \"transform_type\": \"extraction\"\n    },\n    {\n      \"column_name\": \"location\",\n      \"column_type\": \"text\",\n      \"task_description\": \"Extract the location of the job\",\n      \"transform_type\": \"extraction\"\n    },\n    {\n      \"column_name\": \"description\",\n      \"column_type\": \"text[]\",\n      \"task_description\": \"Extract or generate the job descriptions\",\n      \"transform_type\": \"generation\"\n    },\n    {\n      \"column_name\": \"company_description\",\n      \"column_type\": \"text\",\n      \"task_description\": \"Extract or generate the description of the company listing the jobs\",\n      \"transform_type\": \"generation\"\n    },\n    {\n      \"column_name\": \"pay_from\",\n      \"column_type\": \"text[]\",\n      \"task_description\": \"Task: Extract the lower limit of pay ranges from job listings.\\n- If a pay range is provided (e.g., \\\"80k-120k\\\" or \\\"$80,000-$120,000\\\"), extract the upper limit (e.g., 80000).\\n- Do not mention equity\\n- Output null if no lower limit or pay information is provided\",\n      \"transform_type\": \"generation\"\n    },\n    {\n      \"column_name\": \"pay_to\",\n      \"column_type\": \"text[]\",\n      \"task_description\": \"Task: Extract the upper limit of pay ranges from job listings.\\n- If a pay range is provided (e.g., \\\"90k-120k\\\" or \\\"$80,000-$120,000\\\"), extract the upper limit (e.g., 120000).\\n- If only equity is mentioned, extract the percentage and append \\\"equity\\\" (e.g., \\\"0.25% equity\\\").\\n- Output null if no upper limit or pay information is provided.\",\n      \"transform_type\": \"generation\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Slack Chat Interface\nDESCRIPTION: Command to start the Slack chat interface for interacting with the chatbot.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nLOG_LEVEL=ERROR pgml-chat --collection_name <collection_name> --stage chat --chat_interface slack\n```\n\n----------------------------------------\n\nTITLE: Running PgCat from Source\nDESCRIPTION: Command to run PgCat directly when compiled from source\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/pooler.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npgcat /etc/pgcat.toml\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Rust Struct and Implementation\nDESCRIPTION: A simple Rust struct with a constructor and getter method demonstrating the initial structure before translation\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/rust-bridge/README.md#2025-04-19_snippet_0\n\nLANGUAGE: rust\nCODE:\n```\nstruct Test {\n  x: i32\n}\n\nimpl Test {\n  fn new(x: i32) -> Self {\n    Self {\n      x\n    }\n  }\n\n  fn get(&self) -> i32 {\n    self.x\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Stimulus Controller for Dropdown Component\nDESCRIPTION: JavaScript controller using Hotwired Stimulus for a dropdown component, with basic initialization functionality.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Controller } from '@hotwired/stimulus'\n\nexport default class extends Controller {\n\tinitiliaze() {\n\t\tconsole.log('Initialized dropdown controller')\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Second Numerical Vector Representation (384-dimensional)\nDESCRIPTION: This snippet contains another 384-dimensional vector of floating-point values. Similar to the first vector, it likely represents a high-dimensional data point or embedding used in machine learning or data analysis tasks. The presence of two vectors suggests they may be related or used for comparison.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/in-database-generation.md#2025-04-19_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\n{-0.11384405,0.067140445,0.004428383,-0.019213142,0.011713443,0.009808596,0.06439777,-0.014959955,-0.03600561,0.01949383,0.04094742,0.030407589,-0.026018979,0.044171993,0.022412317,-0.057937913,-0.05182386,0.07793179,-0.109105654,0.057499174,0.102279164,-0.04705679,0.0010215766,-0.052305017,-0.0064890077,-0.019298203,0.0027092565,0.07363092,-0.010116459,-0.12196041,-0.025577176,0.010314696,0.031369787,-0.020949671,0.08722754,-0.051809352,0.0007810379,0.07672705,-0.008455481,0.06511949,-0.021327827,-0.060510863,0.044916406,-0.08674781,-0.047401372,-0.01868107,-0.075262256,-0.055392392,0.072947465,-0.01151735,-0.0072187134,0.015544381,0.039965566,0.020232335,0.04894269,0.04900096,0.05358905,0.032501124,0.053288646,0.07584814,0.031957388,0.05976136,-0.12726106,0.103460334,0.06346268,0.06554993,-0.045167506,0.012330433,-0.062929176,0.043507233,-0.008544882,0.027812833,-0.040016085,0.055822216,-0.03835489,0.040096387,0.018063055,0.060356017,-0.0726533,-0.0671456,-0.05047295,-0.042710193,-0.042777598,-0.006822609,0.012524907,-0.032105528,0.026691807,-0.05756205,0.015424967,-0.04767447,-0.036748573,-0.02527533,0.025934244,-0.033328723,-4.1858173e-05,-0.027706677,0.047805857,0.00018475522,0.050902035,0.1352519,0.005388455,0.029921843,-0.02537518,-0.058101207,-0.021984883,-0.059336115,0.03498545,-0.052446626,0.022411253,0.0060822135,-0.068493545,-0.013820616,-0.03522277,-0.018971028,0.07487064,-0.0009035772,-0.009381329,-0.04850395,0.001105027,0.016467793,0.0268643,0.0013964645,0.043346133,-0.009041368,0.07489963,-0.07887815,0.068340026,0.03767777,0.11665796,-0.025433592,0.062018104,-0.030672694,-0.012993033,0.0068405713,-0.03688894,-0.022034604,-0.040981747,-0.033101898,0.071058825,-0.0017327801,-0.021141728,-0.07144207,-0.02906128,-0.095396295,0.006055787,0.08500532,-0.031142898,0.055712428,-0.041926548,-0.042101618,-0.013311086,-0.046836447,0.023902802,0.031264246,-0.012085872,0.042904463,0.011645057,0.049069524,-0.0039288886,-0.014362478,-0.06809574,-0.038734697,0.028410498,0.12843607,0.090781115,-0.119838186,0.016676102,0.0009924435,0.0314442,-0.040607806,0.0020882064,0.044765383,0.01829387,-0.05677682,0.08415222,-0.06399008,-0.010945022,-0.024140757,0.046428833,-0.0651499,0.041250102,-0.06294866,-0.032783676,0.047456875,0.034612734,-0.021892011,-0.050926965,-0.06388983,-0.031164767,0.053277884,-0.069394015,0.03465082,-0.0410735,0.03736871,0.010950864,0.01830701,-0.070063934,-0.06988279,-0.03560967,-0.05519299,0.07882521,0.05533408,-0.02321644,0.007326457,-0.05126765,0.045479607,0.01830127,-0.037239183,-0.08015762,-0.056017533,-0.07647084,-0.0065865014,-0.027235825,-0.039984804,-0.0156225115,-0.014561295,0.024489071,0.009097713,0.04265267,-0.003169223,0.010329996,-0.078917705,-0.026417341,-0.13925064,-0.009786513,-0.037679326,-0.023494951,0.016230932,-0.010068113,0.008919443,0.05672694,-0.0647096,0.0074613485,0.0856074,-0.0072963624,-0.04508945,-0.027654354,0.031864826,0.046863783,-0.032239847,-0.024967564,0.065593235,-0.05142123,-0.011477745,-0.083396286,-0.036403924,-0.030264381,0.060208946,-0.037968345,0.13118903,0.055968005,-0.02204113,-0.00871512,0.06265703,0.024767108,0.06307163,-0.093918525,0.06388834,-0.027308429,0.028177679,0.046643235,-0.008643308,-0.08599351,0.08742052,-0.0045658057,0.009925819,-0.061982065,0.06666853,-0.085638665,-0.008682048,0.016528588,-0.015443429,0.040419903,0.0059123226,-0.04848474,0.026133329,-0.042095724,-0.06860905,-0.033551272,-0.06492134,0.019667841,-0.04917464,-0.0096588,-0.10072659,-0.07769663,0.03221359,0.019174514,0.039727442,0.025392585,-0.016384942,0.0024048705,-0.09175566,-0.03225071,0.0066428655,0.10759633,-0.04011207,0.031578932,0.06299788,0.061487168,0.048043367,-0.0047893273,-0.054848563,-0.06647676,-0.027905045,-0.055799212,0.028914401,0.04013868,0.050728165,-0.0063177645,-0.018899892,0.008193828,0.025991635,-0.08009935,0.044058595,-0.046858713,0.072079815,0.046664152,0.019002488,-0.018447064,-0.15560018,-0.050175466,0.001016439,-0.0035773942,-0.025972001,0.047064543,0.01866733,0.0049167247,-0.052880444,-0.029235922,-0.024581103,0.040634423,0.095990844,-0.019483034,-0.02325509,0.056078408,0.09241045,-0.03079215,-0.023518562,-0.08394134,0.03326668,0.008070111,0.14776507,0.030338759,-0.01846056,0.009517991,0.0034727904,0.007246884,0.015436005,0.058226254,-0.037932027,-0.04309255,0.09766471,0.014914252,0.03149386,0.10146584,0.009303289,0.05649276,0.04743103,-0.016993523,0.054828145,0.033858124,-0.059207607,-0.027288152,0.09254907,0.07817234,-0.047911037,-0.023988279,-0.067968085,-0.03140125,-0.02434741,-0.017226815,0.050405838,0.048384074,0.10386314,-0.05366119,-0.048218876,0.022471255,-0.04470827,-0.055776954,0.0146418335,-0.03505756,0.041757654,0.0076765255,0.0637766}\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies\nDESCRIPTION: This snippet specifies the Python dependencies required for the PostgresML project. Each line indicates a package name and its specific version. This file is typically used with pip to install the necessary packages for the project to run correctly.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/examples/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n\"aiohttp==3.8.5\naiosignal==1.3.1\nasync-timeout==4.0.3\nattrs==23.1.0\ncertifi==2023.7.22\ncharset-normalizer==3.2.0\ndatasets==2.14.4\ndill==0.3.7\nfilelock==3.12.3\nfrozenlist==1.4.0\nfsspec==2023.6.0\nhuggingface-hub==0.16.4\nidna==3.4\nmarkdown-it-py==3.0.0\nmdurl==0.1.2\nmultidict==6.0.4\nmultiprocess==0.70.15\nnumpy==1.25.2\npackaging==23.1\npandas==2.0.3\npgml==1.0.0\npyarrow==13.0.0\nPygments==2.16.1\npython-dateutil==2.8.2\npython-dotenv==1.0.0\npytz==2023.3\nPyYAML==6.0.1\nrequests==2.31.0\nrich==13.5.2\nsix==1.16.0\ntqdm==4.66.1\ntyping_extensions==4.7.1\ntzdata==2023.3\nurllib3==2.0.4\nxxhash==3.3.0\nyarl==1.9.2\"\n```\n\n----------------------------------------\n\nTITLE: Environment Configuration\nDESCRIPTION: Sample environment variables configuration for database connection and API keys.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-apps/pgml-chat/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nDATABASE_URL=<POSTGRES_DATABASE_URL starts with postgres://>\nOPENAI_API_KEY=<OPENAI_API_KEY> # Optional\n```\n\n----------------------------------------\n\nTITLE: Installing Python 3.7 Development Libraries\nDESCRIPTION: This bash script adds a PPA repository and installs Python 3.7 development libraries, which are required if the system's default Python version is 3.6 or lower.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo add-apt-repository ppa:deadsnakes/ppa && \\\nsudo apt update && sudo apt install -y libpython3.7-dev\n```\n\n----------------------------------------\n\nTITLE: Example Rust Module for Dropdown Component\nDESCRIPTION: Rust struct definition for a dropdown component that implements the Sailfish TemplateOnce trait, connecting to the HTML template.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_8\n\nLANGUAGE: rust\nCODE:\n```\nuse sailfish::TemplateOnce;\n\n#[derive(TemplateOnce)]\n#[template(path = \"dropdown/template.html\")]\npub struct Dropdown {\n\tpub value: String,\n}\n```\n\n----------------------------------------\n\nTITLE: Ingesting Documents with PostgresML Chatbot\nDESCRIPTION: Command to ingest documents into PostgresML, chunk them and generate embeddings for the knowledge base. Requires specifying root directory and collection name.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowledge-based-chatbots-part-i.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nLOG_LEVEL=DEBUG pgml-chat --root_dir <directory> --collection_name <collection_name> --stage ingest\n```\n\n----------------------------------------\n\nTITLE: CPU-based Text Generation with Standard GPT2\nDESCRIPTION: Demonstrates text generation using standard GPT2 model forced to run on CPU, showing performance comparison with quantized version.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"gpt2\",\n      \"device\": \"cpu\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Team Member Profile Grid Layout in HTML\nDESCRIPTION: HTML structure for displaying team member profiles in a grid layout with images, names and GitHub links. Each profile is contained in a card element with consistent formatting.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/docs/about/team.md#2025-04-19_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<ul class=\"team grid\">\n    <li class=\"card\">\n        <img src=\"/dashboard/static/images/team/montana.jpg\" alt=\"Montana Low\" />\n        <h3>Montana Low</h3>\n        <a href=\"https://github.com/montanalow\"><span class=\"twemoji\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\"><path d=\"M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34-.46-1.16-1.11-1.47-1.11-1.47-.91-.62.07-.6.07-.6 1 .07 1.53 1.03 1.53 1.03.87 1.52 2.34 1.07 2.91.83.09-.65.35-1.09.63-1.34-2.22-.25-4.55-1.11-4.55-4.92 0-1.11.38-2 1.03-2.71-.1-.25-.45-1.29.1-2.64 0 0 .84-.27 2.75 1.02.79-.22 1.65-.33 2.5-.33.85 0 1.71.11 2.5.33 1.91-1.29 2.75-1.02 2.75-1.02.55 1.35.2 2.39.1 2.64.65.71 1.03 1.6 1.03 2.71 0 3.82-2.34 4.66-4.57 4.91.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2Z\"></path></svg></span> montanalow</a>\n    </li>\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Downloading Environment Template\nDESCRIPTION: Command to download the .env.template file from the PostgresML GitHub repository.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-apps/pgml-chat/.env.template\n```\n\n----------------------------------------\n\nTITLE: Creating Todo Items via cURL\nDESCRIPTION: Example of creating a todo item using the REST API endpoint with cURL.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/using-postgresml-with-django-and-embedding-search.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl \\\n    --silent \\\n    -X POST \\\n    -d '{\"description\": \"Make a New Year resolution list\", \"due_date\": \"2025-01-01\"}' \\\n    -H 'Content-Type: application/json' \\\n    http://localhost:8000/api/todo/\n```\n\n----------------------------------------\n\nTITLE: Bundling JavaScript and Sass Files with pgml-components\nDESCRIPTION: Command to read and bundle all JavaScript and Sass files in the project using Rollup and Sass compiler, generating optimized production assets.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncargo pgml-components bundle\n```\n\n----------------------------------------\n\nTITLE: Font Squirrel Configuration Settings in JSON\nDESCRIPTION: Detailed configuration settings for Font Squirrel's font-face generator including output formats, font metrics, spacing adjustments, and file naming parameters. The configuration specifies generation of TTF, WOFF, WOFF2, and EOTZ formats with various font optimization settings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/static/fonts/generator_config.txt#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"mode\":\"expert\",\"formats\":[\"ttf\",\"woff\",\"woff2\",\"eotz\"],\"tt_instructor\":\"default\",\"fix_gasp\":\"xy\",\"fix_vertical_metrics\":\"Y\",\"metrics_ascent\":\"\",\"metrics_descent\":\"\",\"metrics_linegap\":\"\",\"add_spaces\":\"Y\",\"add_hyphens\":\"Y\",\"fallback\":\"none\",\"fallback_custom\":\"100\",\"webonly\":\"Y\",\"options_subset\":\"none\",\"subset_custom\":\"\",\"subset_custom_range\":\"\",\"subset_ot_features_list\":\"\",\"css_stylesheet\":\"stylesheet.css\",\"filename_suffix\":\"-webfont\",\"emsquare\":\"2048\",\"spacing_adjustment\":\"0\",\"rememberme\":\"Y\"}\n```\n\n----------------------------------------\n\nTITLE: Running PostgresML Dashboard Tests\nDESCRIPTION: This bash command runs the tests for the PostgresML dashboard.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncargo test\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Project Settings\nDESCRIPTION: Basic dbt project configuration defining the project name and version in dbt_project.yml\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/examples/dbt/embeddings/README.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'pgml_flow'\nversion: '1.0.0'\n```\n\n----------------------------------------\n\nTITLE: CLI Chat Interface Command\nDESCRIPTION: Command to start the chatbot in CLI mode with error-level logging.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-apps/pgml-chat/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nLOG_LEVEL=ERROR pgml-chat --collection_name <collection_name> --stage chat --chat_interface cli\n```\n\n----------------------------------------\n\nTITLE: Example Sass Stylesheet for Dropdown Component\nDESCRIPTION: Basic Sass stylesheet for a dropdown component, targeting elements with the dropdown controller attribute.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_7\n\nLANGUAGE: css\nCODE:\n```\ndiv[data-controller=\"dropdown\"] {\n\twidth: 100%;\n\theight: 100px;\n\n\tbackground: red;\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying XGBoost Library Version Requirement\nDESCRIPTION: Defines the required version of XGBoost library as 1.6.2 for the PostgresML project. This ensures compatibility and consistent behavior across installations.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-extension/tests/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nxgboost==1.6.2\n```\n\n----------------------------------------\n\nTITLE: Text Generation with MPT Model\nDESCRIPTION: Implements text generation using the MPT-7B-Storywriter model, demonstrating usage with different model architecture.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/announcing-gptq-and-ggml-quantized-llm-support-for-huggingface-transformers.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task => '{\n      \"task\": \"text-generation\",\n      \"model\": \"TheBloke/MPT-7B-Storywriter-GGML\",\n      \"model_type\": \"mpt\"\n    }'::JSONB,\n    inputs => ARRAY[\n        'Once upon a time,'\n    ],\n    args => '{\"max_new_tokens\": 32}'::JSONB\n);\n```\n\n----------------------------------------\n\nTITLE: Installing Bootstrap via Package Managers\nDESCRIPTION: Commands for installing Bootstrap 5.3.0-alpha1 using different package managers including npm, yarn, Composer, and NuGet.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/static/css/bootstrap-5.3.0-alpha1/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/twbs/bootstrap.git\nnpm install bootstrap@v5.3.0-alpha1\nyarn add bootstrap@v5.3.0-alpha1\ncomposer require twbs/bootstrap:5.3.0-alpha1\nInstall-Package bootstrap\nInstall-Package bootstrap.sass\n```\n\n----------------------------------------\n\nTITLE: Installing NPM Dependencies\nDESCRIPTION: Before running the PostgresML examples, it is necessary to install the required dependencies using NPM. This ensures that all the necessary packages are available for the examples to function correctly.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/javascript/examples/README.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm i\n```\n\n----------------------------------------\n\nTITLE: Setting Up Watch Mode for Automatic Asset Bundling\nDESCRIPTION: Command using cargo-watch to automatically run the bundle command whenever files in the src/ or static/ directories change, ignoring the bundle files themselves.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncargo watch \\\n\t--exec 'pgml-components bundle' \\\n\t--watch src/ \\\n\t--watch static/ \\\n\t--ignore bundle.*.*\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Extension in PostgreSQL\nDESCRIPTION: This snippet shows how to create the vector extension in PostgreSQL, which is required for efficient storage and indexing of embeddings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_13\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE EXTENSION vector;\n```\n\n----------------------------------------\n\nTITLE: Bootstrap Directory Structure\nDESCRIPTION: Detailed file structure showing the organization of Bootstrap's compiled CSS and JavaScript files, including grid system, utilities, and bundled components.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/static/css/bootstrap-5.3.0-alpha1/README.md#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nbootstrap/\n css/\n    bootstrap-grid.css\n    bootstrap-grid.css.map\n    bootstrap-grid.min.css\n    bootstrap-grid.min.css.map\n    bootstrap-grid.rtl.css\n    bootstrap-grid.rtl.css.map\n    bootstrap-grid.rtl.min.css\n    bootstrap-grid.rtl.min.css.map\n    bootstrap-reboot.css\n    bootstrap-reboot.css.map\n    bootstrap-reboot.min.css\n    bootstrap-reboot.min.css.map\n    bootstrap-reboot.rtl.css\n    bootstrap-reboot.rtl.css.map\n    bootstrap-reboot.rtl.min.css\n    bootstrap-reboot.rtl.min.css.map\n    bootstrap-utilities.css\n    bootstrap-utilities.css.map\n    bootstrap-utilities.min.css\n    bootstrap-utilities.min.css.map\n    bootstrap-utilities.rtl.css\n    bootstrap-utilities.rtl.css.map\n    bootstrap-utilities.rtl.min.css\n    bootstrap-utilities.rtl.min.css.map\n    bootstrap.css\n    bootstrap.css.map\n    bootstrap.min.css\n    bootstrap.min.css.map\n    bootstrap.rtl.css\n    bootstrap.rtl.css.map\n    bootstrap.rtl.min.css\n    bootstrap.rtl.min.css.map\n js/\n     bootstrap.bundle.js\n     bootstrap.bundle.js.map\n     bootstrap.bundle.min.js\n     bootstrap.bundle.min.js.map\n     bootstrap.esm.js\n     bootstrap.esm.js.map\n     bootstrap.esm.min.js\n     bootstrap.esm.min.js.map\n     bootstrap.js\n     bootstrap.js.map\n     bootstrap.min.js\n     bootstrap.min.js.map\n```\n\n----------------------------------------\n\nTITLE: Installing Huggingface Transformers Dependencies\nDESCRIPTION: This bash command installs the Python dependencies required for using Huggingface transformers with PostgresML.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/contributing.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nsudo pip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: pgBackRest Retention Configuration\nDESCRIPTION: Configuration settings for pgBackRest retention policy to maintain 14 days of backups and WAL files.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/backups.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n[global]\nrepo1-retention-full=14\nrepo1-retention-archive=14\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Setting Environment\nDESCRIPTION: This snippet provides commands to install required dependencies for the project and set the DATABASE_URL environment variable, which is essential for connecting to the PostgreSQL database.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/python/examples/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\nexport DATABASE_URL={YOUR DATABASE URL}\n```\n\n----------------------------------------\n\nTITLE: Enabling Prepared Statements in PgCat Configuration\nDESCRIPTION: Configuration setting in pgcat.toml to enable prepared statements feature in PgCat.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/making-postgres-30-percent-faster-in-production.md#2025-04-19_snippet_2\n\nLANGUAGE: toml\nCODE:\n```\nprepared_statements = true\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt Project Name and Version\nDESCRIPTION: Basic dbt project configuration defining the project name and version in dbt_project.yml.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/storage-and-retrieval/llm-based-pipelines-with-postgresml-and-dbt-data-build-tool.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'pgml_flow'\nversion: '1.0.0'\n```\n\n----------------------------------------\n\nTITLE: Loading Flight Data\nDESCRIPTION: Commands to download flight data and load it into PostgreSQL, including creating an index for optimization\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/blog/benchmarks/python_microservices_vs_postgresml/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -L -o ~/Desktop/flights.csv https://static.postgresml.org/benchmarks/flights.csv\n```\n\nLANGUAGE: postgresql\nCODE:\n```\n\\copy flights_delay_mat FROM '~/Desktop/flights.csv' CSV HEADER;\n\nCREATE INDEX ON flights_delay_mat USING btree(originairportid, year, month, dayofmonth);\n```\n\n----------------------------------------\n\nTITLE: SQL Operation Keywords in PgCat Query Handling\nDESCRIPTION: Example SQL operation keywords that PgCat uses to differentiate between read and write operations for query routing purposes.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgcat/features.md#2025-04-19_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT\n```\n\nLANGUAGE: sql\nCODE:\n```\nINSERT\n```\n\nLANGUAGE: sql\nCODE:\n```\nUPDATE\n```\n\n----------------------------------------\n\nTITLE: Annotated Rust Struct for Automatic Translation\nDESCRIPTION: A Rust struct annotated with translation macros to enable automatic generation of language bindings\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/rust-bridge/README.md#2025-04-19_snippet_1\n\nLANGUAGE: rust\nCODE:\n```\n#[derive(alias, Debug, Clone)]\npub struct Test {\n    x: i32,\n}\n\n#[alias_methods(new, get)]\nimpl Test {\n    pub fn new(x: i32) -> Self {\n        Self { x }\n    }\n\n    pub fn get(&self) -> i32 {\n        self.x\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: MindsDB Sentiment Analysis - Simple Results\nDESCRIPTION: Simplified query returning only the sentiment label from MindsDB model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/mindsdb-vs-postgresml.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT sentiment\nFROM mindsdb.sentiment_classifier\nWHERE text = 'I am so excited to benchmark deep learning models in SQL. I can not wait to see the results!'\n```\n\n----------------------------------------\n\nTITLE: Adding Generated tsvector Column in PostgreSQL\nDESCRIPTION: Alters the table to add a generated column that automatically creates and stores the tsvector for combined title and body text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/improve-search-results-with-machine-learning.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nALTER TABLE documents\nADD COLUMN title_and_body_text tsvector\nGENERATED ALWAYS AS (to_tsvector('english', title || ' ' || body )) STORED;\n```\n\n----------------------------------------\n\nTITLE: Restarting PostgreSQL Service\nDESCRIPTION: Command to restart the PostgreSQL service to apply the configuration changes.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/building-from-source.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nsudo service postgresql restart\n```\n\n----------------------------------------\n\nTITLE: Generated Py03 Compatible Rust Code\nDESCRIPTION: Automatically generated Rust code with Py03 bindings, showing the wrapper struct and translated method implementations\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/rust-bridge/README.md#2025-04-19_snippet_2\n\nLANGUAGE: rust\nCODE:\n```\npub struct Test {\n    x: i32,\n}\n\n#[cfg(feature = \"python\")]\n#[pyo3::pyclass]\npub struct TestPython {\n    pub wrapped: std::boxed::Box<Test>,\n}\n\nimpl Test {\n    pub fn new(x: i32) -> Self {\n        Self { x }\n    }\n\n    pub fn get(&self) -> i32 {\n        self.x\n    }\n}\n\n#[cfg(feature = \"python\")]\nimpl TestPython {\n    pub fn new<'a>(x: i32, py: pyo3::Python<'a>) -> pyo3::PyResult<Self> {\n        use rust_bridge::python::CustomInto;\n        let x: i32 = x.custom_into();\n        let x = Test::new(x);\n        let x: Self = x.custom_into();\n        Ok(x)\n    }\n\n    pub fn get<'a>(&mut self, py: pyo3::Python<'a>) -> pyo3::PyResult<i32> {\n        use rust_bridge::python::CustomInto;\n        let mut wrapped: &Test = self.custom_into();\n        let x = wrapped.get();\n        let x: i32 = x.custom_into();\n        Ok(x)\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Building and Installing PgCat from Source\nDESCRIPTION: Commands to compile PgCat in release mode and install it into system directories\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/pooler.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd pgcat && \\\ncarbo build --release && \\\nsudo cp target/release/pgcat /usr/local/bin/pgcat && \\\nsudo cp pgcat.toml /etc/pgcat.toml.example\n```\n\n----------------------------------------\n\nTITLE: Example of Using a Component in a Sailfish Template\nDESCRIPTION: Example of importing and using a custom component within another Sailfish template using the <%+ %> syntax.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_9\n\nLANGUAGE: html\nCODE:\n```\n<% use crate::components::Dropdown; %>\n\n<div class=\"row\">\n\t<div class=\"col-6\">\n\t\t<%+ Dropdown::new() %>\n\t</div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Example Sailfish HTML Template for Dropdown Component\nDESCRIPTION: Basic HTML template for a dropdown component that connects to a Stimulus controller using the data-controller attribute.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<div data-controller=\"dropdown\">\n</div>\n```\n\n----------------------------------------\n\nTITLE: Starting PgCat Service\nDESCRIPTION: Command to start PgCat service when installed via APT\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/self-hosting/pooler.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nsudo service pgcat start\n```\n\n----------------------------------------\n\nTITLE: Ranking Search Results with Machine Learning in PostgreSQL\nDESCRIPTION: This complex query demonstrates how to integrate machine learning predictions into a search query. It uses a CTE to perform initial ranking based on text search, then applies the ML model to re-rank results based on predicted click probability.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-to-improve-search-results-with-machine-learning.md#2025-04-19_snippet_12\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH first_pass_ranked_documents AS (\n  SELECT\n    -- Compute the ts_rank for the title and body text of each document \n    ts_rank(title_and_body_text, to_tsquery('english', 'second | title')) AS title_and_body_rank,       \n    ts_rank(to_tsvector('english', title), to_tsquery('english', 'second | title')) AS title_rank, \n    ts_rank(to_tsvector('english', body), to_tsquery('english', 'second | title')) AS body_rank,\n    * \n  FROM documents \n  WHERE title_and_body_text @@ to_tsquery('english', 'second | title')\n  ORDER BY title_and_body_rank DESC\n  LIMIT 100\n)\nSELECT\n    -- Use the ML model to predict the probability that a user will click on the result\n    pgml.predict('Search Ranking', array[title_rank, body_rank]) AS ml_rank,\n    *\nFROM first_pass_ranked_documents\nORDER BY ml_rank DESC\nLIMIT 10;\n```\n\n----------------------------------------\n\nTITLE: Downloading Environment Template\nDESCRIPTION: Command to download the environment template file from PostgresML Github repository.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-apps/pgml-chat/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-apps/pgml-chat/.env.template\n```\n\n----------------------------------------\n\nTITLE: Vector Search with Re-ranking in JavaScript\nDESCRIPTION: This JavaScript code snippet demonstrates how to perform a vector search with re-ranking. It uses the `collection.vector_search` method, passing in a query object with the `rerank` key set to specify the re-ranking model, query, and number of documents to re-rank. The `pipeline` variable is a dependency, assumed to be a pre-configured pipeline for vector search.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/korvus/guides/vector-search.md#2025-04-19_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst results = await collection.vector_search(\n  {\n    query: {\n      fields: {\n        body: {\n          query: \"What is the best database?\", parameters: {\n          prompt:\n              \"Represent this sentence for searching relevant passages: \",\n          }\n        },\n      },\n    },\n    rerank: {\n        model: \"mixedbread-ai/mxbai-rerank-base-v1\",\n        query: \"What is the best database?\",\n        num_documents_to_rerank: 100,\n    },\n    limit: 5,\n  },\n  pipeline,\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Discord Environment Variables\nDESCRIPTION: Environment variable required for setting up the Discord chat interface.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nDISCORD_BOT_TOKEN=<DISCORD_BOT_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Setting Environment Variables in Shell\nDESCRIPTION: The shell commands provided are used to install necessary npm packages and set the DATABASE_URL environment variable for the SDK. The commands ensure the environment is prepared for running the example application. The DATABASE_URL must be correctly set, either via direct export or using a .env file.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/javascript/examples/webpack/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm i\nexport DATABASE_URL={YOUR DATABASE URL}\n```\n\n----------------------------------------\n\nTITLE: Analyzing Label Distribution in IMDB Dataset\nDESCRIPTION: Queries the shuffled IMDB dataset view to count occurrences of each label, providing insights into class distribution.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/llms/fine-tuning.md#2025-04-19_snippet_2\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    class,\n    COUNT(*) AS label_count\nFROM pgml.imdb_shuffled_view\nGROUP BY class\nORDER BY class;\n```\n\n----------------------------------------\n\nTITLE: Starting Discord Chat Interface\nDESCRIPTION: Command to start the Discord chat interface for interacting with the chatbot.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/TODO/chatbots.md#2025-04-19_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npgml-chat --collection_name <collection_name> --stage chat --chat_interface discord\n```\n\n----------------------------------------\n\nTITLE: Generating Single Embedding with PostgresML\nDESCRIPTION: This snippet demonstrates how to use the pgml.embed function to generate an embedding for a single text input using the Alibaba-NLP/gte-base-en-v1.5 model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/generating-llm-embeddings-with-open-source-models-in-postgresml.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed('Alibaba-NLP/gte-base-en-v1.5', 'passage: hi mom');\n```\n\n----------------------------------------\n\nTITLE: Finding Customer by Sentiment Similarity using Vector Embeddings in PostgreSQL\nDESCRIPTION: Searches for a customer whose embedding matches a specific sentiment about Star Wars movies using vector similarity (cosine similarity). The query uses the GTE-base model to generate embeddings and compares them against stored customer embeddings.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/personalization.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH request AS (\n  SELECT pgml.embed(\n    'Alibaba-NLP/gte-base-en-v1.5',\n    'query: I love all Star Wars, but Empire Strikes Back is particularly amazing'\n  )::vector(1024) AS embedding\n)\n\nSELECT\n  id,\n  total_reviews,\n  star_rating_avg,\n  1 - (\n    movie_embedding_e5_large <=> (SELECT embedding FROM request)\n  ) AS cosine_similarity\nFROM customers\nORDER BY cosine_similarity DESC\nLIMIT 1;\n```\n\n----------------------------------------\n\nTITLE: Markdown Table of Contents Structure\nDESCRIPTION: Hierarchical documentation structure using markdown showing the organization of PostgresML machine learning documentation topics.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/test.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Table of contents\n\n* [Machine Learning](machine-learning/README.md)\n    * [Natural Language Processing](machine-learning/natural-language-processing/README.md)\n        * [Embeddings](machine-learning/natural-language-processing/embeddings.md)\n        * [Fill Mask](machine-learning/natural-language-processing/fill-mask.md)\n```\n\n----------------------------------------\n\nTITLE: Non-Personalized Movie Search Query\nDESCRIPTION: Reference query showing the baseline non-personalized search implementation using only vector similarity and rating scores without customer preferences.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/personalize-embedding-results-with-application-data-in-your-database.md#2025-04-19_snippet_6\n\nLANGUAGE: postgresql\nCODE:\n```\n| title                                                | total_reviews | star_rating_avg |       final_score |    star_rating_score | cosine_similarity |\n| ---------------------------------------------------- | -------------: | ----------------: | -----------------: | ---------------------: | -----------------: |\n| Forbidden Planet (Two-Disc 50th Anniversary Edition) |            255 |              4.82 | 1.8216832158805154 | 0.96392156862745098000 | 0.8577616472530644 |\n```\n\n----------------------------------------\n\nTITLE: Styling Team Profile Images with CSS\nDESCRIPTION: CSS style rule to make team member profile images circular using border-radius property\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/docs/about/team.md#2025-04-19_snippet_0\n\nLANGUAGE: CSS\nCODE:\n```\nul.team img {\n    border-radius: 50%;\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying pgml-chat command-line help\nDESCRIPTION: Command to show the help information for pgml-chat, displaying usage instructions, required arguments, and available options for different stages and interfaces.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowledge-based-chatbots-part-i.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n(pgml-bot-builder-py3.9) pgml-chat % pgml-chat --help\nusage: pgml-chat [-h] --collection_name COLLECTION_NAME [--root_dir ROOT_DIR] [--stage {ingest,chat}] [--chat_interface {cli, slack, discord}]\n\nPostgresML Chatbot Builder\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --collection_name COLLECTION_NAME\n                        Name of the collection (schema) to store the data in PostgresML database (default: None)\n  --root_dir ROOT_DIR   Input folder to scan for markdown files. Required for ingest stage. Not required for chat stage (default: None)\n  --stage {ingest,chat}\n                        Stage to run (default: chat)\n  --chat_interface {cli, slack, discord}\n                        Chat interface to use (default: cli)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions\nDESCRIPTION: Demonstrates using the trained model to make predictions on the digits dataset.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/supervised-learning/README.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT\n    target,\n    pgml.predict('Handwritten Digit Image Classifier', image) AS prediction\nFROM pgml.digits \nLIMIT 10;\n```\n\nLANGUAGE: plsql\nCODE:\n```\n target | prediction\n--------+------------\n      0 |          0\n      1 |          1\n      2 |          2\n      3 |          3\n      4 |          4\n      5 |          5\n      6 |          6\n      7 |          7\n      8 |          8\n      9 |          9\n(10 rows)\n```\n\n----------------------------------------\n\nTITLE: Configuring Base Prompt Template for PostgresML Chatbot\nDESCRIPTION: Template string for the base prompt that structures how the chatbot should respond. Includes placeholders for context and question with specific response formatting instructions.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowledge-based-chatbots-part-i.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nBASE_PROMPT=\"Given relevant parts of a document and a question, create a final answer.\\ \n                Include a SQL query in the answer wherever possible. \\\n                Use the following portion of a long document to see if any of the text is relevant to answer the question.\\\n                \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\n \\\n                If the context is empty then ask for clarification and suggest user to send an email to team@postgresml.org or join PostgresML [Discord](https://discord.gg/DmyJP3qJ7U).\"\n```\n\n----------------------------------------\n\nTITLE: Training a Classification Model with XGBoost in PostgresML\nDESCRIPTION: This snippet demonstrates how to use the pgml.train() function to train an XGBoost classification model on the pgml.digits dataset. It specifies project name, task, relation name, target column, algorithm, and hyperparameters.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/quick-start-with-docker.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT * FROM pgml.train(\n    project_name => 'My First PostgresML Project',\n    task => 'classification',\n    relation_name => 'pgml.digits',\n    y_column_name => 'target',\n    algorithm => 'xgboost',\n    hyperparams => '{\n        \"n_estimators\": 25\n    }'\n);\n```\n\n----------------------------------------\n\nTITLE: Output Type Conversion for Python Interoperability\nDESCRIPTION: Function to convert Rust output types to Python-compatible types, with special handling for async functions. This is part of the signature translation phase of the macro processing.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/how-we-generate-javascript-and-python-sdks-from-our-canonical-rust-sdk.md#2025-04-19_snippet_10\n\nLANGUAGE: rust\nCODE:\n```\nfn convert_output_type(\n     ty: &SupportedType,\n     method: &GetImplMethod,\n) -> (\n     Option<proc_macro2::TokenStream>\n) {\n     if method.is_async {\n          Some(quote! {PyResult<&'a PyAny>})\n     } else {\n          let ty = t\n               .to_type()\n               .unwrap();\n          Some(quote! {PyResult<#ty>})\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing PostgresML Connection - JavaScript\nDESCRIPTION: Example code demonstrating how to establish a connection to PostgresML and create a chat completion using the JavaScript SDK. Shows initialization and basic usage with a LLaMA model.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/getting-started/connect-your-app.md#2025-04-19_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst pgml = require(\"pgml\");\n\nconst main = () => {\n    const client = pgml.newOpenSourceAI();\n    const results = client.chat_completions_create(\n          \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n          [\n              {\n                  role: \"system\",\n                  content: \"You are a friendly chatbot who always responds in the style of a pirate\",\n              },\n              {\n                  role: \"user\",\n                  content: \"How many helicopters can a human eat in one sitting?\",\n              },\n          ],\n    );\n    console.log(results);\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with PostgreSQL and PostGresML\nDESCRIPTION: This snippet demonstrates how to use the pgml.embed function to generate embeddings from text in PostgreSQL. It uses the 'mixedbread-ai/mxbai-embed-large-v1' model to create a vector representation of the input text.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/semantic-search-in-postgres-in-15-minutes.md#2025-04-19_snippet_0\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.embed('mixedbread-ai/mxbai-embed-large-v1', 'Generating embeddings in Postgres is fun!');\n```\n\n----------------------------------------\n\nTITLE: Enabling Timing for Performance Monitoring in PostgreSQL\nDESCRIPTION: This code snippet enables the timing option in PostgreSQL, which displays the execution time of SQL statements. It helps in measuring query performance and understanding latency during benchmarking tests.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/guides/embeddings/vector-similarity.md#2025-04-19_snippet_8\n\nLANGUAGE: postgresql\nCODE:\n```\n\"\\timing on\"\n```\n\n----------------------------------------\n\nTITLE: Batch Prediction with Joins Example\nDESCRIPTION: Complex example showing how to join batch predictions with other tables using unique identifiers.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/api/pgml.predict/batch-predictions.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nWITH predictions AS (\n\tSELECT\n\t\t--\n\t\t-- Prediction\n\t\t--\n\t\tpgml.predict_batch(\n\t\t\t'My Bot Detector',\n\t\t\tarray_agg(ARRAY[account_age, city, last_login])\n\t\t) AS prediction,\n\n\t\t--\n\t\t-- The pass-through unique identifier for each row\n\t\t--\n\t\tunnest(\n\t\t\tarray_agg(user_id)\n\t\t) AS target\n\tFROM users\n\n\t--\n\t-- Filter which rows to pass to pgml.predict_batch()\n\t--\n\tWHERE last_login > NOW() - INTERVAL '1 minute'\n)\nSELECT prediction, email, ip_address\nFROM users\nINNER JOIN predictions\nON users.user_id = predictions.user_id\n```\n\n----------------------------------------\n\nTITLE: Loading Serverless Pricing Information with Turbo Frame in HTML\nDESCRIPTION: This snippet uses a Turbo Frame to dynamically load pricing information for PostgresML Serverless engines. It provides a placeholder message while the content is being loaded.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/cloud/serverless.md#2025-04-19_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<turbo-frame id=\"serverless-pricing-turboframe\" src=\"/dashboard/serverless_pricing/turboframe?style=marketing\">\nLoading our current pricing model...\n</turbo-frame>\n```\n\n----------------------------------------\n\nTITLE: Installing JavaScript SDK with npm\nDESCRIPTION: This snippet shows how to install the JavaScript SDK using npm, which is a prerequisite for building vector search applications with PostgreSQL in JavaScript. Ensure Node.js and npm are installed on your system.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-sdks/pgml/javascript/README.md#2025-04-19_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nnpm i pgml\n```\n\n----------------------------------------\n\nTITLE: Custom Django Expression for PostgresML Embedding\nDESCRIPTION: Implements a custom Django Expression class to generate embeddings using the Alibaba-NLP/gte-base-en-v1.5 model via PostgresML.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/using-postgresml-with-django-and-embedding-search.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass EmbedSmallExpression(models.Expression):\n    output_field = VectorField(null=False, blank=False, dimensions=768)\n\n    def __init__(self, field):\n        self.embedding_field = field\n\n    def as_sql(self, compiler, connection, template=None):\n        return f\"pgml.embed('Alibaba-NLP/gte-base-en-v1.5', {self.embedding_field})\", None\n```\n\n----------------------------------------\n\nTITLE: Downloading and copying the environment template file\nDESCRIPTION: Commands to download the .env.template file from PostgresML's GitHub repository and create a local copy as .env for configuration.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowledge-based-chatbots-part-i.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget https://raw.githubusercontent.com/postgresml/postgresml/master/pgml-apps/pgml-chat/.env.template\ncp .env.template .env\n```\n\n----------------------------------------\n\nTITLE: Generating Text with pgml.transform Function\nDESCRIPTION: SQL function to generate or transform text using language models with customizable arguments for the pipeline.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/README.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.transform(\n    task   => TEXT OR JSONB,     -- Pipeline initializer arguments\n    inputs => TEXT[] OR BYTEA[], -- inputs for inference\n    args   => JSONB              -- (optional) arguments to the pipeline.\n)\n```\n\n----------------------------------------\n\nTITLE: Setting System Prompt for PostgresML Chatbot\nDESCRIPTION: Configuration of the system prompt that defines the chatbot's identity and purpose. Sets the bot name and context about PostgresML.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowledge-based-chatbots-part-i.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nSYSTEM_PROMPT=\"You are an assistant to answer questions about an open source software named PostgresML. Your name is PgBot. You are based out of San Francisco, California.\"\n```\n\n----------------------------------------\n\nTITLE: Running CLI Chat Interface for PostgresML Chatbot\nDESCRIPTION: Command to start the chatbot in CLI mode for testing and evaluation purposes. Uses specified collection name and suppresses detailed logging.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowledge-based-chatbots-part-i.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nLOG_LEVEL=ERROR pgml-chat --collection_name <collection_name> --stage chat --chat_interface cli\n```\n\n----------------------------------------\n\nTITLE: Creating a Subscription for Logical Replication in PostgresML\nDESCRIPTION: This SQL command creates a subscription in PostgresML to the publication in the primary database, initiating the replication process. It specifies the connection details and the publication name.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/introduction/import-your-data/logical-replication/README.md#2025-04-19_snippet_4\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE SUBSCRIPTION postgresml\nCONNECTION 'postgres://user:password@your-production-db.amazonaws.com:5432/prodution_db'\nPUBLICATION postgresml;\n```\n\n----------------------------------------\n\nTITLE: Configuring Slack Integration Environment Variables\nDESCRIPTION: Environment variable configuration for connecting the chatbot to Slack. Requires bot token and app token from Slack API.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/pgml-chat-a-command-line-tool-for-deploying-low-latency-knowledge-based-chatbots-part-i.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nSLACK_BOT_TOKEN=<SLACK_BOT_TOKEN>\nSLACK_APP_TOKEN=<SLACK_APP_TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Deleting a Component and Rebundling Assets\nDESCRIPTION: Commands to manually delete a component by removing its directory and then rebundling assets to update the project.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/packages/cargo-pgml-components/README.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nrm -r src/components/dropdown\ncargo pgml-components bundle\n```\n\n----------------------------------------\n\nTITLE: Training Additional Models in PostgresML\nDESCRIPTION: Trains additional linear regression and lasso models on the same dataset for comparison.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/static/images/gym/quick_start.md#2025-04-19_snippet_5\n\nLANGUAGE: postgresql\nCODE:\n```\n-- Simple linear regression.\nSELECT * FROM pgml.train(\n\t'My First Project',\n\talgorithm => 'linear');\n\n-- The Lasso (much fancier linear regression).\nSELECT * FROM pgml.train(\n\t'My First Project',\n\talgorithm => 'lasso');\n```\n\n----------------------------------------\n\nTITLE: Benchmarking PostgresML Performance\nDESCRIPTION: Command to run performance benchmarking using pgbench with 1000 transactions and 10 concurrent clients\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-dashboard/content/blog/benchmarks/python_microservices_vs_postgresml/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npgbench -f pgbench.sql -p 28813 -h 127.0.0.1 pgml -t 1000 -c 10 -j 10\n```\n\n----------------------------------------\n\nTITLE: PostgresML Model Training Example\nDESCRIPTION: Example of training a machine learning model using PostgresML's simplified API interface.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/blog/postgresml-is-moving-to-rust-for-our-2.0-release.md#2025-04-19_snippet_7\n\nLANGUAGE: postgresql\nCODE:\n```\nSELECT pgml.train(\n  project_name => 'Handwritten Digit Classifier',\n  task => 'classification',\n  relation_name => 'pgml.digits',\n  y_column_name => 'target',\n  algorithm => 'xgboost'\n);\n```\n\n----------------------------------------\n\nTITLE: Initializing PostgresML Extension\nDESCRIPTION: SQL commands to create the PostgresML extension and verify its version.\nSOURCE: https://github.com/postgresml/postgresml/blob/master/pgml-cms/docs/open-source/pgml/developers/quick-start-with-docker.md#2025-04-19_snippet_3\n\nLANGUAGE: postgresql\nCODE:\n```\nCREATE EXTENSION IF NOT EXISTS pgml;\nSELECT pgml.version();\n```"
  }
]
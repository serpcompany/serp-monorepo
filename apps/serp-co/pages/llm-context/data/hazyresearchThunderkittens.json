[
  {
    "owner": "hazyresearch",
    "repo": "thunderkittens",
    "content": "TITLE: FlashAttention-2 Kernel in CUDA using ThunderKittens\nDESCRIPTION: This CUDA kernel implements a simplified FlashAttention-2 algorithm using ThunderKittens primitives. It utilizes shared memory, tensor cores, and asynchronous operations to achieve high GPU utilization. Key parameters include the template argument `D` which determines the dimension size, and the `globals` struct which provides access to the input tensors.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/README.md#_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\n#include \"kittens.cuh\"\n\nusing namespace kittens;\n\nconstexpr int NUM_WORKERS = 4; // This kernel uses 4 worker warps per block, and 2 blocks per SM.\ntemplate<int D> constexpr size_t ROWS = 16*(128/D); // height of each worker tile (rows)\ntemplate<int D, typename T=bf16, typename L=row_l> using qkvo_tile = rt<T, ROWS<D>, D, L>;\ntemplate<int D, typename T=float> using attn_tile = rt<T, ROWS<D>, ROWS<D>>;\ntemplate<int D> using shared_tile = st_bf<ROWS<D>, D>;\ntemplate<int D> using global_layout = gl<bf16, -1, -1, -1, D>; // B, H, g.Qg.rows specified at runtime, D=64 known at compile time for this kernel\ntemplate<int D> struct globals { global_layout<D> Qg, Kg, Vg, Og; };\n\ntemplate<int D> __launch_bounds__(NUM_WORKERS*WARP_THREADS, 1)\n__global__ void attend_ker(const __grid_constant__ globals<D> g) {\n    using load_group = kittens::group<2>; // pairs of workers collaboratively load k, v tiles\n    int loadid = load_group::groupid(), workerid = kittens::warpid(); // which worker am I?\n    constexpr int LOAD_BLOCKS = NUM_WORKERS / load_group::GROUP_WARPS;\n    const int batch = blockIdx.z, head  = blockIdx.y, q_seq = blockIdx.x * NUM_WORKERS + workerid;\n\n    extern __shared__ alignment_dummy __shm[]; // this is the CUDA shared memory\n    shared_allocator al((int*)&__shm[0]);\n    // K and V live in shared memory. Here, we instantiate three tiles for a 3-stage pipeline.\n    shared_tile<D> (&k_smem)[LOAD_BLOCKS][3] = al.allocate<shared_tile<D>, LOAD_BLOCKS, 3>();\n    shared_tile<D> (&v_smem)[LOAD_BLOCKS][3] = al.allocate<shared_tile<D>, LOAD_BLOCKS, 3>();\n    // We also reuse this memory to improve coalescing of DRAM reads and writes.\n    shared_tile<D> (&qo_smem)[NUM_WORKERS] = reinterpret_cast<shared_tile<D>(&)[NUM_WORKERS]>(k_smem);\n    // Initialize all of the register tiles.\n    qkvo_tile<D, bf16> q_reg, k_reg; // Q and K are both row layout, as we use mma_ABt.\n    qkvo_tile<D, bf16, col_l> v_reg; // V is column layout, as we use mma_AB.\n    qkvo_tile<D, float> o_reg; // Output tile.\n    attn_tile<D, float> att_block; // attention tile, in float. (We want to use float wherever possible.)\n    attn_tile<D, bf16> att_block_mma; // bf16 attention tile for the second mma_AB. We cast right before that op.\n    typename attn_tile<D, float>::col_vec max_vec_last, max_vec, norm_vec; // these are column vectors for the in-place softmax.\n    // each warp loads its own Q tile of 16x64\n    if (q_seq*ROWS<D> < g.Qg.rows) {\n        load(qo_smem[workerid], g.Qg, {batch, head, q_seq, 0});  // going through shared memory improves coalescing of dram reads.\n        __syncwarp();\n        load(q_reg, qo_smem[workerid]);\n    }\n    __syncthreads();\n    // temperature adjustment. Pre-multiplying by lg2(e), too, so we can use exp2 later.\n    if constexpr(D == 64) mul(q_reg, q_reg, __float2bfloat16(0.125f * 1.44269504089));\n    else if constexpr(D == 128) mul(q_reg, q_reg, __float2bfloat16(0.08838834764f * 1.44269504089));\n    // initialize flash attention L, M, and O registers.\n    neg_infty(max_vec); // zero registers for the Q chunk\n    zero(norm_vec);\n    zero(o_reg);\n    // launch the load of the first k, v tiles\n    int kv_blocks = g.Qg.rows / (LOAD_BLOCKS*ROWS<D>), tic = 0;\n    load_group::load_async(k_smem[loadid][0], g.Kg, {batch, head, loadid, 0});\n    load_group::load_async(v_smem[loadid][0], g.Vg, {batch, head, loadid, 0});\n    // iterate over k, v for these q's that have been loaded\n    for(auto kv_idx = 0; kv_idx < kv_blocks; kv_idx++, tic=(tic+1)%3) {\n        int next_load_idx = (kv_idx+1)*LOAD_BLOCKS + loadid;\n        if(next_load_idx*ROWS<D> < g.Kg.rows) {\n            int next_tic = (tic+1)%3;\n            load_group::load_async(k_smem[loadid][next_tic], g.Kg, {batch, head, next_load_idx, 0});\n            load_group::load_async(v_smem[loadid][next_tic], g.Vg, {batch, head, next_load_idx, 0});\n            load_async_wait<2>(); // next k, v can stay in flight.\n        }\n        else load_async_wait(); // all must arrive\n        __syncthreads(); // Everyone's memory must be ready for the next stage.\n        // now each warp goes through all of the subtiles, loads them, and then does the flash attention internal alg.\n        #pragma unroll LOAD_BLOCKS\n        for(int subtile = 0; subtile < LOAD_BLOCKS && (kv_idx*LOAD_BLOCKS + subtile) < g.Qg.rows; subtile++) {\n            load(k_reg, k_smem[subtile][tic]); // load k from shared into registers\n            zero(att_block); // zero 16x16 attention tile\n            mma_ABt(att_block, q_reg, k_reg, att_block); // Q@K.T\n            copy(max_vec_last,  max_vec);\n            row_max(max_vec, att_block, max_vec); // accumulate onto the max_vec\n            sub_row(att_block, att_block, max_vec); // subtract max from attention -- now all <=0\n            exp2(att_block, att_block); // exponentiate the block in-place.\n            sub(max_vec_last, max_vec_last, max_vec); // subtract new max from old max to find the new normalization.\n            exp2(max_vec_last, max_vec_last); // exponentiate this vector -- this is what we need to normalize by.\n            mul(norm_vec, norm_vec, max_vec_last); // and the norm vec is now normalized.\n            row_sum(norm_vec, att_block, norm_vec); // accumulate the new attention block onto the now-rescaled norm_vec\n            copy(att_block_mma, att_block); // convert to bf16 for mma_AB\n            load(v_reg, v_smem[subtile][tic]); // load v from shared into registers.\n            mul_row(o_reg, o_reg, max_vec_last); // normalize o_reg in advance of mma_AB'ing onto it\n            mma_AB(o_reg, att_block_mma, v_reg, o_reg); // mfma onto o_reg with the local attention@V matmul.\n        }\n    }\n    div_row(o_reg, o_reg, norm_vec);\n    __syncthreads();\n    if (q_seq*ROWS<D> < g.Qg.rows) { // write out o.\n        store(qo_smem[workerid], o_reg); // going through shared memory improves coalescing of dram writes.\n        __syncwarp();\n        store(g.Og, qo_smem[workerid], {batch, head, q_seq, 0});\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setup Python Environment (conda)\nDESCRIPTION: Creates a new conda environment named 'dev' with Python 3.11, installs PyTorch, TorchVision, TorchAudio, Transformers, Einops, Hydra-Core, and Flash-Attention packages.  This environment is necessary to run the demos and benchmarks.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/based_demo/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n dev python=3.11\npip3 install torch torchvision torchaudio\npip install transformers\npip install einops\npip install hydra-core\npip install flash-attn\n```\n\n----------------------------------------\n\nTITLE: Installing GCC and Clang (C++20)\nDESCRIPTION: This snippet installs the necessary versions of GCC and Clang (specifically version 11 for both) to support C++20 compilation, a requirement for ThunderKittens.  It updates the package list, installs the compilers, and sets them as the default compilers using `update-alternatives`.  This ensures that the system uses the correct compiler versions for building ThunderKittens.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt update\nsudo apt install gcc-11 g++-11\n\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100 --slave /usr/bin/g++ g++ /usr/bin/g++-11\n\nsudo apt update\nsudo apt install clang-11\n```\n\n----------------------------------------\n\nTITLE: Install ThunderKittens Kernels\nDESCRIPTION: Installs the ThunderKittens kernels after configuring ``ThunderKittens/config.py`` to select \"based\". This installation step is crucial for utilizing the optimized kernels in subsequent demos and benchmarks. Assumes that the current directory contains `setup.py`.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/based_demo/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython setup.py install\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Environment Variables\nDESCRIPTION: This snippet sets the environment variables required to locate the CUDA installation. It defines `CUDA_HOME`, adds the CUDA binary directory to the `PATH`, and the CUDA library directory to `LD_LIBRARY_PATH`.  This allows the system to find the CUDA compiler (`nvcc`) and libraries, which are essential for compiling and running ThunderKittens kernels. Replace `/usr/local/cuda-12.6` with the actual CUDA installation path.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport CUDA_HOME=/usr/local/cuda-12.6\nexport PATH=${CUDA_HOME}/bin:${PATH}\nexport LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Conda Environment Creation and CUDA Installation\nDESCRIPTION: Creates a new conda environment named 'kittens' with Python 3.11 and installs CUDA 12.4.0 from the NVIDIA channel.  This environment is designed to be compatible with the Thunderkittens project. The environment must be activated after creation.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/docs/conda_setup.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n kittens python=3.11\nconda activate kittens\nconda install cuda==12.4.0 -c nvidia\n```\n\n----------------------------------------\n\nTITLE: Installing ThunderKittens Kernels\nDESCRIPTION: This snippet shows how to install pre-existing ThunderKittens kernels using `setup.py`.  It assumes that the environment variables are set correctly (via `env.src`) and that the desired kernels are selected in `configs.py`.  The `python setup.py install` command compiles and installs the kernels, making them available for use with PyTorch bindings.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython setup.py install\n```\n\n----------------------------------------\n\nTITLE: Project Dependencies\nDESCRIPTION: This snippet specifies the Python packages required to run the Thunderkittens project. It includes dependencies for logging, configuration management (omegaconf, hydra), distributed training (accelerate), model fine-tuning (peft), environment variable loading (python-dotenv), data handling (datasets, transformers), neural network building blocks (timm, einops, flash-attn), metrics (torchmetrics), and PyTorch Lightning for training.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/requirements.txt#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nrich\nomegaconf\naccelerate>=0.26.0\npeft\npython-dotenv\nhydra-core==1.3.1 \nhydra-colorlog==1.2.0 \nhydra-optuna-sweeper==1.2.0\npyrootutils \ntransformers \ndatasets==2.8.0 \npytorch-lightning==1.8.6\ntimm==0.6.12 \ntorchmetrics==0.10.3\neinops\nsentencepiece\nflash-attn\n```\n\n----------------------------------------\n\nTITLE: Resolving libc10.so Error\nDESCRIPTION: This snippet addresses a common error related to the `libc10.so` library. First it uses python to find the torch installation path and then it exports the correct path to the `LD_LIBRARY_PATH`.  This will ensure that the system uses the correct `libc10.so` library, resolving potential compatibility issues. Replace `PRINTED_PATH` with the actual output of the python command.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# take the PRINTED_PATH from below\npython -c \"import torch; print(torch.file)\"\n# and run the command below\nexport LD_LIBRARY_PATH=PRINTED_PATH/lib:$LD_LIBRARY_PATH\n```\n\n----------------------------------------\n\nTITLE: Install Flash Linear Attention and Mamba\nDESCRIPTION: Installs Flash Linear Attention CUDA kernels and Mamba kernels for benchmarking baselines. These external kernels provide a comparison point for ThunderKittens' performance. Uses pip to install the mamba ssm package and git to clone the flash-linear-attention repository.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/based_demo/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/sustcsonglin/flash-linear-attention.git\npip install -U git+https://github.com/sustcsonglin/flash-linear-attention\n\npip install mamba_ssm\n```\n\n----------------------------------------\n\nTITLE: Hugging Face Login\nDESCRIPTION: Logs in to the Hugging Face CLI, which is required to download models for the demonstrations. This allows access to models hosted on the Hugging Face Hub.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Run Document QA/IE Demo (Based, Attn, Mamba)\nDESCRIPTION: Runs the document QA and information extraction demo using Based, Attn, and Mamba models.  These models stress test the in-context learning ability of different architectures.  Requires the user to navigate to the `ThunderKittens/demos/based_demo/` directory. Assumes the existence of the `document_ie_based.py` script in the specified directory.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/based_demo/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ThunderKittens/demos/based_demo/\npython document_ie_based.py -- model_name based\npython document_ie_based.py -- model_name attn\npython document_ie_based.py -- model_name mamba\n```\n\n----------------------------------------\n\nTITLE: Run Llama 3 8B Demo with TK GQA Attention\nDESCRIPTION: Navigates to the llama_demo directory and executes the demo_8b.sh script to run the Llama 3 8B model with TK GQA attention. Requires the Llama 3 8B model and TK kernels to be set up.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd llama_demo/\nbash demo_8b.sh\n```\n\n----------------------------------------\n\nTITLE: Run Generation Demo (Based)\nDESCRIPTION: Runs the generation demo using the Based model with user-provided prompts.  This allows users to experiment with the model's generative capabilities. Assumes the existence of the `generate_based.py` script in the current directory.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/based_demo/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython generate_based.py\n```\n\n----------------------------------------\n\nTITLE: Run LoLCATS-Llama 3.1 8B Demo with TK\nDESCRIPTION: Navigates to the lolcats_demo directory and executes the demo_8b.sh script to run the LoLCATS-Llama 3.1 8B model with TK.  Requires the LoLCATS-Llama 3.1 8B model and TK kernels to be set up.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd lolcats_demo/\nbash demo_8b.sh\n```\n\n----------------------------------------\n\nTITLE: Run End-to-End Model Benchmarking\nDESCRIPTION: Runs the end-to-end model benchmarking script to compare different linear attention approaches. This script generates a plot that visualizes the performance comparison. Assumes the existence of the `benchmark/benchmark.py` script. It is important to have the right packages installed, as shown in previous code snippets.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/based_demo/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark/benchmark.py\n```\n\n----------------------------------------\n\nTITLE: Run Standalone Kernel Benchmarking\nDESCRIPTION: Runs the standalone kernel benchmarking script. This generates data that reproduce the performance plot shown in the README, which helps confirm that the base kernels are working correctly. Assumes the existence of the `benchmark_kernel.py` script.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/based_demo/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark_kernel.py\n```\n\n----------------------------------------\n\nTITLE: Install Flash Linear Attention Dependency\nDESCRIPTION: Installs the flash-linear-attention library from GitHub.  This is a dependency required for the Based architecture demo. The -U flag ensures that the package is upgraded if it is already installed.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/demos/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -U git+https://github.com/sustcsonglin/flash-linear-attention\n```\n\n----------------------------------------\n\nTITLE: Install Flash FFT Conv (Long Convolution)\nDESCRIPTION: Installs the Flash FFT Conv library from the specified Git repository. This library is used as a baseline for long convolution kernel benchmarks.  The installation targets the subdirectory `csrc/flashfftconv` within the repository.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/tests/python/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Flash FFT Conv benchmark\npip install git+https://github.com/HazyResearch/flash-fft-conv.git#subdirectory=csrc/flashfftconv\n```\n\n----------------------------------------\n\nTITLE: Compiling Unit Tests with Makefile\nDESCRIPTION: Compiles the unit tests using the provided Makefile. The `-j` flag enables parallel compilation, speeding up the process. Replace `32` with the number of threads available on your machine.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/tests/unit/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake -j32\n```\n\n----------------------------------------\n\nTITLE: Executing Unit Tests\nDESCRIPTION: Executes the compiled unit tests. It creates an `outputs` directory to store test results and runs the `unit_tests` executable with the `printout` argument, which dumps the results of any failed tests to the `outputs/` folder.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/tests/unit/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir outputs\n./unit_tests printout\n```\n\n----------------------------------------\n\nTITLE: Cleaning the Build Directory\nDESCRIPTION: Removes the compiled binary and cleans the build directory using the `make clean` command.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/tests/unit/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake clean\n```\n\n----------------------------------------\n\nTITLE: Nsight Compute Launcher Workaround\nDESCRIPTION: Defines shell functions `ncu2` and `ncu-ui2` as workarounds for a bug in the Nsight Compute launcher script when used within a conda environment. These functions locate the Nsight Compute installation directory and correctly set the `LATEST_NSIGHT_COMPUTE_TOOL_DIR` environment variable before invoking the original `ncu` and `ncu-ui` commands.\nSOURCE: https://github.com/hazyresearch/thunderkittens/blob/main/docs/conda_setup.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nncu2() {\n    local original_ncu_path=$(which ncu)\n    local nsight_compute_base_dir=$(dirname $(dirname $original_ncu_path))/nsight-compute\n    local nsight_compute_version_dir=$(find $nsight_compute_base_dir -mindepth 1 -maxdepth 1 -type d | head -n 1)\n\n    if [ -n \"$nsight_compute_version_dir\" ]; then\n        export LATEST_NSIGHT_COMPUTE_TOOL_DIR=\"$nsight_compute_version_dir\"\n    else\n        echo \"Nsight Compute directory not found.\"\n        return 1\n    fi\n\n    # Invoke the original ncu command with all passed arguments\n    sudo LATEST_NSIGHT_COMPUTE_TOOL_DIR=\"$nsight_compute_version_dir\" $original_ncu_path \"$@\"\n}\n\n\nncu-ui2() {\n    local original_ncu_ui_path=$(which ncu-ui)\n    local nsight_compute_base_dir=$(dirname $(dirname $original_ncu_ui_path))/nsight-compute    \n    local nsight_compute_version_dir=$(find $nsight_compute_base_dir -mindepth 1 -maxdepth 1 -type d | head -n 1)\n\n    if [ -n \"$nsight_compute_version_dir\" ]; then\n        export LATEST_NSIGHT_COMPUTE_TOOL_DIR=\"$nsight_compute_version_dir\"\n    else\n        echo \"Nsight Compute directory not found.\"\n        return 1\n    fi\n\n    # Invoke the original ncu command with all passed arguments\n    sudo LATEST_NSIGHT_COMPUTE_TOOL_DIR=\"$nsight_compute_version_dir\" $original_ncu_ui_path \"$@\"\n}\n```"
  }
]
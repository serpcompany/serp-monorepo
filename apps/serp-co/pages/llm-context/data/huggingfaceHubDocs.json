[
  {
    "owner": "huggingface",
    "repo": "hub-docs",
    "content": "TITLE: Loading and Using PEFT model with Transformers\nDESCRIPTION: This code snippet demonstrates how to load a PEFT model from the Hugging Face Hub and use it for text generation. It requires the `transformers` and `peft` libraries. The code first loads the base model and the adapter model, then uses them for inference. The adapter model contains the fine-tuned weights. Requires CUDA enabled GPU to move the model to GPU.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/peft.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\nbase_model = \"mistralai/Mistral-7B-v0.1\"\nadapter_model = \"dfurman/Mistral-7B-Instruct-v0.2\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model)\nmodel = PeftModel.from_pretrained(model, adapter_model)\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nmodel = model.to(\"cuda\")\nmodel.eval()\n```\n\n----------------------------------------\n\nTITLE: Uploading a PyTorch Model using PyTorchModelHubMixin\nDESCRIPTION: This snippet demonstrates how to upload a custom PyTorch model to the Hugging Face Hub using the `PyTorchModelHubMixin` class from the `huggingface_hub` library. This allows to add `from_pretrained`, `push_to_hub` methods to any `nn.Module` class. Requires the `torch` and `huggingface_hub` libraries.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-uploading.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nfrom huggingface_hub import PyTorchModelHubMixin\n\n\nclass MyModel(\n    nn.Module,\n    PyTorchModelHubMixin, \n    # optionally, you can add metadata which gets pushed to the model card\n    repo_url=\"your-repo-url\",\n    pipeline_tag=\"text-to-image\",\n    license=\"mit\",\n):\n    def __init__(self, num_channels: int, hidden_size: int, num_classes: int):\n        super().__init__()\n        self.param = nn.Parameter(torch.rand(num_channels, hidden_size))\n        self.linear = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        return self.linear(x + self.param)\n\n# create model\nconfig = {\"num_channels\": 3, \"hidden_size\": 32, \"num_classes\": 10}\nmodel = MyModel(**config)\n\n# save locally\nmodel.save_pretrained(\"my-awesome-model\")\n\n# push to the hub\nmodel.push_to_hub(\"your-hf-username/my-awesome-model\")\n\n# reload\nmodel = MyModel.from_pretrained(\"your-hf-username/my-awesome-model\")\n```\n\n----------------------------------------\n\nTITLE: Text Classification with Transformers Pipeline\nDESCRIPTION: This snippet shows how to use a `transformers` pipeline for text classification on a Pandas DataFrame. It initializes a text classification pipeline, then computes labels and scores for each text in the DataFrame, adding these as new columns.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\nfrom tqdm import tqdm\n\npipe = pipeline(\"text-classification\", model=\"clapAI/modernBERT-base-multilingual-sentiment\")\n\n# Compute labels\ndf[\"label\"] = [y[\"label\"] for y in pipe(x for x in tqdm(df[\"text\"]))]\n# Compute labels and scores\ndf[[\"label\", \"score\"]] = [(y[\"label\"], y[\"score\"]) for y in pipe(x for x in tqdm(df[\"text\"]))]\n```\n\n----------------------------------------\n\nTITLE: Saving Sentence Transformers Model to Hub\nDESCRIPTION: This snippet demonstrates how to save a trained Sentence Transformers model to the Hugging Face Hub using the `save_to_hub` method. This creates a repository with a model card, inference widget, and example code snippets. Requires a trained `SentenceTransformer` model and authentication with the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/sentence-transformers.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\n# Load or train a model\nmodel.save_to_hub(\"my_new_model\")\n```\n\n----------------------------------------\n\nTITLE: Model Parallelism Configuration with Hugging Face Estimator in SageMaker (Python)\nDESCRIPTION: This snippet illustrates configuring the Hugging Face Estimator to use SageMaker's model parallelism library. The `distribution` parameter is defined with `smp_options` and `mpi_options` to configure model parallelism settings such as microbatch size, placement strategy, and the number of partitions.  This method is suitable for large models that cannot fit on a single GPU and relies on the Hugging Face Trainer API in the training script.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# configuration for running training on smdistributed model parallel\nmpi_options = {\n    \"enabled\" : True,\n    \"processes_per_host\" : 8\n}\n\nsmp_options = {\n    \"enabled\":True,\n    \"parameters\": {\n        \"microbatches\": 4,\n        \"placement_strategy\": \"spread\",\n        \"pipeline\": \"interleaved\",\n        \"optimize\": \"speed\",\n        \"partitions\": 4,\n        \"ddp\": True,\n    }\n}\n\ndistribution={\n    \"smdistributed\": {\"modelparallel\": smp_options},\n    \"mpi\": mpi_options\n}\n\n # create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3dn.24xlarge',\n        instance_count=2,\n        role=role,\n        transformers_version='4.26.0',\n        pytorch_version='1.13.1',\n        py_version='py39',\n        hyperparameters = hyperparameters,\n        distribution = distribution\n)\n```\n\n----------------------------------------\n\nTITLE: Pushing Models and Tokenizers to Hub with Transformers\nDESCRIPTION: This code snippet demonstrates how to push a trained model and tokenizer to the Hugging Face Hub using the `push_to_hub` method. It allows you to share your models with the community by uploading the model files to a repository under your account.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/transformers.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Pushing model to your own account\nmodel.push_to_hub(\"my-awesome-model\")\n\n# Pushing your tokenizer\ntokenizer.push_to_hub(\"my-awesome-model\")\n\n# Pushing all things after training\ntrainer.push_to_hub()\n```\n\n----------------------------------------\n\nTITLE: Deploy LLM to SageMaker using Hugging Face TGI (Python)\nDESCRIPTION: This code snippet shows how to deploy a Large Language Model (LLM) to SageMaker using the Hugging Face TGI container. It includes importing necessary libraries, retrieving the LLM image URI, configuring the model, deploying the model, and invoking the model for text generation. It also shows how to delete the endpoint and model resources. Requires the `sagemaker` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport sagemaker\nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\nimport time\n\nsagemaker_session = sagemaker.Session()\nregion = sagemaker_session.boto_region_name\nrole = sagemaker.get_execution_role()\n\nimage_uri = get_huggingface_llm_image_uri(\n  backend=\"huggingface\",\n  region=region\n)\n\nmodel_name = \"llama-3-1-8b-instruct\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n\nhub = {\n    'HF_MODEL_ID':'meta-llama/Llama-3.1-8B-Instruct',\n    'SM_NUM_GPUS':'1',\n\t'HUGGING_FACE_HUB_TOKEN': '<REPLACE WITH YOUR TOKEN>',\n}\n\nassert hub['HUGGING_FACE_HUB_TOKEN'] != '<REPLACE WITH YOUR TOKEN>', \"You have to provide a token.\"\n\n\nmodel = HuggingFaceModel(\n    name=model_name,\n    env=hub,\n    role=role,\n    image_uri=image_uri\n)\n\npredictor = model.deploy(\n  initial_instance_count=1,\n  instance_type=\"ml.g5.2xlarge\",\n  endpoint_name=model_name\n)\n\ninput_data = {\n  \"inputs\": \"The diamondback terrapin was the first reptile to\",\n  \"parameters\": {\n    \"do_sample\": True,\n    \"max_new_tokens\": 100,\n    \"temperature\": 0.7,\n    \"watermark\": True\n  }\n}\n\npredictor.predict(input_data)\n\npredictor.delete_model()\npredictor.delete_endpoint()\n```\n\n----------------------------------------\n\nTITLE: Uploading a Dataset to Hugging Face Hub\nDESCRIPTION: This code snippet demonstrates how to upload a dataset to the Hugging Face Hub using the `push_to_hub` method. It requires the `datasets` library to be installed and the user to be logged in. The dataset will be stored in Parquet format.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-usage.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmy_new_dataset.push_to_hub(\"username/my_new_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Client-Side OAuth Login with huggingface.js\nDESCRIPTION: Demonstrates how to implement client-side OAuth login in a Hugging Face Space using the `huggingface.js` library. The code checks if the user is already logged in using `oauthHandleRedirectIfPresent()`. If not, it redirects the user to the Hugging Face OAuth login page using `oauthLoginUrl()`. After successful login, it retrieves the access token and user information.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { oauthLoginUrl, oauthHandleRedirectIfPresent } from \"@huggingface/hub\";\n\nconst oauthResult = await oauthHandleRedirectIfPresent();\n\nif (!oauthResult) {\n  // If the user is not logged in, redirect to the login page\n  window.location.href = await oauthLoginUrl();\n}\n\n// You can use oauthResult.accessToken, oauthResult.userInfo among other things\nconsole.log(oauthResult);\n```\n\n----------------------------------------\n\nTITLE: Generating Text with PEFT Model\nDESCRIPTION: This code snippet generates text using a loaded PEFT model. It uses the tokenizer to prepare the input and the `model.generate()` method to generate the output. The generated output is then decoded and printed. Requires a loaded PEFT model and tokenizer to be initialized.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/peft.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninputs = tokenizer(\"Tell me the recipe for chocolate chip cookie\", return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=10)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\n```\n\n----------------------------------------\n\nTITLE: Loading Models with AutoModel API in Transformers\nDESCRIPTION: This code snippet demonstrates how to load a pre-trained model and its tokenizer using the `AutoTokenizer` and `AutoModelForCausalLM` classes from the `transformers` library. It allows for more control over the model by explicitly defining the tokenizer and model from the specified pre-trained model \"distilbert/distilgpt2\".\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/transformers.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# If you want more control, you will need to define the tokenizer and model.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n```\n\n----------------------------------------\n\nTITLE: Loading Private Model with Access Token in Transformers\nDESCRIPTION: This code snippet demonstrates how to load a private model from the Hugging Face Hub using an access token within the `transformers` library. It shows how to pass the access token to the `from_pretrained` method of an `AutoModel` instance. It requires the `transformers` library to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-tokens.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModel\n\naccess_token = \"hf_...\"\n\nmodel = AutoModel.from_pretrained(\"private/model\", token=access_token)\n```\n\n----------------------------------------\n\nTITLE: Adding, Committing, and Pushing Files to Hugging Face Hub\nDESCRIPTION: These commands add all changes in the current directory to the staging area, commit those changes with a descriptive message, and then push the commit to the remote Hugging Face Hub repository.  This sequence uploads your local changes to the Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-getting-started.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Create any files you like! Then...\ngit add .\ngit commit -m \"First model version\"  # You can choose any descriptive message\ngit push\n```\n\n----------------------------------------\n\nTITLE: Loading a Dataset from Hugging Face Hub\nDESCRIPTION: This code snippet demonstrates how to load a dataset from the Hugging Face Hub using the `load_dataset` function from the `datasets` library. It shows how to load the entire dataset or specific splits (train, validation, test). Requires the `datasets` library to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-usage.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"username/my_dataset\")\n\n# or load the separate splits if the dataset has train/validation/test splits\ntrain_dataset = load_dataset(\"username/my_dataset\", split=\"train\")\nvalid_dataset = load_dataset(\"username/my_dataset\", split=\"validation\")\ntest_dataset  = load_dataset(\"username/my_dataset\", split=\"test\")\n```\n\n----------------------------------------\n\nTITLE: Perform Vector Similarity Search in DuckDB\nDESCRIPTION: This SQL query calculates the cosine similarity between the embedding of each record and the embedding of the 'Linux Terminal' prompt using the `array_cosine_similarity` function. It orders the results by similarity in descending order and limits the output to the top 3 most similar records.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-vector-similarity-search.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nSELECT act,\n       prompt,\n       array_cosine_similarity(embedding::float[384], (SELECT embedding FROM 'hf://datasets/asoria/awesome-chatgpt-prompts-embeddings/data/*.parquet' WHERE  act = 'Linux Terminal')::float[384]) AS similarity \nFROM 'hf://datasets/asoria/awesome-chatgpt-prompts-embeddings/data/*.parquet'\nORDER BY similarity DESC\nLIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Making a Chat Completion Request with cURL\nDESCRIPTION: This cURL command demonstrates how to send a request to the Hugging Face Inference Providers API for chat completion. It includes the necessary headers for authentication and content type, as well as the JSON payload specifying the model, messages, and streaming option. The API key is passed via the Authorization header.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/index.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncurl https://router.huggingface.co/novita/v3/openai/chat/completions \\\n    -H \"Authorization: Bearer $HF_TOKEN\" \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"How many G in huggingface?\"\n            }\n        ],\n        \"model\": \"deepseek/deepseek-v3-0324\",\n        \"stream\": false\n    }'\n```\n\n----------------------------------------\n\nTITLE: Spot Instance Configuration with Hugging Face Estimator in SageMaker (Python)\nDESCRIPTION: This snippet demonstrates how to configure the Hugging Face Estimator to utilize SageMaker's managed spot instances for cost savings. By setting `use_spot_instances=True` and defining `max_wait` and `max_run` times, the estimator will attempt to use spot instances. It's crucial to set `checkpoint_s3_uri` and enable checkpointing to avoid losing progress if the spot instance is interrupted.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# hyperparameters which are passed to the training job\nhyperparameters={'epochs': 1,\n                 'train_batch_size': 32,\n                 'model_name':'distilbert-base-uncased',\n                 'output_dir':'/opt/ml/checkpoints'\n                 }\n\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3.2xlarge',\n        instance_count=1,\n\t    checkpoint_s3_uri=f's3://{sess.default_bucket()}/checkpoints'\n        use_spot_instances=True,\n        # max_wait should be equal to or greater than max_run in seconds\n        max_wait=3600,\n        max_run=1000,\n        role=role,\n        transformers_version='4.26',\n        pytorch_version='1.13',\n        py_version='py39',\n        hyperparameters = hyperparameters\n)\n\n# Training seconds: 874\n# Billable seconds: 262\n# Managed Spot Training savings: 70.0%\n```\n\n----------------------------------------\n\nTITLE: Initialize SageMaker Session and Role\nDESCRIPTION: This snippet initializes a SageMaker session and retrieves the IAM execution role. It also handles the case where the session bucket is not explicitly defined, defaulting to the session's default bucket.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sagemaker\n\nsess = sagemaker.Session()\nsagemaker_session_bucket = None\nif sagemaker_session_bucket is None and sess is not None:\n    sagemaker_session_bucket = sess.default_bucket()\n\nrole = sagemaker.get_execution_role()\nsess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n```\n\n----------------------------------------\n\nTITLE: Saving DataFrame to Parquet on Hugging Face Hub\nDESCRIPTION: This snippet illustrates how to save a Pandas DataFrame to a Parquet file on the Hugging Face Hub using the `hf://` protocol.  The code demonstrates saving a single DataFrame and splitting the data into train/validation/test datasets.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf.to_parquet(\"hf://datasets/username/my_dataset/imdb.parquet\")\n\n# or write in separate files if the dataset has train/validation/test splits\ndf_train.to_parquet(\"hf://datasets/username/my_dataset/train.parquet\")\ndf_valid.to_parquet(\"hf://datasets/username/my_dataset/validation.parquet\")\ndf_test .to_parquet(\"hf://datasets/username/my_dataset/test.parquet\")\n```\n\n----------------------------------------\n\nTITLE: ZeroGPU Example Usage with Diffusers\nDESCRIPTION: This example demonstrates how to utilize the `@spaces.GPU` decorator to enable dynamic GPU allocation for a function using the `diffusers` library.  It shows importing the `spaces` module, defining a GPU-dependent function that uses a pre-trained diffusion pipeline, and launching a Gradio interface. The `pipe` object is moved to the CUDA device for GPU acceleration.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-zerogpu.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport spaces\nfrom diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(...)\npipe.to('cuda')\n\n@spaces.GPU\ndef generate(prompt):\n    return pipe(prompt).images\n\ngr.Interface(\n    fn=generate,\n    inputs=gr.Text(),\n    outputs=gr.Gallery(),\n).launch()\n```\n\n----------------------------------------\n\nTITLE: Loading TF/Flax Models in PyTorch\nDESCRIPTION: This demonstrates how to load model weights from TensorFlow or Flax checkpoints within PyTorch architectures using the `from_tf` or `from_flax` parameters in the `from_pretrained` method. This is a safe alternative to loading potentially malicious pickle files.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"google-bert/bert-base-cased\", from_flax=True)\n```\n\n----------------------------------------\n\nTITLE: Deploy Hugging Face Model from Hub\nDESCRIPTION: Deploys a Hugging Face model directly from the Hugging Face Model Hub to SageMaker.  It defines environment variables to specify the model ID and task, enabling easy deployment of pre-trained models.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n  'HF_TASK':'question-answering'                           # NLP task you want to use for predictions\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   env=hub,                                                # configuration for loading model from Hub\n   role=role,                                              # IAM role with permissions to create an endpoint\n   transformers_version=\"4.26\",                             # Transformers version used\n   pytorch_version=\"1.13\",                                  # PyTorch version used\n   py_version='py39',                                      # Python version used\n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n   initial_instance_count=1,\n   instance_type=\"ml.m5.xlarge\"\n)\n\n# example request: you always need to define \"inputs\"\ndata = {\n\"inputs\": {\n\t\"question\": \"What is used for inference?\",\n\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n\t}\n}\n\n# request\npredictor.predict(data)\n```\n\n----------------------------------------\n\nTITLE: Start Training Job with Hugging Face Estimator\nDESCRIPTION: This snippet starts a SageMaker training job using the configured Hugging Face Estimator. It provides the S3 paths to the training and test datasets as input to the `fit` method.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nhuggingface_estimator.fit({\"train\": training_input_path, \"test\": test_input_path})\n```\n\n----------------------------------------\n\nTITLE: Python Model Verification with Transformers Pipeline\nDESCRIPTION: This Python snippet demonstrates how to verify the functionality of a transformers compatible LLM after release, using the `pipeline` function from the `transformers` library.  It checks if the basic inference works as expected.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-release-checklist.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nfrom transformers import pipeline\n\n# This should work without errors\npipe = pipeline(\"text-generation\", model=\"your-username/your-model\")\nresult = pipe(\"Your test prompt\")\n```\n\n----------------------------------------\n\nTITLE: Create model.tar.gz from Hugging Face Hub Model\nDESCRIPTION: Creates a `model.tar.gz` archive from a Hugging Face Hub model for deployment to SageMaker. The steps involve installing `git lfs`, cloning the repository, creating the archive, and uploading it to S3.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone git@hf.co:{repository}\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd {repository}\ntar zcvf model.tar.gz *\n```\n\nLANGUAGE: bash\nCODE:\n```\naws s3 cp model.tar.gz <s3://{my-s3-path}>\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Transformers Pipeline\nDESCRIPTION: This snippet demonstrates how to use a `transformers` pipeline for text generation on a Pandas DataFrame. It initializes a text generation pipeline, formats a prompt for each text in the DataFrame, and adds the generated text as a new column.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\nfrom tqdm import tqdm\n\npipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-1.5B-Instruct\")\n\n# Generate chat response\nprompt = \"What is the main topic of this sentence ? REPLY IN LESS THAN 3 WORDS. Sentence: '{}'\"\ndf[\"output\"] = [y[\"generated_text\"][1][\"content\"] for y in pipe([{\"role\": \"user\", \"content\": prompt.format(x)}] for x in tqdm(df[\"text\"]))]\n```\n\n----------------------------------------\n\nTITLE: Loading Models with Pipeline API in Transformers\nDESCRIPTION: This code snippet demonstrates how to use the `pipeline` function from the `transformers` library to load a pre-trained model for text generation. It initializes a pipeline with the task \"text-generation\" and specifies the model ID \"distilbert/distilgpt2\" from the Hugging Face Hub. This provides a high-level API for using the model without needing to define the tokenizer and model separately.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/transformers.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# With pipeline, just specify the task and the model id from the Hub.\nfrom transformers import pipeline\npipe = pipeline(\"text-generation\", model=\"distilbert/distilgpt2\")\n```\n\n----------------------------------------\n\nTITLE: Load Keras model from Hugging Face Hub\nDESCRIPTION: Loads a Keras model from the Hugging Face Hub using the keras.saving.load_model function. The model is loaded from a specified Hugging Face path (repo_id prefixed by hf://).\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/keras.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport keras\n\nmodel = keras.saving.load_model(\"hf://Wauplin/mnist_example\")\n```\n\n----------------------------------------\n\nTITLE: Checking for Duplicates in DuckDB\nDESCRIPTION: This SQL query checks for duplicate records within the MMLU dataset. It groups all columns and counts occurrences, filtering to only show groups that appear more than twice, enabling the detection of duplicate entries in the dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-sql.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT   *,\n         COUNT(*) AS counts\nFROM     'hf://datasets/cais/mmlu/all/test-*.parquet'\nGROUP BY ALL\nHAVING   counts > 2; \n```\n\n----------------------------------------\n\nTITLE: Preloading Files from Hugging Face Hub in Spaces YAML\nDESCRIPTION: This YAML snippet configures the preloading of specific files from the Hugging Face Hub during the build time of a Space. It optimizes startup time by ensuring the files are available when the application starts. It shows how to specify entire repositories and specific files, including specific commit SHAs.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-config-reference.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\npreload_from_hub:\n  - warp-ai/wuerstchen-prior text_encoder/model.safetensors,prior/diffusion_pytorch_model.safetensors\n  - coqui/XTTS-v1\n  - openai-community/gpt2 config.json 11c5a3d5811f50298f278a704980280950aedb10\n```\n\n----------------------------------------\n\nTITLE: Querying Hugging Face Dataset (SQL)\nDESCRIPTION: This example demonstrates a simple SQL query to fetch all rows from a Hugging Face dataset using a standard SELECT * FROM statement with the `hf://` URL.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb.md#_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM 'hf://datasets/ibm/duorc/ParaphraseRC/*.parquet' LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Conversational LLM Inference Snippet\nDESCRIPTION: This snippet demonstrates how to use the Inference API for conversational Language Models (LLMs) within the Chat Completion task. It includes configuration for different providers like Cerebras, Cohere, Fireworks AI, HF Inference, Hyperbolic, Nebius, Novita, Sambanova, and Together AI, mapping model IDs to their respective provider model IDs. The pipeline is specified as `text-generation` and the `conversational` flag is set.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/tasks/chat-completion.md#_snippet_0\n\nLANGUAGE: InferenceSnippet\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"cerebras\":{\"modelId\":\"meta-llama/Llama-3.3-70B-Instruct\",\"providerModelId\":\"llama-3.3-70b\"},\"cohere\":{\"modelId\":\"CohereLabs/c4ai-command-a-03-2025\",\"providerModelId\":\"command-a-03-2025\"},\"fireworks-ai\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"accounts/fireworks/models/deepseek-v3-0324\"},\"hf-inference\":{\"modelId\":\"Qwen/QwQ-32B\",\"providerModelId\":\"Qwen/QwQ-32B\"},\"hyperbolic\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"deepseek-ai/DeepSeek-V3-0324\"},\"nebius\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"deepseek-ai/DeepSeek-V3-0324-fast\"},\"novita\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"deepseek/deepseek-v3-0324\"},\"sambanova\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"DeepSeek-V3-0324\"},\"together\":{\"modelId\":\"deepseek-ai/DeepSeek-R1\",\"providerModelId\":\"deepseek-ai/DeepSeek-R1\"}} }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (LLM) Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Chat Completion (LLM) using the `Qwen/QwQ-32B` model on HF Inference. It sets the pipeline to `text-generation` and enables conversational mode.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"Qwen/QwQ-32B\",\"providerModelId\":\"Qwen/QwQ-32B\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Running GGUF Model with Ollama\nDESCRIPTION: This snippet shows the basic command to run a GGUF model from the Hugging Face Hub using Ollama. It requires Ollama to be installed and configured, and the Hugging Face Hub Local Apps setting to be enabled. The command fetches and runs the specified model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/ollama.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/{username}/{repository}\n```\n\n----------------------------------------\n\nTITLE: Starting Giskard ML Worker with API Key and Space Token (Bash)\nDESCRIPTION: This command starts the Giskard ML worker, which executes models in your Python environment. It requires the Giskard API key, the URL of your Hugging Face Space, and the Giskard Space token. The `-d` flag runs the worker in detached mode. Ensure the Python environment contains all dependencies required by the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-giskard.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngiskard worker start -d -k GISKARD-API-KEY -u https://XXX.hf.space --hf-token GISKARD-SPACE-TOKEN\n```\n\n----------------------------------------\n\nTITLE: Load UNet Component from Hub in Diffusers (Python)\nDESCRIPTION: This code snippet shows how to load a specific component (UNet) of a diffusion pipeline from the Hugging Face Hub using the `UNet2DConditionModel.from_pretrained()` method. The `subfolder` parameter specifies the subdirectory containing the UNet model. It allows for loading individual components for customization.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/diffusers.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom diffusers import UNet2DConditionModel\n\nunet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"unet\")\n```\n\n----------------------------------------\n\nTITLE: Cloning a Hugging Face Model Repository\nDESCRIPTION: This command clones a Hugging Face model repository to your local machine using the git CLI. It downloads all files and the complete history of the repository. The `cd` command navigates into the newly cloned directory.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-getting-started.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/<your-username>/<your-model-name>\ncd <your-model-name>\n```\n\n----------------------------------------\n\nTITLE: Preview Dataset Records in DuckDB\nDESCRIPTION: This SQL query selects and displays the 'act', 'prompt', and the length of the 'embedding' columns from the 'asoria/awesome-chatgpt-prompts-embeddings' dataset, limiting the results to the first 3 records. It provides an initial overview of the dataset's structure and content.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-vector-similarity-search.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFROM 'hf://datasets/asoria/awesome-chatgpt-prompts-embeddings/data/*.parquet' SELECT act, prompt, len(embedding) as embed_len LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Argument Parsing for Training Script\nDESCRIPTION: This Python code snippet uses the `argparse` module to define command-line arguments for the training script. It defines hyperparameters like epochs, batch size, and model name, as well as directories for data, model, and output.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\nimport datasets\nimport argparse\nimport os\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script\n    parser.add_argument(\"--epochs\", type=int, default=3)\n    parser.add_argument(\"--per_device_train_batch_size\", type=int, default=32)\n    parser.add_argument(\"--model_name_or_path\", type=str)\n\n    # data, model, and output directories\n    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n    parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n```\n\n----------------------------------------\n\nTITLE: Querying Data with Spark SQL\nDESCRIPTION: This snippet demonstrates how to query a PySpark Dataframe using Spark SQL. It reads a Parquet file from a Hugging Face dataset, groups the data by the 'source' column, counts the occurrences of each source, orders the results in descending order, and displays the top sources. Requires a SparkSession to be initialized and the `read_parquet` function to read data into a Dataframe. The `df` variable is passed as a parameter to the `spark.sql` function, enabling SQL to reference the Dataframe.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyspark.sql import SparkSession\n>>> spark = SparkSession.builder.appName(\"demo\").getOrCreate()\n>>> df = read_parquet(\"hf://datasets/BAAI/Infinity-Instruct/7M/*.parquet\", columns=[\"source\"])\n>>> spark.sql(\"SELECT source, count(*) AS total FROM {df} GROUP BY source ORDER BY total DESC\", df=df).show()\n+--------------------+-------+\n|              source|  total|\n+--------------------+-------+\n|                flan|2435840|\n|          Subjective|1342427|\n|      OpenHermes-2.5| 855478|\n|            MetaMath| 690138|\n|      code_exercises| 590958|\n|Orca-math-word-pr...| 398168|\n|          code_bagel| 386649|\n|        MathInstruct| 329254|\n|python-code-datas...|  88632|\n|instructional_cod...|  82920|\n|        CodeFeedback|  79513|\n|self-oss-instruct...|  50467|\n|Evol-Instruct-Cod...|  43354|\n|CodeExercise-Pyth...|  27159|\n|code_instructions...|  23130|\n|  Code-Instruct-700k|  10860|\n|Glaive-code-assis...|   9281|\n|python_code_instr...|   2581|\n|Python-Code-23k-S...|   2297|\n+--------------------+-------+\n```\n\n----------------------------------------\n\nTITLE: Defining HF_MODEL_REVISION for Version Control (Bash)\nDESCRIPTION: This snippet defines the `HF_MODEL_REVISION` environment variable, allowing users to pin a specific revision of the model identified by `HF_MODEL_ID`. This ensures consistency in model loading by always loading the same model revision on a SageMaker endpoint.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nHF_MODEL_REVISION=\"03b4d196c19d0a73c7e0322684e97db1ec397613\"\n```\n\n----------------------------------------\n\nTITLE: Get Model Providers (Python)\nDESCRIPTION: This snippet shows how to retrieve the inference providers for a model using the `huggingface_hub` library in Python. The `model_info` function is used with `expand=\"inferenceProviderMapping\"` to fetch the provider mapping. The response is a dictionary containing the providers and their associated information such as status, provider ID, and task.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import model_info\n\n>>> info = model_info(\"google/gemma-3-27b-it\", expand=\"inferenceProviderMapping\")\n>>> info.inference_provider_mapping\n{\n    'hf-inference': InferenceProviderMapping(status='live', provider_id='google/gemma-3-27b-it', task='conversational'),\n    'nebius': InferenceProviderMapping(status='live', provider_id='google/gemma-3-27b-it-fast', task='conversational'),\n}\n```\n\n----------------------------------------\n\nTITLE: Converting and Uploading LLMs to Hugging Face Hub\nDESCRIPTION: This command converts an LLM from the Hugging Face Hub to the MLX format, quantizes it, and then uploads it to a specified repository on the Hub. The `-q` flag enables quantization and `--upload-repo` specifies the repository. Replace `mistralai/Mistral-7B-v0.1` with the model to convert and `<USER_ID>/<MODEL_NAME>` with the destination repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m mlx_lm.convert \\\n    --hf-path mistralai/Mistral-7B-v0.1 \\\n    -q \\\n    --upload-repo <USER_ID>/<MODEL_NAME>\n```\n\n----------------------------------------\n\nTITLE: Initializing Giskard Client in Python\nDESCRIPTION: This Python snippet initializes a Giskard client, which is required to establish a connection between your Python environment and the Giskard app on Hugging Face Spaces. It uses the GiskardClient class from the giskard library and requires the Space URL, API key, and Hugging Face token.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-giskard.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom giskard import GiskardClient\n\nurl = \"https://user_name-space_name.hf.space\"\napi_key = \"gsk-xxx\"\nhf_token = \"xxx\"\n\n# Create a giskard client to communicate with Giskard\nclient = GiskardClient(url, api_key, hf_token)\n```\n\n----------------------------------------\n\nTITLE: Uploading a Model with Transformers\nDESCRIPTION: This snippet demonstrates how to upload a model to the Hugging Face Hub using the Transformers library's `push_to_hub` method. It creates a simple Bert model, pushes it to the Hub, and then reloads it from the Hub. Requires the `transformers` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-uploading.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import BertConfig, BertModel\n\nconfig = BertConfig()\nmodel = BertModel(config)\n\nmodel.push_to_hub(\"nielsr/my-awesome-bert-model\")\n\n# reload\nmodel = BertModel.from_pretrained(\"nielsr/my-awesome-bert-model\")\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (LLM) Inference Snippet\nDESCRIPTION: This snippet configures the Inference API to use SambaNova for Chat Completion (LLM). It specifies the 'deepseek-ai/DeepSeek-V3-0324' model on the Hugging Face Hub and maps it to 'DeepSeek-V3-0324' on the SambaNova provider. The pipeline is set to 'text-generation' and marked as conversational.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/sambanova.md#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"sambanova\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"DeepSeek-V3-0324\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Git Repository Configuration with Hugging Face Estimator in SageMaker (Python)\nDESCRIPTION: This code snippet demonstrates how to configure the Hugging Face Estimator to load a training script from a Git repository.  The `git_config` parameter specifies the repository URL and branch. The `entry_point` and `source_dir` parameters define the relative paths to the training script and its directory within the repository. Ensure the `transformers_version` matches the branch used in `git_config`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# configure git settings\ngit_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.4.2'} # v4.4.2 refers to the transformers_version you use in the estimator\n\n # create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='run_glue.py',\n        source_dir='./examples/pytorch/text-classification',\n        git_config=git_config,\n        instance_type='ml.p3.2xlarge',\n        instance_count=1,\n        role=role,\n        transformers_version='4.26',\n        pytorch_version='1.13',\n        py_version='py39',\n        hyperparameters=hyperparameters\n)\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Image Classification with OpenCLIP\nDESCRIPTION: This code snippet performs zero-shot image classification using a loaded OpenCLIP model. It loads an image from a URL, preprocesses it, tokenizes a list of text labels, encodes both image and text, normalizes the feature vectors, and computes the probabilities for each label. Requires `torch`, `PIL`, and `requests`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/open_clip.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nimage = preprocess(image).unsqueeze(0)\ntext = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)\n```\n\n----------------------------------------\n\nTITLE: List Models by Multiple Providers (cURL)\nDESCRIPTION: This snippet demonstrates how to list models served by multiple inference providers using a comma-separated list for the `inference_provider` parameter. cURL is used to query the API endpoint, and `jq` is used to extract the model IDs from the JSON response.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# List image-text-to-text models served by Novita or Sambanova\n~ curl -s https://huggingface.co/api/models?inference_provider=sambanova,novita&pipeline_tag=image-text-to-text | jq \".[].id\"\n\"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n\"meta-llama/Llama-3.2-90B-Vision-Instruct\"\n\"Qwen/Qwen2-VL-72B-Instruct\"\n```\n\n----------------------------------------\n\nTITLE: Uploading RL-Baselines3-Zoo model using push_to_hub\nDESCRIPTION: This snippet demonstrates how to upload a trained RL-Baselines3-Zoo model to the Hugging Face Hub using the `rl_zoo3.push_to_hub` command. It saves the model, evaluates it, generates a model card, records a replay video, and pushes the repository to the hub under the specified organization or username.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/rl-baselines3-zoo.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/\n```\n\n----------------------------------------\n\nTITLE: Making a Chat Completion Request with Python (requests)\nDESCRIPTION: This Python snippet uses the `requests` library to make a POST request to the Inference Providers API for chat completion. It sets the Authorization header with the API key and sends a JSON payload with the message content and model information. The response is then parsed to extract the generated message.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/index.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nAPI_URL = \"https://router.huggingface.co/novita/v3/openai/chat/completions\"\nheaders = {\"Authorization\": \"Bearer hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"}\npayload = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"How many 'G's in 'huggingface'?\"\n        }\n    ],\n    \"model\": \"deepseek/deepseek-v3-0324\",\n}\n\nresponse = requests.post(API_URL, headers=headers, json=payload)\nprint(response.json()[\"choices\"][0][\"message\"])\n```\n\n----------------------------------------\n\nTITLE: Install Hugging Face, SageMaker, and Datasets\nDESCRIPTION: This snippet installs the necessary Hugging Face libraries (transformers, datasets), and SageMaker using pip. It also upgrades these packages to ensure compatibility.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" --upgrade\n```\n\n----------------------------------------\n\nTITLE: Creating Metadata Key Dictionary\nDESCRIPTION: This Python function creates a dictionary containing metadata fields that are considered desired or required for a model or dataset. It takes the card data and repository type as input. The function returns a dictionary where the keys are the desired metadata fields, and the values are either the corresponding metadata values or `None` if the field is missing.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef create_metadata_key_dict(card_data, repo_type: str):\n    shared_keys = [\"tags\", \"license\"]\n    if repo_type == \"model\":\n        model_keys = [\"library_name\", \"datasets\", \"metrics\", \"co2\", \"pipeline_tag\"]\n        shared_keys.extend(model_keys)\n        keys = shared_keys\n        return {key: card_data.get(key) for key in keys}\n    if repo_type == \"dataset\":\n        # [...]\n```\n\n----------------------------------------\n\nTITLE: Dataset Card Metadata Example YAML\nDESCRIPTION: This YAML snippet shows the basic structure for adding metadata to a dataset card. It includes fields for language, pretty name, tags, license, and task categories.  The language field expects ISO 639-1 codes.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-cards.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nlanguage:\n- \"List of ISO 639-1 code for your language\"\n- lang1\n- lang2\npretty_name: \"Pretty Name of the Dataset\"\ntags:\n- tag1\n- tag2\nlicense: \"any valid license identifier\"\ntask_categories:\n- task1\n- task2\n```\n\n----------------------------------------\n\nTITLE: Compute Mean Token Count with Projection Pushdown\nDESCRIPTION: This Python snippet demonstrates computing the mean of a specific column (`token_count`) in a Dask DataFrame. Dask uses projection pushdown to only load the required columns and skips the rest, optimizing performance. Requires a dask dataframe loaded from a parquet file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Dask will download the 'dump' and 'token_count' needed\n# for the filtering and computation and skip the other columns.\ndf.token_count.mean().compute()\n```\n\n----------------------------------------\n\nTITLE: Accessing Environment Variables in Python\nDESCRIPTION: This code snippet demonstrates how to access environment variables, including secrets and public variables, within a Python application running in a Hugging Face Space. It uses the `os.getenv()` function to retrieve the value of the `MODEL_REPO_ID` environment variable.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-overview.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nprint(os.getenv('MODEL_REPO_ID'))\n```\n\n----------------------------------------\n\nTITLE: Describing Dataset Schema using DuckDB\nDESCRIPTION: This command describes the schema of the MMLU dataset. It leverages DuckDB to display column names, data types, nullability, keys, default values, and extra information about the dataset structure, facilitating understanding of the data's organization.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-sql.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nDESCRIBE FROM 'hf://datasets/cais/mmlu/all/test-*.parquet' USING SAMPLE 3;\n```\n\n----------------------------------------\n\nTITLE: Logging in via CLI - Bash\nDESCRIPTION: This bash command demonstrates how to log in to the Hugging Face Hub using the `huggingface-cli` tool. This is a prerequisite for downloading files from gated models programmatically when not using a browser.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-gated.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Sharing a fastai model to the Hub\nDESCRIPTION: This snippet demonstrates how to share a trained fastai model to the Hugging Face Hub using the `push_to_hub_fastai` function. The `learner` object represents the trained fastai model, and `repo_id` specifies the repository name on the Hub where the model will be uploaded.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/fastai.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import push_to_hub_fastai\n\npush_to_hub_fastai(learner=learn, repo_id=\"espejelomar/identify-my-cat\")\n```\n\n----------------------------------------\n\nTITLE: Save Keras model to Hugging Face Hub\nDESCRIPTION: Saves a Keras model to the Hugging Face Hub using the model.save() function.  The model is saved to a specified Hugging Face path (repo_id prefixed by hf://) under the user's namespace. If the repository doesn't exist, it will be created.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/keras.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmodel = ...\nmodel.save(\"hf://your-username/your-model-name\")\n```\n\n----------------------------------------\n\nTITLE: Running GGUF Model with Specific Quantization Examples\nDESCRIPTION: These snippets demonstrate running GGUF models with specific quantization schemes, including examples with different quantization tags and filename usage. Ollama must be installed and configured.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/ollama.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:IQ3_M\n```\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q8_0\n```\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:iq3_m\n```\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-IQ3_M.gguf\n```\n\n----------------------------------------\n\nTITLE: Uploading a Stable-Baselines3 model to Hugging Face Hub using package_to_hub\nDESCRIPTION: This snippet showcases how to upload a trained Stable-Baselines3 model to the Hugging Face Hub using the `package_to_hub` function.  This function saves the model, evaluates it, generates a model card, records a replay video, and pushes the repo to the Hub. It requires a trained model and the `huggingface-sb3` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/stable-baselines3.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npackage_to_hub(model=model, \n               model_name=\"ppo-LunarLander-v2\",\n               model_architecture=\"PPO\",\n               env_id=env_id,\n               eval_env=eval_env,\n               repo_id=\"ThomasSimonini/ppo-LunarLander-v2\",\n               commit_message=\"Test commit\")\n```\n\n----------------------------------------\n\nTITLE: Uploading a Stable-Baselines3 model to Hugging Face Hub using push_to_hub\nDESCRIPTION: This snippet demonstrates how to directly push a Stable-Baselines3 model file to the Hugging Face Hub using the `push_to_hub` function.  It requires the `huggingface-sb3` library and an existing model file. The `repo_id`, `filename`, and `commit_message` parameters must be specified.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/stable-baselines3.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npush_to_hub(\n    repo_id=\"ThomasSimonini/ppo-LunarLander-v2\",\n    filename=\"ppo-LunarLander-v2.zip\",\n    commit_message=\"Added LunarLander-v2 model trained with PPO\",\n)\n```\n\n----------------------------------------\n\nTITLE: Transforming Data with Polars\nDESCRIPTION: This snippet transforms a Polars DataFrame by adding a new column 'pages_per_domain'. It calculates the value of the new column by dividing 'pages' by 'domains' and uses the `with_columns` method along with `alias` to create the new column.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-operations.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_columns(\n    (pl.col(\"pages\") / pl.col(\"domains\")).alias(\"pages_per_domain\")\n)\ndf.sample(3)\n```\n\n----------------------------------------\n\nTITLE: Querying Hugging Face Dataset (Simple)\nDESCRIPTION: This example demonstrates a simple SQL query to fetch the first 3 rows from a Hugging Face dataset using the FROM clause and the `hf://` URL.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb.md#_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nFROM 'hf://datasets/ibm/duorc/ParaphraseRC/*.parquet' LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (VLM) with Fireworks AI\nDESCRIPTION: This snippet configures an InferenceSnippet component for Chat Completion (VLM) using Fireworks AI. It specifies the `image-text-to-text` pipeline and maps the model `meta-llama/Llama-4-Scout-17B-16E-Instruct` to the provider model ID `accounts/fireworks/models/llama4-scout-instruct-basic` on Fireworks AI. The `conversational` prop enables a conversational interface.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/fireworks-ai.md#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"fireworks-ai\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"accounts/fireworks/models/llama4-scout-instruct-basic\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Moving Repository Payload in JavaScript\nDESCRIPTION: Defines the payload structure required to move/rename a repository using the Hugging Face Hub API. The payload includes the source and destination repository names, and the repository type. This payload corresponds to parameters used in `huggingface_hub.move_repo()`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_7\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"fromRepo\" : \"namespace/repo_name\",\n    \"toRepo\" : \"namespace2/repo_name2\",\n    \"type\": \"model\",\n}\n```\n\n----------------------------------------\n\nTITLE: Loading a Stable-Baselines3 model from Hugging Face Hub\nDESCRIPTION: This snippet demonstrates how to download a pre-trained Stable-Baselines3 model from the Hugging Face Hub using the `load_from_hub` function.  It requires the `huggingface-sb3` library to be installed. The parameters `repo_id` and `filename` must be specified to locate the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/stable-baselines3.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncheckpoint = load_from_hub(\n    repo_id=\"sb3/demo-hf-CartPole-v1\",\n    filename=\"ppo-CartPole-v1.zip\",\n)\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Classification Request Example (JSON)\nDESCRIPTION: This JSON snippet demonstrates a request to the zero-shot-classification task of the Inference Toolkit API. The input includes a text and candidate labels.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\",\n  \"parameters\": {\n    \"candidate_labels\": [\"refund\", \"legal\", \"faq\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting LLMs from Hugging Face Hub to MLX format\nDESCRIPTION: This command uses the `mlx_lm.convert` module to convert a Large Language Model (LLM) from the Hugging Face Hub to the MLX format. The `-q` flag enables quantization during the conversion process. Replace `mistralai/Mistral-7B-v0.1` with the desired model name.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q\n```\n\n----------------------------------------\n\nTITLE: Cloning a Fork Repository\nDESCRIPTION: Clones a specified fork repository from the Hugging Face Hub to the local machine.  Requires Git to be installed. The target repository is specified using the git URL format.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-next-steps.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:me/myfork\n```\n\n----------------------------------------\n\nTITLE: Upload Dataset to S3 Bucket\nDESCRIPTION: This snippet uploads the preprocessed training and test datasets to an S3 bucket using the Hugging Face Datasets S3 filesystem integration. It uses the SageMaker session to determine the default bucket and constructs the S3 paths for the datasets.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# save train_dataset to s3\ntraining_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\ntrain_dataset.save_to_disk(training_input_path)\n\n# save test_dataset to s3\ntest_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\ntest_dataset.save_to_disk(test_input_path)\n```\n\n----------------------------------------\n\nTITLE: Create Hugging Face Estimator\nDESCRIPTION: This Python code creates a Hugging Face Estimator to run Transformers training scripts on SageMaker. It specifies the entry point, instance type, and hyperparameters for the training job.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker.huggingface import HuggingFace\n\n\n# hyperparameters which are passed to the training job\nhyperparameters={'epochs': 1,\n                 'per_device_train_batch_size': 32,\n                 'model_name_or_path': 'distilbert-base-uncased'\n                 }\n\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3.2xlarge',\n        instance_count=1,\n        role=role,\n        transformers_version='4.26',\n        pytorch_version='1.13',\n        py_version='py39',\n        hyperparameters = hyperparameters\n)\n```\n\n----------------------------------------\n\nTITLE: Push Dataset to Hub using datasets library\nDESCRIPTION: This code snippet demonstrates how to push a dataset to the Hugging Face Hub using the `datasets` library. It creates a dataset from a list of dictionaries and then uploads it to the specified repository. It requires the `datasets` library to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-libraries.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import Dataset\n\ndata = [{\"prompt\": \"Write a cake recipe\", \"response\": \"Measure 1 cup ...\"}]\nds = Dataset.from_list(data)\nds.push_to_hub(\"USERNAME_OR_ORG/repo_ID\")\n```\n\n----------------------------------------\n\nTITLE: Provide Hugging Face token in Python\nDESCRIPTION: This snippet shows how to provide your Hugging Face token using the `storage_options` parameter. This allows you to authenticate with the Hugging Face Hub in your Python code.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nstorage_options = {\"token\": \"hf_xxx\"}\n```\n\n----------------------------------------\n\nTITLE: Logging in to Hugging Face CLI\nDESCRIPTION: This command authenticates the user with the Hugging Face Hub using the command line interface. It requires the `huggingface-cli` to be installed and configured.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-usage.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Log Hugging Face Dataset Records to Argilla\nDESCRIPTION: This code demonstrates how to load records from a Hugging Face dataset using the `load_dataset` method and log them into an existing Argilla dataset using the `rg.Dataset.log` method. This allows you to reuse existing Hub datasets within Argilla after configuring the Argilla dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-argilla.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nhf_dataset = load_dataset(\"<repo_id>\")\ndataset.log(hf_dataset)\n```\n\n----------------------------------------\n\nTITLE: Webhook Secret Validation (Python)\nDESCRIPTION: This Python code validates the `X-Webhook-Secret` header against a configured `WEBHOOK_SECRET` environment variable. It raises HTTPExceptions if the header is missing or the secret doesn't match. This ensures that only authorized requests are processed. It uses FastAPI's `Header` and `HTTPException` components, and relies on the `os` module to retrieve the secret.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-auto-retrain.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# [...]\n\nWEBHOOK_SECRET = os.getenv(\"WEBHOOK_SECRET\")\n\n# [...]\n\n@app.post(\"/webhook\")\nasync def post_webhook(\n\t# [...]\n\tx_webhook_secret:  Optional[str] = Header(default=None),\n\t# ^ checks for the X-Webhook-Secret HTTP header\n):\n\tif x_webhook_secret is None:\n\t\traise HTTPException(401)\n\tif x_webhook_secret != WEBHOOK_SECRET:\n\t\traise HTTPException(403)\n\t# [...]\n```\n\n----------------------------------------\n\nTITLE: Cloning and Running Evidence app from CLI\nDESCRIPTION: This snippet clones an Evidence app from Hugging Face Spaces, installs dependencies, and runs the development server using npm. It assumes that npm is installed and configured.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-evidence.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/spaces/your-username/your-space-name \ncd your-space-name\nnpm install\nnpm run sources\nnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Upload a folder to the Hub\nDESCRIPTION: Uploads an entire folder to a Hugging Face Hub repository using the `upload_folder` function. This is useful for uploading serialized models and their associated files. Requires the `huggingface_hub` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import upload_folder\n>>> upload_folder(\n...     folder_path=\"/home/lysandre/dummy-test\",\n...     repo_id=\"lysandre/test-model\",\n... )\n```\n\n----------------------------------------\n\nTITLE: Installing Stable-Baselines3 and huggingface-sb3\nDESCRIPTION: This snippet installs the Stable-Baselines3 library and the huggingface-sb3 package, which provides additional functionality for loading and uploading Stable-Baselines3 models from the Hugging Face Hub. It requires pip to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/stable-baselines3.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install stable-baselines3\npip install huggingface-sb3\n```\n\n----------------------------------------\n\nTITLE: Loading a Sample Factory model from Hub\nDESCRIPTION: This command downloads a model from the Hugging Face Hub to a local directory using the `load_from_hub` script provided by Sample Factory. It requires the repo ID and an optional destination directory. The repo ID should be in the format `<username>/<repo_name>`\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/sample-factory.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npython -m sample_factory.huggingface.load_from_hub -r <HuggingFace_repo_id> -d <train_dir_path>\n```\n\n----------------------------------------\n\nTITLE: Audio Files at Root Directory\nDESCRIPTION: This code snippet demonstrates storing audio files directly in the root directory of the dataset repository. This setup is suitable for datasets with a single audio column.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n 1.wav\n 2.wav\n 3.wav\n 4.wav\n```\n\n----------------------------------------\n\nTITLE: Building and Running Docker Image Locally\nDESCRIPTION: These commands build and run the Docker image locally for testing and debugging. The `docker build` command creates an image named `fastapi` from the Dockerfile, and the `docker run` command runs the image, mapping port 7860 on the host to port 7860 in the container.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t fastapi .\ndocker run  -it -p 7860:7860 fastapi\n```\n\n----------------------------------------\n\nTITLE: Loading a SpanMarker model\nDESCRIPTION: This snippet demonstrates how to load a pre-trained SpanMarker model from the Hugging Face Hub using the `SpanMarkerModel.from_pretrained` method. The model name is passed as a string to the function.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/span_marker.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom span_marker import SpanMarkerModel\n\nmodel = SpanMarkerModel.from_pretrained(\"tomaarsen/span-marker-bert-base-fewnerd-fine-super\")\n```\n\n----------------------------------------\n\nTITLE: Model Artifact Structure\nDESCRIPTION: This shows the directory structure for archiving model artifacts, including the `code/inference.py` file for custom inference logic and `requirements.txt` for dependencies.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Deploy Hugging Face Model to SageMaker\nDESCRIPTION: Deploys a Hugging Face model to SageMaker using the `HuggingFaceModel` class. This snippet showcases the basic deployment process using the Inference Toolkit.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker.huggingface import HuggingFaceModel\n\n# create Hugging Face Model Class and deploy it as SageMaker endpoint\nhuggingface_model = HuggingFaceModel(...).deploy()\n```\n\n----------------------------------------\n\nTITLE: Eager Data Processing with Polars in Python\nDESCRIPTION: This code snippet demonstrates eager data processing using the Polars library in Python. It reads a CSV file from a remote location, applies a series of transformations (selection, filtering, column creation, grouping, aggregation, and sorting), and then selects the top 10 rows. The transformations are executed immediately as they are defined.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-optimizations.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport polars as pl\nimport datetime\n\ndf = pl.read_csv(\"hf://datasets/commoncrawl/statistics/tlds.csv\", try_parse_dates=True)\n\ndf = df.select(\"suffix\", \"crawl\", \"date\", \"tld\", \"pages\", \"domains\")\ndf = df.filter(\n    (pl.col(\"date\") >= datetime.date(2020, 1, 1)) |\n    pl.col(\"crawl\").str.contains(\"CC\")\n)\ndf = df.with_columns(\n    (pl.col(\"pages\") / pl.col(\"domains\")).alias(\"pages_per_domain\")\n)\ndf = df.group_by(\"tld\", \"date\").agg(\n    pl.col(\"pages\").sum(),\n    pl.col(\"domains\").sum(),\n)\ndf = df.group_by(\"tld\").agg(\n    pl.col(\"date\").unique().count().alias(\"number_of_scrapes\"),\n    pl.col(\"domains\").mean().alias(\"avg_number_of_domains\"),\n    pl.col(\"pages\").sort_by(\"date\").pct_change().mean().alias(\"avg_page_growth_rate\"),\n).sort(\"avg_number_of_domains\", descending=True).head(10)\n```\n\n----------------------------------------\n\nTITLE: Create Gradio Interface in app.py\nDESCRIPTION: This Python script creates a Gradio interface for classifying images as 'hot dog' or 'not hot dog' using a Transformers pipeline. It imports Gradio and Transformers, defines a prediction function, creates the Gradio Interface, and launches the app.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-gradio.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport gradio as gr\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"image-classification\", model=\"julien-c/hotdog-not-hotdog\")\n\ndef predict(input_img):\n    predictions = pipeline(input_img)\n    return input_img, {p[\"label\"]: p[\"score\"] for p in predictions}\n\ngradio_app = gr.Interface(\n    predict,\n    inputs=gr.Image(label=\"Select hot dog candidate\", sources=['upload', 'webcam'], type=\"pil\"),\n    outputs=[gr.Image(label=\"Processed Image\"), gr.Label(label=\"Result\", num_top_classes=2)],\n    title=\"Hot Dog? Or Not?\",\n)\n\nif __name__ == \"__main__\":\n    gradio_app.launch()\n```\n\n----------------------------------------\n\nTITLE: Querying Pokemon Wiki Captions Dataset\nDESCRIPTION: This snippet demonstrates how to query the `wanghaofan/pokemon-wiki-captions` dataset in Parquet format using DuckDB. It selects the first 3 rows and displays the image, name_en, name_zh, text_en, and text_zh columns.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-combine-and-export.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nFROM 'hf://datasets/wanghaofan/pokemon-wiki-captions/data/*.parquet' LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Loading TF-Keras Model from Hugging Face Hub in Python\nDESCRIPTION: This code snippet demonstrates how to load a TF-Keras model from the Hugging Face Hub using the `from_pretrained_keras` function from the `huggingface_hub` library. It requires `tf-keras` or `keras<3.x` to be installed. The code then performs a prediction using the loaded model and prints the result.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/tf-keras.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import from_pretrained_keras\n\nmodel = from_pretrained_keras(\"keras-io/mobile-vit-xxs\")\nprediction = model.predict(image)\nprediction = tf.squeeze(tf.round(prediction))\nprint(f'The image is a {classes[(np.argmax(prediction))]}!')\n\n# The image is a sunflower!\n```\n\n----------------------------------------\n\nTITLE: Using pandas-audio-methods with SoundFile\nDESCRIPTION: This snippet demonstrates how to integrate `pandas-audio-methods` with `soundfile` to enable audio processing directly on a Pandas DataFrame column. It registers a series accessor and opens audio files into the DataFrame. The `folder_path` and `df[\"file_name\"]` are used to construct the full file path to open the audio.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pandas_image_methods import SFMethods\n\npd.api.extensions.register_series_accessor(\"sf\")(SFMethods)\n\ndf[\"audio\"] = (folder_path + df[\"file_name\"]).sf.open()\ndf.to_parquet(\"data.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic FastAPI App\nDESCRIPTION: This Python code creates a basic FastAPI application with a single endpoint at the root path (`/`). It defines a `read_root` function that returns a JSON response with a \"Hello\": \"World!\" message. This is used to verify the initial setup of the FastAPI application.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World!\"}\n```\n\n----------------------------------------\n\nTITLE: Retrieve Pending Access Requests via API\nDESCRIPTION: This API endpoint retrieves a list of users with pending access requests for a gated dataset. It requires a token with write access to the repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md#_snippet_0\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /api/datasets/{repo_id}/user-access-request/pending\nHeaders: {\"authorization\": \"Bearer $token\"}\n```\n\n----------------------------------------\n\nTITLE: Loading PaddleNLP models using Taskflow\nDESCRIPTION: This snippet demonstrates loading a pre-trained PaddleNLP model from the Hugging Face Hub using the Taskflow API. Taskflow provides a high-level interface for performing NLP tasks. It initializes a 'fill-mask' task using the 'PaddlePaddle/ernie-1.0-base-zh' model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/paddlenlp.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Taskflow provides a simple end-to-end capability and a more optimized experience for inference\nfrom paddlenlp.transformers import Taskflow\ntaskflow = Taskflow(\"fill-mask\", task_path=\"PaddlePaddle/ernie-1.0-base-zh\", from_hf_hub=True)\n```\n\n----------------------------------------\n\nTITLE: Compromised app.py Example (Python)\nDESCRIPTION: This code snippet demonstrates a common security vulnerability: hard-coding an API key directly into the code. The `api_key` variable stores the secret key, which can be easily exposed. The `call_inference` function uses this API key, highlighting the potential risk of unauthorized access. This example shows how TruffleHog detects such issues to prevent secret leaks.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-secrets.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport scipy as sp\n\napi_key = \"sw-xyz1234567891213\"\n\ndef call_inference(prompt: str) -> str:\n    result = call_api(prompt, api_key)\n    return result\n```\n\n----------------------------------------\n\nTITLE: Download Trained Model from S3\nDESCRIPTION: This Python code snippet downloads the trained model from S3 to a local path using the `S3Downloader` class from the sagemaker library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker.s3 import S3Downloader\n\nS3Downloader.download(\n    s3_uri=huggingface_estimator.model_data, # S3 URI where the trained model is located\n    local_path='.',                          # local path where *.targ.gz is saved\n    sagemaker_session=sess                   # SageMaker session used for training the model\n)\n```\n\n----------------------------------------\n\nTITLE: Create a repository on the Hugging Face Hub\nDESCRIPTION: Creates a new repository on the Hugging Face Hub using the `create_repo` function.  Requires the `huggingface_hub` library and a valid Hugging Face account.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import create_repo\n>>> create_repo(repo_id=\"test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\n----------------------------------------\n\nTITLE: Loading Sentence Transformers Model\nDESCRIPTION: This snippet demonstrates how to load a pre-trained Sentence Transformers model from the Hugging Face Hub using the `SentenceTransformer` class. It requires the `sentence_transformers` library to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/sentence-transformers.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('model_name')\n```\n\n----------------------------------------\n\nTITLE: Semantic Search with Sentence Transformers\nDESCRIPTION: This snippet shows how to use a Sentence Transformers model for semantic search. It encodes a query and a list of passages, then calculates the similarity between the query embedding and each passage embedding using the `util.dot_score` function. It depends on the `sentence_transformers` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/sentence-transformers.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer, util\nmodel = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n\nquery_embedding = model.encode('How big is London')\npassage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',\n                                  'London is known for its finacial district'])\n\nprint(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))\n```\n\n----------------------------------------\n\nTITLE: Create Hugging Face Estimator\nDESCRIPTION: This snippet creates a Hugging Face Estimator for SageMaker training and deployment. It configures the estimator with the entry point script, instance type, hyperparameters, and other necessary parameters like IAM role, Transformers version, PyTorch version, and Python version.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker.huggingface import HuggingFace\n\nhyperparameters={\n    \"epochs\": 1,                                       # number of training epochs\n    \"train_batch_size\": 32,                            # training batch size\n    \"model_name\":\"distilbert/distilbert-base-uncased\"  # name of pretrained model\n}\n\nhuggingface_estimator = HuggingFace(\n    entry_point=\"train.py\",                 # fine-tuning script to use in training job\n    source_dir=\"./scripts\",                 # directory where fine-tuning script is stored\n    instance_type=\"ml.p3.2xlarge\",          # instance type\n    instance_count=1,                       # number of instances\n    role=role,                              # IAM role used in training job to acccess AWS resources (S3)\n    transformers_version=\"4.26\",             # Transformers version\n    pytorch_version=\"1.13\",                  # PyTorch version\n    py_version=\"py39\",                      # Python version\n    hyperparameters=hyperparameters         # hyperparameters to use in training job\n)\n```\n\n----------------------------------------\n\nTITLE: Download entire repo snapshot using snapshot_download\nDESCRIPTION: Downloads an entire repository from the Hugging Face Hub using the `snapshot_download` function. All files in the repository are downloaded in parallel and cached locally. Requires the `huggingface_hub` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import snapshot_download\n>>> snapshot_download(repo_id=\"lysandre/arxiv-nlp\")\n'/home/lysandre/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade'\n```\n\n----------------------------------------\n\nTITLE: Testing SSH Authentication\nDESCRIPTION: Tests the SSH connection to Hugging Face using the `ssh` command. The `-T` option disables pseudo-terminal allocation.  It connects to `git@hf.co` and attempts to authenticate using the SSH key associated with the user's Hugging Face account.  A successful connection will display the user's username.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n$ ssh -T git@hf.co\n```\n\n----------------------------------------\n\nTITLE: Generating a New SSH Keypair (ed25519)\nDESCRIPTION: Generates a new SSH key pair using the `ssh-keygen` command with the ed25519 algorithm. The `-t` option specifies the key type as ed25519, and the `-C` option adds a comment, typically the user's email address.  It prompts the user to enter a passphrase for added security, and stores the key in the default `.ssh` directory.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n$ ssh-keygen -t ed25519 -C \"your.email@example.co\"\n```\n\n----------------------------------------\n\nTITLE: Sharing Adapters on the Hub\nDESCRIPTION: This code snippet demonstrates how to share an adapter on the Hugging Face Hub using the `push_adapter_to_hub` method. It takes the adapter's name, the repository name, and optional tags for AdapterHub and datasets. This creates a repository with a model card and metadata.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/adapters.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmodel.push_adapter_to_hub(\n    \"my-awesome-adapter\",\n    \"awesome_adapter\",\n    adapterhub_tag=\"sentiment/imdb\",\n    datasets_tag=\"imdb\"\n)\n```\n\n----------------------------------------\n\nTITLE: Example Training Execution Command\nDESCRIPTION: This Bash code represents the command executed by SageMaker to start the training job. It calls the training script (`train.py`) with specified hyperparameters.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n/opt/conda/bin/python train.py --epochs 1 --model_name_or_path distilbert-base-uncased --per_device_train_batch_size 32\n```\n\n----------------------------------------\n\nTITLE: Adding Hugging Face Space as a Git Remote (bash)\nDESCRIPTION: This command adds a Hugging Face Space as a remote repository to your local Git repository, allowing you to push changes directly to your Space. Replace HF_USERNAME and SPACE_NAME with your Hugging Face username and Space name respectively.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\n----------------------------------------\n\nTITLE: Disable XSRF Protection via Configuration File in Streamlit\nDESCRIPTION: This configuration snippet disables Streamlit's XSRF protection by setting `enableXsrfProtection = false` in the `.streamlit/config.toml` file.  This approach provides an alternative to the command line argument and persists the setting between application runs.  This is required to use `st.file_uploader()` and components like it properly on HF Spaces when hosting via Docker SDK.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-cookie-limitations.md#_snippet_1\n\nLANGUAGE: TOML\nCODE:\n```\n[server]\nenableXsrfProtection = false\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Files Per Split Using YAML\nDESCRIPTION: This YAML snippet defines a dataset configuration with multiple files per split. The 'train' split uses two CSV files located in the 'data' directory, while the 'test' split uses one CSV file from the 'holdout' directory. The `path` field accepts a list of file paths for each split.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-manual-configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path:\n    - \"data/abc.csv\"\n    - \"data/def.csv\"\n  - split: test\n    path: \"holdout/ghi.csv\"\n---\n```\n\n----------------------------------------\n\nTITLE: Loading a mlx-image model in Python\nDESCRIPTION: This snippet demonstrates how to load a pre-trained mlx-image model using the create_model function. It shows how to load weights either from the Hugging Face Hub or from a local file. It requires the mlxim library to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx-image.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom mlxim.model import create_model\n\n# loading weights from HuggingFace (https://huggingface.co/mlx-vision/resnet18-mlxim)\nmodel = create_model(\"resnet18\") # pretrained weights loaded from HF\n\n# loading weights from local file\nmodel = create_model(\"resnet18\", weights=\"path/to/resnet18/model.safetensors\")\n```\n\n----------------------------------------\n\nTITLE: Downloading model using Hugging Face CLI\nDESCRIPTION: Downloads a model from the Hugging Face Hub using the `huggingface-cli` tool. This command retrieves the specified model repository to the local machine.  It requires the `huggingface_hub` package to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-downloading.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli download HuggingFaceH4/zephyr-7b-beta\n```\n\n----------------------------------------\n\nTITLE: Reading Multiple Parquet Files with Polars using Globbing\nDESCRIPTION: This snippet shows how to read multiple Parquet files at once from the Hugging Face Hub into a single Polars DataFrame using globbing. The `pl.read_parquet` function is used with a Hugging Face URL containing a wildcard character `*` to match multiple files.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\n\npl.read_parquet(\"hf://datasets/roneneldan/TinyStories/data/train-*.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Describing Parquet schema\nDESCRIPTION: This snippet demonstrates retrieving the schema of a Parquet file using the `DESCRIBE` command in DuckDB. It describes the column names, column types, nullability, and other properties of the columns in the Parquet file located at the specified Hugging Face dataset path. It requires the Hugging Face dataset at the specified path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-select.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nDESCRIBE SELECT * FROM 'hf://datasets/jamescalam/world-cities-geo@~parquet/default/train/0000.parquet';\n```\n\n----------------------------------------\n\nTITLE: Running GGUF Model with Custom Quantization\nDESCRIPTION: This snippet shows how to specify a custom quantization scheme when running a GGUF model from the Hugging Face Hub using Ollama. It allows the user to select a specific quantization file available within the model repository.  Ollama must be installed and configured.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/ollama.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/{username}/{repository}:{quantization}\n```\n\n----------------------------------------\n\nTITLE: Data Parallelism Configuration with Hugging Face Estimator in SageMaker (Python)\nDESCRIPTION: This code snippet demonstrates how to configure the Hugging Face Estimator to utilize SageMaker's data parallelism library.  It defines the `distribution` parameter within the estimator to enable data parallelism. This splits the training dataset across multiple GPUs for faster training. It requires the Hugging Face Trainer API to be used within the training script.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# configuration for running training on smdistributed data parallel\ndistribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3dn.24xlarge',\n        instance_count=2,\n        role=role,\n        transformers_version='4.26.0',\n        pytorch_version='1.13.1',\n        py_version='py39',\n        hyperparameters = hyperparameters,\n        distribution = distribution\n)\n```\n\n----------------------------------------\n\nTITLE: Resampling Audio Data with Librosa\nDESCRIPTION: This snippet shows how to use `librosa` to resample audio data stored in a Pandas DataFrame column. It loads the audio using `librosa.load` with a specified sample rate and then writes the resampled audio back to the DataFrame using `pandas-audio-methods`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf[\"audio\"] = [librosa.load(audio, sr=16_000) for audio in df[\"audio\"]]\ndf[\"audio\"] = df[\"audio\"].sf.write()\n```\n\n----------------------------------------\n\nTITLE: GitHub Action: Syncing to Hugging Face Hub (yaml)\nDESCRIPTION: This GitHub Action automatically synchronizes the main branch of a GitHub repository with a Hugging Face Space whenever changes are pushed.  It requires a Hugging Face API token stored as a GitHub secret named `HF_TOKEN`.  It checks out the repository, configures Git LFS, and pushes the changes to the Space using the provided credentials.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: Sync to Hugging Face hub\non:\n  push:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\n```\n\n----------------------------------------\n\nTITLE: Install Keras and Hugging Face Hub\nDESCRIPTION: Installs the required libraries for using Keras with the Hugging Face Hub. This command installs the Keras deep learning framework and the huggingface_hub library, which is used for interacting with the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/keras.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install -U keras huggingface_hub\n```\n\n----------------------------------------\n\nTITLE: Tracing LLM Calls to Inference Providers using Langfuse Python SDK\nDESCRIPTION: This code snippet demonstrates how to trace LLM calls to Inference Providers using the Langfuse Python SDK. It requires the `langfuse`, `langfuse.openai`, and `huggingface_hub` packages. The code initializes an OpenAI client with the Hugging Face Inference API endpoint and API key, then sends a chat completion request and traces the interaction.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-langfuse.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom langfuse.openai import openai\nfrom huggingface_hub import get_token\n\nclient = openai.OpenAI(\n    base_url=\"https://router.huggingface.co/hf-inference/models/meta-llama/Llama-3.3-70B-Instruct/v1\",\n    api_key=get_token(),\n)\n\nmessages = [{\"role\": \"user\", \"content\": \"What is observability for LLMs?\"}]\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n    messages=messages,\n    max_tokens=100,\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting model data from memory as a DDUF file\nDESCRIPTION: This code demonstrates how to export a diffusion model loaded in memory directly into a DDUF file using a generator.  The `as_entries` function creates a generator that yields tuples containing the filename in the DDUF archive and the content of the file (either as bytes or as a string encoded to bytes).  `export_entries_as_dduf` then uses this generator to create the DDUF file. Requires `safetensors` and a `DiffusionPipeline` object.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/dduf.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n(...)\n\n# Export state_dicts one by one from a loaded pipeline\n>>> def as_entries(pipe: DiffusionPipeline) -> Generator[Tuple[str, bytes], None, None]:\n...     # Build a generator that yields the entries to add to the DDUF file.\n...     # The first element of the tuple is the filename in the DDUF archive. The second element is the content of the file.\n...     # Entries will be evaluated lazily when the DDUF file is created (only 1 entry is loaded in memory at a time)\n...     yield \"vae/config.json\", pipe.vae.to_json_string().encode()\n...     yield \"vae/diffusion_pytorch_model.safetensors\", safetensors.torch.save(pipe.vae.state_dict())\n...     yield \"text_encoder/config.json\", pipe.text_encoder.config.to_json_string().encode()\n...     yield \"text_encoder/model.safetensors\", safetensors.torch.save(pipe.text_encoder.state_dict())\n...     # ... add more entries here\n\n>>> export_entries_as_dduf(dduf_path=\"my-cool-diffusion-model.dduf\", entries=as_entries(pipe))\n```\n\n----------------------------------------\n\nTITLE: Creating Flan T5 Inference Endpoint\nDESCRIPTION: This Python code defines a FastAPI endpoint `/infer_t5` that uses the `transformers` pipeline to perform text generation with the google/flan-t5-small model. It takes an input parameter and returns the generated text as a JSON response.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\n\npipe_flan = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n\n@app.get(\"/infer_t5\")\ndef t5(input):\n    output = pipe_flan(input)\n    return {\"output\": output[0][\"generated_text\"]}\n```\n\n----------------------------------------\n\nTITLE: Constructing Hugging Face DLC Image URI\nDESCRIPTION: This snippet illustrates how to manually construct the image URI for a Hugging Face Deep Learning Container (DLC). It includes placeholders for the AWS account ID, region, framework, mode (training/inference), framework version, Transformers version, device type, Python version, and device tag. An example for PyTorch training and Tensorflow Inference is also provided.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_0\n\nLANGUAGE: None\nCODE:\n```\n{dlc-aws-account-id}.dkr.ecr.{region}.amazonaws.com/huggingface-{framework}-{(training | inference)}:{framework-version}-transformers{transformers-version}-{device}-{python-version}-{device-tag}\n```\n\n----------------------------------------\n\nTITLE: Adding SSH Key to SSH Agent\nDESCRIPTION: Adds the generated SSH key to the SSH agent using the `ssh-add` command. The SSH agent manages private keys used for authentication. The path `~/.ssh/id_ed25519` specifies the default location of the private key. Replace with your actual path if you specified a different location.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n$ ssh-add ~/.ssh/id_ed25519\n```\n\n----------------------------------------\n\nTITLE: Sharing a BERTopic Model to Hub\nDESCRIPTION: This snippet shows how to train a BERTopic model and push it to the Hugging Face Hub. It first trains a model using `BERTopic().fit(my_docs)`, then uses the `push_to_hf_hub` function to upload the model to the specified repository. The `save_ctfidf` parameter ensures that the c-TF-IDF matrix is also saved.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/bertopic.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom bertopic import BERTopic\n\n# Train model\ntopic_model = BERTopic().fit(my_docs)\n\n# Push to HuggingFace Hub\ntopic_model.push_to_hf_hub(\n    repo_id=\"MaartenGr/BERTopic_ArXiv\",\n    save_ctfidf=True\n)\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face CLI\nDESCRIPTION: This command authenticates the user with the Hugging Face CLI. This is required to read private/gated datasets or write to your own datasets.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Setup Local Environment for SageMaker\nDESCRIPTION: Sets up the local environment for interacting with SageMaker by importing necessary libraries, retrieving the IAM role, and creating a SageMaker session. It requires `boto3` for interacting with AWS IAM service.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport sagemaker\nimport boto3\n\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']\nsess = sagemaker.Session()\n```\n\n----------------------------------------\n\nTITLE: Loading and Activating Adapters\nDESCRIPTION: This code snippet demonstrates how to load a pre-trained adapter into a Transformers model using the `AutoAdapterModel` class from the `adapters` library.  It loads a base Roberta model, adds the specified adapter, and then activates the adapter for either inference or training. The `adapter_name` variable stores the identifier of the loaded adapter.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/adapters.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom adapters import AutoAdapterModel\n\n# 1.\nmodel = AutoAdapterModel.from_pretrained(\"FacebookAI/roberta-base\")\n# 2.\nadapter_name = model.load_adapter(\"AdapterHub/roberta-base-pf-imdb\")\n# 3.\nmodel.active_adapters = adapter_name\n# or model.train_adapter(adapter_name)\n```\n\n----------------------------------------\n\nTITLE: Passing Access Token as Parameter (Python)\nDESCRIPTION: This snippet shows how to explicitly pass the Hugging Face access token to a Polars reader function (e.g., `read_parquet`) using the `storage_options` parameter. The `token` key within `storage_options` should be set to your access token. This method takes precedence over environment variables and the CLI.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-auth.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npl.read_parquet(\n    \"hf://datasets/roneneldan/TinyStories/data/train-*.parquet\",\n    storage_options={\"token\": ACCESS_TOKEN},\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading models from Hugging Face Hub\nDESCRIPTION: This shows how to download models other than the supported ones from the Hugging Face Hub using `huggingface_hub` and `hf_transfer`. It first installs the packages if they are missing, then exports the `HF_HUB_ENABLE_HF_TRANSFER` environment variable to enable faster downloads, and finally uses `huggingface-cli download` to download the model files to a local directory. Replace `<LOCAL FOLDER PATH>`, `<USER_ID>`, and `<MODEL_NAME>` with the appropriate values.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npip install huggingface_hub hf_transfer\n\nexport HF_HUB_ENABLE_HF_TRANSFER=1\nhuggingface-cli download --local-dir <LOCAL FOLDER PATH> <USER_ID>/<MODEL_NAME>\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Classification Inference with Python\nDESCRIPTION: This snippet demonstrates how to use the Python 'requests' library to perform zero-shot classification using the Inference API.  It sends a POST request with the input text and candidate labels to the API endpoint and prints the response as JSON.  Requires the 'requests' library to be installed.  Uses the facebook/bart-large-mnli model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/tasks/zero-shot-classification.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nAPI_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\"\nheaders = {\"Authorization\": \"Bearer YOUR_API_TOKEN\"}\n\ndef query(payload):\n\tresponse = requests.post(API_URL, headers=headers, json=payload)\n\treturn response.json()\n\noutput = query({\n\t\"inputs\": \"The movie was great!\",\n\t\"parameters\": {\"candidate_labels\": [\"positive\", \"negative\"]}\n})\n\nprint(output)\n```\n\n----------------------------------------\n\nTITLE: Setting a Default Subset Using YAML\nDESCRIPTION: This YAML snippet shows how to set the 'main_data' subset as the default subset. The `default: true` parameter specifies that this subset should be the first to be shown in the Dataset Viewer, and is also the subset data libraries load by default.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-manual-configuration.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n-\n config_name: main_data\n data_files: \"main_data.csv\"\n default: true\n```\n\n----------------------------------------\n\nTITLE: Setup SageMaker environment\nDESCRIPTION: This Python code snippet sets up the SageMaker environment by importing the sagemaker library, creating a SageMaker session, and retrieving the execution role.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n```\n\n----------------------------------------\n\nTITLE: Pydantic Model Definitions for Webhook Payload (Python)\nDESCRIPTION: These Python code snippets define Pydantic models for parsing the webhook payload. The models (`WebhookPayloadEvent`, `WebhookPayloadRepo`, `WebhookPayload`) structure the expected JSON data. The snippet demonstrates how to use `Literal` to restrict accepted values and how to define nested models. These models are used for data validation and parsing within the FastAPI endpoint.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-auto-retrain.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# defined in src/models.py\nclass WebhookPayloadEvent(BaseModel):\n\taction: Literal[\"create\", \"update\", \"delete\"]\n\tscope: str\n\nclass WebhookPayloadRepo(BaseModel):\n\ttype: Literal[\"dataset\", \"model\", \"space\"]\n\tname: str\n\tid: str\n\tprivate: bool\n\theadSha: str\n\nclass WebhookPayload(BaseModel):\n\tevent: WebhookPayloadEvent\n\trepo: WebhookPayloadRepo\n```\n\n----------------------------------------\n\nTITLE: Push a Distilabel Distiset to the Hugging Face Hub (Python)\nDESCRIPTION: This Python code pushes a Distilabel `Distiset` to a Hugging Face repository. It allows specifying the repository name, commit message, privacy settings, and authentication token. The `HF_TOKEN` environment variable should contain the user's Hugging Face API token.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-distilabel.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndistiset.push_to_hub(\n    \"my-org/my-dataset\",\n    commit_message=\"Initial commit\",\n    private=False,\n    token=os.getenv(\"HF_TOKEN\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Squash History After Writing to Hugging Face Hub\nDESCRIPTION: This Python snippet uses the `huggingface_hub` library to squash the commit history of a dataset repository on the Hugging Face Hub. This is recommended after writing data to avoid having a large number of commits (one commit per file).\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\n\nHfApi().super_squash_history(repo_id=repo_id, repo_type=\"dataset\")\n```\n\n----------------------------------------\n\nTITLE: Loading a fastai model from the Hub\nDESCRIPTION: This snippet loads a pre-trained fastai model from the Hugging Face Hub using the `from_pretrained_fastai` function. It then uses the loaded model to make a prediction on an image and prints the probability of the image being a cat. The `from_pretrained_fastai` method ensures version compatibility.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/fastai.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import from_pretrained_fastai\n\nlearner = from_pretrained_fastai(\"espejelomar/identify-my-cat\")\n\n_,_,probs = learner.predict(img)\nprint(f\"Probability it's a cat: {100*probs[1].item():.2f}%\" )\n\n# Probability it's a cat: 100.00%\n```\n\n----------------------------------------\n\nTITLE: Send request to llama.cpp server\nDESCRIPTION: This command sends a chat completions request to the `llama-server` using `curl`. It includes the necessary headers and JSON payload with the chat messages. The server is expected to be running on `localhost:8080`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/gguf-llamacpp.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer no-key\" \\\n-d '{\n\"messages\": [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are an AI assistant. Your top priority is achieving user fulfilment via helping them with their requests.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Write a limerick about Python exceptions\"\n    }\n  ]\n}'\n```\n\n----------------------------------------\n\nTITLE: Execute Training Job\nDESCRIPTION: This Python code snippet starts a SageMaker training job by calling the `fit` method on a Hugging Face Estimator. It specifies the input training data using S3 URIs for training and testing datasets.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nhuggingface_estimator.fit(\n  {'train': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train',\n   'test': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test'}\n)\n```\n\n----------------------------------------\n\nTITLE: Configure Multiple Widget Inputs with Titles YAML\nDESCRIPTION: This YAML snippet configures multiple widget inputs with example titles, showcasing different examples in the widget's dropdown menu. It defines a list of text inputs, each with its own text and example title.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nwidget:\n- text: \"Is this review positive or negative? Review: Best cast iron skillet you will ever buy.\"\n  example_title: \"Sentiment analysis\"\n- text: \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had ...\"\n  example_title: \"Coreference resolution\"\n- text: \"On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book ...\"\n  example_title: \"Logic puzzles\"\n- text: \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night ...\"\n  example_title: \"Reading comprehension\"\n```\n\n----------------------------------------\n\nTITLE: Preprocess IMDb Dataset with Hugging Face Datasets\nDESCRIPTION: This snippet downloads and preprocesses the IMDb dataset using the Hugging Face Datasets library. It loads the dataset, initializes a tokenizer, defines a tokenization function, applies the function to the dataset, and sets the dataset format for PyTorch.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\n# load dataset\ntrain_dataset, test_dataset = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n\n# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n\n# create tokenization function\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True)\n\n# tokenize train and test datasets\ntrain_dataset = train_dataset.map(tokenize, batched=True)\ntest_dataset = test_dataset.map(tokenize, batched=True)\n\n# set dataset format for PyTorch\ntrain_dataset =  train_dataset.rename_column(\"label\", \"labels\")\ntrain_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntest_dataset = test_dataset.rename_column(\"label\", \"labels\")\ntest_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n```\n\n----------------------------------------\n\nTITLE: Deploy Hugging Face Model After Training\nDESCRIPTION: Deploys a Hugging Face model to SageMaker inference directly after training. This snippet shows pseudo-code for training and then deploying the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker.huggingface import HuggingFace\n\n############ pseudo code start ############\n\n# create Hugging Face Estimator for training\nhuggingface_estimator = HuggingFace(....)\n\n# start the train job with our uploaded datasets as input\nhuggingface_estimator.fit(...)\n\n############ pseudo code end ############\n\n# deploy model to SageMaker Inference\npredictor = hf_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n\n# example request: you always need to define \"inputs\"\ndata = {\n   \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n\n# request\npredictor.predict(data)\n```\n\n----------------------------------------\n\nTITLE: Parquet Dataset Structure\nDESCRIPTION: This shows the file structure of a repository using the Parquet format, where all the audio and metadata are embedded inside a Parquet file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n train.parquet\n```\n\n----------------------------------------\n\nTITLE: Exporting Joined Data to Parquet\nDESCRIPTION: This snippet demonstrates how to export the result of a join operation to a Parquet file using DuckDB's `COPY` command. It joins the `TheFusion21/PokemonCards` and `wanghaofan/pokemon-wiki-captions` datasets on the name column, and then exports the resulting data to `output.parquet` in Parquet format.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-combine-and-export.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nCOPY (SELECT a.image_url\n        , a.caption AS card_caption\n        , a.name\n        , a.hp\n        , b.text_en as wiki_caption \nFROM 'hf://datasets/TheFusion21/PokemonCards/train.csv' a \nJOIN 'hf://datasets/wanghaofan/pokemon-wiki-captions/data/*.parquet' b \nON LOWER(a.name) = b.name_en) \nTO 'output.parquet' (FORMAT PARQUET);\n```\n\n----------------------------------------\n\nTITLE: Configuring Git to Sign Commits (Bash)\nDESCRIPTION: This snippet configures Git to use the specified GPG key for signing commits and sets the user's email address. Replace `<Your GPG Key ID>` and `<Your email on hf.co>` with the actual key ID and email address respectively. This configuration is required for Git to automatically sign commits with the selected key.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-gpg.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit config user.signingkey <Your GPG Key ID>\ngit config user.email <Your email on hf.co>\n```\n\n----------------------------------------\n\nTITLE: Build llama.cpp from source\nDESCRIPTION: These commands build llama.cpp from source using CMake. You can optionally add hardware-specific flags such as `-DGGML_CUDA=1` to enable CUDA support for Nvidia GPUs.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/gguf-llamacpp.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncd llama.cpp\ncmake -B build   # optionally, add -DGGML_CUDA=ON to activate CUDA\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Write Pandas DataFrame to Hub\nDESCRIPTION: This code snippet shows how to write a Pandas DataFrame directly to the Hugging Face Hub. It initializes the Hub API, creates a repository (if it doesn't exist), and then saves the DataFrame as a Parquet file to the Hub. It requires the `huggingface_hub` and `pandas` libraries to be installed, and the `HF_TOKEN` environment variable to be set.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-libraries.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\n\n# Initialize the Hub API\nhf_api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n\n# Create a repository (if it doesn't exist)\nhf_api.create_repo(repo_id=\"username/my-dataset\", repo_type=\"dataset\")\n\n# Convert your data to a DataFrame and save directly to the Hub\ndf.to_parquet(\"hf://datasets/username/my-dataset/data.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Conversational VLM Inference Snippet\nDESCRIPTION: This snippet demonstrates how to use the Inference API for conversational Vision-Language Models (VLMs) within the Chat Completion task. It configures various providers, including Cerebras, Cohere, Fireworks AI, HF Inference, Hyperbolic, Nebius, Novita, Sambanova, and Together AI, mapping model IDs to their corresponding provider model IDs. The pipeline is `image-text-to-text` and set as `conversational`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/tasks/chat-completion.md#_snippet_1\n\nLANGUAGE: InferenceSnippet\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"cerebras\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"llama-4-scout-17b-16e-instruct\"},\"cohere\":{\"modelId\":\"CohereLabs/aya-vision-8b\",\"providerModelId\":\"c4ai-aya-vision-8b\"},\"fireworks-ai\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"accounts/fireworks/models/llama4-scout-instruct-basic\"},\"hf-inference\":{\"modelId\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"providerModelId\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\"},\"hyperbolic\":{\"modelId\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"providerModelId\":\"Qwen/Qwen2.5-VL-7B-Instruct\"},\"nebius\":{\"modelId\":\"google/gemma-3-27b-it\",\"providerModelId\":\"google/gemma-3-27b-it-fast\"},\"novita\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"meta-llama/llama-4-scout-17b-16e-instruct\"},\"sambanova\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"Llama-4-Scout-17B-16E-Instruct\"},\"together\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\"}} }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Filtering with Complex Predicates using Polars\nDESCRIPTION: This snippet demonstrates filtering a Polars DataFrame using complex predicates combined with logical operators. It filters based on either the 'date' column being greater than or equal to January 1, 2020, or the 'crawl' column containing 'CC'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-operations.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = df.filter(\n    (pl.col(\"date\") >= datetime.date(2020, 1, 1)) |\n    pl.col(\"crawl\").str.contains(\"CC\")\n)\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face Programmatically in Python\nDESCRIPTION: This Python snippet shows how to log in to the Hugging Face Hub programmatically using the `huggingface_hub` library. This is useful for automating the login process in scripts or notebooks. Requires the `huggingface_hub` library to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import login\n>>> login()\n```\n\n----------------------------------------\n\nTITLE: Cleaning Data and Mapping Answers using DuckDB\nDESCRIPTION: This SQL query cleans question data by removing newline characters and maps questions to their correct answers. It uses `regexp_replace` to remove newline characters from the question text and accesses the correct answer using array indexing.  It assumes the input dataset 'hf://datasets/cais/mmlu/all/test-*.parquet' is available to DuckDB.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-sql.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT regexp_replace(question, '\\n', '') AS question,\n       choices[answer] AS correct_answer\nFROM   'hf://datasets/cais/mmlu/all/test-*.parquet'\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom HTTP Headers in Spaces YAML\nDESCRIPTION: This YAML snippet configures custom HTTP headers for a Hugging Face Space, specifically setting the Cross-Origin Embedder Policy (COEP), Cross-Origin Opener Policy (COOP), and Cross-Origin Resource Policy (CORP). These headers are used to establish a cross-origin isolated environment, enabling features like SharedArrayBuffer.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-config-reference.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncustom_headers:\n  cross-origin-embedder-policy: require-corp\n  cross-origin-opener-policy: same-origin\n  cross-origin-resource-policy: cross-origin\n```\n\n----------------------------------------\n\nTITLE: Setup SageMaker Environment\nDESCRIPTION: Sets up the SageMaker environment by importing the `sagemaker` library and creating a SageMaker session. It also retrieves the execution role for the SageMaker instance.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sagemaker\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n```\n\n----------------------------------------\n\nTITLE: Sentiment Analysis with Specific Model in JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to use a specific model for sentiment analysis with the `@huggingface/transformers` library. It initializes a pipeline for sentiment analysis using the `nlptown/bert-base-multilingual-uncased-sentiment` model. This code requires the `@huggingface/transformers` package to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/transformers-js.md#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// Use a different model for sentiment-analysis\nlet pipe = await pipeline('sentiment-analysis', 'nlptown/bert-base-multilingual-uncased-sentiment');\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Classification Inference with JavaScript\nDESCRIPTION: This snippet demonstrates how to use JavaScript's 'fetch' API to perform zero-shot classification using the Inference API. It sends a POST request with the input text and candidate labels to the API endpoint and logs the response as JSON to the console. Requires an API token for authentication. Uses the facebook/bart-large-mnli model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/tasks/zero-shot-classification.md#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\";\nconst headers = {\"Authorization\": \"Bearer YOUR_API_TOKEN\"};\n\nasync function query(payload) {\n\tconst response = await fetch(\n\t\tAPI_URL,\n\t\t{\n\t\t\theaders: headers,\n\t\t\tmethod: \"POST\",\n\t\t\tbody: JSON.stringify(payload)\n\t\t}\n\t);\n\tconst data = await response.json();\n\treturn data;\n}\n\nquery({\n\t\"inputs\": \"The movie was great!\",\n\t\"parameters\": {\"candidate_labels\": [\"positive\", \"negative\"]}\n}).then((output) => {\n\tconsole.log(output);\n});\n```\n\n----------------------------------------\n\nTITLE: Text Generation with Together\nDESCRIPTION: This snippet configures the InferenceSnippet component for Text Generation using the Together provider. It specifies the text-generation pipeline and maps the `deepseek-ai/DeepSeek-R1` model to its corresponding ID on the Together platform.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/together.md#_snippet_2\n\nLANGUAGE: JSX\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"together\":{\"modelId\":\"deepseek-ai/DeepSeek-R1\",\"providerModelId\":\"deepseek-ai/DeepSeek-R1\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Define Parquet Reader for Hugging Face with PySpark\nDESCRIPTION: This code defines a `read_parquet` function to load Parquet files from the Hugging Face Hub into a PySpark DataFrame. It leverages PyArrow for efficient data reading in a distributed manner. The function handles authentication via `huggingface-cli login` or `storage_options` and allows for column selection and data filtering.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\nfrom typing import Iterator, Optional, Union\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom huggingface_hub import HfFileSystem\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.sql.pandas.types import from_arrow_schema\n\n\ndef _read(iterator: Iterator[pa.RecordBatch], columns: Optional[list[str]], filters: Optional[Union[list[tuple], list[list[tuple]]]], **kwargs) -> Iterator[pa.RecordBatch]:\n    for batch in iterator:\n        paths = batch[0].to_pylist()\n        ds = pq.ParquetDataset(paths, **kwargs)\n        yield from ds._dataset.to_batches(columns=columns, filter=pq.filters_to_expression(filters) if filters else None)\n\n\ndef read_parquet(\n    path: str,\n    columns: Optional[list[str]] = None,\n    filters: Optional[Union[list[tuple], list[list[tuple]]]] = None,\n    **kwargs,\n) -> DataFrame:\n    \"\"\"\n    Loads Parquet files from Hugging Face using PyArrow, returning a PySPark `DataFrame`.\n\n    It reads Parquet files in a distributed manner.\n\n    Access private or gated repositories using `huggingface-cli login` or passing a token\n    using the `storage_options` argument: `storage_options={\\\"token\\\": \\\"hf_xxx\\\"}`\n\n    Parameters\n    ----------\n    path : str\n        Path to the file. Prefix with a protocol like `hf://` to read from Hugging Face.\n        You can read from multiple files if you pass a globstring.\n    columns : list, default None\n        If not None, only these columns will be read from the file.\n    filters : List[Tuple] or List[List[Tuple]], default None\n        To filter out data.\n        Filter syntax: [[(column, op, val), ...],...]\n        where op is [==, =, >, >=, <, <=, !=, in, not in]\n        The innermost tuples are transposed into a set of filters applied\n        through an `AND` operation.\n        The outer list combines these sets of filters through an `OR`\n        operation.\n        A single list of tuples can also be used, meaning that no `OR`\n        operation between set of filters is to be conducted.\n\n    **kwargs\n        Any additional kwargs are passed to pyarrow.parquet.ParquetDataset.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame based on parquet file.\n\n    Examples\n    --------\n    >>> path = \\\"hf://datasets/username/dataset/data.parquet\\\"\n    >>> pd.DataFrame({\\\"foo\\\": range(5), \\\"bar\\\": range(5, 10)}).to_parquet(path)\n    >>> read_parquet(path).show()\n    +---+---+\n    |foo|bar|\n    +---+---+\n    |  0|  5|\n    |  1|  6|\n    |  2|  7|\n    |  3|  8|\n    |  4|  9|\n    +---+---+\n    >>> read_parquet(path, columns=[\\\"bar\\\"]).show()\n    +---+\n    |bar|\n    +---+\n    |  5|\n    |  6|\n    |  7|\n    |  8|\n    |  9|\n    +---+\n    >>> sel = [(\\\"foo\\\", \\\">\\\", 2)]\n    >>> read_parquet(path, filters=sel).show()\n    +---+---+\n    |foo|bar|\n    +---+---+\n    |  3|  8|\n    |  4|  9|\n    +---+---+\n    \"\"\"\n    filesystem: HfFileSystem = kwargs.pop(\"filesystem\") if \"filesystem\" in kwargs else HfFileSystem(**kwargs.pop(\"storage_options\", {}))\n    paths = filesystem.glob(path)\n    if not paths:\n        raise FileNotFoundError(f\"Counldn't find any file at {path}\")\n    rdd = spark.sparkContext.parallelize([{\"path\": path} for path in paths], len(paths))\n    df = spark.createDataFrame(rdd)\n    arrow_schema = pq.read_schema(filesystem.open(paths[0]))\n    schema = pa.schema([field for field in arrow_schema if (columns is None or field.name in columns)], metadata=arrow_schema.metadata)\n    return df.mapInArrow(\n        partial(_read, columns=columns, filters=filters, filesystem=filesystem, schema=arrow_schema, **kwargs),\n        from_arrow_schema(schema),\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Resource Group Payload (POST)\nDESCRIPTION: This payload is used when creating a new resource group in an organization. It includes the name, description, a list of users with their roles (admin, write, or read), and a list of repositories with their types (dataset).\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_10\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"name\": \"name\",\n    \"description\": \"description\",\n    \"users\": [\n        {\n            \"user\": \"username\",\n            \"role\": \"admin\" // or \"write\" or \"read\"\n        }\n    ],\n    \"repos\": [\n        {\n            \"type\": \"dataset\",\n            \"name\": \"huggingface/repo\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image with Secrets\nDESCRIPTION: These commands demonstrate how to build a Docker image with secrets using `docker buildx`. It exports a secret as an environment variable, then uses `docker buildx build` to pass the secret as a build argument. The `docker run` command then passes the secret as an environment variable to the running container.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport SECRET_EXAMPLE=\"my_secret_value\"\ndocker buildx build --secret id=SECRET_EXAMPLE,env=SECRET_EXAMPLE -t fastapi .\n```\n\n----------------------------------------\n\nTITLE: Object Detection Widget YAML Configuration\nDESCRIPTION: This snippet configures an object detection widget. Each example contains the 'src' field for the image URL and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_19\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\n  example_title: Football Match\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\n  example_title: Airport\n```\n\n----------------------------------------\n\nTITLE: Disable XSRF Protection via Command Line in Streamlit\nDESCRIPTION: This command disables Streamlit's XSRF protection by passing `--server.enableXsrfProtection false` as a command-line argument when running the application. This workaround is necessary because of cookie limitations in Hugging Face Spaces when using the Docker SDK.  It is required for `st.file_uploader()` and other similar components to function correctly.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-cookie-limitations.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nstreamlit run app.py --server.enableXsrfProtection false\n```\n\n----------------------------------------\n\nTITLE: Pushing AllenNLP Model to Hub via CLI\nDESCRIPTION: This snippet demonstrates how to push an AllenNLP model to the Hugging Face Hub using the `allennlp push_to_hf` command-line tool. The `--repo_name` argument specifies the repository name on the Hub, and `--archive_path` specifies the path to the zipped model archive. Requires the AllenNLP CLI to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/allennlp.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nallennlp push_to_hf --repo_name test_allennlp --archive_path model\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies in requirements.txt\nDESCRIPTION: This lists the Python dependencies required for the Text Generation Space. It specifies the versions of `fastapi`, `requests`, `sentencepiece`, `torch`, `transformers`, and `uvicorn` to be installed in the Dockerfile.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nfastapi==0.74.*\nrequests==2.27.*\nsentencepiece==0.1.*\ntorch==1.11.*\ntransformers==4.*\nuvicorn[standard]==0.17.*\n```\n\n----------------------------------------\n\nTITLE: Loading a BERTopic Model from Hub\nDESCRIPTION: This snippet loads a pre-trained BERTopic model from the Hugging Face Hub. It initializes a BERTopic object using the `BERTopic.load()` method, which fetches the model weights and configuration from the specified repository ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/bertopic.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom bertopic import BERTopic\ntopic_model = BERTopic.load(\"MaartenGr/BERTopic_Wikipedia\")\n```\n\n----------------------------------------\n\nTITLE: Question Answering Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Question Answering using the `deepset/bert-large-uncased-whole-word-masking-squad2` model on HF Inference. This snippet sets the pipeline and the corresponding model ID for the inference provider.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_6\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=question-answering\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"deepset/bert-large-uncased-whole-word-masking-squad2\",\"providerModelId\":\"deepset/bert-large-uncased-whole-word-masking-squad2\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: info.json Example with Code, Models, and Data\nDESCRIPTION: This JSON file provides information about the project's files, including C# code, Sentis models, and data files. It's used for tracking downloads and understanding the project structure.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/unity-sentis.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"code\": [ \"mycode.cs\"],\n   \"models\": [ \"model1.sentis\", \"model2.sentis\"],\n   \"data\": [ \"vocab.txt\" ]\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading CSV dataset using huggingface_hub and Pandas (Python)\nDESCRIPTION: This code snippet demonstrates how to download a CSV dataset from the Hugging Face Hub using the `huggingface_hub` library and load it into a Pandas DataFrame. It utilizes the `hf_hub_download` function to retrieve the file and then uses `pd.read_csv` to load the data. The `repo_type` parameter is set to \"dataset\".\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-downloading.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"YOUR_REPO_ID\"\nFILENAME = \"data.csv\"\n\ndataset = pd.read_csv(\n    hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")\n)\n```\n\n----------------------------------------\n\nTITLE: Defining the Dockerfile\nDESCRIPTION: This Dockerfile sets up the environment for running the FastAPI application. It starts from a Python 3.9 base image, creates a user, sets the working directory, copies and installs the dependencies from `requirements.txt`, copies the application code, and defines the command to start the Uvicorn server.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_4\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# read the doc: https://huggingface.co/docs/hub/spaces-sdks-docker\n# you will also find guides on how best to write your Dockerfile\n\nFROM python:3.9\n\n# The two following lines are requirements for the Dev Mode to be functional\n# Learn more about the Dev Mode at https://huggingface.co/dev-mode-explorers\nRUN useradd -m -u 1000 user\nWORKDIR /app\n\nCOPY --chown=user ./requirements.txt requirements.txt\nRUN pip install --no-cache-dir --upgrade -r requirements.txt\n\nCOPY --chown=user . /app\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n```\n\n----------------------------------------\n\nTITLE: Querying JSONL datasets with FROM syntax\nDESCRIPTION: This snippet demonstrates querying a JSONL dataset using the `FROM` syntax in DuckDB. It selects the `city`, `country`, and `region` columns and limits the result to 3 rows. It requires the Hugging Face dataset at the specified path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-select.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFROM 'hf://datasets/jamescalam/world-cities-geo/train.jsonl' SELECT city, country, region LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Writing PySpark Dataframe to Hugging Face\nDESCRIPTION: This snippet defines a `write_parquet` function to write a PySpark Dataframe to a Hugging Face repository in a distributed manner. It uses `huggingface_hub` to pre-upload Parquet files in parallel using LFS and then commits the files in a single operation. Authentication is handled via `huggingface-cli login` or by providing a token in the `storage_options`.  It also defines helper functions `_preupload` and `_commit` to manage the upload and commit processes, using temporary files and binary serialization for commit operations.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport pickle\nimport tempfile\nfrom functools import partial\nfrom typing import Iterator, Optional\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom huggingface_hub import CommitOperationAdd, HfFileSystem\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.sql.pandas.types import from_arrow_schema, to_arrow_schema\n\n\ndef _preupload(iterator: Iterator[pa.RecordBatch], path: str, schema: pa.Schema, filesystem: HfFileSystem, row_group_size: Optional[int] = None, **kwargs) -> Iterator[pa.RecordBatch]:\n    resolved_path = filesystem.resolve_path(path)\n    with tempfile.NamedTemporaryFile(suffix=\".parquet\") as temp_file:\n        with pq.ParquetWriter(temp_file.name, schema=schema, **kwargs) as writer:\n            for batch in iterator:\n                writer.write_batch(batch, row_group_size=row_group_size)\n        addition = CommitOperationAdd(path_in_repo=temp_file.name, path_or_fileobj=temp_file.name)\n        filesystem._api.preupload_lfs_files(repo_id=resolved_path.repo_id, additions=[addition], repo_type=resolved_path.repo_type, revision=resolved_path.revision)\n    yield pa.record_batch({\"addition\": [pickle.dumps(addition)]}, schema=pa.schema({\"addition\": pa.binary()}))\n\n\ndef _commit(iterator: Iterator[pa.RecordBatch], path: str, filesystem: HfFileSystem, max_operations_per_commit=50) -> Iterator[pa.RecordBatch]:\n    resolved_path = filesystem.resolve_path(path)\n    additions: list[CommitOperationAdd] = [pickle.loads(addition) for addition in pa.Table.from_batches(iterator, schema=pa.schema({\"addition\": pa.binary()}))[0].to_pylist()]\n    num_commits = math.ceil(len(additions) / max_operations_per_commit)\n    for shard_idx, addition in enumerate(additions):\n        addition.path_in_repo = resolved_path.path_in_repo.replace(\"{shard_idx:05d}\", f\"{shard_idx:05d}\")\n    for i in range(0, num_commits):\n        operations = additions[i * max_operations_per_commit : (i + 1) * max_operations_per_commit]\n        commit_message = \"Upload using PySpark\" + (f\" (part {i:05d}-of-{num_commits:05d})\" if num_commits > 1 else \"\")\n        filesystem._api.create_commit(repo_id=resolved_path.repo_id, repo_type=resolved_path.repo_type, revision=resolved_path.revision, operations=operations, commit_message=commit_message)\n        yield pa.record_batch({\"path\": [addition.path_in_repo for addition in operations]}, schema=pa.schema({\"path\": pa.string()}))\n\n\ndef write_parquet(df: DataFrame, path: str, **kwargs) -> None:\n    \"\"\"\n    Write Parquet files to Hugging Face using PyArrow.\n\n    It uploads Parquet files in a distributed manner in two steps:\n\n    1. Preupload the Parquet files in parallel in a distributed banner\n    2. Commit the preuploaded files\n\n    Authenticate using `huggingface-cli login` or passing a token\n    using the `storage_options` argument: `storage_options={\"token\": \"hf_xxx\"}`\n\n    Parameters\n    ----------\n    path : str\n        Path of the file or directory. Prefix with a protocol like `hf://` to read from Hugging Face.\n        It writes Parquet files in the form \"part-xxxxx.parquet\", or to a single file if `path ends with \".parquet\".\n\n    **kwargs\n        Any additional kwargs are passed to pyarrow.parquet.ParquetWriter.\n\n    Returns\n    -------\n    DataFrame\n        DataFrame based on parquet file.\n\n    Examples\n    --------\n    >>> spark.createDataFrame(pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)}))\n    >>> # Save to one file\n    >>> write_parquet(df, \"hf://datasets/username/dataset/data.parquet\")\n    >>> # OR save to a directory (possibly in many files)\n    >>> write_parquet(df, \"hf://datasets/username/dataset\")\n    \"\"\"\n    filesystem: HfFileSystem = kwargs.pop(\"filesystem\", HfFileSystem(**kwargs.pop(\"storage_options\", {})))\n    if path.endswith(\".parquet\") or path.endswith(\".pq\"):\n        df = df.coalesce(1)\n    else:\n        path += \"/part-{shard_idx:05d}.parquet\"\n    df.mapInArrow(\n        partial(_preupload, path=path, schema=to_arrow_schema(df.schema), filesystem=filesystem, **kwargs),\n        from_arrow_schema(pa.schema({\"addition\": pa.binary()})),\n    ).repartition(1).mapInArrow(\n        partial(_commit, path=path, filesystem=filesystem),\n        from_arrow_schema(pa.schema({\"path\": pa.string()})),\n    ).collect()\n```\n\n----------------------------------------\n\nTITLE: Relative Paths Metadata CSV Example\nDESCRIPTION: This snippet shows a metadata.csv file where the 'file_name' column contains relative paths to the video files in a subdirectory.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_8\n\nLANGUAGE: csv\nCODE:\n```\nfile_name,text\nvideos/1.mp4,an animation of a green pokemon with red eyes\nvideos/2.mp4,a short video of a green and yellow toy with a red nose\nvideos/3.mp4,a red and white ball shows an angry look on its face\nvideos/4.mp4,a cartoon ball is smiling\n```\n\n----------------------------------------\n\nTITLE: Configuring NER Aggregation Strategy YAML\nDESCRIPTION: This snippet shows how to specify the aggregation strategy parameter for a Named Entity Recognition (NER) task within the HF-Inference API widget. The `aggregation_strategy` is set to \"none\", which disables aggregation. This configuration is added to the model card metadata in YAML format.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ninference:\n  parameters:\n    aggregation_strategy: \"none\"\n```\n\n----------------------------------------\n\nTITLE: GitHub Action: Checking File Size (yaml)\nDESCRIPTION: This GitHub Action checks the file size of files in a pull request against a specified limit (10MB in this case) to ensure they are compatible with Hugging Face Spaces. It uses the `ActionsDesk/lfs-warning@v2.0` action to perform the file size check.  It runs on pull requests targeting the main branch.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```\n\n----------------------------------------\n\nTITLE: Custom Inference Module with model_fn, input_fn, predict_fn, output_fn\nDESCRIPTION: This Python code demonstrates a custom inference module that overrides the default `model_fn`, `input_fn`, `predict_fn`, and `output_fn` methods.  It imports `decoder_encoder` for data conversion and includes placeholder logic for loading the model, decoding input, predicting, and encoding output.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\n\ndef model_fn(model_dir):\n    # implement custom code to load the model\n    loaded_model = ...\n    \n    return loaded_model \n\ndef input_fn(input_data, content_type):\n    # decode the input data  (e.g. JSON string -> dict)\n    data = decoder_encoder.decode(input_data, content_type)\n    return data\n\ndef predict_fn(data, model):\n    # call your custom model with the data\n    outputs = model(data , ... )\n    return predictions\n\ndef output_fn(prediction, accept):\n    # convert the model output to the desired output format (e.g. dict -> JSON string)\n    response = decoder_encoder.encode(prediction, accept)\n    return response\n```\n\n----------------------------------------\n\nTITLE: Configuring OAuth in Space Metadata with YAML\nDESCRIPTION: Shows how to configure OAuth settings in a Hugging Face Space by adding `hf_oauth: true` to the Space's metadata inside the `README.md` file. It includes optional configurations for setting the OAuth expiration time, defining scopes, and restricting access to specific organizations.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-oauth.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Gradio Oauth Test\nemoji: \ncolorFrom: pink\ncolorTo: pink\nsdk: gradio\nsdk_version: 3.40.0\npython_version: 3.10.6\napp_file: app.py\n\nhf_oauth: true\n# optional, default duration is 8 hours/480 minutes. Max duration is 30 days/43200 minutes.\nhf_oauth_expiration_minutes: 480\n# optional, see \"Scopes\" below. \"openid profile\" is always included.\nhf_oauth_scopes:\n - read-repos\n - write-repos\n - manage-repos\n - inference-api\n# optional, restrict access to members of specific organizations\nhf_oauth_authorized_org: ORG_NAME\nhf_oauth_authorized_org:\n  - ORG_NAME1\n  - ORG_NAME2\n```\n\n----------------------------------------\n\nTITLE: Accelerating downloads with hf_transfer\nDESCRIPTION: Accelerates downloads from the Hugging Face Hub using the `hf_transfer` library. It is a rust-based library focused on speed. Requires installing `huggingface_hub` with the `hf_transfer` extra and setting an environment variable.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-downloading.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install \"huggingface_hub[hf_transfer]\"\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download ...\n```\n\n----------------------------------------\n\nTITLE: Downloading ML-Agents model from Hugging Face Hub\nDESCRIPTION: This command downloads an ML-Agents model from the Hugging Face Hub using the `mlagents-load-from-hf` tool. It requires specifying the repository ID and the local directory where the model should be saved. The repository ID is the name of the Hugging Face repository containing the model, and the local directory is the path to the directory on the local machine where the model files will be downloaded.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/ml-agents.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmlagents-load-from-hf --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --local-dir=\"./downloads\"\n```\n\n----------------------------------------\n\nTITLE: Preparing LFS Files for Push\nDESCRIPTION: Installs LFS hooks, forcing a reinstall, and enables large file support using `huggingface-cli`.  This ensures that files larger than 5GB are handled correctly. Requires the `huggingface_hub` CLI to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-next-steps.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install --force --local # this reinstalls the LFS hooks\nhuggingface-cli lfs-enable-largefiles . # needed if some files are bigger than 5GB\n```\n\n----------------------------------------\n\nTITLE: Logging in Programmatically - Python\nDESCRIPTION: This Python snippet shows how to log in to the Hugging Face Hub programmatically using the `login()` function from the `huggingface_hub` library. This is useful for automating the login process in scripts or notebooks.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-gated.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import login\n>>> login()\n```\n\n----------------------------------------\n\nTITLE: Image Classification Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Image Classification using the `Falconsai/nsfw_image_detection` model on HF Inference.  This specifies the pipeline and the relevant model ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-classification\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"Falconsai/nsfw_image_detection\",\"providerModelId\":\"Falconsai/nsfw_image_detection\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Example Dockerfile for Spaces Dev Mode\nDESCRIPTION: This Dockerfile provides an example of a configuration compatible with Spaces Dev Mode. It installs necessary packages like bash, git, wget, curl, and procps. It sets the working directory to /app, copies the application code, installs dependencies, sets the user to uid 1000, and defines the CMD instruction to start a NodeJS application. This example uses node:19-slim as the base image.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-dev-mode.md#_snippet_1\n\nLANGUAGE: Dockerfile\nCODE:\n```\nFROM node:19-slim\n\nRUN apt-get update && \\\n    apt-get install -y \\\n      bash \\\n      git git-lfs \\\n      wget curl procps \\\n      htop vim nano && \\\n    rm -rf /var/lib/apt/lists/*\n\n\nWORKDIR /app\nCOPY --link ./ /app\nRUN  npm i \n\nRUN chown 1000 /app\nUSER 1000\nCMD [\"node\", \"index.js\"]\n```\n\n----------------------------------------\n\nTITLE: Text2Text Generation Widget YAML Configuration\nDESCRIPTION: This snippet configures a text-to-text generation widget with two examples. Each example includes a 'text' field representing the input text and an 'example_title' for the example.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"My name is Julien and I like to\"\n  example_title: \"Julien\"\n- text: \"My name is Merve and my favorite\"\n  example_title: \"Merve\"\n```\n\n----------------------------------------\n\nTITLE: Transforming Text with BERTopic\nDESCRIPTION: This snippet uses a loaded BERTopic model to predict the topic of a given text.  It calls the `transform` method to get the predicted topic and probability, then uses `topic_model.topic_labels_` to get a human-readable label for that topic.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/bertopic.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntopic, prob = topic_model.transform(\"This is an incredible movie!\")\ntopic_model.topic_labels_[topic]\n```\n\n----------------------------------------\n\nTITLE: Setting Organization Billing in JavaScript\nDESCRIPTION: This snippet demonstrates how to specify the organization to be billed when using the JavaScript InferenceClient. It imports the InferenceClient from the @huggingface/inference package, instantiates the client with a Hugging Face token and the organization name, and then uses the client to generate an image. The generated image is then used as a Blob.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/pricing.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { InferenceClient } from \"@huggingface/inference\";\n\nconst client = new InferenceClient(\"hf_token\", { billTo: \"my-org-name\" });\n\nconst image = await client.textToImage({\n\tmodel: \"black-forest-labs/FLUX.1-schnell\",\n\tinputs: \"A majestic lion in a fantasy forest\",\n\tprovider: \"fal-ai\",\n});\n/// Use the generated image (it's a Blob)\n```\n\n----------------------------------------\n\nTITLE: Mapping Questions to Correct Answers using DuckDB\nDESCRIPTION: This query maps questions to their corresponding correct answers from the 'choices' array in the MMLU dataset. It uses the 'answer' column as an index to extract the correct answer from the 'choices' array for questions related to 'nutrition', limiting the output to 3 rows.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-sql.md#_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT question,\n       choices[answer] AS correct_answer\nFROM   'hf://datasets/cais/mmlu/all/test-*.parquet'\nWHERE  subject = 'nutrition' LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Sampling Data from Hugging Face Dataset with DuckDB\nDESCRIPTION: This command retrieves a random sample of 3 rows from the MMLU dataset hosted on Hugging Face. It demonstrates how to preview the dataset's contents for initial exploration using DuckDB's `SAMPLE` clause.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-sql.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nFROM 'hf://datasets/cais/mmlu/all/test-*.parquet' USING SAMPLE 3;\n```\n\n----------------------------------------\n\nTITLE: Batch Transform with Hugging Face Estimator in SageMaker (Python)\nDESCRIPTION: This snippet demonstrates how to create a SageMaker batch transform job for a model trained using the Hugging Face Estimator. It sets instance count, instance type, and strategy, and then transforms the data from an S3 URI using the specified content type and split type. Requires `huggingface_estimator` to be a pre-existing Hugging Face Estimator object.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nbatch_job = huggingface_estimator.transformer(\n    instance_count=1,\n    instance_type='ml.p3.2xlarge',\n    strategy='SingleRecord')\n\n\nbatch_job.transform(\n    data='s3://s3-uri-to-batch-data',\n    content_type='application/json',    \n    split_type='Line')\n```\n\n----------------------------------------\n\nTITLE: Deploy Trained Model\nDESCRIPTION: This snippet deploys the fine-tuned model to a SageMaker endpoint. It specifies the initial instance count and instance type for the deployment. It uses the Hugging Face estimator's `deploy` method.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npredictor = huggingface_estimator.deploy(initial_instance_count=1,\"ml.g4dn.xlarge\")\n```\n\n----------------------------------------\n\nTITLE: Filter Webhook events based on action, scope, and content - TypeScript\nDESCRIPTION: This code snippet filters Webhook events based on three conditions: the event action must be 'create', the event scope must be 'discussion.comment', and the comment content must include the bot's username (BOT_USERNAME). This ensures that the bot only reacts to newly created comments that mention it.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-discussion-bot.md#_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\n\tconst event = req.body.event;\n\tif (\n\t\tevent.action === \"create\" &&\n\t\tevent.scope === \"discussion.comment\" &&\n\t\treq.body.comment.content.includes(BOT_USERNAME)\n\t) {\n\t\t...\n```\n\n----------------------------------------\n\nTITLE: Passing Builder Parameters in YAML\nDESCRIPTION: This YAML snippet demonstrates how to pass builder-specific parameters like the separator (`sep`) for CSV files. The 'tab' subset uses a tab separator ('\\t'), while the 'comma' subset uses a comma separator (','). This allows loading CSV files with different delimiters without custom code.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-manual-configuration.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: tab\n  data_files: \"main_data.csv\"\n  sep: \"\\t\"\n- config_name: comma\n  data_files: \"additional_data.csv\"\n  sep: \",\"\n---\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet File with Polars\nDESCRIPTION: This snippet demonstrates reading a Parquet file directly from the Hugging Face Hub using Polars. The `pl.read_parquet` function is used with a Hugging Face URL to access the dataset. No other dependencies are required beyond Polars itself.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\n\npl.read_parquet(\"hf://datasets/roneneldan/TinyStories/data/train-00000-of-00004-2d5a1467fff1081b.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Dataset Repository for Video Classification\nDESCRIPTION: This snippet demonstrates a directory structure for video classification, where subdirectories represent video classes. The directory names serve as labels for the videos within.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n green\n  1.mp4\n  2.mp4\n red\n     3.mp4\n     4.mp4\n```\n\n----------------------------------------\n\nTITLE: Upload Folder to Hub\nDESCRIPTION: This snippet uploads a folder to a Hugging Face Hub repository using the `huggingface_hub` library. It specifies the folder path, repository ID, repository type, commit message, and allowed file patterns. Requires the `huggingface_hub` library and `HF_TOKEN` to be defined.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-libraries.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\napi = HfApi(token=HF_TOKEN)\n\napi.upload_folder(\n    folder_path=\"/my-cool-library/data-folder\",\n    repo_id=\"username/my-cool-space\",\n    repo_type=\"dataset\",\n    commit_message=\"Push annotations to Hub\"\n    allow_patterns=\"*.jsonl\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Parquet Datasets from Hub\nDESCRIPTION: This code snippet loads a dataset from the Hugging Face Hub stored in Parquet format using FiftyOne's `load_from_hub()` function.  It specifies the `repo_id`, `format` as \"parquet\", classification fields, maximum number of samples, and the name of the dataset. Type conversions are automatically handled, and images are downloaded from URLs if necessary.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-fiftyone.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport fiftyone as fo\nfrom fiftyone.utils.huggingface import load_from_hub\n\ndataset = load_from_hub(\n    \"huggan/wikiart\",  ## repo_id\n    format=\"parquet\",  ## for Parquet format\n    classification_fields=[\"artist\", \"style\", \"genre\"], ## columns to treat as classification labels\n    max_samples=1000,  # number of samples to load\n    name=\"wikiart\",  # name of the dataset in FiftyOne\n)\n```\n\n----------------------------------------\n\nTITLE: Modify Argilla Dataset Settings\nDESCRIPTION: This snippet demonstrates how to modify the settings of an Argilla dataset after pulling its configuration from the Hugging Face Hub. In this example, it changes the dataset's questions to a simple text question named 'answer'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-argilla.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset.settings.questions = [rg.TextQuestion(name=\"answer\")]\n```\n\n----------------------------------------\n\nTITLE: Summarizing Dataset Statistics with DuckDB\nDESCRIPTION: This code snippet demonstrates how to use the SUMMARIZE command in DuckDB to calculate and display various aggregate statistics (min, max, approx_unique, avg, std, q25, q50, q75, count, null_percentage) for the 'latitude' and 'longitude' columns within a Parquet dataset hosted on the Hugging Face Hub. It selects data from a specific Parquet file within the dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-select.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nSUMMARIZE SELECT latitude, longitude FROM 'hf://datasets/jamescalam/world-cities-geo@~parquet/default/train/0000.parquet';\n```\n\n----------------------------------------\n\nTITLE: Setting Application Port in README\nDESCRIPTION: This YAML snippet allows changing the default application port of the Space. The `app_port` property in the `README.md` file's YAML block sets the port. The default port is `7860`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napp_port: 7860\n```\n\n----------------------------------------\n\nTITLE: Shuffle WebDataset\nDESCRIPTION: Shuffles a WebDataset dataset using the `shardshuffle` and `shuffle` methods. `shardshuffle` shuffles the list of shard files, and `shuffle` shuffles the data within a buffer to provide approximate shuffling without sacrificing speed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-webdataset.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> buffer_size = 1000\n>>> dataset = (\n...     wds.WebDataset(url, shardshuffle=True)\n...     .shuffle(buffer_size)\n...     .decode()\n... )\n```\n\n----------------------------------------\n\nTITLE: Creating a Webhook Listener with FastAPI\nDESCRIPTION: This code snippet demonstrates how to create a webhook listener using the FastAPI framework in Python. It defines a route that accepts POST requests at the /webhook endpoint. Authentication is implemented by comparing the X-Webhook-Secret header with a WEBHOOK_SECRET environment variable.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI, Request, Response\nimport os\n\nKEY = os.environ.get(\"WEBHOOK_SECRET\")\n\napp = FastAPI()\n\n@app.post(\"/webhook\")\nasync def webhook(request: Request):\n    if request.method == \"POST\":\n        if request.headers.get(\"X-Webhook-Secret\") != KEY:\n            return Response(\"Invalid secret\", status_code=401)\n        data = await request.json()\n        result = create_or_update_report(data)\n        return \"Webhook received!\" if result else result\n```\n\n----------------------------------------\n\nTITLE: Specifying Library Name in YAML\nDESCRIPTION: This YAML snippet shows how to specify the library name for a model in the model card metadata. This is recommended for models that are not transformers models. It allows the Hub to correctly identify the library the model uses.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nlibrary_name: flair\n```\n\n----------------------------------------\n\nTITLE: Inference with timm Model from Hub\nDESCRIPTION: This snippet showcases how to perform inference with a timm model loaded from the Hugging Face Hub. It includes loading the model, setting it to evaluation mode, creating data transforms, and performing the inference. It requires the `timm`, `torch`, and `PIL` libraries.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/timm.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport timm\nimport torch\nfrom PIL import Image\nfrom timm.data import resolve_data_config\nfrom timm.data.transforms_factory import create_transform\n\n# Load from Hub \nmodel = timm.create_model(\n    'hf-hub:nateraw/resnet50-oxford-iiit-pet',\n    pretrained=True\n)\n\n# Set model to eval mode for inference\nmodel.eval()\n\n# Create Transform\ntransform = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\n\n# Get the labels from the model config\nlabels = model.pretrained_cfg['label_names']\ntop_k = min(len(labels), 5)\n\n# Use your own image file here...\nimage = Image.open('boxer.jpg').convert('RGB')\n\n# Process PIL image with transforms and add a batch dimension\nx = transform(image).unsqueeze(0)\n\n# Pass inputs to model forward function to get outputs\nout = model(x)\n\n# Apply softmax to get predicted probabilities for each class\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\n\n# Grab the values and indices of top 5 predicted classes\nvalues, indices = torch.topk(probabilities, top_k)\n\n# Prepare a nice dict of top k predictions\npredictions = [\n    {\"label\": labels[i], \"score\": v.item()}\n    for i, v in zip(indices, values)\n]\nprint(predictions)\n```\n\n----------------------------------------\n\nTITLE: Loading and generating text in Python with MLX-LM\nDESCRIPTION: This Python code snippet demonstrates how to load a pre-trained language model using `mlx_lm` and generate text. It loads the model and tokenizer, then generates a response based on the prompt \"hello\", printing verbose output. Requires the `mlx_lm` package.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\nresponse = generate(model, tokenizer, prompt=\"hello\", verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Updating Repository Settings Payload in JavaScript\nDESCRIPTION: Defines the payload structure required to update repository settings, specifically visibility (private or public), using the Hugging Face Hub API. This payload corresponds to parameters used in `huggingface_hub.update_repo_settings()`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"private\": \"private\",\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Repository Payload in JavaScript\nDESCRIPTION: Defines the payload structure required to create a repository using the Hugging Face Hub API. The payload includes parameters such as the repository type, name, organization, privacy settings, and SDK type.  This corresponds to parameters used in `huggingface_hub.create_repo()`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"type\":\"model\",\n    \"name\":\"name\",\n    \"organization\": \"organization\",\n    \"private\":\"private\",\n    \"sdk\": \"sdk\"\n}\n```\n\n----------------------------------------\n\nTITLE: Dockerfile Secret Exposure - Curl Request\nDESCRIPTION: This Dockerfile snippet demonstrates how to expose a secret at build time and use it as a Bearer token for a curl request. It requires a secret named `SECRET_EXAMPLE` to be defined in the Space settings.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker.md#_snippet_3\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Expose the secret SECRET_EXAMPLE at buildtime and use its value as a Bearer token for a curl request\nRUN --mount=type=secret,id=SECRET_EXAMPLE,mode=0444,required=true \\\n\tcurl test -H 'Authorization: Bearer $(cat /run/secrets/SECRET_EXAMPLE)'\n```\n\n----------------------------------------\n\nTITLE: Handle Access Request via API (Accept/Reject/Pending)\nDESCRIPTION: This API endpoint changes the status of a user's access request for a gated dataset. It requires a token with write access and a payload specifying the new status (accepted/rejected/pending) and the username.  An optional rejection reason can be included.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md#_snippet_3\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /api/datasets/{repo_id}/user-access-request/handle\nHeaders: {\"authorization\": \"Bearer $token\"}\nPayload: {\"status\": \"accepted\"/\"rejected\"/\"pending\", \"user\": \"username\", \"rejectionReason\": \"Optional rejection reason that will be visible to the user (max 200 characters).\"}\n```\n\n----------------------------------------\n\nTITLE: Getting User Information Headers in JavaScript\nDESCRIPTION: Defines the headers required to authenticate and get user information using the Hugging Face Hub API. The header includes the authorization token. This corresponds to parameters used in `huggingface_hub.whoami()`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_8\n\nLANGUAGE: JavaScript\nCODE:\n```\nheaders = { \"authorization\" :  \"Bearer $token\" }\n```\n\n----------------------------------------\n\nTITLE: Training a mlx-image model in Python\nDESCRIPTION: This snippet shows how to train a mlx-image model using mlx.nn and mlx.optimizers. It includes data loading using LabelFolderDataset and DataLoader, model creation, optimizer setup, and a training loop. It requires the mlxim and mlx libraries to be installed. The `train_step` function calculates the cross-entropy loss.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx-image.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport mlx.nn as nn\nimport mlx.optimizers as optim\nfrom mlxim.model import create_model\nfrom mlxim.data import LabelFolderDataset, DataLoader\n\ntrain_dataset = LabelFolderDataset(\n    root_dir=\"path/to/train\",\n    class_map={0: \"class_0\", 1: \"class_1\", 2: [\"class_2\", \"class_3\"]}\n)\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4\n)\nmodel = create_model(\"resnet18\") # pretrained weights loaded from HF\noptimizer = optim.Adam(learning_rate=1e-3)\n\ndef train_step(model, inputs, targets):\n    logits = model(inputs)\n    loss = mx.mean(nn.losses.cross_entropy(logits, target))\n    return loss\n\nmodel.train()\nfor epoch in range(10):\n    for batch in train_loader:\n        x, target = batch\n        train_step_fn = nn.value_and_grad(model, train_step)\n        loss, grads = train_step_fn(x, target)\n        optimizer.update(model, grads)\n        mx.eval(model.state, optimizer.state)\n```\n\n----------------------------------------\n\nTITLE: Compress Aim repository for deployment\nDESCRIPTION: This bash command compresses the `.aim` folder, which contains the tracked experiment data, into a `tar.gz` archive named `aim_repo.tar.gz`. This archive can then be uploaded to a Hugging Face Space for visualization in the Aim UI. It requires `tar` to be available in the environment.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-aim.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntar -czvf aim_repo.tar.gz .aim\n```\n\n----------------------------------------\n\nTITLE: Specify CO2 Emissions in Model Card Metadata\nDESCRIPTION: This YAML snippet demonstrates how to specify the CO2 emissions in the model card metadata after calculating it using tools like Code Carbon. The `emissions` field represents the carbon emissions value in grams.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards-co2.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nco2_eq_emissions: \n  emissions: 1.2345\n```\n\n----------------------------------------\n\nTITLE: Batch Transform with Hugging Face Model in SageMaker (Python)\nDESCRIPTION: This snippet demonstrates how to create a SageMaker batch transform job for a model from the Hugging Face Hub.  It creates a `HuggingFaceModel` instance with specified environment variables (including the Hugging Face model ID and task), IAM role, Transformers version, PyTorch version, and Python version. Then it creates a transformer job and initiates the transform using data from an S3 bucket. Requires the `sagemaker` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n# Hub model configuration <https://huggingface.co/models>\nhub = {\n\t'HF_MODEL_ID':'distilbert/distilbert-base-uncased-finetuned-sst-2-english',\n\t'HF_TASK':'text-classification'\n}\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   env=hub,                                                # configuration for loading model from Hub\n   role=role,                                              # IAM role with permissions to create an endpoint\n   transformers_version=\"4.26\",                             # Transformers version used\n   pytorch_version=\"1.13\",                                  # PyTorch version used\n   py_version='py39',                                      # Python version used\n)\n\n# create transformer to run a batch job\nbatch_job = huggingface_model.transformer(\n    instance_count=1,\n    instance_type='ml.p3.2xlarge',\n    strategy='SingleRecord'\n)\n\n# starts batch transform job and uses S3 data as input\nbatch_job.transform(\n    data='s3://sagemaker-s3-demo-test/samples/input.jsonl',\n    content_type='application/json',    \n    split_type='Line'\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Splits Using Glob Patterns in YAML\nDESCRIPTION: This YAML snippet uses glob patterns to define the data files for the 'train' and 'test' splits. It automatically lists all CSV files in the 'data' and 'holdout' directories, respectively. The `path` field uses the `*` wildcard to match all files with the '.csv' extension in the specified directories.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-manual-configuration.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: \"data/*.csv\"\n  - split: test\n    path: \"holdout/*.csv\"\n---\n```\n\n----------------------------------------\n\nTITLE: Packaging and Pushing spaCy Model to Hugging Face Hub\nDESCRIPTION: This set of commands packages a spaCy model, navigates to the output directory, and pushes the packaged model to the Hugging Face Hub.  It first logs into the Hugging Face CLI, then packages the model using `spacy package`, changes the current directory to the `dist` folder where the `.whl` file is located, and finally pushes the `.whl` file to the Hub using the `spacy huggingface-hub push` command.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spacy.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Loading Repo Card Metadata with huggingface_hub\nDESCRIPTION: This Python function loads the metadata of a repository (either a dataset or a model) from the Hugging Face Hub using the `huggingface_hub` library. It handles exceptions in case the repository card is not found. The function returns a Python dictionary containing the metadata.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import DatasetCard, ModelCard\nfrom huggingface_hub.utils import EntryNotFoundError \n\ndef load_repo_card_metadata(repo_type, repo_name):\n    if repo_type == \"dataset\":\n        try:\n            return DatasetCard.load(repo_name).data.to_dict()\n        except EntryNotFoundError:\n            return {}\n    if repo_type == \"model\":\n        try:\n            return ModelCard.load(repo_name).data.to_dict()\n        except EntryNotFoundError:\n            return {}\n```\n\n----------------------------------------\n\nTITLE: Run llama.cpp server\nDESCRIPTION: This command starts the `llama-server` which serves an OpenAI-compatible chat completions endpoint using a specified GGUF model from the Hugging Face Hub. The `-hf` flag specifies the repository and filename of the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/gguf-llamacpp.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nllama-server -hf bartowski/Llama-3.2-3B-Instruct-GGUF:Q8_0\n```\n\n----------------------------------------\n\nTITLE: CircleCI Workflow Configuration\nDESCRIPTION: This YAML configuration defines a CircleCI workflow that synchronizes a GitHub repository's `main` branch with a Hugging Face Space. It specifies the Docker image to use, installs necessary dependencies (git), checks out the repository, and pushes the code to the Hugging Face Space using an API token. Requires setting up a CircleCI context named `HuggingFace` and adding an environment variable `HF_PERSONAL_TOKEN` containing your Hugging Face API token.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-circleci.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nversion: 2.1\n\nworkflows:\n  main:\n    jobs:\n      - sync-to-huggingface:\n          context:\n            - HuggingFace\n          filters:\n            branches:\n              only:\n                - main\n\njobs:\n  sync-to-huggingface:\n    docker:\n      - image: alpine\n    resource_class: small\n    steps:\n      - run: \n          name: install git\n          command: apk update && apk add openssh-client git\n      - checkout\n      - run:\n          name: push to Huggingface hub\n          command: |\n                  git config user.email \"<your-email@here>\" \n                  git config user.name \"<your-identifier>\" \n                  git push -f https://HF_USERNAME:${HF_PERSONAL_TOKEN}@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\n```\n\n----------------------------------------\n\nTITLE: Uploading models with enjoy.py\nDESCRIPTION: This command uploads a Sample Factory model to the Hugging Face Hub using the `enjoy` script with the `--push_to_hub` flag. It generates evaluation metrics and a replay video. Several command line arguments are used to specify configuration such as `--hf_repository`, `--max_num_episodes`, `--max_num_frames`, `--no_render`, `--save_video`, `--video_frames` and `--video_name`\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/sample-factory.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\npython -m sf_examples.mujoco_examples.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir --max_num_episodes=10 --push_to_hub --hf_username=<username> --hf_repository=<hf_repo_name> --save_video --no_render\n```\n\n----------------------------------------\n\nTITLE: Text Classification Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Text Classification using the `j-hartmann/emotion-english-distilroberta-base` model on HF Inference. This specifies the pipeline and the relevant model ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_8\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-classification\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"j-hartmann/emotion-english-distilroberta-base\",\"providerModelId\":\"j-hartmann/emotion-english-distilroberta-base\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Joining Pokemon Datasets on Name\nDESCRIPTION: This snippet demonstrates how to join two datasets (`TheFusion21/PokemonCards` and `wanghaofan/pokemon-wiki-captions`) based on the `name` column using DuckDB. It selects the image_url, card_caption, name, hp from the first dataset and wiki_caption from the second dataset, limiting the output to 3 rows. The join is performed on the lowercase version of the `name` column from the first dataset and the `name_en` column from the second dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-combine-and-export.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nSELECT a.image_url\n        , a.caption AS card_caption\n        , a.name\n        , a.hp\n        , b.text_en as wiki_caption \nFROM 'hf://datasets/TheFusion21/PokemonCards/train.csv' a \nJOIN 'hf://datasets/wanghaofan/pokemon-wiki-captions/data/*.parquet' b \nON LOWER(a.name) = b.name_en\nLIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Specifying Datasets in YAML\nDESCRIPTION: This YAML snippet shows how to specify the datasets used to train a model in the model card metadata. The datasets are specified by their Hub identifiers.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ndatasets:\n- imdb\n- HuggingFaceH4/no_robots\n```\n\n----------------------------------------\n\nTITLE: Listing FiftyOne Datasets on Hub\nDESCRIPTION: This code snippet demonstrates how to list all available FiftyOne datasets on the Hugging Face Hub using the `HfApi` class from the `huggingface_hub` library. It filters datasets by the 'fiftyone' tag.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-fiftyone.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\napi = HfApi()\napi.list_datasets(tags=\"fiftyone\")\n```\n\n----------------------------------------\n\nTITLE: Uploading Audio Folder to Hugging Face Hub\nDESCRIPTION: This snippet demonstrates how to upload a folder containing audio files and metadata to the Hugging Face Hub using the `huggingface_hub` library. It initializes the `HfApi` class and calls the `upload_folder` method, specifying the `folder_path`, `repo_id`, and `repo_type`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\napi = HfApi()\n\napi.upload_folder(\n    folder_path=folder_path,\n    repo_id=\"username/my_audio_dataset\",\n    repo_type=\"dataset\",\n)\n```\n\n----------------------------------------\n\nTITLE: Read and Display Parquet Data from Hugging Face using PySpark\nDESCRIPTION: This code snippet demonstrates how to read Parquet data from a gated Hugging Face dataset (BAAI/Infinity-Instruct) using the `read_parquet` function and display the DataFrame. It assumes you have already logged in to Hugging Face using `huggingface-cli login` to access the gated repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from pyspark.sql import SparkSession\n>>> spark = SparkSession.builder.appName(\"demo\").getOrCreate()\n>>> df = read_parquet(\"hf://datasets/BAAI/Infinity-Instruct/7M/*.parquet\")\n>>> df.show()\n+---+----------------------------+-----+----------+--------------------+\n| id|               conversations|label|langdetect|              source|\n+---+----------------------------+-----+----------+--------------------+\n|  0|        [{human, def exti...|     |        en|      code_exercises|\n|  1|        [{human, See the ...|     |        en|                flan|\n|  2|        [{human, This is ...|     |        en|                flan|\n|  3|        [{human, If you d...|     |        en|                flan|\n|  4|        [{human, In a Uni...|     |        en|                flan|\n|  5|        [{human, Read the...|     |        en|                flan|\n|  6|        [{human, You are ...|     |        en|          code_bagel|\n|  7|        [{human, I want y...|     |        en|          Subjective|\n|  8|        [{human, Given th...|     |        en|                flan|\n|  9|[{human, ...|     |     zh-cn|          Subjective|\n| 10|        [{human, Provide ...|     |        en|self-oss-instruct...|\n| 11|        [{human, The univ...|     |        en|                flan|\n| 12|        [{human, Q: I am ...|     |        en|                flan|\n| 13|        [{human, What is ...|     |        en|      OpenHermes-2.5|\n| 14|        [{human, In react...|     |        en|                flan|\n| 15|        [{human, Write Py...|     |        en|      code_exercises|\n| 16|        [{human, Find the...|     |        en|            MetaMath|\n| 17|        [{human, Three of...|     |        en|            MetaMath|\n| 18|        [{human, Chandra ...|     |        en|            MetaMath|\n| 19|[{human, ...|     |     zh-cn|          Subjective|\n+---+----------------------------+-----+----------+--------------------+\n```\n\n----------------------------------------\n\nTITLE: Iterating on Audio Paths in DataFrame\nDESCRIPTION: This snippet shows how to load a metadata CSV file containing audio paths and iterate over those paths. It imports the pandas library, reads the CSV, and constructs the full audio path for each entry.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfolder_path = \"path/to/folder/\"\ndf = pd.read_csv(folder_path + \"metadata.csv\")\nfor audio_path in (folder_path + df[\"file_name\"]):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Add Carbon Footprint Metadata to Model Card\nDESCRIPTION: This YAML snippet defines the structure for adding carbon footprint metadata to a model card's README.md file. It includes fields for emissions (in grams of CO2), the data source, training type (pre-training or fine-tuning), geographical location, and hardware used. This information provides context for understanding the environmental impact of the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards-co2.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nco2_eq_emissions:\n  emissions: number (in grams of CO2)\n  source: \"source of the information, either directly from AutoTrain, code carbon or from a scientific article documenting the model\"\n  training_type: \"pre-training or fine-tuning\"\n  geographical_location: \"as granular as possible, for instance Quebec, Canada or Brooklyn, NY, USA. To check your compute's electricity grid, you can check out https://app.electricitymap.org.\"\n  hardware_used: \"how much compute and what kind, e.g. 8 v100 GPUs\"\n---\n\n```\n\n----------------------------------------\n\nTITLE: Pushing FiftyOne Datasets with Options\nDESCRIPTION: This code snippet shows how to push a FiftyOne dataset to the Hugging Face Hub with additional options. It loads a video dataset from the FiftyOne Dataset Zoo and then pushes it to a private repository with specified tags, license, description, and a preview image path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-fiftyone.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndataset = foz.load_from_zoo(\"quickstart-video\", max_samples=3)\n\npush_to_hub(\n    dataset,\n    \"my-quickstart-video-dataset\",\n    tags=[\"video\", \"tracking\"],\n    license=\"mit\",\n    description=\"A dataset of video samples for tracking tasks\",\n    private=True,\n    preview_path=\"<path/to/preview.png>\"\n)\n```\n\n----------------------------------------\n\nTITLE: Running Specific GGUF Models with Ollama\nDESCRIPTION: These snippets provide examples of running specific GGUF models from the Hugging Face Hub using Ollama.  They illustrate how to specify different models. Ollama must be installed and configured to use these commands.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/ollama.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n```\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF\n```\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/arcee-ai/SuperNova-Medius-GGUF\n```\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/bartowski/Humanish-LLama3-8B-Instruct-GGUF\n```\n\n----------------------------------------\n\nTITLE: Setting Label Studio Secrets - Disable Signups\nDESCRIPTION: These settings configure account creation restrictions in Label Studio by disabling unrestricted account creation and setting the initial username and password. These are configured as secrets in the Space settings for security and require a restart of the Space to take effect.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-label-studio.md#_snippet_0\n\nLANGUAGE: N/A\nCODE:\n```\n`LABEL_STUDIO_DISABLE_SIGNUP_WITHOUT_LINK`: `true`\n`LABEL_STUDIO_USERNAME`: `<YOUR_EMAIL_ADDRESS>`\n`LABEL_STUDIO_PASSWORD`: `<YOUR_PASSWORD>`\n```\n\n----------------------------------------\n\nTITLE: Force Pushing to Hugging Face Space\nDESCRIPTION: This command force pushes your local `main` branch to the Hugging Face Space remote. This is typically done the first time to synchronize the entire repository. Use with caution, as it overwrites the remote repository's history.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-circleci.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit push --force space main\n```\n\n----------------------------------------\n\nTITLE: Downloading RL-Baselines3-Zoo model using load_from_hub\nDESCRIPTION: This snippet demonstrates how to download a pre-trained RL-Baselines3-Zoo model from the Hugging Face Hub using the `rl_zoo3.load_from_hub` command. It downloads a DQN model trained on the SpaceInvadersNoFrameskip-v4 environment and saves it to the specified logs folder.  The subsequent command shows how to run/enjoy the downloaded model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/rl-baselines3-zoo.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Download ppo SpaceInvadersNoFrameskip-v4 model and save it into the logs/ folder\npython -m rl_zoo3.load_from_hub --algo dqn --env SpaceInvadersNoFrameskip-v4 -f logs/ -orga sb3\npython enjoy.py --algo dqn --env SpaceInvadersNoFrameskip-v4  -f logs/\n```\n\n----------------------------------------\n\nTITLE: Tracking PyTorch CNN training with Aim\nDESCRIPTION: This Python code snippet demonstrates how to track training metrics, weights, and gradients of a PyTorch CNN model using the Aim experiment tracker. It imports necessary functions from the `aim` and `aim.pytorch` libraries, initializes a new Aim Run, tracks metrics like accuracy and loss, and records the distributions of model parameters and gradients. The tracked data is stored in the `.aim` folder.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-aim.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom aim import Run\nfrom aim.pytorch import track_gradients_dists, track_params_dists\n\n# Initialize a new Run\naim_run = Run()\n...\nitems = {'accuracy': acc, 'loss': loss}\naim_run.track(items, epoch=epoch, context={'subset': 'train'})\n\n# Track weights and gradients distributions\ntrack_params_dists(model, aim_run)\ntrack_gradients_dists(model, aim_run)\n```\n\n----------------------------------------\n\nTITLE: CSV with Relative Paths Example\nDESCRIPTION: The `metadata.csv` shows the 'file_name' column containing relative paths to the audio files in the 'audio' subdirectory.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_8\n\nLANGUAGE: csv\nCODE:\n```\nfile_name,animal\naudio/1.wav,cat\naudio/2.wav,cat\naudio/3.wav,dog\naudio/4.wav,dog\n```\n\n----------------------------------------\n\nTITLE: Sharing timm Model to Hugging Face Hub\nDESCRIPTION: This snippet demonstrates how to push a timm model to the Hugging Face Hub using the `push_to_hf_hub` method. It includes building or loading a model, optionally fine-tuning it, and then pushing it to the Hub. Make sure you are logged into Hugging Face before running this snippet.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/timm.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport timm\n\n# Build or load a model, e.g. timm's pretrained resnet18\nmodel = timm.create_model('resnet18', pretrained=True, num_classes=4)\n\n###########################\n# [Fine tune your model...]\n###########################\n\n# Push it to the  Hub\ntimm.models.hub.push_to_hf_hub(\n    model,\n    'resnet18-random-classifier',\n    model_config={'labels': ['a', 'b', 'c', 'd']}\n)\n\n# Load your model from the Hub\nmodel_reloaded = timm.create_model(\n    'hf-hub:<your-username>/resnet18-random-classifier',\n    pretrained=True\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Models Parameters in JavaScript\nDESCRIPTION: Defines the parameters that can be used when listing models via the Hugging Face Hub API. These parameters allow filtering, sorting, and limiting the results of the model listing API call. They correspond to the arguments available in the `huggingface_hub.list_models()` function.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nparams = {\n    \"search\":\"search\",\n    \"author\":\"author\",\n    \"filter\":\"filter\",\n    \"sort\":\"sort\",\n    \"direction\":\"direction\",\n    \"limit\":\"limit\",\n    \"full\":\"full\",\n    \"config\":\"config\"\n}\n```\n\n----------------------------------------\n\nTITLE: Cloning dataset repository using Git (Bash)\nDESCRIPTION: This snippet illustrates how to clone a dataset repository from the Hugging Face Hub using Git. It first installs Git LFS and then clones the specified dataset repository. The dataset ID needs to be provided in the git clone command. This method is appropriate for accessing the entire dataset repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-downloading.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ngit lfs install\ngit clone git@hf.co:datasets/<dataset ID> # example: git clone git@hf.co:datasets/allenai/c4\n```\n\n----------------------------------------\n\nTITLE: Registering a Model Mapping Item\nDESCRIPTION: This code snippet shows the structure for registering a model mapping item using the Model Mapping API. It defines the JSON payload for a POST request to `/api/partners/{provider}/models`.  The `task`, `hfModel`, and `providerModel` fields are required, representing the task type, Hugging Face model ID, and provider's model ID, respectively. The `status` field is optional and defaults to \"staging\".\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"task\": \"WidgetType\", // required\n    \"hfModel\": \"string\", // required: the name of the model on HF: namespace/model-name\n    \"providerModel\": \"string\", // required: the partner's \"model id\" i.e. id on your side\n    \"status\": \"live\" | \"staging\" // Optional: defaults to \"staging\". \"staging\" models are only available to members of the partner's org, then you switch them to \"live\" when they're ready to go live\n}\n```\n\n----------------------------------------\n\nTITLE: Read Dask DataFrame from Hugging Face Hub\nDESCRIPTION: This Python snippet demonstrates reading a Dask DataFrame from a Parquet file on the Hugging Face Hub using the `hf://` path. It shows how to read from a single directory and from separate directories for train/validation/test splits. Requires the `dask.dataframe` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\n\ndf = dd.read_parquet(\"hf://datasets/username/my_dataset\")\n\n# or read from separate directories if the dataset has train/validation/test splits\ndf_train = dd.read_parquet(\"hf://datasets/username/my_dataset/train\")\ndf_valid = dd.read_parquet(\"hf://datasets/username/my_dataset/validation\")\ndf_test  = dd.read_parquet(\"hf://datasets/username/my_dataset/test\")\n```\n\n----------------------------------------\n\nTITLE: Hugging Face URL with Parquet Conversion\nDESCRIPTION: This example demonstrates the URL format to access auto-converted Parquet files using the @~parquet branch. This is useful when querying datasets automatically converted to Parquet format.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb.md#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nhf://datasets/{my-username}/{my-dataset}@~parquet/{path_to_file}\n```\n\n----------------------------------------\n\nTITLE: Making a Chat Completion Request with Python (huggingface_hub)\nDESCRIPTION: This Python snippet uses the `huggingface_hub` library's `InferenceClient` to make a chat completion request to the Inference Providers API. It authenticates using the API key, specifies the provider and model, and sends a message to the model. The response is then printed to the console.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/index.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient(\n    provider=\"novita\",\n    api_key=\"hf_xxxxxxxxxxxxxxxxxxxxxxxx\",\n)\n\ncompletion = client.chat.completions.create(\n    model=\"deepseek-ai/DeepSeek-V3-0324\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How many 'G's in 'huggingface'?\"\n        }\n    ],\n)\n\nprint(completion.choices[0].message)\n```\n\n----------------------------------------\n\nTITLE: YAML for Linking Datasets on the Model Page\nDESCRIPTION: This YAML snippet shows how to link datasets in your README.md metadata to display those used directly from your model page, providing users with immediate access to the training data.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-release-checklist.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\n---\ndatasets:\n- username/dataset\n- username/dataset-2\n---\n```\n\n----------------------------------------\n\nTITLE: Creating Collection Payload (POST)\nDESCRIPTION: This payload is used when creating a new collection. It includes the title, namespace, description, an optional item (type and id), and whether the collection is private.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_15\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"title\": \"My cool models\",\n    \"namespace\": \"username_or_org\",\n    \"description\": \"Here is a shortlist of models I've trained.\",\n    \"item\" : {\n        \"type\": \"model\",\n        \"id\": \"username/cool-model\",\n    }\n    \"private\": false,\n\n}\n```\n\n----------------------------------------\n\nTITLE: Registering a Model Mapping Item with hfFilter\nDESCRIPTION: This code snippet extends the model mapping item registration with an optional `hfFilter` parameter.  The `hfFilter` allows registering a tag slice of Hugging Face models in one go. This example shows how to register all LoRAs for the Flux-dev model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"hfFilter\": [\"string\"]\n    // ^Power user move: register a \"tag\" slice of HF in one go.\n    // Example: tag == \"base_model:adapter:black-forest-labs/FLUX.1-dev\" for all Flux-dev LoRAs\n}\n```\n\n----------------------------------------\n\nTITLE: Specify Text-to-Image Widget Output YAML\nDESCRIPTION: This YAML configures the example output for a text-to-image widget, pointing to an image file within the model repository or a remote URL. The `url` property specifies the location of the generated image.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nwidget:\n  - text: \"picture of a futuristic tiger, artstation\"\n    output:\n      url: images/tiger.jpg\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration to Drop Labels\nDESCRIPTION: This code demonstrates how to disable the automatic addition of the 'label' column by setting `drop_labels: true` in the README header's YAML configuration.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nconfigs:\n  - config_name: default  # Name of the dataset subset, if applicable.\n    drop_labels: true\n```\n\n----------------------------------------\n\nTITLE: Pickle Serialization and Disassembly in Python\nDESCRIPTION: This snippet demonstrates how to serialize a Python object using the `pickle` module and then disassemble the resulting pickle file to inspect its opcodes using `pickletools`. This allows understanding the structure of the pickle data without executing it.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\nimport pickletools\n\nvar = \"data I want to share with a friend\"\n\n# store the pickle data in a file named 'payload.pkl'\nwith open('payload.pkl', 'wb') as f:\n    pickle.dump(var, f)\n\n# disassemble the pickle\n# and print the instructions to the command line\nwith open('payload.pkl', 'rb') as f:\n    pickletools.dis(f)\n```\n\n----------------------------------------\n\nTITLE: Dockerfile Copy with --chown Flag\nDESCRIPTION: This Dockerfile snippet shows how to use the `--chown` flag with the `COPY` command to ensure the copied files are owned by the specified user, avoiding permission issues.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker.md#_snippet_6\n\nLANGUAGE: Dockerfile\nCODE:\n```\nCOPY --chown=user checkpoint .\n```\n\n----------------------------------------\n\nTITLE: Text Classification Request Example (JSON)\nDESCRIPTION: This JSON snippet demonstrates a request to the text-classification task of the Inference Toolkit API. The input is a string representing a review of a soundtrack.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": \"This sound track was beautiful! It paints the senery in your mind so well I would recomend it\n  even to people who hate vid. game music!\"\n}\n```\n\n----------------------------------------\n\nTITLE: Making a Chat Completion Request with JavaScript (fetch)\nDESCRIPTION: This JavaScript snippet utilizes the `fetch` API to send a POST request to the Inference Providers API for chat completion. It includes headers for authorization and content type, along with a JSON body containing the provider, model, and message details. The API key is passed via the Authorization header and the response from the API is logged to the console.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/index.md#_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport fetch from \"node-fetch\";\n\nconst response = await fetch(\n    \"https://router.huggingface.co/novita/v3/openai/chat/completions\",\n    {\n        method: \"POST\",\n        headers: {\n            Authorization: `Bearer hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx`,\n            \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify({\n            provider: \"novita\",\n            model: \"deepseek-ai/DeepSeek-V3-0324\",\n            messages: [\n                {\n                    role: \"user\",\n                    content: \"How many 'G's in 'huggingface'?\",\n                },\n            ],\n        }),\n    }\n);\nconsole.log(await response.json());\n```\n\n----------------------------------------\n\nTITLE: Pull Argilla Dataset from Hugging Face Hub\nDESCRIPTION: This code snippet demonstrates how to pull an Argilla dataset from the Hugging Face Hub. It imports the `argilla` library, initializes the Argilla client, and retrieves a dataset from the specified repository ID on the Hub. Both the dataset configuration and records are loaded.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-argilla.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport argilla as rg\n\nclient = rg.Argilla(api_url=\"<api_url>\", api_key=\"<api_key>\")\ndataset = rg.Dataset.from_hub(repo_id=\"<repo_id>\")\n```\n\n----------------------------------------\n\nTITLE: Pushing FiftyOne Datasets to Hub\nDESCRIPTION: This code demonstrates how to push a FiftyOne dataset to the Hugging Face Hub using the `push_to_hub()` function. It first loads an example dataset from the FiftyOne Dataset Zoo and then pushes it to the Hub with a specified repo name.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-fiftyone.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport fiftyone as fo\nimport fiftyone.zoo as foz\nfrom fiftyone.utils.huggingface import push_to_hub\n\n## load example dataset\ndataset = foz.load_zoo_dataset(\"quickstart\")\n\n## push to hub\npush_to_hub(dataset, \"my-hf-dataset\")\n```\n\n----------------------------------------\n\nTITLE: Schedule AutoTrain Retraining (Python)\nDESCRIPTION: This Python function, `schedule_retrain`, is responsible for creating an AutoTrain project, adding data to it, and starting the processing.  It also includes error handling to catch HTTP errors during the AutoTrain API calls and raises them. It calls AutoTrain API and `notify_success` function.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-auto-retrain.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef schedule_retrain(payload: WebhookPayload):\n\t# Create the autotrain project\n\ttry:\n\t\tproject = AutoTrain.create_project(payload)\n\t\tAutoTrain.add_data(project_id=project[\"id\"])\n\t\tAutoTrain.start_processing(project_id=project[\"id\"])\n\texcept requests.HTTPError as err:\n\t\tprint(\"ERROR while requesting AutoTrain API:\")\n\t\tprint(f\"  code: {err.response.status_code}\")\n\t\tprint(f\"  {err.response.json()}\")\n\t\traise\n\t# Notify in the community tab\n\tnotify_success(project[\"id\"])\n```\n\n----------------------------------------\n\nTITLE: Counting JSONL files with a glob pattern\nDESCRIPTION: This snippet demonstrates counting the number of JSONL files matching a glob pattern in DuckDB. It uses the `COUNT(*)` function to count all files under the specified Hugging Face dataset path that match the `*.jsonl` pattern. It requires the Hugging Face dataset at the specified path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-select.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nSELECT COUNT(*) FROM 'hf://datasets/jamescalam/world-cities-geo/*.jsonl';\n```\n\n----------------------------------------\n\nTITLE: Deleting Repository Payload in JavaScript\nDESCRIPTION: Defines the payload structure required to delete a repository using the Hugging Face Hub API. The payload includes the repository type, name, and organization. This corresponds to parameters used in `huggingface_hub.delete_repo()`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"type\": \"model\",\n    \"name\": \"name\",\n    \"organization\": \"organization\",\n}\n```\n\n----------------------------------------\n\nTITLE: Uploading ML-Agents model to Hugging Face Hub\nDESCRIPTION: This command uploads an ML-Agents model to the Hugging Face Hub using the `mlagents-push-to-hf` tool. It requires specifying the training run ID, the local directory where the model is saved, the repository ID (your Hugging Face username/the repo name), and a commit message.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/ml-agents.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmlagents-push-to-hf --run-id=\"First Training\" --local-dir=\"results/First Training\" --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --commit-message=\"Pyramids\"\n```\n\n----------------------------------------\n\nTITLE: Adding Task Type to PIPELINE_DATA in huggingface.js\nDESCRIPTION: This snippet describes adding the task type to the PIPELINE_DATA constant within the huggingface.js library. This involves modifying the pipelines.ts file within the tasks package of the huggingface.js library to include the new task type. The pipeline types are categorized for organization.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-tasks.md#_snippet_2\n\n\n\n----------------------------------------\n\nTITLE: Dockerfile Directory Permissions with chmod\nDESCRIPTION: This Dockerfile snippet demonstrates how to change permissions on a directory using `chmod`. This is used to grant read, write, and execute permissions to all users for the `/data` directory.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker.md#_snippet_5\n\nLANGUAGE: Dockerfile\nCODE:\n```\nRUN mkdir -p /data\nRUN chmod 777 /data\n```\n\n----------------------------------------\n\nTITLE: Configure Audio Widget Input YAML\nDESCRIPTION: This YAML snippet demonstrates how to specify audio file inputs for a widget in the model card metadata.  It defines the input as an audio source using the `src` attribute, along with an optional `example_title`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nwidget:\n- src: https://example.org/somewhere/speech_samples/sample1.flac\n  example_title: Speech sample 1\n- src: https://example.org/somewhere/speech_samples/sample2.flac\n  example_title: Speech sample 2\n```\n\n----------------------------------------\n\nTITLE: Custom Inference Module with model_fn and transform_fn\nDESCRIPTION: This Python code demonstrates a custom inference module that overrides the default `model_fn` and `transform_fn` methods. It imports `decoder_encoder` for data conversion and includes placeholder logic for loading the model, decoding input, predicting, and encoding output within the `transform_fn`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker_huggingface_inference_toolkit import decoder_encoder\n\ndef model_fn(model_dir):\n    # implement custom code to load the model\n    loaded_model = ...\n    \n    return loaded_model \n\ndef transform_fn(model, input_data, content_type, accept):\n     # decode the input data (e.g. JSON string -> dict)\n    data = decoder_encoder.decode(input_data, content_type)\n\n    # call your custom model with the data\n    outputs = model(data , ... ) \n\n    # convert the model output to the desired output format (e.g. dict -> JSON string)\n    response = decoder_encoder.encode(output, accept)\n\n    return response\n```\n\n----------------------------------------\n\nTITLE: Persisting Changes with Git in Spaces Dev Mode (Shell)\nDESCRIPTION: This shell script demonstrates how to persist changes made in Spaces Dev Mode. It adds all changes, commits them with a message, and pushes the commit to the repository. This ensures the changes are saved even after Dev Mode is disabled or the Space goes to sleep. Requires git to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-dev-mode.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Add changes and commit them\ngit add .\ngit commit -m \"Persist changes from Dev Mode\"\n\n# Push the commit to persist them in the repo\ngit push\n```\n\n----------------------------------------\n\nTITLE: Example LLM Text Response (Python)\nDESCRIPTION: This snippet is an example of the output from the TGI endpoint once deployed. It shows the generated text from the LLM model.  Note that this is a string within a list.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n[{'generated_text': 'The diamondback terrapin was the first reptile to make the list, followed by the American alligator, the American crocodile, and the American box turtle. The polecat, a ferret-like animal, and the skunk rounded out the list, both having gained their slots because they have proven to be particularly dangerous to humans.\\n\\nCalifornians also seemed to appreciate the new list, judging by the comments left after the election.\\n\\nThis is fantastic, one commenter declared.\\n\\nCalifornia is a very'}]\n```\n\n----------------------------------------\n\nTITLE: Installing PEFT\nDESCRIPTION: This command installs the PEFT library using pip. It is a prerequisite for using PEFT models. Make sure you have Python and pip installed before running this command.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/peft.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ pip install peft\n```\n\n----------------------------------------\n\nTITLE: List Models Served by Any Provider (cURL)\nDESCRIPTION: This snippet shows how to list models served by at least one inference provider, using 'all' as the `inference_provider` parameter. cURL is used to make the API request, and `jq` is used to parse the JSON response and extract the model IDs.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\n# List text-to-video models served by any provider\n~ curl -s https://huggingface.co/api/models?inference_provider=all&pipeline_tag=text-to-video | jq \".[].id\"\n\"Wan-AI/Wan2.1-T2V-14B\"\n\"Lightricks/LTX-Video\"\n\"tencent/HunyuanVideo\"\n\"Wan-AI/Wan2.1-T2V-1.3B\"\n\"THUDM/CogVideoX-5b\"\n\"genmo/mochi-1-preview\"\n\"BagOu22/Lora_HKLPAZ\"\n```\n\n----------------------------------------\n\nTITLE: Switching User in Dockerfile for Permissions\nDESCRIPTION: This Dockerfile snippet switches to the \"user\" user and sets the home directory and PATH environment variables. This is used to grant the correct permissions to the application and ensure that `transformers` can download and cache the models correctly.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_12\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Switch to the \"user\" user\nUSER user\n\n# Set home to the user's home directory\nENV HOME=/home/user \\\n\tPATH=/home/user/.local/bin:$PATH\n```\n\n----------------------------------------\n\nTITLE: Setting HF_TOKEN Environment Variable (Bash)\nDESCRIPTION: This snippet demonstrates how to set the `HF_TOKEN` environment variable with your Hugging Face access token. Polars will automatically use this token when requesting datasets from Hugging Face. Replace `hf_xxxxxxxxxxxxx` with your actual token.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-auth.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_TOKEN=\"hf_xxxxxxxxxxxxx\"\n```\n\n----------------------------------------\n\nTITLE: Structured Data Classification Widget YAML Configuration\nDESCRIPTION: This snippet configures a structured data classification widget. It includes the 'structured_data' field, containing data for various features (e.g., fixed_acidity, volatile_acidity) as lists, and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_26\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- structured_data:\n    fixed_acidity:\n      - 7.4\n      - 7.8\n      - 10.3\n    volatile_acidity:\n      - 0.7\n      - 0.88\n      - 0.32\n    citric_acid:\n      - 0\n      - 0\n      - 0.45\n    residual_sugar:\n      - 1.9\n      - 2.6\n      - 6.4\n    chlorides:\n      - 0.076\n      - 0.098\n      - 0.073\n    free_sulfur_dioxide:\n      - 11\n      - 25\n      - 5\n    total_sulfur_dioxide:\n      - 34\n      - 67\n      - 13\n    density:\n      - 0.9978\n      - 0.9968\n      - 0.9976\n    pH:\n      - 3.51\n      - 3.2\n      - 3.23\n    sulphates:\n      - 0.56\n      - 0.68\n      - 0.82\n    alcohol:\n      - 9.4\n      - 9.8\n      - 12.6\n  example_title: \"Wine\"\n```\n\n----------------------------------------\n\nTITLE: Listing available mlx-image models in Python\nDESCRIPTION: This snippet demonstrates how to list all available models in the mlx-image library using the list_models function. It requires the mlxim library to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx-image.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom mlxim.model import list_models\nlist_models()\n```\n\n----------------------------------------\n\nTITLE: Signing a Git Commit (Bash)\nDESCRIPTION: This snippet demonstrates how to sign a Git commit using the `-S` flag. It adds a signature to the commit using the configured GPG key. Once pushed to the Hub, a verified badge will be displayed for the commit, confirming its authenticity.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-gpg.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -S -m \"My first signed commit\"\n```\n\n----------------------------------------\n\nTITLE: Define Model Card Metadata\nDESCRIPTION: This YAML snippet defines the structure for model card metadata, including languages, license information, used datasets, evaluation metrics, the base model, and structured evaluation results using the 'model-index' key. It shows how to specify tasks, datasets, metrics, and their corresponding values and configurations.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/modelcard.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlanguage:\n- {lang_0}  # Example: fr\n- {lang_1}  # Example: en\nlicense: {license}  # Example: apache-2.0 or any license from https://hf.co/docs/hub/repositories-licenses\nlicense_name: {license_name}  # If license = other (license not in https://hf.co/docs/hub/repositories-licenses), specify an id for it here, like `my-license-1.0`.\nlicense_link: {license_link}  # If license = other, specify \"LICENSE\" or \"LICENSE.md\" to link to a file of that name inside the repo, or a URL to a remote file.\nlibrary_name: {library_name}  # Optional. Example: keras or any library from https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries.ts\ntags:\n- {tag_0}  # Example: audio\n- {tag_1}  # Example: automatic-speech-recognition\n- {tag_2}  # Example: speech\n- {tag_3}  # Example to specify a library: allennlp\ndatasets:\n- {dataset_0}  # Example: common_voice. Use dataset id from https://hf.co/datasets\nmetrics:\n- {metric_0}  # Example: wer. Use metric id from https://hf.co/metrics\nbase_model: {base_model}  # Example: stabilityai/stable-diffusion-xl-base-1.0. Can also be a list (for merges)\n\n# Optional. Add this if you want to encode your eval results in a structured way.\nmodel-index:\n- name: {model_id}\n  results:\n  - task:\n      type: {task_type}             # Required. Example: automatic-speech-recognition\n      name: {task_name}             # Optional. Example: Speech Recognition\n    dataset:\n      type: {dataset_type}          # Required. Example: common_voice. Use dataset id from https://hf.co/datasets\n      name: {dataset_name}          # Required. A pretty name for the dataset. Example: Common Voice (French)\n      config: {dataset_config}      # Optional. The name of the dataset subset used in `load_dataset()` Example: fr in `load_dataset(\"common_voice\", \"fr\")`. See the `datasets` docs for more info: https://huggingface.co/docs/datasets/package_reference/loading_methods#datasets.load_dataset.name\n      split: {dataset_split}        # Optional. Example: test\n      revision: {dataset_revision}  # Optional. Example: 5503434ddd753f426f4b38109466949a1217c2bb\n      args:\n        {arg_0}: {value_0}          # Optional. Additional arguments to `load_dataset()` Example for wikipedia: language: en\n        {arg_1}: {value_1}          # Optional. Example for wikipedia: date: 20220301\n    metrics:\n      - type: {metric_type}         # Required. Example: wer. Use metric id from https://hf.co/metrics\n        value: {metric_value}       # Required. Example: 20.90\n        name: {metric_name}         # Optional. Example: Test WER\n        config: {metric_config}     # Optional. The name of the metric configuration used in `load_metric()` Example: bleurt-large-512 in `load_metric(\"bleurt\", \"bleurt-large-512\")`. See the `datasets` docs for more info: https://huggingface.co/docs/datasets/v2.1.0/en/loading#load-configurations\n        args:\n          {arg_0}: {value_0}        # Optional. The arguments passed during `Metric.compute()`. Example for `bleu`: max_order: 4\n        verifyToken: {verify_token} # Optional. If present, this is a signature that can be used to prove that evaluation was generated by Hugging Face (vs. self-reported).\n    source:                         # Optional. The source for this result.\n      name: {source_name}           # Optional. The name of the source. Example: Open LLM Leaderboard.\n      url: {source_url}             # Required if source is provided. A link to the source. Example: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard.\n---\n\n```\n\n----------------------------------------\n\nTITLE: Cloning a Hugging Face Dataset Repository\nDESCRIPTION: This command clones a Hugging Face dataset repository to your local machine using the git CLI. It downloads all files and the complete history of the repository. The `cd` command navigates into the newly cloned directory.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-getting-started.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://huggingface.co/datasets/<your-username>/<your-dataset-name>\ncd <your-dataset-name>\n```\n\n----------------------------------------\n\nTITLE: Initializing Stanza Pipeline in Python\nDESCRIPTION: This code snippet demonstrates how to download a pre-trained Stanza model (English in this case) from the Hugging Face Hub and initialize a Stanza pipeline. It then runs annotation on a sample sentence. The `stanza` library is required. The input is a language code. The output is a processed document.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/stanza.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport stanza\n\nnlp = stanza.Pipeline('en') # download th English model and initialize an English neural pipeline\ndoc = nlp(\"Barack Obama was born in Hawaii.\") # run annotation over a sentence\n```\n\n----------------------------------------\n\nTITLE: Verifying JAX CUDA Devices\nDESCRIPTION: This code snippet checks if JAX is using the CUDA device and prints the device type. It verifies that JAX is utilizing the GPU after installation and configuration. It depends on the JAX library and proper CUDA setup.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-gpus.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport jax\n\nprint(f\"JAX devices: {jax.devices()}\")\n# JAX devices: [StreamExecutorGpuDevice(id=0, process_index=0)]\nprint(f\"JAX device type: {jax.devices()[0].device_kind}\")\n# JAX device type: Tesla T4\n```\n\n----------------------------------------\n\nTITLE: Reading JSON Lines File with Polars\nDESCRIPTION: Reads a JSON Lines (newline delimited JSON) file from Hugging Face using Polars. The `read_ndjson` function is used to load the file. It assumes the `pl` alias is defined for the `polars` library. This snippet demonstrates reading the file with default configurations.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-file-formats.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npl.read_ndjson(\"hf://datasets/proj-persona/PersonaHub/persona.jsonl\")\n```\n\n----------------------------------------\n\nTITLE: Loading Scikit-learn model with huggingface_hub\nDESCRIPTION: Downloads and loads a Scikit-learn model from the Hugging Face Hub using the `huggingface_hub` library and `joblib`. The snippet downloads a specified file from the hub, then loads it as a Scikit-learn model. Requires `huggingface_hub` and `joblib` packages.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-downloading.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import hf_hub_download\nimport joblib\n\nREPO_ID = \"YOUR_REPO_ID\"\nFILENAME = \"sklearn_model.joblib\"\n\nmodel = joblib.load(\n    hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n)\n```\n\n----------------------------------------\n\nTITLE: Pushing spaCy model to Hugging Face Hub via Python\nDESCRIPTION: This Python code snippet demonstrates how to push a spaCy model to the Hugging Face Hub using the `push` function from the `spacy_huggingface_hub` library.  It requires the `spacy_huggingface_hub` library to be installed. The output is a dictionary containing the URL of the published model and the URL of the wheel file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spacy.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom spacy_huggingface_hub import push\n\nresult = push(\"./en_ner_fashion-0.0.0-py3-none-any.whl\")\nprint(result[\"url\"])\n```\n\n----------------------------------------\n\nTITLE: Enable Large Files for Hugging Face CLI\nDESCRIPTION: This command enables the handling of very large files (over 5GB) using the Hugging Face CLI. It is necessary for repositories containing very large files.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-getting-started.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli lfs-enable-largefiles .\n```\n\n----------------------------------------\n\nTITLE: Post a reply to the discussion thread using the Hub API - TypeScript\nDESCRIPTION: This code snippet constructs the URL for the comment API based on the incoming event's discussion URL. It then sends a POST request to this URL, including the `HF_TOKEN` in the Authorization header and the generated `continuationText` as the comment content in the request body. This posts the bot's reply to the discussion.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-discussion-bot.md#_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\n\tconst commentUrl = req.body.discussion.url.api + \"/comment\";\n\n\tconst commentApiResponse = await fetch(commentUrl, {\n\t\tmethod: \"POST\",\n\t\theaders: {\n\t\t\tAuthorization: `Bearer ${process.env.HF_TOKEN}`,\n\t\t\t\"Content-Type\": \"application/json\",\n\t\t},\n\t\tbody: JSON.stringify({ comment: continuationText }),\n\t});\n\n\tconst apiOutput = await commentApiResponse.json();\n```\n\n----------------------------------------\n\nTITLE: Defining Dataset Splits in YAML\nDESCRIPTION: This YAML snippet defines a dataset configuration with two splits, 'train' and 'test', specifying the paths to the corresponding CSV files. The `config_name` field is required even for a single subset. The `data_files` list contains the paths to files associated with each split.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-manual-configuration.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: \"data.csv\"\n  - split: test\n    path: \"holdout.csv\"\n---\n```\n\n----------------------------------------\n\nTITLE: info.json Example with Sample URL and Models\nDESCRIPTION: This JSON file specifies the external sample URL and Sentis models associated with the project. It's used when the code sample is hosted externally.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/unity-sentis.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"sampleURL\": [ \"http://sampleunityproject\"],\n   \"models\": [ \"model1.sentis\", \"model2.sentis\"]\n}\n```\n\n----------------------------------------\n\nTITLE: HTML Example for Hugging Face Course Organization Card\nDESCRIPTION: This HTML snippet demonstrates how to create a basic organization card for the Hugging Face Course organization.  It includes a paragraph with a link to the Hugging Face course. The card is implemented as a `README.md` file in a Space repository named `README`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/organizations-cards.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<p>\nThis is the organization grouping all the models and datasets used in the <a href=\"https://huggingface.co/course/chapter1\" class=\"underline\">Hugging Face course</a>.\n</p>\n```\n\n----------------------------------------\n\nTITLE: Sentiment Analysis Pipeline in Python\nDESCRIPTION: This Python snippet demonstrates how to use the `transformers` library to perform sentiment analysis using a pipeline. It initializes a pipeline for sentiment analysis and then runs it on the input text 'I love transformers!'. It outputs a list containing the label and the score.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/transformers-js.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\npipe = pipeline('sentiment-analysis')\n\nout = pipe('I love transformers!')\n# [{'label': 'POSITIVE', 'score': 0.999806941}]\n```\n\n----------------------------------------\n\nTITLE: Loading PaddleNLP models using AutoModel and AutoTokenizer\nDESCRIPTION: This snippet demonstrates loading a pre-trained PaddleNLP model from the Hugging Face Hub using the AutoModel and AutoTokenizer classes. This approach offers more control over the model and tokenizer. It initializes a tokenizer and a masked language model using the 'PaddlePaddle/ernie-1.0-base-zh' model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/paddlenlp.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# If you want more control, you will need to define the tokenizer and model.\nfrom paddlenlp.transformers import AutoTokenizer, AutoModelForMaskedLM\ntokenizer = AutoTokenizer.from_pretrained(\"PaddlePaddle/ernie-1.0-base-zh\", from_hf_hub=True)\nmodel = AutoModelForMaskedLM.from_pretrained(\"PaddlePaddle/ernie-1.0-base-zh\", from_hf_hub=True)\n```\n\n----------------------------------------\n\nTITLE: Automatic Speech Recognition Inference Snippet\nDESCRIPTION: This snippet configures the Inference API to use SambaNova for Automatic Speech Recognition. It specifies the 'openai/whisper-large-v3' model on the Hugging Face Hub and maps it to 'Whisper-Large-v3' on the SambaNova provider. The pipeline is set to 'automatic-speech-recognition'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/sambanova.md#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=automatic-speech-recognition\n    providersMapping={ {\"sambanova\":{\"modelId\":\"openai/whisper-large-v3\",\"providerModelId\":\"Whisper-Large-v3\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Setting Organization Billing in Python\nDESCRIPTION: This Python snippet illustrates how to specify the organization to be billed when using the InferenceClient. It imports the InferenceClient from the huggingface_hub package, instantiates the client with the specified provider and organization name, and then uses the client to generate an image from a text prompt. The generated image is saved as 'lion.png'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/pricing.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import InferenceClient\nclient = InferenceClient(provider=\"fal-ai\", bill_to=\"my-org-name\")\nimage = client.text_to_image(\n    \"A majestic lion in a fantasy forest\",\n    model=\"black-forest-labs/FLUX.1-schnell\",\n)\nimage.save(\"lion.png\")\n```\n\n----------------------------------------\n\nTITLE: Install Transformers.js via NPM\nDESCRIPTION: This bash command shows how to install the `@huggingface/transformers` package using NPM. This allows the user to use the Transformers.js library in their JavaScript projects.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/transformers-js.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @huggingface/transformers\n```\n\n----------------------------------------\n\nTITLE: Load Diffusion Pipeline from Hub in Diffusers (Python)\nDESCRIPTION: This code snippet demonstrates how to load a pre-trained diffusion pipeline from the Hugging Face Hub using the `DiffusionPipeline.from_pretrained()` method. It initializes the pipeline with the specified model identifier, allowing for immediate inference. The 'stabilityai/stable-diffusion-xl-base-1.0' is passed to indicate the model to be loaded.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/diffusers.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\n\npipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\")\n```\n\n----------------------------------------\n\nTITLE: API: Retrieve Accepted Access Requests\nDESCRIPTION: This API endpoint retrieves the list of accepted access requests for a gated model. It requires a token with write access to the repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-gated.md#_snippet_1\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /api/models/{repo_id}/user-access-request/accepted\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nHeaders: {\"authorization\": \"Bearer $token\"}\n```\n\n----------------------------------------\n\nTITLE: Model Card Contact Placeholder\nDESCRIPTION: This snippet represents a placeholder for contact information related to the model card.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n`model_card_contact`\n```\n\n----------------------------------------\n\nTITLE: Image Classification Dataset with Splits\nDESCRIPTION: Demonstrates the directory structure for an image classification dataset with multiple splits (e.g., 'train' and 'test'), nested with class directories.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_10\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n test\n  green\n   2.jpg\n  red\n      4.jpg\n train\n     green\n      1.jpg\n     red\n         3.jpg\n```\n\n----------------------------------------\n\nTITLE: Dataset Repository Structure with Video Files (Subdirectory)\nDESCRIPTION: This snippet shows a dataset repository where video files are organized within a dedicated subdirectory named 'videos'. This structure can improve organization, especially when other files might be present at the root level.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n videos\n     1.mp4\n     2.mp4\n     3.mp4\n     4.mp4\n```\n\n----------------------------------------\n\nTITLE: Pushing AllenNLP Model to Hub via Python\nDESCRIPTION: This snippet shows how to push an AllenNLP model to the Hugging Face Hub using the `push_to_hf` function from the `allennlp.common.push_to_hf` module. The `repo_name` argument specifies the repository name, `serialization_dir` specifies the directory with serialized model, and `local_repo_path` specifies the local path to the model repository. Requires the `allennlp` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/allennlp.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom allennlp.common.push_to_hf import push_to_hf\n\nserialization_dir = \"path/to/serialization/directory\"\npush_to_hf(\n    repo_name=\"my_repo_name\",\n    serialization_dir=serialization_dir,\n    local_repo_path=self.local_repo_path\n)\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face programmatically\nDESCRIPTION: Authenticates the user with the Hugging Face Hub programmatically. This allows users to authenticate from within a notebook or script. Requires the `huggingface_hub` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import login\nlogin()\n```\n\n----------------------------------------\n\nTITLE: FastAPI Webhook Endpoint Definition (Python)\nDESCRIPTION: This Python code snippet defines a FastAPI endpoint '/webhook' that listens for HTTP POST requests.  It's designed to receive webhook events, process them, and trigger actions based on the event data. The endpoint expects a specific header and payload structure, and it performs validation to ensure the event is relevant before proceeding. It uses FastAPI, pydantic, and the `os` module.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-auto-retrain.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom fastapi import FastAPI\n\n# [...]\n@app.post(\"/webhook\")\nasync def post_webhook(\n\t# ...\n):\n\n# ...\n```\n\n----------------------------------------\n\nTITLE: HuggingFace SSH Key Entries for known_hosts\nDESCRIPTION: Provides a pre-configured set of SSH key entries for the `~/.ssh/known_hosts` file to avoid manually verifying HuggingFace hosts. Includes `ssh-rsa`, `ssh-dss`, `ecdsa-sha2-nistp256`, and `ssh-ed25519` key types and their corresponding host keys. Adding these entries will improve security by confirming the HuggingFace host identity and preventing man-in-the-middle attacks.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-git-ssh.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\nhf.co ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDtPB+snz63eZvTrbMY2Qt39a6HYile89JOum55z3lhIqAqUHxLtXFd+q+ED8izQvyORFPSmFIaPw05rtXo37bm+ixL6wDmvWrHN74oUUWmtrv2MNCLHE5VDb3+Q6MJjjDVIoK5QZIuTStlq0cUbGGxQk7vFZZ2VXdTPqgPjw4hMV7MGp3RFY/+Wy8rIMRv+kRCIwSAOeuaLPT7FzL0zUMDwj/VRjlzC08+srTQHqfoh0RguZiXZQneZKmM75AFhoMbP5x4AW2bVoZam864DSGiEwL8R2jMiyXxL3OuicZteZqll0qfRlNopKnzoxS29eBbXTr++ILqYz1QFqaruUgqSi3MIC9sDYEqh2Q8UxP5+Hh97AnlgWDZC0IhojVmEPNAc7Y2d+ctQl4Bt91Ik4hVf9bU+tqMXgaTrTMXeTURSXRxJEm2zfKQVkqn3vS/zGVnkDS+2b2qlVtrgbGdU/we8Fux5uOAn/dq5GygW/DUlHFw412GtKYDFdWjt3nJCY8=\nhf.co ssh-dss AAAAB3NzaC1kc3MAAACBAORXmoE8fn/UTweWy7tCYXZxigmODg71CIvs/haZQN6GYqg0scv8OFgeIQvBmIYMnKNJ7eoo5ZK+fk1yPv8aa9+8jfKXNJmMnObQVyObxFVzB51x8yvtHSSrL4J3z9EAGX9l9b+Fr2+VmVFZ7a90j2kYC+8WzQ9HaCYOlrALzz2VAAAAFQC0RGD5dE5Du2vKoyGsTaG/mO2E5QAAAIAHXRCMYdZij+BYGC9cYn5Oa6ZGW9rmGk98p1Xc4oW+O9E/kvu4pCimS9zZordLAwHHWwOUH6BBtPfdxZamYsBgO8KsXOWugqyXeFcFkEm3c1HK/ysllZ5kM36wI9CUWLedc2vj5JC+xb5CUzhVlGp+Xjn59rGSFiYzIGQC6pVkHgAAAIBve2DugKh3x8qq56sdOH4pVlEDe997ovEg3TUxPPIDMSCROSxSR85fa0aMpxqTndFMNPM81U/+ye4qQC/mr0dpFLBzGuum4u2dEpjQ7B2UyJL9qhs1Ubby5hJ8Z3bmHfOK9/hV8nhyN8gf5uGdrJw6yL0IXCOPr/VDWSUbFrsdeQ==\nhf.co ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBL0wtM52yIjm8gRecBy2wRyEMqr8ulG0uewT/IQOGz5K0ZPTIy6GIGHsTi8UXBiEzEIznV3asIz2sS7SiQ311tU=\nhf.co ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINJjhgtT9FOQrsVSarIoPVI1jFMh3VSHdKfdqp/O776s\n```\n\n----------------------------------------\n\nTITLE: Mounting Static Files and Serving HTML\nDESCRIPTION: This Python code mounts the static files directory and serves the `index.html` file at the root route (`/`). It uses `StaticFiles` from `fastapi.staticfiles` to serve static content and `FileResponse` from `fastapi.responses` to send the HTML file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\napp.mount(\"/\", StaticFiles(directory=\"static\", html=True), name=\"static\")\n\n@app.get(\"/\")\ndef index() -> FileResponse:\n    return FileResponse(path=\"/app/static/index.html\", media_type=\"text/html\")\n```\n\n----------------------------------------\n\nTITLE: Handling Text Generation Request in JavaScript\nDESCRIPTION: This JavaScript code handles the form submission event, retrieves the input text, sends a request to the `/infer_t5` endpoint, and displays the generated text in the output paragraph. It uses `fetch` to make the API call and `await` to handle the asynchronous response.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nconst textGenForm = document.querySelector(\".text-gen-form\");\n\nconst translateText = async (text) => {\n  const inferResponse = await fetch(`infer_t5?input=${text}`);\n  const inferJson = await inferResponse.json();\n\n  return inferJson.output;\n};\n\ntextGenForm.addEventListener(\"submit\", async (event) => {\n  event.preventDefault();\n\n  const textGenInput = document.getElementById(\"text-gen-input\");\n  const textGenParagraph = document.querySelector(\".text-gen-output\");\n\n  textGenParagraph.textContent = await translateText(textGenInput.value);\n});\n```\n\n----------------------------------------\n\nTITLE: Loading LayoutLM model in a Space\nDESCRIPTION: This Python code snippet demonstrates how to load the LayoutLM model in a Hugging Face Space using the transformers library. It imports the LayoutLMForTokenClassification class and then instantiates the model using the from_pretrained method, specifying the model name 'microsoft/layoutlm-base-uncased'. This inclusion of the model name is crucial for Hugging Face to detect the association between the Space and the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-add-to-arxiv.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import LayoutLMForTokenClassification\n\nlayoutlm_dummy = LayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=1)\n```\n\n----------------------------------------\n\nTITLE: Creating Hugging Face secret with CONFIG provider in DuckDB\nDESCRIPTION: This command creates a secret in DuckDB using the CONFIG provider to store a Hugging Face token. Replace 'your_hf_token' with your actual Hugging Face token.  The secret is named 'hf_token' and the type is set to HUGGINGFACE.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-auth.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nCREATE SECRET hf_token (TYPE HUGGINGFACE, TOKEN 'your_hf_token');\n```\n\n----------------------------------------\n\nTITLE: Automatic Speech Recognition Widget YAML Configuration\nDESCRIPTION: This snippet configures an automatic speech recognition widget with two examples. Each example includes a 'src' field representing the URL of the audio file and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_14\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n  example_title: Librispeech sample 1\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\n  example_title: Librispeech sample 2\n```\n\n----------------------------------------\n\nTITLE: Updating Resource Group Settings Payload (POST)\nDESCRIPTION: This payload is used when updating the settings of a resource group.  It allows enabling or disabling auto-joining and setting the default role for new users.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_12\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"autoJoin\": {\n        \"enabled\": true,\n        \"role\": \"read\" // or \"write\" or \"admin\"\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Resetting Fork History to Upstream\nDESCRIPTION: Resets the fork's history to match the upstream's main branch, effectively overriding the existing history of the fork. This is a destructive operation that should be performed with caution.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-next-steps.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit reset --hard upstream/main\n```\n\n----------------------------------------\n\nTITLE: Lazy Data Processing with Polars in Python\nDESCRIPTION: This code snippet demonstrates lazy data processing using the Polars library in Python. It reads a CSV file, applies the same transformations as the eager example, but defers the execution until `collect()` is called. This allows Polars to optimize the query execution, potentially leading to significant performance improvements.  Requires the `polars` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-optimizations.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport polars as pl\nimport datetime\n\nlf = (\n    pl.scan_csv(\"hf://datasets/commoncrawl/statistics/tlds.csv\", try_parse_dates=True)\n    .filter(\n        (pl.col(\"date\") >= datetime.date(2020, 1, 1)) |\n        pl.col(\"crawl\").str.contains(\"CC\")\n    ).with_columns(\n        (pl.col(\"pages\") / pl.col(\"domains\")).alias(\"pages_per_domain\")\n    ).group_by(\"tld\", \"date\").agg(\n        pl.col(\"pages\").sum(),\n        pl.col(\"domains\").sum(),\n    ).group_by(\"tld\").agg(\n        pl.col(\"date\").unique().count().alias(\"number_of_scrapes\"),\n        pl.col(\"domains\").mean().alias(\"avg_number_of_domains\"),\n        pl.col(\"pages\").sort_by(\"date\").pct_change().mean().alias(\"avg_page_growth_rate\"),\n    ).sort(\"avg_number_of_domains\", descending=True).head(10)\n)\ndf = lf.collect()\n```\n\n----------------------------------------\n\nTITLE: Updating Resource Group Metadata Payload (PATCH)\nDESCRIPTION: This payload is used when updating the metadata of a resource group. It allows updating the name and description of the resource group.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_11\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"name\": \"name\",\n    \"description\": \"description\"\n}\n```\n\n----------------------------------------\n\nTITLE: Make Prediction with Deployed Model\nDESCRIPTION: This snippet uses the deployed model to make a prediction on a sample input. It sends a text input to the endpoint using the `predict` method and retrieves the sentiment prediction.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsentiment_input = {\"inputs\": \"It feels like a curtain closing...there was an elegance in the way they moved toward conclusion. No fan is going to watch and feel short-changed.\"}\n\npredictor.predict(sentiment_input)\n```\n\n----------------------------------------\n\nTITLE: IFrame Resizer Script Inclusion (HTML)\nDESCRIPTION: This snippet includes the iFrame Resizer script from a CDN. This is needed when providing a custom frontend in the Gradio SDK and the content height exceeds the viewport. Including this script allows the iframe to automatically resize and enable scrolling within the Space.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-python.md#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/iframe-resizer/4.3.2/iframeResizer.contentWindow.min.js\"></script>\n```\n\n----------------------------------------\n\nTITLE: Modify Gated Dataset Text YAML\nDESCRIPTION: This YAML snippet demonstrates how to modify the default text in the gate heading, description, and button content for a gated dataset. This allows customization of the user interface presented when requesting access to the dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n---\nextra_gated_heading: \"Acknowledge license to accept the repository\"\nextra_gated_description: \"Our team may take 2-3 days to process your request\"\nextra_gated_button_content: \"Acknowledge license\"\n---\n```\n\n----------------------------------------\n\nTITLE: Run a Distilabel Pipeline (Python)\nDESCRIPTION: This Python code runs the defined Distilabel pipeline with specified runtime parameters. It sets the `max_new_tokens` to 1024 and `temperature` to 0.7 for the LLM used in the PrometheusEval task.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-distilabel.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndistiset = pipeline.run(\n    parameters={\n        task.name: {\n            \"llm\": {\n                \"generation_kwargs\": {\n                    \"max_new_tokens\": 1024,\n                    \"temperature\": 0.7,\n                },\n            },\n        },\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieve Rejected Access Requests via API\nDESCRIPTION: This API endpoint retrieves a list of users with rejected access requests for a gated dataset. It requires a token with write access to the repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md#_snippet_2\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /api/datasets/{repo_id}/user-access-request/rejected\nHeaders: {\"authorization\": \"Bearer $token\"}\n```\n\n----------------------------------------\n\nTITLE: Configure Local Audio Widget Input YAML\nDESCRIPTION: This YAML example shows how to specify a local audio file within the model repository as widget input. It defines the audio source `src` by referencing a file inside the repository along with its example title.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nwidget:\n  - src: https://huggingface.co/username/model_repo/resolve/main/sample1.flac\n    example_title: Custom Speech Sample 1\n```\n\n----------------------------------------\n\nTITLE: Installing FiftyOne via pip\nDESCRIPTION: This command installs or upgrades FiftyOne to version 0.24.0 or higher using pip.  This is a prerequisite for using FiftyOne's Hugging Face Hub integration.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-fiftyone.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -U fiftyone\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (VLM) Inference Snippet\nDESCRIPTION: This snippet demonstrates how to use Cohere for Chat Completion (VLM) with the `image-text-to-text` pipeline, utilizing the `CohereLabs/aya-vision-8b` model. It sets up the configuration needed for conversational image-to-text and text-to-text generation through Cohere.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/cohere.md#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"cohere\":{\"modelId\":\"CohereLabs/aya-vision-8b\",\"providerModelId\":\"c4ai-aya-vision-8b\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Retrieve Accepted Access Requests via API\nDESCRIPTION: This API endpoint retrieves a list of users with accepted access requests for a gated dataset.  It requires a token with write access to the repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md#_snippet_1\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /api/datasets/{repo_id}/user-access-request/accepted\nHeaders: {\"authorization\": \"Bearer $token\"}\n```\n\n----------------------------------------\n\nTITLE: Fetching Non-LFS Files from Upstream\nDESCRIPTION: Fetches non-LFS files from the upstream repository after setting up the upstream remote and skipping smudge operations during LFS installation.  Requires Git and Git LFS to be installed.  The upstream repository is assumed to be available.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-next-steps.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd myfork\ngit lfs install --skip-smudge --local # affects only this clone\ngit remote add upstream git@hf.co:friend/upstream\ngit fetch upstream\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container with Secrets\nDESCRIPTION: This command runs a Docker container while passing in secrets as environment variables. This allows secure access to sensitive information during runtime.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport SECRET_EXAMPLE=\"my_secret_value\"\ndocker run -it -p 7860:7860 -e SECRET_EXAMPLE=$SECRET_EXAMPLE fastapi\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to Origin Main Branch\nDESCRIPTION: Pushes the changes, including LFS files, to the origin's main branch, effectively updating the fork on the Hugging Face Hub.  The `--force` flag is used, which should be done with caution.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-next-steps.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit push --force origin main # this can take time depending on your upload bandwidth\n```\n\n----------------------------------------\n\nTITLE: Filtering Data with Polars\nDESCRIPTION: This snippet filters a Polars DataFrame based on a date condition using the `filter` method. It imports the `datetime` module and filters the DataFrame to include only rows where the 'date' column is greater than or equal to January 1, 2020.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-operations.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\ndf = df.filter(pl.col(\"date\") >= datetime.date(2020, 1, 1))\n```\n\n----------------------------------------\n\nTITLE: Setup local SageMaker environment\nDESCRIPTION: This Python code snippet sets up the local SageMaker environment by importing the sagemaker and boto3 libraries, creating a boto3 IAM client, retrieving the IAM role ARN, and creating a SageMaker session.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport sagemaker\nimport boto3\n\niam_client = boto3.client('iam')\nrole = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']\nsess = sagemaker.Session()\n```\n\n----------------------------------------\n\nTITLE: Querying Nutrition Data in DuckDB\nDESCRIPTION: This SQL query selects questions and correct answers from a table where the subject is 'nutrition' and the length of the correct answer is greater than 0. It limits the results to 3 rows.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-sql.md#_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nWHERE  subject = 'nutrition' AND LENGTH(correct_answer) > 0 LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Aggregating and Sorting with Polars\nDESCRIPTION: This snippet aggregates data in a Polars DataFrame, calculating statistics per top-level domain ('tld'). It calculates the number of unique scrape dates, average number of domains, and average page growth rate. The results are then sorted by the average number of domains in descending order.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-operations.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = df.group_by(\"tld\").agg(\n    pl.col(\"date\").unique().count().alias(\"number_of_scrapes\"),\n    pl.col(\"domains\").mean().alias(\"avg_number_of_domains\"),\n    pl.col(\"pages\").sort_by(\"date\").pct_change().mean().alias(\"avg_page_growth_rate\"),\n)\ndf = df.sort(\"avg_number_of_domains\", descending=True)\ndf.head(10)\n```\n\n----------------------------------------\n\nTITLE: Connecting to ZenML Server via CLI (Shell)\nDESCRIPTION: This command allows you to connect to your ZenML server running on Hugging Face Spaces from your local machine. Replace `<YOUR_HF_SPACES_DIRECT_URL>` with the direct URL of your Space, obtained from the Space's interface. The command utilizes the ZenML CLI to establish the connection using the default username and an empty password. Requires ZenML to be installed locally.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-zenml.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nzenml connect --url '<YOUR_HF_SPACES_DIRECT_URL>' --username='default' --password=''\n```\n\n----------------------------------------\n\nTITLE: Configure Space Settings with YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure a Hugging Face Space. It sets the title, emoji, color gradient, SDK, application file, and pinned status. The `sdk` parameter specifies the framework used to build the Space, and `app_file` points to the main application file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-settings.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Demo Space\nemoji: \ncolorFrom: yellow\ncolorTo: orange\nsdk: gradio\napp_file: app.py\npinned: false\n---\n```\n\n----------------------------------------\n\nTITLE: Translation Widget YAML Configuration\nDESCRIPTION: This snippet configures a translation widget with two examples. Each example includes a 'text' field representing the text to be translated and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_8\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"My name is Sylvain and I live in Paris\"\n  example_title: \"Parisian\"\n- text: \"My name is Sarah and I live in London\"\n  example_title: \"Londoner\"\n```\n\n----------------------------------------\n\nTITLE: Setting HF_API_TOKEN for Authentication (Bash)\nDESCRIPTION: This snippet sets the `HF_API_TOKEN` environment variable, which provides the Hugging Face API token for authentication. This token is used as a bearer token for accessing remote files, such as private models, and can be found in the user's Hugging Face account settings.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nHF_API_TOKEN=\"api_XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n```\n\n----------------------------------------\n\nTITLE: Aggregating Data with Polars\nDESCRIPTION: This snippet aggregates data in a Polars DataFrame by grouping by 'tld' and 'date'. It calculates the sum of 'pages' and 'domains' for each group using the `group_by` and `agg` methods.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-operations.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = df.group_by(\"tld\", \"date\").agg(\n    pl.col(\"pages\").sum(),\n    pl.col(\"domains\").sum(),\n)\n```\n\n----------------------------------------\n\nTITLE: Filling Minor Changes for Task in huggingface.js\nDESCRIPTION: This snippet refers to making minor changes related to the task in the index.ts file within the tasks package of huggingface.js. This step is required to fully integrate the new task into the Hugging Face Hub's UI elements.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-tasks.md#_snippet_3\n\n\n\n----------------------------------------\n\nTITLE: Installing mlx-image with pip\nDESCRIPTION: This snippet shows how to install the mlx-image library using pip.  It is a straightforward installation command that retrieves and installs the package and its dependencies.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx-image.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlx-image\n```\n\n----------------------------------------\n\nTITLE: Filter Parquet Data by Language using PySpark\nDESCRIPTION: This code demonstrates how to filter the BAAI/Infinity-Instruct dataset to only include dialogues in Chinese. It uses the `read_parquet` function with a filter criteria to select rows where the `langdetect` column is equal to `zh-cn`. The resulting DataFrame is then displayed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> criteria = [(\"langdetect\", \"=\", \"zh-cn\")]\n>>> df_chinese_only = read_parquet(\"hf://datasets/BAAI/Infinity-Instruct/7M/*.parquet\", filters=criteria)\n>>> df_chinese_only.show()\n+---+----------------------------+-----+----------+----------+\n| id|               conversations|label|langdetect|    source|\n+---+----------------------------+-----+----------+----------+\n|  9|[{human, ...|     |     zh-cn|Subjective|\n| 19|[{human, ...|     |     zh-cn|Subjective|\n| 38| [{human, A...|     |     zh-cn|Subjective|\n| 39|[{human, ...|     |     zh-cn|Subjective|\n| 57|[{human, ...|     |     zh-cn|Subjective|\n| 61|[{human, ...|     |     zh-cn|Subjective|\n| 66|[{human, ...|     |     zh-cn|Subjective|\n| 94|[{human, ...|     |     zh-cn|Subjective|\n|102|[{human, ...|     |     zh-cn|Subjective|\n|106|[{human, ...|     |     zh-cn|Subjective|\n|118| [{human, }...|     |     zh-cn|Subjective|\n|174|[{human, ...|     |     zh-cn|Subjective|\n|180|[{human, ...|     |     zh-cn|Subjective|\n|192|[{human, ...|     |     zh-cn|Subjective|\n|221|[{human, ...|     |     zh-cn|Subjective|\n|228|[{human, ...|     |     zh-cn|Subjective|\n|236|[{human, ...|     |     zh-cn|Subjective|\n|260|[{human, ...|     |     zh-cn|Subjective|\n|268|[{human, ...|     |     zh-cn|Subjective|\n|273| [{human, 5...|     |     zh-cn|Subjective|\n+---+----------------------------+-----+----------+----------+\n```\n\n----------------------------------------\n\nTITLE: Install pyarrow\nDESCRIPTION: This command installs the `pyarrow` library, which is needed for reading and writing Parquet, JSON, and CSV files using the filesystem API provided by `huggingFace_hub`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npip install pyarrow\n```\n\n----------------------------------------\n\nTITLE: Markdown for Referencing Research Papers\nDESCRIPTION: This Markdown snippet shows how to cite research papers associated with your model in the model card. Linking research papers provides academic context, allows users to delve deeper into the theoretical foundations of your work, and increases citations.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-release-checklist.md#_snippet_2\n\nLANGUAGE: Markdown\nCODE:\n```\n## References\n\n* [Model Paper](https://arxiv.org/abs/xxxx.xxxxx)\n```\n\n----------------------------------------\n\nTITLE: Installing SpanMarker\nDESCRIPTION: This command installs the SpanMarker library using pip. The -U flag ensures that SpanMarker and its dependencies are upgraded to the latest version if they are already installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/span_marker.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U span_marker\n```\n\n----------------------------------------\n\nTITLE: Loading Flair model from Hub\nDESCRIPTION: Loads a pre-trained Flair sequence tagging model from the Hugging Face Hub.  It imports necessary modules from the flair library and then loads the model using its identifier.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/flair.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom flair.data import Sentence\nfrom flair.models import SequenceTagger\n\n# load tagger\ntagger = SequenceTagger.load(\"flair/ner-multi\")\n```\n\n----------------------------------------\n\nTITLE: Installing BERTopic with pip\nDESCRIPTION: This snippet demonstrates how to install the BERTopic library using pip, a package installer for Python. It's a one-line command that downloads and installs BERTopic and its dependencies.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/bertopic.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install bertopic\n```\n\n----------------------------------------\n\nTITLE: Setting HF_MODEL_ID for Model Loading (Bash)\nDESCRIPTION: This snippet sets the `HF_MODEL_ID` environment variable, which specifies the model to be loaded from the Hugging Face Model Hub.  This variable is used when creating a SageMaker endpoint to automatically load the specified model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nHF_MODEL_ID=\"distilbert-base-uncased-finetuned-sst-2-english\"\n```\n\n----------------------------------------\n\nTITLE: Read Parquet Data with Predicate Pushdown\nDESCRIPTION: This Python snippet demonstrates reading Parquet data from Hugging Face Datasets using Dask and applying a filter. Dask leverages predicate pushdown to skip files or row groups that do not match the query.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\n\ndf = dd.read_parquet(\"hf://datasets/HuggingFaceFW/fineweb-edu/sample/10BT/*.parquet\")\n\n# Dask will skip the files or row groups that don't\n# match the query without downloading them.\ndf = df[df.dump >= \"CC-MAIN-2023\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring Summarization Temperature YAML\nDESCRIPTION: This snippet illustrates how to change the temperature parameter for a summarization task in the HF-Inference API widget. The `temperature` is set to 0.7, influencing the randomness of the generated text. This configuration is applied by modifying the model card metadata in YAML format.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ninference:\n  parameters:\n    temperature: 0.7\n```\n\n----------------------------------------\n\nTITLE: Loading a SetFit Model from the Hub\nDESCRIPTION: This code snippet loads a pre-trained SetFit model from the Hugging Face Hub using the `SetFitModel.from_pretrained` method.  The `tomaarsen/setfit-paraphrase-mpnet-base-v2-sst2-8-shot` is the model identifier on the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/setfit.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom setfit import SetFitModel\n\nmodel = SetFitModel.from_pretrained(\"tomaarsen/setfit-paraphrase-mpnet-base-v2-sst2-8-shot\")\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Shiny R App\nDESCRIPTION: The Dockerfile for a Shiny R app on Hugging Face Spaces builds upon the rocker/shiny image. It allows for installing additional R packages through `install2.r` and GitHub packages using `installGithub.r`. It's crucial to expose the correct port (default 7860) and use the development version of httpuv to avoid app timeouts.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-shiny.md#_snippet_1\n\nLANGUAGE: Dockerfile\nCODE:\n```\nThe Dockerfile builds off of the the [rocker shiny](https://hub.docker.com/r/rocker/shiny) image. You'll need to modify this file to use additional packages. \nIf you are using a lot of tidyverse packages we recommend switching the base image to [rocker/shiny-verse](https://hub.docker.com/r/rocker/shiny-verse).\nYou can install additional R packages by adding them under the `RUN install2.r` section of the dockerfile, and github packages can be installed by adding the repository under `RUN installGithub.r`.\n\nThere are two main requirements for this Dockerfile:\n\n-   First, the file must expose the port that you have listed in the README. The default is 7860 and we recommend not changing this port unless you have a reason to.\n\n-   Second, for the moment you must use the development version of [httpuv](https://github.com/rstudio/httpuv) which resolves an issue with app timeouts on Hugging Face.\n```\n\n----------------------------------------\n\nTITLE: Simple Image Dataset Repository Structure\nDESCRIPTION: Demonstrates the simplest structure for an image dataset repository with images stored at the root directory, indicating that all files are images.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_0\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n 1.jpg\n 2.jpg\n 3.jpg\n 4.jpg\n```\n\n----------------------------------------\n\nTITLE: Loading safetensors weights from a DDUF archive\nDESCRIPTION: This snippet demonstrates how to load safetensors weights from a binary file within a DDUF archive using memory-mapping.  It accesses the file's raw bytes as a memory-mapping using `as_mmap` and then uses `safetensors.torch.load` to load the weights. The `as_mmap` call must be used within a context manager. It requires the `safetensors` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/dduf.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n>>> import safetensors.torch\n>>> with dduf_entries[\"vae/diffusion_pytorch_model.safetensors\"].as_mmap() as mm:\n...     state_dict = safetensors.torch.load(mm) # `mm` is a bytes object\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face from Notebook\nDESCRIPTION: This snippet demonstrates how to log in to your Hugging Face account from a Jupyter or Colaboratory notebook. This is an alternative to using the command line interface when working in a notebook environment. Requires the `huggingface_hub` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/timm.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import notebook_login\nnotebook_login()\n```\n\n----------------------------------------\n\nTITLE: Install @huggingface/hub via npm\nDESCRIPTION: Installs the `@huggingface/hub` library, which is required for integrating with the Hugging Face Hub when the library is implemented in Javascript. This library provides the necessary tools for downloading and uploading files to the Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nnpm add @huggingface/hub\n```\n\n----------------------------------------\n\nTITLE: Loading OpenCLIP Model from Hugging Face Hub\nDESCRIPTION: This code snippet demonstrates how to load a pre-trained OpenCLIP model from the Hugging Face Hub using the `open_clip` library. It creates the model and retrieves the tokenizer for the specified model identifier. It requires the `open_clip` library to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/open_clip.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport open_clip\n\nmodel, preprocess = open_clip.create_model_from_pretrained('hf-hub:laion/CLIP-ViT-g-14-laion2B-s12B-b42K')\ntokenizer = open_clip.get_tokenizer('hf-hub:laion/CLIP-ViT-g-14-laion2B-s12B-b42K')\n```\n\n----------------------------------------\n\nTITLE: Get Model Providers (cURL)\nDESCRIPTION: This snippet demonstrates how to retrieve a list of inference providers serving a specific model using the Hugging Face Hub API. It uses cURL to query the `/api/models/{model_id}` endpoint, expanding the `inferenceProviderMapping` attribute. The response contains a mapping of providers, their status, task, and provider ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\n# List google/gemma-3-27b-it providers\n~ curl -s https://huggingface.co/api/models/google/gemma-3-27b-it?expand[]=inferenceProviderMapping\n{\n    \"_id\": \"67c35b9bb236f0d365bf29d3\",\n    \"id\": \"google/gemma-3-27b-it\",\n    \"inferenceProviderMapping\": {\n        \"hf-inference\": {\n            \"status\": \"live\",\n            \"providerId\": \"google/gemma-3-27b-it\",\n            \"task\": \"conversational\"\n        },\n        \"nebius\": {\n            \"status\": \"live\",\n            \"providerId\": \"google/gemma-3-27b-it-fast\",\n            \"task\": \"conversational\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Malicious Pickle Creation with Fickling\nDESCRIPTION: This snippet demonstrates how to create a malicious pickle file using the `fickling` library.  It inserts Python code into the pickle that will be executed upon deserialization, highlighting the arbitrary code execution vulnerability. `fickling` allows inserting arbitrary python code via the `exec` function.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom fickling.pickle import Pickled\nimport pickle\n\n# Create a malicious pickle\ndata = \"my friend needs to know this\"\n\npickle_bin = pickle.dumps(data)\n\np = Pickled.load(pickle_bin)\n\np.insert_python_exec('print(\"you've been pwned !\")')\n\nwith open('payload.pkl', 'wb') as f:\n    p.dump(f)\n\n# innocently unpickle and get your friend's data\nwith open('payload.pkl', 'rb') as f:\n    data = pickle.load(f)\n    print(data)\n```\n\n----------------------------------------\n\nTITLE: Dummy Word Count Function\nDESCRIPTION: Defines a simple function to count the number of words in a list of text strings using pandas Series.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef dummy_count_words(texts):\n    return pd.Series([len(text.split(\" \")) for text in texts])\n```\n\n----------------------------------------\n\nTITLE: Markdown Snippet With Gallery Component\nDESCRIPTION: Example model card content showing how to insert the <Gallery /> component within the markdown of the model card. This displays a generated media gallery based on the defined widget metadata.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards-components.md#_snippet_2\n\nLANGUAGE: Markdown\nCODE:\n```\n<Gallery />\n\n## Model description\n\nA very classic hand drawn cartoon style.\n```\n\n----------------------------------------\n\nTITLE: Run inference with llama-cli\nDESCRIPTION: This command runs inference using the `llama-cli` tool with a specified GGUF model from the Hugging Face Hub. The `-hf` flag specifies the repository and filename of the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/gguf-llamacpp.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nllama-cli -hf bartowski/Llama-3.2-3B-Instruct-GGUF:Q8_0\n```\n\n----------------------------------------\n\nTITLE: Implementing the Provider Helper (JS)\nDESCRIPTION: This code snippet provides a template for creating a provider helper class in JavaScript, to be placed in `packages/inference/src/providers/{provider_name}.ts`. It extends the `TaskProviderHelper` class and requires implementing methods like `makeRoute`, `preparePayload`, and `getResponse` for custom handling of API requests and responses. These methods handle the route creation, payload preparation and response formatting specific to the provider's API.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TaskProviderHelper } from \"./providerHelper\";\n\nexport class MyNewProviderTask extends TaskProviderHelper {\n\n\tconstructor() {\n\t\tsuper(\"your-provider-name\", \"your-api-base-url\", \"task-name\");\n\t}\n\n    override prepareHeaders(params: HeaderParams, binary: boolean): Record<string, string> {\n        // Override the headers to use for the request.\n        return super.prepareHeaders(params, binary);\n    }\n\n\tmakeRoute(params: UrlParams): string {\n        // Return the route to use for the request. e.g. /v1/chat/completions route is commonly use for chat completion.\n\t\tthrow new Error(\"Needs to be implemented\");\n\t}\n\n\tpreparePayload(params: BodyParams): Record<string, unknown> {\n        // Return the payload to use for the request, as a dict.\n\t\tthrow new Error(\"Needs to be implemented\");\n\t}\n\n\tgetResponse(response: unknown, outputType?: \"url\" | \"blob\"): string | Promise<Blob>{\n\t\t// Return the response in the expected format.\n        throw new Error(\"Needs to be implemented\");\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding a Code Snippet for Asteroid Models in TypeScript\nDESCRIPTION: This TypeScript code defines a function that generates a Python code snippet for loading an Asteroid model using the `asteroid` library. The snippet uses the model ID to load the model from Hugging Face Hub. The snippet is designed to be added to `model-libraries-snippets.ts`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst asteroid = (model: ModelData) =>\n`from asteroid.models import BaseModel\n  \nmodel = BaseModel.from_pretrained(\"${model.id}\")`;\n```\n\n----------------------------------------\n\nTITLE: Text To Image Inference Snippet\nDESCRIPTION: This snippet configures the InferenceSnippet component for Text To Image tasks. It utilizes the 'text-to-image' pipeline and maps the model 'black-forest-labs/FLUX.1-dev' to the Nebius provider model 'black-forest-labs/flux-dev'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/nebius.md#_snippet_2\n\nLANGUAGE: HTML\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-to-image\n    providersMapping={ {\"nebius\":{\"modelId\":\"black-forest-labs/FLUX.1-dev\",\"providerModelId\":\"black-forest-labs/flux-dev\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Deploy Hugging Face Model with model_data\nDESCRIPTION: Deploys a pre-trained Hugging Face model from S3 to SageMaker for inference using `model_data`. It showcases how to specify the model location and other required parameters for deployment.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sagemaker.huggingface.model import HuggingFaceModel\n\n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n   model_data=\"s3://models/my-bert-model/model.tar.gz\",  # path to your trained SageMaker model\n   role=role,                                            # IAM role with permissions to create an endpoint\n   transformers_version=\"4.26\",                           # Transformers version used\n   pytorch_version=\"1.13\",                                # PyTorch version used\n   py_version='py39',                                    # Python version used\n)\n\n# deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n   initial_instance_count=1,\n   instance_type=\"ml.m5.xlarge\"\n)\n\n# example request: you always need to define \"inputs\"\ndata = {\n   \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n}\n\n# request\npredictor.predict(data)\n```\n\n----------------------------------------\n\nTITLE: Performing inference with Flair\nDESCRIPTION: Performs inference with a loaded Flair sequence tagging model.  It creates a Sentence object, predicts tags for it using the loaded tagger, and then prints the tagged sentence.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/flair.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsentence = Sentence(\"George Washington ging nach Washington.\")\ntagger.predict(sentence)\n\n# print sentence\nprint(sentence)\n```\n\n----------------------------------------\n\nTITLE: Sample Hugging Face URL with Parquet Conversion\nDESCRIPTION: A complete example of how to access a specific Parquet file within a dataset's converted Parquet branch. This shows the complete URL string.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb.md#_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nhf://datasets/ibm/duorc@~parquet/ParaphraseRC/test/0000.parquet\n```\n\n----------------------------------------\n\nTITLE: Upload a single file to the Hub\nDESCRIPTION: Uploads a single file to a Hugging Face Hub repository using the `upload_file` function. This requires a path to the file, the desired path within the repository, and the repository ID. Requires the `huggingface_hub` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import upload_file\n>>> upload_file(\n...    path_or_fileobj=\"/home/lysandre/dummy-test/README.md\", \n...    path_in_repo=\"README.md\", \n...    repo_id=\"lysandre/test-model\"\n... )\n'https://huggingface.co/lysandre/test-model/blob/main/README.md'\n```\n\n----------------------------------------\n\nTITLE: Loading a DDUF file with Diffusers\nDESCRIPTION: This snippet showcases how to load a diffusion pipeline from a DDUF file stored on the Hub using the `Diffusers` library. It utilizes the `from_pretrained` method with the `dduf_file` argument to specify the DDUF file. The loaded pipeline generates an image based on a text prompt, which is then saved to a file. It requires the `diffusers` and `torch` libraries.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/dduf.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    \"DDUF/FLUX.1-dev-DDUF\", dduf_file=\"FLUX.1-dev.dduf\", torch_dtype=torch.bfloat16\n).to(\"cuda\")\nimage = pipe(\n    \"photo a cat holding a sign that says Diffusers\", num_inference_steps=50, guidance_scale=3.5\n).images[0]\nimage.save(\"cat.png\")\n```\n\n----------------------------------------\n\nTITLE: Configure Nested Path Audio Input YAML\nDESCRIPTION: This YAML configures a widget input using a nested relative file path for an audio file located within a subdirectory of the model repository. It uses `src` to point to the nested file inside the repo.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nwidget:\n  - src: nested/directory/sample1.flac\n```\n\n----------------------------------------\n\nTITLE: Adding Hugging Face Space as a Git Remote\nDESCRIPTION: This command adds your Hugging Face Space as a remote repository to your local Git repository. Replace `HF_USERNAME` with your Hugging Face username and `SPACE_NAME` with your Space's name. This allows you to push changes directly to your Space from your local repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-circleci.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\n----------------------------------------\n\nTITLE: Querying PokemonCards Dataset\nDESCRIPTION: This snippet demonstrates how to query the `TheFusion21/PokemonCards` dataset in CSV format using DuckDB. It selects the first 3 rows and displays the id, image_url, caption, name, hp, and set_name columns.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-combine-and-export.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nFROM 'hf://datasets/TheFusion21/PokemonCards/train.csv' LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Complete Dockerfile with User Permissions\nDESCRIPTION: This is the complete Dockerfile, incorporating the user creation, dependency installation, file copying, permission adjustments, and command to start the Uvicorn server.  It includes setting the correct user for the application to address potential permission issues.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_13\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# read the doc: https://huggingface.co/docs/hub/spaces-sdks-docker\n# you will also find guides on how best to write your Dockerfile\n\nFROM python:3.9\n\n# The two following lines are requirements for the Dev Mode to be functional\n# Learn more about the Dev Mode at https://huggingface.co/dev-mode-explorers\nRUN useradd -m -u 1000 user\nWORKDIR /app\n\nCOPY --chown=user ./requirements.txt requirements.txt\nRUN pip install --no-cache-dir --upgrade -r requirements.txt\n\nCOPY --chown=user . /app\n\nUSER user\n\nENV HOME=/home/user \\\n\tPATH=/home/user/.local/bin:$PATH\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"7860\"]\n```\n\n----------------------------------------\n\nTITLE: Specify Text Classification Widget Output YAML\nDESCRIPTION: This YAML snippet shows how to define the example outputs for a text classification widget. It includes a list of labels with their associated scores.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nwidget:\n  - text: \"I liked this movie\"\n    output:\n      - label: POSITIVE\n        score: 0.8\n      - label: NEGATIVE\n        score: 0.2\n```\n\n----------------------------------------\n\nTITLE: Example Discussion Webhook Payload\nDESCRIPTION: This JSON payload represents the structure of a webhook payload regarding a discussion.  It provides details such as the discussion's ID, title, URLs, status, author information, a flag indicating if it's also a pull request, associated changes, and the discussion number.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"id\": \"639885d811ae2bad2b7ba461\",\n\t\"title\": \"Hello!\",\n\t\"url\": {\n\t\t\"web\": \"https://huggingface.co/some-user/some-repo/discussions/3\",\n\t\t\"api\": \"https://huggingface.co/api/models/some-user/some-repo/discussions/3\"\n\t},\n\t\"status\": \"open\",\n\t\"author\": {\n\t\t\"id\": \"61d2000c3c2083e1c08af22d\"\n\t},\n\t\"isPullRequest\": true,\n\t\"changes\": {\n\t\t\"base\": \"refs/heads/main\"\n\t}\n\t\"num\": 3\n}\n\n```\n\n----------------------------------------\n\nTITLE: Dataset Structure with Alternative Split Keywords\nDESCRIPTION: Demonstrates the use of alternative keywords for train/validation/test splits, such as `training`, `eval`, and `valid`. This example showcases a valid repository structure using these alternative names.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-file-names-and-splits.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n README.md\n data/\n     training.csv\n     eval.csv\n     valid.csv\n```\n\n----------------------------------------\n\nTITLE: Audio Files with Splits\nDESCRIPTION: This snippet demonstrates organizing audio files into 'train' and 'test' split directories. This structure allows for specifying different datasets for training and evaluation.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n train\n  1.wav\n  2.wav\n test\n     3.wav\n     4.wav\n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech Inference with ESPnet\nDESCRIPTION: This code snippet demonstrates how to perform text-to-speech inference using a pre-trained ESPnet model from the Hugging Face Hub. It loads the model using the `Text2Speech.from_pretrained` method, generates speech from a given text, and saves the output as a WAV file. The `soundfile` library is used for writing the audio to a file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/espnet.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport soundfile\nfrom espnet2.bin.tts_inference import Text2Speech\n\ntext2speech = Text2Speech.from_pretrained(\"model_name\")\nspeech = text2speech(\"foobar\")[\"wav\"]\nsoundfile.write(\"out.wav\", speech.numpy(), text2speech.fs, \"PCM_16\")\n```\n\n----------------------------------------\n\nTITLE: Install huggingface_hub\nDESCRIPTION: This command installs the `huggingface_hub` library, which is required to interact with Hugging Face datasets repositories.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install huggingface_hub\n```\n\n----------------------------------------\n\nTITLE: Get Model Inference Status - No Inference (Python)\nDESCRIPTION: This snippet retrieves the inference status of a model using the `huggingface_hub` library's `model_info` function when no inference endpoint is defined. The `expand` parameter is set to \"inference\". The expected output is `None`, indicating no inference is available for the specified model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import model_info\n\n>>> info = model_info(\"manycore-research/SpatialLM-Llama-1B\", expand=\"inference\")\n>>> info.inference\nNone\n```\n\n----------------------------------------\n\nTITLE: Image Dataset with Mixed Image Formats\nDESCRIPTION: Shows how to include different image formats (JPEG, PNG, TIFF, WebP) within the same 'images' subdirectory, which the Hub automatically supports.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_2\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n images\n     1.jpg\n     2.png\n     3.tiff\n     4.webp\n```\n\n----------------------------------------\n\nTITLE: Initialize Git LFS\nDESCRIPTION: This command initializes Git LFS (Large File Storage) in your local repository. Git LFS is used to handle large files such as images and model weights efficiently.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-getting-started.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\n```\n\n----------------------------------------\n\nTITLE: API: Retrieve Pending Access Requests\nDESCRIPTION: This API endpoint retrieves the list of pending access requests for a gated model. It requires a token with write access to the repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-gated.md#_snippet_0\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /api/models/{repo_id}/user-access-request/pending\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nHeaders: {\"authorization\": \"Bearer $token\"}\n```\n\n----------------------------------------\n\nTITLE: Fetch and Checkout Hugging Face Hub Pull Request - Bash\nDESCRIPTION: This snippet demonstrates how to fetch a specific pull request from the Hugging Face Hub, checkout the code locally, make changes, and push them back to the pull request's ref.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-pull-requests-discussions.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch origin refs/pr/42:pr/42\ngit checkout pr/42\n# Do your changes\ngit add .\ngit commit -m \"Add your change\"\ngit push origin pr/42:refs/pr/42\n```\n\n----------------------------------------\n\nTITLE: LaTeX display mode in Model Card\nDESCRIPTION: Example usage of LaTeX display mode in Hugging Face Model Cards using `$$ ... $$` delimiters. The Hub uses the KaTeX library to render the math formulas.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_9\n\nLANGUAGE: latex\nCODE:\n```\n$$\n\\LaTeX\n$$\n\n$$ \n\\mathrm{MSE} = \\left(\\frac{1}{n}\\right)\\sum_{i=1}^{n}(y_{i} - x_{i})^{2}\n$$\n\n$$ E=mc^2 $$\n```\n\n----------------------------------------\n\nTITLE: Installing Sample Factory\nDESCRIPTION: This command installs the sample-factory library using pip. Sample Factory is known to work on Linux and MacOS. There is no Windows support at this time.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/sample-factory.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install sample-factory\n```\n\n----------------------------------------\n\nTITLE: Image Dataset with Metadata CSV\nDESCRIPTION: Demonstrates how to include a metadata CSV file alongside images in the 'train' directory to provide additional information such as captions or bounding boxes.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_4\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n train\n     1.jpg\n     2.jpg\n     3.jpg\n     4.jpg\n     metadata.csv\n```\n\n----------------------------------------\n\nTITLE: Loading DataFrame from Parquet on Hugging Face Hub\nDESCRIPTION: This snippet shows how to load a DataFrame from a Parquet file on the Hugging Face Hub using the `hf://` protocol. The path starts with `hf://` followed by the dataset repository and file path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import pandas as pd\n>>> df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/train-00000-of-00001.parquet\")\n>>> df\n                                                    text  label\n0      I rented I AM CURIOUS-YELLOW from my video sto...      0\n1      \"I Am Curious: Yellow\" is a risible and preten...      0\n2      If only to avoid making this type of film in t...      0\n3      This film was probably inspired by Godard's Ma...      0\n4      Oh, brother...after hearing about this ridicul...      0\n...                                                  ...    ...\n24995  A hit at the time but now better categorised a...      1\n24996  I love this movie like no other. Another time ...      1\n24997  This film and it's sequel Barry Mckenzie holds...      1\n24998  'The Adventures Of Barry McKenzie' started lif...      1\n24999  The story centers around Barry McKenzie who mu...      1\n```\n\n----------------------------------------\n\nTITLE: Handle POST request and verify Webhook secret - TypeScript\nDESCRIPTION: This code snippet listens for POST requests to the root path ('/') and verifies the `X-Webhook-Secret` header against the configured `WEBHOOK_SECRET` environment variable. If the secret doesn't match, it returns a 400 error with an 'incorrect secret' message. This ensures that only requests from the configured Webhook are processed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-discussion-bot.md#_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\napp.post(\"/\", async (req, res) => {\n\tif (req.header(\"X-Webhook-Secret\") !== process.env.WEBHOOK_SECRET) {\n\t\tconsole.error(\"incorrect secret\");\n\t\treturn res.status(400).json({ error: \"incorrect secret\" });\n\t}\n\t...\n```\n\n----------------------------------------\n\nTITLE: Specifying Base Model in YAML\nDESCRIPTION: This YAML snippet shows how to specify the base model for a fine-tuned, adapter, or quantized version of a base model in the model card metadata. This helps users to find models that are derived from a specific base model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nbase_model: HuggingFaceH4/zephyr-7b-beta\n```\n\n----------------------------------------\n\nTITLE: Running Private GGUF Model with Ollama\nDESCRIPTION: This snippet shows the command to run a private GGUF model from the Hugging Face Hub using Ollama.  It requires Ollama to be installed, configured, and the Ollama SSH key to be added to the Hugging Face account.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/ollama.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\nollama run hf.co/{username}/{repository}\n```\n\n----------------------------------------\n\nTITLE: Build and Run a Distilabel Pipeline with PrometheusEval Task (Python)\nDESCRIPTION: This Python code defines a Distilabel pipeline that loads a dataset from the Hugging Face Hub, performs evaluation using the PrometheusEval task, and keeps specific columns.  It uses the `vLLM` for the LLM inference and connects all the steps using the `>>` operator. Finally, it runs the pipeline with specific parameters such as `max_new_tokens` and `temperature`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-distilabel.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom distilabel.llms import vLLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.steps import KeepColumns, LoadDataFromHub\nfrom distilabel.steps.tasks import PrometheusEval\n\nif __name__ == \"__main__\":\n    with Pipeline(name=\"prometheus\") as pipeline:\n        load_dataset = LoadDataFromHub(\n            name=\"load_dataset\",\n            repo_id=\"HuggingFaceH4/instruction-dataset\",\n            split=\"test\",\n            output_mappings={\"prompt\": \"instruction\", \"completion\": \"generation\"},\n        )\n\n        task = PrometheusEval(\n            name=\"task\",\n            llm=vLLM(\n                model=\"prometheus-eval/prometheus-7b-v2.0\",\n                chat_template=\"[INST] {{ messages[0]['content'] }}\\n{{ messages[1]['content'] }}[/INST]\",\n            ),\n            mode=\"absolute\",\n            rubric=\"factual-validity\",\n            reference=False,\n            num_generations=1,\n            group_generations=False,\n        )\n\n        keep_columns = KeepColumns(\n            name=\"keep_columns\",\n            columns=[\"instruction\", \"generation\", \"feedback\", \"result\", \"model_name\"],\n        )\n\n        load_dataset >> task >> keep_columns\n```\n\n----------------------------------------\n\nTITLE: Exporting GPG Public Key (Bash)\nDESCRIPTION: This snippet exports the public part of a specified GPG key in ASCII armored format, making it suitable for copying and pasting into a text area.  The `<YOUR KEY ID>` needs to be replaced with the actual key ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-gpg.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngpg --armor --export <YOUR KEY ID>\n```\n\n----------------------------------------\n\nTITLE: Using write_parquet to upload a Dataframe\nDESCRIPTION: This snippet shows how to use the `write_parquet` function to write a PySpark Dataframe to a Hugging Face dataset repository.  It assumes you have already created a dataset repository and are authenticated. The `df_chinese_only` Dataframe is written to the specified Hugging Face dataset path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> write_parquet(df_chinese_only, \"hf://datasets/username/Infinity-Instruct-Chinese-Only\")\n```\n\n----------------------------------------\n\nTITLE: Install SageMaker SDK (Bash)\nDESCRIPTION: This command installs the SageMaker SDK using pip. The version must be at least 2.231.0 for compatibility with features used in the LLM deployment example. Requires pip to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install sagemaker>=2.231.0\n```\n\n----------------------------------------\n\nTITLE: Metadata JSONL Example\nDESCRIPTION: This snippet shows an example of metadata stored in a JSONL file. Each line is a JSON object containing the 'file_name' and corresponding 'text' description for a video.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_6\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"file_name\": \"1.mp4\",\"text\": \"an animation of a green pokemon with red eyes\"}\n{\"file_name\": \"2.mp4\",\"text\": \"a short video of a green and yellow toy with a red nose\"}\n{\"file_name\": \"3.mp4\",\"text\": \"a red and white ball shows an angry look on its face\"}\n{\"file_name\": \"4.mp4\",\"text\": \"a cartoon ball is smiling\"}\n```\n\n----------------------------------------\n\nTITLE: Table Question Answering Widget YAML Configuration\nDESCRIPTION: This snippet configures a table question answering widget with an example. The example includes a 'text' field for the question, a 'table' field containing the table data, and 'example_title' for the example's title. The 'table' field contains column names (e.g., Repository, Stars) and corresponding data.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"How many stars does the transformers repository have?\"\n  table:\n    Repository:\n      - \"Transformers\"\n      - \"Datasets\"\n      - \"Tokenizers\"\n    Stars:\n      - 36542\n      - 4512\n      - 3934\n    Contributors:\n      - 651\n      - 77\n      - 34\n    Programming language:\n      - \"Python\"\n      - \"Python\"\n      - \"Rust, Python and NodeJS\"\n  example_title: \"Github stars\"\n```\n\n----------------------------------------\n\nTITLE: Dataset Repository with Splits for Video Classification\nDESCRIPTION: This snippet shows a directory structure for video classification with training and testing splits. This allows for proper model evaluation.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n test\n  green\n   2.mp4\n  red\n      4.mp4\n train\n     green\n      1.mp4\n     red\n         3.mp4\n```\n\n----------------------------------------\n\nTITLE: Creating Hugging Face secret with CREDENTIAL_CHAIN provider in DuckDB\nDESCRIPTION: This command creates a secret in DuckDB using the CREDENTIAL_CHAIN provider, which automatically retrieves the Hugging Face token from `~/.cache/huggingface/token`.  The secret is named 'hf_token' and the type is set to HUGGINGFACE.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-auth.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nCREATE SECRET hf_token (TYPE HUGGINGFACE, PROVIDER credential_chain);\n```\n\n----------------------------------------\n\nTITLE: Setting Docker SDK in README\nDESCRIPTION: This YAML snippet initializes the Docker Space by setting the `sdk` property to `docker` in the `README.md` file's YAML block. This configuration is required for the Space to be recognized as a Docker Space.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nsdk: docker\n```\n\n----------------------------------------\n\nTITLE: Inference API request with cURL\nDESCRIPTION: This snippet shows how to make an inference request to a timm model on the Hugging Face Hub using cURL. It requires an API token for authorization.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/timm.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://api-inference.huggingface.co/models/nateraw/timm-resnet50-beans \\\n        -X POST \\\n        --data-binary '@beans.jpeg' \\\n        -H \"Authorization: Bearer {$HF_API_TOKEN}\"\n```\n\n----------------------------------------\n\nTITLE: CSV with Relative Paths Example\nDESCRIPTION: Shows an example of a metadata.csv file that uses relative paths (e.g., 'images/1.jpg') to link metadata to image files located in a subdirectory.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_8\n\nLANGUAGE: csv\nCODE:\n```\nfile_name,text\nimages/1.jpg,a drawing of a green pokemon with red eyes\nimages/2.jpg,a green and yellow toy with a red nose\nimages/3.jpg,a red and white ball with an angry look on its face\nimages/4.jpg,a cartoon ball with a smile on it's face\n```\n\n----------------------------------------\n\nTITLE: Installing and Logging into Hugging Face Hub CLI\nDESCRIPTION: These commands install the `huggingface_hub` CLI and then authenticate with the Hugging Face Hub. This is a prerequisite for pushing code to the Hub from the terminal.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-getting-started.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install huggingface_hub\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: OAuth Authorization URL Example\nDESCRIPTION: This snippet demonstrates the structure of an OAuth authorization URL for initiating the 'Sign in with Hugging Face' flow.  It shows how to include the `client_id`, `redirect_uri`, `scope`, and `state` parameters. Replace `CLIENT_ID` and `REDIRECT_URI` with your application's values and `STATE` with a unique session identifier.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/oauth.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<a href=\"https://huggingface.co/oauth/authorize?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&scope=openid%20profile&state=STATE\">Sign in with Hugging Face</a>\n```\n\n----------------------------------------\n\nTITLE: Install hf-xet\nDESCRIPTION: Installs the `hf-xet` package.  This is required if you're using `transformers` or `datasets` which already depend on `huggingface_hub`. It enables Xet deduplication for uploads and downloads.  `huggingface_hub` should be >= 0.30.0\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/storage-backends.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install hf-xet\n```\n\n----------------------------------------\n\nTITLE: Setting Hugging Face token as an environment variable\nDESCRIPTION: This command sets the Hugging Face token as an environment variable. The token can then be accessed by other applications.  Replace \"hf_xxxxxxxxxxxxx\" with your actual Hugging Face token.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-auth.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_TOKEN=\"hf_xxxxxxxxxxxxx\"\n```\n\n----------------------------------------\n\nTITLE: Example Comment Webhook Payload\nDESCRIPTION: This JSON payload illustrates the format of a webhook payload when a comment is created or updated. It provides information about the comment, including its ID, author, content, visibility (hidden or not), and associated URLs.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"id\": \"6398872887bfcfb93a306f18\",\n\t\"author\": {\n\t\t\"id\": \"61d2000c3c2083e1c08af22d\"\n\t},\n\t\"content\": \"This adds an env key\",\n\t\"hidden\": false,\n\t\"url\": {\n\t\t\"web\": \"https://huggingface.co/some-user/some-repo/discussions/4#6398872887bfcfb93a306f18\"\n\t}\n}\n\n```\n\n----------------------------------------\n\nTITLE: Disable Label Creation in YAML\nDESCRIPTION: Shows how to disable the automatic addition of a 'label' column for image classification datasets by setting `drop_labels: true` in the YAML configuration in the README header.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nconfigs:\n  - config_name: default  # Name of the dataset subset, if applicable.\n    drop_labels: true\n```\n\n----------------------------------------\n\nTITLE: Push Argilla Dataset to Hugging Face Hub\nDESCRIPTION: This code snippet demonstrates how to push an Argilla dataset to the Hugging Face Hub. It imports the `argilla` library, initializes the Argilla client, retrieves a dataset by name, and then pushes it to the specified repository ID on the Hub. Both dataset settings and records are pushed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-argilla.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport argilla as rg\n\nclient = rg.Argilla(api_url=\"<api_url>\", api_key=\"<api_key>\")\ndataset = client.datasets(name=\"my_dataset\")\ndataset.to_hub(repo_id=\"<repo_id>\")\n```\n\n----------------------------------------\n\nTITLE: Uploading Image Folder to Hugging Face Hub\nDESCRIPTION: This snippet demonstrates how to upload a folder containing images and metadata to the Hugging Face Hub using the `huggingface_hub` library. It initializes the `HfApi` class and calls the `upload_folder` method, specifying the `folder_path`, `repo_id`, and `repo_type`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\napi = HfApi()\n\napi.upload_folder(\n    folder_path=folder_path,\n    repo_id=\"username/my_image_dataset\",\n    repo_type=\"dataset\",\n)\n```\n\n----------------------------------------\n\nTITLE: Fetching All LFS Files from Upstream\nDESCRIPTION: Fetches all Large File Storage (LFS) files from the upstream repository.  This operation may take a significant amount of time depending on the download bandwidth available.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-next-steps.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs fetch --all upstream # this can take time depending on your download bandwidth\n```\n\n----------------------------------------\n\nTITLE: Generating text with MLX-LM\nDESCRIPTION: This command utilizes the `mlx_lm.generate` module to download and load a specified model (mistralai/Mistral-7B-Instruct-v0.2) and generate text based on a given prompt (\"hello\").  It requires the `mlx-lm` package to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt \"hello\"\n```\n\n----------------------------------------\n\nTITLE: Text Generation Widget YAML Configuration\nDESCRIPTION: This snippet configures a text generation widget with two examples. Each example includes a 'text' field representing the text used for generation and an 'example_title' providing context.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"My name is Julien and I like to\"\n  example_title: \"Julien\"\n- text: \"My name is Merve and my favorite\"\n  example_title: \"Merve\"\n```\n\n----------------------------------------\n\nTITLE: WebDataset TAR Archive Structure\nDESCRIPTION: This illustrates the directory structure inside a WebDataset TAR archive where each audio file and metadata pair shares the same file prefix.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\ntrain-0000/\n 000.flac\n 000.json\n 001.flac\n 001.json\n ...\n 999.flac\n 999.json\n```\n\n----------------------------------------\n\nTITLE: Inference for Sound Recognition with SpeechBrain\nDESCRIPTION: This snippet demonstrates how to load a pre-trained SpeechBrain model from the Hugging Face Hub for sound recognition and perform inference on an audio file. It utilizes the `EncoderClassifier` class and its `from_hparams` method to load the model. The `classify_file` method is then used to classify the audio file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/speechbrain.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torchaudio\nfrom speechbrain.pretrained import EncoderClassifier\n\nclassifier = EncoderClassifier.from_hparams(\n    source=\"speechbrain/urbansound8k_ecapa\"\n)\nout_prob, score, index, text_lab = classifier.classify_file('speechbrain/urbansound8k_ecapa/dog_bark.wav')\n```\n\n----------------------------------------\n\nTITLE: Install llama.cpp with brew\nDESCRIPTION: This command installs llama.cpp using the Homebrew package manager on macOS or Linux.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/gguf-llamacpp.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install llama.cpp\n```\n\n----------------------------------------\n\nTITLE: Configure Relative Path Audio Input YAML\nDESCRIPTION: This YAML configures a widget input using a relative file path for an audio file located within the model repository.  It uses `src` to point to a file or nested file inside the repo.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nwidget:\n  - src: sample1.flac\n    example_title: Custom Speech Sample 1\n```\n\n----------------------------------------\n\nTITLE: Markdown for Visual Examples\nDESCRIPTION: This Markdown snippet shows how to include visual examples directly on your model page using the `<Gallery>` card component for image or video generation models. Visual examples provide immediate insight into your model's capabilities.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-release-checklist.md#_snippet_6\n\nLANGUAGE: Markdown\nCODE:\n```\n<Gallery>\n![Example 1](./images/example1.png)\n![Example 2](./images/example2.png)\n</Gallery>\n```\n\n----------------------------------------\n\nTITLE: Reading CSV File with Polars\nDESCRIPTION: Reads a CSV file from Hugging Face using Polars. The `read_csv` function is used to load the file. It assumes the `pl` alias is defined for the `polars` library. This snippet demonstrates reading the file with default configurations.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-file-formats.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npl.read_csv(\"hf://datasets/lhoestq/demo1/data/train.csv\")\n```\n\n----------------------------------------\n\nTITLE: OAuth Authorization URL Example with orgIds Parameter\nDESCRIPTION: This snippet demonstrates the structure of an OAuth authorization URL for initiating the 'Sign in with Hugging Face' flow. It shows how to include the `client_id`, `redirect_uri`, `scope`, and `state` parameters and the `orgIds` parameter to request access to a specific organization. Replace `CLIENT_ID`, `REDIRECT_URI`, and `ORG_ID` with your application's values and `STATE` with a unique session identifier.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/oauth.md#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<a href=\"https://huggingface.co/oauth/authorize?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&scope=openid%20profile&state=STATE&orgIds=ORG_ID\">Sign in with Hugging Face</a>\n```\n\n----------------------------------------\n\nTITLE: Sentiment Analysis Pipeline in JavaScript\nDESCRIPTION: This JavaScript snippet demonstrates how to use the `@huggingface/transformers` library to perform sentiment analysis using a pipeline. It initializes a pipeline for sentiment analysis and then runs it on the input text 'I love transformers!'. The code requires the `@huggingface/transformers` package to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/transformers-js.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { pipeline } from '@huggingface/transformers';\n\n// Allocate a pipeline for sentiment-analysis\nlet pipe = await pipeline('sentiment-analysis');\n\nlet out = await pipe('I love transformers!');\n// [{'label': 'POSITIVE', 'score': 0.999817686}]\n```\n\n----------------------------------------\n\nTITLE: Cloning a model repository using Git\nDESCRIPTION: Clones a model repository from the Hugging Face Hub using Git. This method treats the model repository as a standard Git repository, enabling version control and collaborative workflows. Requires Git and Git LFS to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-downloading.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone git@hf.co:<MODEL ID> # example: git clone git@hf.co:bigscience/bloom\n```\n\n----------------------------------------\n\nTITLE: Dataset Repository with Metadata CSV File\nDESCRIPTION: This snippet shows a dataset repository that includes a metadata.csv file alongside the video files.  The metadata file provides additional information (e.g., captions) associated with each video, linking video files with their descriptions for tasks like video generation or object detection.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n train\n     1.mp4\n     2.mp4\n     3.mp4\n     4.mp4\n     metadata.csv\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Classification Inference with cURL\nDESCRIPTION: This snippet demonstrates how to use the curl command to perform zero-shot classification using the Inference API. It sends a POST request with the input text and candidate labels to the API endpoint and prints the response to the console. This is useful for quick testing and integration in shell scripts.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/tasks/zero-shot-classification.md#_snippet_0\n\nLANGUAGE: curl\nCODE:\n```\ncurl -X POST -H \"Authorization: Bearer YOUR_API_TOKEN\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"inputs\": \"The movie was great!\", \"parameters\": {\"candidate_labels\": [\"positive\", \"negative\"]}}' \\\n\"https://api-inference.huggingface.co/models/facebook/bart-large-mnli\"\n```\n\n----------------------------------------\n\nTITLE: WebDataset Repository Structure\nDESCRIPTION: This shows the file structure of a repository using the WebDataset format with multiple TAR archives. The usual size per archive is generally around 1GB.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n train-0000.tar\n train-0001.tar\n ...\n train-1023.tar\n```\n\n----------------------------------------\n\nTITLE: Installing PaddleNLP\nDESCRIPTION: This command installs the PaddleNLP library using pip. The -U flag ensures that PaddleNLP is upgraded to the latest version if it is already installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/paddlenlp.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\npip install -U paddlenlp\n```\n\n----------------------------------------\n\nTITLE: Cerebras Chat Completion (LLM) InferenceSnippet\nDESCRIPTION: This InferenceSnippet configures Cerebras as the provider for Chat Completion (LLM) tasks. It specifies the pipeline as text-generation and maps the model 'meta-llama/Llama-3.3-70B-Instruct' to Cerebras' 'llama-3.3-70b' model. The `conversational` prop enables conversational behavior.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/cerebras.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"cerebras\":{\"modelId\":\"meta-llama/Llama-3.3-70B-Instruct\",\"providerModelId\":\"llama-3.3-70b\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Image Segmentation Widget YAML Configuration\nDESCRIPTION: This snippet configures an image segmentation widget. Each example includes the 'src' field representing the image URL and the 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_20\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\n  example_title: Football Match\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\n  example_title: Airport\n```\n\n----------------------------------------\n\nTITLE: Setting Label Studio Secrets - Postgres Database\nDESCRIPTION: These environment variables configure the connection to an external Postgres database for persistent storage of Label Studio configurations and annotations.  Setting these as secrets prevents exposing database credentials. Requires a Space restart for changes to apply.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-label-studio.md#_snippet_1\n\nLANGUAGE: N/A\nCODE:\n```\n`DJANGO_DB`: `default`\n`POSTGRE_NAME`: `<YOUR_POSTGRES_DATABASE_NAME>`\n`POSTGRE_USER`: `<YOUR_POSTGRES_USERNAME>`\n`POSTGRE_PASSWORD`: `<YOUR_POSTGRES_PASSWORD>`\n`POSTGRE_HOST`: `<YOUR_POSTGRES_HOST>`\n`POSTGRE_PORT`: `<YOUR_POSTGRES_PORT>`\n`STORAGE_PERSISTENCE`: `1`\n```\n\n----------------------------------------\n\nTITLE: Setting Label Studio Secrets - Amazon S3\nDESCRIPTION: These environment variables configure Label Studio to use Amazon S3 for cloud storage, allowing the storage of labeling tasks and data items. Requires specifying the access key, secret key, bucket name, region, and folder. Setting these as secrets is highly recommended.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-label-studio.md#_snippet_2\n\nLANGUAGE: N/A\nCODE:\n```\n`STORAGE_TYPE`: `s3`\n`STORAGE_AWS_ACCESS_KEY_ID`: `<YOUR_ACCESS_KEY_ID>`\n`STORAGE_AWS_SECRET_ACCESS_KEY`: `<YOUR_SECRET_ACCESS_KEY>`\n`STORAGE_AWS_BUCKET_NAME`: `<YOUR_BUCKET_NAME>`\n`STORAGE_AWS_REGION_NAME`: `<YOUR_BUCKET_REGION>`\n`STORAGE_AWS_FOLDER`: ``\n```\n\n----------------------------------------\n\nTITLE: Listing Available Adapters Programmatically\nDESCRIPTION: This code snippet shows how to use the `list_adapters` function to programmatically find available adapter models on the Hugging Face Hub. It specifies the model name and source (either AdapterHub \"ah\", Hugging Face Hub \"hf\", or both if `source` is None). The returned `adapter_infos` variable contains information about the found adapters.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/adapters.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom adapters import list_adapters\n\n# source can be \"ah\" (AdapterHub), \"hf\" (hf.co) or None (for both, default)\nadapter_infos = list_adapters(source=\"hf\", model_name=\"FacebookAI/roberta-base\")\n```\n\n----------------------------------------\n\nTITLE: Cloning a Hugging Face Model Repository with SSH\nDESCRIPTION: This command clones a Hugging Face model repository to your local machine using the git CLI over SSH. You need to have added your SSH key to your Hugging Face profile to use this command. The `cd` command navigates into the newly cloned directory.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-getting-started.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@hf.co:<your-username>/<your-model-name>\ncd <your-model-name>\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face CLI\nDESCRIPTION: This command allows you to log in to your Hugging Face account via the command line interface, enabling access to private repositories and other authenticated features.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-distilabel.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Push Argilla Dataset Configuration to Hub\nDESCRIPTION: This snippet shows how to push only the configuration (settings) of an Argilla dataset to the Hugging Face Hub, excluding the records. The `with_records` parameter is set to `False` in the `to_hub` method.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-argilla.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset.to_hub(repo_id=\"<repo_id>\", with_records=False)\n```\n\n----------------------------------------\n\nTITLE: Updating Collection Item Payload (PATCH)\nDESCRIPTION: This payload is used when updating an item within a collection. It allows you to modify the item's position in the collection and add a note to the item.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_19\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"position\": 0,\n    \"note\": \"Here is the model I trained on ...\",\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained ConvTasNet Model with Asteroid\nDESCRIPTION: This snippet demonstrates how to load a pre-trained ConvTasNet model from the Hugging Face Hub using the `from_pretrained` method of the `ConvTasNet` class in the `asteroid.models` module. The model is identified by its repository name ('mpariente/ConvTasNet_WHAM_sepclean').\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/asteroid.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom asteroid.models import ConvTasNet\nmodel = ConvTasNet.from_pretrained('mpariente/ConvTasNet_WHAM_sepclean')\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with SpanMarker\nDESCRIPTION: This snippet demonstrates how to perform inference using a loaded SpanMarker model with the `model.predict` method.  The input text is passed as a string, and the method returns a list of predicted entities with their labels and scores.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/span_marker.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel.predict(\"Amelia Earhart flew her single engine Lockheed Vega 5B across the Atlantic to Paris.\")\n```\n\n----------------------------------------\n\nTITLE: YAML Metadata for Quantized Model\nDESCRIPTION: This YAML snippet shows how to use the `base_model` metadata field on the quantized model cards, linking back to the original model.  This informs users about the relationship between the original and quantized versions.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-release-checklist.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\n---\nbase_model: username/original-model\nbase_model_relation: quantized\n---\n```\n\n----------------------------------------\n\nTITLE: Audio Classification Widget YAML Configuration\nDESCRIPTION: This snippet configures an audio classification widget.  Each example includes a 'src' field representing the URL of the audio file and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_16\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n  example_title: Librispeech sample 1\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\n  example_title: Librispeech sample 2\n```\n\n----------------------------------------\n\nTITLE: Dataset Structure with Train/Test/Validation Splits (File Names)\nDESCRIPTION: Illustrates how to structure a dataset repository with separate files for train, test, and validation splits, named accordingly.  File names must contain split names like `train`, `test`, or `validation` delimited by non-word characters.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-file-names-and-splits.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n README.md\n train.csv\n test.csv\n validation.csv\n```\n\n----------------------------------------\n\nTITLE: Embedding Space with Web Components - HTML\nDESCRIPTION: This snippet shows how to embed a Gradio-based Hugging Face Space using Web Components. It assumes that the Gradio JS library has been imported. Replace the `src` attribute with the URL of the Space. Web Components automatically adjust to the webpage, so `width` and `height` configuration are not needed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-embed.md#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<gradio-app src=\"https://<space-subdomain>.hf.space\"></gradio-app>\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet File with Polars\nDESCRIPTION: Reads a Parquet file from Hugging Face using Polars.  The `read_parquet` function is used to load the file. Parquet is the preferred format as it stores schema and type information within the file.  It assumes the `pl` alias is defined for the `polars` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-file-formats.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npl.read_parquet(\"hf://datasets/roneneldan/TinyStories/data/train-00000-of-00004-2d5a1467fff1081b.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Audio Classification with Splits\nDESCRIPTION: This snippet showcases using a nested directory structure for audio classification with train and test splits. Each split contains subdirectories representing the audio classes.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n test\n  cat\n   2.wav\n  dog\n      4.wav\n train\n     cat\n      1.wav\n     dog\n         3.wav\n```\n\n----------------------------------------\n\nTITLE: Image-to-Image Widget YAML Configuration\nDESCRIPTION: This snippet configures an image-to-image widget. The example includes the 'src' field for the input image URL and an optional 'prompt' field for text guidance if the underlying model supports it.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_21\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/canny-edge.jpg\n  prompt: Girl with Pearl Earring # `prompt` field is optional in case the underlying model supports text guidance\n```\n\n----------------------------------------\n\nTITLE: Table Question Answering Request Example (JSON)\nDESCRIPTION: This JSON snippet demonstrates a request to the table-question-answering task of the Inference Toolkit API. The input includes a question and a table with its associated headers and rows.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": {\n    \"query\": \"How many stars does the transformers repository have?\",\n    \"table\": {\n      \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n      \"Stars\": [\"36542\", \"4512\", \"3934\"],\n      \"Contributors\": [\"651\", \"77\", \"34\"],\n      \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Widget Metadata for Gallery - YAML\nDESCRIPTION: This YAML snippet shows how to define the `widget` metadata in a Model Card to include text prompts and associated image URLs.  The `<Gallery />` component will use this metadata to display the media. The `text` field contains the prompt, and the `output` field contains a URL to the image.  This is a prerequisite to using the Gallery component.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards-components.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n  - text: a girl wandering through the forest\n    output:\n      url: images/6CD03C101B7F6545EB60E9F48D60B8B3C2D31D42D20F8B7B9B149DD0C646C0C2.jpeg\n  - text: a tiny witch child\n    output:\n      url: images/7B482E1FDB39DA5A102B9CD041F4A2902A8395B3835105C736C5AD9C1D905157.jpeg\n  - text: an artist leaning over to draw something\n    output:\n      url: images/7CCEA11F1B74C8D8992C47C1C5DEA9BD6F75940B380E9E6EC7D01D85863AF718.jpeg\n```\n\n----------------------------------------\n\nTITLE: WebDataset Format Structure\nDESCRIPTION: This snippet shows a directory structure for video datasets using the WebDataset format.  It consists of TAR archives containing videos and their metadata, optimized for streaming and large-scale training.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_12\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n train-0000.tar\n train-0001.tar\n ...\n train-1023.tar\n```\n\n----------------------------------------\n\nTITLE: Token Classification Widget YAML Configuration\nDESCRIPTION: This snippet configures a token classification widget with two examples. Each example contains a 'text' field for the input text and an 'example_title' field.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"My name is Sylvain and I live in Paris\"\n  example_title: \"Parisian\"\n- text: \"My name is Sarah and I live in London\"\n  example_title: \"Londoner\"\n```\n\n----------------------------------------\n\nTITLE: Moving PyTorch Model to GPU\nDESCRIPTION: This code snippet demonstrates how to move a PyTorch model to the GPU using the `.to()` method. This is necessary to ensure that computations are performed on the GPU and not on the CPU. It requires a PyTorch model to be loaded beforehand.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-gpus.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmodel = load_pytorch_model()\nmodel = model.to(\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Parquet Feature Types\nDESCRIPTION: This shows a YAML configuration snippet specifying the feature types for audio and caption columns in a Parquet dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\ndataset_info:\n  features:\n  - name: audio\n    dtype: audio\n  - name: caption\n    dtype: string\n```\n\n----------------------------------------\n\nTITLE: Adding Gallery Component to Model Card - Markdown\nDESCRIPTION: This Markdown snippet demonstrates how to add the `<Gallery />` component to your Model Card. This component automatically displays the media specified in the `widget` metadata. The component dynamically renders the gallery using the URLs provided in the `widget` section. This is the final step to showcase the generated media.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards-components.md#_snippet_1\n\nLANGUAGE: Markdown\nCODE:\n```\n<Gallery />\n```\n\n----------------------------------------\n\nTITLE: Loading FiftyOne Datasets from Hub\nDESCRIPTION: This code snippet demonstrates how to load a FiftyOne dataset from the Hugging Face Hub using the `load_from_hub()` function.  It imports the necessary modules from FiftyOne and then loads the dataset specified by its `repo_id`. It also launches the FiftyOne app to visualize the dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-fiftyone.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport fiftyone as fo\nfrom fiftyone.utils import load_from_hub\n\n## load from the hub\ndataset = load_from_hub(\"Voxel51/VisDrone2019-DET\")\n\n## visualize in app\nsession = fo.launch_app(dataset)\n```\n\n----------------------------------------\n\nTITLE: Parsing GGUF Metadata with @huggingface/gguf in JavaScript\nDESCRIPTION: This code snippet demonstrates how to use the `@huggingface/gguf` package to parse metadata and tensor information from a remotely hosted GGUF file on the Hugging Face Hub. It imports the `gguf` function, specifies the URL of the GGUF file, and then uses `await gguf(URL_LLAMA)` to retrieve the metadata and tensor information.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/gguf.md#_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { gguf } from \"@huggingface/gguf\";\n// remote GGUF file from https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF\nconst URL_LLAMA = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/191239b/llama-2-7b-chat.Q2_K.gguf\";\nconst { metadata, tensorInfos } = await gguf(URL_LLAMA);\n```\n\n----------------------------------------\n\nTITLE: Sharing PaddleNLP models to Hugging Face Hub\nDESCRIPTION: This snippet demonstrates how to save a PaddleNLP model and tokenizer to the Hugging Face Hub using the `save_to_hf_hub` method.  It requires the model and tokenizer to be initialized first and then specifies the repository ID where the model and tokenizer should be saved.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/paddlenlp.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom paddlenlp.transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"PaddlePaddle/ernie-1.0-base-zh\", from_hf_hub=True)\nmodel = AutoModelForMaskedLM.from_pretrained(\"PaddlePaddle/ernie-1.0-base-zh\", from_hf_hub=True)\n\ntokenizer.save_to_hf_hub(repo_id=\"<my_org_name>/<my_repo_name>\")\nmodel.save_to_hf_hub(repo_id=\"<my_org_name>/<my_repo_name>\")\n```\n\n----------------------------------------\n\nTITLE: Listing Collections Parameters (GET)\nDESCRIPTION: These parameters are used when listing collections. They include the owner, item, sort, limit and query for filtering. Limit has a maximum value of 100.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_16\n\nLANGUAGE: JavaScript\nCODE:\n```\nparams = {\n    \"owner\": \"TheBloke\",\n    \"item\": \"models/teknium/OpenHermes-2.5-Mistral-7B\",\n    \"sort\": \"lastModified\",\n    \"limit\" : 1,\n}\n```\n\n----------------------------------------\n\nTITLE: Dataset Repository Structure with Mixed Video Formats\nDESCRIPTION: This snippet illustrates a dataset repository containing video files in various formats (MP4, MOV, AVI) within a subdirectory. The Hugging Face Hub supports multiple video formats within the same dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n videos\n     1.mp4\n     2.mov\n     3.avi\n```\n\n----------------------------------------\n\nTITLE: Verifying PyTorch CUDA Availability\nDESCRIPTION: This code snippet checks if CUDA is available for PyTorch and prints the CUDA device name. It's used to verify that PyTorch is using the GPU after a GPU upgrade. It requires the PyTorch library to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-gpus.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nprint(f\"Is CUDA available: {torch.cuda.is_available()}\")\n# True\nprint(f\"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n# Tesla T4\n```\n\n----------------------------------------\n\nTITLE: Adding Users to Resource Group Payload (POST)\nDESCRIPTION: This payload is used when adding users to an existing resource group. It includes a list of users to add, along with their respective roles (admin, write, or read).\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_13\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"users\": [\n        {\n            \"user\": \"username\",\n            \"role\": \"admin\" // or \"write\" or \"read\"\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Multiple Base Models in YAML\nDESCRIPTION: This YAML snippet shows how to specify multiple base models for a model that is a merge of two or more existing models in the model card metadata.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nbase_model:\n- Endevor/InfinityRP-v1-7B\n- l3utterfly/mistral-7b-v0.1-layla-v4\n```\n\n----------------------------------------\n\nTITLE: Malicious Pickle File Contents (Hexyl Command)\nDESCRIPTION: This shows the hex representation of the malicious pickle created with fickling. It is useful for inspecting the instructions.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# hexyl payload.pkl\n\n00000000 63 5f 5f 62 75 69 6c 74  69 6e 5f 5f 0a 65 78 65 c__builtin___exe\n00000010 63 0a 28 56 70 72 69 6e  74 28 22 79 6f 75 27 76 c_(Vprint(\"you'v\n00000020 65 20 62 65 65 6e 20 70  77 6e 64 20 21 22 29 0a e been pwned !\")\n00000030 74 52 80 04 95 20 00 00  00 00 00 00 00 8c 1c 6d _tR  0000000m\n00000040 79 20 66 72 69 65 6e 64  20 6e 65 65 64 73 20 74 y friend needs t\n00000050 6f 20 6b 6e 6f 77 20 74  68 69 73 94 2e       o know this.  \n\n```\n\n----------------------------------------\n\nTITLE: Dataset Structure with Train/Test/Validation Splits (Directory Names)\nDESCRIPTION: Shows how to structure a dataset repository by placing data files into directories named `train`, `test`, and `validation`. Each directory contains the data files specific to that split.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-file-names-and-splits.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n README.md\n data/\n     train/\n        data.csv\n     test/\n        more_data.csv\n     validation/\n         even_more_data.csv\n```\n\n----------------------------------------\n\nTITLE: Uploading ESPnet Model to Hugging Face Hub\nDESCRIPTION: This bash script demonstrates how to upload an ESPnet model to a Hugging Face repository. It uses the `run.sh` script with specific parameters to skip the local model execution stages up to stage 15 and then upload the model to the specified Hugging Face repository. You will need to provide your username and repository name in the `hf_repo` parameter. Ensure you have the necessary credentials configured for uploading to the Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/espnet.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n----------------------------------------\n\nTITLE: Customize Gated Dataset Request Form YAML\nDESCRIPTION: This YAML snippet demonstrates how to customize the request form for a gated dataset by adding extra gated fields such as company, country, a specific date, a select field for usage purpose, and a checkbox for non-commercial use agreement.  It also includes an extra gated prompt that displays a message to the user.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n---\nextra_gated_prompt: \"You agree to not use the dataset to conduct experiments that cause harm to human subjects.\"\nextra_gated_fields:\n  Company: text\n  Country: country\n  Specific date: date_picker\n  I want to use this dataset for:\n    type: select\n    options: \n      - Research\n      - Education\n      - label: Other\n        value: other\n  I agree to use this dataset for non-commercial use ONLY: checkbox\n---\n```\n\n----------------------------------------\n\nTITLE: Upgrade ipywidgets in SageMaker Studio\nDESCRIPTION: This snippet upgrades the ipywidgets library in a SageMaker Studio environment. It then restarts the kernel to apply the changes. The `%%capture` cell magic suppresses output during installation.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nimport IPython\n!conda install -c conda-forge ipywidgets -y\nIPython.Application.instance().kernel.do_shutdown(True)\n```\n\n----------------------------------------\n\nTITLE: Get Model Inference Status (Python)\nDESCRIPTION: This snippet demonstrates how to retrieve the inference status of a specific model using the `huggingface_hub` library in Python. It utilizes the `model_info` function with the `expand` parameter set to \"inference\". The code outputs the value of the `inference` attribute, which can be 'warm' or None.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import model_info\n\n>>> info = model_info(\"google/gemma-3-27b-it\", expand=\"inference\")\n>>> info.inference\n'warm'\n```\n\n----------------------------------------\n\nTITLE: Listing Spaces Parameters in JavaScript\nDESCRIPTION: Defines the parameters that can be used when listing Spaces via the Hugging Face Hub API. These parameters allow filtering, sorting, and limiting the results of the space listing API call. They correspond to the arguments available in the `huggingface_hub.list_spaces()` function.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nparams = {\n    \"search\":\"search\",\n    \"author\":\"author\",\n    \"filter\":\"filter\",\n    \"sort\":\"sort\",\n    \"direction\":\"direction\",\n    \"limit\":\"limit\",\n    \"full\":\"full\",\n    \"config\":\"config\"\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Task to API Validation in Community Inference API\nDESCRIPTION: This snippet shows how to add a new task to the API validation in the Community Inference API. This code validates that the inference input is valid for the given task. The file location is api_inference_community/validation.py in the api-inference-community repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-tasks.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Grant Access to User via API\nDESCRIPTION: This API endpoint grants a specific user access to a gated dataset. It requires a token with write access and a payload specifying the username to grant access to.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md#_snippet_4\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /api/datasets/{repo_id}/user-access-request/grant\nHeaders: {\"authorization\":  \"Bearer $token\"}\nPayload: {\"user\": \"username\"}\n```\n\n----------------------------------------\n\nTITLE: YAML for New Model Version\nDESCRIPTION: This YAML snippet shows how to specify the updated version of the current model on the older version's model card, so that it [displays a banner] linking directly to this updated version.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-release-checklist.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\n---\nnew_version: username/updated-model\n---\n```\n\n----------------------------------------\n\nTITLE: Specifying a custom license with YAML in Model Card\nDESCRIPTION: This code snippet demonstrates how to specify a custom license in the model card metadata using YAML. It includes the `license` field set to `other`, along with `license_name` and `license_link` to provide details about the custom license.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlicense: other\nlicense_name: coqui-public-model-license\nlicense_link: https://coqui.ai/cpml\n---\n```\n\n----------------------------------------\n\nTITLE: Generate Documentation using pnpm\nDESCRIPTION: This command generates the documentation for the Hugging Face Hub project using the pnpm package manager. It triggers the documentation generation process, creating the necessary output files for viewing and deploying the documentation.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/scripts/inference-providers/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npnpm run generate\n```\n\n----------------------------------------\n\nTITLE: Provider Helper Implementation (Python)\nDESCRIPTION: This Python code snippet defines a base class `MyNewProviderTaskProviderHelper` for creating provider-specific helpers. It includes methods for defining parameters, handling responses, preparing headers, routes, and payloads as dictionaries or bytes. At least one of `_prepare_payload_as_dict` or `_prepare_payload_as_bytes` must be overwritten.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Any, Dict, Optional, Union\n\nfrom ._common import TaskProviderHelper\n\n\nclass MyNewProviderTaskProviderHelper(TaskProviderHelper):\n    def __init__(self):\n        \"\"\"Define high-level parameters.\"\"\"\n        super().__init__(provider=..., base_url=..., task=...)\n\n    def get_response(\n        self,\n        response: Union[bytes, Dict],\n        request_params: Optional[RequestParameters] = None,\n    ) -> Any:\n        \"\"\"\n        Return the response in the expected format.\n\n        Override this method in subclasses for customized response handling.\"\"\"\n        return super().get_response(response)\n\n    def _prepare_headers(self, headers: Dict, api_key: str) -> Dict:\n        \"\"\"Return the headers to use for the request.\n\n        Override this method in subclasses for customized headers.\n        \"\"\"\n        return super()._prepare_headers(headers, api_key)\n\n    def _prepare_route(self, mapped_model: str, api_key: str) -> str:\n        \"\"\"Return the route to use for the request.\n\n        Override this method in subclasses for customized routes.\n        \"\"\"\n        return super()._prepare_route(mapped_model)\n\n    def _prepare_payload_as_dict(self, inputs: Any, parameters: Dict, mapped_model: str) -> Optional[Dict]:\n        \"\"\"Return the payload to use for the request, as a dict.\n\n        Override this method in subclasses for customized payloads.\n        Only one of `_prepare_payload_as_dict` and `_prepare_payload_as_bytes` should return a value.\n        \"\"\"\n        return super()._prepare_payload_as_dict(inputs, parameters, mapped_model)\n\n    def _prepare_payload_as_bytes(\n        self, inputs: Any, parameters: Dict, mapped_model: str, extra_payload: Optional[Dict]\n    ) -> Optional[bytes]:\n        \"\"\"Return the body to use for the request, as bytes.\n\n        Override this method in subclasses for customized body data.\n        Only one of `_prepare_payload_as_dict` and `_prepare_payload_as_bytes` should return a value.\n        \"\"\"\n        return super()._prepare_payload_as_bytes(inputs, parameters, mapped_model, extra_payload)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with a SetFit Model\nDESCRIPTION: This code snippet demonstrates how to use a loaded SetFit model to predict the class of a given input text using the `model.predict` method.  The model outputs a list of predicted labels.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/setfit.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel.predict(\"Amelia Earhart flew her single engine Lockheed Vega 5B across the Atlantic to Paris.\")\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration for Manual Splits and Subsets\nDESCRIPTION: This YAML configuration defines a subset named \"benchmark\" with a \"test\" split. This allows manual specification of which files belong to which splits and subsets, overriding automatic detection. It's useful for datasets where file names don't directly correspond to split names or for defining multiple subsets.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-data-files-configuration.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nconfigs:\n- config_name: benchmark\n  data_files:\n  - split: test\n    path: benchmark.csv\n```\n\n----------------------------------------\n\nTITLE: Updating Collection Metadata Payload (PATCH)\nDESCRIPTION: This payload is used when updating the metadata of a collection. It allows updating the title, description, privacy, position and theme of the collection.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_17\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"title\": \"My cool models\",\n    \"description\": \"Here is a shortlist of models I've trained.\",\n    \"private\": false,\n    \"position\": 0, // position of the collection on your profile\n    \"theme\": \"green\",\n}\n```\n\n----------------------------------------\n\nTITLE: Loading spaCy model after installation\nDESCRIPTION: This Python code snippet demonstrates two ways to load a spaCy model after it has been installed. The first method uses `spacy.load()` and the second imports the model directly as a module and uses its `load()` method.  It assumes the model `en_core_web_sm` is installed. The output is a loaded spaCy pipeline object.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spacy.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Using spacy.load().\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Importing as module.\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n```\n\n----------------------------------------\n\nTITLE: Metadata JSONL Example\nDESCRIPTION: Shows the format for a metadata.jsonl file, another method for associating image files with metadata in a line-delimited JSON format.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_6\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"file_name\": \"1.jpg\",\"text\": \"a drawing of a green pokemon with red eyes\"}\n{\"file_name\": \"2.jpg\",\"text\": \"a green and yellow toy with a red nose\"}\n{\"file_name\": \"3.jpg\",\"text\": \"a red and white ball with an angry look on its face\"}\n{\"file_name\": \"4.jpg\",\"text\": \"a cartoon ball with a smile on it's face\"}\n```\n\n----------------------------------------\n\nTITLE: Querying Parquet files and including filename\nDESCRIPTION: This snippet demonstrates querying Parquet files and including the filename column in DuckDB. It selects the `city`, `country`, and `filename` columns from the Parquet files located under the specified Hugging Face dataset path (auto-converted to Parquet). The `filename = true` parameter adds a `filename` column indicating the source file for each row, and limits the result to 3 rows. It requires the Hugging Face dataset at the specified path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-select.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nSELECT city, country, filename FROM read_parquet('hf://datasets/jamescalam/world-cities-geo@~parquet/default/**/*.parquet', filename = true) LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Example updatedConfig Webhook Payload (XetEnabled)\nDESCRIPTION: This JSON payload provides an example of the `updatedConfig` property when the event scope is `repo.config`. In this case it shows the updated config when the xetEnabled property is modified.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"xetEnabled\": true,\n}\n\n```\n\n----------------------------------------\n\nTITLE: Previewing Documentation Locally (Bash)\nDESCRIPTION: This snippet demonstrates how to preview the Hugging Face Hub documentation locally using the `doc-builder` tool. It involves installing `hf-doc-builder` and other dependencies, followed by executing the `doc-builder preview` command with the appropriate parameters. This allows contributors to verify their changes before submitting a pull request.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# install doc-builder (if not done already)\npip install hf-doc-builder\n\n# you may also need to install some extra dependencies\npip install black watchdog\n\n# run `doc-builder preview` cmd\ndoc-builder preview hub {YOUR_PATH}/hub-docs/docs/hub/ --not_python_module\n```\n\n----------------------------------------\n\nTITLE: Certificate Format for Hugging Face SSO\nDESCRIPTION: This code snippet shows the required format for the public certificate when configuring SSO on Hugging Face. The certificate obtained from Azure needs to be wrapped with -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- markers.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-sso-azure-saml.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-----BEGIN CERTIFICATE-----\n{certificate}\n-----END CERTIFICATE-----\n```\n\n----------------------------------------\n\nTITLE: Audio Files in Subdirectory\nDESCRIPTION: This snippet shows how to organize audio files within a subdirectory named 'audio'. This maintains a cleaner root directory when other files might be present.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n audio\n     1.wav\n     2.wav\n     3.wav\n     4.wav\n```\n\n----------------------------------------\n\nTITLE: Customizing Request Form - YAML\nDESCRIPTION: This YAML snippet demonstrates how to customize the request form for a gated model. It includes fields for company name, country, a specific date, a select dropdown for model usage, and a checkbox for agreeing to non-commercial use. It also allows personalization of the message displayed to the user.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-gated.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\n---\nextra_gated_prompt: \"You agree to not use the model to conduct experiments that cause harm to human subjects.\"\nextra_gated_fields:\n  Company: text\n  Country: country\n  Specific date: date_picker\n  I want to use this model for:\n    type: select\n    options: \n      - Research\n      - Education\n      - label: Other\n        value: other\n  I agree to use this model for non-commercial use ONLY: checkbox\n---\n```\n\n----------------------------------------\n\nTITLE: Modifying Default Text - YAML\nDESCRIPTION: This YAML snippet shows how to modify the default text in the gate heading, description, and button content of a gated model. It customizes the user experience by changing the prompts and messages displayed during the access request process.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-gated.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n---\nextra_gated_heading: \"Acknowledge license to accept the repository\"\nextra_gated_description: \"Our team may take 2-3 days to process your request\"\nextra_gated_button_content: \"Acknowledge license\"\n---\n```\n\n----------------------------------------\n\nTITLE: Updating Resource Group Repository Payload (POST)\nDESCRIPTION: This payload is used to update the resource group associated with a repository.  The resourceGroupId can be set to null to remove the repository from any resource group.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_14\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"resourceGroupId\": \"6771d4700000000000000000\" // (allow `null` for removing the repo's resource group)\n}\n```\n\n----------------------------------------\n\nTITLE: Hardware Requirements Placeholder\nDESCRIPTION: This snippet represents a placeholder for the minimum hardware requirements of the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n`hardware_requirements`\n```\n\n----------------------------------------\n\nTITLE: Analyzing Subject Proportions in DuckDB\nDESCRIPTION: This query calculates the proportion of questions based on the 'subject' column in the MMLU dataset. It uses the `BAR` function to create a bar representation of the counts for each subject, ordered by count in descending order.  This provides a visual overview of the subject distribution.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-sql.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT \n    subject, \n    COUNT(*) AS counts, \n    BAR(COUNT(*), 0, (SELECT COUNT(*) FROM 'hf://datasets/cais/mmlu/all/test-*.parquet')) AS percentage \nFROM \n    'hf://datasets/cais/mmlu/all/test-*.parquet' \nGROUP BY \n    subject \nORDER BY \n    counts DESC;\n```\n\n----------------------------------------\n\nTITLE: Deleting a Mapping Item\nDESCRIPTION: This HTTP DELETE request is used to remove a model mapping item from the database for a specific provider. The `hfModel` parameter specifies the model on the Hugging Face Hub to be deleted.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_3\n\nLANGUAGE: http\nCODE:\n```\nDELETE /api/partners/{provider}/models?hfModel=namespace/model-name\n```\n\n----------------------------------------\n\nTITLE: Model Card Authors Placeholder\nDESCRIPTION: This snippet represents a placeholder for listing the authors of the model card.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n`model_card_authors`\n```\n\n----------------------------------------\n\nTITLE: Glossary Placeholder\nDESCRIPTION: This snippet represents a placeholder for a glossary of terms related to the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n`glossary`\n```\n\n----------------------------------------\n\nTITLE: Getting Dataset Information Parameters in JavaScript\nDESCRIPTION: Defines the parameters that can be used when getting information for a specific dataset via the Hugging Face Hub API. The 'full' parameter controls whether to fetch all dataset data, including tags and files. Corresponds to arguments in `huggingface_hub.dataset_info(repo_id, revision)`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nparams = {\"full\": \"full\"}\n```\n\n----------------------------------------\n\nTITLE: Webhook Event Filtering (Python)\nDESCRIPTION: This Python code filters webhook events based on specific criteria. It checks the `action`, `scope`, `repo.name`, and `repo.type` fields of the payload. If the event doesn't match the expected values (update on the repo content of the input dataset), the function returns early and does nothing. This helps to ensure that only relevant events trigger further processing. It depends on the Pydantic models for payload parsing and `config.input_dataset` for the dataset name.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-auto-retrain.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [...]\n\n@app.post(\"/webhook\")\nasync def post_webhook(\n\t# [...]\n\tpayload: WebhookPayload,\n\t# ^ Pydantic model defining the payload format\n):\n\t# [...]\n\tif not (\n\t\tpayload.event.action == \"update\"\n\t\tand payload.event.scope.startswith(\"repo.content\")\n\t\tand payload.repo.name == config.input_dataset\n\t\tand payload.repo.type == \"dataset\"\n\t):\n\t\t# no-op if the payload does not match our expectations\n\t\treturn {\"processed\": False}\n\t#[...]\n```\n\n----------------------------------------\n\nTITLE: JSON payload structure for Inference API request - JSON\nDESCRIPTION: This is an example of the JSON payload structure sent to the Inference API. It contains the 'inputs' key, whose value is a string comprising the prompt and the comment content from the discussion.  The inference API uses this string to generate a response.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-discussion-bot.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"inputs\": PROMPT + req.body.comment.content\n}\n```\n\n----------------------------------------\n\nTITLE: Calculate Metadata Grade\nDESCRIPTION: This Python function calculates a grade, which represents the percentage of desired metadata fields that are present in a given dictionary.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef calculate_grade(desired_metadata_dictionary):\n    # [...]\n    return round(score, 2)\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Image Classification Widget YAML Configuration\nDESCRIPTION: This snippet configures a zero-shot image classification widget. The example includes 'src' which is the image URL, 'candidate_labels' representing possible labels, and 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_25\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n```\n\n----------------------------------------\n\nTITLE: Software Requirements Placeholder\nDESCRIPTION: This snippet represents a placeholder for the software requirements of the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n`software`\n```\n\n----------------------------------------\n\nTITLE: Loading timm Model from Hub\nDESCRIPTION: This snippet demonstrates how to load a pre-trained timm model from the Hugging Face Hub using the `timm.create_model` function. It downloads the model and instantiates it for use.  Make sure the `timm` library is installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/timm.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport timm\n\n# Loading https://huggingface.co/timm/eca_nfnet_l0\nmodel = timm.create_model(\"hf-hub:timm/eca_nfnet_l0\", pretrained=True)\n```\n\n----------------------------------------\n\nTITLE: Querying Parquet files with read_parquet function\nDESCRIPTION: This snippet demonstrates querying Parquet files using the `read_parquet` function in DuckDB. It selects all columns from the Parquet files located under the specified Hugging Face dataset path (auto-converted to Parquet) and limits the result to 3 rows. It requires the Hugging Face dataset at the specified path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-select.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nSELECT * FROM read_parquet('hf://datasets/jamescalam/world-cities-geo@~parquet/default/**/*.parquet') LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Compute Infrastructure Placeholder\nDESCRIPTION: This snippet represents a placeholder for information about the compute infrastructure used for the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n`compute_infrastructure`\n```\n\n----------------------------------------\n\nTITLE: APA Citation Placeholder\nDESCRIPTION: This snippet represents a placeholder for the APA citation of the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n`citation_apa`\n```\n\n----------------------------------------\n\nTITLE: Pull Argilla Dataset Configuration from Hub\nDESCRIPTION: This snippet shows how to pull only the configuration (settings) of an Argilla dataset from the Hugging Face Hub, excluding the records. The `with_records` parameter is set to `False` in the `from_hub` method.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-argilla.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndataset = rg.Dataset.from_hub(repo_id=\"<repo_id>\", with_records=False)\n```\n\n----------------------------------------\n\nTITLE: Specify ASR Widget Output YAML\nDESCRIPTION: This YAML shows how to specify example outputs for an automatic speech recognition (ASR) widget in the model card metadata.  It defines the output as a dictionary including the transcribed text.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nwidget:\n  - src: sample1.flac\n    output:\n      text: \"Hello my name is Julien\"\n```\n\n----------------------------------------\n\nTITLE: Relative Paths with Metadata CSV\nDESCRIPTION: This example shows a setup where the metadata file is located in a parent directory of the audio files, requiring the 'file_name' column to contain the full relative path to each audio file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n test\n     audio\n      1.wav\n      2.wav\n      3.wav\n      4.wav\n     metadata.csv\n```\n\n----------------------------------------\n\nTITLE: Install Argilla Python Package\nDESCRIPTION: This command installs or upgrades the Argilla Python package to version 2.0.0 or later. This is a necessary dependency for using the Argilla library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-argilla.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -U argilla\n```\n\n----------------------------------------\n\nTITLE: Select Embedding in DuckDB\nDESCRIPTION: This SQL query retrieves the 'embedding' column from the 'asoria/awesome-chatgpt-prompts-embeddings' dataset where the 'act' column equals 'Linux Terminal'. This serves as the base embedding for calculating similarity with other records.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-vector-similarity-search.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nFROM 'hf://datasets/asoria/awesome-chatgpt-prompts-embeddings/data/*.parquet' SELECT  embedding  WHERE act = 'Linux Terminal';\n```\n\n----------------------------------------\n\nTITLE: Specifying New Version in YAML\nDESCRIPTION: This YAML snippet shows how to specify a new version of a model in the model card metadata.  This allows users to easily find the latest version of a model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nnew_version: l3utterfly/mistral-7b-v0.1-layla-v4\n```\n\n----------------------------------------\n\nTITLE: Fetching Parquet internal schema\nDESCRIPTION: This snippet demonstrates fetching the internal schema of a Parquet file using the `parquet_schema` function in DuckDB. It excludes the file name column using `EXCLUDE (file_name)`. This provides detailed type information about each column. It requires the Hugging Face dataset at the specified path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-select.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nSELECT * EXCLUDE (file_name) FROM parquet_schema('hf://datasets/jamescalam/world-cities-geo@~parquet/default/train/0000.parquet');\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Data with Polars\nDESCRIPTION: This snippet reads a CSV file from a Hugging Face dataset using Polars and parses dates. It utilizes the `pl.read_csv` function with the dataset URL and `try_parse_dates` option.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-operations.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\n\ndf = pl.read_csv(\n    \"hf://datasets/commoncrawl/statistics/tlds.csv\",\n    try_parse_dates=True,\n)\ndf.head(3)\n```\n\n----------------------------------------\n\nTITLE: Generating a New GPG Key (Bash)\nDESCRIPTION: This snippet generates a new GPG key pair. It prompts the user to specify key type, size, validity period, user ID, and passphrase. The email address provided should match the one used in the Hugging Face account.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-gpg.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngpg --gen-key\n```\n\n----------------------------------------\n\nTITLE: Image Dataset with Train/Test Splits\nDESCRIPTION: Shows directory structure for organizing images into 'train' and 'test' splits, enabling dataset partitioning for model training and evaluation.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_3\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n train\n  1.jpg\n  2.jpg\n test\n     3.jpg\n     4.jpg\n```\n\n----------------------------------------\n\nTITLE: Install SageMaker package\nDESCRIPTION: This command upgrades the sagemaker package to the latest version using pip.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install sagemaker --upgrade\n```\n\n----------------------------------------\n\nTITLE: Call Inference API with a prompt and comment content - TypeScript\nDESCRIPTION: This code snippet defines the `INFERENCE_URL` and `PROMPT` for the BLOOM model. It constructs a JSON payload containing the prompt concatenated with the comment content from the Webhook event. This payload is then sent as a POST request to the Inference API, and the response is parsed to extract the generated text.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-discussion-bot.md#_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\n\tconst INFERENCE_URL =\n\t\t\"https://api-inference.huggingface.co/models/bigscience/bloom\";\n\tconst PROMPT = `Pretend that you are a bot that replies to discussions about machine learning, and reply to the following comment:\\n`;\n\n\tconst response = await fetch(INFERENCE_URL, {\n\t\tmethod: \"POST\",\n\t\tbody: JSON.stringify({ inputs: PROMPT + req.body.comment.content }),\n\t});\n\tif (response.ok) {\n\t\tconst output = await response.json();\n\t\tconst continuationText = output[0].generated_text.replace(\n\t\t\tPROMPT + req.body.comment.content,\n\t\t\t\"\"\n\t\t);\n\t\t...\n```\n\n----------------------------------------\n\nTITLE: Image Dataset with Image Subdirectory\nDESCRIPTION: Illustrates organizing images within a dedicated 'images' subdirectory inside the dataset repository, commonly used for clarity and organization.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_1\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n images\n     1.jpg\n     2.jpg\n     3.jpg\n     4.jpg\n```\n\n----------------------------------------\n\nTITLE: Text Generation Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Text Generation using the `Qwen/QwQ-32B` model on HF Inference. This specifies the pipeline and the relevant model ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_9\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"Qwen/QwQ-32B\",\"providerModelId\":\"Qwen/QwQ-32B\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Configuration File (JSON)\nDESCRIPTION: This JSON snippet represents a configuration file (`config.json`) used to store parameters for the auto-retraining process. It defines properties like `target_namespace`, `input_dataset`, `input_model`, and `autotrain_project_prefix`. These parameters are likely used to configure the AutoTrain project and specify the dataset and model for retraining.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-auto-retrain.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"target_namespace\": \"the namespace where the trained model should end up\",\n\t\"input_dataset\": \"the dataset on which the model will be trained\",\n\t\"input_model\": \"the base model to re-train\",\n\t\"autotrain_project_prefix\": \"A prefix for the AutoTrain project\"\n}\n```\n\n----------------------------------------\n\nTITLE: Image Dataset with Relative Paths in CSV\nDESCRIPTION: Illustrates using relative paths in the metadata.csv file when images are in a subdirectory, ensuring the 'file_name' column contains the correct path to each image.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_7\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n train\n     images\n      1.jpg\n      2.jpg\n      3.jpg\n      4.jpg\n     metadata.csv\n```\n\n----------------------------------------\n\nTITLE: Installing MLX with pip\nDESCRIPTION: This command installs the core MLX package using pip, the Python package installer.  This is an alternative installation method if the user doesn't want the Large Language Model specific features of mlx-lm.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install mlx\n```\n\n----------------------------------------\n\nTITLE: Conversational Widget YAML Configuration\nDESCRIPTION: This snippet configures a conversational widget with two examples. Each example includes a 'text' field representing the conversation text and 'example_title' providing context.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_11\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"Hey my name is Julien! How are you?\"\n  example_title: \"Julien\"\n- text: \"Hey my name is Clara! How are you?\"\n  example_title: \"Clara\"\n```\n\n----------------------------------------\n\nTITLE: Embedding Space with IFrame - HTML\nDESCRIPTION: This snippet demonstrates how to embed a Hugging Face Space using an IFrame.  The `src` attribute should be replaced with the actual URL of the Space.  Adjust `width` and `height` to fit the design of the containing webpage. This is the default embedding method.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-embed.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<iframe\n    src=\"https://<space-subdomain>.hf.space\"\n    frameborder=\"0\"\n    width=\"850\"\n    height=\"450\"\n></iframe>\n```\n\n----------------------------------------\n\nTITLE: Sentiment Analysis Request Example (JSON)\nDESCRIPTION: This JSON snippet demonstrates a request to the sentiment-analysis task of the Inference Toolkit API. The input is a string representing a negative review.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": \"Don't waste your time.  We had two different people come to our house to give us estimates for\na deck (one of them the OWNER).  Both times, we never heard from them.  Not a call, not the estimate, nothing.\"\n}\n```\n\n----------------------------------------\n\nTITLE: WebDataset TAR Archive Structure\nDESCRIPTION: Illustrates a WebDataset structure with TAR archives, suitable for large-scale image datasets, optimized for streaming images and metadata.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_12\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n train-0000.tar\n train-0001.tar\n ...\n train-1023.tar\n```\n\n----------------------------------------\n\nTITLE: Token Classification Request Example (JSON)\nDESCRIPTION: This JSON snippet demonstrates a request to the token-classification task of the Inference Toolkit API. The input is a sentence including named entities.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n}\n```\n\n----------------------------------------\n\nTITLE: List Models by Provider and Task (cURL)\nDESCRIPTION: This snippet shows how to list models served by a specific inference provider and with a specific pipeline tag (task) using the Hugging Face Hub API.  It uses cURL to make a GET request to the `/api/models` endpoint with the `inference_provider` and `pipeline_tag` query parameters.  The output is then filtered using `jq` to extract the model IDs.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# List text-to-image models served by Fal AI\n~ curl -s https://huggingface.co/api/models?inference_provider=fal-ai&pipeline_tag=text-to-image | jq \".[].id\"\n\"black-forest-labs/FLUX.1-dev\"\n\"stabilityai/stable-diffusion-3.5-large\"\n\"black-forest-labs/FLUX.1-schnell\"\n\"stabilityai/stable-diffusion-3.5-large-turbo\"\n...\n```\n\n----------------------------------------\n\nTITLE: Install Distilabel with vLLM support\nDESCRIPTION: This command installs the `distilabel` library along with the `vllm` extra, providing support for using vLLM (a fast and easy-to-use library for LLM inference) within Distilabel pipelines. The `-U` flag ensures that the package and its dependencies are upgraded to the latest versions.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-distilabel.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -U distilabel[vllm]\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom License in Model Card Metadata (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define a custom, non-standard license in a Hugging Face Model Card's metadata. When the license is not officially supported, `license` is set to `other`, and `license_name` and `license_link` are provided to specify the license's name and URL respectively. This ensures proper attribution and provides access to the full license terms.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlicense: other\nlicense_name: coqui-public-model-license\nlicense_link: https://coqui.ai/cpml\n---\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns with Polars\nDESCRIPTION: This snippet selects specific columns from a Polars DataFrame using the `select` method. It reduces the DataFrame to only include the specified columns: 'suffix', 'date', 'tld', 'pages', and 'domains'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars-operations.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = df.select(\"suffix\", \"date\", \"tld\", \"pages\", \"domains\")\ndf.head(3)\n```\n\n----------------------------------------\n\nTITLE: Metadata CSV Example\nDESCRIPTION: This snippet demonstrates the structure of a metadata.csv file used to link video files with textual descriptions.  The 'file_name' column contains the name of the video file, and the 'text' column contains the corresponding description.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_5\n\nLANGUAGE: csv\nCODE:\n```\nfile_name,text\n1.mp4,an animation of a green pokemon with red eyes\n2.mp4,a short video of a green and yellow toy with a red nose\n3.mp4,a red and white ball shows an angry look on its face\n4.mp4,a cartoon ball is smiling\n```\n\n----------------------------------------\n\nTITLE: CSV Example: Repository Analytics Data\nDESCRIPTION: Example CSV structure showcasing daily download records for models and datasets on the Hugging Face Hub. The structure includes fields for repository type, name, total downloads, timestamp, and daily downloads.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/enterprise-hub-analytics.md#_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\nrepoType,repoName,total,timestamp,downloads\nmodel,huggingface/CodeBERTa-small-v1,4362460,2021-01-22T00:00:00.000Z,4\nmodel,huggingface/CodeBERTa-small-v1,4362460,2021-01-23T00:00:00.000Z,7\nmodel,huggingface/CodeBERTa-small-v1,4362460,2021-01-24T00:00:00.000Z,2\ndataset,huggingface/documentation-images,2167284,2021-11-27T00:00:00.000Z,3\ndataset,huggingface/documentation-images,2167284,2021-11-28T00:00:00.000Z,18\ndataset,huggingface/documentation-images,2167284,2021-11-29T00:00:00.000Z,7\n```\n\n----------------------------------------\n\nTITLE: Add dependencies in requirements.txt\nDESCRIPTION: Defines the dependencies required for the Hot Dog Classifier Space. These dependencies include transformers and torch.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-gradio.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ntransformers\ntorch\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (LLM) Inference Configuration\nDESCRIPTION: This snippet configures the InferenceSnippet component to use Novita for Chat Completion (LLM) with the 'deepseek-ai/DeepSeek-V3-0324' model. It maps the 'modelId' to the corresponding 'providerModelId' on the Novita platform. The 'conversational' prop enables conversational behavior.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/novita.md#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"novita\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"deepseek/deepseek-v3-0324\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Validating Exported Parquet File\nDESCRIPTION: This snippet validates the exported Parquet file (`output.parquet`) by counting the number of rows using a `SELECT COUNT(*)` query in DuckDB.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-combine-and-export.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nSELECT COUNT(*) FROM 'output.parquet';\n```\n\n----------------------------------------\n\nTITLE: Object Detection Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Object Detection using the `facebook/detr-resnet-50` model on HF Inference. This specifies the pipeline and the relevant model ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=object-detection\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"facebook/detr-resnet-50\",\"providerModelId\":\"facebook/detr-resnet-50\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Metadata JSONL File Example\nDESCRIPTION: This code shows the structure of the `metadata.jsonl` file, including a `file_name` field to link audio files with their respective metadata entries, such as 'text' transcription.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_6\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"file_name\": \"1.wav\",\"text\": \"cat\"}\n{\"file_name\": \"2.wav\",\"text\": \"cat\"}\n{\"file_name\": \"3.wav\",\"text\": \"dog\"}\n{\"file_name\": \"4.wav\",\"text\": \"dog\"}\n```\n\n----------------------------------------\n\nTITLE: JSON payload structure for posting a comment - JSON\nDESCRIPTION: This JSON payload structure is used when posting a comment to the Hugging Face Hub API. The `comment` key's value is the text of the comment to be posted. This is the content that the bot generated using the BLOOM model via the Inference API.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-discussion-bot.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"comment\": continuationText\n}\n```\n\n----------------------------------------\n\nTITLE: Post Message to Parent Page for URL Sync - JavaScript\nDESCRIPTION: This JavaScript code snippet is designed for Docker or static Spaces to update the query string and hash of the parent page URL. It uses `window.parent.postMessage` to send a message containing the `queryString` and `hash` values to the parent page, targeting the `https://huggingface.co` origin. This allows deep-linking and state management within embedded applications.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-handle-url-parameters.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nconst queryString = \"...\";\nconst hash = \"...\";\n\nwindow.parent.postMessage({\n    queryString,\n    hash,\n}, \"https://huggingface.co\");\n```\n\n----------------------------------------\n\nTITLE: Creating or Updating Report Based on Webhook Data\nDESCRIPTION: This Python function handles the creation or update of a metadata review report based on webhook data received from the Hub. It parses the data and either creates a new report or posts an update to an existing metadata review thread, depending on whether a report has already been created.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef create_or_update_report(data):\n    if parsed_post := parse_webhook_post(data):\n        repo_type, repo_name = parsed_post\n    else:\n        return Response(\"Unable to parse webhook data\", status_code=400)\n    # [...]\n    return True\n```\n\n----------------------------------------\n\nTITLE: Pandas DataFrame Word Count\nDESCRIPTION: This snippet demonstrates applying the `dummy_count_words` function to a pandas DataFrame column named `text`, creating a new column called `num_words`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# pandas API\ndf[\"num_words\"] = dummy_count_words(df.text)\n```\n\n----------------------------------------\n\nTITLE: Querying Parquet metadata with parquet_metadata function\nDESCRIPTION: This snippet demonstrates querying the metadata of a Parquet file using the `parquet_metadata` function in DuckDB. It selects all columns from the metadata of the Parquet file located at the specified Hugging Face dataset path. It provides information about the file, row groups, and compression used. It requires the Hugging Face dataset at the specified path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-select.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nSELECT * FROM parquet_metadata('hf://datasets/jamescalam/world-cities-geo@~parquet/default/train/0000.parquet');\n```\n\n----------------------------------------\n\nTITLE: Stream WebDataset from Hugging Face Hub\nDESCRIPTION: Streams a WebDataset dataset from the Hugging Face Hub using the `webdataset` library. It retrieves the Hugging Face token, constructs the URL for the dataset shards, and creates a `WebDataset` instance with the URL. It then uses `DataLoader` to manage the dataset efficiently.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-webdataset.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> import webdataset as wds\n>>> from huggingface_hub import get_token\n>>> from torch.utils.data import DataLoader\n\n>>> hf_token = get_token()\n>>> url = \"https://huggingface.co/datasets/timm/imagenet-12k-wds/resolve/main/imagenet12k-train-{{0000..1023}}.tar\"\n>>> url = f\"pipe:curl -s -L {url} -H 'Authorization:Bearer {hf_token}'\"\n>>> dataset = wds.WebDataset(url).decode()\n>>> dataloader = DataLoader(dataset, batch_size=64, num_workers=4)\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (VLM) Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Chat Completion (VLM) using the `meta-llama/Llama-3.2-11B-Vision-Instruct` model on HF Inference.  It sets the pipeline to `image-text-to-text` and enables conversational mode.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\"providerModelId\":\"meta-llama/Llama-3.2-11B-Vision-Instruct\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Dataset Structure with Custom Split Names\nDESCRIPTION: Illustrates how to structure a dataset repository with custom split names. The files are named in the format `data/<split_name>-xxxxx-of-xxxxx.csv`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-file-names-and-splits.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n README.md\n data/\n     train-00000-of-00003.csv\n     train-00001-of-00003.csv\n     train-00002-of-00003.csv\n     test-00000-of-00001.csv\n     random-00000-of-00003.csv\n     random-00001-of-00003.csv\n     random-00002-of-00003.csv\n```\n\n----------------------------------------\n\nTITLE: Pushing spaCy model to Hugging Face Hub via CLI\nDESCRIPTION: This command pushes a packaged spaCy model to the Hugging Face Hub using the `spacy-huggingface-hub` CLI. It requires a `.whl` file, an optional organization name, a commit message, a local repository path, and a verbose flag. The `whl_path` argument specifies the path to the packaged model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spacy.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy huggingface-hub push [whl_path] [--org] [--msg] [--local-repo] [--verbose]\n```\n\n----------------------------------------\n\nTITLE: Exporting a folder as a DDUF file\nDESCRIPTION: This snippet demonstrates how to export a folder as a DDUF file using the `export_folder_as_dduf` function from the `huggingface_hub` library. This function scans the folder, adds the relevant entries, and ensures the exported file is valid. If anything goes wrong during the process, a `DDUFExportError` is raised. It requires the `huggingface_hub` package to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/dduf.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Export a folder as a DDUF file\n>>> from huggingface_hub import export_folder_as_dduf\n>>> export_folder_as_dduf(\"FLUX.1-dev.dduf\", folder_path=\"path/to/FLUX.1-dev\")\n```\n\n----------------------------------------\n\nTITLE: Automatic Speech Recognition with fal.ai\nDESCRIPTION: This snippet configures the `InferenceSnippet` component to use fal.ai for Automatic Speech Recognition. It maps the `openai/whisper-large-v3` model to the `fal-ai/whisper` provider model on fal.ai.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/fal-ai.md#_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n<InferenceSnippet\n    pipeline=automatic-speech-recognition\n    providersMapping={ {\"fal-ai\":{\"modelId\":\"openai/whisper-large-v3\",\"providerModelId\":\"fal-ai/whisper\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Accessing Space Variables in JavaScript\nDESCRIPTION: Demonstrates how to access custom environment variables and OAuth information within a static HTML Space using JavaScript. The `window.huggingface.variables` object provides access to variables like `OAUTH_CLIENT_ID`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-static.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nwindow.huggingface.variables.OAUTH_CLIENT_ID\n```\n\n----------------------------------------\n\nTITLE: Querying JSONL datasets with SELECT and FROM syntax\nDESCRIPTION: This snippet demonstrates querying a JSONL dataset using the `SELECT` and `FROM` syntax in DuckDB with sampling. It selects the `city`, `country`, and `region` columns from the dataset and uses the `SAMPLE` clause to retrieve a random sample of 3 rows. It requires the Hugging Face dataset at the specified path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-select.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nSELECT city, country, region FROM 'hf://datasets/jamescalam/world-cities-geo/train.jsonl' USING SAMPLE 3;\n```\n\n----------------------------------------\n\nTITLE: Applying PIL Image Methods to DataFrame\nDESCRIPTION: This code snippet demonstrates how to apply a `PIL.Image` method (specifically `rotate`) to a column of images within a Pandas DataFrame that has been enhanced by `pandas-image-methods`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf[\"image\"] = df[\"image\"].pil.rotate(90)\n```\n\n----------------------------------------\n\nTITLE: YAML Metadata Example\nDESCRIPTION: This YAML snippet shows an example of model card metadata, including language, thumbnail, tags, license, datasets, metrics, and base model information. It illustrates the structure and types of metadata that can be added to a model card.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlanguage:\n  - \"List of ISO 639-1 code for your language\"\n  - lang1\n  - lang2\nthumbnail: \"url to a thumbnail used in social sharing\"\ntags:\n- tag1\n- tag2\nlicense: \"any valid license identifier\"\ndatasets:\n- dataset1\n- dataset2\nmetrics:\n- metric1\n- metric2\nbase_model: \"base model Hub identifier\"\n---\n\n```\n\n----------------------------------------\n\nTITLE: Setting Label Studio Secrets - Google Cloud Storage\nDESCRIPTION: These environment variables configure Label Studio to use Google Cloud Storage (GCS) for cloud storage.  Requires setting the bucket name, project ID, folder, and specifying the path to the Google Application Credentials JSON key file. Setting these as secrets is highly recommended.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-label-studio.md#_snippet_3\n\nLANGUAGE: N/A\nCODE:\n```\n`STORAGE_TYPE`: `gcs`\n`STORAGE_GCS_BUCKET_NAME`: `<YOUR_BUCKET_NAME>`\n`STORAGE_GCS_PROJECT_ID`: `<YOUR_PROJECT_ID>`\n`STORAGE_GCS_FOLDER`: ``\n`GOOGLE_APPLICATION_CREDENTIALS`: `/opt/heartex/secrets/key.json`\n```\n\n----------------------------------------\n\nTITLE: Multiple Audio Formats in Subdirectory\nDESCRIPTION: This snippet shows how to store audio files of different formats in a subdirectory named 'audio'. Several audio formats (AIFF, FLAC, MP3, OGG, WAV) are supported.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n audio\n     1.aiff\n     2.ogg\n     3.mp3\n     4.flac\n```\n\n----------------------------------------\n\nTITLE: Installing and Logging into Hugging Face\nDESCRIPTION: This snippet shows how to install the `huggingface_hub` library and log in to your Hugging Face account using the command line interface. This is a prerequisite for pushing models to the Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/timm.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install huggingface_hub\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (LLM) Inference Snippet\nDESCRIPTION: This snippet configures the InferenceSnippet component for Chat Completion (LLM) tasks, utilizing the 'text-generation' pipeline and mapping the model 'deepseek-ai/DeepSeek-V3-0324' to the Nebius provider model 'deepseek-ai/DeepSeek-V3-0324-fast'. The 'conversational' prop indicates that this is for a conversational model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/nebius.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"nebius\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"deepseek-ai/DeepSeek-V3-0324-fast\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Specifying evaluation results with YAML in Model Card\nDESCRIPTION: This YAML snippet shows how to define evaluation results within the `model-index` section of the model card metadata. It includes details such as the model name, task type, dataset, metrics, and the source from which the results were obtained, along with a URL to the source.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n---\nmodel-index:\n  - name: Yi-34B\n    results:\n      - task:\n          type: text-generation\n        dataset:\n          name: ai2_arc\n          type: ai2_arc\n        metrics:\n          - name: AI2 Reasoning Challenge (25-Shot)\n            type: AI2 Reasoning Challenge (25-Shot)\n            value: 64.59\n        source:\n          name: Open LLM Leaderboard\n          url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n---\n```\n\n----------------------------------------\n\nTITLE: Feature Extraction Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Feature Extraction using the `intfloat/multilingual-e5-large-instruct` model on HF Inference. This specifies the pipeline and the relevant model ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=feature-extraction\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"intfloat/multilingual-e5-large-instruct\",\"providerModelId\":\"intfloat/multilingual-e5-large-instruct\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Translation Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Translation using the `facebook/nllb-200-distilled-600M` model on HF Inference. This specifies the pipeline and the relevant model ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_11\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=translation\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"facebook/nllb-200-distilled-600M\",\"providerModelId\":\"facebook/nllb-200-distilled-600M\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Embedding Dataset Viewer with search parameters\nDESCRIPTION: This snippet demonstrates how to embed the Dataset Viewer with search parameters. It displays the results of the search on `mangrove` in the `test` split of the `rte` subset of the `nyu-mll/glue` dataset. Parameters are added to the iframe URL to filter and search the dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-embed.md#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<iframe\n  src=\"https://huggingface.co/datasets/nyu-mll/glue/embed/viewer/rte/split?search=mangrove\"\n  frameborder=\"0\"\n  width=\"100%\"\n  height=\"560px\"\n></iframe>\n```\n\n----------------------------------------\n\nTITLE: Create Dataset Repository on Hugging Face Hub\nDESCRIPTION: This Python snippet uses the `huggingface_hub` library to create a new dataset repository on the Hugging Face Hub. It initializes the `HfApi` and calls the `create_repo` method with the desired repository ID and type.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\n\nHfApi().create_repo(repo_id=\"username/my_dataset\", repo_type=\"dataset\")\n```\n\n----------------------------------------\n\nTITLE: Loading a JSON file from a DDUF archive\nDESCRIPTION: This snippet shows how to load the content of a JSON file (specifically `model_index.json`) from a DDUF archive.  It uses the information returned by `read_dduf_file` to access the file's content and then uses the `json` library to parse the JSON data.  Requires `json` module.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/dduf.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n>>> import json\n>>> json.loads(dduf_entries[\"model_index.json\"].read_text())\n{'_class_name': 'FluxPipeline', '_diffusers_version': '0.32.0.dev0', '_name_or_path': 'black-forest-labs/FLUX.1-dev', ...\n```\n\n----------------------------------------\n\nTITLE: Text-to-Image Widget YAML Configuration\nDESCRIPTION: This snippet configures a text-to-image widget with two examples. Each example includes a 'text' field representing the prompt and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_22\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"A cat playing with a ball\"\n  example_title: \"Cat\"\n- text: \"A dog jumping over a fence\"\n  example_title: \"Dog\"\n```\n\n----------------------------------------\n\nTITLE: Selecting Nutrition Questions in DuckDB\nDESCRIPTION: This SQL query retrieves a subset of the MMLU dataset specifically for questions related to the 'nutrition' subject. It limits the results to 3 rows using the LIMIT clause for previewing relevant data.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-sql.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT *\nFROM   'hf://datasets/cais/mmlu/all/test-*.parquet'\nWHERE  subject = 'nutrition' LIMIT 3;\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Subsets in YAML\nDESCRIPTION: This YAML snippet defines two subsets, 'main_data' and 'additional_data', each associated with a different CSV file.  Each subset will have its own dropdown in the Dataset Viewer. The `config_name` field specifies the name of the subset and `data_files` specifies the associated data file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-manual-configuration.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n---\nconfigs:\n- config_name: main_data\n  data_files: \"main_data.csv\"\n- config_name: additional_data\n  data_files: \"additional_data.csv\"\n---\n```\n\n----------------------------------------\n\nTITLE: Cerebras Chat Completion (VLM) InferenceSnippet\nDESCRIPTION: This InferenceSnippet configures Cerebras as the provider for Chat Completion (VLM) tasks. It specifies the pipeline as image-text-to-text and maps the model 'meta-llama/Llama-4-Scout-17B-16E-Instruct' to Cerebras' 'llama-4-scout-17b-16e-instruct' model. The `conversational` prop enables conversational behavior.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/cerebras.md#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"cerebras\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"llama-4-scout-17b-16e-instruct\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (VLM) Inference Snippet\nDESCRIPTION: This snippet configures the InferenceSnippet component for Chat Completion (VLM) tasks.  It uses the 'image-text-to-text' pipeline and maps the model 'google/gemma-3-27b-it' to the Nebius provider model 'google/gemma-3-27b-it-fast'. The 'conversational' prop indicates this is for a conversational model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/nebius.md#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"nebius\":{\"modelId\":\"google/gemma-3-27b-it\",\"providerModelId\":\"google/gemma-3-27b-it-fast\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Creating a Hugging Face Dataset Repository\nDESCRIPTION: This snippet demonstrates how to create a dataset repository on the Hugging Face Hub using the `huggingface_hub` library. It imports the `HfApi` class and uses the `create_repo` method to create a new repository with the specified `repo_id` and `repo_type`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import HfApi\n\nHfApi().create_repo(repo_id=\"username/my_dataset\", repo_type=\"dataset\")\n```\n\n----------------------------------------\n\nTITLE: Embedding Dataset Viewer with iframe\nDESCRIPTION: This snippet shows how to embed the Hugging Face Dataset Viewer for the `glue` dataset from the `nyu-mll` organization in a webpage using an iframe. The `src` attribute points to the Dataset Viewer URL, and the `width` and `height` attributes control the size of the embedded viewer.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-embed.md#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe\n  src=\"https://huggingface.co/datasets/nyu-mll/glue/embed/viewer\"\n  frameborder=\"0\"\n  width=\"100%\"\n  height=\"560px\"\n></iframe>\n```\n\n----------------------------------------\n\nTITLE: Pushing a model to the Hub\nDESCRIPTION: This command uploads a Sample Factory model to the Hugging Face Hub using the `push_to_hub` script. It requires the repo ID and the path to the experiment directory. The repo ID must be in the form `<hf_username>/<hf_repo_name>`\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/sample-factory.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\npython -m sample_factory.huggingface.push_to_hub -r <hf_username>/<hf_repo_name> -d <experiment_dir_path>\n```\n\n----------------------------------------\n\nTITLE: Force Pushing to Hugging Face Space (bash)\nDESCRIPTION: This command force pushes the contents of your local repository's main branch to the Hugging Face Space remote. This is typically done for the initial synchronization of the repository with the Space. Using `--force` overwrites any existing content in the Space.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit push --force space main\n```\n\n----------------------------------------\n\nTITLE: Metadata CSV Example\nDESCRIPTION: Example of metadata.csv file format, showing the 'file_name' column that links image files with corresponding metadata like text captions.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_5\n\nLANGUAGE: csv\nCODE:\n```\nfile_name,text\n1.jpg,a drawing of a green pokemon with red eyes\n2.jpg,a green and yellow toy with a red nose\n3.jpg,a red and white ball with an angry look on its face\n4.jpg,a cartoon ball with a smile on it's face\n```\n\n----------------------------------------\n\nTITLE: Fill-Mask Widget YAML Configuration\nDESCRIPTION: This snippet configures a fill-mask widget with two example texts. Each example includes the 'text' field, representing the input text with a masked token ('<mask>'), and 'example_title' to provide a descriptive title for the example.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"Paris is the <mask> of France.\"\n  example_title: \"Capital\"\n- text: \"The goal of life is <mask>.\"\n  example_title: \"Philosophy\"\n```\n\n----------------------------------------\n\nTITLE: Reading DDUF metadata with huggingface_hub\nDESCRIPTION: This snippet demonstrates how to read metadata from a DDUF file using the `read_dduf_file` function from the `huggingface_hub` library. This function reads only the metadata and returns a mapping where each entry corresponds to a file in the DDUF archive. It requires the `huggingface_hub` package to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/dduf.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> from huggingface_hub import read_dduf_file\n\n# Read DDUF metadata\n>>> dduf_entries = read_dduf_file(\"FLUX.1-dev.dduf\")\n```\n\n----------------------------------------\n\nTITLE: YAML Example: Defining supported license in Model Card\nDESCRIPTION: This YAML snippet demonstrates how to directly specify a supported license (e.g., 'llama2') in the Model Card metadata.  This is used when the Hugging Face Hub officially supports the license, simplifying the metadata by omitting the license_name and license_link fields.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n# Example from https://huggingface.co/codellama/CodeLlama-34b-hf\n---\nlicense: llama2\n---\n```\n\n----------------------------------------\n\nTITLE: Feature Extraction Widget YAML Configuration\nDESCRIPTION: This snippet configures a feature extraction widget with two examples. Each example includes a 'text' field and 'example_title' field.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_12\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"My name is Sylvain and I live in Paris\"\n  example_title: \"Parisian\"\n- text: \"My name is Sarah and I live in London\"\n  example_title: \"Londoner\"\n```\n\n----------------------------------------\n\nTITLE: Question Answering Widget YAML Configuration\nDESCRIPTION: This snippet configures a question answering widget with two examples. Each example includes a 'text' field for the question, a 'context' field providing the context for answering the question, and 'example_title' for the example's title.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"What's my name?\"\n  context: \"My name is Clara and I live in Berkeley.\"\n  example_title: \"Name\"\n- text: \"Where do I live?\"\n  context: \"My name is Sarah and I live in London\"\n  example_title: \"Location\"\n```\n\n----------------------------------------\n\nTITLE: Iterating on Image Paths in DataFrame\nDESCRIPTION: This snippet shows how to load a metadata CSV file containing image paths and iterate over those paths. It imports the pandas library, reads the CSV, and constructs the full image path for each entry.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfolder_path = \"path/to/folder/\"\ndf = pd.read_csv(folder_path + \"metadata.csv\")\nfor image_path in (folder_path + df[\"file_name\"]):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (VLM) Inference Snippet\nDESCRIPTION: Configures an Inference Snippet for Chat Completion (VLM) using the Hyperbolic provider. It specifies the 'image-text-to-text' pipeline and maps the 'Qwen/Qwen2.5-VL-7B-Instruct' model ID to the same provider model ID on Hyperbolic. The conversational flag enables a chat-style interaction.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hyperbolic.md#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"hyperbolic\":{\"modelId\":\"Qwen/Qwen2.5-VL-7B-Instruct\",\"providerModelId\":\"Qwen/Qwen2.5-VL-7B-Instruct\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Zero-Shot Classification Widget YAML Configuration\nDESCRIPTION: This snippet configures a zero-shot classification widget. Each example includes a 'text' field for the input text, 'candidate_labels' for possible labels, 'multi_class' to indicate if multiple labels are allowed, and 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_9\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"I have a problem with my car that needs to be resolved asap!!\"\n  candidate_labels: \"urgent, not urgent, phone, tablet, computer\"\n  multi_class: true\n  example_title: \"Car problem\"\n- text: \"Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\"\n  candidate_labels: \"mobile, website, billing, account access\"\n  multi_class: false\n  example_title: \"Phone issue\"\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (LLM) Inference Snippet\nDESCRIPTION: This snippet demonstrates how to use Cohere for Chat Completion (LLM) with the `text-generation` pipeline, utilizing the `CohereLabs/c4ai-command-a-03-2025` model. It sets up the necessary configuration for conversational text generation through Cohere's provider.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/cohere.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"cohere\":{\"modelId\":\"CohereLabs/c4ai-command-a-03-2025\",\"providerModelId\":\"command-a-03-2025\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Loading Model from Specific Version in Transformers\nDESCRIPTION: This snippet shows how to load a specific version of a pre-trained model from the Hugging Face Hub using the `revision` parameter in `AutoModel.from_pretrained`. This allows you to load a model based on a tag name, branch name, or commit hash, ensuring reproducibility.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/transformers.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nmodel = AutoModel.from_pretrained(\n    \"julien-c/EsperBERTo-small\", revision=\"v2.0.1\"  # tag name, or branch name, or commit hash\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Markdown Report\nDESCRIPTION: This Python function creates a markdown report for a metadata review. The report includes a score, a metadata table, and an explanation of the report's contents.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef create_markdown_report(\n    desired_metadata_dictionary, repo_name, repo_type, score, update: bool = False\n):\n    # [...]\n    return report\n```\n\n----------------------------------------\n\nTITLE: Dask DataFrame Word Count using Map Partitions\nDESCRIPTION: This snippet demonstrates applying the `dummy_count_words` function to a Dask DataFrame using `map_partitions`. It processes each partition in parallel and specifies the `meta` argument to define the data type of the new column.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Dask API: run the function on every partition\ndf[\"num_words\"] = df.text.map_partitions(dummy_count_words, meta=int)\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Inference DLC Image URI Example\nDESCRIPTION: This example demonstrates the construction of a TensorFlow inference DLC image URI. It specifies the AWS account ID, region, framework, mode, framework version, Transformers version, device type, Python version, and device tag.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_2\n\nLANGUAGE: None\nCODE:\n```\n763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-tensorflow-inference:2.4.1-transformers4.6.1-cpu-py37-ubuntu18.04\n```\n\n----------------------------------------\n\nTITLE: Install huggingface_hub via pip\nDESCRIPTION: Installs the `huggingface_hub` library, which is required for integrating with the Hugging Face Hub. This library provides the necessary tools for downloading and uploading files to the Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install huggingface_hub\n```\n\n----------------------------------------\n\nTITLE: Checking spacy-huggingface-hub command registration\nDESCRIPTION: This command checks if the `spacy-huggingface-hub` command has been successfully registered with the spaCy CLI. It uses `python -m spacy` to invoke the spaCy module and then calls the `huggingface-hub --help` command to display the help information for the Hugging Face Hub integration.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spacy.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m spacy huggingface-hub --help\n```\n\n----------------------------------------\n\nTITLE: Linking Models in Space README - YAML\nDESCRIPTION: This code snippet demonstrates how to link models to a Hugging Face Space by adding their identifiers under the `models` key in the Space's README metadata. The `models` field in the YAML frontmatter of the README will be parsed to display the linked models in the Space's interface.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-overview.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: My lovely space\nemoji: \ncolorFrom: blue\ncolorTo: green\nsdk: docker\npinned: false\nmodels:\n- reach-vb/musicgen-large-fp16-endpoint\n- reach-vb/wav2vec2-large-xls-r-1B-common_voice7-lt-ft\n```\n\n----------------------------------------\n\nTITLE: YAML Metadata Configuration for Transformers Library Integration\nDESCRIPTION: This YAML snippet shows how to specify that your model works with the Transformers library in your model card. This integration significantly increases your model's accessibility by providing users with readily available code snippets for working with your model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-release-checklist.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\n---\nlibrary_name: transformers\n---\n```\n\n----------------------------------------\n\nTITLE: Summarization Widget YAML Configuration\nDESCRIPTION: This snippet shows the configuration for a summarization widget with two examples. Each example includes a 'text' field containing the text to be summarized and an 'example_title' for the example's title.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n  example_title: \"Eiffel Tower\"\n- text: \"Laika, a dog that was the first living creature to be launched into Earth orbit, on board the Soviet artificial satellite Sputnik 2, on November 3, 1957. It was always understood that Laika would not survive the mission, but her actual fate was misrepresented for decades. Laika was a small (13 pounds [6 kg]), even-tempered, mixed-breed dog about two years of age. She was one of a number of stray dogs that were taken into the Soviet spaceflight program after being rescued from the streets. Only female dogs were used because they were considered to be anatomically better suited than males for close confinement.\"\n  example_title: \"First in Space\"\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (VLM) Inference Snippet\nDESCRIPTION: This snippet configures the Inference API to use SambaNova for Chat Completion (VLM). It specifies the 'meta-llama/Llama-4-Scout-17B-16E-Instruct' model on the Hugging Face Hub and maps it to 'Llama-4-Scout-17B-16E-Instruct' on the SambaNova provider. The pipeline is set to 'image-text-to-text' and marked as conversational.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/sambanova.md#_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"sambanova\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"Llama-4-Scout-17B-16E-Instruct\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Installing MLX with conda\nDESCRIPTION: This command installs the core MLX package using conda, a package, dependency and environment management for many languagesPython, R, Ruby, Lua, Scala, Java, JavaScript, C/ C++, FORTRAN. It installs MLX from the conda-forge channel.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge mlx\n```\n\n----------------------------------------\n\nTITLE: Basic Docker SDK Space Configuration YAML\nDESCRIPTION: This YAML snippet configures a basic Docker SDK Space by setting the title, emoji, color scheme, SDK type, and application port in the Space's README.md file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n---\ntitle: Basic Docker SDK Space\nemoji: \ncolorFrom: purple\ncolorTo: gray\nsdk: docker\napp_port: 7860\n---\n```\n\n----------------------------------------\n\nTITLE: Dockerfile User Setup and Permissions\nDESCRIPTION: This Dockerfile snippet sets up a new user with a specific user ID, switches to that user, sets the working directory, and copies files with correct ownership to avoid permission issues. It also installs pip and upgrades it.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker.md#_snippet_4\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Set up a new user named \"user\" with user ID 1000\nRUN useradd -m -u 1000 user\n\n# Switch to the \"user\" user\nUSER user\n\n# Set home to the user's home directory\nENV HOME=/home/user \\\n\tPATH=/home/user/.local/bin:$PATH\n\n# Set the working directory to the user's home directory\nWORKDIR $HOME/app\n\n# Try and run pip command after setting the user with `USER user` to avoid permission issues with Python\nRUN pip install --no-cache-dir --upgrade pip\n\n# Copy the current directory contents into the container at $HOME/app setting the owner to the user\nCOPY --chown=user . $HOME/app\n\n# Download a checkpoint\nRUN mkdir content\nADD --chown=user https://<SOME_ASSET_URL> content/<SOME_ASSET_NAME>\n```\n\n----------------------------------------\n\nTITLE: Dataset Split Detection by File Naming\nDESCRIPTION: This example demonstrates how dataset splits (train, test, validation) can be automatically detected by the Dataset Viewer based on the names of the data files within the repository. No explicit configuration is required if files are named according to split names.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-data-files-configuration.md#_snippet_0\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n README.md\n train.csv\n test.csv\n validation.csv\n```\n\n----------------------------------------\n\nTITLE: Model Examination Placeholder\nDESCRIPTION: This snippet represents a placeholder for the Model Examination section, intended for explainability and interpretability work.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n`model_examination`\n```\n\n----------------------------------------\n\nTITLE: Example Webhook Payload (Pull Request Opened)\nDESCRIPTION: This JSON payload represents the structure of a webhook notification when a pull request is opened. It includes information about the event, repository, discussion, comment, and the webhook itself. It demonstrates the format and content of data sent to the webhook target URL.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"event\": {\n    \"action\": \"create\",\n    \"scope\": \"discussion\"\n  },\n  \"repo\": {\n    \"type\": \"model\",\n    \"name\": \"openai-community/gpt2\",\n    \"id\": \"621ffdc036468d709f17434d\",\n    \"private\": false,\n    \"url\": {\n      \"web\": \"https://huggingface.co/openai-community/gpt2\",\n      \"api\": \"https://huggingface.co/api/models/openai-community/gpt2\"\n    },\n    \"owner\": {\n      \"id\": \"628b753283ef59b5be89e937\"\n    }\n  },\n  \"discussion\": {\n    \"id\": \"6399f58518721fdd27fc9ca9\",\n    \"title\": \"Update co2 emissions\",\n    \"url\": {\n      \"web\": \"https://huggingface.co/openai-community/gpt2/discussions/19\",\n      \"api\": \"https://huggingface.co/api/models/openai-community/gpt2/discussions/19\"\n    },\n    \"status\": \"open\",\n    \"author\": {\n      \"id\": \"61d2f90c3c2083e1c08af22d\"\n    },\n    \"num\": 19,\n    \"isPullRequest\": true,\n    \"changes\": {\n      \"base\": \"refs/heads/main\"\n    }\n  },\n  \"comment\": {\n    \"id\": \"6399f58518721fdd27fc9caa\",\n    \"author\": {\n      \"id\": \"61d2f90c3c2083e1c08af22d\"\n    },\n    \"content\": \"Add co2 emissions information to the model card\",\n    \"hidden\": false,\n    \"url\": {\n      \"web\": \"https://huggingface.co/openai-community/gpt2/discussions/19#6399f58518721fdd27fc9caa\"\n    }\n  },\n  \"webhook\": {\n    \"id\": \"6390e855e30d9209411de93b\",\n    \"version\": 3\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Installing spacy-huggingface-hub\nDESCRIPTION: This command installs the `spacy-huggingface-hub` library using pip. This library extends the spaCy CLI to facilitate pushing models to the Hugging Face Hub. It requires pip to be installed and configured.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spacy.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install spacy-huggingface-hub\n```\n\n----------------------------------------\n\nTITLE: Download a single file from Hub using hf_hub_download\nDESCRIPTION: Downloads a specific file from a Hugging Face Hub repository using the `hf_hub_download` function. The downloaded file is cached locally to avoid redundant downloads.  Requires the `huggingface_hub` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from huggingface_hub import hf_hub_download\n>>> config_path = hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\")\n>>> config_path\n'/home/lysandre/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/894a9adde21d9a3e3843e6d5aeaaf01875c7fade/config.json'\n```\n\n----------------------------------------\n\nTITLE: Document Question Answering Widget YAML Configuration\nDESCRIPTION: This snippet configures a document question answering widget. Each example includes 'text' field for the question and a 'src' field for the document URL.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_23\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"What is the invoice number?\"\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\"\n- text: \"What is the purchase amount?\"\n  src: \"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg\"\n```\n\n----------------------------------------\n\nTITLE: Automatic Speech Recognition Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Automatic Speech Recognition using the `openai/whisper-large-v3-turbo` model on HF Inference. This snippet specifies the pipeline and the corresponding model ID for the inference provider.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=automatic-speech-recognition\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"openai/whisper-large-v3-turbo\",\"providerModelId\":\"openai/whisper-large-v3-turbo\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Metadata CSV File Example\nDESCRIPTION: This code shows the structure of the `metadata.csv` file, including a `file_name` column to link audio files with their respective metadata entries, such as an 'animal' label.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_5\n\nLANGUAGE: csv\nCODE:\n```\nfile_name,animal\n1.wav,cat\n2.wav,cat\n3.wav,dog\n4.wav,dog\n```\n\n----------------------------------------\n\nTITLE: Displaying generation options for MLX-LM\nDESCRIPTION: This command runs the `mlx_lm.generate` module with the `--help` flag, displaying a list of available generation options and their descriptions. It's useful for understanding the configurable parameters for text generation.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m mlx_lm.generate --help\n```\n\n----------------------------------------\n\nTITLE: Downloading a Model Repository using Git\nDESCRIPTION: This command clones a Hugging Face repository directly using git. Replace `<Name of HuggingFace Repo>` with the actual repository name (e.g., `bigscience/bloom`).\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/sample-factory.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\ngit clone git@hf.co:<Name of HuggingFace Repo> # example: git clone git@hf.co:bigscience/bloom\n```\n\n----------------------------------------\n\nTITLE: Dataset Structure with Multiple Files Per Split\nDESCRIPTION: Illustrates how to structure a dataset repository where each split spans multiple files. Files for the same split must include the split name in their filename (e.g., `train_0.csv`, `train_1.csv`).\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-file-names-and-splits.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n README.md\n train_0.csv\n train_1.csv\n train_2.csv\n train_3.csv\n test_0.csv\n test_1.csv\n```\n\n----------------------------------------\n\nTITLE: Compute Dialogue Count Per Language using PySpark\nDESCRIPTION: This code demonstrates how to compute the number of dialogues per language in the BAAI/Infinity-Instruct dataset.  It utilizes the `read_parquet` function to read only the `langdetect` column for efficiency. The results are grouped by language and the count is displayed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-spark.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> df_langdetect_only = read_parquet(\"hf://datasets/BAAI/Infinity-Instruct/7M/*.parquet\", columns=[\"langdetect\"])\n>>> df_langdetect_only.groupBy(\"langdetect\").count().show()\n+----------+-------+                                                            \n|langdetect|  count|\n+----------+-------+\n|        en|6697793|\n|     zh-cn| 751313|\n+----------+-------+\n```\n\n----------------------------------------\n\nTITLE: Uploading TF-Keras Model to Hugging Face Hub in Python\nDESCRIPTION: This code snippet shows how to upload a TF-Keras model to the Hugging Face Hub using the `push_to_hub_keras` function from the `huggingface_hub` library. It takes the model, repository name, TensorBoard log directory, and optional tags as input. `model_save_kwargs` allows passing keyword arguments to the model saving function.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/tf-keras.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import push_to_hub_keras\n\npush_to_hub_keras(model,\n    \"your-username/your-model-name\",\n    \"your-tensorboard-log-directory\",\n    tags = [\"object-detection\", \"some_other_tag\"],\n    **model_save_kwargs,\n)\n```\n\n----------------------------------------\n\nTITLE: Defining HF_TASK for Transformers Pipeline (Bash)\nDESCRIPTION: This snippet defines the `HF_TASK` environment variable, which specifies the task to be used by the  Transformers pipeline. The task determines the type of inference performed by the pipeline.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nHF_TASK=\"question-answering\"\n```\n\n----------------------------------------\n\nTITLE: Listing Datasets Parameters in JavaScript\nDESCRIPTION: Defines the parameters that can be used when listing datasets via the Hugging Face Hub API. These parameters allow filtering, sorting, and limiting the results of the dataset listing API call. They correspond to the arguments available in the `huggingface_hub.list_datasets()` function.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nparams = {\n    \"search\":\"search\",\n    \"author\":\"author\",\n    \"filter\":\"filter\",\n    \"sort\":\"sort\",\n    \"direction\":\"direction\",\n    \"limit\":\"limit\",\n    \"full\":\"full\",\n    \"config\":\"config\"\n}\n```\n\n----------------------------------------\n\nTITLE: Text Classification Widget YAML Configuration\nDESCRIPTION: This snippet configures a text classification widget with two examples. Each example includes a 'text' field containing the text to be classified and an 'example_title' to represent the classification.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"I love football so much\"\n  example_title: \"Positive\"\n- text: \"I don't really like this type of food\"\n  example_title: \"Negative\"\n```\n\n----------------------------------------\n\nTITLE: Question Answering Request Example (JSON)\nDESCRIPTION: This JSON snippet demonstrates a request to the question-answering task of the Inference Toolkit API. The input includes a question and a context.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": {\n    \"question\": \"What is used for inference?\",\n    \"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Text To Image Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Text to Image generation using the `black-forest-labs/FLUX.1-dev` model on HF Inference. This specifies the pipeline and the relevant model ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_10\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-to-image\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"black-forest-labs/FLUX.1-dev\",\"providerModelId\":\"black-forest-labs/FLUX.1-dev\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Zero Shot Classification Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Zero Shot Classification using the `facebook/bart-large-mnli` model on HF Inference. This specifies the pipeline and the relevant model ID.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_12\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=zero-shot-classification\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"facebook/bart-large-mnli\",\"providerModelId\":\"facebook/bart-large-mnli\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Summarization Configuration\nDESCRIPTION: Configures the `InferenceSnippet` component for Summarization using the `facebook/bart-large-cnn` model on HF Inference. This snippet sets the pipeline and the corresponding model ID for the inference provider.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hf-inference.md#_snippet_7\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=summarization\n    providersMapping={ {\"hf-inference\":{\"modelId\":\"facebook/bart-large-cnn\",\"providerModelId\":\"facebook/bart-large-cnn\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Audio Classification Directory Structure\nDESCRIPTION: This demonstrates organizing audio files by class labels using subdirectories (e.g., 'cat' and 'dog'). The dataset will automatically contain 'audio' and 'label' columns.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n cat\n  1.wav\n  2.wav\n dog\n     3.wav\n     4.wav\n```\n\n----------------------------------------\n\nTITLE: Clone llama.cpp from GitHub\nDESCRIPTION: This command clones the llama.cpp repository from GitHub. This is the first step in building llama.cpp from source.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/gguf-llamacpp.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/ggerganov/llama.cpp\n```\n\n----------------------------------------\n\nTITLE: List Models by Inference Provider (cURL)\nDESCRIPTION: This snippet demonstrates how to list models served by a specific inference provider using the Hugging Face Hub API. It uses cURL to make a GET request to the `/api/models` endpoint with the `inference_provider` query parameter.  The output is then filtered using `jq` to extract the model IDs.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# List all models served by Fireworks AI\n~ curl -s https://huggingface.co/api/models?inference_provider=fireworks-ai | jq \".[].id\"\n\"deepseek-ai/DeepSeek-V3-0324\"\n\"deepseek-ai/DeepSeek-R1\"\n\"Qwen/QwQ-32B\"\n\"deepseek-ai/DeepSeek-V3\"\n...\n```\n\n----------------------------------------\n\nTITLE: API: Grant Access to User\nDESCRIPTION: This API endpoint allows a specific user to access the gated repository. It requires a token with write access to the repository and accepts a JSON payload.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-gated.md#_snippet_4\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /api/models/{repo_id}/user-access-request/grant\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nHeaders: {\"authorization\":  \"Bearer $token\"}\n```\n\nLANGUAGE: JSON\nCODE:\n```\nPayload: {\"user\": \"username\"} \n```\n\n----------------------------------------\n\nTITLE: Text-to-Speech Widget YAML Configuration\nDESCRIPTION: This snippet configures a text-to-speech widget with two examples. Each example includes a 'text' field representing the text to be converted to speech and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_13\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"My name is Sylvain and I live in Paris\"\n  example_title: \"Parisian\"\n- text: \"My name is Sarah and I live in London\"\n  example_title: \"Londoner\"\n```\n\n----------------------------------------\n\nTITLE: Setting SDK to Static in YAML\nDESCRIPTION: Configures a Hugging Face Space to use a static HTML application by setting the `sdk` parameter to `static` within the YAML block in the README.md file. This tells the platform that the Space will be serving static HTML, CSS, and JavaScript files instead of a Streamlit or Gradio application.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-static.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nsdk: static\n```\n\n----------------------------------------\n\nTITLE: Exporting specific files as a DDUF file\nDESCRIPTION: This snippet shows how to export specific files from the local disk as a DDUF file using the `export_entries_as_dduf` function from the `huggingface_hub` library.  The function takes a list of tuples where each tuple contains the filename in the DDUF archive and the path to the local file. It requires the `huggingface_hub` package to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/dduf.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Export specific files from the local disk.\n>>> from huggingface_hub import export_entries_as_dduf\n>>> export_entries_as_dduf(\n...     dduf_path=\"stable-diffusion-v1-4-FP16.dduf\",\n...     entries=[ # List entries to add to the DDUF file (here, only FP16 weights)\n...         (\"model_index.json\", \"path/to/model_index.json\"),\n...         (\"vae/config.json\", \"path/to/vae/config.json\"),\n...         (\"vae/diffusion_pytorch_model.fp16.safetensors\", \"path/to/vae/diffusion_pytorch_model.fp16.safetensors\"),\n...         (\"text_encoder/config.json\", \"path/to/text_encoder/config.json\"),\n...         (\"text_encoder/model.fp16.safetensors\", \"path/to/text_encoder/model.fp16.safetensors\"),\n...         # ... add more entries here\n...     ]\n... )\n```\n\n----------------------------------------\n\nTITLE: Get Model Inference Status (cURL)\nDESCRIPTION: This snippet shows how to retrieve the inference status of a specific model using the Hugging Face Hub API. It uses cURL to make a GET request to the `/api/models/{model_id}` endpoint, expanding the `inference` attribute. The output indicates whether the model is \"warm\" (ready for inference) or undefined.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\n# Get google/gemma-3-27b-it inference status (warm)\n~ curl -s https://huggingface.co/api/models/google/gemma-3-27b-it?expand[]=inference\n{\n\"_id\": \"67c35b9bb236f0d365bf29d3\",\n\"id\": \"google/gemma-3-27b-it\",\n\"inference\": \"warm\"\n}\n```\n\n----------------------------------------\n\nTITLE: Image Classification Dataset Structure\nDESCRIPTION: Illustrates the directory structure for an image classification dataset, where subdirectories represent image classes (e.g., 'green' and 'red').\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_9\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n green\n  1.jpg\n  2.jpg\n red\n     3.jpg\n     4.jpg\n```\n\n----------------------------------------\n\nTITLE: Dockerfile Secret Exposure - Git Remote\nDESCRIPTION: This Dockerfile snippet shows how to expose a secret during build time using the --mount flag and utilize it as a git remote URL. It requires a secret named `SECRET_EXAMPLE` to be set in the Space's settings.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker.md#_snippet_2\n\nLANGUAGE: Dockerfile\nCODE:\n```\n# Expose the secret SECRET_EXAMPLE at buildtime and use its value as git remote URL\nRUN --mount=type=secret,id=SECRET_EXAMPLE,mode=0444,required=true \\\n git init && \\\n git remote add origin $(cat /run/secrets/SECRET_EXAMPLE)\n```\n\n----------------------------------------\n\nTITLE: Forcing Dataset Modality YAML\nDESCRIPTION: This YAML snippet demonstrates how to force a specific modality for a dataset.  Adding the `audio` tag will force the Hub to recognize the dataset as audio, regardless of the file types it contains.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-cards.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\ntags:\n- audio\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (VLM) Inference Configuration\nDESCRIPTION: This snippet configures the InferenceSnippet component to use Novita for Chat Completion (VLM) with the 'meta-llama/Llama-4-Scout-17B-16E-Instruct' model. It maps the 'modelId' to the corresponding 'providerModelId' on the Novita platform. The 'conversational' prop enables conversational behavior.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/novita.md#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"novita\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"meta-llama/llama-4-scout-17b-16e-instruct\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Image Classification Widget YAML Configuration\nDESCRIPTION: This snippet configures an image classification widget.  Each example includes a 'src' field representing the URL of the image and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_18\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n```\n\n----------------------------------------\n\nTITLE: Configure Widget Input YAML\nDESCRIPTION: This YAML snippet shows how to specify widget input using text within the model card metadata.  It defines a single text input for the widget. The text provided will be used as an example when the widget is displayed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nwidget:\n- text: \"Jens Peter Hansen kommer fra Danmark\"\n```\n\n----------------------------------------\n\nTITLE: Loading DataFrame from CSV with Pandas\nDESCRIPTION: This snippet shows how to load a DataFrame from a CSV file using Pandas. It imports the pandas library and uses the `read_csv` function to load the data into a DataFrame.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import pandas as pd\n>>> df = pd.read_csv(\"path/to/data.csv\")\n```\n\n----------------------------------------\n\nTITLE: Basic Dataset Structure with Single Data File\nDESCRIPTION: Demonstrates the simplest dataset structure with a single data file (e.g., `data.csv`) and a `README.md` file in the repository. This structure is suitable for datasets that aren't split into train/validation/test sets.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-file-names-and-splits.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n README.md\n data.csv\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face CLI\nDESCRIPTION: Logs in to the Hugging Face Hub using the `huggingface-cli` tool. This is necessary to access private datasets or datasets that require authentication.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-webdataset.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Loading and Using AllenNLP Model from Hub\nDESCRIPTION: This snippet shows how to load an AllenNLP model from the Hugging Face Hub using the `Predictor` class and the `from_path` method with the `hf://` prefix.  It then provides an example input and makes a prediction using the loaded model. Requires the `allennlp_models` package and the `allennlp` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/allennlp.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path(\"hf://allenai/bidaf-elmo\")\npredictor_input = {\n    \"passage\": \"My name is Wolfgang and I live in Berlin\", \n    \"question\": \"Where do I live?\"\n}\npredictions = predictor.predict_json(predictor_input)\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face via CLI\nDESCRIPTION: Authenticates the user with the Hugging Face Hub via the command line. This stores the user's token locally, allowing them to download private models and upload models to the Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-adding-libraries.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Installing MLX-LM with pip\nDESCRIPTION: This command installs the MLX-LM package, which includes Hugging Face integration for Large Language Models.  It uses pip, the Python package installer. Installing `mlx-lm` negates the need to install `mlx` separately.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/mlx.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mlx-lm\n```\n\n----------------------------------------\n\nTITLE: SageMaker Metrics Configuration with Hugging Face Estimator (Python)\nDESCRIPTION: This snippet showcases how to configure the Hugging Face Estimator to define SageMaker metrics that are automatically parsed from the training job logs and sent to CloudWatch. The `metric_definitions` parameter contains a list of dictionaries, each defining a metric's name and a regular expression to extract it from the logs.  This enables monitoring and analysis of training performance.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/train.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# define metrics definitions\nmetric_definitions = [\n    {\"Name\": \"train_runtime\", \"Regex\": \"train_runtime.*=\\D*(.*?)$\"},\n    {\"Name\": \"eval_accuracy\", \"Regex\": \"eval_accuracy.*=\\D*(.*?)$\"},\n    {\"Name\": \"eval_loss\", \"Regex\": \"eval_loss.*=\\D*(.*?)$\"},\n]\n\n# create the Estimator\nhuggingface_estimator = HuggingFace(\n        entry_point='train.py',\n        source_dir='./scripts',\n        instance_type='ml.p3.2xlarge',\n        instance_count=1,\n        role=role,\n        transformers_version='4.26',\n        pytorch_version='1.13',\n        py_version='py39',\n        metric_definitions=metric_definitions,\n        hyperparameters = hyperparameters)\n```\n\n----------------------------------------\n\nTITLE: Filter Datasets by Downloads with Hugging Face Hub Python\nDESCRIPTION: This Python code snippet demonstrates how to use the `huggingface_hub` library to list datasets and filter them based on the number of downloads. It initializes a list of datasets and then filters them to only include datasets with more than 20 downloads.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/hacktoberfest_challenges/datasets_without_language.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom huggingface_hub import list_datasets\n\ndatasets = list_datasets(full=True)\ndatasets_with_at_least_20_downloads = [dataset for dataset in datasets if dataset.downloads >20]\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face with CLI\nDESCRIPTION: This command authenticates the user with their Hugging Face account using the Hugging Face CLI. This is a prerequisite for interacting with the Hugging Face Hub programmatically.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Pickle Class Serialization in Python\nDESCRIPTION: This snippet demonstrates serializing a Python class instance using `pickle`. It defines a simple class `Data` and pickles an instance of it to a file. This highlights the inherent risk in pickling complex objects as they can contain arbitrary code.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\nimport pickletools\n\nclass Data:\n    def __init__(self, important_stuff: str):\n        self.important_stuff = important_stuff\n\nd = Data(\"42\")\n\nwith open('payload.pkl', 'wb') as f:\n    pickle.dump(d, f)\n```\n\n----------------------------------------\n\nTITLE: ZeroGPU Duration Management\nDESCRIPTION: This example shows how to specify a custom duration for a function using the `@spaces.GPU` decorator. This is useful for functions that may exceed the default 60-second GPU runtime. Setting shorter durations for faster functions improves queue priority for Space visitors.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-zerogpu.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n@spaces.GPU(duration=120)\ndef generate(prompt):\n   return pipe(prompt).images\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face CLI\nDESCRIPTION: This command logs into the Hugging Face CLI using your Hugging Face account. This is a prerequisite for interacting with the Hugging Face Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-argilla.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Adding Task to SpaCy in Community Inference API\nDESCRIPTION: This snippet pertains to adding a new task to a library docker image, specifically showing an example of adding text-classification to spaCy. The `docker_images/common/app/pipelines` directory in the api-inference-community repository holds templates for integrating tasks across various libraries. This involves modifying the docker image for the specified library to include the new task.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-tasks.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Example updatedConfig Webhook Payload (Private)\nDESCRIPTION: This JSON payload provides an example of the `updatedConfig` property when the event scope is `repo.config`. In this case it shows the updated config when the private property is modified.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"private\": false\n}\n\n```\n\n----------------------------------------\n\nTITLE: Example Repo Webhook Payload\nDESCRIPTION: This JSON payload shows an example of the `repo` property in a webhook payload.  It describes the repository details, including its type, name, ID, privacy status, URLs, associated tags, head SHA (latest commit SHA on the main branch), and the owner's ID.  The `repo.headSha` is only included for repo-related events, not community events.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"type\": \"model\",\n\t\"name\": \"some-user/some-repo\",\n\t\"id\": \"6366c000a2abcdf2fd69a080\",\n\t\"private\": false,\n\t\"url\": {\n\t\t\"web\": \"https://huggingface.co/some-user/some-repo\",\n\t\t\"api\": \"https://huggingface.co/api/models/some-user/some-repo\"\n\t},\n\t\"headSha\": \"c379e821c9c95d613899e8c4343e4bfee2b0c600\",\n\t\"tags\": [\n\t\t\"license:other\",\n\t\t\"has_space\"\n\t],\n\t\"owner\": {\n\t\t\"id\": \"61d2000c3c2083e1c08af22d\"\n\t}\n}\n\n```\n\n----------------------------------------\n\nTITLE: WebDataset TAR Archive Content Structure\nDESCRIPTION: This snippet shows the internal structure of a WebDataset TAR archive, where each video and its corresponding metadata file share the same prefix.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\ntrain-0000/\n 000.mp4\n 000.json\n 001.mp4\n 001.json\n ...\n 999.mp4\n 999.json\n```\n\n----------------------------------------\n\nTITLE: Associating a Library with a Dataset YAML\nDESCRIPTION: This YAML snippet shows how to associate a specific library with a dataset card, which can be useful for highlighting tools that natively load the dataset.  In this case, the `argilla` library is being associated with the dataset.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-cards.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\ntags:\n- argilla\n```\n\n----------------------------------------\n\nTITLE: Rebasing Fork from Upstream\nDESCRIPTION: Rebases the fork's main branch onto the upstream's main branch, allowing you to incorporate changes from the upstream repository.  This requires resolving any conflicts that may arise during the rebase process.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-next-steps.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit rebase upstream/main\n```\n\n----------------------------------------\n\nTITLE: Pickle Output Opcodes\nDESCRIPTION: This is the output from the pickletools.dis() function showing opcodes after pickling a string. It shows protocol version, the string itself, memoization and the STOP opcode.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n0: \\x80 PROTO      4\n    2: \\x95 FRAME      48\n   11: \\x8c SHORT_BINUNICODE 'data I want to share with a friend'\n   57: \\x94 MEMOIZE    (as 0)\n   58: .    STOP\nhighest protocol among opcodes = 4\n```\n\n----------------------------------------\n\nTITLE: Dockerfile for Shiny Python App\nDESCRIPTION: This is a template Dockerfile for deploying a Shiny for Python application on Hugging Face Spaces. It handles the installation of dependencies and ensures the app runs on the correct port (7860 by default). The Dockerfile exposes the specified port to make the application accessible.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-shiny.md#_snippet_0\n\nLANGUAGE: Dockerfile\nCODE:\n```\nThe Dockerfile for a Shiny for Python app is very minimal because the library doesn't have many system dependencies, but you may need to modify this file if your application has additional system dependencies. \nThe one essential feature of this file is that it exposes and runs the app on the port specified in the space README file (which is 7860 by default).\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration to Disable Automatic Label Addition\nDESCRIPTION: This snippet shows a YAML configuration to disable the automatic addition of the `label` column in video classification datasets when directory names have no special meaning.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nconfigs:\n  - config_name: default  # Name of the dataset subset, if applicable.\n    drop_labels: true\n```\n\n----------------------------------------\n\nTITLE: Adding Item to Collection Payload (POST)\nDESCRIPTION: This payload is used when adding an item to a collection. An item is defined by its type (model, dataset, space, or paper) and its ID. A note can also be attached to the item.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_18\n\nLANGUAGE: JavaScript\nCODE:\n```\npayload = {\n    \"item\" : {\n        \"type\": \"model\",\n        \"id\": \"username/cool-model\",\n    }\n    \"note\": \"Here is the model I trained on ...\",\n}\n```\n\n----------------------------------------\n\nTITLE: Delete SageMaker Endpoint\nDESCRIPTION: Deletes a SageMaker endpoint using the `delete_endpoint()` method. It removes the deployed model and its associated resources from SageMaker.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# delete endpoint\npredictor.delete_endpoint()\n```\n\n----------------------------------------\n\nTITLE: Dataset Repository Structure with Video Files (Root)\nDESCRIPTION: This snippet demonstrates a dataset repository structure where video files are stored directly at the root level. This is suitable for datasets consisting solely of video files without any associated metadata.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n 1.mp4\n 2.mp4\n 3.mp4\n 4.mp4\n```\n\n----------------------------------------\n\nTITLE: Defining Request ID Header\nDESCRIPTION: This example shows how to define a custom header named `Inference-Id` in your inference response. It provides a unique request (or response) ID for each request/generation served which is used for billing purposes.  The `Inference-Id` should contain a UUID character string.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_10\n\nLANGUAGE: http\nCODE:\n```\nPOST /v1/chat/completions\nContent-Type: application/json\n[request headers]\n[request body]\n------\nHTTP/1.1 200 OK\nContent-Type: application/json\n[other request headers]\nInference-Id: unique-id-00131\n[response body]\n```\n\n----------------------------------------\n\nTITLE: Installing Flair\nDESCRIPTION: Installs the Flair NLP library using pip.  This command updates Flair to the latest version.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/flair.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -U flair\n```\n\n----------------------------------------\n\nTITLE: Installing huggingface_hub with fastai support\nDESCRIPTION: This command installs the huggingface_hub library with the fastai extension, enabling functionality for interacting with fastai models on the Hugging Face Hub. This is required to download and upload fastai models.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/fastai.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install huggingface_hub[\"fastai\"]\n```\n\n----------------------------------------\n\nTITLE: Dataset Structure with Multiple Files Per Split (Directories)\nDESCRIPTION: Demonstrates a dataset structure with multiple files per split, organized into directories named after the splits. The split name is inferred from the directory name.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-file-names-and-splits.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n README.md\n data/\n     train/\n        shard_0.csv\n        shard_1.csv\n        shard_2.csv\n        shard_3.csv\n     test/\n         shard_0.csv\n         shard_1.csv\n```\n\n----------------------------------------\n\nTITLE: Installing OpenCLIP\nDESCRIPTION: This command installs the OpenCLIP library with PyTorch support using pip. This is necessary to use OpenCLIP models with Hugging Face.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/open_clip.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n$ pip install open_clip_torch\n```\n\n----------------------------------------\n\nTITLE: Install sagemaker library\nDESCRIPTION: This command upgrades the `sagemaker` library using pip. It ensures you are using the latest version of the SageMaker SDK.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install sagemaker --upgrade\n```\n\n----------------------------------------\n\nTITLE: Fetch All Hugging Face Hub Pull Requests - Bash\nDESCRIPTION: This snippet shows how to configure Git to fetch all pull requests from the Hugging Face Hub using refspecs. It then demonstrates how to checkout a specific pull request and push changes to it.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/repositories-pull-requests-discussions.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit fetch origin refs/pr/*:refs/remotes/origin/pr/*\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout pr/{PR_NUMBER}\n# for example: git checkout pr/42\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit push origin pr/{PR_NUMBER}:refs/pr/{PR_NUMBER}\n# for example: git push origin pr/42:refs/pr/42\n```\n\n----------------------------------------\n\nTITLE: Dockerfile ARG Directive Example\nDESCRIPTION: This Dockerfile snippet demonstrates how to declare and use environment variables using the ARG directive. The variable is passed during the build process and can be used within RUN commands.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker.md#_snippet_1\n\nLANGUAGE: Dockerfile\nCODE:\n```\n\t# Declare your environment variables with the ARG directive\n\tARG MODEL_REPO_NAME\n\n\tFROM python:latest\n\t# [...]\n\t# You can use them like environment variables\n\tRUN predict.py $MODEL_REPO_NAME\n```\n\n----------------------------------------\n\nTITLE: Visual Question Answering Widget YAML Configuration\nDESCRIPTION: This snippet configures a visual question answering widget.  Each example includes 'text' representing the question and 'src' representing the image URL.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_24\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- text: \"What animal is it?\"\n  src: \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\"\n- text: \"Where is it?\"\n  src: \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Hugging Face URL Format\nDESCRIPTION: This shows the required format for constructing Hugging Face dataset URLs for use with DuckDB. It specifies the structure including dataset owner, dataset name, and file path.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb.md#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhf://datasets/{my-username}/{my-dataset}/{path_to_file}\n```\n\n----------------------------------------\n\nTITLE: Running a Downloaded Model with Sample Factory\nDESCRIPTION: This command executes a Sample Factory model for the `mujoco-ant` environment using the `enjoy_mujoco` script. It specifies the algorithm, environment, experiment name, and training directory. You may have to specify the `--train_dir` if your local train_dir has a different path than the one in the `cfg.json`\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/sample-factory.md#_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\npython -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=<repo_name> --train_dir=./train_dir\n```\n\n----------------------------------------\n\nTITLE: Text To Video with fal.ai\nDESCRIPTION: This snippet configures the `InferenceSnippet` component to use fal.ai for Text To Video generation. It maps the `Wan-AI/Wan2.1-T2V-14B` model to the `fal-ai/wan-t2v` provider model on fal.ai.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/fal-ai.md#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-to-video\n    providersMapping={ {\"fal-ai\":{\"modelId\":\"Wan-AI/Wan2.1-T2V-14B\",\"providerModelId\":\"fal-ai/wan-t2v\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: YAML Metadata Example\nDESCRIPTION: This YAML snippet demonstrates the structure of a model or dataset card's metadata. It includes fields such as language, tags, license, and datasets, which are essential for potential users to understand the model or dataset's properties and usage terms.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlanguage: \n  - \"List of ISO 639-1 code for your language\"\n  - lang1\n  - lang2\ntags:\n- tag1\n- tag2\nlicense: \"any valid license identifier\"\ndatasets:\n- dataset1\n---\n```\n\n----------------------------------------\n\nTITLE: Installing Adapters Library\nDESCRIPTION: This command installs the Adapters library using pip. Adapters is an add-on library to Hugging Face Transformers for efficient fine-tuning of pre-trained language models using adapters and other parameter-efficient methods.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/adapters.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip install adapters\n```\n\n----------------------------------------\n\nTITLE: PyTorch Training DLC Image URI Example\nDESCRIPTION: This example demonstrates the construction of a PyTorch training DLC image URI. It specifies the AWS account ID, region, framework, mode, framework version, Transformers version, device type, Python version, and device tag.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_1\n\nLANGUAGE: None\nCODE:\n```\n763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.6.0-transformers4.4.2-gpu-py36-cu110-ubuntu18.04\n```\n\n----------------------------------------\n\nTITLE: Installing Polars\nDESCRIPTION: This command installs the Polars DataFrame library using pip. Polars is required to execute the subsequent Python code examples.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-polars.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install polars\n```\n\n----------------------------------------\n\nTITLE: Dataset Repository Structure with Splits (Train/Test)\nDESCRIPTION: This snippet demonstrates how to organize video files into different splits (train and test) using subdirectories. This structure is essential for training and evaluating machine learning models.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n train\n  1.mp4\n  2.mp4\n test\n     3.mp4\n     4.mp4\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (LLM) Inference Snippet\nDESCRIPTION: Configures an Inference Snippet for Chat Completion (LLM) using the Hyperbolic provider. It specifies the 'text-generation' pipeline and maps the 'deepseek-ai/DeepSeek-V3-0324' model ID to the same provider model ID on Hyperbolic.  The conversational flag enables a chat-style interaction.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/hyperbolic.md#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"hyperbolic\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"deepseek-ai/DeepSeek-V3-0324\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Output of Zero-Shot Image Classification\nDESCRIPTION: This is the output of the zero-shot image classification example, showing the probability of each text label given the input image.  The highest probability indicates the predicted class.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/open_clip.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nLabel probs: tensor([[0.0020, 0.0034, 0.9946]])\n```\n\n----------------------------------------\n\nTITLE: Feature Extraction Inference Snippet\nDESCRIPTION: This snippet configures the Inference API to use SambaNova for Feature Extraction. It specifies the 'intfloat/e5-mistral-7b-instruct' model on the Hugging Face Hub and maps it to 'E5-Mistral-7B-Instruct' on the SambaNova provider. The pipeline is set to 'feature-extraction'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/sambanova.md#_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<InferenceSnippet\n    pipeline=feature-extraction\n    providersMapping={ {\"sambanova\":{\"modelId\":\"intfloat/e5-mistral-7b-instruct\",\"providerModelId\":\"E5-Mistral-7B-Instruct\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Listing Organization Members Headers in JavaScript\nDESCRIPTION: Defines the headers required to authenticate and list organization members using the Hugging Face Hub API. The header includes the authorization token. This corresponds to parameters used in `huggingface_hub.list_organization_members()`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/api.md#_snippet_9\n\nLANGUAGE: JavaScript\nCODE:\n```\nheaders = { \"authorization\" :  \"Bearer $token\" }\n```\n\n----------------------------------------\n\nTITLE: Installing SetFit with pip\nDESCRIPTION: This command installs the SetFit library using pip. The `-U` flag ensures that SetFit is upgraded to the latest version if it is already installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/setfit.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U setfit\n```\n\n----------------------------------------\n\nTITLE: Malicious Pickle Output\nDESCRIPTION: This output shows the result of running the malicious pickle creation code. First the 'pwned' message is printed as part of unpickling and then the actual data is printed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nyou've been pwned !\nmy friend needs to know this\n```\n\n----------------------------------------\n\nTITLE: Installing spaCy model from Hugging Face Hub using pip\nDESCRIPTION: This command installs a specific spaCy model directly from the Hugging Face Hub using pip.  The URL points to the .whl file of the model. This allows for easy installation of spaCy models hosted on the Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spacy.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"en_core_web_sm @ https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl\"\n```\n\n----------------------------------------\n\nTITLE: Example SpanMarker Output\nDESCRIPTION: This is an example of the JSON output returned by the SpanMarker model's `predict` method.  Each object in the array represents a detected entity, including the extracted span, entity label, confidence score, and character indices in the input text.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/span_marker.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[\n    {\"span\": \"Amelia Earhart\", \"label\": \"person-other\", \"score\": 0.7629689574241638, \"char_start_index\": 0, \"char_end_index\": 14},\n    {\"span\": \"Lockheed Vega 5B\", \"label\": \"product-airplane\", \"score\": 0.9833564758300781, \"char_start_index\": 38, \"char_end_index\": 54},\n    {\"span\": \"Atlantic\", \"label\": \"location-bodiesofwater\", \"score\": 0.7621214389801025, \"char_start_index\": 66, \"char_end_index\": 74},\n    {\"span\": \"Paris\", \"label\": \"location-GPE\", \"score\": 0.9807717204093933, \"char_start_index\": 78, \"char_end_index\": 83}\n]\n```\n\n----------------------------------------\n\nTITLE: Text To Image with fal.ai\nDESCRIPTION: This snippet configures the `InferenceSnippet` component to use fal.ai for Text To Image generation. It maps the `HiDream-ai/HiDream-I1-Full` model to the `fal-ai/hidream-i1-full` provider model on fal.ai.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/fal-ai.md#_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-to-image\n    providersMapping={ {\"fal-ai\":{\"modelId\":\"HiDream-ai/HiDream-I1-Full\",\"providerModelId\":\"fal-ai/hidream-i1-full\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Installing ML-Agents library\nDESCRIPTION: To install the ML-Agents library, first clone the repository from GitHub, then navigate to the repository directory and install the necessary packages using pip.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/ml-agents.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone the repository\ngit clone https://github.com/Unity-Technologies/ml-agents\n\n# Go inside the repository and install the package\ncd ml-agents\npip3 install -e ./ml-agents-envs\npip3 install -e ./ml-agents\n```\n\n----------------------------------------\n\nTITLE: Setting Label Studio Secrets - Azure Blob Storage\nDESCRIPTION: These environment variables configure Label Studio to use Azure Blob Storage for cloud storage of labeling tasks and data. Requires specifying the account name, account key, container name, and folder. Store these as secrets for security.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-label-studio.md#_snippet_4\n\nLANGUAGE: N/A\nCODE:\n```\n`STORAGE_TYPE`: `azure`\n`STORAGE_AZURE_ACCOUNT_NAME`: `<YOUR_STORAGE_ACCOUNT>`\n`STORAGE_AZURE_ACCOUNT_KEY`: `<YOUR_STORAGE_KEY>`\n`STORAGE_AZURE_CONTAINER_NAME`: `<YOUR_CONTAINER_NAME>`\n`STORAGE_AZURE_FOLDER`: ``\n```\n\n----------------------------------------\n\nTITLE: Installing @huggingface/gguf package using npm\nDESCRIPTION: This command installs the `@huggingface/gguf` package using npm. This package provides functionality for parsing GGUF files.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/gguf.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @huggingface/gguf\n```\n\n----------------------------------------\n\nTITLE: Making a Chat Completion Request with JavaScript (@huggingface/inference)\nDESCRIPTION: This JavaScript snippet demonstrates how to use the `@huggingface/inference` library's `InferenceClient` to perform chat completion via the Inference Providers API. It initializes the client with an API key, specifies the provider and model, and defines the conversation's message. It then prints the generated message from the model to the console.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/index.md#_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { InferenceClient } from \"@huggingface/inference\";\n\nconst client = new InferenceClient(\"hf_xxxxxxxxxxxxxxxxxxxxxxxx\");\n\nconst chatCompletion = await client.chatCompletion({\n    provider: \"novita\",\n    model: \"deepseek-ai/DeepSeek-V3-0324\",\n    messages: [\n        {\n            role: \"user\",\n            content: \"How many 'G's in 'huggingface'?\",\n        },\n    ],\n});\n\nconsole.log(chatCompletion.choices[0].message);\n```\n\n----------------------------------------\n\nTITLE: Listing All Mappings - Response Example\nDESCRIPTION: Example JSON response for listing all mappings, grouped by task. Each task contains a dictionary of models, where each model contains `providerId` and `status` information.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"text-to-image\": {\n        \"black-forest-labs/FLUX.1-Canny-dev\": {\n            \"providerId\": \"black-forest-labs/FLUX.1-canny\",\n            \"status\": \"live\"\n        },\n        \"black-forest-labs/FLUX.1-Depth-dev\": {\n            \"providerId\": \"black-forest-labs/FLUX.1-depth\",\n            \"status\": \"live\"\n        }\n    },\n    \"conversational\": {\n        \"deepseek-ai/DeepSeek-R1\": {\n            \"providerId\": \"deepseek-ai/DeepSeek-R1\",\n            \"status\": \"live\"\n        }\n    },\n    \"text-generation\": {\n        \"meta-llama/Llama-2-70b-hf\": {\n            \"providerId\": \"meta-llama/Llama-2-70b-hf\",\n            \"status\": \"live\"\n        },\n        \"mistralai/Mixtral-8x7B-v0.1\": {\n            \"providerId\": \"mistralai/Mixtral-8x7B-v0.1\",\n            \"status\": \"live\"\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Example Metadata Dictionary\nDESCRIPTION: This is an example of a Python dictionary containing metadata loaded from a repository card.  It shows the 'license' field having a value, indicating that this piece of metadata is present.  Other fields would be present depending on what metadata is available in the card.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{'license': 'afl-3.0'}\n```\n\n----------------------------------------\n\nTITLE: Opening Spaces Links in New Windows (HTML)\nDESCRIPTION: This snippet demonstrates how to ensure links within a Space open in a new browser window by using the `target=\"_blank\"` attribute and `rel=\"noopener\"` for security. This is needed because Spaces are served in iframes, which restrict links from opening in the parent page by default.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-python.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<a href=\"https://hf.space\" rel=\"noopener\" target=\"_blank\">Spaces</a>\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (LLM) with Together\nDESCRIPTION: This snippet configures the InferenceSnippet component for Chat Completion (LLM) using the Together provider. It specifies the text-generation pipeline and maps the `deepseek-ai/DeepSeek-R1` model to its corresponding ID on the Together platform. The `conversational` prop enables a conversational interface.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/together.md#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"together\":{\"modelId\":\"deepseek-ai/DeepSeek-R1\",\"providerModelId\":\"deepseek-ai/DeepSeek-R1\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Diffusers Download Filter Configuration\nDESCRIPTION: This snippet shows the JSON configuration used to filter downloads for the `diffusers` library. It includes documents that match `model_index.json` and directly downloaded safetensors, ckpt, and bin files. The filter is designed to avoid double-counting downloads from the `diffusers` library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-download-stats.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"filter\": [\n\t\t{\n\t\t\t\"bool\": {\n\t\t\t\t/// Include documents that match at least one of the following rules\n\t\t\t\t\"should\": [\n\t\t\t\t\t/// Downloaded from diffusers lib\n\t\t\t\t\t{\n\t\t\t\t\t\t\"term\": { \"path\": \"model_index.json\" }\n\t\t\t\t\t},\n\t\t\t\t\t/// Direct downloads (LoRa, Auto1111 and others)\n\t\t\t\t\t/// Filter out nested safetensors and pickle weights to avoid double counting downloads from the diffusers lib\n\t\t\t\t\t{\n\t\t\t\t\t\t\"regexp\": { \"path\": \"[^/]*\\.safetensors\" }\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"regexp\": { \"path\": \"[^/]*\\.ckpt\" }\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"regexp\": { \"path\": \"[^/]*\\.bin\" }\n\t\t\t\t\t},\n\t\t\t\t],\n\t\t\t\t\"minimum_should_match\": 1,\n\t\t\t}\n\t\t},\n\t]\n}\n```\n\n----------------------------------------\n\nTITLE: Example Dictionary with Missing Fields\nDESCRIPTION: This Python example shows a dictionary where the keys represent metadata fields, and the values are the metadata content or `None` if the content is missing. The example shows the structure of the dictionary returned by `create_metadata_key_dict`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n{'tags': None,\n 'license': 'afl-3.0',\n 'library_name': None,\n 'datasets': None,\n 'metrics': None,\n 'co2': None,\n 'pipeline_tag': None}\n```\n\n----------------------------------------\n\nTITLE: Using pandas-image-methods with PIL\nDESCRIPTION: This snippet shows how to integrate `pandas-image-methods` with `PIL` to enable image processing directly on a Pandas DataFrame column. It registers a series accessor and opens image files into the DataFrame. The `folder_path` and `df[\"file_name\"]` are used to construct the full file path to open the images.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-pandas.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom pandas_image_methods import PILMethods\n\npd.api.extensions.register_series_accessor(\"pil\")(PILMethods)\n\ndf[\"image\"] = (folder_path + df[\"file_name\"]).pil.open()\ndf.to_parquet(\"data.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Creating Metadata Breakdown Table\nDESCRIPTION: This Python function creates a markdown table from a dictionary containing metadata fields and their values.  The library `tabulate` is used to create the table in GitHub markdown format.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks-guide-metadata-review.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef create_metadata_breakdown_table(desired_metadata_dictionary):\n    # [...]\n    return tabulate(\n        table_data, tablefmt=\"github\", headers=(\"Metadata Field\", \"Provided Value\")\n    )\n```\n\n----------------------------------------\n\nTITLE: Text To Image with Together\nDESCRIPTION: This snippet configures the InferenceSnippet component for Text to Image using the Together provider. It specifies the text-to-image pipeline and maps the `black-forest-labs/FLUX.1-dev` model to its corresponding ID on the Together platform.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/together.md#_snippet_3\n\nLANGUAGE: JSX\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-to-image\n    providersMapping={ {\"together\":{\"modelId\":\"black-forest-labs/FLUX.1-dev\",\"providerModelId\":\"black-forest-labs/FLUX.1-dev\"} } }\n/>\n```\n\n----------------------------------------\n\nTITLE: Example updatedConfig Webhook Payload (Empty)\nDESCRIPTION: This JSON payload shows an example of the `updatedConfig` property when the event scope is `repo.config` but the config key is not supported by the webhook.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"updatedConfig\": {}\n}\n\n```\n\n----------------------------------------\n\nTITLE: Write Dask DataFrame to Hugging Face Hub\nDESCRIPTION: This Python snippet demonstrates writing a Dask DataFrame to a Parquet file on the Hugging Face Hub using the `hf://` path. It shows how to write to a single directory and to separate directories for train/validation/test splits. Requires `dask.dataframe` and a logged-in Hugging Face account.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-dask.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport dask.dataframe as dd\n\ndf.to_parquet(\"hf://datasets/username/my_dataset\")\n\n# or write in separate directories if the dataset has train/validation/test splits\ndf_train.to_parquet(\"hf://datasets/username/my_dataset/train\")\ndf_valid.to_parquet(\"hf://datasets/username/my_dataset/validation\")\ndf_test .to_parquet(\"hf://datasets/username/my_dataset/test\")\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (VLM) with Together\nDESCRIPTION: This snippet configures the InferenceSnippet component for Chat Completion (VLM) using the Together provider. It specifies the image-text-to-text pipeline and maps the `meta-llama/Llama-4-Scout-17B-16E-Instruct` model to its corresponding ID on the Together platform. The `conversational` prop enables a conversational interface.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/together.md#_snippet_1\n\nLANGUAGE: JSX\nCODE:\n```\n<InferenceSnippet\n    pipeline=image-text-to-text\n    providersMapping={ {\"together\":{\"modelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\"providerModelId\":\"meta-llama/Llama-4-Scout-17B-16E-Instruct\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: API: Handle Access Request\nDESCRIPTION: This API endpoint changes the status of a given access request to 'accepted', 'rejected', or 'pending'. It requires a token with write access to the repository and accepts a JSON payload.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-gated.md#_snippet_3\n\nLANGUAGE: HTTP\nCODE:\n```\nPOST /api/models/{repo_id}/user-access-request/handle\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nHeaders: {\"authorization\": \"Bearer $token\"}\n```\n\nLANGUAGE: JSON\nCODE:\n```\nPayload: {\"status\": \"accepted\"/\"rejected\"/\"pending\", \"user\": \"username\", \"rejectionReason\": \"Optional rejection reason that will be visible to the user (max 200 characters).\"}\n```\n\n----------------------------------------\n\nTITLE: WebDataset File Prefix Example\nDESCRIPTION: Shows a WebDataset directory structure, demonstrating how images and their metadata share the same file prefix inside a TAR archive.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_13\n\nLANGUAGE: filesystem\nCODE:\n```\ntrain-0000/\n 000.jpg\n 000.json\n 001.jpg\n 001.json\n ...\n 999.jpg\n 999.json\n```\n\n----------------------------------------\n\nTITLE: Audio Files with Metadata CSV\nDESCRIPTION: This example illustrates how to include a `metadata.csv` file in the dataset repository alongside audio files. The CSV file links audio files with additional information (e.g., transcriptions).\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-audio.md#_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_dataset_repository/\n 1.wav\n 2.wav\n 3.wav\n 4.wav\n metadata.csv\n```\n\n----------------------------------------\n\nTITLE: Example updatedRefs Webhook Payload\nDESCRIPTION: This JSON payload shows an example of the `updatedRefs` property in a webhook payload triggered by code changes. It is an array of updated references, where each reference includes the ref name (e.g., branch or tag), the old SHA, and the new SHA. Newly created references have `oldSha` set to null, and deleted references have `newSha` set to null.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/webhooks.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"ref\": \"refs/heads/main\",\n    \"oldSha\": \"ce9a4674fa833a68d5a73ec355f0ea95eedd60b7\",\n    \"newSha\": \"575db8b7a51b6f85eb06eee540738584589f131c\"\n  },\n  {\n    \"ref\": \"refs/tags/test\",\n    \"oldSha\": null,\n    \"newSha\": \"575db8b7a51b6f85eb06eee540738584589f131c\"\n  }\n]\n\n```\n\n----------------------------------------\n\nTITLE: Starting DuckDB CLI\nDESCRIPTION: This command starts the DuckDB command-line interface. It assumes that the executable is located in the current directory.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./duckdb\n```\n\n----------------------------------------\n\nTITLE: Logging in with Hugging Face CLI\nDESCRIPTION: This command uses the Hugging Face CLI to log in to your Hugging Face account. This stores your token in `~/.cache/huggingface/token`, which can then be used by the `CREDENTIAL_CHAIN` provider.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-duckdb-auth.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Tracking LFS File References using Git Log\nDESCRIPTION: This command helps track the history of a specific LFS file within a Git repository by using its SHA-256 OID. It searches through all branches and commits to find instances where the file was added, modified, or removed.  It requires Git to be installed and configured for the repository in question.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/storage-limits.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit log --all -p -S <SHA-256-OID>\n```\n\n----------------------------------------\n\nTITLE: Chat Completion (LLM) with Fireworks AI\nDESCRIPTION: This snippet configures an InferenceSnippet component for Chat Completion (LLM) using Fireworks AI. It specifies the `text-generation` pipeline and maps the model `deepseek-ai/DeepSeek-V3-0324` to the provider model ID `accounts/fireworks/models/deepseek-v3-0324` on Fireworks AI. The `conversational` prop enables a conversational interface.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/providers/fireworks-ai.md#_snippet_0\n\nLANGUAGE: JSX\nCODE:\n```\n<InferenceSnippet\n    pipeline=text-generation\n    providersMapping={ {\"fireworks-ai\":{\"modelId\":\"deepseek-ai/DeepSeek-V3-0324\",\"providerModelId\":\"accounts/fireworks/models/deepseek-v3-0324\"} } }\nconversational />\n```\n\n----------------------------------------\n\nTITLE: Specifying Library Name with Tags in YAML\nDESCRIPTION: This YAML snippet shows how to specify the library name for a model using tags in the model card metadata. If `library_name` is not specified, the Hub will look for a tag with the name of a supported library.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-cards.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ntags:\n- flair\n```\n\n----------------------------------------\n\nTITLE: Audio-to-Audio Widget YAML Configuration\nDESCRIPTION: This snippet configures an audio-to-audio widget with two examples.  Each example includes a 'src' field representing the URL of the audio file and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_15\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n  example_title: Librispeech sample 1\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\n  example_title: Librispeech sample 2\n```\n\n----------------------------------------\n\nTITLE: Login to Hugging Face CLI\nDESCRIPTION: This bash command demonstrates how to log in to the Hugging Face Hub using the command-line interface. This is required for authenticated access to gated datasets.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-gated.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: API: Retrieve Rejected Access Requests\nDESCRIPTION: This API endpoint retrieves the list of rejected access requests for a gated model. It requires a token with write access to the repository.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-gated.md#_snippet_2\n\nLANGUAGE: HTTP\nCODE:\n```\nGET /api/models/{repo_id}/user-access-request/rejected\n```\n\nLANGUAGE: HTTP\nCODE:\n```\nHeaders: {\"authorization\": \"Bearer $token\"}\n```\n\n----------------------------------------\n\nTITLE: Pickle File Contents (Cat Command)\nDESCRIPTION: This shows the contents of a pickle file after pickling a simple class instance. It shows the class name, attributes and their values. Note that this is still not human readable.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# cat payload.pkl\n__main__Data)}important_stuff42sb.%\n```\n\n----------------------------------------\n\nTITLE: Delete SageMaker Endpoint\nDESCRIPTION: This snippet deletes the SageMaker endpoint to release resources. It calls the `delete_endpoint` method on the predictor object.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/getting-started.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npredictor.delete_endpoint()\n```\n\n----------------------------------------\n\nTITLE: Verifying TensorFlow GPU Availability\nDESCRIPTION: This code snippet checks if TensorFlow recognizes the CUDA device. It's used to verify that TensorFlow is using the GPU after installation. Requires TensorFlow to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-gpus.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n```\n\n----------------------------------------\n\nTITLE: Model Specifications Placeholder\nDESCRIPTION: This snippet represents a placeholder for details about the model's objective and architecture.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n`model_specs`\n```\n\n----------------------------------------\n\nTITLE: Configure Public Certificate format for SAML with Okta\nDESCRIPTION: This code snippet demonstrates the required format for the public certificate when configuring SAML SSO with Okta on Hugging Face. The certificate content should be enclosed within the `-----BEGIN CERTIFICATE-----` and `-----END CERTIFICATE-----` markers. The actual certificate content replaces `{certificate}`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-sso-okta-saml.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-----BEGIN CERTIFICATE-----\n{certificate}\n-----END CERTIFICATE-----\n```\n\n----------------------------------------\n\nTITLE: Voice Activity Detection Widget YAML Configuration\nDESCRIPTION: This snippet configures a Voice Activity Detection widget. Each example contains a 'src' field with the audio file URL and an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_17\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n  example_title: Librispeech sample 1\n- src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\n  example_title: Librispeech sample 2\n```\n\n----------------------------------------\n\nTITLE: Install huggingface_hub with hf_xet support\nDESCRIPTION: Installs the `huggingface_hub` library with `hf_xet` support, enabling Xet storage functionality. Requires `pip` to be installed. This ensures a Xet-aware client for uploads and downloads.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/storage-backends.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U huggingface_hub[hf_xet]\n```\n\n----------------------------------------\n\nTITLE: Specify Image Feature Type in YAML\nDESCRIPTION: Shows how to manually define the image feature type in YAML within the README header for Parquet datasets, specifying the 'bytes' and 'path' fields.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\ndataset_info:\n  features:\n  - name: image\n    dtype: image\n  - name: caption\n    dtype: string\n```\n\n----------------------------------------\n\nTITLE: Pickle File Contents (Hexyl Command)\nDESCRIPTION: This shows the hex representation of a pickled class instance using hexyl. It is useful for analyzing the binary structure of the pickle file and the opcodes.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# hexyl payload.pkl\n\n00000000 80 04 95 33 00 00 00 00  00 00 00 8c 08 5f 5f 6d 30000000__m\n00000010 61 69 6e 5f 5f 94 8c 04  44 61 74 61 94 93 94 29 ain__Data)\n00000020 81 94 7d 94 8c 0f 69 6d  70 6f 72 74 61 6e 74 5f }important_\n00000030 73 74 75 66 66 94 8c 02  34 32 94 73 62 2e       stuff42sb.  \n\n```\n\n----------------------------------------\n\nTITLE: YAML Example: Defining custom license in Model Card\nDESCRIPTION: This YAML snippet demonstrates how to define a custom license ('other') in the Model Card metadata, including the license name and a link to the license URL. This format is used when the model's license is not officially supported by the Hugging Face Hub and requires manual specification.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_license_other.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Example from https://huggingface.co/coqui/XTTS-v1\n---\nlicense: other\nlicense_name: coqui-public-model-license\nlicense_link: https://coqui.ai/cpml\n---\n```\n\n----------------------------------------\n\nTITLE: Sentence Similarity Widget YAML Configuration\nDESCRIPTION: This snippet configures a sentence similarity widget. Each example includes a 'source_sentence' and a list of 'sentences' to compare against, along with an 'example_title'.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/models-widgets-examples.md#_snippet_10\n\nLANGUAGE: YAML\nCODE:\n```\nwidget:\n- source_sentence: \"That is a happy person\"\n  sentences:\n    - \"That is a happy dog\"\n    - \"That is a very happy person\"\n    - \"Today is a sunny day\"\n  example_title: \"Happy\"\n```\n\n----------------------------------------\n\nTITLE: Updating a Mapping Item Status\nDESCRIPTION: This HTTP PUT request updates the status of a model mapping item. The request body contains the `hfModel` and the desired `status` (either \"live\" or \"staging\").\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_4\n\nLANGUAGE: http\nCODE:\n```\nPUT /api/partners/{provider}/models/status\n```\n\n----------------------------------------\n\nTITLE: Example SetFit Model Output\nDESCRIPTION: This is an example of the output from the `model.predict` function. It shows the returned list of labels (positive, negative).\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/setfit.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n['positive', 'negative']\n```\n\n----------------------------------------\n\nTITLE: Disable Dataset Viewer YAML\nDESCRIPTION: This YAML snippet demonstrates how to disable the Dataset Viewer for a specific dataset.  It involves adding a `viewer` property with the value `false` in the dataset's `README.md` file. This configuration prevents the dataset from being displayed in the Dataset Viewer.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-viewer-configure.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nviewer: false\n---\n```\n\n----------------------------------------\n\nTITLE: Dataset Card Metadata Example YAML\nDESCRIPTION: This YAML snippet demonstrates the structure of a dataset card's metadata, including the `language` field. The `language` field should contain a list of ISO 639-1 language codes. Other fields shown are `pretty_name`, `tags`, `license`, and `task_categories`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/hacktoberfest_challenges/datasets_without_language.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\nlanguage:\n- \"List of ISO 639-1 code for your language\"\n- lang1\npretty_name: \"Pretty Name of the Dataset\"\ntags:\n- tag1\n- tag2\nlicense: \"any valid license identifier\"\ntask_categories:\n- task1\n```\n\n----------------------------------------\n\nTITLE: Creating HTML Form for Text Generation\nDESCRIPTION: This HTML code creates a simple form with an input field for text prompts and a submit button. The form is used to interact with the Flan T5 model for text generation. It also includes a paragraph element to display the output.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/spaces-sdks-docker-first-demo.md#_snippet_9\n\nLANGUAGE: html\nCODE:\n```\n<main>\n  <section id=\"text-gen\">\n    <h2>Text generation using Flan T5</h2>\n    <p>\n      Model:\n      <a\n        href=\"https://huggingface.co/google/flan-t5-small\"\n        rel=\"noreferrer\"\n        target=\"_blank\"\n        >google/flan-t5-small\n      </a>\n    </p>\n    <form class=\"text-gen-form\">\n      <label for=\"text-gen-input\">Text prompt</label>\n      <input\n        id=\"text-gen-input\"\n        type=\"text\"\n        value=\"German: There are many ducks\"\n      />\n      <button id=\"text-gen-submit\">Submit</button>\n      <p class=\"text-gen-output\"></p>\n    </form>\n  </section>\n</main>\n```\n\n----------------------------------------\n\nTITLE: Listing All Mappings\nDESCRIPTION: This HTTP GET request retrieves all model mapping items from the database for a specific provider. The optional `status` parameter can be used to filter the results by status (either \"staging\" or \"live\").\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_6\n\nLANGUAGE: http\nCODE:\n```\nGET /api/partners/{provider}/models?status=staging|live\n```\n\n----------------------------------------\n\nTITLE: Parquet Dataset Structure\nDESCRIPTION: Illustrates the structure for storing image datasets in Parquet format, where images and metadata are embedded within a single file.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-image.md#_snippet_14\n\nLANGUAGE: filesystem\nCODE:\n```\nmy_dataset_repository/\n train.parquet\n```\n\n----------------------------------------\n\nTITLE: Updating a Mapping Item Status - JSON Body\nDESCRIPTION: This JSON payload is sent with the PUT request to update the model status. It requires the `hfModel` which is the name of the model on HF, and `status` which can be either \"live\" or \"staging\".\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"hfModel\": \"namespace/model-name\", // The name of the model on HF\n    \"status\": \"live\" | \"staging\" // The new status, one of \"staging\" or \"live\"\n}   \n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation Placeholder\nDESCRIPTION: This snippet represents a placeholder for the BibTeX citation of the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n`citation_bibtex`\n```\n\n----------------------------------------\n\nTITLE: Logging into Hugging Face via CLI\nDESCRIPTION: This command logs into the Hugging Face Hub using the Hugging Face CLI. It requires the `huggingface-cli` to be installed.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-fiftyone.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhuggingface-cli login\n```\n\n----------------------------------------\n\nTITLE: Parameterized Request Example (JSON)\nDESCRIPTION: This JSON snippet demonstrates a request to the Inference Toolkit API with custom parameters. It shows how to adjust the behavior of the model, in this case using repetition_penalty and length_penalty.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/reference.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"inputs\": \"Hugging Face, the winner of VentureBeats Innovation in Natural Language Process/Understanding Award for 2021, is looking to level the playing field. The team, launched by Clment Delangue and Julien Chaumond in 2016, was recognized for its work in democratizing NLP, the global market value for which is expected to hit $35.1 billion by 2026. This week, Googles former head of Ethical AI Margaret Mitchell joined the team.\",\n  \"parameters\": {\n    \"repetition_penalty\": 4.0,\n    \"length_penalty\": 1.5\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Requesting Cost - HTTP POST\nDESCRIPTION: This HTTP POST request is sent to the partner's billing API to request the cost for a list of request IDs. The `Content-Type` header is set to `application/json`.  The body contains a JSON-encoded object with an array of `requestIds`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_8\n\nLANGUAGE: http\nCODE:\n```\nPOST {your URL here}\nContent-Type: application/json\n\n{\n    \"requestIds\": [\n        \"deadbeef0\",\n        \"deadbeef1\",\n        \"deadbeef2\",\n        \"deadbeef3\"\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Cost Response - HTTP 200 OK\nDESCRIPTION: This is an example of the HTTP 200 OK response from the partner's billing API.  The response body is JSON-encoded and contains an array of objects. Each object contains the `requestId` and its corresponding `costNanoUsd` in nano-USD.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/register-as-a-provider.md#_snippet_9\n\nLANGUAGE: http\nCODE:\n```\nHTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"requests\": [\n        { \"requestId\": \"deadbeef0\", \"costNanoUsd\": 100 },\n        { \"requestId\": \"deadbeef1\", \"costNanoUsd\": 100 },\n        { \"requestId\": \"deadbeef2\", \"costNanoUsd\": 100 },\n        { \"requestId\": \"deadbeef3\", \"costNanoUsd\": 100 }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: YAML Metadata Configuration for Model Card\nDESCRIPTION: This YAML snippet shows the basic metadata configuration for a Hugging Face model card. It includes fields like pipeline tag, library name, language, license, and datasets, which are essential for discoverability and categorization on the Hub.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-release-checklist.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n---\npipeline_tag: text-generation  # Specify the task\nlibrary_name: transformers  # Specify the library\nlanguage:\n  - en  # List language for your model\nlicense: apache-2.0 # Specify a license\ndatasets:\n  - username/dataset  # List datasets used for training\nbase_model: username/base-model  # If applicable\n---\n```\n\n----------------------------------------\n\nTITLE: Example: Tracking LFS File References using Git Log\nDESCRIPTION: This is an example demonstrating the usage of the `git log` command to trace the history of a specific LFS file, identified by its SHA-256 OID.  The output shows the commit history related to the specified file, including additions, modifications, and deletions.  It requires Git to be installed and configured for the repository in question.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/storage-limits.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit log --all -p -S 68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3\n\ncommit 5af368743e3f1d81c2a846f7c8d4a028ad9fb021\nDate:   Sun Apr 28 02:01:18 2024 +0200\n\n    Update LayerNorm tensor names to weight and bias\n\ndiff --git a/model.safetensors b/model.safetensors\nindex a090ee7..e79c80e 100644\n--- a/model.safetensors\n+++ b/model.safetensors\n@@ -1,3 +1,3 @@\n version https://git-lfs.github.com/spec/v1\n-oid sha256:68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3\n+oid sha256:0bb7a1683251b832d6f4644e523b325adcf485b7193379f5515e6083b5ed174b\n size 440449768\n\ncommit 0a6aa9128b6194f4f3c4db429b6cb4891cdb421b (origin/pr/28)\nDate:   Wed Nov 16 15:15:39 2022 +0000\n\n    Adding `safetensors` variant of this model (#15)\n    \n    \n    - Adding `safetensors` variant of this model (18c87780b5e54825a2454d5855a354ad46c5b87e)\n    \n    \n    Co-authored-by: Nicolas Patry <Narsil@users.noreply.huggingface.co>\n\ndiff --git a/model.safetensors b/model.safetensors\nnew file mode 100644\nindex 0000000..a090ee7\n--- /dev/null\n+++ b/model.safetensors\n@@ -0,0 +1,3 @@\n+version https://git-lfs.github.com/spec/v1\n+oid sha256:68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3\n+size 440449768\n\ncommit 18c87780b5e54825a2454d5855a354ad46c5b87e (origin/pr/15)\nDate:   Thu Nov 10 09:35:55 2022 +0000\n\n    Adding `safetensors` variant of this model\n\ndiff --git a/model.safetensors b/model.safetensors\nnew file mode 100644\nindex 0000000..a090ee7\n--- /dev/null\n+++ b/model.safetensors\n@@ -0,0 +1,3 @@\n+version https://git-lfs.github.com/spec/v1\n+oid sha256:68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3\n+size 440449768\n\n```\n\n----------------------------------------\n\nTITLE: How to Get Started Code Placeholder\nDESCRIPTION: This snippet represents a placeholder for code that demonstrates how to use the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n`get_started_code`\n```\n\n----------------------------------------\n\nTITLE: Example Input Data for Batch Transform (JSONL)\nDESCRIPTION: This snippet shows the structure of the `input.jsonl` file expected by the batch transform job. Each line is a JSON object with an `inputs` key containing the text to be processed.  The `content_type` in `batch_job.transform` must be set to `application/json` and `split_type` to `Line`.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/sagemaker/inference.md#_snippet_11\n\nLANGUAGE: jsonl\nCODE:\n```\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"SageMaker is pretty cool\"}\n{\"inputs\":\"this movie is terrible\"}\n{\"inputs\":\"this movie is amazing\"}\n```\n\n----------------------------------------\n\nTITLE: Malicious Pickle File Contents (Cat Command)\nDESCRIPTION: This shows the contents of the malicious pickle file created with fickling using cat. It includes instructions to execute code.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/security-pickle.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# cat payload.pkl\nc__builtin__\nexec\n(Vprint(\"you've been pwned !\")\ntR my friend needs to know this.%\n```\n\n----------------------------------------\n\nTITLE: Dataset Repository with Relative Paths in Metadata\nDESCRIPTION: This snippet shows a dataset directory structure where the metadata.csv is in a parent directory, requiring relative paths to be specified in the file_name column.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/datasets-video.md#_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nmy_dataset_repository/\n train\n     videos\n      1.mp4\n      2.mp4\n      3.mp4\n      4.mp4\n     metadata.csv\n```\n\n----------------------------------------\n\nTITLE: Install Dependencies using pnpm\nDESCRIPTION: This command installs the necessary dependencies for the Hugging Face Hub documentation project using the pnpm package manager. It ensures that all required libraries and tools are available for building and running the documentation.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/scripts/inference-providers/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npnpm install\n```\n\n----------------------------------------\n\nTITLE: YAML for Carbon Emissions\nDESCRIPTION: This YAML snippet shows how to specify the carbon emissions associated with training your model. This information helps environmentally conscious users and organizations make informed decisions.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-release-checklist.md#_snippet_7\n\nLANGUAGE: YAML\nCODE:\n```\n---\nco2_eq_emissions:\n  emissions: 123.45\n  source: \"CodeCarbon\"\n  training_type: \"pre-training\"\n  geographical_location: \"US-East\"\n  hardware_used: \"8xA100 GPUs\"\n---\n```\n\n----------------------------------------\n\nTITLE: Get Model Inference Status - No Inference (cURL)\nDESCRIPTION: This snippet illustrates how to retrieve the inference status for a model without a defined inference endpoint. Using cURL, it queries the `/api/models/{model_id}` endpoint with `expand[]=inference`. The absence of an 'inference' field in the JSON response indicates that no inference endpoint is available.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/inference-providers/hub-api.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\n# Get inference status (no inference)\n~ curl -s https://huggingface.co/api/models/manycore-research/SpatialLM-Llama-1B?expand[]=inference\n{\n\"_id\": \"67d3b141d8b6e20c6d009c8b\",\n\"id\": \"manycore-research/SpatialLM-Llama-1B\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Standard License in Model Card Metadata (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to define a standard, officially-supported license in a Hugging Face Model Card's metadata.  The `license` field is set to the identifier of the standard license (e.g., `llama2`). No other fields are required. This allows users to easily identify and filter models based on their license type.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlicense: llama2\n---\n```\n\n----------------------------------------\n\nTITLE: More Information Placeholder\nDESCRIPTION: This snippet represents a placeholder for links to additional information about the model.\nSOURCE: https://github.com/huggingface/hub-docs/blob/main/docs/hub/model-card-annotated.md#_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n`more_information`\n```"
  }
]
[
  {
    "owner": "sciml",
    "repo": "optimization.jl",
    "content": "TITLE: Solving Rosenbrock Problem with Various Constraints\nDESCRIPTION: Demonstrates solving the Rosenbrock problem with different types of constraints, including equality constraints, inequality constraints, and box constraints using the IPNewton solver.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\ncons = (res, x, p) -> res .= [x[1]^2 + x[2]^2]\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff(); cons = cons)\n\nprob = OptimizationProblem(optf, x0, _p, lcons = [-Inf], ucons = [Inf])\nsol = solve(prob, IPNewton()) # Note that -Inf < x[1]^2 + x[2]^2 < Inf is always true\n\nprob = OptimizationProblem(optf, x0, _p, lcons = [-5.0], ucons = [10.0])\nsol = solve(prob, IPNewton()) # Again, -5.0 < x[1]^2 + x[2]^2 < 10.0\n\nprob = OptimizationProblem(optf, x0, _p, lcons = [-Inf], ucons = [Inf],\n    lb = [-500.0, -500.0], ub = [50.0, 50.0])\nsol = solve(prob, IPNewton())\n\nprob = OptimizationProblem(optf, x0, _p, lcons = [0.5], ucons = [0.5],\n    lb = [-500.0, -500.0], ub = [50.0, 50.0])\nsol = solve(prob, IPNewton())\n\n# Notice now that x[1]^2 + x[2]^2 ≈ 0.5:\nres = zeros(1)\ncons(res, sol.u, _p)\nprintln(res)\n```\n\n----------------------------------------\n\nTITLE: Solving Short-Term Financing Linear Optimization Problem with HiGHS in Julia\nDESCRIPTION: This code implements a short-term financing optimization model to maximize end-of-period wealth by managing credit line usage, commercial paper issuance, and surplus funds. It uses the Optimization.jl and HiGHS packages to define and solve the linear optimization problem with appropriate constraints and bounds.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/linearandinteger.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationMOI, ModelingToolkit, HiGHS, LinearAlgebra\n\n@variables u[1:5] [bounds = (0.0, 100.0)]\n@variables v[1:3] [bounds = (0.0, Inf)]\n@variables w[1:5] [bounds = (0.0, Inf)]\n@variables m [bounds = (0.0, Inf)]\n\ncons = [u[1] + v[1] - w[1] ~ 150 # January\n        u[2] + v[2] - w[2] - 1.01u[1] + 1.003w[1] ~ 100 # February\n        u[3] + v[3] - w[3] - 1.01u[2] + 1.003w[2] ~ -200 # March\n        u[4] - w[4] - 1.02v[1] - 1.01u[3] + 1.003w[3] ~ 200 # April\n        u[5] - w[5] - 1.02v[2] - 1.01u[4] + 1.003w[4] ~ -50 # May\n        -m - 1.02v[3] - 1.01u[5] + 1.003w[5] ~ -300]\n\n@named optsys = OptimizationSystem(m, [u..., v..., w..., m], [], constraints = cons)\noptsys = complete(optsys)\noptprob = OptimizationProblem(optsys,\n    vcat(fill(0.0, 13), 300.0);\n    grad = true,\n    hess = true,\n    sense = Optimization.MaxSense)\nsol = solve(optprob, HiGHS.Optimizer())\n```\n\n----------------------------------------\n\nTITLE: Optimizing Constrained Rosenbrock Function using IPNewton\nDESCRIPTION: Demonstrates how to optimize the Rosenbrock function with constraints using the IPNewton optimizer. The example includes function definition, constraint setup, and problem solving using Optimization.jl framework.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optim.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationOptimJL\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\ncons = (res, x, p) -> res .= [x[1]^2 + x[2]^2]\nx0 = zeros(2)\np = [1.0, 100.0]\nprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff(); cons = cons)\nprob = Optimization.OptimizationProblem(prob, x0, p, lcons = [-5.0], ucons = [10.0])\nsol = solve(prob, IPNewton())\n```\n\n----------------------------------------\n\nTITLE: Using NLopt.jl with Symbolic Differentiation\nDESCRIPTION: Solves the Rosenbrock problem using different NLopt algorithms with gradients generated symbolically via ModelingToolkit.jl, demonstrating both local (BOBYQA, L-BFGS) and global (MLSL) optimization methods.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_11\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationNLopt, ModelingToolkit\noptf = OptimizationFunction(rosenbrock, Optimization.AutoModelingToolkit())\nprob = OptimizationProblem(optf, x0, _p)\n\nsol = solve(prob, Opt(:LN_BOBYQA, 2))\nsol = solve(prob, Opt(:LD_LBFGS, 2))\n```\n\n----------------------------------------\n\nTITLE: Solving Constrained Optimization Problem with LBFGS in Julia\nDESCRIPTION: This example shows how to solve a constrained optimization problem using LBFGS in Optimization.jl. It includes nonlinear constraints and bounds, demonstrating the flexibility of the solver.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optimization.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nfunction con2_c(res, x, p)\n    res .= [x[1]^2 + x[2]^2, (x[2] * sin(x[1]) + x[1]) - 5]\nend\n\noptf = OptimizationFunction(rosenbrock, AutoZygote(), cons = con2_c)\nprob = OptimizationProblem(optf, x0, p, lcons = [1.0, -Inf],\n    ucons = [1.0, 0.0], lb = [-1.0, -1.0],\n    ub = [1.0, 1.0])\nres = solve(prob, Optimization.LBFGS(), maxiters = 100)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multistart Optimization with EnsembleProblem in Julia\nDESCRIPTION: This code snippet demonstrates the implementation of multistart optimization using EnsembleProblem in Julia. It defines the rosenbrock function, sets up single and ensemble optimization problems, and compares their results. The ensemble approach uses multiple initial points to achieve a lower objective value.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/ensemble.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationOptimJL, Random\n\nRandom.seed!(100)\n\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\n\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(optf, x0, [1.0, 100.0])\n@time sol1 = Optimization.solve(prob, OptimizationOptimJL.BFGS(), maxiters = 5)\n\n@show sol1.objective\n\nensembleprob = Optimization.EnsembleProblem(\n    prob, [x0, x0 .+ rand(2), x0 .+ rand(2), x0 .+ rand(2)])\n\n@time sol = Optimization.solve(ensembleprob, OptimizationOptimJL.BFGS(),\n    EnsembleThreads(), trajectories = 4, maxiters = 5)\n@show findmin(i -> sol[i].objective, 1:4)[1]\n```\n\n----------------------------------------\n\nTITLE: Solving Rosenbrock Problem with LBFGS in Julia\nDESCRIPTION: Demonstrates how to solve the Rosenbrock optimization problem using LBFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) algorithm with automatic differentiation via Zygote.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/getting_started.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n# Import the package and define the problem to optimize\nusing Optimization, Zygote\nrosenbrock(u, p) = (p[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2\nu0 = zeros(2)\np = [1.0, 100.0]\n\noptf = OptimizationFunction(rosenbrock, AutoZygote())\nprob = OptimizationProblem(optf, u0, p)\n\nsol = solve(prob, Optimization.LBFGS())\n```\n\n----------------------------------------\n\nTITLE: Optimization with Automatic Differentiation using BFGS\nDESCRIPTION: This example shows how to use automatic differentiation with ForwardDiff to create an optimization function and solve it using the BFGS algorithm.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/README.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nusing ForwardDiff\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(f, x0, p)\nsol = solve(prob, BFGS())\n```\n\n----------------------------------------\n\nTITLE: Solving Rosenbrock with Circle Constraint\nDESCRIPTION: Solves the Rosenbrock problem with a circular constraint (x[1]^2 + x[2]^2 ≤ 0.25^2) using the IPNewton method from Optim.jl with forward-mode automatic differentiation.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nfunction con_c(res, x, p)\n    res .= [x[1]^2 + x[2]^2]\nend\n\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff(); cons = con_c)\nprob = OptimizationProblem(optf, x0, _p, lcons = [-Inf], ucons = [0.25^2])\nsol = solve(prob, IPNewton()) # -Inf < cons_circ(sol.u, _p) = 0.25^2\n```\n\n----------------------------------------\n\nTITLE: Defining the Rosenbrock Problem with Automatic Differentiation in Julia\nDESCRIPTION: Sets up the Rosenbrock optimization problem with initial conditions and parameters, then creates an OptimizationFunction with ForwardDiff for automatic differentiation.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n# Define the problem to solve\nusing Optimization, ForwardDiff, Zygote\n\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\n_p = [1.0, 100.0]\n\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nl1 = rosenbrock(x0, _p)\nprob = OptimizationProblem(f, x0, _p)\n```\n\n----------------------------------------\n\nTITLE: Solving Box-Constrained Rosenbrock Problem with NLopt.jl\nDESCRIPTION: Applies NLopt.jl solvers to the Rosenbrock problem with box constraints, demonstrating both local (L-BFGS) and global (MLSL) optimization strategies with bounds on the variables.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_12\n\nLANGUAGE: julia\nCODE:\n```\nprob = OptimizationProblem(optf, x0, _p, lb = [-1.0, -1.0], ub = [0.8, 0.8])\nsol = solve(prob, Opt(:LD_LBFGS, 2))\nsol = solve(prob, Opt(:G_MLSL_LDS, 2), local_method = Opt(:LD_LBFGS, 2), maxiters = 10000) #a global optimizer with random starts of local optimization\n```\n\n----------------------------------------\n\nTITLE: Integer Programming with Knapsack Problem Example\nDESCRIPTION: Implementation of the classical Knapsack Problem using integer linear programming with Juniper solver and Ipopt as the nonlinear solver.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/mathoptinterface.md#2025-04-21_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nv = [1.0, 2.0, 4.0, 3.0]\nw = [5.0, 4.0, 3.0, 2.0]\nW = 4.0\nu0 = [0.0, 0.0, 0.0, 1.0]\n\noptfun = OptimizationFunction((u, p) -> -v'u, cons = (res, u, p) -> res .= w'u,\n    Optimization.AutoForwardDiff())\n\noptprob = OptimizationProblem(optfun, u0; lb = zero.(u0), ub = one.(u0),\n    int = ones(Bool, length(u0)),\n    lcons = [-Inf;], ucons = [W;])\n\nnl_solver = OptimizationMOI.MOI.OptimizerWithAttributes(Ipopt.Optimizer,\n    \"print_level\" => 0)\nminlp_solver = OptimizationMOI.MOI.OptimizerWithAttributes(Juniper.Optimizer,\n    \"nl_solver\" => nl_solver)\n\nres = solve(optprob, minlp_solver)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function with LBFGS in Julia\nDESCRIPTION: This example demonstrates how to optimize the Rosenbrock function using the Limited-memory BFGS algorithm from Optim.jl. The problem includes bound constraints on the variables and uses automatic differentiation for computing gradients.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optim.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationOptimJL\nrosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\noptprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(optprob, x0, p, lb = [-1.0, -1.0], ub = [0.8, 0.8])\nsol = solve(prob, Optim.LBFGS())\n```\n\n----------------------------------------\n\nTITLE: Global Optimization of Rosenbrock Function using Particle Swarm in Julia\nDESCRIPTION: This example demonstrates global optimization of the Rosenbrock function using Particle Swarm Optimization from Optim.jl. It sets up the optimization problem with box constraints and solves it using Optim.ParticleSwarm() with specified lower and upper bounds and number of particles.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optim.md#2025-04-21_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationOptimJL\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, Optim.ParticleSwarm(lower = prob.lb, upper = prob.ub, n_particles = 100))\n```\n\n----------------------------------------\n\nTITLE: Solving Unconstrained Rosenbrock Problem with LBFGS in Julia\nDESCRIPTION: This snippet demonstrates how to solve an unconstrained Rosenbrock optimization problem using the LBFGS method from Optimization.jl. It defines the problem, sets up the optimization function, and solves it.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optimization.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, Zygote\n\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\n\noptf = OptimizationFunction(rosenbrock, AutoZygote())\nprob = Optimization.OptimizationProblem(optf, x0, p)\nsol = solve(prob, Optimization.LBFGS())\n```\n\n----------------------------------------\n\nTITLE: Using Second-Order Optimizer with Hessian Information\nDESCRIPTION: Applies Newton's method to solve the Rosenbrock problem. This algorithm utilizes both gradient and Hessian information generated by forward-mode automatic differentiation.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nsol = solve(prob, Newton())\n```\n\n----------------------------------------\n\nTITLE: Setting Box Constraints for Gradient-Based Optimization\nDESCRIPTION: Demonstrates how to set lower and upper bounds (box constraints) on the optimization variables when using gradient-based methods like BFGS.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/getting_started.md#2025-04-21_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\nprob = OptimizationProblem(optf, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, BFGS())\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function using Newton's Method in Julia\nDESCRIPTION: This snippet demonstrates how to optimize the Rosenbrock function using the Newton's method with line search from Optim.jl. It sets up the optimization problem using Optimization.jl and solves it with Optim.Newton().\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optim.md#2025-04-21_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationOptimJL, ModelingToolkit\nrosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock, Optimization.AutoModelingToolkit())\nprob = Optimization.OptimizationProblem(f, x0, p)\nsol = solve(prob, Optim.Newton())\n```\n\n----------------------------------------\n\nTITLE: Implementing Powell's Optimization Methods with PRIMA.jl\nDESCRIPTION: Comprehensive example demonstrating the use of all five Powell's optimization algorithms (UOBYQA, NEWUOA, BOBYQA, LINCOA, and COBYLA) on the Rosenbrock function, including constrained optimization.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/prima.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationPRIMA\n\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\n_p = [1.0, 100.0]\n\nprob = OptimizationProblem(rosenbrock, x0, _p)\n\nsol = Optimization.solve(prob, UOBYQA(), maxiters = 1000)\n\nsol = Optimization.solve(prob, NEWUOA(), maxiters = 1000)\n\nsol = Optimization.solve(prob, BOBYQA(), maxiters = 1000)\n\nsol = Optimization.solve(prob, LINCOA(), maxiters = 1000)\n\nfunction con2_c(res, x, p)\n    res .= [x[1] + x[2], x[2] * sin(x[1]) - x[1]]\nend\noptprob = OptimizationFunction(rosenbrock, AutoForwardDiff(), cons = con2_c)\nprob = OptimizationProblem(optprob, x0, _p, lcons = [1, -100], ucons = [1, 100])\nsol = Optimization.solve(prob, COBYLA(), maxiters = 1000)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function Using BlackBoxOptim in Julia\nDESCRIPTION: Example demonstrating how to optimize the Rosenbrock function using BlackBoxOptim's adaptive differential evolution algorithm. The code defines the function, initial point, parameters, and optimization problem with bounds before calling the solver.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/blackboxoptim.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationBBO\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, BBO_adaptive_de_rand_1_bin_radiuslimited(), maxiters = 100000,\n    maxtime = 1000.0)\n```\n\n----------------------------------------\n\nTITLE: Solving with IPOPT through OptimizationMOI\nDESCRIPTION: Uses the IPOPT solver through OptimizationMOI.jl to solve the Rosenbrock problem with multiple nonlinear constraints. This example demonstrates Zygote-based automatic differentiation for gradient calculations.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_8\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationMOI, Ipopt\n\nfunction con2_c(res, x, p)\n    res .= [x[1]^2 + x[2]^2, x[2] * sin(x[1]) - x[1]]\nend\n\noptf = OptimizationFunction(rosenbrock, Optimization.AutoZygote(); cons = con2_c)\nprob = OptimizationProblem(optf, x0, _p, lcons = [-Inf, -Inf], ucons = [100.0, 100.0])\nsol = solve(prob, Ipopt.Optimizer())\n```\n\n----------------------------------------\n\nTITLE: Hybrid Global-Local Optimization using MLSL with LBFGS\nDESCRIPTION: Example showing hybrid optimization combining global MLSL algorithm with local LBFGS method for the Rosenbrock function.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nlopt.md#2025-04-21_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationNLopt\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, NLopt.G_MLSL_LDS(), local_method = NLopt.LD_LBFGS(), maxtime = 10.0,\n    local_maxiters = 10)\n```\n\n----------------------------------------\n\nTITLE: Solving Mixed Integer Nonlinear Optimization Problem with Juniper in Julia\nDESCRIPTION: This code implements a mixed integer nonlinear optimization problem with binary variables to maximize the dot product of a vector v and the decision variables u, subject to a quadratic constraint. It uses Juniper.jl as the mixed-integer solver with Ipopt as the underlying nonlinear solver.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/linearandinteger.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Juniper, Ipopt\n\nv = [10, 20, 12, 23, 42]\nw = [12, 45, 12, 22, 21]\n\nobjective = (u, p) -> (v = p[1:5]; dot(v, u))\n\ncons = (res, u, p) -> (w = p[6:10]; res .= [sum(w[i] * u[i]^2 for i in 1:5)])\n\noptf = OptimizationFunction(objective, Optimization.AutoModelingToolkit(), cons = cons)\noptprob = OptimizationProblem(optf,\n    zeros(5),\n    vcat(v, w);\n    sense = Optimization.MaxSense,\n    lb = zeros(5),\n    ub = ones(5),\n    lcons = [-Inf],\n    ucons = [45.0],\n    int = fill(true, 5))\n\nnl_solver = OptimizationMOI.MOI.OptimizerWithAttributes(Ipopt.Optimizer,\n    \"print_level\" => 0)\nminlp_solver = OptimizationMOI.MOI.OptimizerWithAttributes(Juniper.Optimizer,\n    \"nl_solver\" => nl_solver)\n\nsol = solve(optprob, minlp_solver)\n```\n\n----------------------------------------\n\nTITLE: Global Optimization of Rosenbrock Function using DIRECT\nDESCRIPTION: Example demonstrating global optimization of the Rosenbrock function using NLopt's DIRECT algorithm with time constraints.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nlopt.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationNLopt\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, NLopt.GN_DIRECT(), maxtime = 10.0)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function Using GCMAES With Gradient Information\nDESCRIPTION: Example of optimizing the Rosenbrock function using GCMAES optimizer with gradient information provided through automatic differentiation (AutoForwardDiff). This approach can speed up the optimization process.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/gcmaes.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, GCMAESOpt())\n```\n\n----------------------------------------\n\nTITLE: Using PolyOpt Algorithm with the Rosenbrock Function in Julia\nDESCRIPTION: Example demonstrating how to use the PolyOpt polyalgorithm to optimize the Rosenbrock function. This combines Adam and BFGS optimizers to explore the loss surface efficiently and then converge quickly to the minima.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/polyopt.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationPolyalgorithms\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\n_p = [1.0, 100.0]\n\noptprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(optprob, x0, _p)\nsol = Optimization.solve(prob, PolyOpt(), maxiters = 1000)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function Using MultistartOptimization with NLopt\nDESCRIPTION: Demonstrates optimization of the Rosenbrock function using MultistartOptimization.TikTak with 100 initial points and NLopt.LD_LBFGS as the local method. Includes problem setup with boundary constraints.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/multistartoptimization.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationMultistartOptimization, OptimizationNLopt\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, MultistartOptimization.TikTak(100), NLopt.LD_LBFGS())\n```\n\n----------------------------------------\n\nTITLE: Solving Rosenbrock Function Optimization with SpeedMappingOpt\nDESCRIPTION: Example of using SpeedMappingOpt to optimize the Rosenbrock function both with and without bound constraints. The example demonstrates how to define an optimization problem and solve it using the SpeedMapping algorithm.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/speedmapping.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationSpeedMapping\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(f, x0, p)\nsol = solve(prob, SpeedMappingOpt())\n\nprob = OptimizationProblem(f, x0, p; lb = [0.0, 0.0], ub = [1.0, 1.0])\nsol = solve(prob, SpeedMappingOpt())\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function using Krylov Trust Region Method in Julia\nDESCRIPTION: This code snippet shows how to optimize the Rosenbrock function using the Krylov Trust Region method from Optim.jl. It sets up the optimization problem using Optimization.jl with automatic differentiation and solves it with Optim.KrylovTrustRegion().\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optim.md#2025-04-21_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationOptimJL\nrosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\noptprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(optprob, x0, p)\nsol = solve(prob, Optim.KrylovTrustRegion())\n```\n\n----------------------------------------\n\nTITLE: Using CMAEvolutionStrategy.jl for Evolutionary Optimization\nDESCRIPTION: Demonstrates the CMAEvolutionStrategy solver from OptimizationCMAEvolutionStrategy.jl to solve the Rosenbrock problem, a population-based stochastic optimization method for difficult non-convex problems.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_10\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationCMAEvolutionStrategy\nsol = solve(prob, CMAEvolutionStrategyOpt())\n```\n\n----------------------------------------\n\nTITLE: Using Evolutionary Algorithms with CMAES\nDESCRIPTION: Solves the constrained Rosenbrock problem using Covariance Matrix Adaptation Evolution Strategy (CMAES) from OptimizationEvolutionary.jl, a derivative-free optimization method suited for complex landscapes.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationEvolutionary\nsol = solve(prob, CMAES(μ = 40, λ = 100), abstol = 1e-15) # -Inf < cons_circ(sol.u, _p) = 0.25^2\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function Using GCMAES Without Gradient Information\nDESCRIPTION: Example of optimizing the Rosenbrock function using GCMAES optimizer without utilizing gradient information. The optimization problem requires lower and upper bounds constraints.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/gcmaes.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationGCMAES\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, GCMAESOpt())\n```\n\n----------------------------------------\n\nTITLE: Gradient-Based Optimization of Rosenbrock Function using LBFGS\nDESCRIPTION: Example showing gradient-based optimization of the Rosenbrock function using NLopt's L-BFGS algorithm with automatic differentiation.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nlopt.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationNLopt\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, NLopt.LD_LBFGS())\n```\n\n----------------------------------------\n\nTITLE: Using BlackBoxOptim.jl for Global Optimization\nDESCRIPTION: Solves the Rosenbrock problem with tight box constraints using BlackBoxOptim.jl's adaptive differential evolution algorithm, a global optimizer designed for black-box functions with complex landscapes.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_13\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationBBO\nprob = Optimization.OptimizationProblem(rosenbrock, [0.0, 0.3], _p, lb = [-1.0, 0.2],\n    ub = [0.8, 0.43])\nsol = solve(prob, BBO_adaptive_de_rand_1_bin()) # -1.0 ≤ x[1] ≤ 0.8, 0.2 ≤ x[2] ≤ 0.43\n```\n\n----------------------------------------\n\nTITLE: Solving OptimizationProblem using Newton's Method in Julia\nDESCRIPTION: This code completes the optimization system, creates an OptimizationProblem with gradient and Hessian information, and solves it using Newton's method. It demonstrates the final step in the optimization process.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/symbolic.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nsys = complete(sys)\nprob = OptimizationProblem(sys, u0, p, grad = true, hess = true)\nsolve(prob, Newton())\n```\n\n----------------------------------------\n\nTITLE: Constrained Optimization with BlackBoxOptim\nDESCRIPTION: This example demonstrates how to define a constrained optimization problem with lower and upper bounds, solved using BlackBoxOptim's adaptive differential evolution algorithm.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/README.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationBBO\nprob = OptimizationProblem(rosenbrock, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, BBO_adaptive_de_rand_1_bin_radiuslimited())\n```\n\n----------------------------------------\n\nTITLE: Using Nelder-Mead Solver from Optim.jl\nDESCRIPTION: Shows how to solve the Rosenbrock optimization problem using the derivative-free Nelder-Mead algorithm from Optim.jl through the OptimizationOptimJL wrapper.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/getting_started.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationOptimJL\nsol = solve(prob, Optim.NelderMead())\n```\n\n----------------------------------------\n\nTITLE: Using Adam Optimizer with Reverse-Mode AD\nDESCRIPTION: Applies the Adam optimizer from OptimizationOptimisers.jl using reverse-mode automatic differentiation via Zygote.jl to solve the Rosenbrock problem with a specified learning rate.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_9\n\nLANGUAGE: julia\nCODE:\n```\nimport OptimizationOptimisers\noptf = OptimizationFunction(rosenbrock, Optimization.AutoZygote())\nprob = OptimizationProblem(optf, x0, _p)\nsol = solve(prob, OptimizationOptimisers.Adam(0.05), maxiters = 1000, progress = false)\n```\n\n----------------------------------------\n\nTITLE: Second Optimization Phase Using LBFGS in Julia\nDESCRIPTION: Refines the solution using LBFGS optimizer, starting from the result of the previous optimization phase using remake to create a new problem instance.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/remakecomposition.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nprob = remake(prob, u0 = res1.minimizer)\nres2 = solve(prob, Optimization.LBFGS(), maxiters = 100)\n\n@show res2.objective\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function with QuadDIRECT in Julia\nDESCRIPTION: Example showing how to optimize the Rosenbrock function using the QuadDIRECT algorithm. The code defines the function, sets initial conditions, parameters, bounds, and configures the splits parameter required by QuadDIRECT.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/quaddirect.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsolve(prob, QuadDirect(), splits = ([-0.9, 0, 0.9], [-0.8, 0, 0.8]))\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function Using MultistartOptimization with Optim.jl\nDESCRIPTION: Shows an alternative implementation using Optim.jl's LBFGS optimizer with automatic differentiation and custom iteration limits.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/multistartoptimization.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationOptimJL\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, MultistartOptimization.TikTak(100), LBFGS(), maxiters = 5)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function Using CMAES Algorithm\nDESCRIPTION: Example demonstrating how to define and solve an optimization problem for the Rosenbrock function using the Covariance Matrix Adaptation Evolution Strategy (CMAES) algorithm. The example includes defining the objective function, initial conditions, parameters, and optimization constraints.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/evolutionary.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationEvolutionary\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, Evolutionary.CMAES(μ = 40, λ = 100))\n```\n\n----------------------------------------\n\nTITLE: Constrained Optimization with Fminbox and GradientDescent\nDESCRIPTION: This example demonstrates how to solve a bounded optimization problem using the Fminbox algorithm with GradientDescent as the interior point solver.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/README.md#2025-04-21_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nprob = OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, Fminbox(GradientDescent()))\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function with ECA Algorithm in Julia\nDESCRIPTION: Example of using the Evolutionary Centers Algorithm (ECA) to optimize the Rosenbrock function. The optimization problem is defined with function, initial values, parameters, and bounds.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/metaheuristics.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationMetaheuristics\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, ECA(), maxiters = 100000, maxtime = 1000.0)\n```\n\n----------------------------------------\n\nTITLE: Optimizing the Rosenbrock Function with CMAEvolutionStrategyOpt in Julia\nDESCRIPTION: Example demonstrating how to optimize the Rosenbrock function using CMAEvolutionStrategy. The code defines the function, initial conditions, parameters, and optimization constraints before solving with the CMAEvolutionStrategyOpt() solver.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/cmaevolutionstrategy.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationCMAEvolutionStrategy\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, CMAEvolutionStrategyOpt())\n```\n\n----------------------------------------\n\nTITLE: Setting up DataLoader and Optimization Problem in Julia\nDESCRIPTION: This snippet demonstrates how to set up a DataLoader for minibatching and define the optimization problem. It uses MLUtils.DataLoader to create batches and sets up the optimization function and problem.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/minibatch.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nk = 10\n# Pass the data for the batches as separate vectors wrapped in a tuple\ntrain_loader = MLUtils.DataLoader((ode_data, t), batchsize = k)\n\nnumEpochs = 300\nl1 = loss_adjoint(ps_ca, train_loader.data)[1]\n\noptfun = OptimizationFunction(\n    loss_adjoint,\n    Optimization.AutoZygote())\noptprob = OptimizationProblem(optfun, ps_ca, train_loader)\n```\n\n----------------------------------------\n\nTITLE: Rayleigh Quotient Optimization on Sphere Manifold\nDESCRIPTION: Implementation of Rayleigh quotient optimization problem on the Sphere manifold using gradient descent optimization.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/manopt.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationManopt\nusing Manifolds, LinearAlgebra\nusing Manopt\n\nn = 1000\nA = Symmetric(randn(n, n) / n)\nmanifold = Sphere(n - 1)\n\ncost(x, p = nothing) = -x' * A * x\negrad(G, x, p = nothing) = (G .= -2 * A * x)\n\noptf = OptimizationFunction(cost, grad = egrad)\nx0 = rand(manifold)\nprob = OptimizationProblem(optf, x0, manifold = manifold)\n\nsol = solve(prob, GradientDescentOptimizer())\n```\n\n----------------------------------------\n\nTITLE: Training Neural Network with Sophia Optimizer in Julia\nDESCRIPTION: This snippet demonstrates how to use the Sophia optimizer from Optimization.jl to train a neural network. It sets up a simple regression problem, defines a neural network model, and uses Sophia to optimize the model parameters.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optimization.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, Lux, Zygote, MLUtils, Statistics, Plots, Random, ComponentArrays\n\nx = rand(10000)\ny = sin.(x)\ndata = MLUtils.DataLoader((x, y), batchsize = 100)\n\n# Define the neural network\nmodel = Chain(Dense(1, 32, tanh), Dense(32, 1))\nps, st = Lux.setup(Random.default_rng(), model)\nps_ca = ComponentArray(ps)\nsmodel = StatefulLuxLayer{true}(model, nothing, st)\n\nfunction callback(state, l)\n    state.iter % 25 == 1 && @show \"Iteration: %5d, Loss: %.6e\\n\" state.iter l\n    return l < 1e-1 ## Terminate if loss is small\nend\n\nfunction loss(ps, data)\n    ypred = [smodel([data[1][i]], ps)[1] for i in eachindex(data[1])]\n    return sum(abs2, ypred .- data[2])\nend\n\noptf = OptimizationFunction(loss, AutoZygote())\nprob = OptimizationProblem(optf, ps_ca, data)\n\nres = Optimization.solve(prob, Optimization.Sophia(), callback = callback)\n```\n\n----------------------------------------\n\nTITLE: Basic Ipopt Solver Usage with OptimizationMOI\nDESCRIPTION: Demonstrates the basic usage of Ipopt optimizer with OptimizationMOI package.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/mathoptinterface.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationMOI, Ipopt\nsol = solve(prob, Ipopt.Optimizer())\n```\n\n----------------------------------------\n\nTITLE: Constructing OptimizationSystem with Symbolic Loss Function in Julia\nDESCRIPTION: This code creates an OptimizationSystem by defining a symbolic expression for the loss function. It uses the previously defined variables and parameters to construct the optimization system.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/symbolic.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nloss = (a - x)^2 + b * (y - x^2)^2\n@named sys = OptimizationSystem(loss, [x, y], [a, b])\n```\n\n----------------------------------------\n\nTITLE: Juniper Solver Configuration with Ipopt as NL Solver\nDESCRIPTION: Example of using Juniper optimizer with Ipopt as the nonlinear solver for the Rosenbrock function optimization.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/mathoptinterface.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationMOI, Juniper, Ipopt\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\n_p = [1.0, 100.0]\n\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, _p)\n\nopt = OptimizationMOI.MOI.OptimizerWithAttributes(Juniper.Optimizer,\n    \"nl_solver\" => OptimizationMOI.MOI.OptimizerWithAttributes(Ipopt.Optimizer,\n        \"print_level\" => 0))\nsol = solve(prob, opt)\n```\n\n----------------------------------------\n\nTITLE: Box-Constrained Karcher Mean on SPD Manifold\nDESCRIPTION: Implementation of box-constrained Karcher mean problem on the Symmetric Positive Definite manifold using Frank-Wolfe algorithm.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/manopt.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nM = SymmetricPositiveDefinite(5)\nm = 100\nσ = 0.005\nq = Matrix{Float64}(I, 5, 5) .+ 2.0\ndata2 = [exp(M, q, σ * rand(M; vector_at = q)) for i in 1:m]\n\nf(x, p = nothing) = sum(distance(M, x, data2[i])^2 for i in 1:m)\noptf = OptimizationFunction(f, Optimization.AutoZygote())\nprob = OptimizationProblem(optf, data2[1]; manifold = M, maxiters = 1000)\n\nfunction closed_form_solution!(M::SymmetricPositiveDefinite, q, L, U, p, X)\n    # extract p^1/2 and p^{-1/2}\n    (p_sqrt_inv, p_sqrt) = Manifolds.spd_sqrt_and_sqrt_inv(p)\n    # Compute D & Q\n    e2 = eigen(p_sqrt_inv * X * p_sqrt_inv) # decompose Sk  = QDQ'\n    D = Diagonal(1.0 .* (e2.values .< 0))\n    Q = e2.vectors\n\n    Uprime = Q' * p_sqrt_inv * U * p_sqrt_inv * Q\n    Lprime = Q' * p_sqrt_inv * L * p_sqrt_inv * Q\n    P = cholesky(Hermitian(Uprime - Lprime))\n    z = P.U' * D * P.U + Lprime\n    copyto!(M, q, p_sqrt * Q * z * Q' * p_sqrt)\n    return q\nend\nN = m\nU = mean(data2)\nL = inv(sum(1 / N * inv(matrix) for matrix in data2))\n\noptf = OptimizationFunction(f, Optimization.AutoZygote())\nprob = OptimizationProblem(optf, U; manifold = M, maxiters = 1000)\n\nsol = Optimization.solve(\n    prob, opt, sub_problem = (M, q, p, X) -> closed_form_solution!(M, q, L, U, p, X))\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function on Euclidean Manifold\nDESCRIPTION: Example showing how to optimize the Rosenbrock function on a 2D Euclidean manifold using GradientDescentOptimizer with Armijo line search.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/manopt.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationManopt, Manifolds, LinearAlgebra\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\n\nR2 = Euclidean(2)\n\nstepsize = Manopt.ArmijoLinesearch(R2)\nopt = OptimizationManopt.GradientDescentOptimizer()\n\noptf = OptimizationFunction(rosenbrock, Optimization.AutoZygote())\n\nprob = OptimizationProblem(\n    optf, x0, p; manifold = R2, stepsize = stepsize)\n\nsol = Optimization.solve(prob, opt)\n```\n\n----------------------------------------\n\nTITLE: Specifying Initial Conditions and Parameters for OptimizationProblem in Julia\nDESCRIPTION: This snippet shows how to set initial conditions for variables and values for parameters. It prepares the optimization system for numerical solution by specifying concrete values.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/symbolic.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nu0 = [x => 1.0\n      y => 2.0]\np = [a => 6.0\n     b => 7.0]\n```\n\n----------------------------------------\n\nTITLE: Creating and Setting Up Optimization Problem with NLPModels\nDESCRIPTION: Shows how to create an optimization problem using NLPModelsTest's HS10 example and convert it to an OptimizationProblem using AutoForwardDiff.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nlpmodels.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationNLPModels, Optimization, NLPModelsTest, Ipopt\nusing Optimization: OptimizationProblem\nnlpmodel = NLPModelsTest.HS10()\nprob = OptimizationProblem(nlpmodel, AutoForwardDiff())\n```\n\n----------------------------------------\n\nTITLE: Global Optimization of Rosenbrock Function using Simulated Annealing in Julia\nDESCRIPTION: This snippet shows how to perform global optimization of the Rosenbrock function using Simulated Annealing with bounds from Optim.jl. It sets up the optimization problem with box constraints and solves it using Optim.SAMIN() with default parameters.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optim.md#2025-04-21_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationOptimJL\nrosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, Optim.SAMIN())\n```\n\n----------------------------------------\n\nTITLE: Basic Optimization with Convexity Analysis in Julia\nDESCRIPTION: Demonstrates basic optimization using SymbolicAnalysis.jl with a simple objective function. Uses AutoForwardDiff for derivatives and LBFGS as the optimizer with structural analysis enabled.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/certification.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing SymbolicAnalysis, Zygote, LinearAlgebra, Optimization\n\nfunction f(x, p = nothing)\n    return exp(x[1]) + x[1]^2\nend\n\noptf = OptimizationFunction(f, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(optf, [0.4], structural_analysis = true)\n\nsol = solve(prob, Optimization.LBFGS(), maxiters = 1000)\n```\n\n----------------------------------------\n\nTITLE: ModelingToolkit Implementation\nDESCRIPTION: Implements equality constraints using ModelingToolkit as the automatic differentiation backend\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/constraints.md#2025-04-21_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\noptprob = OptimizationFunction(rosenbrock, Optimization.AutoModelingToolkit(), cons = cons)\nprob = OptimizationProblem(optprob, x0, _p, lcons = [1.0, 0.5], ucons = [1.0, 0.5])\n```\n\n----------------------------------------\n\nTITLE: Using ForwardDiff for Gradient-Based Optimization\nDESCRIPTION: Demonstrates setting up a gradient-based optimization with ForwardDiff automatic differentiation backend for BFGS method, which is more efficient than derivative-free methods.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/getting_started.md#2025-04-21_snippet_5\n\nLANGUAGE: julia\nCODE:\n```\nusing ForwardDiff\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(optf, u0, p)\nsol = solve(prob, BFGS())\n```\n\n----------------------------------------\n\nTITLE: Global Optimization with BlackBoxOptim.jl\nDESCRIPTION: Demonstrates using a global optimization solver from BlackBoxOptim.jl with the Rosenbrock problem, including setting box constraints with lower and upper bounds.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/getting_started.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationBBO\nprob = OptimizationProblem(rosenbrock, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, BBO_adaptive_de_rand_1_bin_radiuslimited())\n```\n\n----------------------------------------\n\nTITLE: AmplNLWriter Implementation\nDESCRIPTION: Uses AmplNLWriter package with Ipopt to solve the optimization problem with equality constraints\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/constraints.md#2025-04-21_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nusing AmplNLWriter, Ipopt_jll\nsol = solve(prob, AmplNLWriter.Optimizer(Ipopt_jll.amplexe))\n```\n\n----------------------------------------\n\nTITLE: Riemannian Optimization on SPD Manifold in Julia\nDESCRIPTION: Implements Riemannian optimization for finding the center of mass of SPD matrices. Uses SymbolicAnalysis for structural analysis on the SPD manifold with gradient descent optimization.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/certification.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationManopt, Symbolics, Manifolds, Random, LinearAlgebra,\n      SymbolicAnalysis\n\nM = SymmetricPositiveDefinite(5)\nm = 100\nσ = 0.005\nq = Matrix{Float64}(LinearAlgebra.I(5)) .+ 2.0\n\ndata2 = [exp(M, q, σ * rand(M; vector_at = q)) for i in 1:m];\n\nf(x, p = nothing) = sum(SymbolicAnalysis.distance(M, data2[i], x)^2 for i in 1:5)\noptf = OptimizationFunction(f, Optimization.AutoZygote())\nprob = OptimizationProblem(optf, data2[1]; manifold = M, structural_analysis = true)\n\nopt = OptimizationManopt.GradientDescentOptimizer()\nsol = solve(prob, opt, maxiters = 100)\n```\n\n----------------------------------------\n\nTITLE: Solving Rosenbrock Problem with Gradient-Based Optimization\nDESCRIPTION: Uses BFGS algorithm with forward-mode automatic differentiation to solve the Rosenbrock problem. The OptimizationFunction is configured with AutoForwardDiff for gradient calculation.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(optf, x0, _p)\nsol = solve(prob, BFGS())\n```\n\n----------------------------------------\n\nTITLE: Solving Optimization Problem with ADAM in Julia\nDESCRIPTION: This snippet shows how to solve the optimization problem using the ADAM optimizer. It uses Optimization.solve with a specified callback function and number of epochs.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/minibatch.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nusing IterTools: ncycle\nres1 = Optimization.solve(\n    optprob, Optimisers.ADAM(0.05); callback = callback, epochs = 1000)\n```\n\n----------------------------------------\n\nTITLE: Implementing Domain Transformation in Julia Optimization\nDESCRIPTION: This code snippet demonstrates how to implement a domain transformation in the objective function for optimization. It transforms the variable x from [0,Inf] to [-Inf,Inf] using the exponential function.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/FAQ.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nfunction my_objective(u)\n    x = exp(u[1])\n    # ... use x\nend\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function using Nelder-Mead\nDESCRIPTION: Shows how to optimize the Rosenbrock function using the derivative-free Nelder-Mead optimizer. The example includes function definition and problem solving using the Optimization.jl framework.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optim.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationOptimJL\nrosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nprob = Optimization.OptimizationProblem(rosenbrock, x0, p)\nsol = solve(prob, Optim.NelderMead())\n```\n\n----------------------------------------\n\nTITLE: First Optimization Phase Using BlackBoxOptim in Julia\nDESCRIPTION: Applies the BBO_adaptive_de_rand_1_bin algorithm for global exploration of the parameter space with 4000 maximum iterations.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/remakecomposition.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nres1 = solve(prob, BBO_adaptive_de_rand_1_bin(), maxiters = 4000)\n\n@show res1.objective\n```\n\n----------------------------------------\n\nTITLE: Initializing Optimization Problem Components\nDESCRIPTION: Sets up the Rosenbrock function as objective function and initializes basic parameters. Uses Optimization, OptimizationMOI, OptimizationOptimJL, and Ipopt packages.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/constraints.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationMOI, OptimizationOptimJL, Ipopt\nusing ForwardDiff, ModelingToolkit\n\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\n_p = [1.0, 1.0]\n```\n\n----------------------------------------\n\nTITLE: Optimizing Rosenbrock Function using NOMADOpt\nDESCRIPTION: Example demonstrating how to optimize the Rosenbrock function using NOMADOpt solver, both with and without box constraints. Shows problem setup, function definition, and solver initialization.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nomad.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, OptimizationNOMAD\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\n\nprob = OptimizationProblem(f, x0, p)\nsol = Optimization.solve(prob, NOMADOpt())\n\nprob = OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.5, 1.5])\nsol = Optimization.solve(prob, NOMADOpt())\n```\n\n----------------------------------------\n\nTITLE: Example of Nonlinear Constraints in Julia Optimization\nDESCRIPTION: This snippet shows an example set of nonlinear constraints that could be used in an optimization problem. It demonstrates the kind of constraints that symbolic interfaces like ModelingToolkit can simplify.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/FAQ.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\n    0 ~ u1 - sin(u5) * h,\n    0 ~ u2 - cos(u1),\n    0 ~ u3 - hypot(u1, u2),\n    0 ~ u4 - hypot(u2, u3),\n```\n\n----------------------------------------\n\nTITLE: Using Initial Values with ECA Algorithm in Julia\nDESCRIPTION: Example showing how to force Metaheuristics to use the initial values provided in the OptimizationProblem by setting use_initial=true, rather than ignoring them as is the default behavior.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/metaheuristics.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nsol = solve(prob, ECA(), use_initial = true, maxiters = 100000, maxtime = 1000.0)\n```\n\n----------------------------------------\n\nTITLE: Solving Rosenbrock Problem with Derivative-Free Optimizers\nDESCRIPTION: Demonstrates solving the Rosenbrock problem using derivative-free optimizers from Optim.jl including SimulatedAnnealing, SAMIN, and NelderMead with box constraints.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationOptimJL\nsol = solve(prob, SimulatedAnnealing())\nprob = OptimizationProblem(f, x0, _p, lb = [-1.0, -1.0], ub = [0.8, 0.8])\nsol = solve(prob, SAMIN())\n\nl1 = rosenbrock(x0, _p)\nprob = OptimizationProblem(rosenbrock, x0, _p)\nsol = solve(prob, NelderMead())\n```\n\n----------------------------------------\n\nTITLE: Simplified Constraint Functions in Julia Optimization\nDESCRIPTION: This code snippet shows how nonlinear constraints can be simplified into a series of function calls. This demonstrates the result of constraint tearing performed by symbolic interfaces like ModelingToolkit.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/FAQ.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nu1 = f1(u5)\nu2 = f2(u1)\nu3 = f3(u1, u2)\nu4 = f4(u2, u3)\n```\n\n----------------------------------------\n\nTITLE: Derivative-Free Optimization of Rosenbrock Function using NLopt\nDESCRIPTION: Example demonstrating how to optimize the Rosenbrock function using NLopt's Nelder-Mead algorithm without derivatives.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nlopt.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization\nusing OptimizationNLopt\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])\nsol = solve(prob, NLopt.LN_NELDERMEAD())\n```\n\n----------------------------------------\n\nTITLE: Setting up Neural ODE for Newton's Cooling in Julia\nDESCRIPTION: This snippet defines the ODE problem for Newton's cooling, sets up a Lux neural network model, and prepares the optimization problem. It includes functions for the true solution and the neural ODE model.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/minibatch.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Lux, Optimization, OptimizationOptimisers, OrdinaryDiffEq, SciMLSensitivity, MLUtils,\n      Random, ComponentArrays\n\nfunction newtons_cooling(du, u, p, t)\n    temp = u[1]\n    k, temp_m = p\n    du[1] = dT = -k * (temp - temp_m)\nend\n\nfunction true_sol(du, u, p, t)\n    true_p = [log(2) / 8.0, 100.0]\n    newtons_cooling(du, u, true_p, t)\nend\n\nmodel = Chain(Dense(1, 32, tanh), Dense(32, 1))\nps, st = Lux.setup(Random.default_rng(), model)\nps_ca = ComponentArray(ps)\nsmodel = StatefulLuxLayer{true}(model, nothing, st)\n\nfunction dudt_(u, p, t)\n    smodel(u, p) .* u\nend\n\nfunction callback(state, l) #callback function to observe training\n    display(l)\n    return false\nend\n\nu0 = Float32[200.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\n\nt = range(tspan[1], tspan[2], length = datasize)\ntrue_prob = ODEProblem(true_sol, u0, tspan)\node_data = Array(solve(true_prob, Tsit5(), saveat = t))\n\nprob = ODEProblem{false}(dudt_, u0, tspan, ps_ca)\n```\n\n----------------------------------------\n\nTITLE: Applying Hessian-Free Second-Order Optimization\nDESCRIPTION: Uses a Krylov Trust Region method from Optim.jl to solve the Rosenbrock problem without explicitly constructing the Hessian matrix, making it more memory-efficient for large problems.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/examples/rosenbrock.md#2025-04-21_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nsol = solve(prob, Optim.KrylovTrustRegion())\n```\n\n----------------------------------------\n\nTITLE: Accessing Original Solver Output\nDESCRIPTION: Shows how to access the original solver output after solving an optimization problem, which contains additional information about the solution process.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/getting_started.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nsol.original\n```\n\n----------------------------------------\n\nTITLE: Defining Prediction and Loss Functions for Neural ODE in Julia\nDESCRIPTION: This snippet defines the prediction and loss functions for the neural ODE model. The predict_adjoint function solves the ODE problem, while the loss_adjoint function computes the mean squared error between predictions and data.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/minibatch.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nfunction predict_adjoint(fullp, time_batch)\n    Array(solve(prob, Tsit5(), p = fullp, saveat = time_batch))\nend\n\nfunction loss_adjoint(fullp, data)\n    batch, time_batch = data\n    pred = predict_adjoint(fullp, time_batch)\n    sum(abs2, batch .- pred)\nend\n```\n\n----------------------------------------\n\nTITLE: Using Zygote for Reverse-Mode Automatic Differentiation\nDESCRIPTION: Example of using Zygote.jl for reverse-mode automatic differentiation with BFGS, which is more efficient for optimization problems with many state variables.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/getting_started.md#2025-04-21_snippet_7\n\nLANGUAGE: julia\nCODE:\n```\nusing Zygote\noptf = OptimizationFunction(rosenbrock, Optimization.AutoZygote())\nprob = OptimizationProblem(optf, u0, p)\nsol = solve(prob, BFGS())\n```\n\n----------------------------------------\n\nTITLE: Basic Optimization Example with Rosenbrock Function using NelderMead\nDESCRIPTION: This example shows how to define and solve a basic optimization problem using the Rosenbrock function with NelderMead solver from OptimizationOptimJL.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/README.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization\nrosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np = [1.0, 100.0]\n\nprob = OptimizationProblem(rosenbrock, x0, p)\n\nusing OptimizationOptimJL\nsol = solve(prob, NelderMead())\n```\n\n----------------------------------------\n\nTITLE: Initializing Schwefel Function Optimization Problem in Julia\nDESCRIPTION: Sets up a 10-dimensional Schwefel function optimization problem with bounds [-500, 500]. Uses ReverseDiff for automatic differentiation and includes problem definition with constraints.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/remakecomposition.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Optimization, Random\nusing OptimizationBBO, ReverseDiff\n\nRandom.seed!(122333)\n\nfunction f_schwefel(x, p = [418.9829])\n    result = p[1] * length(x)\n    for i in 1:length(x)\n        result -= x[i] * sin(sqrt(abs(x[i])))\n    end\n    return result\nend\n\noptf = OptimizationFunction(f_schwefel, Optimization.AutoReverseDiff(compile = true))\n\nx0 = ones(10) .* 200.0\nprob = OptimizationProblem(\n    optf, x0, [418.9829], lb = fill(-500.0, 10), ub = fill(500.0, 10))\n\n@show f_schwefel(x0)\n```\n\n----------------------------------------\n\nTITLE: Defining Symbolic Variables in Julia using ModelingToolkit\nDESCRIPTION: This snippet demonstrates how to define symbolic variables and parameters using ModelingToolkit.jl. It creates variables x and y, and parameters a and b.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/symbolic.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing ModelingToolkit, Optimization, OptimizationOptimJL\n\n@variables x y\n@parameters a b\n```\n\n----------------------------------------\n\nTITLE: Defining Wrapper for Objective Functions\nDESCRIPTION: Example of how to create an anonymous function wrapper for objective functions that don't match the expected signature in Optimization.jl, discarding extra parameters.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/getting_started.md#2025-04-21_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nobj = (x, p) -> objective(x) # Pass this function into OptimizationFunction\n```\n\n----------------------------------------\n\nTITLE: Accessing Analysis Results in Julia\nDESCRIPTION: Shows how to access the convexity analysis results from the optimization solution cache.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/certification.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nsol.cache.analysis_results.objective\n```\n\n----------------------------------------\n\nTITLE: Solving with IPNewton Optimizer\nDESCRIPTION: Creates and solves optimization problem using IPNewton solver with inequality constraints\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/constraints.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\noptprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff(), cons = cons)\nprob = OptimizationProblem(optprob, x0, _p, lcons = [-Inf, -1.0], ucons = [0.8, 2.0])\nsol = solve(prob, IPNewton())\n```\n\n----------------------------------------\n\nTITLE: Solving OptimizationProblem in Julia\nDESCRIPTION: Documentation stub for the solve method that takes an OptimizationProblem and solver as arguments. This represents the core solving functionality for optimization problems in the SciML framework.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/solve.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nsolve(::OptimizationProblem,::Any)\n```\n\n----------------------------------------\n\nTITLE: Solving with Ipopt Optimizer\nDESCRIPTION: Demonstrates solving the same optimization problem using the Ipopt solver\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/constraints.md#2025-04-21_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\nsol = solve(prob, Ipopt.Optimizer())\n```\n\n----------------------------------------\n\nTITLE: Documenting Automatic Differentiation Choices in Julia\nDESCRIPTION: This code snippet uses Julia's documentation syntax to create documentation entries for various automatic differentiation methods. It includes OptimizationBase.AutoForwardDiff, AutoFiniteDiff, AutoReverseDiff, AutoZygote, AutoTracker, AutoModelingToolkit, and AutoEnzyme.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/ad.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nOptimizationBase.AutoForwardDiff\nOptimizationBase.AutoFiniteDiff\nOptimizationBase.AutoReverseDiff\nOptimizationBase.AutoZygote\nOptimizationBase.AutoTracker\nOptimizationBase.AutoModelingToolkit\nOptimizationBase.AutoEnzyme\n```\n```\n\n----------------------------------------\n\nTITLE: Solving NLP Optimization Problem with Ipopt\nDESCRIPTION: Demonstrates how to solve the created optimization problem using the Ipopt optimizer.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nlpmodels.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nsol = solve(prob, Ipopt.Optimizer())\n```\n\n----------------------------------------\n\nTITLE: Documentation Reference for OptimizationProblem\nDESCRIPTION: Docstring reference for the OptimizationProblem type from SciMLBase. This type is used for representing optimization problems within the SciML framework.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/optimization_problem.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n@docs\nSciMLBase.OptimizationProblem\n```\n\n----------------------------------------\n\nTITLE: Configuring Ipopt Solver Options in OptimizationMOI\nDESCRIPTION: Shows two methods for configuring Ipopt optimizer options: using OptimizerWithAttributes or keyword arguments.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/mathoptinterface.md#2025-04-21_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing OptimizationMOI, Ipopt\nopt = OptimizationMOI.MOI.OptimizerWithAttributes(Ipopt.Optimizer,\n    \"option_name\" => option_value, ...)\nsol = solve(prob, opt)\n\nsol = solve(prob, Ipopt.Optimizer(); option_name = option_value, ...)\n```\n\n----------------------------------------\n\nTITLE: Defining Constraint Functions\nDESCRIPTION: Implements constraint functions for sum of squares and product of optimization variables\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/constraints.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\ncons(res, x, p) = (res .= [x[1]^2 + x[2]^2, x[1] * x[2])\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationEvolutionary.jl Package in Julia\nDESCRIPTION: Commands to install the OptimizationEvolutionary package through Julia's package manager. This package is required to use Evolutionary.jl algorithms with the Optimization.jl framework.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/evolutionary.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationEvolutionary\");\n```\n\n----------------------------------------\n\nTITLE: Constraint Verification\nDESCRIPTION: Verifies that the optimization constraints are satisfied with the solution\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/tutorials/constraints.md#2025-04-21_snippet_3\n\nLANGUAGE: julia\nCODE:\n```\nres = zeros(2)\ncons(res, sol.u, _p)\nres\n```\n\n----------------------------------------\n\nTITLE: Eigenvalue Verification for Rayleigh Quotient\nDESCRIPTION: Code to verify the optimization result by comparing with the minimum eigenvalue of matrix A.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/manopt.md#2025-04-21_snippet_4\n\nLANGUAGE: julia\nCODE:\n```\n@show eigmin(A)\n@show sol.objective\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationNLPModels Package in Julia\nDESCRIPTION: Shows how to install the OptimizationNLPModels package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nlpmodels.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationNLPModels\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting Optimization Statistics\nDESCRIPTION: Shows how to inspect the statistics of the optimization process, such as number of iterations and gradient computations, by accessing the original solver output.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/getting_started.md#2025-04-21_snippet_6\n\nLANGUAGE: julia\nCODE:\n```\nsol.original\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationPRIMA Package in Julia\nDESCRIPTION: Commands to install the OptimizationPRIMA package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/prima.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationPRIMA\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationMOI Package in Julia\nDESCRIPTION: Instructions for installing the OptimizationMOI package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/mathoptinterface.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationMOI\");\n```\n\n----------------------------------------\n\nTITLE: Referencing OptimizationState Documentation in Julia\nDESCRIPTION: This code snippet is a documentation reference block that links to the OptimizationState type documentation in the Optimization module.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/optimization_state.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nOptimization.OptimizationState\n```\n```\n\n----------------------------------------\n\nTITLE: Documentation Reference for OptimizationStats in Julia\nDESCRIPTION: A reference to the OptimizationStats documentation in the SciMLBase module, using Julia's documentation system.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/optimization_stats.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nSciMLBase.OptimizationStats\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for NLPModelsTest Example\nDESCRIPTION: Demonstrates installation of NLPModelsTest and Ipopt packages needed for the optimization example.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nlpmodels.md#2025-04-21_snippet_1\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"NLPModelsTest\", \"Ipopt\")\n```\n\n----------------------------------------\n\nTITLE: Citation Information for Optimization.jl\nDESCRIPTION: BibTeX citation for the Optimization.jl package, including authors, title, version, DOI, and other publication details.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/index.md#2025-04-21_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@software{vaibhav_kumar_dixit_2023_7738525,\n\tauthor = {Vaibhav Kumar Dixit and Christopher Rackauckas},\n\tmonth = mar,\n\tpublisher = {Zenodo},\n\ttitle = {Optimization.jl: A Unified Optimization Package},\n\tversion = {v3.12.1},\n\tdoi = {10.5281/zenodo.7738525},\n  \turl = {https://doi.org/10.5281/zenodo.7738525},\n\tyear = 2023}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Domain Error in Julia\nDESCRIPTION: This snippet shows how calling log() with a negative argument results in a DomainError in Julia. It illustrates why constraint violations during optimization can cause issues.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/FAQ.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\njulia> log(-1)\nERROR: DomainError with -1.0:\nlog will only return a complex result if called with a complex argument. Try log(Complex(x)).\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationOptimJL Package in Julia\nDESCRIPTION: Shows how to install the OptimizationOptimJL package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optim.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationOptimJL\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationNOMAD Package in Julia\nDESCRIPTION: Code snippet showing how to install the OptimizationNOMAD package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nomad.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationNOMAD\");\n```\n\n----------------------------------------\n\nTITLE: Installing Optimization.jl in Julia\nDESCRIPTION: Code snippet demonstrating how to install the Optimization.jl package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/index.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg\nPkg.add(\"Optimization\")\n```\n\n----------------------------------------\n\nTITLE: Installing Optimization.jl in Julia\nDESCRIPTION: This snippet demonstrates how to install the Optimization.jl package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/README.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nusing Pkg\nPkg.add(\"Optimization\")\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationCMAEvolutionStrategy in Julia\nDESCRIPTION: Commands to install the OptimizationCMAEvolutionStrategy package using Julia's package manager. This is required to use the CMAEvolutionStrategy algorithm for optimization tasks.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/cmaevolutionstrategy.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationCMAEvolutionStrategy\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationBBO Package in Julia\nDESCRIPTION: Code for installing the OptimizationBBO package which provides an interface to BlackBoxOptim.jl functionality. This package is required to use the BlackBoxOptim algorithms described in the documentation.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/blackboxoptim.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationBBO\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationGCMAES Package in Julia\nDESCRIPTION: Code for installing the OptimizationGCMAES package in Julia, which is required to use the GCMAES optimization algorithm.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/gcmaes.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationGCMAES\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationPolyalgorithms.jl in Julia\nDESCRIPTION: Code snippet showing how to install the OptimizationPolyalgorithms package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/polyopt.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationPolyalgorithms\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationMultistartOptimization Package in Julia\nDESCRIPTION: Shows how to install the OptimizationMultistartOptimization package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/multistartoptimization.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationMultistartOptimization\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationManopt Package in Julia\nDESCRIPTION: Command to install the OptimizationManopt package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/manopt.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationManopt\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationNLopt Package in Julia\nDESCRIPTION: Code snippet showing how to install the OptimizationNLopt package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/nlopt.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationNLopt\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationMetaheuristics.jl in Julia\nDESCRIPTION: Code to install the OptimizationMetaheuristics package in Julia using the package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/metaheuristics.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationMetaheuristics\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationOptimisers.jl in Julia\nDESCRIPTION: Code for installing the OptimizationOptimisers package using Julia's package manager. This package extends the base Optimisers.jl functionality with additional optimization algorithms like Sophia.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/optimisers.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationOptimisers\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationQuadDIRECT.jl in Julia\nDESCRIPTION: Code snippet demonstrating how to install the OptimizationQuadDIRECT package using Pkg.add() with a GitHub URL and subdir specification. This installation approach is necessary because the package is not registered in the General registry.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/quaddirect.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(url = \"https://github.com/SciML/Optimization.jl\",\n    subdir = \"lib/OptimizationQuadDIRECT\");\n```\n\n----------------------------------------\n\nTITLE: Installing OptimizationSpeedMapping.jl in Julia\nDESCRIPTION: Instructions for installing the OptimizationSpeedMapping package using Julia's package manager.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/optimization_packages/speedmapping.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\nimport Pkg;\nPkg.add(\"OptimizationSpeedMapping\");\n```\n\n----------------------------------------\n\nTITLE: Linking OptimizationFunction Documentation in Julia\nDESCRIPTION: This code snippet is a Julia documentation directive that links to the detailed API documentation for the OptimizationFunction type in the SciMLBase module. It uses the @docs macro to generate the documentation.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/optimization_function.md#2025-04-21_snippet_0\n\nLANGUAGE: julia\nCODE:\n```\n```@docs\nSciMLBase.OptimizationFunction\n```\n```\n\n----------------------------------------\n\nTITLE: Referencing OptimizationSolution Documentation in Julia\nDESCRIPTION: Markdown code block that references the documentation for the OptimizationSolution structure in the SciMLBase module. This is used to generate API documentation for this type.\nSOURCE: https://github.com/sciml/optimization.jl/blob/master/docs/src/API/optimization_solution.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```@docs\nSciMLBase.OptimizationSolution\n```\n```"
  }
]
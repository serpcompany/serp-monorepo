[
  {
    "owner": "topoteretes",
    "repo": "cognee",
    "content": "TITLE: Setting OpenAI API Key\nDESCRIPTION: Sets the OpenAI API key as an environment variable for Cognee to use. The API key is needed to access OpenAI's LLM models, with gpt-4o-mini being the default.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_simple_demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"LLM_API_KEY\"] = \"\"\n```\n\n----------------------------------------\n\nTITLE: Basic Usage Example of Cognee\nDESCRIPTION: A complete example demonstrating the basic workflow of Cognee: adding text, generating a knowledge graph with cognify(), and searching the graph. Shows the core functionality of the memory layer.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/README.md#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Add text to cognee\n    await cognee.add(\"Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval.\")\n\n    # Generate the knowledge graph\n    await cognee.cognify()\n\n    # Query the knowledge graph\n    results = await cognee.search(\"Tell me about NLP\")\n\n    # Display the results\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Cognee\nDESCRIPTION: Code snippet showing how to set up the LLM API key as an environment variable for Cognee. This is required for Cognee to work with language models.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/README.md#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"LLM_API_KEY\"] = \"YOUR OPENAI_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Installing Cognee Package\nDESCRIPTION: Installs the Cognee package version 0.1.26 using pip.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_simple_demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install cognee==0.1.26\n```\n\n----------------------------------------\n\nTITLE: Initializing CogneeGraphRAG Framework\nDESCRIPTION: Configures and instantiates the CogneeGraphRAG framework with specific LLM, graph, and database providers.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/llama_index_cognee_integration.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncogneeRAG = CogneeGraphRAG(\n    llm_api_key=os.environ[\"OPENAI_API_KEY\"],\n    llm_provider=\"openai\",\n    llm_model=\"gpt-4o-mini\",\n    graph_db_provider=\"networkx\",\n    vector_db_provider=\"lancedb\",\n    relational_db_provider=\"sqlite\",\n    relational_db_name=\"cognee_db\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Cognee Environment\nDESCRIPTION: Initializes the Cognee environment by importing necessary modules, pruning existing data, and configuring the OpenAI API key.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/graphrag_vs_rag.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport cognee\n\nawait cognee.prune.prune_data()\nawait cognee.prune.prune_system(metadata=True)\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = \"\"\n```\n\n----------------------------------------\n\nTITLE: Cognify Pipeline Implementation\nDESCRIPTION: Implements a complete pipeline for processing documents including classification, chunk extraction, graph generation, and summarization\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.shared.data_models import KnowledgeGraph\nfrom cognee.modules.data.models import Dataset, Data\nfrom cognee.modules.data.methods.get_dataset_data import get_dataset_data\nfrom cognee.modules.cognify.config import get_cognify_config\nfrom cognee.modules.pipelines import run_tasks\nfrom cognee.modules.pipelines.tasks import Task, TaskConfig\nfrom cognee.modules.pipelines.operations.needs import merge_needs\nfrom cognee.modules.users.models import User\nfrom cognee.tasks.documents import (\n    classify_documents,\n    extract_chunks_from_documents,\n)\nfrom cognee.infrastructure.llm import get_max_chunk_tokens\nfrom cognee.tasks.graph import extract_graph_from_data\nfrom cognee.tasks.storage import add_data_points\nfrom cognee.tasks.summarization import summarize_text\n\n\nasync def run_cognify_pipeline(dataset: Dataset, user: User = None):\n    data_documents: list[Data] = await get_dataset_data(dataset_id=dataset.id)\n\n    try:\n        cognee_config = get_cognify_config()\n\n        tasks = [\n            Task(classify_documents),\n            Task(  # Extract text chunks based on the document type.\n                extract_chunks_from_documents,\n                max_chunk_size=get_max_chunk_tokens(),\n                task_config=TaskConfig(needs=[classify_documents], output_batch_size=10),\n            ),\n            Task(  # Generate knowledge graphs from the document chunks.\n                extract_graph_from_data,\n                graph_model=KnowledgeGraph,\n                task_config=TaskConfig(needs=[extract_chunks_from_documents]),\n            ),\n            Task(\n                summarize_text,\n                summarization_model=cognee_config.summarization_model,\n                task_config=TaskConfig(needs=[extract_chunks_from_documents]),\n            ),\n            Task(\n                add_data_points,\n                task_config=TaskConfig(needs=[merge_needs(summarize_text, extract_graph_from_data)]),\n            ),\n        ]\n\n        pipeline_run = run_tasks(tasks, dataset.id, data_documents, \"cognify_pipeline\")\n        pipeline_run_status = None\n\n        async for run_status in pipeline_run:\n            pipeline_run_status = run_status\n\n    except Exception as error:\n        raise error\n```\n\n----------------------------------------\n\nTITLE: Installing Cognee Using Pip\nDESCRIPTION: Command for installing the Cognee package using pip package manager. This is the simplest way to get started with Cognee.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install cognee\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Cognee Framework\nDESCRIPTION: Loads the document dataset into the cognee framework for processing.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/llama_index_cognee_integration.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait cogneeRAG.add(documents, \"test\")\n```\n\n----------------------------------------\n\nTITLE: Resetting Cognee System in Python\nDESCRIPTION: Imports the Cognee module and resets the system by pruning all data and system metadata. This ensures a clean state before adding new data for processing.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport cognee\n\nawait cognee.prune.prune_data()\nawait cognee.prune.prune_system(metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Initializing Cognee Pipeline for Document Processing\nDESCRIPTION: Sets up and runs a pipeline to process documents through classification, chunking, graph extraction, and summarization stages. Uses async tasks to handle data processing efficiently.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.shared.data_models import KnowledgeGraph\nfrom cognee.modules.data.models import Dataset, Data\n# [imports...]\n\nasync def run_cognify_pipeline(dataset: Dataset, user: User = None):\n    data_documents: list[Data] = await get_dataset_data(dataset_id=dataset.id)\n\n    try:\n        cognee_config = get_cognify_config()\n\n        tasks = [\n            Task(classify_documents),\n            Task(extract_chunks_from_documents,\n                max_chunk_size=get_max_chunk_tokens(),\n                task_config=TaskConfig(needs=[classify_documents], output_batch_size=10)),\n            Task(extract_graph_from_data,\n                graph_model=KnowledgeGraph,\n                task_config=TaskConfig(needs=[extract_chunks_from_documents])),\n            Task(summarize_text,\n                summarization_model=cognee_config.summarization_model,\n                task_config=TaskConfig(needs=[extract_chunks_from_documents])),\n            Task(add_data_points,\n                task_config=TaskConfig(needs=[merge_needs(summarize_text, extract_graph_from_data)])),\n        ]\n\n        pipeline = run_tasks(tasks, data_documents)\n\n        async for result in pipeline:\n            print(result)\n    except Exception as error:\n        raise error\n```\n\n----------------------------------------\n\nTITLE: Configuring Cognee Environment Variables in Python\nDESCRIPTION: Sets up environment variables needed for Cognee, including API keys, database providers, and connection details. This configuration uses networkx for graph database and lancedb for vector database with SQLite as the relational database.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Setting environment variables\nif \"GRAPHISTRY_USERNAME\" not in os.environ:\n    os.environ[\"GRAPHISTRY_USERNAME\"] = \"\"\n\nif \"GRAPHISTRY_PASSWORD\" not in os.environ:\n    os.environ[\"GRAPHISTRY_PASSWORD\"] = \"\"\n\nif \"LLM_API_KEY\" not in os.environ:\n    os.environ[\"LLM_API_KEY\"] = \"\"\n\n# \"neo4j\" or \"networkx\"\nos.environ[\"GRAPH_DATABASE_PROVIDER\"] = \"networkx\"\n# Not needed if using networkx\n# os.environ[\"GRAPH_DATABASE_URL\"]=\"\"\n# os.environ[\"GRAPH_DATABASE_USERNAME\"]=\"\"\n# os.environ[\"GRAPH_DATABASE_PASSWORD\"]=\"\"\n\n# \"pgvector\", \"qdrant\", \"weaviate\" or \"lancedb\"\nos.environ[\"VECTOR_DB_PROVIDER\"] = \"lancedb\"\n# Not needed if using \"lancedb\" or \"pgvector\"\n# os.environ[\"VECTOR_DB_URL\"]=\"\"\n# os.environ[\"VECTOR_DB_KEY\"]=\"\"\n\n# Relational Database provider \"sqlite\" or \"postgres\"\nos.environ[\"DB_PROVIDER\"] = \"sqlite\"\n\n# Database name\nos.environ[\"DB_NAME\"] = \"cognee_db\"\n\n# Postgres specific parameters (Only if Postgres or PGVector is used)\n# os.environ[\"DB_HOST\"]=\"127.0.0.1\"\n# os.environ[\"DB_PORT\"]=\"5432\"\n# os.environ[\"DB_USERNAME\"]=\"cognee\"\n# os.environ[\"DB_PASSWORD\"]=\"cognee\"\n```\n\n----------------------------------------\n\nTITLE: Processing Data into Knowledge Graph\nDESCRIPTION: Transforms the loaded documents into a structured knowledge graph using Cognee's cognify function.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/graphrag_vs_rag.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait cognee.cognify()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment and Importing Dependencies\nDESCRIPTION: Imports required libraries and sets up OpenAI API key environment variable for the application.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/llama_index_cognee_integration.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport asyncio\nfrom llama_index.core import Document\nfrom llama_index.graph_rag.cognee import CogneeGraphRAG\n\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = \"\"\n```\n\n----------------------------------------\n\nTITLE: Generating Answers for QA Instances in Python\nDESCRIPTION: This code generates answers for question answering instances using the specified context provider. It randomly samples from the dataset if num_samples is provided, creates output directories if they don't exist, and processes each instance to generate and collect answers.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nrandom.seed(42)\ninstances = dataset if not num_samples else random.sample(dataset, num_samples)\n\nout_path = \"out\"\nif not Path(out_path).exists():\n    Path(out_path).mkdir()\ncontexts_filename = out_path / Path(\n    f\"contexts_{dataset_name_or_filename.split('.')[0]}_{context_provider_name}.json\"\n)\n\nanswers = []\nfor instance in tqdm(instances, desc=\"Getting answers\"):\n    answer = await answer_qa_instance(instance, context_provider, contexts_filename)\n    answers.append(answer)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Environment for Cognee in Python\nDESCRIPTION: This code imports necessary libraries for Cognee, sets up logging, and configures the OpenAI API key required for Cognee's LLM functionality.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/ontology_demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Import required libraries\nimport cognee\nimport asyncio\nfrom cognee.shared.logging_utils import get_logger\nimport os\nimport textwrap\nfrom cognee.api.v1.search import SearchType\nfrom cognee.api.v1.visualize.visualize import visualize_graph\n\nlogger = get_logger()\n\n# Set up OpenAI API key (required for Cognee's LLM functionality)\nos.environ[\"LLM_API_KEY\"] = \"your-api-key-here\"  # Replace with your API key\n```\n\n----------------------------------------\n\nTITLE: Performing RAG Search\nDESCRIPTION: Executes a search query using the traditional RAG approach and displays results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/llama_index_cognee_integration.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsearch_results = await cogneeRAG.rag_search(\"Tell me who are the people mentioned?\")\n\nprint(\"\\n\\nAnswer based on RAG:\\n\")\nfor result in search_results:\n    print(f\"{result}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Installing Cognee with DeepEval Support in Python\nDESCRIPTION: This snippet installs the Cognee library with DeepEval support using pip. This is a prerequisite for running the evaluation code.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install \"cognee[deepeval]\"\n```\n\n----------------------------------------\n\nTITLE: Executing the Code Analysis Pipeline\nDESCRIPTION: Runs the configured tasks as a pipeline, processing the cloned repository to extract and analyze code dependencies. Uses UUID for pipeline identification and prints results as they are generated.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_code_graph_demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.modules.pipelines import run_tasks\nfrom uuid import uuid5, NAMESPACE_OID\n\npipeline = run_tasks(tasks, uuid5(NAMESPACE_OID, repo_clone_location), repo_clone_location, \"code_graph_pipeline\")\n\nasync for result in pipeline:\n    print(result)\n```\n\n----------------------------------------\n\nTITLE: Installing Homebrew Package Manager Dependencies\nDESCRIPTION: Installs the uv package manager using Homebrew, which is required for setting up the Cognee environment.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-mcp/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbrew install uv\n```\n\n----------------------------------------\n\nTITLE: Calculating Correctness Metric in Python\nDESCRIPTION: This code calculates the 'Correctness' metric for a question answering system. It evaluates the factual accuracy of the answers and calculates the mean score across all test results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Correctness\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nCorrectness = statistics.mean(\n    [result.metrics_data[0].score for result in eval_results.test_results]\n)\nprint(Correctness)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Graphistry and OpenAI Authentication\nDESCRIPTION: Prompts the user to input authentication credentials for Graphistry and OpenAI services, then sets them as environment variables for the application to use.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_code_graph_demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.environ[\"GRAPHISTRY_USERNAME\"] = input(\"Please enter your graphistry username\")\nos.environ[\"GRAPHISTRY_PASSWORD\"] = input(\"Please enter your graphistry password\")\nos.environ[\"OPENAI_API_KEY\"] = input(\"Please enter your OpenAI API key\")\n```\n\n----------------------------------------\n\nTITLE: Calculating F1 Score Metric in Python\nDESCRIPTION: This code calculates the F1 Score metric for a question answering system. It evaluates the balance between precision and recall of the answers and calculates the mean score across all test results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"F1 Score\"]\neval_metrics = get_metrics(metric_name_list)\n```\n\nLANGUAGE: python\nCODE:\n```\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nF1_score = statistics.mean([result.metrics_data[0].score for result in eval_results.test_results])\nprint(F1_score)\n```\n\n----------------------------------------\n\nTITLE: Visualization Server Launch\nDESCRIPTION: Starts the visualization server on port 8002\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.api.v1.visualize import visualization_server\n\nvisualization_server(port=8002)\n```\n\n----------------------------------------\n\nTITLE: Calculating Empowerment Metric in Python\nDESCRIPTION: This code calculates the 'Empowerment' metric for a question answering system. It evaluates how well the answers empower users by providing useful information and calculates the mean score across all test results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Empowerment\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nEmpowerment = statistics.mean(\n    [result.metrics_data[0].score for result in eval_results.test_results]\n)\nprint(Empowerment)\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Cognee in Python\nDESCRIPTION: Imports the Cognee module and adds all the defined job and CV data to the Cognee system with the tag 'example'. This makes the data available for processing and searching.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport cognee\n\nawait cognee.add([job_1, job_2, job_3, job_4, job_5, job_position], \"example\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Diversity Metric in Python\nDESCRIPTION: This code calculates the 'Diversity' metric for a question answering system. It uses the deepeval library to evaluate the variety of information in the answers and calculates the mean score across test results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Diversity\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nDiversity = statistics.mean([result.metrics_data[0].score for result in eval_results.test_results])\nprint(Diversity)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Knowledge Graph\nDESCRIPTION: Generates and displays a visualization of the knowledge graph created from the processed data. It saves the visualization as an HTML file and opens it in a web browser.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_simple_demo.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport webbrowser\nimport os\nfrom cognee.api.v1.visualize.visualize import visualize_graph\nhtml = await visualize_graph()\nhome_dir = os.path.expanduser(\"~\")\nhtml_file = os.path.join(home_dir, \"graph_visualization.html\")\ndisplay(html_file)\nwebbrowser.open(f\"file://{html_file}\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Exact Match (EM) Score for HotpotQA Answers in Python\nDESCRIPTION: This code calculates the Exact Match (EM) Score for the generated answers using DeepEval. It defines the metric, evaluates the answers, and computes the average score.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"EM\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n\nEM = statistics.mean([result.metrics_data[0].score for result in eval_results.test_results])\nprint(EM)\n```\n\n----------------------------------------\n\nTITLE: Generating Answers for HotpotQA Instances using Cognee in Python\nDESCRIPTION: This snippet generates answers for the selected HotpotQA instances using the specified context provider. It sets up random sampling, defines output paths, and iterates through instances to generate answers.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrandom.seed(42)\ninstances = dataset if not num_samples else random.sample(dataset, num_samples)\n\nout_path = \"out\"\nif not Path(out_path).exists():\n    Path(out_path).mkdir()\ncontexts_filename = out_path / Path(\n    f\"contexts_{dataset_name_or_filename.split('.')[0]}_{context_provider_name}.json\"\n)\n\nanswers = []\nfor instance in tqdm(instances, desc=\"Getting answers\"):\n    answer = await answer_qa_instance(instance, context_provider, contexts_filename)\n    answers.append(answer)\n```\n\n----------------------------------------\n\nTITLE: Node Search and Summarization\nDESCRIPTION: Searches for and summarizes information about specific nodes in the knowledge graph\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.api.v1.search import SearchType\n\nnode = (await vector_engine.search(\"Entity_name\", \"sarah.nguyen@example.com\"))[0]\nnode_name = node.payload[\"text\"]\n\nsearch_results = await cognee.search(query_type=SearchType.SUMMARIES, query_text=node_name)\nprint(\"\\n\\Extracted summaries are:\\n\")\nfor result in search_results:\n    print(f\"{result}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Running Cognify Pipeline\nDESCRIPTION: Executes the cognify pipeline by getting the default user and datasets by name\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.modules.users.methods import get_default_user\nfrom cognee.modules.data.methods import get_datasets_by_name\n\nuser = await get_default_user()\n\ndatasets = await get_datasets_by_name([\"example\"], user.id)\n\nawait run_cognify_pipeline(datasets[0], user)\n```\n\n----------------------------------------\n\nTITLE: Vector Search Implementation\nDESCRIPTION: Implements vector-based search functionality for querying the knowledge graph\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nasync def search(\n    vector_engine,\n    collection_name: str,\n    query_text: str = None,\n):\n    query_vector = (await vector_engine.embedding_engine.embed_text([query_text]))[0]\n\n    connection = await vector_engine.get_connection()\n    collection = await connection.open_table(collection_name)\n\n    results = await collection.vector_search(query_vector).limit(10).to_pandas()\n\n    result_values = list(results.to_dict(\"index\").values())\n\n    return [\n        dict(\n            id=str(result[\"id\"]),\n            payload=result[\"payload\"],\n            score=result[\"_distance\"],\n        )\n        for result in result_values\n    ]\n\n\nfrom cognee.infrastructure.databases.vector import get_vector_engine\n\nvector_engine = get_vector_engine()\nresults = await search(vector_engine, \"Entity_name\", \"sarah.nguyen@example.com\")\nfor result in results:\n    print(result)\n```\n\n----------------------------------------\n\nTITLE: Converting Pandas DataFrame to LlamaIndex Documents\nDESCRIPTION: Transforms the pandas DataFrame rows into LlamaIndex Document objects by combining the title and text fields of each news article. This prepares the data for ingestion into Cognee.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_llama_index.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndocuments = [Document(text=f\"{row['title']}: {row['text']}\") for i, row in news.iterrows()]\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Search Functionality\nDESCRIPTION: Defines a search function to query the vector database using embedded text. Returns closest matches with similarity scores.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nasync def search(\n    vector_engine,\n    collection_name: str,\n    query_text: str = None,\n):\n    query_vector = (await vector_engine.embedding_engine.embed_text([query_text]))[0]\n\n    connection = await vector_engine.get_connection()\n    collection = await connection.open_table(collection_name)\n\n    results = await collection.vector_search(query_vector).limit(10).to_pandas()\n\n    result_values = list(results.to_dict(\"index\").values())\n\n    return [\n        dict(\n            id=str(result[\"id\"]),\n            payload=result[\"payload\"],\n            score=result[\"_distance\"],\n        )\n        for result in result_values\n    ]\n```\n\n----------------------------------------\n\nTITLE: Adding Pok√©mon Data Points to Cognee in Python\nDESCRIPTION: Processes Pok√©mon data points and adds them to Cognee using a pipeline. It uses Cognee tasks and modules to handle the data processing.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.modules.pipelines.tasks.Task import Task\nfrom cognee.tasks.storage import add_data_points\nfrom cognee.modules.pipelines import run_tasks\n\ntasks = [Task(add_data_points)]\npipeline_run = run_tasks(\n    tasks=tasks,\n    data=pokemons,\n    dataset_id=uuid5(NAMESPACE_OID, \"Pokemon\"),\n    pipeline_name='pokemon_pipeline',\n)\n\nasync for run_info in pipeline_run:\n    print(run_info.__dict__)\nprint(\"Done\")\n```\n\n----------------------------------------\n\nTITLE: Running Cognee Pipeline with Example Dataset\nDESCRIPTION: Executes the cognify pipeline on a sample dataset using the default user. Demonstrates how to retrieve datasets and initiate processing.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.modules.users.methods import get_default_user\nfrom cognee.modules.data.methods import get_datasets_by_name\n\nuser = await get_default_user()\n\ndatasets = await get_datasets_by_name([\"example\"], user.id)\n\nawait run_cognify_pipeline(datasets[0], user)\n```\n\n----------------------------------------\n\nTITLE: Querying for Alice's Entry to Wonderland\nDESCRIPTION: Uses Cognee's search functionality to query about how Alice ended up in Wonderland from the processed data.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_simple_demo.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nawait cognee.search(\"How did Alice end up in Wonderland?\")\n```\n\n----------------------------------------\n\nTITLE: Querying for Characters in Alice in Wonderland\nDESCRIPTION: Uses Cognee's search functionality to query for influential characters in Alice in Wonderland from the processed data.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_simple_demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait cognee.search(\"List me all the influential characters in Alice in Wonderland.\")\n```\n\n----------------------------------------\n\nTITLE: Querying for Alice's Personality\nDESCRIPTION: Uses Cognee's search functionality to query about Alice's personality traits from the processed data.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_simple_demo.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nawait cognee.search(\"Tell me about Alice's personality.\")\n```\n\n----------------------------------------\n\nTITLE: Searching Pok√©mon Data Using Cognee in Python\nDESCRIPTION: Executes a search query on the processed Pok√©mon data using Cognee. It demonstrates how to use Cognee's search functionality with a specific query type.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.api.v1.search import SearchType\n\nsearch_results = await cognee.search(\n    query_type=SearchType.GRAPH_COMPLETION,\n    query_text=\"pokemons?\"\n)\n\nprint(\"Search results:\")\nfor result_text in search_results:\n    print(result_text)\n```\n\n----------------------------------------\n\nTITLE: Performing a Code Search Query\nDESCRIPTION: Executes a search query against the processed code graph to find occurrences of a specific function definition. Demonstrates the search capability of the Cognee system.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_code_graph_demo.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee import search, SearchType\n\nresults = await search(query_type=SearchType.CODE, query_text=\"def create_graphrag_config\")\n\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: HTTP Server Implementation\nDESCRIPTION: Creates a simple HTTP server for serving the visualization on port 8001\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport http.server\nimport socketserver\nfrom threading import Thread\n\nPORT = 8001\n\n\nclass ServerThread(Thread):\n    def run(self):\n        Handler = http.server.SimpleHTTPRequestHandler\n        with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n            print(\"serving at port\", PORT)\n            httpd.serve_forever()\n\n\nServerThread().start()\n```\n\n----------------------------------------\n\nTITLE: Finding Related Nodes in Knowledge Graph\nDESCRIPTION: Retrieves and displays nodes related to a specific type in the knowledge graph.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/llama_index_cognee_integration.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrelated_nodes = await cogneeRAG.get_related_nodes(\"person\")\n\nprint(\"\\n\\nRelated nodes are:\\n\")\nfor node in related_nodes:\n    print(f\"{node}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Cognee\nDESCRIPTION: Demonstrates how to add multiple job-related data points to Cognee with a specified example name\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport cognee\n\nawait cognee.add([job_1, job_2, job_3, job_4, job_5, job_position], \"example\")\n```\n\n----------------------------------------\n\nTITLE: Loading HotpotQA Dataset in Python\nDESCRIPTION: This snippet loads a subset of the HotpotQA dataset for evaluation. It specifies the number of samples to use and the dataset name.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 10  # With cognee, it takes ~1m10s per sample\ndataset_name_or_filename = \"hotpotqa\"\ndataset = load_qa_dataset(dataset_name_or_filename)\n```\n\n----------------------------------------\n\nTITLE: Resetting Cognee System\nDESCRIPTION: Commands to reset and prune the Cognee system data and metadata.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cognee\n\nawait cognee.prune.prune_data()\nawait cognee.prune.prune_system(metadata=True)\n```\n\n----------------------------------------\n\nTITLE: Performing Graph-Based Search\nDESCRIPTION: Executes a search using the knowledge graph approach to identify people mentioned across the documents.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/graphrag_vs_rag.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.modules.search.types import SearchType\nsearch_results = await cognee.search(SearchType.GRAPH_COMPLETION, \"Tell me who are the people mentioned?\")\n\nprint(\"\\n\\nAnswer based on knowledge graph:\\n\")\nfor result in search_results:\n    print(f\"{result}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Cognee\nDESCRIPTION: Configuration of environment variables for Cognee system including database providers, credentials, and API keys.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Setting environment variables\nif \"GRAPHISTRY_USERNAME\" not in os.environ:\n    os.environ[\"GRAPHISTRY_USERNAME\"] = \"\"\n\nif \"GRAPHISTRY_PASSWORD\" not in os.environ:\n    os.environ[\"GRAPHISTRY_PASSWORD\"] = \"\"\n\nif \"LLM_API_KEY\" not in os.environ:\n    os.environ[\"LLM_API_KEY\"] = \"\"\n\n# \"neo4j\" or \"networkx\"\nos.environ[\"GRAPH_DATABASE_PROVIDER\"] = \"networkx\"\n# Not needed if using networkx\n# os.environ[\"GRAPH_DATABASE_URL\"]=\"\"\n# os.environ[\"GRAPH_DATABASE_USERNAME\"]=\"\"\n# os.environ[\"GRAPH_DATABASE_PASSWORD\"]=\"\"\n\n# \"pgvector\", \"qdrant\", \"weaviate\" or \"lancedb\"\nos.environ[\"VECTOR_DB_PROVIDER\"] = \"lancedb\"\n# Not needed if using \"lancedb\" or \"pgvector\"\n# os.environ[\"VECTOR_DB_URL\"]=\"\"\n# os.environ[\"VECTOR_DB_KEY\"]=\"\"\n\n# Relational Database provider \"sqlite\" or \"postgres\"\nos.environ[\"DB_PROVIDER\"] = \"sqlite\"\n\n# Database name\nos.environ[\"DB_NAME\"] = \"cognee_db\"\n\n# Postgres specific parameters (Only if Postgres or PGVector is used)\n# os.environ[\"DB_HOST\"]=\"127.0.0.1\"\n# os.environ[\"DB_PORT\"]=\"5432\"\n# os.environ[\"DB_USERNAME\"]=\"cognee\"\n# os.environ[\"DB_PASSWORD\"]=\"cognee\"\n```\n\n----------------------------------------\n\nTITLE: Graph Visualization Setup\nDESCRIPTION: Sets up the visualization of the knowledge graph using local file paths\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pathlib\nimport os\nfrom cognee.api.v1.visualize import visualize_graph\n\n# Use the current working directory instead of __file__:\nnotebook_dir = pathlib.Path.cwd()\n\ngraph_file_path = (notebook_dir / \".artifacts\" / \"graph_visualization.html\").resolve()\n\n# Make sure to convert to string if visualize_graph expects a string\nb = await visualize_graph(str(graph_file_path))\n```\n\n----------------------------------------\n\nTITLE: Initializing Cognee Data Directories and Database\nDESCRIPTION: Sets up the required directory structure for Cognee, configures data and system root directories, prunes existing data and metadata, and initializes the database tables.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_code_graph_demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pathlib\nimport cognee\nfrom cognee.infrastructure.databases.relational import create_db_and_tables\n\nnotebook_path = os.path.abspath(\"\")\ndata_directory_path = str(\n    pathlib.Path(os.path.join(notebook_path, \".data_storage/code_graph\")).resolve()\n)\ncognee.config.data_root_directory(data_directory_path)\ncognee_directory_path = str(\n    pathlib.Path(os.path.join(notebook_path, \".cognee_system/code_graph\")).resolve()\n)\ncognee.config.system_root_directory(cognee_directory_path)\n\nawait cognee.prune.prune_data()\nawait cognee.prune.prune_system(metadata=True)\n\nawait create_db_and_tables()\n```\n\n----------------------------------------\n\nTITLE: Loading and Associating Pok√©mon Data in Python\nDESCRIPTION: Defines classes and functions to load Pok√©mon details and associate them with abilities. It uses Cognee DataPoint classes and handles complex data structures.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional\n\nclass Pokemons(DataPoint):\n    name: str = \"Pokemons\"\n    have: Abilities\n    metadata: dict = {\"index_fields\": [\"name\"]}\n\nclass PokemonAbility(DataPoint):\n    name: str\n    ability__name: str\n    ability__url: str\n    is_hidden: bool\n    slot: int\n    _dlt_load_id: str\n    _dlt_id: str\n    _dlt_parent_id: str\n    _dlt_list_idx: str\n    is_type: Abilities\n    metadata: dict = {\"index_fields\": [\"ability__name\"]}\n\nclass Pokemon(DataPoint):\n    name: str\n    base_experience: int\n    height: int\n    weight: int\n    is_default: bool\n    order: int\n    location_area_encounters: str\n    species__name: str\n    species__url: str\n    cries__latest: str\n    cries__legacy: str\n    sprites__front_default: str\n    sprites__front_shiny: str\n    sprites__back_default: Optional[str]\n    sprites__back_shiny: Optional[str]\n    _dlt_load_id: str\n    _dlt_id: str\n    is_type: Pokemons\n    abilities: List[PokemonAbility]\n    metadata: dict = {\"index_fields\": [\"name\"]}\n\ndef load_pokemon_data(jsonl_pokemons, pokemon_abilities, pokemon_root):\n    pokemons = []\n\n    for jsonl_pokemon in jsonl_pokemons:\n        with open(jsonl_pokemon, \"r\") as f:\n            for line in f:\n                pokemon_data = json.loads(line)\n                abilities = [\n                    ability for ability in pokemon_abilities\n                    if ability[\"_dlt_parent_id\"] == pokemon_data[\"_dlt_id\"]\n                ]\n                pokemon_data[\"external_id\"] = pokemon_data[\"id\"]\n                pokemon_data[\"id\"] = uuid5(NAMESPACE_OID, str(pokemon_data[\"id\"]))\n                pokemon_data[\"abilities\"] = [PokemonAbility(**ability) for ability in abilities]\n                pokemon_data[\"is_type\"] = pokemon_root\n                pokemons.append(Pokemon(**pokemon_data))\n\n    return pokemons\n```\n\n----------------------------------------\n\nTITLE: Rendering Knowledge Graph with Graphistry\nDESCRIPTION: Visualizes the generated knowledge graph using Graphistry. Authenticates with Graphistry credentials, retrieves the graph from the graph engine, and renders it to produce a shareable URL.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_llama_index.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport graphistry\n\nfrom cognee.infrastructure.databases.graph import get_graph_engine\nfrom cognee.shared.utils import render_graph\n\n# Get graph\ngraphistry.login(\n    username=os.getenv(\"GRAPHISTRY_USERNAME\"), password=os.getenv(\"GRAPHISTRY_PASSWORD\")\n)\ngraph_engine = await get_graph_engine()\n\ngraph_url = await render_graph(graph_engine.graph)\nprint(graph_url)\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline for Processing Scientific Papers with Cognee in Python\nDESCRIPTION: This code defines two asynchronous functions: run_pipeline() for cleaning data, processing papers, and applying ontological knowledge, and query_pipeline() for querying the processed data with specific questions.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/ontology_demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def run_pipeline(ontology_path=None):\n    # Clean existing data\n    await cognee.prune.prune_data()\n    await cognee.prune.prune_system(metadata=True)\n    \n    # Set up path to scientific papers\n    scientific_papers_dir = os.path.join(\n        os.path.dirname(os.path.dirname(os.path.abspath(\".\"))), \n        \"cognee\",\n        \"examples\",\n        \"data\", \n        \"scientific_papers/\"\n    )\n    \n    # Add papers to the system\n    await cognee.add(scientific_papers_dir)\n    \n    # Cognify with optional ontology\n    return await cognee.cognify(ontology_file_path=ontology_path)\n\nasync def query_pipeline(questions):\n    answers = []\n    for question in questions:\n        search_results = await cognee.search(\n            query_type=SearchType.GRAPH_COMPLETION,\n            query_text=question,\n        )\n        answers.append(search_results)\n    return answers\n```\n\n----------------------------------------\n\nTITLE: Visualizing Knowledge Graph with Graphistry\nDESCRIPTION: Renders the generated knowledge graph using Graphistry visualization tool. Requires Graphistry credentials for authentication.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom cognee.shared.utils import render_graph\nfrom cognee.infrastructure.databases.graph import get_graph_engine\nimport graphistry\n\ngraphistry.login(\n    username=os.getenv(\"GRAPHISTRY_USERNAME\"), password=os.getenv(\"GRAPHISTRY_PASSWORD\")\n)\n\ngraph_engine = await get_graph_engine()\n\ngraph_url = await render_graph(graph_engine.graph)\nprint(graph_url)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Brute Force Context Provider in Python\nDESCRIPTION: This code initializes the 'brute_force' context provider for the QA system. It selects the appropriate context provider from the qa_context_providers dictionary based on the context provider name.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ncontext_provider_name = \"brute_force\"\ncontext_provider = qa_context_providers[context_provider_name]\n```\n\n----------------------------------------\n\nTITLE: Querying Cognee for Multimedia Content Summaries\nDESCRIPTION: Demonstrates how to query the Cognee knowledge graph for summaries of content extracted from multimedia files. The search function is used with the SUMMARIES search type to retrieve descriptive information about the multimedia content.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_multimedia_demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.api.v1.search import SearchType\n\n# Query cognee for summaries of the data in the multimedia files\nsearch_results = await cognee.search(\n    query_type=SearchType.SUMMARIES,\n    query_text=\"What is in the multimedia files?\",\n)\n\n# Display search results\nfor result_text in search_results:\n    print(result_text)\n```\n\n----------------------------------------\n\nTITLE: Processing Pok√©mon Data and Abilities in Python\nDESCRIPTION: Loads and associates Pok√©mon abilities with their respective Pok√©mon data. It uses previously defined functions to process data from stored files.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nSTORAGE_PATH = Path(\".data_storage/pokemon_data/pokemon_details\")\njsonl_pokemons = sorted(STORAGE_PATH.glob(\"*.jsonl\"))\n\nABILITIES_PATH = Path(\".data_storage/pokemon_data/pokemon_details__abilities\")\njsonl_abilities = sorted(ABILITIES_PATH.glob(\"*.jsonl\"))\n\nabilities_root, pokemon_abilities = load_abilities_data(jsonl_abilities)\npokemon_root = Pokemons(have=abilities_root)\npokemons = load_pokemon_data(jsonl_pokemons, pokemon_abilities, pokemon_root)\n```\n\n----------------------------------------\n\nTITLE: Running Cognee with Multimedia Files\nDESCRIPTION: Initializes Cognee, resets its state, adds multimedia files to the system, and builds a knowledge graph from them using the cognify function. This process extracts information from the MP3 and PNG files to create a semantic graph representation.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_multimedia_demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport cognee\n\n# Create a clean slate for cognee -- reset data and system state\nawait cognee.prune.prune_data()\nawait cognee.prune.prune_system(metadata=True)\n\n# Add multimedia files and make them available for cognify\nawait cognee.add([mp3_file_path, png_file_path])\n\n# Create knowledge graph with cognee\nawait cognee.cognify()\n```\n\n----------------------------------------\n\nTITLE: Configuring Code Analysis Pipeline Tasks\nDESCRIPTION: Defines a sequence of tasks for the code analysis pipeline. The tasks extract repository file dependencies and add them as data points to the system, with configurable batch size based on extraction detail level.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_code_graph_demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.tasks.repo_processor import (\n    get_repo_file_dependencies,\n)\nfrom cognee.tasks.storage import add_data_points\nfrom cognee.modules.pipelines.tasks import Task, TaskConfig\n\ndetailed_extraction = True\n\ntasks = [\n    Task(get_repo_file_dependencies, detailed_extraction=detailed_extraction),\n    Task(add_data_points, task_config=TaskConfig(needs=[get_repo_file_dependencies], output_batch_size=100 if detailed_extraction else 500)),\n]\n```\n\n----------------------------------------\n\nTITLE: Initializing Cognee for Pok√©mon Data Processing in Python\nDESCRIPTION: Sets up Cognee for data processing by pruning existing data and system metadata. It defines an asynchronous function to initialize Cognee.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.low_level import setup as cognee_setup\n\nasync def initialize_cognee():\n    await cognee.prune.prune_data()\n    await cognee.prune.prune_system(metadata=True)\n    await cognee_setup()\n\nawait initialize_cognee()\n```\n\n----------------------------------------\n\nTITLE: Setting File Path\nDESCRIPTION: Gets the current working directory and constructs the file path to the Alice in Wonderland text file that will be processed.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_simple_demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncurrent_directory = os.getcwd()\nfile_path = os.path.join(current_directory, \"data\", \"alice_in_wonderland.txt\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Generating Answers from Temporal Graph\nDESCRIPTION: Implements query processing and answer generation using the temporal graph, including triplet search and LLM-based answer generation.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_graphiti_demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Step 1: Formulating the Query üîç\nquery = \"When was Kamala Harris in office?\"\n\n# Step 2: Searching for Relevant Triplets üìä\ntriplets = await brute_force_triplet_search(\n    query=query,\n    top_k=3,\n    collections=[\"graphitinode_content\", \"graphitinode_name\", \"graphitinode_summary\"],\n)\n\n# Step 3: Preparing the Context for the LLM\nretriever = GraphCompletionRetriever()\ncontext = await retriever.resolve_edges_to_text(triplets)\n\nargs = {\"question\": query, \"context\": context}\n\n# Step 4: Generating Prompts ‚úçÔ∏è\nuser_prompt = render_prompt(\"graph_context_for_question.txt\", args)\nsystem_prompt = read_query_prompt(\"answer_simple_question_restricted.txt\")\n\n# Step 5: Interacting with the LLM ü§ñ\nllm_client = get_llm_client()\ncomputed_answer = await llm_client.acreate_structured_output(\n    text_input=user_prompt,  # Input prompt for the user context\n    system_prompt=system_prompt,  # System-level instructions for the model\n    response_model=str,\n)\n\n# Step 6: Displaying the Computed Answer ‚ú®\nprint(f\"üí° Answer: {computed_answer}\")\n```\n\n----------------------------------------\n\nTITLE: Calculating F1 Score for HotpotQA Answers in Python\nDESCRIPTION: This snippet calculates the F1 Score for the generated answers using DeepEval. It defines the metric, evaluates the answers, and computes the average score.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"F1\"]\neval_metrics = get_metrics(metric_name_list)\n\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n\nF1_score = statistics.mean([result.metrics_data[0].score for result in eval_results.test_results])\nprint(F1_score)\n```\n\n----------------------------------------\n\nTITLE: Processing Temporal Graph Data with Cognee Pipeline\nDESCRIPTION: Implements the pipeline for processing temporal data, including database setup, text addition, and graph transformation with temporal awareness.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_graphiti_demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# üîß Setting Up Logging to Suppress Errors\nlogger = get_logger(level=ERROR) # Keeping logs clean and focused\n\n# üßπ Pruning Old Data and Metadata\nawait cognee.prune.prune_data()  # Removing outdated data\nawait cognee.prune.prune_system(metadata=True)\n\n# üèóÔ∏è Creating Relational Database and Tables\nawait create_relational_db_and_tables()\n\n# üìö Adding Text Data to Cognee\nfor text in text_list:\n    await cognee.add(text)\n\n# üï∞Ô∏è Building Temporal-Aware Graphs\ntasks = [\n    Task(build_graph_with_temporal_awareness, text_list=text_list),\n]\n\n# üöÄ Running the Task Pipeline\npipeline = run_tasks(tasks)\n\n# üåü Processing Pipeline Results\nasync for result in pipeline:\n    print(f\"‚úÖ Result Processed: {result}\")\n\n# üîÑ Indexing and Transforming Graph Data\nawait index_and_transform_graphiti_nodes_and_edges()\n```\n\n----------------------------------------\n\nTITLE: Preparing Sample Document Dataset\nDESCRIPTION: Creates sample documents containing profile information about individuals for demonstration.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/llama_index_cognee_integration.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndocuments = [\n    Document(\n        text=\"Jessica Miller, Experienced Sales Manager with a strong track record in driving sales growth and building high-performing teams.\"\n    ),\n    Document(\n        text=\"David Thompson, Creative Graphic Designer with over 8 years of experience in visual design and branding.\"\n    ),\n]\n```\n\n----------------------------------------\n\nTITLE: Processing Text with Cognee\nDESCRIPTION: Adds the text file to Cognee and processes its contents using the cognify method. This prepares the data for querying.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_simple_demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport cognee\nawait cognee.add(file_path)\nawait cognee.cognify()\n```\n\n----------------------------------------\n\nTITLE: Calculating Comprehensiveness Metric for HotpotQA Answers in Python\nDESCRIPTION: This snippet calculates the Comprehensiveness metric for the generated answers using DeepEval. It defines the metric, evaluates the answers, and computes the average score.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Comprehensiveness\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n\nComprehensiveness = statistics.mean(\n    [result.metrics_data[0].score for result in eval_results.test_results]\n)\nprint(Comprehensiveness)\n```\n\n----------------------------------------\n\nTITLE: Executing Pok√©mon Data Pipeline in Python\nDESCRIPTION: Runs the DLT pipeline to fetch and store Pok√©mon data. It uses the previously defined functions to retrieve and process the data.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ninfo = pipeline.run([pokemon_list, pokemon_details])\nprint(info)\n```\n\n----------------------------------------\n\nTITLE: Calculating Empowerment Metric for HotpotQA Answers in Python\nDESCRIPTION: This snippet calculates the Empowerment metric for the generated answers using DeepEval. It defines the metric, evaluates the answers, and computes the average score.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Empowerment\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n\nEmpowerment = statistics.mean(\n    [result.metrics_data[0].score for result in eval_results.test_results]\n)\nprint(Empowerment)\n```\n\n----------------------------------------\n\nTITLE: Fetching Detailed Pok√©mon Information in Python\nDESCRIPTION: Defines a transformer function to fetch detailed information about each Pok√©mon. It takes the list of Pok√©mon and makes individual API calls for each one.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dlt.transformer(data_from=pokemon_list)\ndef pokemon_details(pokemons):\n    \"\"\"Fetches detailed info for each Pok√©mon\"\"\"\n    import requests\n    for pokemon in pokemons:\n        response = requests.get(pokemon[\"url\"])\n        response.raise_for_status()\n        yield response.json()\n```\n\n----------------------------------------\n\nTITLE: Calculating Comprehensiveness Metric in Python\nDESCRIPTION: This code calculates the 'Comprehensiveness' metric for a question answering system. It uses the deepeval library to evaluate answers against test instances and calculates the mean score across all test results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Comprehensiveness\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nComprehensiveness = statistics.mean(\n    [result.metrics_data[0].score for result in eval_results.test_results]\n)\nprint(Comprehensiveness)\n```\n\n----------------------------------------\n\nTITLE: Fetching Pok√©mon List from API in Python\nDESCRIPTION: Defines a function to retrieve a list of Pok√©mon from the API. It uses the requests library to make an HTTP GET request and yields the results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dlt.resource(write_disposition=\"replace\")\ndef pokemon_list(limit: int = 50):\n    import requests\n    response = requests.get(f\"{BASE_URL}pokemon\", params={\"limit\": limit})\n    response.raise_for_status()\n    yield response.json()[\"results\"]\n```\n\n----------------------------------------\n\nTITLE: Defining Input Text with Temporal Information\nDESCRIPTION: Creates a list of text samples containing temporal information about Kamala Harris's career positions.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_graphiti_demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntext_list = [\n    \"Kamala Harris is the Attorney General of California. She was previously \"\n    \"the district attorney for San Francisco.\",\n    \"As AG, Harris was in office from January 3, 2011 ‚Äì January 3, 2017\",\n]\n```\n\n----------------------------------------\n\nTITLE: Initializing DLT Pipeline for Pok√©mon Data in Python\nDESCRIPTION: Creates a DLT pipeline to fetch and store Pok√©mon data. It sets up the pipeline with a specific name, destination, and dataset name.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport dlt\nfrom pathlib import Path\n\npipeline = dlt.pipeline(\n    pipeline_name=\"pokemon_pipeline\",\n    destination=\"filesystem\",\n    dataset_name=\"pokemon_data\",\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Directness Metric in Python\nDESCRIPTION: This code calculates the 'Directness' metric for a question answering system. It evaluates how directly the answers address the questions and calculates the mean score across all test results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Directness\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nDirectness = statistics.mean([result.metrics_data[0].score for result in eval_results.test_results])\nprint(Directness)\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment for Pok√©mon Data Pipeline in Python\nDESCRIPTION: Sets up required directories and environment variables for the Pok√©mon data pipeline. It configures Cognee directories and sets the base URL for the Pok√©mon API.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pathlib\nimport os\nimport cognee\n\nnotebook_dir = pathlib.Path().resolve()\ndata_directory_path = str(notebook_dir / \".data_storage\")\ncognee_directory_path = str(notebook_dir / \".cognee_system\")\n\ncognee.config.data_root_directory(data_directory_path)\ncognee.config.system_root_directory(cognee_directory_path)\n\nBASE_URL = \"https://pokeapi.co/api/v2/\"\nos.environ[\"BUCKET_URL\"] = data_directory_path\nos.environ[\"DATA_WRITER__DISABLE_COMPRESSION\"] = \"true\"\n```\n\n----------------------------------------\n\nTITLE: Calculating Exact Match (EM) Score in Python\nDESCRIPTION: This code calculates the Exact Match (EM) Score for a question answering system. It evaluates whether the predicted answers exactly match the reference answers and calculates the mean score across all test results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"EM\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nEM = statistics.mean([result.metrics_data[0].score for result in eval_results.test_results])\nprint(EM)\n```\n\n----------------------------------------\n\nTITLE: Managing Student Data and Operations in JavaScript\nDESCRIPTION: This snippet demonstrates the usage of the defined classes and functions. It enrolls students, celebrates their birthdays, promotes them, and displays their information.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/tests/test_data/code.txt#2025-04-07_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Variable definition and assignment\nlet schoolName = \"Greenwood High School\";\nlet students = [];\n\n// Enrolling students\nstudents.push(enrollStudent(\"Alice\", 14, 9));\nstudents.push(enrollStudent(\"Bob\", 15, 10));\n\n// Looping through students to celebrate their birthdays\nstudents.forEach(student => {\n  console.log(student.celebrateBirthday());\n});\n\n// Promoting all students\nstudents = students.map(promoteStudent);\n\n// Displaying the final state of all students\nconsole.log(\"Final Students List:\");\nstudents.forEach(student => console.log(student.describe()));\n\n// Updating the school name\nschoolName = \"Greenwood International School\";\nconsole.log(`School Name Updated to: ${schoolName}`);\n```\n\n----------------------------------------\n\nTITLE: Activating Python Virtual Environment\nDESCRIPTION: Activates the Python virtual environment created by UV for the project.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-mcp/README.md#2025-04-07_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\nsource .venv/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Implementing Student Management Functions in JavaScript\nDESCRIPTION: This snippet defines two functions: enrollStudent for creating and enrolling a new student, and promoteStudent for advancing a student to the next grade.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/tests/test_data/code.txt#2025-04-07_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Function to enroll a new student\nfunction enrollStudent(name, age, grade) {\n  const student = new Student(name, age, grade);\n  console.log(student.greet());\n  console.log(student.describe());\n  return student;\n}\n\n// Function to promote a student to the next grade\nfunction promoteStudent(student) {\n  student.grade += 1;\n  console.log(`${student.name} has been promoted to grade ${student.grade}.`);\n  return student;\n}\n```\n\n----------------------------------------\n\nTITLE: Generating HotpotQA Dataset with Cognee Framework\nDESCRIPTION: This code snippet demonstrates how to use the Cognee framework to generate a dataset from HotpotQA. It creates two JSON files: one containing 50 randomly selected instances from the corpus and another with corresponding question-answer pairs.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/evals/README.md#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom cognee.eval_framework.benchmark_adapters.hotpot_qa_adapter import HotpotQAAdapter\n\nadapter = HotpotQAAdapter()\ncorpus_list, question_answer_pairs = adapter.load_corpus(limit=50)\n\nwith open('hotpot_50_corpus.json', 'w') as outfile:\n    json.dump(corpus_list, outfile)\nwith open('hotpot_50_qa_pairs.json', 'w') as outfile:\n    json.dump(question_answer_pairs, outfile)\n```\n\n----------------------------------------\n\nTITLE: Defining Person and Student Classes in JavaScript\nDESCRIPTION: This snippet defines a Person class with basic attributes and methods, and a Student class that extends Person with additional properties and methods specific to students.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/tests/test_data/code.txt#2025-04-07_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Class definition for a Person\nclass Person {\n  constructor(name, age) {\n    this.name = name;\n    this.age = age;\n  }\n\n  // Method to return a greeting message\n  greet() {\n    return `Hello, my name is ${this.name} and I'm ${this.age} years old.`;\n  }\n\n  // Method to celebrate birthday\n  celebrateBirthday() {\n    this.age += 1;\n    return `Happy Birthday, ${this.name}! You are now ${this.age} years old.`;\n  }\n}\n\n// Class definition for a Student, extending from Person\nclass Student extends Person {\n  constructor(name, age, grade) {\n    super(name, age);\n    this.grade = grade;\n  }\n\n  // Method to describe the student\n  describe() {\n    return `${this.name} is a ${this.grade} grade student and is ${this.age} years old.`;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Relevant CV Data (Example 3) in Python\nDESCRIPTION: Creates a sample CV for a third qualified data scientist with a statistics background. This candidate has experience in machine learning algorithm design and implementation, specifically for financial forecasting.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\njob_3 = \"\"\"\nCV 3: Relevant\nName: Sarah Nguyen\nContact Information:\n\nEmail: sarah.nguyen@example.com\nPhone: (555) 345-6789\nSummary:\n\nData Scientist specializing in machine learning with 6 years of experience. Passionate about leveraging data to drive business solutions and improve product performance.\n\nEducation:\n\nM.S. in Statistics, University of Washington (2014)\nB.S. in Applied Mathematics, University of Texas at Austin (2012)\nExperience:\n\nData Scientist, QuantumTech (2016 ‚Äì Present)\nDesigned and implemented machine learning algorithms for financial forecasting.\nImproved model efficiency by 20% through algorithm optimization.\nJunior Data Scientist, DataCore Solutions (2014 ‚Äì 2016)\nAssisted in developing predictive models for supply chain optimization.\nConducted data cleaning and preprocessing on large datasets.\nSkills:\n\nProgramming Languages: Python, R\nMachine Learning Frameworks: PyTorch, Scikit-Learn\nStatistical Analysis: SAS, SPSS\nCloud Platforms: AWS, Azure\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Calculating Directness Metric for HotpotQA Answers in Python\nDESCRIPTION: This code calculates the Directness metric for the generated answers using DeepEval. It defines the metric, evaluates the answers, and computes the average score.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Directness\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n\nDirectness = statistics.mean([result.metrics_data[0].score for result in eval_results.test_results])\nprint(Directness)\n```\n\n----------------------------------------\n\nTITLE: Installing Cognee Helm Chart\nDESCRIPTION: Command to install the Cognee Helm chart into a Kubernetes cluster. This deploys all infrastructure components defined in the cognee-chart directory.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/helm/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhelm install cognee ./cognee-chart\n```\n\n----------------------------------------\n\nTITLE: Calculating Diversity Metric for HotpotQA Answers in Python\nDESCRIPTION: This code calculates the Diversity metric for the generated answers using DeepEval. It defines the metric, evaluates the answers, and computes the average score.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Diversity\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n\nDiversity = statistics.mean([result.metrics_data[0].score for result in eval_results.test_results])\nprint(Diversity)\n```\n\n----------------------------------------\n\nTITLE: Navigating to Claude Configuration Directory\nDESCRIPTION: Changes directory to the Claude Desktop configuration folder to access configuration files.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-mcp/README.md#2025-04-07_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/Library/Application\\ Support/Claude/\n```\n\n----------------------------------------\n\nTITLE: Calculating Correctness Metric for HotpotQA Answers in Python\nDESCRIPTION: This code calculates the Correctness metric for the generated answers using DeepEval. It defines the metric, evaluates the answers, and computes the average score.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmetric_name_list = [\"Correctness\"]\neval_metrics = get_metrics(metric_name_list)\neval_results = await deepeval_answers(instances, answers, eval_metrics[\"deepeval_metrics\"])\n\nCorrectness = statistics.mean(\n    [result.metrics_data[0].score for result in eval_results.test_results]\n)\nprint(Correctness)\n```\n\n----------------------------------------\n\nTITLE: Installing Cognee via Smithery CLI\nDESCRIPTION: Automated installation of Cognee for Claude Desktop using the Smithery CLI tool.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-mcp/README.md#2025-04-07_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnpx -y @smithery/cli install cognee --client claude\n```\n\n----------------------------------------\n\nTITLE: Selecting Context Provider for HotpotQA Evaluation in Python\nDESCRIPTION: This code selects a context provider for the evaluation. In this case, it uses the 'cognee' context provider from the available options.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncontext_provider_name = \"cognee\"\ncontext_provider = qa_context_providers[context_provider_name]\n```\n\n----------------------------------------\n\nTITLE: Example Entity Extraction JSON Response Format\nDESCRIPTION: Demonstrates the expected JSON response format for the entity extraction system, showing how to structure extracted entities with their types and descriptions.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/extract_entities_system.txt#2025-04-07_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"entities\": [\n    {\n      \"name\": \"Albert Einstein\",\n      \"is_a\": {\n        \"name\": \"PERSON\",\n        \"description\": \"Entity type for person entities\"\n      },\n      \"description\": \"A theoretical physicist who developed the theory of relativity.\"\n    },\n    {\n      \"name\": \"Theory of Relativity\",\n      \"is_a\": {\n        \"name\": \"CONCEPT\",\n        \"description\": \"Entity type for concept entities\"\n      },\n      \"description\": \"A physics theory describing the relationship between space and time.\"\n    },\n    {\n      \"name\": \"Princeton University\",\n      \"is_a\": {\n        \"name\": \"ORGANIZATION\",\n        \"description\": \"Entity type for organization entities\"\n      },\n      \"description\": \"An Ivy League research university in Princeton, New Jersey.\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for HotpotQA Evaluation in Python\nDESCRIPTION: This code imports necessary functions and libraries for evaluating question answering on the HotpotQA dataset. It includes imports for answer generation, dataset loading, metric calculation, and context providers.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_hotpot_eval.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom evals.eval_on_hotpot import deepeval_answers, answer_qa_instance\nfrom evals.qa_dataset_utils import load_qa_dataset\nfrom evals.qa_metrics_utils import get_metrics\nfrom evals.qa_context_provider_utils import qa_context_providers\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport statistics\nimport random\n```\n\n----------------------------------------\n\nTITLE: Defining Content Classification Schema in JSON\nDESCRIPTION: This JSON schema defines a hierarchical classification system for various content types. It categorizes content into major types like text, audio, image, video, and other formats, with detailed subcategories for each type to enable precise content classification.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/classify_content.txt#2025-04-07_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n    \"Natural Language Text\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Articles, essays, and reports\",\n            \"Books and manuscripts\",\n            \"News stories and blog posts\",\n            \"Research papers and academic publications\",\n            \"Social media posts and comments\",\n            \"Website content and product descriptions\",\n            \"Personal narratives and stories\"\n        ]\n    },\n    \"Structured Documents\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Spreadsheets and tables\",\n            \"Forms and surveys\",\n            \"Databases and CSV files\"\n        ]\n    },\n    \"Code and Scripts\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Source code in various programming languages\",\n            \"Shell commands and scripts\",\n            \"Markup languages (HTML, XML)\",\n            \"Stylesheets (CSS) and configuration files (YAML, JSON, INI)\"\n        ]\n    },\n    \"Conversational Data\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Chat transcripts and messaging history\",\n            \"Customer service logs and interactions\",\n            \"Conversational AI training data\"\n        ]\n    },\n    \"Educational Content\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Textbook content and lecture notes\",\n            \"Exam questions and academic exercises\",\n            \"E-learning course materials\"\n        ]\n    },\n    \"Creative Writing\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Poetry and prose\",\n            \"Scripts for plays, movies, and television\",\n            \"Song lyrics\"\n        ]\n    },\n    \"Technical Documentation\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Manuals and user guides\",\n            \"Technical specifications and API documentation\",\n            \"Helpdesk articles and FAQs\"\n        ]\n    },\n    \"Legal and Regulatory Documents\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Contracts and agreements\",\n            \"Laws, regulations, and legal case documents\",\n            \"Policy documents and compliance materials\"\n        ]\n    },\n    \"Medical and Scientific Texts\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Clinical trial reports\",\n            \"Patient records and case notes\",\n            \"Scientific journal articles\"\n        ]\n    },\n    \"Financial and Business Documents\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Financial reports and statements\",\n            \"Business plans and proposals\",\n            \"Market research and analysis reports\"\n        ]\n    },\n    \"Advertising and Marketing Materials\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Ad copies and marketing slogans\",\n            \"Product catalogs and brochures\",\n            \"Press releases and promotional content\"\n        ]\n    },\n    \"Emails and Correspondence\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Professional and formal correspondence\",\n            \"Personal emails and letters\"\n        ]\n    },\n    \"Metadata and Annotations\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Image and video captions\",\n            \"Annotations and metadata for various media\"\n        ]\n    },\n    \"Language Learning Materials\": {\n        \"type\": \"TEXT\",\n        \"subclass\": [\n            \"Vocabulary lists and grammar rules\",\n            \"Language exercises and quizzes\"\n        ]\n    },\n    \"Audio Content\": {\n    \"type\": \"AUDIO\",\n    \"subclass\": [\n        \"Music tracks and albums\",\n        \"Podcasts and radio broadcasts\",\n        \"Audiobooks and audio guides\",\n        \"Recorded interviews and speeches\",\n        \"Sound effects and ambient sounds\"\n    ]\n    },\n    \"Image Content\": {\n        \"type\": \"IMAGE\",\n        \"subclass\": [\n            \"Photographs and digital images\",\n            \"Illustrations, diagrams, and charts\",\n            \"Infographics and visual data representations\",\n            \"Artwork and paintings\",\n            \"Screenshots and graphical user interfaces\"\n        ]\n    },\n    \"Video Content\": {\n        \"type\": \"VIDEO\",\n        \"subclass\": [\n            \"Movies and short films\",\n            \"Documentaries and educational videos\",\n            \"Video tutorials and how-to guides\",\n            \"Animated features and cartoons\",\n            \"Live event recordings and sports broadcasts\"\n        ]\n    },\n    \"Multimedia Content\": {\n        \"type\": \"MULTIMEDIA\",\n        \"subclass\": [\n            \"Interactive web content and games\",\n            \"Virtual reality (VR) and augmented reality (AR) experiences\",\n            \"Mixed media presentations and slide decks\",\n            \"E-learning modules with integrated multimedia\",\n            \"Digital exhibitions and virtual tours\"\n        ]\n    },\n    \"3D Models and CAD Content\": {\n        \"type\": \"3D_MODEL\",\n        \"subclass\": [\n            \"Architectural renderings and building plans\",\n            \"Product design models and prototypes\",\n            \"3D animations and character models\",\n            \"Scientific simulations and visualizations\",\n            \"Virtual objects for AR/VR environments\"\n        ]\n    },\n    \"Procedural Content\": {\n        \"type\": \"PROCEDURAL\",\n        \"subclass\": [\n            \"Tutorials and step-by-step guides\",\n            \"Workflow and process descriptions\",\n            \"Simulation and training exercises\",\n            \"Recipes and crafting instructions\"\n        ]\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Evaluation Framework\nDESCRIPTION: Initializes the evaluation environment with deepeval package and required dependencies. Prepares for running various evaluation metrics.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom evals.eval_on_hotpot import deepeval_answers, answer_qa_instance\nfrom evals.qa_dataset_utils import load_qa_dataset\nfrom evals.qa_metrics_utils import get_metrics\nfrom evals.qa_context_provider_utils import qa_context_providers\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport statistics\nimport random\n\nnum_samples = 10\ndataset_name_or_filename = \"hotpotqa\"\ndataset = load_qa_dataset(dataset_name_or_filename)\n\ncontext_provider_name = \"cognee\"\ncontext_provider = qa_context_providers[context_provider_name]\n```\n\n----------------------------------------\n\nTITLE: Defining Relevant CV Data - Example 1\nDESCRIPTION: Sample CV data for a qualified Data Scientist candidate with PhD and relevant experience.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\njob_1 = \"\"\"\nCV 1: Relevant\nName: Dr. Emily Carter\nContact Information:\n\nEmail: emily.carter@example.com\nPhone: (555) 123-4567\nSummary:\n\nSenior Data Scientist with over 8 years of experience in machine learning and predictive analytics. Expertise in developing advanced algorithms and deploying scalable models in production environments.\n\nEducation:\n\nPh.D. in Computer Science, Stanford University (2014)\nB.S. in Mathematics, University of California, Berkeley (2010)\nExperience:\n\nSenior Data Scientist, InnovateAI Labs (2016 ‚Äì Present)\nLed a team in developing machine learning models for natural language processing applications.\nImplemented deep learning algorithms that improved prediction accuracy by 25%.\nCollaborated with cross-functional teams to integrate models into cloud-based platforms.\nData Scientist, DataWave Analytics (2014 ‚Äì 2016)\nDeveloped predictive models for customer segmentation and churn analysis.\nAnalyzed large datasets using Hadoop and Spark frameworks.\nSkills:\n\nProgramming Languages: Python, R, SQL\nMachine Learning: TensorFlow, Keras, Scikit-Learn\nBig Data Technologies: Hadoop, Spark\nData Visualization: Tableau, Matplotlib\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Defining Non-Relevant CV Data (Example 5) in Python\nDESCRIPTION: Creates a sample CV for a sales manager which is not relevant to the data scientist position. This CV demonstrates a candidate with sales and business background but no technical or data science skills.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\njob_5 = \"\"\"\nCV 5: Not Relevant\nName: Jessica Miller\nContact Information:\n\nEmail: jessica.miller@example.com\nPhone: (555) 567-8901\nSummary:\n\nExperienced Sales Manager with a strong track record in driving sales growth and building high-performing teams. Excellent communication and leadership skills.\n\nEducation:\n\nB.A. in Business Administration, University of Southern California (2010)\nExperience:\n\nSales Manager, Global Enterprises (2015 ‚Äì Present)\nManaged a sales team of 15 members, achieving a 20% increase in annual revenue.\nDeveloped sales strategies that expanded customer base by 25%.\nSales Representative, Market Leaders Inc. (2010 ‚Äì 2015)\nConsistently exceeded sales targets and received the 'Top Salesperson' award in 2013.\nSkills:\n\nSales Strategy and Planning\nTeam Leadership and Development\nCRM Software: Salesforce, Zoho\nNegotiation and Relationship Building\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Sample Log Entry Format in Structured Logging\nDESCRIPTION: Demonstrates the standard format for log entries including timestamp, log level, structured message with key-value pairs, and logger name. Uses ISO timestamp format and consistent padding for log levels.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/logs/README.md#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n2025-03-27T13:05:27.481446Z [INFO    ] Structured log message user_id=user123 action=login status=success [TestLogger]\n```\n\n----------------------------------------\n\nTITLE: Defining Non-Relevant CV Data (Example 4) in Python\nDESCRIPTION: Creates a sample CV for a graphic designer which is not relevant to the data scientist position. This CV demonstrates a candidate with no data science or machine learning experience.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\njob_4 = \"\"\"\nCV 4: Not Relevant\nName: David Thompson\nContact Information:\n\nEmail: david.thompson@example.com\nPhone: (555) 456-7890\nSummary:\n\nCreative Graphic Designer with over 8 years of experience in visual design and branding. Proficient in Adobe Creative Suite and passionate about creating compelling visuals.\n\nEducation:\n\nB.F.A. in Graphic Design, Rhode Island School of Design (2012)\nExperience:\n\nSenior Graphic Designer, CreativeWorks Agency (2015 ‚Äì Present)\nLed design projects for clients in various industries.\nCreated branding materials that increased client engagement by 30%.\nGraphic Designer, Visual Innovations (2012 ‚Äì 2015)\nDesigned marketing collateral, including brochures, logos, and websites.\nCollaborated with the marketing team to develop cohesive brand strategies.\nSkills:\n\nDesign Software: Adobe Photoshop, Illustrator, InDesign\nWeb Design: HTML, CSS\nSpecialties: Branding and Identity, Typography\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Base64 Encoded Image Data\nDESCRIPTION: A base64 encoded string containing binary image data, likely representing a JPEG image based on the header pattern. This appears to be part of an image handling or processing system.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/graphrag_vs_rag.ipynb#2025-04-07_snippet_8\n\nLANGUAGE: base64\nCODE:\n```\nAhPpR3o60dqBhj0paOtFAhPpR3o61FcXEFrA1zdOscaDLMxCgD3J4oGS49KWvn/wAZftQfBPwUWhvtZjvJ0O0w2Q+0NnuMr8g/FhXjDftM/F74hZg+C/gi4eNuBeX4IjGeg4KJ7/6w/SgR9zfSvN/GPxf+GXgEH/hK9atbWQZ/db98vHX92m5/0r5g/wCFIftJ/Es+Z8VPGX9l2khy1npwxwOx2bF/MvXpHg/9kH4K+FsXF5YPrFzj5pL5/MBPc7BtTk+qmgZw2o/tk2euXTaT8H/DWoeIrnor7DHGCfUKHbGPULVAaR+2b8UDv1G9tPBljJzshwZwOgHyl3z3++tfbOm6Xpej2wsdItorWFekcKKij8FAH6VfoEfGnh/9i3wILoat8Q9TvvEt5wWM8jIh7ngEuRn1evprwt8PvBHgiAW/hLSrXTwO8MYDH6t94/ia7DrR2oGGPSlo60UCE+lHejrR2oGGPSlo60UCE+lHejrR2oGGPSlo60UCE+lHejrR2oGGPSlo60UCE+lHejrR2oGGPSlo60UCE+lHejrR2oGGPSlo60UCE+lHejrR2oGfxp/8HQfh2XwB+1j+yP8Atc/GXTrjWfgz4N166tNcVIXmg0+7muLO5SWZEBLi4jg3CLDCQWjIQdwVsf8A4Ka/tt/sM6r/AME6/iKbzxzoPiNPG3hq6tdDsbK6hu7i4vryF/7PkW3RjIghuFWVndR5RjJbDLtr+vH40/A74O/tHfDfUfg/8efDOm+L/C+rKEutM1W3jubeTadyPskB2yRsA0ci4eNwHQqwBH49fCP/AINsP+COnwa+KVt8WvDnwq/tG90+6F5Y2esapqGo2Fs67dg+y3Nw8c6IQSFuhMMsSc4Xb+LeI/gphOJM1wea1684SoNaRtZpS5v+3XfquluyPyrjnwpw2e5jhcxq1pQdFrRbOzv8nfr29D0T/g3z+HfxO+Ff/BHX4I+Cvi/p9xpWuw6bqFw1rdQtBNHa3mq3l1ZB42VGUm0lhPK5OcnPU/sp3o60dq/aT9VDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS0daKBCfSjvR1o7UDDHpS4zR1pCAetNAf\n```\n\n----------------------------------------\n\nTITLE: Defining Relevant CV Data (Example 2) in Python\nDESCRIPTION: Creates a sample CV for another qualified data scientist with a Master's degree and machine learning background. This candidate has experience in developing predictive models and analyzing user behavior data.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\njob_2 = \"\"\"\nCV 2: Relevant\nName: Michael Rodriguez\nContact Information:\n\nEmail: michael.rodriguez@example.com\nPhone: (555) 234-5678\nSummary:\n\nData Scientist with a strong background in machine learning and statistical modeling. Skilled in handling large datasets and translating data into actionable business insights.\n\nEducation:\n\nM.S. in Data Science, Carnegie Mellon University (2013)\nB.S. in Computer Science, University of Michigan (2011)\nExperience:\n\nSenior Data Scientist, Alpha Analytics (2017 ‚Äì Present)\nDeveloped machine learning models to optimize marketing strategies.\nReduced customer acquisition cost by 15% through predictive modeling.\nData Scientist, TechInsights (2013 ‚Äì 2017)\nAnalyzed user behavior data to improve product features.\nImplemented A/B testing frameworks to evaluate product changes.\nSkills:\n\nProgramming Languages: Python, Java, SQL\nMachine Learning: Scikit-Learn, XGBoost\nData Visualization: Seaborn, Plotly\nDatabases: MySQL, MongoDB\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Knowledge Graph Analysis Template in Markdown\nDESCRIPTION: Template structure for analyzing text and extracting relationships between entities, showing placeholders for round numbers, input text, potential nodes, and previously identified nodes and relationships.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/tasks/graph/cascade_extract/prompts/extract_graph_relationship_names_prompt_input.txt#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nAnalyze the following text to identify relationships between entities in the knowledge graph. This is round {{ round_number }} of {{ total_rounds }}.\n\n**Text:**\n{{ text }}\n\n**Previously Extracted Potential Nodes:**\n{{ potential_nodes }}\n\n**Nodes Identified in Previous Rounds:**\n{{ previous_nodes }}\n\n**Relationships Identified in Previous Rounds:**\n{{ previous_relationship_names }}\n\nExtract both explicit and implicit relationships between the nodes, building upon previous findings while ensuring completeness and consistency.\n```\n\n----------------------------------------\n\nTITLE: Defining Relevant CV Data (Example 1) in Python\nDESCRIPTION: Creates a sample CV for a qualified data scientist with relevant experience in machine learning. This CV matches the job requirements with PhD credentials and machine learning expertise.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\njob_1 = \"\"\"\nCV 1: Relevant\nName: Dr. Emily Carter\nContact Information:\n\nEmail: emily.carter@example.com\nPhone: (555) 123-4567\nSummary:\n\nSenior Data Scientist with over 8 years of experience in machine learning and predictive analytics. Expertise in developing advanced algorithms and deploying scalable models in production environments.\n\nEducation:\n\nPh.D. in Computer Science, Stanford University (2014)\nB.S. in Mathematics, University of California, Berkeley (2010)\nExperience:\n\nSenior Data Scientist, InnovateAI Labs (2016 ‚Äì Present)\nLed a team in developing machine learning models for natural language processing applications.\nImplemented deep learning algorithms that improved prediction accuracy by 25%.\nCollaborated with cross-functional teams to integrate models into cloud-based platforms.\nData Scientist, DataWave Analytics (2014 ‚Äì 2016)\nDeveloped predictive models for customer segmentation and churn analysis.\nAnalyzed large datasets using Hadoop and Spark frameworks.\nSkills:\n\nProgramming Languages: Python, R, SQL\nMachine Learning: TensorFlow, Keras, Scikit-Learn\nBig Data Technologies: Hadoop, Spark\nData Visualization: Tableau, Matplotlib\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Selecting Relevant Summaries Based on Query (Plaintext)\nDESCRIPTION: This snippet provides a template for selecting summaries that are relevant to a given query. It uses placeholder variables '{{ query }}' for the user's query and '{{ summaries }}' for the list of all available summaries. These placeholders would be replaced with actual values when the template is processed.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/categorize_summary.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nChose the summaries that are relevant to the following query: `{{ query }}`\nHere are the all summaries: `{{ summaries }}`\n```\n\n----------------------------------------\n\nTITLE: Defining Job Position Data in Python\nDESCRIPTION: Defines a senior data scientist job position with requirements and responsibilities, which will be used for processing and search operations in Cognee.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/hr_demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\njob_position = \"\"\"Senior Data Scientist (Machine Learning)\n\nCompany: TechNova Solutions\nLocation: San Francisco, CA\n\nJob Description:\n\nTechNova Solutions is seeking a Senior Data Scientist specializing in Machine Learning to join our dynamic analytics team. The ideal candidate will have a strong background in developing and deploying machine learning models, working with large datasets, and translating complex data into actionable insights.\n\nResponsibilities:\n\nDevelop and implement advanced machine learning algorithms and models.\nAnalyze large, complex datasets to extract meaningful patterns and insights.\nCollaborate with cross-functional teams to integrate predictive models into products.\nStay updated with the latest advancements in machine learning and data science.\nMentor junior data scientists and provide technical guidance.\nQualifications:\n\nMaster's or Ph.D. in Data Science, Computer Science, Statistics, or a related field.\n5+ years of experience in data science and machine learning.\nProficient in Python, R, and SQL.\nExperience with deep learning frameworks (e.g., TensorFlow, PyTorch).\nStrong problem-solving skills and attention to detail.\nCandidate CVs\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for Metrics Plotting\nDESCRIPTION: Lists required Python packages and their specific versions needed for data analysis and visualization. Includes numpy for numerical operations, matplotlib and seaborn for plotting, scipy for scientific computing, and pathlib for file path handling.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/evals/requirements.txt#2025-04-07_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nnumpy==1.26.4\nmatplotlib==3.10.0\nseaborn==0.13.2\nscipy==1.11.4\npathlib\n```\n\n----------------------------------------\n\nTITLE: Finding Related Nodes in Knowledge Graph\nDESCRIPTION: Explores relationships in the knowledge graph by searching for nodes related to the concept of \"person\".\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/graphrag_vs_rag.ipynb#2025-04-07_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrelated_nodes = await cognee.search(SearchType.INSIGHTS, \"person\")\n\nprint(\"\\n\\nRelated nodes are:\\n\")\nfor node in related_nodes:\n    print(f\"{node}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Person Node from Text in Knowledge Graph\nDESCRIPTION: Demonstrates how to extract a Person node from unstructured text, following the node label consistency rule.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/generate_graph_prompt_oneshot.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nMarie Curie (Person)\n```\n\n----------------------------------------\n\nTITLE: Performing RAG-Based Search\nDESCRIPTION: Executes a search using the traditional RAG approach to compare its results with the graph-based approach.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/graphrag_vs_rag.ipynb#2025-04-07_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nsearch_results = await cognee.search(SearchType.RAG_COMPLETION, \"Tell me who are the people mentioned?\")\n\nprint(\"\\n\\nAnswer based on RAG:\\n\")\nfor result in search_results:\n    print(f\"{result}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Creating Relationship Edge between Concept and Person Nodes in Knowledge Graph\nDESCRIPTION: Illustrates how to create a directional edge between a Concept node and a Person node, ensuring logical consistency in the relationship direction.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/generate_graph_prompt_oneshot.txt#2025-04-07_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nRadioactivity (Concept) ‚Äì discovered_by -> Marie Curie (Person)\n```\n\n----------------------------------------\n\nTITLE: Adding Data to Cognee\nDESCRIPTION: Loads the dataset into the Cognee framework to prepare it for processing and knowledge graph creation.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/graphrag_vs_rag.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait cognee.add(documents)\n```\n\n----------------------------------------\n\nTITLE: Adding Date and Numeric Properties to Organization Node in Knowledge Graph\nDESCRIPTION: Demonstrates how to add date properties using ISO 8601 format and numeric properties as literal values to an Organization node.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/generate_graph_prompt_oneshot.txt#2025-04-07_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nGoogle (Organization)\n   founded_on: \"1998-09-04\"\n   market_cap: \"800000000000\"\n```\n\n----------------------------------------\n\nTITLE: Preparing Sample Dataset\nDESCRIPTION: Creates a simple dataset with two text documents containing information about individuals for demonstration purposes.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/graphrag_vs_rag.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndocuments = [\"Jessica Miller, Experienced Sales Manager with a strong track record in building high-performing teams.\",\n             \"David Thompson, Creative Graphic Designer with over 8 years of experience in visual design and branding.\"\n    ]\n```\n\n----------------------------------------\n\nTITLE: Adding Properties to Person Node in Knowledge Graph\nDESCRIPTION: Shows how to add properties as key-value pairs to a Person node, using snake_case for property names.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/generate_graph_prompt_oneshot.txt#2025-04-07_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nMarie Curie (Person)\n   birth_place: \"Warsaw\"\n   birth_year: \"1867\"\n```\n\n----------------------------------------\n\nTITLE: Installing Cognee Package\nDESCRIPTION: Installs the Cognee library (version 0.1.24) using pip for knowledge graph creation and management.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/graphrag_vs_rag.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install cognee==0.1.24\n```\n\n----------------------------------------\n\nTITLE: Creating Relationship Edge between Person and Location Nodes in Knowledge Graph\nDESCRIPTION: Shows how to create a directional edge between a Person node and a Location node, using a descriptive, lowercase, snake_case relationship label.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/generate_graph_prompt_oneshot.txt#2025-04-07_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nMarie Curie (Person) ‚Äì born_in -> Warsaw (Location)\n```\n\n----------------------------------------\n\nTITLE: Querying Cognee for Document Summaries\nDESCRIPTION: Demonstrates how to search the knowledge graph for summaries of the main news discussed in the documents. Uses the SearchType.SUMMARIES query type to extract and print relevant information from the processed data.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_llama_index.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee import SearchType\n\n# Query cognee for summaries\nsearch_results = await cognee.search(\n    query_type=SearchType.SUMMARIES, query_text=\"What are the main news discussed in the document?\"\n)\n# Display search results\nprint(\"\\n Summary of main news discussed:\\n\")\nprint(search_results[0][\"text\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Summary Property to Person Node in Knowledge Graph\nDESCRIPTION: Illustrates how to add a summary property to a Person node, using plain strings without escaped quotes.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/generate_graph_prompt_oneshot.txt#2025-04-07_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nAlbert Einstein (Person)\n   summary: \"Developed the theory of relativity\"\n```\n\n----------------------------------------\n\nTITLE: Running Cognee with LlamaIndex Documents\nDESCRIPTION: Defines a function to ingest documents into Cognee, including database setup, user authentication, and data ingestion. After adding the documents, it runs the cognify process to create a knowledge graph from the ingested data.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_llama_index.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union, BinaryIO\n\nfrom cognee.infrastructure.databases.vector.pgvector import (\n    create_db_and_tables as create_pgvector_db_and_tables,\n)\nfrom cognee.infrastructure.databases.relational import (\n    create_db_and_tables as create_relational_db_and_tables,\n)\nfrom cognee.modules.users.models import User\nfrom cognee.modules.users.methods import get_default_user\nfrom cognee.tasks.ingestion.ingest_data import ingest_data\nimport cognee\n\n# Create a clean slate for cognee -- reset data and system state\nawait cognee.prune.prune_data()\nawait cognee.prune.prune_system(metadata=True)\n\n\n# Add the LlamaIndex documents, and make it available for cognify\nasync def add(\n    data: Union[BinaryIO, list[BinaryIO], str, list[str]],\n    dataset_name: str = \"main_dataset\",\n    user: User = None,\n):\n    await create_relational_db_and_tables()\n    await create_pgvector_db_and_tables()\n\n    if user is None:\n        user = await get_default_user()\n\n    await ingest_data(data, dataset_name, user)\n\n\nawait add(documents)\n\n# Use LLMs and cognee to create knowledge graph\nawait cognee.cognify()\n```\n\n----------------------------------------\n\nTITLE: Question-Answer Template Variables\nDESCRIPTION: Template structure containing placeholders for storing question-answer pairs and reference answers. Uses double curly braces for variable interpolation.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/direct_llm_eval_prompt.txt#2025-04-07_snippet_0\n\nLANGUAGE: template\nCODE:\n```\nQuestion: {{question}}\nProvided Answer: {{answer}}\nGolden Answer: {{golden_answer}}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Cognee Configuration\nDESCRIPTION: Configures the necessary environment variables for Cognee, including authentication credentials, database providers (graph, vector, and relational), and connection parameters. Uses NetworkX for graph database and LanceDB for vector storage by default.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_llama_index.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Setting environment variables\nif \"GRAPHISTRY_USERNAME\" not in os.environ:\n    os.environ[\"GRAPHISTRY_USERNAME\"] = \"\"\n\nif \"GRAPHISTRY_PASSWORD\" not in os.environ:\n    os.environ[\"GRAPHISTRY_PASSWORD\"] = \"\"\n\nif \"LLM_API_KEY\" not in os.environ:\n    os.environ[\"LLM_API_KEY\"] = \"\"\n\n# \"neo4j\" or \"networkx\"\nos.environ[\"GRAPH_DATABASE_PROVIDER\"] = \"networkx\"\n# Not needed if using networkx\n# os.environ[\"GRAPH_DATABASE_URL\"]=\"\"\n# os.environ[\"GRAPH_DATABASE_USERNAME\"]=\"\"\n# os.environ[\"GRAPH_DATABASE_PASSWORD\"]=\"\"\n\n# \"pgvector\", \"qdrant\", \"weaviate\" or \"lancedb\"\nos.environ[\"VECTOR_DB_PROVIDER\"] = \"lancedb\"\n# Not needed if using \"lancedb\" or \"pgvector\"\n# os.environ[\"VECTOR_DB_URL\"]=\"\"\n# os.environ[\"VECTOR_DB_KEY\"]=\"\"\n\n# Relational Database provider \"sqlite\" or \"postgres\"\nos.environ[\"DB_PROVIDER\"] = \"sqlite\"\n\n# Database name\nos.environ[\"DB_NAME\"] = \"cognee_db\"\n\n# Postgres specific parameters (Only if Postgres or PGVector is used)\n# os.environ[\"DB_HOST\"]=\"127.0.0.1\"\n# os.environ[\"DB_PORT\"]=\"5432\"\n# os.environ[\"DB_USERNAME\"]=\"cognee\"\n# os.environ[\"DB_PASSWORD\"]=\"cognee\"\n```\n\n----------------------------------------\n\nTITLE: Loading News Articles Dataset from GitHub\nDESCRIPTION: Loads a sample news article dataset from GitHub using pandas and converts it to LlamaIndex Document format. The dataset contains news articles with title and text fields, limited to 5 samples for this demonstration.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_llama_index.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom llama_index.core import Document\n\nnews = pd.read_csv(\n    \"https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/news_articles.csv\"\n)[:5]\n\nnews.head()\n```\n\n----------------------------------------\n\nTITLE: Installing LlamaIndex Core Package with pip\nDESCRIPTION: Installs the llama-index-core package using pip, which is required for document processing and RAG operations in this tutorial.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_llama_index.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install llama-index-core\n```\n\n----------------------------------------\n\nTITLE: Rendering the Generated Code Graph\nDESCRIPTION: Visualizes the code dependency graph that was generated from the analysis pipeline. This provides a graphical representation of the code structure and relationships.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_code_graph_demo.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom cognee.shared.utils import render_graph\n\nawait render_graph()\n```\n\n----------------------------------------\n\nTITLE: Cloning GraphRAG Repository for Analysis\nDESCRIPTION: Clones the Microsoft GraphRAG GitHub repository into a local directory for analysis. Removes any existing content in the target directory before cloning to ensure a clean state.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_code_graph_demo.ipynb#2025-04-07_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom os import path\nfrom pathlib import Path\nfrom cognee.infrastructure.files.storage import LocalStorage\nimport git\n\nnotebook_path = path.abspath(\"\")\nrepo_clone_location = path.join(notebook_path, \".data/graphrag\")\n\nLocalStorage.remove_all(repo_clone_location)\n\ngit.Repo.clone_from(\n    \"git@github.com:microsoft/graphrag.git\",\n    Path(repo_clone_location),\n    branch=\"main\",\n    single_branch=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Knowledge Graph with Cognee in Python\nDESCRIPTION: This code generates a visualization of the knowledge graph created by Cognee and starts a visualization server to display the results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/ontology_demo.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Generate and display the visualization\nhtml = await visualize_graph(destination_file_path= \"/Users/vasilije/cognee/.artifacts/graph.html\")\nfrom cognee.api.v1.visualize.start_visualization_server import start_visualization_server\n\nstart_visualization_server(port=8003)\n```\n\n----------------------------------------\n\nTITLE: Running Cognee Pipeline without Ontology and Querying Results in Python\nDESCRIPTION: This code demonstrates running the Cognee pipeline without an ontology, querying the system with the same medical questions, and printing the results for comparison.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/ontology_demo.ipynb#2025-04-07_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Run without ontology\nprint(\"\\n--- Results WITHOUT ontology ---\\n\")\nawait run_pipeline()\nanswers_without = await query_pipeline(questions)\nfor q, a in zip(questions, answers_without):\n    print(f\"Q: {q}\\nA: {a}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Running Cognee Pipeline with Ontology and Querying Results in Python\nDESCRIPTION: This code demonstrates running the Cognee pipeline with an ontology, querying the system with medical questions, and printing the results.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/ontology_demo.ipynb#2025-04-07_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Test questions\nquestions = [\n    \"What are common risk factors for Type 2 Diabetes?\",\n    \"What preventive measures reduce the risk of Hypertension?\",\n    \"What symptoms indicate possible Cardiovascular Disease?\",\n    \"What diseases are associated with Obesity?\"\n]\n\n# Path to medical ontology\nontology_path = \"examples/python/ontology_input_example/enriched_medical_ontology_with_classes.owl\"  # Update with your ontology path\n\n# Run with ontology\nprint(\"\\n--- Results WITH ontology ---\\n\")\nawait run_pipeline(ontology_path=ontology_path)\nanswers_with = await query_pipeline(questions)\nfor q, a in zip(questions, answers_with):\n    print(f\"Q: {q}\\nA: {a}\\n\")\n```\n\n----------------------------------------\n\nTITLE: Installing Cognee Package in Python\nDESCRIPTION: This code snippet shows how to install the Cognee package using pip. The comment is left uncommented for demonstration purposes.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/ontology_demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install required package\n# !pip install cognee\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Cognee\nDESCRIPTION: Sets up environment variables required for Cognee to function, including database configuration for graph database, vector database, and relational database. This example configures Cognee to use NetworkX for graph storage and LanceDB for vector storage.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_multimedia_demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# Setting environment variables\nif \"GRAPHISTRY_USERNAME\" not in os.environ:\n    os.environ[\"GRAPHISTRY_USERNAME\"] = \"\"\n\nif \"GRAPHISTRY_PASSWORD\" not in os.environ:\n    os.environ[\"GRAPHISTRY_PASSWORD\"] = \"\"\n\nif \"LLM_API_KEY\" not in os.environ:\n    os.environ[\"LLM_API_KEY\"] = \"\"\n\n# \"neo4j\" or \"networkx\"\nos.environ[\"GRAPH_DATABASE_PROVIDER\"] = \"networkx\"\n# Not needed if using networkx\n# os.environ[\"GRAPH_DATABASE_URL\"]=\"\"\n# os.environ[\"GRAPH_DATABASE_USERNAME\"]=\"\"\n# os.environ[\"GRAPH_DATABASE_PASSWORD\"]=\"\"\n\n# \"pgvector\", \"qdrant\", \"weaviate\" or \"lancedb\"\nos.environ[\"VECTOR_DB_PROVIDER\"] = \"lancedb\"\n# Not needed if using \"lancedb\" or \"pgvector\"\n# os.environ[\"VECTOR_DB_URL\"]=\"\"\n# os.environ[\"VECTOR_DB_KEY\"]=\"\"\n\n# Relational Database provider \"sqlite\" or \"postgres\"\nos.environ[\"DB_PROVIDER\"] = \"sqlite\"\n\n# Database name\nos.environ[\"DB_NAME\"] = \"cognee_db\"\n\n# Postgres specific parameters (Only if Postgres or PGVector is used)\n# os.environ[\"DB_HOST\"]=\"127.0.0.1\"\n# os.environ[\"DB_PORT\"]=\"5432\"\n# os.environ[\"DB_USERNAME\"]=\"cognee\"\n# os.environ[\"DB_PASSWORD\"]=\"cognee\"\n```\n\n----------------------------------------\n\nTITLE: Loading Multimedia File Paths in Python\nDESCRIPTION: Sets up file paths to sample multimedia files (MP3 and PNG) that will be used to create a knowledge graph in Cognee. The files are referenced via relative paths from the current directory.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_multimedia_demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport pathlib\n\n# cognee knowledge graph will be created based on the text\n# and description of these files\nmp3_file_path = os.path.join(\n    os.path.abspath(\"\"),\n    \"../\",\n    \".data/multimedia/text_to_speech.mp3\",\n)\npng_file_path = os.path.join(\n    os.path.abspath(\"\"),\n    \"../\",\n    \".data/multimedia/example.png\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Database and API Configuration\nDESCRIPTION: Configures environment variables for LLM and Graphiti API keys, and sets up Neo4j database connection parameters.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_graphiti_demo.ipynb#2025-04-07_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\n# We ignore warnigns for now\nwarnings.filterwarnings(\"ignore\")\n\n# API key for cognee\nif \"LLM_API_KEY\" not in os.environ:\n    os.environ[\"LLM_API_KEY\"] = \"\"\n\n# API key for graphiti\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = \"\"\n\nGRAPH_DATABASE_PROVIDER = \"neo4j\"\nGRAPH_DATABASE_USERNAME = \"neo4j\"\nGRAPH_DATABASE_PASSWORD = \"pleaseletmein\"\nGRAPH_DATABASE_URL = \"bolt://localhost:7687\"\n\nos.environ[\"GRAPH_DATABASE_PROVIDER\"] = GRAPH_DATABASE_PROVIDER\nos.environ[\"GRAPH_DATABASE_USERNAME\"] = GRAPH_DATABASE_USERNAME\nos.environ[\"GRAPH_DATABASE_PASSWORD\"] = GRAPH_DATABASE_PASSWORD\nos.environ[\"GRAPH_DATABASE_URL\"] = GRAPH_DATABASE_URL\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Cognee-Graphiti Integration\nDESCRIPTION: Imports necessary modules and dependencies for Cognee and Graphiti integration, including logging, pipeline, temporal awareness, and database utilities.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_graphiti_demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport cognee\nfrom cognee.shared.logging_utils import get_logger, ERROR\nimport warnings\nfrom cognee.modules.pipelines import Task, run_tasks\nfrom cognee.tasks.temporal_awareness import build_graph_with_temporal_awareness\nfrom cognee.infrastructure.databases.relational import (\n    create_db_and_tables as create_relational_db_and_tables,\n)\nfrom cognee.tasks.temporal_awareness.index_graphiti_objects import (\n    index_and_transform_graphiti_nodes_and_edges,\n)\nfrom cognee.modules.retrieval.utils.brute_force_triplet_search import brute_force_triplet_search\nfrom cognee.modules.retrieval.graph_completion_retriever import GraphCompletionRetriever\nfrom cognee.infrastructure.llm.prompts import read_query_prompt, render_prompt\nfrom cognee.infrastructure.llm.get_llm_client import get_llm_client\n```\n\n----------------------------------------\n\nTITLE: Loading Pok√©mon Abilities Data in Python\nDESCRIPTION: Defines a class and function to load Pok√©mon ability data from stored files. It uses the Cognee DataPoint class and UUID for data handling.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/pokemon_datapoints_notebook.ipynb#2025-04-07_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom cognee.low_level import DataPoint\nfrom uuid import uuid5, NAMESPACE_OID\n\nclass Abilities(DataPoint):\n    name: str = \"Abilities\"\n    metadata: dict = {\"index_fields\": [\"name\"]}\n\ndef load_abilities_data(jsonl_abilities):\n    abilities_root = Abilities()\n    pokemon_abilities = []\n\n    for jsonl_ability in jsonl_abilities:\n        with open(jsonl_ability, \"r\") as f:\n            for line in f:\n                ability = json.loads(line)\n                ability[\"id\"] = uuid5(NAMESPACE_OID, ability[\"_dlt_id\"])\n                ability[\"name\"] = ability[\"ability__name\"]\n                ability[\"is_type\"] = abilities_root\n                pokemon_abilities.append(ability)\n\n    return abilities_root, pokemon_abilities\n```\n\n----------------------------------------\n\nTITLE: Graphistry Integration\nDESCRIPTION: Sets up Graphistry integration for graph visualization and rendering\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom cognee.shared.utils import render_graph\nfrom cognee.infrastructure.databases.graph import get_graph_engine\nimport graphistry\n\n# from dotenv import load_dotenv\ngraphistry.login(\n    username=os.getenv(\"GRAPHISTRY_USERNAME\"), password=os.getenv(\"GRAPHISTRY_PASSWORD\")\n)\n\ngraph_engine = await get_graph_engine()\n\ngraph_url = await render_graph(graph_engine.graph)\nprint(graph_url)\n```\n\n----------------------------------------\n\nTITLE: Graph Display in Notebook\nDESCRIPTION: Displays the graph visualization in a Jupyter notebook using IFrame\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import IFrame, display, HTML\n\nIFrame(\"http://127.0.0.1:8002/.artifacts/graph_visualization.html\", width=800, height=600)\n```\n\n----------------------------------------\n\nTITLE: Defining Job Position Data in Python\nDESCRIPTION: Sample job posting data for a Senior Data Scientist position at TechNova Solutions including responsibilities and qualifications.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/cognee_demo.ipynb#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\njob_position = \"\"\"Senior Data Scientist (Machine Learning)\n\nCompany: TechNova Solutions\nLocation: San Francisco, CA\n\nJob Description:\n\nTechNova Solutions is seeking a Senior Data Scientist specializing in Machine Learning to join our dynamic analytics team. The ideal candidate will have a strong background in developing and deploying machine learning models, working with large datasets, and translating complex data into actionable insights.\n\nResponsibilities:\n\nDevelop and implement advanced machine learning algorithms and models.\nAnalyze large, complex datasets to extract meaningful patterns and insights.\nCollaborate with cross-functional teams to integrate predictive models into products.\nStay updated with the latest advancements in machine learning and data science.\nMentor junior data scientists and provide technical guidance.\nQualifications:\n\nMaster's or Ph.D. in Data Science, Computer Science, Statistics, or a related field.\n5+ years of experience in data science and machine learning.\nProficient in Python, R, and SQL.\nExperience with deep learning frameworks (e.g., TensorFlow, PyTorch).\nStrong problem-solving skills and attention to detail.\nCandidate CVs\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Template for Knowledge Graph Entity Extraction\nDESCRIPTION: A template that outlines the process for extracting entities from text to expand a knowledge graph. The template tracks extraction rounds and references previously extracted entities to ensure consistency in the knowledge graph building process.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/tasks/graph/cascade_extract/prompts/extract_graph_nodes_prompt_input.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nExtract distinct entities and concepts from the following text to expand the knowledge graph. Build upon previously extracted entities, ensuring completeness and consistency. This is round {{ round_number }} of {{ total_rounds }}.\n\n**Text:**\n{{ text }}\n\n**Previously Extracted Entities:**\n{{ previous_entities }}\n```\n\n----------------------------------------\n\nTITLE: Extracting Knowledge Graph Edges from Text in Markdown\nDESCRIPTION: This snippet outlines the process for extracting concrete edges from text using potential nodes and relationships. It emphasizes building upon previously extracted information and ensuring each connection is supported by the text content.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/tasks/graph/cascade_extract/prompts/extract_graph_edge_triplets_prompt_input.txt#2025-04-07_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nUsing provided potential nodes and relationships, extract concrete edges from the following text. Build upon previously extracted nodes and edges (if any), as this is round {{ round_number }} of {{ total_rounds }}.\n\n**Text:**\n{{ text }}\n\n**Potential Nodes to Use:**\n{{ potential_nodes }}\n\n**Potential Relationships to Use:**\n{{ potential_relationship_names }}\n\n**Previously Extracted Nodes:**\n{{ previous_nodes }}\n\n**Previously Extracted Edge Triplets:**\n{{ previous_edge_triplets }}\n\nCreate specific edge triplets between nodes, ensuring each connection is clearly supported by the text content. Use the potential nodes and relationships as your primary building blocks, while considering previously extracted nodes and edges for consistency and completeness.\n```\n\n----------------------------------------\n\nTITLE: Querying Connected Nodes in Neo4j Cypher\nDESCRIPTION: This Cypher query retrieves all nodes connected to an Entity node named 'John'. It demonstrates basic node matching and relationship traversal in Neo4j.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/natural_language_retriever_system.txt#2025-04-07_snippet_0\n\nLANGUAGE: cypher\nCODE:\n```\nMATCH (n:Entity {'name': 'John'})--(neighbor)\nRETURN n, neighbor\n```\n\n----------------------------------------\n\nTITLE: Defining Question-Context Template Format for Knowledge Graph Data\nDESCRIPTION: A template structure that formats a user question alongside context information from a knowledge graph. The context is formatted as a series of triplets (node1 -- relation -- node2) with each relationship separated by '\\n---\\n' delimiters.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/graph_context_for_question.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nThe question is: `{{ question }}`\nand here is the context provided with a set of relationships from a knowledge graph separated by \\n---\\n each represented as node1 -- relation -- node2 triplet: `{{ context }}`\n```\n\n----------------------------------------\n\nTITLE: Entity Extraction Prompt Template\nDESCRIPTION: A prompt template that instructs the extraction of key entities from a given text input. Uses double curly braces {{text}} as a placeholder for the input text to analyze.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/extract_entities_user.txt#2025-04-07_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nExtract key entities from this text:\n\n{{ text }}\n```\n\n----------------------------------------\n\nTITLE: Defining Question-Context Template Variables\nDESCRIPTION: A template that defines placeholder variables for a question and its context. The variables are marked with double curly braces for replacement during template processing.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/context_for_question.txt#2025-04-07_snippet_0\n\nLANGUAGE: template\nCODE:\n```\nThe question is: `{{ question }}`\nAnd here is the context: `{{ context }}`\n```\n\n----------------------------------------\n\nTITLE: Print Statement Example in Python\nDESCRIPTION: Simple print statement example showing basic string output in Python. Used to demonstrate code extraction from text.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee/infrastructure/llm/prompts/codegraph_retriever_system.txt#2025-04-07_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nprint('Hello World')\n```\n\n----------------------------------------\n\nTITLE: Opening Inspector with Extended Timeout\nDESCRIPTION: URL for opening the inspector interface with an extended timeout parameter for debugging.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-mcp/README.md#2025-04-07_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nhttp://localhost:5173?timeout=120000\n```\n\n----------------------------------------\n\nTITLE: Running MCP Server in Development Mode\nDESCRIPTION: Starts the MCP server in development mode for debugging purposes.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-mcp/README.md#2025-04-07_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nmcp dev src/server.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude Desktop for Cognee MCP Server\nDESCRIPTION: JSON configuration for the Claude Desktop to use Cognee MCP server. Includes command path, arguments, and environment variables including the LLM API key.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-mcp/README.md#2025-04-07_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"mcpServers\": {\n\t\t\"cognee\": {\n\t\t\t\"command\": \"/Users/{user}/cognee/.venv/bin/uv\",\n\t\t\t\"args\": [\n        \"--directory\",\n        \"/Users/{user}/cognee/cognee-mcp\",\n        \"run\",\n        \"cognee\"\n      ],\n      \"env\": {\n        \"ENV\": \"local\",\n        \"TOKENIZERS_PARALLELISM\": \"false\",\n        \"LLM_API_KEY\": \"sk-\"\n      }\n\t\t}\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Opening Configuration File with Nano Editor\nDESCRIPTION: Opens the Claude Desktop configuration file with the Nano text editor for modification.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-mcp/README.md#2025-04-07_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnano claude_desktop_config.json\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Dependencies with UV\nDESCRIPTION: Synchronizes all project dependencies using UV package manager with development extras and forces reinstallation of packages.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-mcp/README.md#2025-04-07_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\ncd cognee-mcp\nuv sync --dev --all-extras --reinstall\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Cognee Helm Release\nDESCRIPTION: Command to remove the Cognee Helm release and all associated Kubernetes resources from the cluster.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/helm/README.md#2025-04-07_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm uninstall cognee\n```\n\n----------------------------------------\n\nTITLE: Running Next.js Development Server\nDESCRIPTION: Commands to start the Next.js development server using different package managers. This allows developers to run the project locally for development and testing.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/cognee-frontend/README.md#2025-04-07_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\n----------------------------------------\n\nTITLE: Commenting External Software Attribution in HTML\nDESCRIPTION: This HTML comment provides a template for attributing external software sources used in the project. It demonstrates how to properly format the attribution for redistributed code.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/NOTICE.md#2025-04-07_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<!-- Add software from external sources that you used here. e.g.\n\nThis project redistributes code originally from <website>.\n\n -->\n```\n\n----------------------------------------\n\nTITLE: Configuring Git for DCO\nDESCRIPTION: Setting up Git alias for signed commits to comply with Developer Certificate of Origin\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/CONTRIBUTING.md#2025-04-07_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit config alias.cos \"commit -s\"\n```\n\n----------------------------------------\n\nTITLE: Submitting Changes to Cognee\nDESCRIPTION: Git commands for committing and pushing changes to the repository\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/CONTRIBUTING.md#2025-04-07_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit add .\\ngit commit -s -m \"Description of your changes\"\\ngit push origin feature/your-feature-name\n```\n\n----------------------------------------\n\nTITLE: Running Cognee Tests\nDESCRIPTION: Command to execute the project's test suite\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/CONTRIBUTING.md#2025-04-07_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython cognee/cognee/tests/test_library.py\n```\n\n----------------------------------------\n\nTITLE: Creating Feature Branch\nDESCRIPTION: Command to create a new branch for feature development\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/CONTRIBUTING.md#2025-04-07_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout -b feature/your-feature-name\n```\n\n----------------------------------------\n\nTITLE: Cloning Cognee Repository\nDESCRIPTION: Instructions for forking and cloning the Cognee repository locally\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/CONTRIBUTING.md#2025-04-07_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/<your-github-username>/cognee.git\\ncd cognee\n```\n\n----------------------------------------\n\nTITLE: Encoded Base64 Image Data\nDESCRIPTION: Contains what appears to be a Base64 encoded image file, likely in JPEG format based on the header signature. The actual image contents are obfuscated.\nSOURCE: https://github.com/topoteretes/cognee/blob/dev/notebooks/llama_index_cognee_integration.ipynb#2025-04-07_snippet_9\n\nLANGUAGE: base64\nCODE:\n```\nAhPpR3o60dqBhj0paOtFAhPpR3o61FcXEFrA1zdO... [truncated for brevity]\n```"
  }
]
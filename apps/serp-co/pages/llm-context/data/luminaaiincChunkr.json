[
  {
    "owner": "lumina-ai-inc",
    "repo": "chunkr",
    "content": "TITLE: Using Chunkr SDK to Process Documents in Python\nDESCRIPTION: Sample Python code demonstrating how to use the Chunkr SDK to upload a document, process it, and export the results in various formats. Shows initialization with API key, document uploading, and exporting to HTML, markdown, text, and JSON.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom chunkr_ai import Chunkr\n\n# Initialize with your API key from chunkr.ai\nchunkr = Chunkr(api_key=\"your_api_key\")\n\n# Upload a document (URL or local file path)\nurl = \"https://chunkr-web.s3.us-east-1.amazonaws.com/landing_page/input/science.pdf\"\ntask = chunkr.upload(url)\n\n# Export results in various formats\nhtml = task.html(output_file=\"output.html\")\nmarkdown = task.markdown(output_file=\"output.md\")\ncontent = task.content(output_file=\"output.txt\")\ntask.json(output_file=\"output.json\")\n\n# Clean up\nchunkr.close()\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Usage of Chunkr Client\nDESCRIPTION: Shows how to use the Chunkr client in an asynchronous context with asyncio, including proper resource management with try/finally and async functions for uploading and polling.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom chunkr_ai import Chunkr\nimport asyncio\n\nasync def process_document():\n    # Initialize client\n    chunkr = Chunkr()\n\n    try:\n        # Upload a file and wait for processing\n        task = await chunkr.upload(\"document.pdf\")\n        print(task.task_id)\n\n        # Create task without waiting\n        task = await chunkr.create_task(\"document.pdf\")\n        result = await task.poll()  # Check status when needed\n    finally:\n        await chunkr.close()\n\n# Run the async function\nasyncio.run(process_document())\n```\n\n----------------------------------------\n\nTITLE: Quick Start Guide for Chunkr AI SDK in TypeScript\nDESCRIPTION: This snippet demonstrates how to initialize the Chunkr client with an API key, upload a document for processing, and access the processed content in various formats.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/node-client/README.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Chunkr } from \"chunkr-ai\";\n\n// Initialize client with API key\nconst chunkr = new Chunkr(\"your-api-key\");\n\n// Process a document\nconst task = await chunkr.upload(\"path/to/document.pdf\");\n\n// Access the processed content\nconsole.log(task.getHtml()); // Get HTML output\nconsole.log(task.getMarkdown()); // Get Markdown output\nconsole.log(task.output); // Get full structured output\n```\n\n----------------------------------------\n\nTITLE: Concurrent Processing with Chunkr\nDESCRIPTION: Demonstrates two approaches for concurrent document processing: using asyncio gather for asynchronous concurrency and using multiprocessing Pool for parallel processing across multiple CPU cores.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Async concurrency\nasync def process_multiple():\n    chunkr = Chunkr()\n    try:\n        tasks = [\n            chunkr.upload(\"doc1.pdf\"),\n            chunkr.upload(\"doc2.pdf\"),\n            chunkr.upload(\"doc3.pdf\")\n        ]\n        results = await asyncio.gather(*tasks)\n    finally:\n        await chunkr.close()\n\n# Multiprocessing\nfrom multiprocessing import Pool\n\ndef process_file(path):\n    chunkr = Chunkr()\n    try:\n        return chunkr.upload(path)\n    finally:\n        chunkr.close()\n\nwith Pool(processes=3) as pool:\n    results = pool.map(process_file, [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"])\n```\n\n----------------------------------------\n\nTITLE: Authentication Methods for Chunkr AI SDK in TypeScript\nDESCRIPTION: This snippet shows three different ways to authenticate with the Chunkr API: direct initialization with an API key, using an environment variable, and using a configuration object. It also demonstrates how to set a custom API URL for version control.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/node-client/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// 1. Direct initialization\nconst chunkr = new Chunkr(\"your-api-key\");\n// 2. Environment variable\n// Set CHUNKR_API_KEY in your environment or .env file\nconst chunkr = new Chunkr();\n// 3. Configuration object\nconst chunkr = new Chunkr({\n  apiKey: \"your-api-key\",\n  baseUrl: \"https://api.chunkr.ai\", // Optional custom API URL for version control\n});\n```\n\n----------------------------------------\n\nTITLE: Synchronous Usage of Chunkr Client\nDESCRIPTION: Demonstrates the synchronous usage pattern for the Chunkr client, including initializing the client, uploading a file, creating a task without waiting, and properly closing the client.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom chunkr_ai import Chunkr\n\n# Initialize client\nchunkr = Chunkr()\n\n# Upload a file and wait for processing\ntask = chunkr.upload(\"document.pdf\")\nprint(task.task_id)\n\n# Create task without waiting\ntask = chunkr.create_task(\"document.pdf\")\nresult = task.poll()  # Check status when needed\n\n# Clean up when done\nchunkr.close()\n```\n\n----------------------------------------\n\nTITLE: Configuring Chunkr Processing Options\nDESCRIPTION: Shows how to customize processing behavior by passing a Configuration object with specific processing strategies and options for both synchronous and asynchronous contexts.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom chunkr_ai.models import (\n    Configuration, \n    OcrStrategy, \n    SegmentationStrategy, \n    GenerationStrategy\n)\n\nconfig = Configuration(\n    ocr_strategy=OcrStrategy.AUTO,\n    segmentation_strategy=SegmentationStrategy.LAYOUT_ANALYSIS,\n    high_resolution=True,\n    expires_in=3600,  # seconds\n)\n\n# Works in both sync and async contexts\ntask = chunkr.upload(\"document.pdf\", config)  # sync\ntask = await chunkr.upload(\"document.pdf\", config)  # async\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple LLM Models in YAML\nDESCRIPTION: Example YAML configuration for setting up language models in Chunkr. Shows how to configure a GPT-4o model as the default with API key, provider URL, and rate limiting settings.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - id: gpt-4o\n    model: gpt-4o\n    provider_url: https://api.openai.com/v1/chat/completions\n    api_key: \"your_openai_api_key_here\"\n    default: true\n    rate-limit: 200 # requests per minute - optional\n```\n\n----------------------------------------\n\nTITLE: Installing Chunkr Python SDK\nDESCRIPTION: Command to install the Chunkr Python SDK via pip. This is the first step to interact with the Chunkr API programmatically.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install chunkr-ai\n```\n\n----------------------------------------\n\nTITLE: Resource Management with Chunkr Client\nDESCRIPTION: Demonstrates proper resource management patterns for both synchronous and asynchronous contexts using try/finally blocks to ensure the client is properly closed.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Sync context\nchunkr = Chunkr()\ntry:\n    result = chunkr.upload(\"document.pdf\")\nfinally:\n    chunkr.close()\n\n# Async context\nasync def process():\n    chunkr = Chunkr()\n    try:\n        result = await chunkr.upload(\"document.pdf\")\n    finally:\n        await chunkr.close()\n```\n\n----------------------------------------\n\nTITLE: Cloning the Chunkr Repository\nDESCRIPTION: Git commands to clone the Chunkr repository from GitHub and navigate to the project directory. This is the first step for self-hosting the service.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/lumina-ai-inc/chunkr\ncd chunkr\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Chunkr\nDESCRIPTION: Commands to copy the example environment and model configuration files to set up Chunkr. These files contain necessary configuration for running the service.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Copy the example environment file\ncp .env.example .env\n\n# Configure your llm models\ncp models.example.yaml models.yaml\n```\n\n----------------------------------------\n\nTITLE: Initializing Chunkr Client with API Credentials\nDESCRIPTION: Shows how to directly initialize the Chunkr client with API key and URL instead of using environment variables.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nchunkr = Chunkr(\n    api_key=\"your-api-key\",\n    url=\"https://api.chunkr.ai\"\n)\n```\n\n----------------------------------------\n\nTITLE: Starting Chunkr Services with Docker Compose\nDESCRIPTION: Docker Compose commands to start Chunkr services in different deployment modes. Includes options for both GPU and CPU deployments.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# For GPU deployment, use the following command:\ndocker compose up -d\n\n# For CPU deployment, use the following command:\ndocker compose -f compose-cpu.yaml up -d\n```\n\n----------------------------------------\n\nTITLE: Stopping Chunkr Services with Docker Compose\nDESCRIPTION: Docker Compose commands to stop Chunkr services. Includes options for both GPU and CPU deployments.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# For GPU deployment, use the following command:\ndocker compose down\n\n# For CPU deployment, use the following command:\ndocker compose -f compose-cpu.yaml down\n```\n\n----------------------------------------\n\nTITLE: Configuring Models for Chunkr Kubernetes Deployment\nDESCRIPTION: This code snippet shows how to set up the models.yaml file for Chunkr, including copying the example file, editing it, and creating a Kubernetes ConfigMap from it.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Copy the example models.yaml\ncp ../models.yaml.example secrets/local/models.yaml\n\n# Edit the models.yaml file with your values\nvim secrets/local/models.yaml\n\n# Create the llm configmap\nkubectl create configmap llm-models-configmap --from-file=models.yaml=./secrets/local/models.yaml -n chunkr\n```\n\n----------------------------------------\n\nTITLE: Installing Chunkr with Helm and Cloudflare Tunnel\nDESCRIPTION: This Helm command installs Chunkr on Kubernetes with custom domain configuration using Cloudflare Tunnel. It sets various parameters including subdomains, ingress type, and storage class.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nhelm install chunkr ./charts/chunkr \\\n  -f ./charts/chunkr/values.yaml \\\n  -f ./charts/chunkr/infrastructure.yaml \\\n  --namespace chunkr \\\n  --create-namespace \\\n  --set ingress.subdomains.root=false \\\n  --set \"services.web.ingress.subdomain=chunkr\" \\\n  --set \"services.server.ingress.subdomain=chunkr-api\" \\\n  --set \"services.keycloak.ingress.subdomain=chunkr-auth\" \\\n  --set \"services.minio.ingress.subdomain=chunkr-s3\" \\\n  --set ingress.type=cloudflare \\\n  --set cloudflared.enabled=true \\\n  --set cloudflared.config.tunnelName=YOUR_TUNNEL_NAME \\\n  --set global.storageClass=standard\n```\n\n----------------------------------------\n\nTITLE: Updating Chunkr Kubernetes Deployment with Helm\nDESCRIPTION: This Helm command upgrades the existing Chunkr deployment on Kubernetes, allowing for updates to configuration and settings.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nhelm upgrade chunkr ./charts/chunkr \\\n  -f ./charts/chunkr/values.yaml \\\n  -f ./charts/chunkr/infrastructure.yaml \\\n  --namespace chunkr \\\n  --set ingress.subdomains.root=false \\\n  --set \"services.web.ingress.subdomain=chunkr\" \\\n  --set \"services.server.ingress.subdomain=chunkr-api\" \\\n  --set \"services.keycloak.ingress.subdomain=chunkr-auth\" \\\n  --set \"services.minio.ingress.subdomain=chunkr-s3\" \\\n  --set ingress.type=cloudflare \\\n  --set cloudflared.enabled=true \\\n  --set cloudflared.config.tunnelName=YOUR_TUNNEL_NAME \\\n  --set global.storageClass=standard\n```\n\n----------------------------------------\n\nTITLE: Supporting Various Input Types in Chunkr\nDESCRIPTION: Shows different input types supported by the Chunkr client: file paths, file objects, and PIL Images.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# File path\nchunkr.upload(\"document.pdf\")\n\n# Opened file\nwith open(\"document.pdf\", \"rb\") as f:\n    chunkr.upload(f)\n\n# PIL Image\nfrom PIL import Image\nimg = Image.open(\"photo.jpg\")\nchunkr.upload(img)\n```\n\n----------------------------------------\n\nTITLE: Configuring External S3 Provider for Chunkr\nDESCRIPTION: This snippet shows how to configure an external S3 provider for Chunkr by updating the secret file and disabling the default MinIO service using Helm.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Update the chunkr-secret.yaml file with the credentials for the external S3 provider\nAWS__ACCESS_KEY=\nAWS__SECRET_KEY=\nAWS__ENDPOINT=\n\n# Disable MinIO\nhelm upgrade chunkr ./charts/chunkr \\\n  -f ./charts/chunkr/values.yaml \\\n  -f ./charts/chunkr/infrastructure.yaml \\\n  --namespace chunkr \\\n  --set services.minio.enabled=false\n```\n\n----------------------------------------\n\nTITLE: Configuring External Redis for Chunkr\nDESCRIPTION: This code demonstrates how to configure an external Redis instance for Chunkr by updating the secret file and disabling the default Redis service using Helm.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# Update the chunkr-secret.yaml file with the credentials for the external Redis instance\nREDIS__URL=\n\n# Disable Redis\nhelm upgrade chunkr ./charts/chunkr \\\n  -f ./charts/chunkr/values.yaml \\\n  -f ./charts/chunkr/infrastructure.yaml \\\n  --namespace chunkr \\\n  --set services.redis.enabled=false \\\n```\n\n----------------------------------------\n\nTITLE: Configuring JSON Schema for Structured Output in Chunkr\nDESCRIPTION: Example of defining a JSON schema with properties for structured data extraction from documents.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nconfig = Configuration(json_schema=JsonSchema(\n    title=\"Sales Data\",\n    properties=[\n        Property(name=\"Person with highest sales\", prop_type=\"string\", description=\"The person with the highest sales\"),\n        Property(name=\"Person with lowest sales\", prop_type=\"string\", description=\"The person with the lowest sales\"),\n    ]\n))\n```\n\n----------------------------------------\n\nTITLE: Enabling and Configuring Postgres for Chunkr\nDESCRIPTION: These snippets show how to enable and configure Postgres for Chunkr, including enabling the service with Helm and updating the secret file with external Postgres credentials.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# Enable Postgres\nhelm upgrade chunkr ./charts/chunkr \\\n  -f ./charts/chunkr/values.yaml \\\n  -f ./charts/chunkr/infrastructure.yaml \\\n  --namespace chunkr \\\n  --set services.postgres.enabled=true \\\n  --set services.postgres.credentials.username={YOUR_USERNAME} \\\n  --set services.postgres.credentials.password={YOUR_PASSWORD}\n\n# Update the chunkr-secret.yaml file with the credentials for the external Postgres instance\nPG__URL=\n```\n\n----------------------------------------\n\nTITLE: Configuring Chunk Processing in Chunkr\nDESCRIPTION: Example of configuring chunk processing with a target length parameter.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom chunkr_ai.models import ChunkProcessing\nconfig = Configuration(\n    chunk_processing=ChunkProcessing(target_length=1024)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Segment Processing in Chunkr\nDESCRIPTION: Example of configuring segment processing options with generation strategies for HTML and Markdown output.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom chunkr_ai.models import SegmentProcessing, GenerationConfig, GenerationStrategy\nconfig = Configuration(\n    segment_processing=SegmentProcessing(\n        page=GenerationConfig(\n            html=GenerationStrategy.LLM,\n            markdown=GenerationStrategy.LLM\n        )\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling High Resolution Processing in Chunkr\nDESCRIPTION: Example of enabling high resolution processing for documents.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconfig = Configuration(high_resolution=True)\n```\n\n----------------------------------------\n\nTITLE: Setting OCR Strategy in Chunkr\nDESCRIPTION: Example of setting the OCR (Optical Character Recognition) strategy for document processing.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconfig = Configuration(ocr_strategy=OcrStrategy.AUTO)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Secrets for Chunkr Kubernetes Deployment\nDESCRIPTION: This snippet demonstrates how to set up and apply secrets for the Chunkr deployment in Kubernetes. It includes creating a secrets directory, copying example secrets, and applying them to the cluster.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Create a secrets directory \nmkdir -p secrets/local\n\n# Copy the example secrets\ncp secrets/chunkr-secret.example.yaml secrets/local/chunkr-secret.yaml\n\n# Edit each secret file with your values\nvim secrets/local/chunkr-secret.yaml  \n\n# Apply secrets\nkubectl apply -f secrets/local/ -n chunkr\n```\n\n----------------------------------------\n\nTITLE: Setting Segmentation Strategy in Chunkr\nDESCRIPTION: Example of setting the segmentation strategy for document processing, with options for layout analysis or page-based segmentation.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nconfig = Configuration(\n    segmentation_strategy=SegmentationStrategy.LAYOUT_ANALYSIS  # or SegmentationStrategy.PAGE\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Rust Development Environment\nDESCRIPTION: Instructions for installing Rust and Cargo, setting up the development environment, and configuring environment variables for the Chunkr API.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/core/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install Rust and Cargo\ncurl https://sh.rustup.rs -sSf | sh\n\n# Source the cargo environment\nsource $HOME/.cargo/env\n\n# Copy the example env file\ncp .env.example .env\n```\n\n----------------------------------------\n\nTITLE: Environment Variables for Basic LLM Configuration\nDESCRIPTION: Example environment variables for basic LLM configuration in Chunkr. Shows the three essential variables needed to set up a language model API connection.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nLLM__KEY:\nLLM__MODEL:\nLLM__URL:\n```\n\n----------------------------------------\n\nTITLE: Running Chunkr Rust Services\nDESCRIPTION: Commands for starting the Chunkr server and worker tasks using Cargo.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/core/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Start the server\ncargo run\n\n# Start the workers\n## Each worker is 1 task\ncargo run --bin task\n```\n\n----------------------------------------\n\nTITLE: Generating Self-Signed Certificate for HTTPS Setup\nDESCRIPTION: Commands to generate a self-signed SSL certificate for HTTPS setup with Chunkr. Creates a certificate valid for a year with proper subject alternative names for localhost access.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Create a certs directory\nmkdir certs\n\n# Generate the certificate\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout certs/nginx.key -out certs/nginx.crt -subj \"/CN=localhost\" -addext \"subjectAltName=DNS:localhost,IP:127.0.0.1\"\n```\n\n----------------------------------------\n\nTITLE: Copying Models Configuration Template\nDESCRIPTION: Command to copy the example models configuration file for customizing LLM settings. This is the recommended approach for configuring multiple language models in Chunkr.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncp models.example.yaml models.yaml\n```\n\n----------------------------------------\n\nTITLE: Starting Chunkr Services with HTTPS Proxy\nDESCRIPTION: Docker Compose commands to start Chunkr services with HTTPS proxy enabled. Includes options for both GPU and CPU deployments using the proxy profile.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# For GPU deployment, use the following command:\ndocker compose --profile proxy up -d\n\n# For CPU deployment, use the following command:\ndocker compose -f compose-cpu.yaml --profile proxy up -d\n```\n\n----------------------------------------\n\nTITLE: Stopping Chunkr Services with HTTPS Proxy\nDESCRIPTION: Docker Compose commands to stop Chunkr services with HTTPS proxy enabled. Includes options for both GPU and CPU deployments using the proxy profile.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# For GPU deployment, use the following command:\ndocker compose --profile proxy down\n\n# For CPU deployment, use the following command:\ndocker compose -f compose-cpu.yaml --profile proxy down\n```\n\n----------------------------------------\n\nTITLE: Setting Document Expiration Time in Chunkr\nDESCRIPTION: Example of setting an expiration time for documents in seconds.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconfig = Configuration(expires_in=3600)\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA GPU Operator with Time-Slicing in Kubernetes\nDESCRIPTION: This snippet shows how to install the NVIDIA GPU Operator with time-slicing configuration for Kubernetes distributions other than GKE. It adds the NVIDIA Helm repository, installs the GPU Operator, and applies time-slicing configuration.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Add the NVIDIA Helm repository\nhelm repo add nvidia https://helm.ngc.nvidia.com/nvidia \\\n  && helm repo update\n\n# Install the GPU Operator\nhelm install --wait --generate-name \\\n  -n gpu-operator --create-namespace \\\n  nvidia/gpu-operator \\\n  --version=v24.9.1\n\nkubectl create -f time-slicing-config-all.yaml -n gpu-operator\n\nkubectl patch clusterpolicy/cluster-policy \\\n  -n gpu-operator \\\n  --type merge \\\n  -p '{\"spec\": {\"devicePlugin\": {\"config\": {\"name\": \"time-slicing-config-all\", \"default\": \"any\"}}}}}'\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Namespace for Chunkr\nDESCRIPTION: This command creates a new Kubernetes namespace called 'chunkr' for deploying the Chunkr application.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create namespace chunkr\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for VGT Model\nDESCRIPTION: Command sequence for installing necessary dependencies including PyTorch, torchvision, and git-lfs for handling large files in the repository.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n# Install `git lfs`\ncurl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\nsudo apt-get install git-lfs\n```\n\n----------------------------------------\n\nTITLE: Setting Storage Class for Chunkr Kubernetes Deployment\nDESCRIPTION: These Helm commands demonstrate how to set the appropriate storage class for different cloud providers (GCP, AWS, Azure) when installing or upgrading Chunkr.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# For GCP (default)\nhelm install chunkr ./charts/chunkr \\\n  -f ./charts/chunkr/values.yaml \\\n  -f ./charts/chunkr/infrastructure.yaml \\\n  --namespace chunkr \\\n  --set global.storageClass=standard\n\n# For AWS\nhelm install chunkr ./charts/chunkr \\\n  -f ./charts/chunkr/values.yaml \\\n  -f ./charts/chunkr/infrastructure.yaml \\\n  --namespace chunkr \\\n  --set global.storageClass=gp2\n\n# For Azure\nhelm install chunkr ./charts/chunkr \\\n  -f ./charts/chunkr/values.yaml \\\n  -f ./charts/chunkr/infrastructure.yaml \\\n  --namespace chunkr \\\n  --set global.storageClass=managed-premium\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning VGT on PublayNet Dataset\nDESCRIPTION: Command to fine-tune the VGT model on the PublayNet dataset using a pre-trained model, requiring 8 GPUs for training.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython train_VGT.py --config-file Configs/cascade/publaynet_VGT_cascade_PTM.yaml --num-gpus 8 MODEL.WEIGHTS <VGT-pretrain-model_file_path> OUTPUT_DIR <your_output_dir>\n```\n\n----------------------------------------\n\nTITLE: Installing Detectron2 for Object Detection\nDESCRIPTION: Command to install Detectron2 library version 0.6 with CUDA 10.2 and PyTorch 1.9 support, required for object detection tasks in VGT.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install `detectron2`\npython -m pip install detectron2==0.6 -f \\\n    https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning VGT on D4LA Dataset\nDESCRIPTION: Command to fine-tune the VGT model on the D4LA dataset using a pre-trained model and D4LA-specific configuration file.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython train_VGT.py --config-file Configs/cascade/D4LA_VGT_cascade_PTM.yaml --num-gpus 8 MODEL.WEIGHTS <VGT-pretrain-model_file_path> OUTPUT_DIR <your_output_dir>\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning VGT on DocLayNet Dataset\nDESCRIPTION: Command to fine-tune the VGT model on the DocLayNet dataset using a pre-trained model and a specific configuration for DocLayNet.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython train_VGT.py --config-file Configs/cascade/doclaynet_VGT_cascade_PTM.yaml --num-gpus 8 MODEL.WEIGHTS <VGT-pretrain-model_file_path> OUTPUT_DIR <your_output_dir>\n```\n\n----------------------------------------\n\nTITLE: Installing Chunkr Python Client with pip\nDESCRIPTION: Command to install the Chunkr Python client package using pip.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install chunkr-ai\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Apex for Mixed-Precision Training\nDESCRIPTION: Commands to install NVIDIA Apex, which enables mixed-precision training for faster and memory-efficient model training.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n```\n\n----------------------------------------\n\nTITLE: Converting PDF to Images for VGT Processing\nDESCRIPTION: Script command to convert PDF files to images (PNG format) for preprocessing before using with the VGT model.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython pdf2img.py \\\n--pdf 'input-pdf-path' \\\n--output 'output-folder-path' \\\n--format 'png'\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Docker Container\nDESCRIPTION: Initializes a Docker container with PyTorch 2.3.1, CUDA 11.8, and CUDNN8 support. Maps port 8001 to 8000 and mounts necessary volumes.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker run -it --gpus all -p 8001:8000\\\n    --name vgt \\\n    -v $PWD:/app \\\n    -v ~/.bash_history_vgt:/root/.bash_history \\\n    pytorch/pytorch:2.3.1-cuda11.8-cudnn8-runtime bash\n```\n\n----------------------------------------\n\nTITLE: Generating Grid Information for Machine-Readable PDFs\nDESCRIPTION: Command to create grid information files (.pkl) for PDFs containing the structured data needed by Grid Transformer, with options for tokenizer and model selection.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython create_grid_input.py \\\n--pdf 'path-to-pdf-file' \\\n--output 'path-to-output-folder' \\\n--tokenizer 'google-bert/bert-base-uncased' \\\n--model 'doclaynet'\n```\n\n----------------------------------------\n\nTITLE: Running Inference with VGT Model on DocBank Dataset\nDESCRIPTION: Command to perform inference on a specific image using the VGT model trained on DocBank dataset, leveraging grid information for document layout analysis.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython inference.py \\\n--image_root '/DocBank_root_path/DocBank/DocBank_500K_ori_img/' \\\n--grid_root '/DocBank_root_path/DocBank/VGT_docbank_grid_pkl/' \\\n--image_name '1.tar_1401.0001.gz_infoingames_without_metric_arxiv_47_ori' \\\n--dataset docbank \\\n--output_root <your_output_dir> / \\\n--config Configs/cascade/docbank_VGT_cascade_PTM.yaml \\\n--opts MODEL.WEIGHTS  <finetuned_checkpoint_file_path>\n```\n\n----------------------------------------\n\nTITLE: Setting Up OCR Service Inside Doctr Container in Bash\nDESCRIPTION: Commands to be run inside the Docker container to set up the OCR service. Installs Rust and Cargo via uv, sources the cargo environment, and runs the main Python script for OCR processing.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/doctr/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install Rust and Cargo\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Source the cargo environment\nsource $HOME/.local/bin/env\n\ncd /doctr\n\n# For General OCR processing\nuv run python main.py\n```\n\n----------------------------------------\n\nTITLE: Creating a Doctr Container History File in Bash\nDESCRIPTION: Creates a bash history file that will be mounted to the Docker container to persist command history between container sessions.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/doctr/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntouch ~/.bash_history_doctr\n```\n\n----------------------------------------\n\nTITLE: Creating and Running Doctr Container in Bash\nDESCRIPTION: Creates and runs a Docker container for Doctr with GPU support, port mapping, and volume mounts for project code and command history. Uses the official Doctr image with PyTorch and GPU support.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/doctr/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker run -it --gpus all -p 8002:8000\\\n    --name doctr \\\n    -v $PWD:/doctr \\\n    -v ~/.bash_history_doctr:/root/.bash_history \\\n    ghcr.io/mindee/doctr:torch-py3.9.18-gpu-2023-09 bash\n```\n\n----------------------------------------\n\nTITLE: Starting an Existing Doctr Container in Bash\nDESCRIPTION: Command to start an existing Doctr Docker container in interactive mode, allowing interaction with the container's shell.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/doctr/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker start -i doctr  \n```\n\n----------------------------------------\n\nTITLE: Stopping and Cleaning Up Doctr Container in Bash\nDESCRIPTION: Commands to properly stop and optionally remove the Doctr Docker container. Shows how to exit the container, stop it from outside, and optionally remove the container and its volumes.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/doctr/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Exit the container\nexit\n\n# Stop container (run this after exiting)\nsudo docker stop doctr\n\n# Optional: Remove the container and volume (run this after exiting)\nsudo docker rm -v doctr\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Setting Up Environment\nDESCRIPTION: Installs system dependencies, creates directories, sets up Python virtual environment, and installs required Python packages including Detectron2.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ../app\n\napt-get update && apt-get install -y -q --no-install-recommends \\\n    libgomp1 ffmpeg libsm6 libxext6 git ninja-build g++ || true\napt-get update --fix-missing\n\nmkdir -p object_detection \\\n    && mkdir -p object_detection/weights\n    && mkdir -p object_detection/configs\n\npython -m venv .venv\nsource .venv/bin/activate\n\npip install --upgrade pip\npip install huggingface_hub==0.24.3\npip install wheel setuptools\npip install torch torchvision torchaudio\npip install --no-build-isolation 'git+https://github.com/facebookresearch/detectron2.git@70f454304e1a38378200459dd2dbca0f0f4a5ab4'\n\npip --default-timeout=1000 install -r requirements.txt\npython download_models.py\n\n#entrypoint\n\npython object_detection/server.py\n```\n\n----------------------------------------\n\nTITLE: Container Cleanup Commands\nDESCRIPTION: Commands for stopping and optionally removing the VGT container and its associated volume.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Exit the container\nexit\n\n# Stop container (run this after exiting)\nsudo docker stop vgt\n\n# Optional: Remove the container and volume (run this after exiting)\nsudo docker rm -v vgt\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Chunkr from Kubernetes\nDESCRIPTION: This command uninstalls the Chunkr deployment from the Kubernetes cluster using Helm.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/kube/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nhelm uninstall chunkr --namespace chunkr\n```\n\n----------------------------------------\n\nTITLE: Installing Chunkr AI SDK via npm\nDESCRIPTION: This command installs the Chunkr AI Node.js SDK using npm package manager.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/node-client/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install chunkr-ai\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Python uv for Chunkr\nDESCRIPTION: Two-step process for running tests on the Chunkr project. First installs the project with test dependencies in development mode, then executes the test suite using pytest through the uv runner.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/clients/python-client/tests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Install dependencies\nuv pip install -e \".[test]\"\n\n# Run tests\nuv run pytest\n```\n\n----------------------------------------\n\nTITLE: Evaluating VGT Model on PublayNet Dataset\nDESCRIPTION: Command for evaluating a fine-tuned VGT model with Cascade R-CNN on the PublayNet dataset using a specific configuration file.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython train_VGT.py --config-file Configs/cascade/publaynet_VGT_cascade_PTM.yaml --eval-only --num-gpus 1 MODEL.WEIGHTS <finetuned_checkpoint_file_path> OUTPUT_DIR <your_output_dir>\n```\n\n----------------------------------------\n\nTITLE: Running VGT Server Test Script\nDESCRIPTION: Command to execute the VGT server test script using uv runner. The script hit_vgt_server.py tests the VGT server outputs.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/tests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nuv run hit_vgt_server.py\n```\n\n----------------------------------------\n\nTITLE: Route Additions in Markdown\nDESCRIPTION: Documents new API routes for task parsing that match existing routes but don't require multipart requests. Notes that old routes are deprecated but will remain functional.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nAdded route `POST /task/parse` and `PATCH /task/{task_id}/parse` to parse a task. These routes are exactly the same as the `POST /task` and `PATCH /task/{task_id}` routes, but don't use a multipart request.\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning VGT on DocBank Dataset\nDESCRIPTION: Command to fine-tune the VGT model on the DocBank dataset using a pre-trained model and specific configuration for DocBank.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython train_VGT.py --config-file Configs/cascade/docbank_VGT_cascade_PTM.yaml --num-gpus 8 MODEL.WEIGHTS <VGT-pretrain-model_file_path> OUTPUT_DIR <your_output_dir>\n```\n\n----------------------------------------\n\nTITLE: Configuration Changes in Markdown\nDESCRIPTION: Describes deprecated configuration options and their replacements, ensuring backward compatibility.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/CHANGELOG.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n- Deprecated `model` config\n- Deprecated `target_chunk_length`, you can now use `chunk_processing.target_length` instead\n- Deprecated `structured_extraction.json_schema.type`\n- Deprecated `ocr_strategy.Off`\n- Deprecated `expires_at` in the Python client\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies Definition\nDESCRIPTION: Defines the required Python packages and their minimum version requirements for a document processing application. Includes FastAPI for web API, python-doctr for document processing, and supporting packages for environment variables and file handling.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/doctr/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\naiohttp\nfastapi>=0.115.5\npydantic>=2.9.2\npython-doctr[contrib,html,torch,viz]\npython-dotenv>=1.0.1\npython-multipart>=0.0.17\nuvicorn>=0.32.0\n```\n\n----------------------------------------\n\nTITLE: Starting VGT Container\nDESCRIPTION: Command to start an existing VGT container in interactive mode.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nsudo docker start -i vgt\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies for Machine Learning and Web Application\nDESCRIPTION: A requirements.txt file listing all required Python packages with their specific versions. This includes libraries for machine learning models, web API development, image and PDF processing, along with various utility packages.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nscikit-learn==1.3.1\nfastapi==0.111.1\npython-multipart==0.0.9\nuvicorn==0.30.3\ngunicorn==22.0.0\nrequests==2.32.3\ntorch==2.4.0\ntorchvision==0.19.0\ntimm==1.0.8\nPillow==10.4.0\npdf-annotate==0.12.0\nrequests==2.32.3\nscipy==1.14.0\nopencv-python==4.10.0.84\nShapely==2.0.5\ntransformers==4.40.2\npdf2image==1.17.0\nlxml==5.2.2\nlightgbm==4.5.0\nhuggingface_hub==0.24.3\nsetuptools==72.1.0\nroman==4.2\nhydra-core==1.3.2\npsutil\n```\n\n----------------------------------------\n\nTITLE: Creating History File for VGT\nDESCRIPTION: Creates a bash history file for the VGT container to maintain command history persistence.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntouch ~/.bash_history_vgt\n```\n\n----------------------------------------\n\nTITLE: VGT License Statement with Apache License 2.0 Terms\nDESCRIPTION: The license statement for VGT, indicating it's an algorithm for Document Layout Analysis created by Alibaba that can only be used for research purposes. It's released under the Apache License 2.0, which specifies terms for usage, distribution, and modification of the software.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/services/vgt/object_detection/README.md#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\nVGT is an algorithm for Document Layout Analysis and the code and models herein created by the authors from Alibaba can only be used for research purpose.\nCopyright (C) 1999-2022 Alibaba Group Holding Ltd. \n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Configuring Selective Web Crawler Access with Robots.txt\nDESCRIPTION: This robots.txt configuration allows Googlebot to access the site's homepage and paths containing UUIDs, while blocking all other crawlers from the entire site. The UUID pattern uses a regex-like syntax to match standard UUIDs.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/apps/web/robots.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nUser-agent: Googlebot\nAllow: /[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$\nAllow: /$\n\nUser-agent: *\nDisallow: /\n```\n\n----------------------------------------\n\nTITLE: Configuring React ESLint Plugin\nDESCRIPTION: Setup for integrating the React ESLint plugin, including version configuration and recommended rule sets. Demonstrates how to enable React-specific linting rules and JSX runtime configuration.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/apps/web/README.md#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// eslint.config.js\nimport react from \"eslint-plugin-react\";\n\nexport default tseslint.config({\n  // Set the react version\n  settings: { react: { version: \"18.3\" } },\n  plugins: {\n    // Add the react plugin\n    react,\n  },\n  rules: {\n    // other rules...\n    // Enable its recommended rules\n    ...react.configs.recommended.rules,\n    ...react.configs[\"jsx-runtime\"].rules,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring TypeScript ESLint Parser Options\nDESCRIPTION: Configuration for enabling type-aware lint rules by setting up parser options for TypeScript ESLint. Specifies the TypeScript configuration files and root directory for the project.\nSOURCE: https://github.com/lumina-ai-inc/chunkr/blob/main/apps/web/README.md#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nexport default tseslint.config({\n  languageOptions: {\n    // other options...\n    parserOptions: {\n      project: [\"./tsconfig.node.json\", \"./tsconfig.app.json\"],\n      tsconfigRootDir: import.meta.dirname,\n    },\n  },\n});\n```"
  }
]
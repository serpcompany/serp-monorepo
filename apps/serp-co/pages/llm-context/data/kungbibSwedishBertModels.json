[
  {
    "owner": "kungbib",
    "repo": "swedish-bert-models",
    "content": "TITLE: Using Swedish BERT NER Model with Huggingface Pipeline\nDESCRIPTION: Shows how to use the Swedish BERT model fine-tuned for Named Entity Recognition (NER) using Huggingface's pipeline functionality. The example demonstrates processing a simple Swedish text for entity recognition.\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline,TFBertForTokenClassification\n\nnlp = pipeline('ner', model='KB/bert-base-swedish-cased-ner', tokenizer='KB/bert-base-swedish-cased-ner')\n\nnlp('Kalle och Pelle startar firman Kalle och Pelle.')\n```\n\n----------------------------------------\n\nTITLE: Loading BERT Base Swedish Model with Huggingface Transformers\nDESCRIPTION: Demonstrates how to load the Swedish BERT base model using the Huggingface Transformers library. The example shows loading both the tokenizer and model in both PyTorch and TensorFlow versions.\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModel,AutoTokenizer,TFAutoModel\n\ntok = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')\nmodel = AutoModel.from_pretrained('KB/bert-base-swedish-cased')\n\n# Using TF models\nmodel = TFAutoModel.from_pretrained('KB/bert-base-swedish-cased')\n```\n\n----------------------------------------\n\nTITLE: Processing Swedish Text with BERT Tokenization Handling\nDESCRIPTION: Code that processes Swedish text for NER while handling BERT's tokenization quirks, specifically dealing with word fragments marked with ##. The implementation glues token fragments back together for cleaner entity extraction.\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntext = 'Engelbert tar sin Rolls-Royce till Tele2 Arena för att titta på Djurgården IF ' +\\\n       'som spelar fotboll i VM klockan två på kvällen.'\n\nnlp = pipeline('ner', model='KB/bert-base-swedish-cased-ner', tokenizer='KB/bert-base-swedish-cased-ner', ignore_labels=[])\nl = []\nt = nlp(text)\nin_word=False\n\nfor i,token in enumerate(t):\n    if token['entity'] == 'O':\n        in_word = False\n        continue\n\n    if token['word'].startswith('##'):\n        # deal with (one level of) orphaned ##-tokens\n        if not in_word:\n            l += [ t[i-1] ]\n            l[-1]['entity'] = token['entity']\n        \n        l[-1]['word'] += token['word'][2:]\n    else:\n        l += [ token ]\n\n    in_word = True\n\nprint(l)\n```\n\n----------------------------------------\n\nTITLE: Loading Swedish ALBERT Base Model with Huggingface\nDESCRIPTION: Example of how to load the Swedish ALBERT base model using Huggingface's AutoModel and AutoTokenizer, showing the simplest approach to initialize both the tokenizer and model.\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/README.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModel,AutoTokenizer\n\ntok = AutoTokenizer.from_pretrained('KB/albert-base-swedish-cased-alpha'),\nmodel = AutoModel.from_pretrained('KB/albert-base-swedish-cased-alpha')\n```\n\n----------------------------------------\n\nTITLE: Loading TensorFlow BERT Model for Swedish NER\nDESCRIPTION: Example of loading a pre-trained Swedish BERT model for named entity recognition using TensorFlow through Huggingface's Transformers library.\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntf = TFBertForTokenClassification.from_pretrained('KB/bert-base-swedish-cased-ner')\nnlp = pipeline('ner', model=tf, tokenizer='KB/bert-base-swedish-cased-ner')\n```\n\n----------------------------------------\n\nTITLE: Installing Environment for Swedish BERT Models\nDESCRIPTION: Sets up a Python virtual environment with the required dependencies to use the Swedish BERT models. This includes cloning the repository, creating a virtual environment, and installing the required packages.\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# git clone https://github.com/Kungbib/swedish-bert-models\n# cd swedish-bert-models\n# python3 -m venv venv\n# source venv/bin/activate\n# pip install --upgrade pip\n# pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies for Swedish BERT Models\nDESCRIPTION: This snippet lists the required Python packages and their minimum versions for working with Swedish BERT models. It includes the Transformers library version 2.4.1 or higher and PyTorch version 1.3.1 or higher.\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\ntransformers>=2.4.1\ntorch>=1.3.1\n```\n\n----------------------------------------\n\nTITLE: Example Output from Swedish BERT NER with Token Recombination\nDESCRIPTION: Sample output after processing Swedish text through the NER pipeline with token recombination, showing identified entities with their types such as persons (PRS), locations (LOC), organizations (ORG), events (EVN), and time expressions (TME).\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[ { 'word': 'Engelbert',     'score': 0.99..., 'entity': 'PRS'},\n  { 'word': 'Rolls',         'score': 0.99..., 'entity': 'OBJ'},\n  { 'word': '-',             'score': 0.99..., 'entity': 'OBJ'},\n  { 'word': 'Royce',         'score': 0.99..., 'entity': 'OBJ'},\n  { 'word': 'Tele2',         'score': 0.99..., 'entity': 'LOC'},\n  { 'word': 'Arena',         'score': 0.99..., 'entity': 'LOC'},\n  { 'word': 'Djurgården',    'score': 0.99..., 'entity': 'ORG'},\n  { 'word': 'IF',            'score': 0.99..., 'entity': 'ORG'},\n  { 'word': 'VM',            'score': 0.99..., 'entity': 'EVN'},\n  { 'word': 'klockan',       'score': 0.99..., 'entity': 'TME'},\n  { 'word': 'två',           'score': 0.99..., 'entity': 'TME'},\n  { 'word': 'på',            'score': 0.99..., 'entity': 'TME'},\n  { 'word': 'kvällen',       'score': 0.54..., 'entity': 'TME'} ]\n```\n\n----------------------------------------\n\nTITLE: Example NER Output from Swedish BERT Model\nDESCRIPTION: Sample output showing entity recognition results from the Swedish BERT model, demonstrating how it disambiguates between person names and company names.\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n[ { 'word': 'Kalle', 'score': 0.9998126029968262, 'entity': 'PER' },\n  { 'word': 'Pelle', 'score': 0.9998126029968262, 'entity': 'PER' },\n  { 'word': 'Kalle',   'score': 0.9814832210540771, 'entity': 'ORG' }\n  { 'word': 'och',   'score': 0.9814832210540771, 'entity': 'ORG' }\n  { 'word': 'Pelle',   'score': 0.9814832210540771, 'entity': 'ORG' } ]\n```\n\n----------------------------------------\n\nTITLE: Citation Format for Swedish BERT Models\nDESCRIPTION: BibTex citation format for referencing the Swedish BERT models in academic or research contexts.\nSOURCE: https://github.com/kungbib/swedish-bert-models/blob/master/README.md#2025-04-23_snippet_8\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{swedish-bert,\n    title={Playing with Words at the National Library of Sweden -- Making a Swedish BERT},\n    author={Martin Malmsten and Love Börjeson and Chris Haffenden},\n    year={2020},\n    eprint={2007.01658},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL}\n}\n```"
  }
]
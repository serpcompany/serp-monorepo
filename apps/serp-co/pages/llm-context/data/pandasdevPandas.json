[
  {
    "owner": "pandas-dev",
    "repo": "pandas",
    "content": "TITLE: Creating a pandas DataFrame from a Dictionary of Objects - Python\nDESCRIPTION: A DataFrame 'df2' is created from a dictionary, where each key is a column name and the value is an array-like or scalar. This demonstrates pandas' automatic alignment and handling of various dtypes, including float, timestamp, categorical, and object types. Dependencies: pandas, numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf2 = pd.DataFrame(\\n    {\\n        \"A\": 1.0,\\n        \"B\": pd.Timestamp(\"20130102\"),\\n        \"C\": pd.Series(1, index=list(range(4)), dtype=\"float32\"),\\n        \"D\": np.array([3] * 4, dtype=\"int32\"),\\n        \"E\": pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\\n        \"F\": \"foo\",\\n    }\\n)\\ndf2\n```\n\n----------------------------------------\n\nTITLE: Detecting NaN Values with pd.isna() - Python\nDESCRIPTION: Returns a Boolean mask for missing values (nan) in 'df1' using pd.isna(df1). Useful for further selection, filtering, or imputation operations where identifying missingness is key.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npd.isna(df1)\n```\n\n----------------------------------------\n\nTITLE: Importing pandas Library in Python\nDESCRIPTION: Initializes the pandas library for data manipulation and analysis in Python by importing it with the alias 'pd'. This is a prerequisite for using all subsequent pandas DataFrame functionality in the tutorial. No parameters or outputs; after execution, all pandas functions are available under the 'pd' namespace.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Creating Example Pandas Objects in Python\nDESCRIPTION: This snippet demonstrates initializing several basic pandas objects, including a DatetimeIndex, a Series with labeled axes, and a DataFrame with datetime indices and multiple columns. These objects serve as foundational test or demo data for subsequent pandas operations. The code depends on the pandas and numpy libraries (imported as pd and np, respectively), and highlights key parameters such as the index, columns, and data shape.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.date_range(\"1/1/2000\", periods=8)\ns = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\ndf = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas and NumPy Libraries in Python\nDESCRIPTION: This code snippet shows the standard way to import pandas and NumPy libraries in Python. It uses the conventional aliases 'pd' for pandas and 'np' for NumPy, which are widely used in the data science community.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/introduction.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Importing pandas Library in Python\nDESCRIPTION: This snippet demonstrates how to import the pandas library as 'pd', which is a common convention in the Python data analysis ecosystem. The pandas library is a dependency required for all subsequent DataFrame operations shown in this tutorial. No input or output is involved in this snippet, but it is a prerequisite for running any pandas code.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/05_add_columns.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Broadcasting and Aligning for Flexible Binary Operations in Pandas DataFrames in Python\nDESCRIPTION: This comprehensive snippet demonstrates the creation and broadcasting of DataFrames and Series for flexible binary operations such as subtraction. It shows axis-based alignment, use of iloc and data selection, and leverages both keyword axis and integer axis indices. MultiIndexed DataFrames are also aligned on a specific level for subtraction. These patterns are vital for combining disparate data sources, regularizing data, and vectorized arithmetic.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"one\": pd.Series(np.random.randn(3), index=[\"a\", \"b\", \"c\"]),\n        \"two\": pd.Series(np.random.randn(4), index=[\"a\", \"b\", \"c\", \"d\"]),\n        \"three\": pd.Series(np.random.randn(3), index=[\"b\", \"c\", \"d\"]),\n    }\n)\ndf\nrow = df.iloc[1]\ncolumn = df[\"two\"]\n\ndf.sub(row, axis=\"columns\")\ndf.sub(row, axis=1)\n\ndf.sub(column, axis=\"index\")\ndf.sub(column, axis=0)\n```\n\nLANGUAGE: python\nCODE:\n```\ndfmi = df.copy()\ndfmi.index = pd.MultiIndex.from_tuples(\n    [(1, \"a\"), (1, \"b\"), (1, \"c\"), (2, \"a\")], names=[\"first\", \"second\"]\n)\ndfmi.sub(column, axis=0, level=\"second\")\n```\n\n----------------------------------------\n\nTITLE: Creating a pandas DataFrame from a Python dictionary in Python\nDESCRIPTION: This code initializes a pandas DataFrame using a dictionary of lists, storing Titanic passenger data with columns for Name, Age, and Sex. The dictionary keys become column headers and each list becomes a column. This approach requires pandas to be imported as pd.\nInputs: Dictionary with lists for each column.\nOutputs: DataFrame with specified columns and rows. Limitations: Assumes all lists are of equal length.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/01_table_oriented.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"Name\": [\n            \"Braund, Mr. Owen Harris\",\n            \"Allen, Mr. William Henry\",\n            \"Bonnell, Miss Elizabeth\",\n        ],\n        \"Age\": [22, 35, 58],\n        \"Sex\": [\"male\", \"male\", \"female\"],\n    }\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Data into a pandas DataFrame in Python\nDESCRIPTION: Loads data from a CSV file ('data/titanic.csv') into a pandas DataFrame object named 'titanic'. Requires pandas and access to the specified file path. Key parameter: file path as string. Returns a DataFrame containing the CSV's tabular data. The resulting DataFrame can be used for downstream analysis and manipulation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntitanic = pd.read_csv(\"data/titanic.csv\")\n```\n\n----------------------------------------\n\nTITLE: Matching and Filtering Data in pandas\nDESCRIPTION: Shows how to use the isin method in pandas for data selection and matching, similar to R's %in% operator.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.arange(5), dtype=np.float32)\ns.isin([2, 4])\n```\n\n----------------------------------------\n\nTITLE: Concatenating DataFrames with pandas.concat in Python\nDESCRIPTION: Illustrates how to split a DataFrame into pieces and concatenate them row-wise using pandas.concat. Requires pandas and numpy to be imported as 'pd' and 'np'. The input is an initial DataFrame with random numbers which is divided into segments, then concatenated back together. Output is a combined DataFrame where rows from all pieces are appended in order.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 4))\\ndf\\n\\n# break it into pieces\\npieces = [df[:3], df[3:7], df[7:]]\\n\\npd.concat(pieces)\n```\n\n----------------------------------------\n\nTITLE: Imputing Missing Data with Group Means in pandas GroupBy (Python)\nDESCRIPTION: This snippet demonstrates how to fill in missing values in a DataFrame by replacing them with the mean of their group. It creates a DataFrame with missing values, assigns group keys at random, and applies a groupby transform with fillna. Dependencies: numpy and pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ncols = [\"A\", \"B\", \"C\"]\nvalues = np.random.randn(1000, 3)\nvalues[np.random.randint(0, 1000, 100), 0] = np.nan\nvalues[np.random.randint(0, 1000, 50), 1] = np.nan\nvalues[np.random.randint(0, 1000, 200), 2] = np.nan\ndata_df = pd.DataFrame(values, columns=cols)\ndata_df\n\ncountries = np.array([\"US\", \"UK\", \"GR\", \"JP\"])\nkey = countries[np.random.randint(0, 4, 1000)]\n\ngrouped = data_df.groupby(key)\n\n# Non-NA count in each group\ngrouped.count()\n\ntransformed = grouped.transform(lambda x: x.fillna(x.mean()))\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Data with Default Column Names\nDESCRIPTION: Demonstrates how pandas by default uses the first row as column names when reading CSV data without specifying the header parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\nprint(data)\npd.read_csv(StringIO(data))\n```\n\n----------------------------------------\n\nTITLE: Creating and writing a DataFrame to SQL database\nDESCRIPTION: Demonstrates creating a sample DataFrame with various data types and writing it to a SQLite database using the to_sql method. This example includes datetime, string, numeric, and boolean data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_224\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nc = [\"id\", \"Date\", \"Col_1\", \"Col_2\", \"Col_3\"]\nd = [\n    (26, datetime.datetime(2010, 10, 18), \"X\", 27.5, True),\n    (42, datetime.datetime(2010, 10, 19), \"Y\", -12.5, False),\n    (63, datetime.datetime(2010, 10, 20), \"Z\", 5.73, True),\n]\n\ndata = pd.DataFrame(d, columns=c)\n\ndata\ndata.to_sql(\"data\", con=engine)\n```\n\n----------------------------------------\n\nTITLE: Importing and Exporting Data: CSV, Parquet, and Excel with pandas in Python\nDESCRIPTION: Demonstrates reading and writing DataFrames in CSV, Parquet, and Excel formats. Functions shown include DataFrame.to_csv, read_csv, to_parquet, read_parquet, to_excel, and read_excel. Inputs are DataFrames or filepaths; outputs are files or loaded DataFrames. Requires pandas and, for Excel, the appropriate engine. Includes cleanup via os.remove where shown.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randint(0, 5, (10, 5)))\\ndf.to_csv(\"foo.csv\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(\"foo.csv\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\\n\\nos.remove(\"foo.csv\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.to_parquet(\"foo.parquet\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_parquet(\"foo.parquet\")\n```\n\nLANGUAGE: python\nCODE:\n```\nos.remove(\"foo.parquet\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.to_excel(\"foo.xlsx\", sheet_name=\"Sheet1\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_excel(\"foo.xlsx\", \"Sheet1\", index_col=None, na_values=[\"NA\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nos.remove(\"foo.xlsx\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows Based on a Conditional Expression in pandas (Python)\nDESCRIPTION: This snippet selects rows in a DataFrame where the 'Age' column value is greater than 35. It uses boolean indexing inside brackets for filtering and displays the first few matching rows. The resulting DataFrame contains all columns but only rows where Age > 35.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nabove_35 = titanic[titanic[\"Age\"] > 35]\nabove_35.head()\n```\n\n----------------------------------------\n\nTITLE: Retrieving the Data Type of a Series - pandas - Python\nDESCRIPTION: Retrieves and displays the dtype attribute of a Series, providing information on the underlying data type (usually a NumPy dtype or pandas extension type). This is crucial for understanding memory usage and applicable operations. Requires an existing Series 's'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ns.dtype\n```\n\n----------------------------------------\n\nTITLE: Accessing DataFrame Columns as Attributes and Demonstrating IPython Completion (Python)\nDESCRIPTION: Shows that DataFrame columns with valid Python variable names can be accessed as attributes (df.foo1). Also notes their availability for tab-completion in IPython. Requires pandas, NumPy, and creation of df. df.foo1 returns a Series of the column's data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"foo1\": np.random.randn(5), \"foo2\": np.random.randn(5)})\ndf\ndf.foo1\n```\n\n----------------------------------------\n\nTITLE: Creating a pandas DataFrame with Datetime Index and Random Values - Python\nDESCRIPTION: This code constructs a pandas DataFrame named 'df' with a datetime index generated using pd.date_range and four columns labeled 'A', 'B', 'C', 'D'. Random values are generated via np.random.randn. Input requires pandas and numpy, and the DataFrame holds 6 rows. The variable 'dates' contains the index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndates = pd.date_range(\"20130101\", periods=6)\\ndates\\ndf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\"ABCD\"))\\ndf\n```\n\n----------------------------------------\n\nTITLE: Value Counting in Pandas DataFrame\nDESCRIPTION: Demonstrates using value_counts() method to count combinations across multiple columns and finding mode of values in Series and DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndata = {\"a\": [1, 2, 3, 4], \"b\": [\"x\", \"x\", \"y\", \"y\"]}\nframe = pd.DataFrame(data)\nframe.value_counts()\n\ns5 = pd.Series([1, 1, 3, 3, 3, 5, 5, 7, 7, 7])\ns5.mode()\ndf5 = pd.DataFrame(\n    {\n        \"A\": np.random.randint(0, 7, size=50),\n        \"B\": np.random.randint(-10, 15, size=50),\n    }\n)\ndf5.mode()\n```\n\n----------------------------------------\n\nTITLE: Conditional Assignment with loc in Python\nDESCRIPTION: Demonstrates how to use conditional statements with loc to modify DataFrame values. This example shows how to apply if-then logic to assign values to columns based on conditions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[df.AAA >= 5, \"BBB\"] = -1\ndf\n```\n\n----------------------------------------\n\nTITLE: Aggregating Data Using groupby in Pandas (Python)\nDESCRIPTION: This snippet demonstrates how to group a Pandas DataFrame by the columns 'sex' and 'smoker', then calculate the sum of the 'total_bill' and 'tip' columns for each group. It requires the 'pandas' package and assumes that a DataFrame named 'tips' is defined with relevant columns. The operation returns a DataFrame indexed by grouped values; users should ensure that all column names exist as specified. No constraints on data size, but very large data may impact performance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/groupby.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips_summed = tips.groupby([\"sex\", \"smoker\"])[[\"total_bill\", \"tip\"]].sum()\ntips_summed\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregating Data in Pandas (Equivalent to SQL GROUP BY)\nDESCRIPTION: These snippets show how to group and aggregate data in pandas using the groupby method, which is equivalent to SQL's GROUP BY. It demonstrates simple grouping, counting, and multiple aggregations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntips.groupby(\"sex\").size()\n```\n\nLANGUAGE: python\nCODE:\n```\ntips.groupby(\"day\").agg({\"tip\": \"mean\", \"day\": \"size\"})\n```\n\nLANGUAGE: python\nCODE:\n```\ntips.groupby([\"smoker\", \"day\"]).agg({\"tip\": [\"size\", \"mean\"]})\n```\n\n----------------------------------------\n\nTITLE: Grouping a Series by Index Level and Aggregating (Python)\nDESCRIPTION: Shows how to group a pandas Series by its index (with possible duplicate index values) and perform aggregation functions: first(), last(), and sum(). This illustrates how pandas handles non-unique indices in groupby and how each aggregation works. Requires pandas (as pd) and numpy (as np) to be imported.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nindex = [1, 2, 3, 1, 2, 3]\n\ns = pd.Series([1, 2, 3, 10, 20, 30], index=index)\n\ns\n\ngrouped = s.groupby(level=0)\n\ngrouped.first()\n\ngrouped.last()\n\ngrouped.sum()\n```\n\n----------------------------------------\n\nTITLE: Creating and Inspecting dtypes in a pandas DataFrame - Python\nDESCRIPTION: This snippet demonstrates how to create a pandas DataFrame with heterogeneous column types and inspect the dtypes of each column using the .dtypes attribute. Dependencies include pandas and NumPy. Inputs are constructed via literal assignments, and outputs are the DataFrame display and a Series of column dtypes. All code and formatting are preserved for reproducibility.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_72\n\nLANGUAGE: python\nCODE:\n```\ndft = pd.DataFrame(\n    {\n        \"A\": np.random.rand(3),\n        \"B\": 1,\n        \"C\": \"foo\",\n        \"D\": pd.Timestamp(\"20010102\"),\n        \"E\": pd.Series([1.0] * 3).astype(\"float32\"),\n        \"F\": False,\n        \"G\": pd.Series([1] * 3, dtype=\"int8\"),\n    }\n)\ndft\ndft.dtypes\n```\n\n----------------------------------------\n\nTITLE: Generating Descriptive Statistics of a pandas DataFrame in Python\nDESCRIPTION: This snippet calls the describe() method on the DataFrame 'df', generating summary statistics (count, mean, std, min, quartiles, max) for all numeric columns. String columns are ignored by default. Input is a DataFrame; output is a new DataFrame with statistical metrics as rows and columns as original numeric columns. Requires pandas and a valid DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/01_table_oriented.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.describe()\n```\n\n----------------------------------------\n\nTITLE: Creating Dummy Variables in Python\nDESCRIPTION: Demonstrates how to use pandas.get_dummies() to create indicator variables from categorical data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"key\": list(\"bbacab\"), \"data1\": range(6)})\n\npd.get_dummies(df[\"key\"])\ndf[\"key\"].str.get_dummies()\n\ndummies = pd.get_dummies(df[\"key\"], prefix=\"key\")\ndummies\n\ndf[[\"data1\"]].join(dummies)\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to Excel File with Pandas\nDESCRIPTION: Demonstrates how to write a pandas DataFrame to an Excel file using the to_excel method. This is equivalent to saving data as an Excel file in spreadsheet software.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntips.to_excel(\"./tips.xlsx\")\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data Into a pandas DataFrame in Python\nDESCRIPTION: This snippet reads air quality data from a CSV file named 'data/air_quality_no2.csv' using pandas' read_csv function, setting the first column as the index and parsing it as dates. It assigns the resulting DataFrame to the variable 'air_quality' and displays the first few rows using the head() method. Dependencies include pandas and a CSV file with the expected structure. The main inputs are the filename, index_col, and parse_dates settings, and the output is a DataFrame preview.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/05_add_columns.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nair_quality = pd.read_csv(\"data/air_quality_no2.csv\", index_col=0, parse_dates=True)\nair_quality.head()\n```\n\n----------------------------------------\n\nTITLE: Creating a pandas Series from a Python list in Python\nDESCRIPTION: This code demonstrates creating a pandas Series object from a Python list, with an explicit name 'Age' for the Series. It shows that Series are single columns with row labels, unlike DataFrames. Input is a list of numerical data; output is a Series with index labels starting at zero by default.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/01_table_oriented.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nages = pd.Series([22, 35, 58], name=\"Age\")\nages\n```\n\n----------------------------------------\n\nTITLE: Forward Filling After Reindexing a pandas Series (Python)\nDESCRIPTION: Shows reindexing a Series followed by a forward fill using the ffill method. Requires pandas and a Series; the index to reindex and subsequent filling method are key parameters. Returns a Series where missing values after reindexing are filled forward.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nts2.reindex(ts.index).ffill()\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Data with Pandas\nDESCRIPTION: Demonstrates how to read a CSV file from a URL using pandas read_csv function. This is equivalent to opening a CSV file in Excel.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nurl = (\n    \"https://raw.githubusercontent.com/pandas-dev\"\n    \"/pandas/main/pandas/tests/io/data/csv/tips.csv\"\n)\ntips = pd.read_csv(url)\ntips\n```\n\n----------------------------------------\n\nTITLE: String Cleaning on pandas Index with .str Methods (Python)\nDESCRIPTION: Illustrates the use of .str.strip(), .str.lstrip(), and .str.rstrip() to remove whitespace from beginning, end, or both ends of Index elements. Especially useful for cleaning DataFrame column names or indexes. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([\" jack\", \"jill \", \" jesse \", \"frank\"])\nidx.str.strip()\nidx.str.lstrip()\nidx.str.rstrip()\n```\n\n----------------------------------------\n\nTITLE: Exploding List-like Column Values in Python\nDESCRIPTION: Shows how to use pandas.explode() to transform list-like column values into separate rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nkeys = [\"panda1\", \"panda2\", \"panda3\"]\nvalues = [[\"eats\", \"shoots\"], [\"shoots\", \"leaves\"], [\"eats\", \"leaves\"]]\ndf = pd.DataFrame({\"keys\": keys, \"values\": values})\ndf\ndf[\"values\"].explode()\n\ndf.explode(\"values\")\n\ns = pd.Series([[1, 2, 3], \"foo\", [], [\"a\", \"b\"]])\ns\ns.explode()\n\ndf = pd.DataFrame([{\"var1\": \"a,b,c\", \"var2\": 1}, {\"var1\": \"d,e,f\", \"var2\": 2}])\ndf.assign(var1=df.var1.str.split(\",\")).explode(\"var1\")\n```\n\n----------------------------------------\n\nTITLE: Aggregating Multiple Statistics by Column with pandas.agg() - Python\nDESCRIPTION: This snippet applies custom aggregating statistics to 'Age' and 'Fare' columns using the DataFrame.agg() method, specifying lists of functions (e.g., min, max, median, skew, mean) for each column. Dependencies include pandas and a DataFrame with 'Age' and 'Fare'. The result is a DataFrame showing multiple statistics per field, supporting extended exploration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntitanic.agg(\n    {\n        \"Age\": [\"min\", \"max\", \"median\", \"skew\"],\n        \"Fare\": [\"min\", \"max\", \"median\", \"mean\"],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Designating NA (Missing) Values in read_csv - pandas - Python\nDESCRIPTION: Shows how to specify custom missing value indicators with the na_values keyword. In this example, numbers 5 and 5.0 will be interpreted as NaN (in addition to the defaults). Requires pandas and a CSV input file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(\"path_to_file.csv\", na_values=[5])\n```\n\n----------------------------------------\n\nTITLE: Converting All DataFrame Columns to a Specified dtype Using astype - Python\nDESCRIPTION: Applies the .astype method to change the dtype of all columns in a DataFrame to a specified type, here float32. Outputs are the altered DataFrame and the confirmed new dtypes. Dependencies are pandas and NumPy. The code points out that a copy is returned by default, and type conversion errors will raise exceptions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_81\n\nLANGUAGE: python\nCODE:\n```\ndf3\ndf3.dtypes\n\n# conversion of dtypes\ndf3.astype(\"float32\").dtypes\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns in pandas DataFrame using Python\nDESCRIPTION: This snippet demonstrates how to select specific columns ('sex', 'total_bill', and 'tip') from a pandas DataFrame called 'tips'. The operation returns a new DataFrame containing only these columns. Requires the pandas library and a pre-existing DataFrame named 'tips'. No parameters beyond the column names to keep are required. Outputs a DataFrame with only the selected columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/column_selection.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips[[\"sex\", \"total_bill\", \"tip\"]]\n```\n\n----------------------------------------\n\nTITLE: Concatenating Series Elements Using str.cat in Pandas - Python\nDESCRIPTION: Showcases several ways to concatenate string elements in a pandas Series using the str.cat method, with custom separators, missing value handling (na_rep), and combining with other list-like or array-like objects. It covers cases with Series, different alignments via join keyword, and handling multiple concatenation sources in a list. Requires pandas and numpy. Inputs are Series or compatible array-like objects; outputs are concatenated strings or new Series depending on usage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"b\", \"c\", \"d\"], dtype=\"string\")\ns.str.cat(sep=\",\")\n```\n\nLANGUAGE: python\nCODE:\n```\ns.str.cat()\n```\n\nLANGUAGE: python\nCODE:\n```\nt = pd.Series([\"a\", \"b\", np.nan, \"d\"], dtype=\"string\")\nt.str.cat(sep=\",\")\nt.str.cat(sep=\",\", na_rep=\"-\")\n```\n\nLANGUAGE: python\nCODE:\n```\ns.str.cat([\"A\", \"B\", \"C\", \"D\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ns.str.cat(t)\ns.str.cat(t, na_rep=\"-\")\n```\n\nLANGUAGE: python\nCODE:\n```\nd = pd.concat([t, s], axis=1)\ns\n\nd\n\ns.str.cat(d, na_rep=\"-\")\n```\n\nLANGUAGE: python\nCODE:\n```\nu = pd.Series([\"b\", \"d\", \"a\", \"c\"], index=[1, 3, 0, 2], dtype=\"string\")\ns\n\nu\n\ns.str.cat(u)\ns.str.cat(u, join=\"left\")\n```\n\nLANGUAGE: python\nCODE:\n```\nv = pd.Series([\"z\", \"a\", \"b\", \"d\", \"e\"], index=[-1, 0, 1, 3, 4], dtype=\"string\")\ns\n\nv\n\ns.str.cat(v, join=\"left\", na_rep=\"-\")\ns.str.cat(v, join=\"outer\", na_rep=\"-\")\n```\n\nLANGUAGE: python\nCODE:\n```\nf = d.loc[[3, 2, 1, 0], :]\ns\n\nf\n\ns.str.cat(f, join=\"left\", na_rep=\"-\")\n```\n\nLANGUAGE: python\nCODE:\n```\ns\nu\ns.str.cat([u, u.to_numpy()], join=\"left\")\n```\n\nLANGUAGE: python\nCODE:\n```\nv\ns.str.cat([v, u, u.to_numpy()], join=\"outer\", na_rep=\"-\")\n```\n\nLANGUAGE: python\nCODE:\n```\nu.loc[[3]]\nv.loc[[-1, 0]]\ns.str.cat([u.loc[[3]], v.loc[[-1, 0]]], join=\"right\", na_rep=\"-\")\n```\n\n----------------------------------------\n\nTITLE: Accessing DataFrame Index and Columns Attributes - Python\nDESCRIPTION: This code accesses the .index and .columns attributes of a DataFrame 'df' to display the index (row labels) and column names. No parameters are required beyond the DataFrame object, and output are pandas Index objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.index\\ndf.columns\n```\n\n----------------------------------------\n\nTITLE: Expanding Split Results to DataFrame with .str.split(expand=True) (Python)\nDESCRIPTION: Shows use of expand=True in .str.split(), splitting each string into multiple columns of a DataFrame. All result columns inherit StringDtype if input Series has StringDtype. Input is Series of delimited strings; output is a new DataFrame. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ns2.str.split(\"_\", expand=True)\n```\n\n----------------------------------------\n\nTITLE: Skipping Bad Lines When Reading CSVs - pandas - Python\nDESCRIPTION: Reads a CSV string with malformed lines and uses the on_bad_lines=\"skip\" parameter to skip over lines that do not match the header, instead of raising an error. Useful for importing imperfect datasets. Requires pandas and StringIO.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\npd.read_csv(StringIO(data), on_bad_lines=\"skip\")\n```\n\n----------------------------------------\n\nTITLE: Displaying a pandas DataFrame in Python\nDESCRIPTION: Outputs the current contents of the DataFrame 'titanic'. In interactive environments, such as Jupyter notebooks, this will render the first and last five rows by default. No parameters; serves as a visual check for loaded data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntitanic\n```\n\n----------------------------------------\n\nTITLE: Optimizing Memory Usage with Efficient Data Types in Python\nDESCRIPTION: Demonstrates how to reduce memory usage by converting string columns to categorical type and downcasting numeric columns. This snippet shows the process of analyzing and optimizing data types for memory efficiency.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/scale.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nts = make_timeseries(freq=\"30s\", seed=0)\nts.to_parquet(\"timeseries.parquet\")\nts = pd.read_parquet(\"timeseries.parquet\")\n\n# Convert 'name' to categorical\nts2 = ts.copy()\nts2[\"name\"] = ts2[\"name\"].astype(\"category\")\n\n# Downcast numeric columns\nts2[\"id\"] = pd.to_numeric(ts2[\"id\"], downcast=\"unsigned\")\nts2[[\"x\", \"y\"]] = ts2[[\"x\", \"y\"]].apply(pd.to_numeric, downcast=\"float\")\n\n# Compare memory usage\nreduction = ts2.memory_usage(deep=True).sum() / ts.memory_usage(deep=True).sum()\nprint(f\"{reduction:0.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Files using Pandas (Python)\nDESCRIPTION: Illustrates basic usage of pandas' read_csv function to read a CSV file ('tmp.csv') into a DataFrame, and then displays it. Requires the pandas library, and expects 'tmp.csv' to be present and properly formatted. Input is the file path; output is the loaded DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ntable = pd.read_csv(\"tmp.csv\")\ntable\n```\n\n----------------------------------------\n\nTITLE: Concatenating Air Quality Measurements with pandas.concat in Python\nDESCRIPTION: This example combines the PM2.5 and NO2 air quality DataFrames into a single DataFrame using pandas.concat along axis=0 (row-wise). The head of the combined table is displayed. Both tables must have matching columns for seamless concatenation. The resulting DataFrame stacks observations from both measurement sources.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nair_quality = pd.concat([air_quality_pm25, air_quality_no2], axis=0)\nair_quality.head()\n```\n\n----------------------------------------\n\nTITLE: More Examples of DataFrame Slicing with .iloc\nDESCRIPTION: Additional examples of .iloc slicing showing row-only slices, column-only slices, and accessing a single element.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf1.iloc[1:3, :]\n\ndf1.iloc[:, 1:3]\n\n# this is also equivalent to ``df1.iat[1,1]``\ndf1.iloc[1, 1]\n```\n\n----------------------------------------\n\nTITLE: Iterating Through DataFrame with itertuples (Python)\nDESCRIPTION: Shows iteration over DataFrame rows as namedtuples using itertuples, preserving data types and maximizing performance. Each namedtuple includes the row index as the first field and then the column values. Suitable for row-wise processing without dtype conversion overhead.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nfor row in df.itertuples():\n    print(row)\n```\n\n----------------------------------------\n\nTITLE: Boolean Indexing of DataFrames - Python\nDESCRIPTION: This set of snippets shows how to filter DataFrames using boolean conditions, such as selecting rows where a column value is greater than zero, using a boolean mask over the entire DataFrame, or selecting values with isin().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf[df[\"A\"] > 0]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf[df > 0]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.copy()\\ndf2[\"E\"] = [\"one\", \"one\", \"two\", \"three\", \"four\", \"three\"]\\ndf2\\ndf2[df2[\"E\"].isin([\"two\", \"four\"])]\n```\n\n----------------------------------------\n\nTITLE: Handling Trailing Delimiters in CSV Data\nDESCRIPTION: Shows how to handle CSV files with trailing delimiters by explicitly disabling index column inference using index_col=False.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\nprint(data)\npd.read_csv(StringIO(data))\npd.read_csv(StringIO(data), index_col=False)\n```\n\n----------------------------------------\n\nTITLE: Series and DataFrame Attribute Access\nDESCRIPTION: Shows how to access and modify Series and DataFrame elements using attribute notation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsa = pd.Series([1, 2, 3], index=list('abc'))\ndfa = df.copy()\nsa.b\ndfa.A\nsa.a = 5\ndfa.A = list(range(len(dfa.index)))\n```\n\n----------------------------------------\n\nTITLE: Column-Specific Aggregation with Dict - pandas - Python\nDESCRIPTION: Aggregates DataFrame columns with a dictionary, mapping column names to functions or lists of functions. Produces output where each specified function applies only to relevant columns. Supports lists for multiple aggregations per column. Output shape depends on input mapping.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ntsdf.agg({\"A\": \"mean\", \"B\": \"sum\"})\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf.agg({\"A\": [\"mean\", \"min\"], \"B\": \"sum\"})\n```\n\n----------------------------------------\n\nTITLE: Getting a NumPy Array from a DataFrame (to_numpy) - Python\nDESCRIPTION: The df.to_numpy() method converts the DataFrame to a NumPy ndarray without row and column labels. The resulting array will have a common dtype that can hold all DataFrame dtypes, possibly object. Requires pandas >= 0.24.0 and reflects a copy if necessary.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.to_numpy()\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrames with MultiIndex Columns in Python\nDESCRIPTION: Demonstrates how to use a MultiIndex for column labels in a DataFrame, showing how to create a DataFrame with MultiIndex columns and regular row index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(3, 8), index=[\"A\", \"B\", \"C\"], columns=index)\ndf\npd.DataFrame(np.random.randn(6, 6), index=index[:6], columns=index[:6])\n```\n\n----------------------------------------\n\nTITLE: Disabling Accelerated Operations Using Pandas Options in Python\nDESCRIPTION: This code shows how to disable the use of optional acceleration libraries 'bottleneck' and 'numexpr' via pandas set_option. This might be necessary when debugging, benchmarking, or if library incompatibilities arise. No parameters other than the configuration strings and booleans are required; this alters global options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"compute.use_bottleneck\", False)\npd.set_option(\"compute.use_numexpr\", False)\n```\n\n----------------------------------------\n\nTITLE: Enumerate Row Order Within Group with cumcount - pandas Python\nDESCRIPTION: Demonstrates use of cumcount to enumerate the position of each item within its group, with ascending and descending options, to track intra-group order. Useful for ranking or ordering within groups. Input: DataFrame, Output: Series of counts.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndfg = pd.DataFrame(list(\"aaabba\"), columns=[\"A\"])\ndfg\n```\n\nLANGUAGE: python\nCODE:\n```\ndfg.groupby(\"A\").cumcount()\n```\n\nLANGUAGE: python\nCODE:\n```\ndfg.groupby(\"A\").cumcount(ascending=False)\n```\n\n----------------------------------------\n\nTITLE: Selecting Multiple Columns from a DataFrame with pandas (Python)\nDESCRIPTION: This snippet illustrates how to select multiple columns ('Age' and 'Sex') from the Titanic DataFrame by passing a list of column names inside the brackets. The output is a pandas DataFrame containing only the specified columns, and head() displays the first few rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nage_sex = titanic[[\"Age\", \"Sex\"]]\nage_sex.head()\n```\n\n----------------------------------------\n\nTITLE: Inspecting DataFrame Column Data Types - Python\nDESCRIPTION: This snippet returns the data types of each column in 'df2' using the .dtypes attribute. This is useful for verifying and understanding the dtypes (e.g., float, int, object, category) inferred or set in the DataFrame. Requires a DataFrame object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf2.dtypes\n```\n\n----------------------------------------\n\nTITLE: Describing Age and Fare Statistics with pandas - Python\nDESCRIPTION: This snippet generates descriptive statistics (count, mean, std, min, quartiles, max) for the 'Age' and 'Fare' columns of the DataFrame using the .describe() method. The output is a DataFrame summarizing key statistics for each specified column. It requires a valid DataFrame containing the 'Age' and 'Fare' columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntitanic[[\"Age\", \"Fare\"]].describe()\n```\n\n----------------------------------------\n\nTITLE: Iterative Chunked Reading of CSV Files using Pandas (Python)\nDESCRIPTION: Demonstrates using read_csv with chunksize to return a TextFileReader object, enabling iteration over a CSV file in chunks of 4 rows. The context manager ensures proper resource management. Requires pandas and a valid 'tmp.csv'. The code prints the TextFileReader and each resulting chunk as DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nwith pd.read_csv(\"tmp.csv\", chunksize=4) as reader:\n    print(reader)\n    for chunk in reader:\n        print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Pivot Table with Margins (Totals) and Custom Aggregation - pandas - Python\nDESCRIPTION: Calls DataFrame.pivot_table with margins=True and aggfunc='std', resulting in row and column totals labeled 'All'. The table includes both 'D' and 'E' columns, grouped by ['A', 'B'] as index and 'C' as columns. Shows how to calculate summary statistics like standard deviation for all groupings. Suitable for adding grand total or summary rows and columns for dashboards or reporting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ntable = df.pivot_table(\n    index=[\"A\", \"B\"],\n    columns=\"C\",\n    values=[\"D\", \"E\"],\n    margins=True,\n    aggfunc=\"std\"\n)\ntable\n```\n\n----------------------------------------\n\nTITLE: Evaluating Expressions within a DataFrame Context Using DataFrame.eval - Python\nDESCRIPTION: Demonstrates DataFrame.eval for evaluating column expressions within the local DataFrame context. This eliminates the need to prefix column names. Requires pandas and numpy. Applies an arithmetic expression to DataFrame columns 'a' and 'b', returning a new Series with the result. Useful for concise syntax and performance gains with large DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(5, 2), columns=[\"a\", \"b\"])\ndf.eval(\"a + b\")\n```\n\n----------------------------------------\n\nTITLE: Creating Pivot Tables with pandas.pivot_table in Python\nDESCRIPTION: Shows how to use pandas.pivot_table for computing pivot tables from DataFrames. Requires pandas and numpy. Inputs are categorical columns and random data. Function pivots the DataFrame by specifying 'values', 'index', and 'columns', returning a new summarizing DataFrame. Useful for cross-tabulation and aggregation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\\n    {\\n        \"A\": [\"one\", \"one\", \"two\", \"three\"] * 3,\\n        \"B\": [\"A\", \"B\", \"C\"] * 4,\\n        \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 2,\\n        \"D\": np.random.randn(12),\\n        \"E\": np.random.randn(12),\\n    }\\n)\\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\npd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=[\"C\"])\n```\n\n----------------------------------------\n\nTITLE: Numeric Type Conversion with Error Handling\nDESCRIPTION: Shows how to use pd.to_numeric to convert column data to numeric types after reading, with error handling for invalid values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf2 = pd.read_csv(StringIO(data))\ndf2[\"col_1\"] = pd.to_numeric(df2[\"col_1\"], errors=\"coerce\")\ndf2\ndf2[\"col_1\"].apply(type).value_counts()\n```\n\n----------------------------------------\n\nTITLE: Testing Pattern Occurrence and Matches with str.contains, str.match, and str.fullmatch - Pandas Python\nDESCRIPTION: Illustrates testing elements of a pandas Series for the presence of regex patterns via str.contains, as well as stricter matching with str.match (regex at string start) and str.fullmatch (entire string matches pattern). Requires familiarity with regular expressions, pandas, and Python's re package. Takes a regex pattern and an input Series, and returns boolean Series indicating matches.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\npattern = r\"[0-9][a-z]\"\npd.Series(\n    [\"1\", \"2\", \"3a\", \"3b\", \"03c\", \"4dx\"],\n    dtype=\"string\",\n).str.contains(pattern)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series(\n    [\"1\", \"2\", \"3a\", \"3b\", \"03c\", \"4dx\"],\n    dtype=\"string\",\n).str.match(pattern)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series(\n    [\"1\", \"2\", \"3a\", \"3b\", \"03c\", \"4dx\"],\n    dtype=\"string\",\n).str.fullmatch(pattern)\n```\n\n----------------------------------------\n\nTITLE: Using Boolean Arrays with NA Values for Filtering\nDESCRIPTION: Shows how NA values in a boolean array propagate as False when used for DataFrame filtering with pandas boolean data type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmask = pd.array([True, False, True, False, pd.NA, False], dtype=\"boolean\")\nmask\ndf1[mask]\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame with Boolean Condition in pandas\nDESCRIPTION: This snippet demonstrates how to filter a DataFrame using a boolean condition. It selects rows where the 'total_bill' column has values greater than 10.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/filtering.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips[tips[\"total_bill\"] > 10]\n```\n\n----------------------------------------\n\nTITLE: Computing Value Counts in Python with Pandas\nDESCRIPTION: This snippet shows how to use the 'value_counts' method to compute a histogram of values in a Series. It demonstrates usage with both a Pandas Series and a NumPy array.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndata = np.random.randint(0, 7, size=50)\ns = pd.Series(data)\ns.value_counts()\n```\n\n----------------------------------------\n\nTITLE: Filtering with Multiple Conditions Using | Operator in pandas (Python)\nDESCRIPTION: This example shows how to apply multiple conditions by combining them with the bitwise OR operator (|). Rows where 'Pclass' is either 2 or 3 are selected. Parentheses are required around each condition; the output is a DataFrame with all columns and matching rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass_23 = titanic[(titanic[\"Pclass\"] == 2) | (titanic[\"Pclass\"] == 3)]\nclass_23.head()\n```\n\n----------------------------------------\n\nTITLE: Creating Series of Timedeltas in Pandas\nDESCRIPTION: Creates a Series of datetime.timedelta objects with increasing day values. This demonstrates how to generate time differences programmatically for time series analysis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndeltas = pd.Series([datetime.timedelta(days=i) for i in range(3)])\n```\n\n----------------------------------------\n\nTITLE: Setting and Resetting Index in Pandas DataFrame\nDESCRIPTION: This section covers how to set and reset the index of a Pandas DataFrame using set_index() and reset_index() methods. It demonstrates various options like creating MultiIndex and handling of existing columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_75\n\nLANGUAGE: python\nCODE:\n```\ndata = pd.DataFrame({'a': ['bar', 'bar', 'foo', 'foo'],\n                    'b': ['one', 'two', 'one', 'two'],\n                    'c': ['z', 'y', 'x', 'w'],\n                    'd': [1., 2., 3, 4]})\ndata\nindexed1 = data.set_index('c')\nindexed1\nindexed2 = data.set_index(['a', 'b'])\nindexed2\n\nframe = data.set_index('c', drop=False)\nframe = frame.set_index(['a', 'b'], append=True)\nframe\n\ndata.set_index('c', drop=False)\n\ndata\ndata.reset_index()\n\nframe\nframe.reset_index(level=1)\n\ndf_idx = pd.DataFrame(range(4))\ndf_idx.index = pd.Index([10, 20, 30, 40], name=\"a\")\ndf_idx\n```\n\n----------------------------------------\n\nTITLE: Creating Indicator Variables from String Series in Pandas\nDESCRIPTION: Demonstrates using str.get_dummies() to create indicator variables from a string Series where values are separated by a delimiter character.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"a|b\", np.nan, \"a|c\"], dtype=\"string\")\ns.str.get_dummies(sep=\"|\")\n```\n\n----------------------------------------\n\nTITLE: Clipboard Import/Export Workflow in Pandas\nDESCRIPTION: Demonstrates creating a DataFrame, writing it to the clipboard with to_clipboard(), and reading it back with read_clipboard(). This shows the round-trip clipboard operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_154\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame(\n...     {\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [\"p\", \"q\", \"r\"]}, index=[\"x\", \"y\", \"z\"]\n... )\n\n>>> df\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n>>> df.to_clipboard()\n>>> pd.read_clipboard()\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n```\n\n----------------------------------------\n\nTITLE: Using the indicator Argument in pandas merge for Origin Tracking\nDESCRIPTION: Explains how to use the 'indicator' argument in pandas' merge function to add a column indicating the source of each row in the merged DataFrame. If a string is provided, the new column takes that name. Requires pandas. Input: DataFrames and indicator argument; Output: merged DataFrame with indicator column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame({\"col1\": [0, 1], \"col_left\": [\"a\", \"b\"]})\ndf2 = pd.DataFrame({\"col1\": [1, 2, 2], \"col_right\": [2, 2, 2]})\npd.merge(df1, df2, on=\"col1\", how=\"outer\", indicator=True)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge(df1, df2, on=\"col1\", how=\"outer\", indicator=\"indicator_column\")\n```\n\n----------------------------------------\n\nTITLE: Basic Series Indexing with .loc\nDESCRIPTION: Demonstrates label-based indexing on a pandas Series using .loc accessor to select multiple specific labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(range(5), index=list(\"abcde\"))\nser.loc[[\"a\", \"c\", \"e\"]]\n```\n\n----------------------------------------\n\nTITLE: Counting Number of Records in Each Passenger Class with pandas.value_counts() - Python\nDESCRIPTION: This snippet uses the Series.value_counts() method on the 'Pclass' column to count how many passengers belong to each cabin class. The output is a Series with cabin classes as the index and passenger counts as values. This is a shortcut for groupby-count patterns and efficiently summarizes categorical distributions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Pclass\"].value_counts()\n```\n\n----------------------------------------\n\nTITLE: Computing the Maximum Value of a DataFrame Column with pandas in Python\nDESCRIPTION: This snippet selects the 'Age' column from the DataFrame 'df' and applies the max() method to return the maximum value found in that column. The input is a DataFrame column (Series) and the output is a single numerical value. Requires the DataFrame to exist with an 'Age' column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/01_table_oriented.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf[\"Age\"].max()\n```\n\n----------------------------------------\n\nTITLE: Parsing Categorical Data Directly with Pandas read_csv in Python\nDESCRIPTION: Demonstrates how the read_csv function in pandas now supports direct parsing of categorical data by specifying dtype as 'category'. Examples include parsing an entire DataFrame, parsing with a dtype dictionary, and handling category conversion. Requires pandas and StringIO; input is CSV text; gives DataFrame with columns of categorical dtype. Note: categories parsed from CSV are always string-valued by default.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata = \"\"\"\ncol1,col2,col3\na,b,1\na,b,2\nc,d,3\n\"\"\"\n\npd.read_csv(StringIO(data))\npd.read_csv(StringIO(data)).dtypes\npd.read_csv(StringIO(data), dtype=\"category\").dtypes\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(StringIO(data), dtype=\"category\")\ndf.dtypes\ndf[\"col3\"]\nnew_categories = pd.to_numeric(df[\"col3\"].cat.categories)\ndf[\"col3\"] = df[\"col3\"].cat.rename_categories(new_categories)\ndf[\"col3\"]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Expressions with pandas.eval and DataFrame.query (ipython)\nDESCRIPTION: Illustrates using pandas DataFrame.eval and query methods to evaluate expressions involving DataFrame columns and external variables. Demonstrates correct use of the '@' prefix for referencing local variables within expressions. Requires pandas/numpy. Inputs: DataFrame, local variable(s). Outputs: evaluated Series or filtered DataFrame results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_17\n\nLANGUAGE: ipython\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(5, 2), columns=list(\"ab\"))\nnewcol = np.random.randn(len(df))\ndf.eval(\"b + @newcol\")\ndf.query(\"b < @newcol\")\n```\n\n----------------------------------------\n\nTITLE: Selecting Group Rows with Dropna Logic and Multiple nth Values - pandas Python\nDESCRIPTION: Applies nth with dropna argument to select not-null elements, compares to g.first/g.last, and demonstrates selecting multiple rows with lists and slices. This enables complex per-group filtering or selection. Inputs: groupby object, output: filtered DataFrame/Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\n# nth(0) is the same as g.first()\ng.nth(0, dropna=\"any\")\ng.first()\n\n# nth(-1) is the same as g.last()\ng.nth(-1, dropna=\"any\")\ng.last()\n\ng.B.nth(0, dropna=\"all\")\n```\n\nLANGUAGE: python\nCODE:\n```\nbusiness_dates = pd.date_range(start=\"4/1/2014\", end=\"6/30/2014\", freq=\"B\")\ndf = pd.DataFrame(1, index=business_dates, columns=[\"a\", \"b\"])\n# get the first, 4th, and last date index for each month\ndf.groupby([df.index.year, df.index.month]).nth([0, 3, -1])\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([df.index.year, df.index.month]).nth[1:]\ndf.groupby([df.index.year, df.index.month]).nth[1:, :-1]\n```\n\n----------------------------------------\n\nTITLE: GroupBy Aggregations for Multiple Columns using pandas in Python\nDESCRIPTION: This snippet shows how to perform aggregate statistical functions across groups within a DataFrame using pandas' groupby and agg methods. It groups the 'tips' DataFrame by the 'sex' column and computes mean, standard deviation, min, and max for the 'total_bill' and 'tip' columns. Requires pandas and the DataFrame 'tips'. Outputs a new DataFrame with hierarchical columns for each aggregation statistic per gender. This solution is flexible and supports further aggregation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spss.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    tips.groupby(\"sex\")[[\"total_bill\", \"tip\"]].agg([\"mean\", \"std\", \"min\", \"max\"])\n```\n\n----------------------------------------\n\nTITLE: Loading and Filtering NO2 Air Quality Data with pandas in Python\nDESCRIPTION: This snippet reads the 'air_quality_no2_long.csv' file containing NO2 measurements into a pandas DataFrame. It then filters the DataFrame to retain only the 'date.utc', 'location', 'parameter', and 'value' columns, and displays the first few rows. Dependencies: pandas, a local copy of the CSV file. Input is a CSV file path, output is a filtered DataFrame ready for further analysis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nair_quality_no2 = pd.read_csv(\"data/air_quality_no2_long.csv\",\n                                  parse_dates=True)\nair_quality_no2 = air_quality_no2[[\"date.utc\", \"location\",\n                                   \"parameter\", \"value\"]]\nair_quality_no2.head()\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame Groups by Aggregated Data - Pandas - Python\nDESCRIPTION: Groups a DataFrame by the 'code' column, sums the 'data' column in each group, and sorts the DataFrame rows according to the group aggregate totals. Inputs include columns for code, data, and flag; outputs a sorted DataFrame. Requires only pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"code\": [\"foo\", \"bar\", \"baz\"] * 2,\n        \"data\": [0.16, -0.21, 0.33, 0.45, -0.59, 0.62],\n        \"flag\": [False, True] * 3,\n    }\n)\n\ncode_groups = df.groupby(\"code\")\n\nagg_n_sort_order = code_groups[[\"data\"]].transform(\"sum\").sort_values(by=\"data\")\n\nsorted_df = df.loc[agg_n_sort_order.index]\n\nsorted_df\n```\n\n----------------------------------------\n\nTITLE: Displaying the First N Rows of a pandas DataFrame in Python\nDESCRIPTION: Shows the first 8 rows of the 'titanic' DataFrame using the '.head' method. The 'n' parameter lets users specify the number of rows previewed (here: 8). No data is modified; this is for data exploration and validation purposes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntitanic.head(8)\n```\n\n----------------------------------------\n\nTITLE: Computing Descriptive Statistics in Python with Pandas\nDESCRIPTION: This snippet demonstrates how to compute various descriptive statistics on a DataFrame using methods like 'mean', 'sum', and 'std'. It also shows how to standardize data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.mean(axis=0)\ndf.mean(axis=1)\ndf.sum(axis=0, skipna=False)\ndf.sum(axis=1, skipna=True)\nts_stand = (df - df.mean()) / df.std()\nxs_stand = df.sub(df.mean(axis=1), axis=0).div(df.std(axis=1), axis=0)\n```\n\n----------------------------------------\n\nTITLE: Importing pandas and Creating a DataFrame - Python\nDESCRIPTION: This snippet demonstrates the basic initialization and use of pandas within an in-browser Jupyterlite/Pyodide session. It imports the pandas library as 'pd', creates a DataFrame with columns 'num_legs' and 'num_wings' indexed by animal types, and assigns it to the variable 'df'. No additional dependencies are required beyond pandas, which is provided by the environment. Expected input is the embedded code itself, and the output is the creation of the DataFrame for further experimentation. This code is executed automatically in the live browser shell for demonstration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/try.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\\ndf = pd.DataFrame({\"num_legs\": [2, 4], \"num_wings\": [2, 0]}, index=[\"falcon\", \"dog\"])\\ndf\n```\n\n----------------------------------------\n\nTITLE: Merging DataFrames on Common Keys with pandas.merge in Python\nDESCRIPTION: Shows how to merge two DataFrames using pandas.merge to perform SQL-style joins on specific columns. Relies on pandas as 'pd'. The inputs are two DataFrames with a common 'key' column. The function merges rows with matching 'key' values, yielding a unified DataFrame containing columns from both sources.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"lval\": [1, 2]})\\nright = pd.DataFrame({\"key\": [\"foo\", \"foo\"], \"rval\": [4, 5]})\\nleft\\nright\\npd.merge(left, right, on=\"key\")\n```\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"lval\": [1, 2]})\\nright = pd.DataFrame({\"key\": [\"foo\", \"bar\"], \"rval\": [4, 5]})\\nleft\\nright\\npd.merge(left, right, on=\"key\")\n```\n\n----------------------------------------\n\nTITLE: Splitting Strings into Lists with .str.split (Python)\nDESCRIPTION: Applies .str.split to divide each entry in a Series based on a delimiter, returning a Series of lists where each list is the split result. Handles missing (NaN) values gracefully. Input is a Series of delimited strings; output is a Series of list objects. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ns2 = pd.Series([\"a_b_c\", \"c_d_e\", np.nan, \"f_g_h\"], dtype=\"string\")\ns2.str.split(\"_\")\n```\n\n----------------------------------------\n\nTITLE: DataFrame Operations with .loc Label-Based Indexing\nDESCRIPTION: Creates a DataFrame with alphabetic row and column labels and demonstrates different .loc selection methods including list of labels and slicing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(np.random.randn(6, 4),\n                  index=list('abcdef'),\n                  columns=list('ABCD'))\ndf1\ndf1.loc[['a', 'b', 'd'], :]\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregating DataFrames using pandas DataFrame.groupby in Python\nDESCRIPTION: Demonstrates groupby operations for splitting data, applying functions, and combining results. Shows how to group by one or more columns and apply sum aggregations. Dependencies are pandas and numpy. Inputs are a DataFrame with multiple categorical columns and random data; outputs are aggregated DataFrames, optionally with MultiIndex structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\\n    {\\n        \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\\n        \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\\n        \"C\": np.random.randn(8),\\n        \"D\": np.random.randn(8),\\n    }\\n)\\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"A\")[[\"C\", \"D\"]].sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([\"A\", \"B\"]).sum()\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame and Series by Index and Axis in pandas (Python)\nDESCRIPTION: This snippet illustrates multiple ways to sort a DataFrame by index (default and reversed) and by columns (axis=1), as well as sorting an individual Series. It also demonstrates reindexing to create an unsorted DataFrame. Dependencies: pandas and numpy for np.random.randn. Inputs: DataFrame with labeled indexes and columns. Outputs: Sorted DataFrame or Series copies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"one\": pd.Series(np.random.randn(3), index=[\"a\", \"b\", \"c\"]),\n        \"two\": pd.Series(np.random.randn(4), index=[\"a\", \"b\", \"c\", \"d\"]),\n        \"three\": pd.Series(np.random.randn(3), index=[\"b\", \"c\", \"d\"]),\n    }\n)\n\nunsorted_df = df.reindex(\n    index=[\"a\", \"d\", \"c\", \"b\"], columns=[\"three\", \"two\", \"one\"]\n)\nunsorted_df\n\n# DataFrame\nunsorted_df.sort_index()\nunsorted_df.sort_index(ascending=False)\nunsorted_df.sort_index(axis=1)\n\n# Series\nunsorted_df[\"three\"].sort_index()\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Rows and a Column with .loc in pandas (Python)\nDESCRIPTION: This snippet uses the .loc accessor to select the 'Name' column for all passengers whose 'Age' is greater than 35. The first argument (before the comma) specifies the boolean condition on rows; the second specifies the column name. The result is a Series of names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nadult_names = titanic.loc[titanic[\"Age\"] > 35, \"Name\"]\nadult_names.head()\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Groups with pandas GroupBy in Python\nDESCRIPTION: Demonstrates iterating over groups obtained from a GroupBy object on a DataFrame in Python. For each group, it prints the group name and data. Useful for exploring groups or applying custom operations. Requires a DataFrame and pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby('A')\n\nfor name, group in grouped:\n    print(name)\n    print(group)\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows with Missing Values Using Pandas Series Methods - Python\nDESCRIPTION: These code snippets show how to filter DataFrame rows where a specified column (here, 'value_x') either contains or does not contain missing (NaN) values, using pandas Series methods isna() and notna(). The input is a pandas DataFrame ('outer_join'); the output is a filtered DataFrame showing only the desired rows. Requires pandas as a dependency. The 'value_x' column must exist in the DataFrame. Returns all rows where 'value_x' is NaN or not NaN, respectively.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/missing.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nouter_join[outer_join[\"value_x\"].isna()]\n```\n\nLANGUAGE: Python\nCODE:\n```\nouter_join[outer_join[\"value_x\"].notna()]\n```\n\n----------------------------------------\n\nTITLE: Selecting Data by Label with .loc and .at - Python\nDESCRIPTION: Several snippets show DataFrame row and column selection by label using .loc and .at. .loc can select rows or columns, single values, or slices with label inclusion, while .at retrieves a single value efficiently by label.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[dates[0]]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[:, [\"A\", \"B\"]]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[\"20130102\":\"20130104\", [\"A\", \"B\"]]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[dates[0], \"A\"]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.at[dates[0], \"A\"]\n```\n\n----------------------------------------\n\nTITLE: Using Callables for Condition and Other in pandas.DataFrame.where - Python\nDESCRIPTION: Shows pandas' support for passing callables to where for both condition and 'other'. Each receives the full DataFrame as an argument, allowing flexible conditional logic. Requires pandas. Increments all values by 10 if not greater than 4; otherwise, leaves unchanged. Inputs: DataFrame; Outputs: conditionally modified DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\ndf3 = pd.DataFrame({'A': [1, 2, 3],\n                   'B': [4, 5, 6],\n                   'C': [7, 8, 9]})\ndf3.where(lambda x: x > 4, lambda x: x + 10)\n```\n\n----------------------------------------\n\nTITLE: Renaming DataFrame/Series Labels with Functions or Mappings (Python)\nDESCRIPTION: Demonstrates label relabeling with rename, supporting both function and mapping (dict/Series) objects. Function input must process all labels uniquely. Includes DataFrame index/columns renaming with mappings, axis-style calls, and renaming Series name attribute. Returns DataFrame or Series with altered labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ns\ns.rename(str.upper)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.rename(\n    columns={\"one\": \"foo\", \"two\": \"bar\"},\n    index={\"a\": \"apple\", \"b\": \"banana\", \"d\": \"durian\"},\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.rename({\"one\": \"foo\", \"two\": \"bar\"}, axis=\"columns\")\ndf.rename({\"a\": \"apple\", \"b\": \"banana\", \"d\": \"durian\"}, axis=\"index\")\n```\n\nLANGUAGE: python\nCODE:\n```\ns.rename(\"scalar-name\")\n```\n\n----------------------------------------\n\nTITLE: Grouping DataFrames by Columns (Python)\nDESCRIPTION: Shows how to perform GroupBy operations on a DataFrame by column 'A', by column 'B', and by both. This demonstrates the ability to flexibly group over single or multiple columns in pandas, enabling different levels of data partitioning. The snippet assumes a DataFrame 'df' is already defined and pandas is imported.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby(\"A\")\ngrouped = df.groupby(\"B\")\ngrouped = df.groupby([\"A\", \"B\"])\n```\n\n----------------------------------------\n\nTITLE: Indexing, Boolean Selection and NumPy Operations on Series - pandas - Python\nDESCRIPTION: Shows various ways to access and manipulate Series data using integer-location indexing, slicing, conditional filtering, and applying NumPy universal functions. It demonstrates how Series supports advanced and vectorized operations, leveraging both pandas indexing and NumPy methods. Requires that a Series 's' exists and NumPy is imported.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns.iloc[0]\ns.iloc[:3]\ns[s > s.median()]\ns.iloc[[4, 3, 1]]\nnp.exp(s)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Single Series to Scalar - pandas - Python\nDESCRIPTION: Performs aggregation on a single DataFrame column (Series), producing a scalar (sum). This demonstrates how .agg() with a single function returns a scalar value from a Series. Input: Series object and aggregation function; Output: scalar value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ntsdf[\"A\"].agg(\"sum\")\n```\n\n----------------------------------------\n\nTITLE: Handling Inline Comments in CSV Data\nDESCRIPTION: Shows how comments at the end of data lines are included by default and can be excluded using the comment parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndata = (\n    \"ID,level,category\\n\"\n    \"Patient1,123000,x # really unpleasant\\n\"\n    \"Patient2,23000,y # wouldn't take his medicine\\n\"\n    \"Patient3,1234018,z # awesome\"\n)\nwith open(\"tmp.csv\", \"w\") as fh:\n    fh.write(data)\n\nprint(open(\"tmp.csv\").read())\n```\n\n----------------------------------------\n\nTITLE: Chaining Data Selection Operations with Callable Indexing\nDESCRIPTION: Example of chaining data operations by reading a CSV, grouping data, and then using callable indexing to filter results without temporary variables.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nbb = pd.read_csv('data/baseball.csv', index_col='id')\n(bb.groupby(['year', 'team']).sum(numeric_only=True)\n  .loc[lambda df: df['r'] > 100])\n```\n\n----------------------------------------\n\nTITLE: Grouped Boxplot using DataFrame.plot.box Method\nDESCRIPTION: This example shows how to create grouped boxplots using the plot.box() method. The column parameter selects which columns to plot, and the by parameter specifies the grouping variable.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(10, 3), columns=[\"Col1\", \"Col2\", \"Col3\"])\ndf[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\nplt.figure();\n\nbp = df.plot.box(column=[\"Col1\", \"Col2\"], by=\"X\")\n```\n\n----------------------------------------\n\nTITLE: Joining Station Coordinates into Measurement Table using pandas.merge in Python\nDESCRIPTION: This snippet enriches the main measurement DataFrame by merging it with station coordinates on the 'location' column. A left join ensures only station coordinates corresponding to measurements in air_quality are retained. After the merge, the head of the augmented DataFrame is shown. Dependencies: pandas, both air_quality and stations_coord tables.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nair_quality = pd.merge(air_quality, stations_coord, how=\"left\", on=\"location\")\nair_quality.head()\n```\n\n----------------------------------------\n\nTITLE: Setting DataFrame Columns and Values (Label & Position) - Python\nDESCRIPTION: Demonstrates adding a new column ('F') to a DataFrame by aligning a Series on the index, setting values by label using .at, setting by position using .iat, and assigning a numpy array to a column. Shows how DataFrames manage alignment and in-place modifications.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range(\"20130102\", periods=6))\\ns1\\ndf[\"F\"] = s1\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.at[dates[0], \"A\"] = 0\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.iat[0, 1] = 0\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[:, \"D\"] = np.array([5] * len(df))\n```\n\n----------------------------------------\n\nTITLE: Creating and Grouping DataFrame by Index Level and Column with pandas Grouper in Python\nDESCRIPTION: Creates a MultiIndex DataFrame and uses pandas Grouper to group both by index level and a column. It demonstrates initializing the DataFrame and grouping by the index level at position 1 and column 'A' for summation. Requires pandas and numpy. Expected output is a DataFrame grouped and summed by the specified index level and column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\narrays = [\n    [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n    [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n]\n\nindex = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\n\ndf = pd.DataFrame({\"A\": [1, 1, 1, 1, 2, 2, 3, 3], \"B\": np.arange(8)}, index=index)\n\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([pd.Grouper(level=1), \"A\"]).sum()\n```\n\n----------------------------------------\n\nTITLE: Looking up Values by Index/Column Labels in Pandas DataFrame\nDESCRIPTION: This code demonstrates how to extract values from a DataFrame given a sequence of row and column labels using pandas.factorize and NumPy indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'col': [\"A\", \"A\", \"B\", \"B\"],\n                   'A': [80, 23, np.nan, 22],\n                   'B': [80, 55, 76, 67]})\ndf\nidx, cols = pd.factorize(df['col'])\ndf.reindex(cols, axis=1).to_numpy()[np.arange(len(df)), idx]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Original and Transformed Data with pandas DataFrame and plot in Python\nDESCRIPTION: This snippet creates a DataFrame that compares original and transformed time series data and plots them. It depends on the previously created ts and transformed series and the pandas plotting backend. The plot helps visualize the effect of the group-wise standardization.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ncompare = pd.DataFrame({\"Original\": ts, \"Transformed\": transformed})\n\n@savefig groupby_transform_plot.png\ncompare.plot()\n```\n\n----------------------------------------\n\nTITLE: Group-Based Means for All Numeric Columns by Sex with pandas.groupby() - Python\nDESCRIPTION: This code groups the DataFrame by the 'Sex' column and computes the mean for all columns containing numeric data, by passing numeric_only=True to .mean(). This ensures that only numeric columns are averaged for each gender. Requires pandas version supporting numeric_only and a DataFrame with mixed data types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntitanic.groupby(\"Sex\").mean(numeric_only=True)\n```\n\n----------------------------------------\n\nTITLE: Setting Basic Plot Style with Line Series - pandas/matplotlib - Python\nDESCRIPTION: This snippet shows how to initialize a matplotlib figure and plot a pandas Series ('ts') with a custom line style ('k--') and label. It demonstrates passing style and label parameters to control the plot appearance. Requires pandas, matplotlib, and a defined variable 'ts'. The function ts.plot returns a formatted line plot.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\n@savefig series_plot_basic2.png\nts.plot(style=\"k--\", label=\"Series\");\n```\n\n----------------------------------------\n\nTITLE: Stacking Multiple Levels in MultiIndex Columns - pandas - Python\nDESCRIPTION: Demonstrates stacking over multiple column levels at once by passing a list of level names or indices. Inputs include a MultiIndex for columns constructed from tuples, and a DataFrame with random values. Stacking allows transforming complex, multi-dimensional data to a more hierarchical row index. Equivalent forms are given by specifying name or index of levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ncolumns = pd.MultiIndex.from_tuples(\n    [\n        (\"A\", \"cat\", \"long\"),\n        (\"B\", \"cat\", \"long\"),\n        (\"A\", \"dog\", \"short\"),\n        (\"B\", \"dog\", \"short\"),\n    ],\n    names=[\"exp\", \"animal\", \"hair_length\"],\n)\ndf = pd.DataFrame(np.random.randn(4, 4), columns=columns)\ndf\n\ndf.stack(level=[\"animal\", \"hair_length\"])\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Excel with MultiIndex using pandas - Python\nDESCRIPTION: Illustrates writing a DataFrame with MultiIndex rows and columns to an Excel file and reading it back using pandas. Highlights new support for this feature and the way to specify multi-level headers and index columns in read_excel. Requires pandas, and for writing/reading Excel, an Excel backend. Input is a DataFrame and file path; behavior is preserved for complex index structures.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    [[1, 2, 3, 4], [5, 6, 7, 8]],\n    columns=pd.MultiIndex.from_product(\n        [[\"foo\", \"bar\"], [\"a\", \"b\"]], names=[\"col1\", \"col2\"]\n    ),\n    index=pd.MultiIndex.from_product([[\"j\"], [\"l\", \"k\"]], names=[\"i1\", \"i2\"]),\n)\n\ndf\ndf.to_excel(\"test.xlsx\")\n\ndf = pd.read_excel(\"test.xlsx\", header=[0, 1], index_col=[0, 1])\ndf\n```\n\n----------------------------------------\n\nTITLE: Inclusive Label-Based Slicing with loc on Series - pandas - Python\nDESCRIPTION: Demonstrates correct usage of .loc for label-based inclusive slicing in pandas Series, where both start and end labels are included. This is practical for domains like time series, where next-label calculation is non-trivial. Requires Series 's' with appropriate string labels. Returns a new Series containing rows from index 'c' through 'e', inclusively.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\ns.loc[\"c\":\"e\"]\n```\n\n----------------------------------------\n\nTITLE: Creating Stratified Boxplots using the 'by' Parameter\nDESCRIPTION: This example shows how to create a grouped boxplot by passing a categorical column to the 'by' parameter. The data is grouped based on the values in column 'X'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(10, 2), columns=[\"Col1\", \"Col2\"])\ndf[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\nplt.figure();\n\nbp = df.boxplot(by=\"X\")\n```\n\n----------------------------------------\n\nTITLE: Generating Group Summary Statistics with pandas GroupBy.describe in Python\nDESCRIPTION: Shows usage of the describe method on a pandas GroupBy object to generate summary statistics for each group. The output presents key statistics for each group. Requires pandas and a DataFrame grouped by two columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ngrouped.describe()\n```\n\n----------------------------------------\n\nTITLE: Joining DataFrames by Index with DataFrame.join in pandas Python\nDESCRIPTION: Demonstrates joining two DataFrames using DataFrame.join by matching their index values. Shows default (left), outer, and inner joins on the index. Requires pandas. Inputs: DataFrames with possibly different indices; Output: joined DataFrame on index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame(\n    {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]}, index=[\"K0\", \"K1\", \"K2\"]\n)\n\nright = pd.DataFrame(\n    {\"C\": [\"C0\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D2\", \"D3\"]}, index=[\"K0\", \"K2\", \"K3\"]\n)\n\nresult = left.join(right)\nresult\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = left.join(right, how=\"outer\")\nresult\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = left.join(right, how=\"inner\")\nresult\n```\n\n----------------------------------------\n\nTITLE: Reading DataFrame from Feather File using pandas (Python)\nDESCRIPTION: This code reads a DataFrame from a Feather file ('example.feather') using pandas. All available dtypes are preserved during the process. Requires pandas and the feather file written previously. The outputs are the loaded DataFrame and its dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_204\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.read_feather(\"example.feather\")\nresult\n\n# we preserve dtypes\nresult.dtypes\n```\n\n----------------------------------------\n\nTITLE: Creating Conditional Column using NumPy where\nDESCRIPTION: Creates a new 'bucket' column in the tips DataFrame by categorizing total_bill values as 'low' or 'high' based on whether they are less than 10.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/if_then.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips[\"bucket\"] = np.where(tips[\"total_bill\"] < 10, \"low\", \"high\")\ntips\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns and Rows Using [] Operator - Python\nDESCRIPTION: This demonstrates selecting a column (df[\"A\"]) which returns a Series, and selecting rows using slice notation either by position (df[0:3]) or by date label (df[\"20130102\":\"20130104\"]). Standard for basic single-label and slice selection in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf[\"A\"]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf[0:3]\\ndf[\"20130102\":\"20130104\"]\n```\n\n----------------------------------------\n\nTITLE: Exporting a Styled DataFrame to Excel Using Styler.to_excel (Python)\nDESCRIPTION: This snippet exports a styled DataFrame to an Excel file using Styler.to_excel and the OpenPyXL engine. Requires pandas, a defined style_negative function, and either OpenPyXL or XlsxWriter. Input is a DataFrame styled via map and highlight_max; output is a file named 'styled.xlsx' with Excel styling reflecting the CSS2.2-like properties specified in previous steps. Limitations: only supported style properties and colors are preserved, and only individual cell styles (not table-level) are included.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf2.style.map(style_negative, props=\"color:red;\").highlight_max(axis=0).to_excel(\n    \"styled.xlsx\", engine=\"openpyxl\"\n)\n```\n\n----------------------------------------\n\nTITLE: Basic Pivot Table Aggregation - pandas - Python\nDESCRIPTION: This example calls pd.pivot_table to aggregate 'D' grouped by ['A', 'B'] as index and 'C' as columns. Numeric values in 'D' are aggregated (default function mean). Requires pandas and a properly structured DataFrame with the specified columns. Returns a DataFrame with grouped and aggregated values, possibly with MultiIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\npd.pivot_table(df, values=\"D\", index=[\"A\", \"B\"], columns=[\"C\"])\n```\n\n----------------------------------------\n\nTITLE: Detecting Missing Values with isna in pandas (Python)\nDESCRIPTION: These snippets demonstrate how to use pd.isna to detect missing values in pandas Series of various types, including those containing pd.Timestamp, pd.NaT, None, and different types of missing value sentinels (<NA>). isna works for datetime, object, and other types. These methods have no external dependencies beyond pandas. The main parameter is the input Series containing possibly-missing elements. Output is a boolean Series indicating which values are missing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([pd.Timestamp(\"2020-01-01\"), pd.NaT])\nser\npd.isna(ser)\n```\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, None], dtype=object)\nser\npd.isna(ser)\n```\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([True, None], dtype=\"boolean[pyarrow]\")\nser == pd.NA\npd.isna(ser)\n```\n\n----------------------------------------\n\nTITLE: Using get_dummies with DataFrame in Python\nDESCRIPTION: Demonstrates various ways to use pandas.get_dummies() with a DataFrame, including specifying columns and prefixes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [\"a\", \"b\", \"a\"], \"B\": [\"c\", \"c\", \"b\"], \"C\": [1, 2, 3]})\npd.get_dummies(df)\n\npd.get_dummies(df, columns=[\"A\"])\n\nsimple = pd.get_dummies(df, prefix=\"new_prefix\")\nsimple\nfrom_list = pd.get_dummies(df, prefix=[\"from_A\", \"from_B\"])\nfrom_list\nfrom_dict = pd.get_dummies(df, prefix={\"B\": \"from_B\", \"A\": \"from_A\"})\nfrom_dict\n```\n\n----------------------------------------\n\nTITLE: Efficient Built-in GroupBy Transformations in pandas (Replacing UDFs) in Python\nDESCRIPTION: This snippet demonstrates more efficient alternatives to groupby transformations using built-in string methods rather than user-defined functions. It replaces z-score standardization, range calculation, and groupwise fillna with highly optimized pandas code. Ensure that the variables ts, grouped, data_df, and key are defined.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# result = ts.groupby(lambda x: x.year).transform(\n#     lambda x: (x - x.mean()) / x.std()\n# )\ngrouped = ts.groupby(lambda x: x.year)\nresult = (ts - grouped.transform(\"mean\")) / grouped.transform(\"std\")\n\n# result = ts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min())\ngrouped = ts.groupby(lambda x: x.year)\nresult = grouped.transform(\"max\") - grouped.transform(\"min\")\n\n# grouped = data_df.groupby(key)\n# result = grouped.transform(lambda x: x.fillna(x.mean()))\ngrouped = data_df.groupby(key)\nresult = data_df.fillna(grouped.transform(\"mean\"))\n```\n\n----------------------------------------\n\nTITLE: Identifying and Removing Duplicate Rows in Pandas DataFrame\nDESCRIPTION: This snippet demonstrates how to use duplicated() and drop_duplicates() methods to identify and remove duplicate rows in a Pandas DataFrame. It shows different keep options and how to specify columns for duplication checks.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndf2 = pd.DataFrame({'a': ['one', 'one', 'two', 'two', 'two', 'three', 'four'],\n                   'b': ['x', 'y', 'x', 'y', 'x', 'x', 'x'],\n                   'c': np.random.randn(7)})\ndf2\ndf2.duplicated('a')\ndf2.duplicated('a', keep='last')\ndf2.duplicated('a', keep=False)\ndf2.drop_duplicates('a')\ndf2.drop_duplicates('a', keep='last')\ndf2.drop_duplicates('a', keep=False)\n\ndf2.duplicated(['a', 'b'])\ndf2.drop_duplicates(['a', 'b'])\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to XML with Custom Root and Row Names using pandas.to_xml (Python)\nDESCRIPTION: Demonstrates customization of the XML root and row element names when exporting a DataFrame with to_xml. Accepts root_name and row_name parameters for flexible XML structure. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_125\n\nLANGUAGE: python\nCODE:\n```\nprint(geom_df.to_xml(root_name=\"geometry\", row_name=\"objects\"))\n```\n\n----------------------------------------\n\nTITLE: Loading Air Quality Data from CSV with pandas in Python\nDESCRIPTION: This snippet reads a CSV file (data/air_quality_no2.csv) into a pandas DataFrame using specific options: index_col=0 sets the first column as the index, and parse_dates=True converts this index to Timestamp objects. The 'air_quality.head()' call shows the first rows for a quick data preview. Requires the CSV file to be present at the specified path. Outputs a DataFrame with date-indexed air quality measurements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nair_quality = pd.read_csv(\"data/air_quality_no2.csv\", index_col=0, parse_dates=True)\nair_quality.head()\n```\n\n----------------------------------------\n\nTITLE: Normalizing nested lists within JSON using pandas.json_normalize with record_path and meta - Python\nDESCRIPTION: This snippet normalizes a list of state entries, each with nested lists for counties, into a DataFrame of counties, carrying over metadata from the parent (state) dictionary structure. The json_normalize parameters record_path and meta are used to select which data to flatten. Dependencies: pandas; input: nested list-of-dict structure; output: flat DataFrame with state, shortname, governor info, and county details.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_81\n\nLANGUAGE: python\nCODE:\n```\ndata = [\n    {\n        \"state\": \"Florida\",\n        \"shortname\": \"FL\",\n        \"info\": {\"governor\": \"Rick Scott\"},\n        \"county\": [\n            {\"name\": \"Dade\", \"population\": 12345},\n            {\"name\": \"Broward\", \"population\": 40000},\n            {\"name\": \"Palm Beach\", \"population\": 60000},\n        ],\n    },\n    {\n        \"state\": \"Ohio\",\n        \"shortname\": \"OH\",\n        \"info\": {\"governor\": \"John Kasich\"},\n        \"county\": [\n            {\"name\": \"Summit\", \"population\": 1234},\n            {\"name\": \"Cuyahoga\", \"population\": 1337},\n        ],\n    },\n]\n\npd.json_normalize(data, \"county\", [\"state\", \"shortname\", [\"info\", \"governor\"]])\n```\n\n----------------------------------------\n\nTITLE: Using FixedForwardWindowIndexer for Forward-Looking Rolling Windows - pandas - Python\nDESCRIPTION: This code illustrates the use of FixedForwardWindowIndexer to compute forward-looking, fixed-width rolling windows. Instantiates the indexer (window_size=2) and applies it to a DataFrame to compute forward rolling sums. Requires pandas >= 1.3.0. Inputs: DataFrame, window size; Output: DataFrame of forward-rolling sums. This is useful where future knowledge is accessible and leads to forward-oriented window calculations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.indexers import FixedForwardWindowIndexer\nindexer = FixedForwardWindowIndexer(window_size=2)\ndf.rolling(indexer, min_periods=1).sum()\n```\n\n----------------------------------------\n\nTITLE: Constructing Arrow-Backed pandas Series/Index/DataFrame in Python\nDESCRIPTION: Demonstrates how to construct pandas Series, Index, and DataFrame objects with columns explicitly set to be Arrow-backed using the dtype syntax, such as \\\"float32[pyarrow]\\\" or \\\"bool[pyarrow]\\\". This approach enables support for missing data and efficient storage in pandas leveraging Arrow memory. Dependencies include pandas and pyarrow. Expected inputs are data arrays with appropriate value types and handling of None for missing values; outputs are pandas objects with Arrow-backed dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([-1.5, 0.2, None], dtype=\"float32[pyarrow]\")\nser\n```\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([True, None], dtype=\"bool[pyarrow]\")\nidx\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[1, 2], [3, 4]], dtype=\"uint64[pyarrow]\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Aligning pandas DataFrames and Series (Python)\nDESCRIPTION: Demonstrates aligning a DataFrame with a Series using the axis argument in pandas' align method. The Series is selected by slicing a DataFrame row, and alignment is performed on columns (axis=1). Requires pandas library and a compatible DataFrame and Series as inputs. Returns a tuple of aligned objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndf.align(df2.iloc[0], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrames by Multiple Columns using pandas in Python\nDESCRIPTION: This snippet demonstrates sorting a DataFrame by multiple columns ('sex' and 'total_bill') using the sort_values method in pandas. The code assumes that a DataFrame named 'tips' exists with these columns. The result is a new DataFrame sorted first by 'sex' and then by 'total_bill'. This operation does not modify the original DataFrame unless the 'inplace' parameter is set.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spss.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    tips.sort_values([\"sex\", \"total_bill\"])\n```\n\n----------------------------------------\n\nTITLE: Handling Categorical Data in pandas DataFrames in Python\nDESCRIPTION: Explains workflows for including and manipulating categorical data in pandas DataFrames. Includes conversion to categorical type, renaming and reordering categories, sorting, and groupby operations. Inputs are DataFrames with string labels, and outputs include categorical Series and grouped counts. Dependencies: pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\\n    {\"id\": [1, 2, 3, 4, 5, 6], \"raw_grade\": [\"a\", \"b\", \"b\", \"a\", \"a\", \"e\"]}\\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf[\"grade\"] = df[\"raw_grade\"].astype(\"category\")\\ndf[\"grade\"]\n```\n\nLANGUAGE: python\nCODE:\n```\nnew_categories = [\"very good\", \"good\", \"very bad\"]\\ndf[\"grade\"] = df[\"grade\"].cat.rename_categories(new_categories)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf[\"grade\"] = df[\"grade\"].cat.set_categories(\\n    [\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"]\\n)\\ndf[\"grade\"]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.sort_values(by=\"grade\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"grade\", observed=False).size()\n```\n\n----------------------------------------\n\nTITLE: Row and Column-wise Function Application using apply()\nDESCRIPTION: Shows how to use apply() method for row-wise and column-wise operations on DataFrames with various function types and return value handling.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf.apply(lambda x: np.mean(x))\ndf.apply(lambda x: np.mean(x), axis=1)\ndf.apply(lambda x: x.max() - x.min())\ndf.apply(np.cumsum)\ndf.apply(np.exp)\n\ndf.apply(\"mean\")\ndf.apply(\"mean\", axis=1)\n\ndef subtract_and_divide(x, sub, divide=1):\n    return (x - sub) / divide\n\ndf_udf = pd.DataFrame(np.ones((2, 2)))\ndf_udf.apply(subtract_and_divide, args=(5,), divide=3)\n```\n\n----------------------------------------\n\nTITLE: Negate, Multiply, and Absolute Value on Timedelta - pandas - Python\nDESCRIPTION: Demonstrates negation, multiplication by integer, and abs() on a Timedelta object. Useful for flipping direction of time difference or computing magnitude. Shows conversion of negative to positive durations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntd1 = pd.Timedelta(\"-1 days 2 hours 3 seconds\")\ntd1\n-1 * td1\n-td1\nabs(td1)\n```\n\n----------------------------------------\n\nTITLE: Converting Wide to Long Format using Melt\nDESCRIPTION: Reshaping data from wide to long format using melt() with custom column naming\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/07_reshape_table_layout.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nno2_pivoted = no2.pivot(columns=\"location\", values=\"value\").reset_index()\nno_2 = no2_pivoted.melt(\n    id_vars=\"date.utc\",\n    value_vars=[\"BETR801\", \"FR04014\", \"London Westminster\"],\n    value_name=\"NO_2\",\n    var_name=\"id_location\"\n)\n```\n\n----------------------------------------\n\nTITLE: Time Series DataFrame Creation and Basic Indexing\nDESCRIPTION: Creates a time series DataFrame with random values and demonstrates basic column selection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndates = pd.date_range('1/1/2000', periods=8)\ndf = pd.DataFrame(np.random.randn(8, 4),\n                 index=dates, columns=['A', 'B', 'C', 'D'])\ns = df['A']\ns[dates[5]]\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Categorical Data Types with Ordered Categories\nDESCRIPTION: Shows how to create a custom CategoricalDtype with specific ordered categories and use it when reading CSV data to enforce both category membership and ordering.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import CategoricalDtype\n\ndtype = CategoricalDtype([\"d\", \"c\", \"b\", \"a\"], ordered=True)\npd.read_csv(StringIO(data), dtype={\"col1\": dtype}).dtypes\n```\n\n----------------------------------------\n\nTITLE: Out of Bounds Slicing in DataFrame with .iloc\nDESCRIPTION: Shows how using slices that go out of bounds with .iloc can result in empty DataFrames or partial selections.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndfl = pd.DataFrame(np.random.randn(5, 2), columns=list('AB'))\ndfl\ndfl.iloc[:, 2:3]\ndfl.iloc[:, 1:3]\ndfl.iloc[4:6]\n```\n\n----------------------------------------\n\nTITLE: Comparing Conditional Setting: pandas.DataFrame.where vs numpy.where - Python\nDESCRIPTION: Demonstrates the equivalence between pandas' DataFrame.where and numpy.where, showing that both can be used for conditional assignment in DataFrame objects. It highlights pandas' alignment and missing value semantics, requiring pandas and numpy as dependencies. The main parameters are the boolean mask (m), the DataFrame to update (df1/df), and the alternative DataFrame (df2 or -df). Input: DataFrame and boolean condition; Output: DataFrame with selectively replaced values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndf.where(df < 0, -df) == np.where(df < 0, df, -df)\n```\n\n----------------------------------------\n\nTITLE: Row Filtering by Column Relationships and pandas.DataFrame.query - Python\nDESCRIPTION: Shows how to filter rows where column values satisfy chained inequalities, using both pure Python and query string syntax with DataFrame.query. Requires pandas and numpy. Demonstrates idiomatic filtering: (a < b) & (b < c). Inputs: Random-valued DataFrame with columns 'a', 'b', 'c'; Output: Filtered DataFrame according to the condition.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nn = 10\ndf = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\ndf\n\n# pure python\ndf[(df['a'] < df['b']) & (df['b'] < df['c'])]\n\n# query\ndf.query('(a < b) & (b < c)')\n```\n\n----------------------------------------\n\nTITLE: DataFrame Column Type Selection\nDESCRIPTION: Demonstrates the use of select_dtypes() method to filter DataFrame columns based on their data types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_89\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"string\": list(\"abc\"),\n        \"int64\": list(range(1, 4)),\n        \"uint8\": np.arange(3, 6).astype(\"u1\"),\n        \"float64\": np.arange(4.0, 7.0),\n        \"bool1\": [True, False, True],\n        \"bool2\": [False, True, False],\n        \"dates\": pd.date_range(\"now\", periods=3),\n        \"category\": pd.Series(list(\"ABC\")).astype(\"category\"),\n    }\n)\ndf[\"tdeltas\"] = df.dates.diff()\ndf[\"uint64\"] = np.arange(3, 6).astype(\"u8\")\ndf[\"other_dates\"] = pd.date_range(\"20130101\", periods=3)\ndf[\"tz_aware_dates\"] = pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\")\n\ndf.select_dtypes(include=[bool])\ndf.select_dtypes(include=[\"number\", \"bool\"], exclude=[\"unsignedinteger\"])\ndf.select_dtypes(include=[\"object\"])\n```\n\n----------------------------------------\n\nTITLE: Value-based Selection with Boolean Indexing - pandas - Python\nDESCRIPTION: Shows how to use boolean Series masks for selecting rows in a MultiIndex DataFrame, enabling value-dependent filtering. The mask is built via comparison against a column, and .loc is used with IndexSlice. Outputs a filtered DataFrame; requires the input mask, pandas, np, and a suitable DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nmask = dfmi[(\"a\", \"foo\")] > 200\ndfmi.loc[idx[mask, :, [\"C1\", \"C3\"]], idx[:, \"foo\"]]\n```\n\n----------------------------------------\n\nTITLE: Using High-Level HDF5 API with to_hdf and read_hdf in Pandas\nDESCRIPTION: Demonstrates the top-level API for HDF5 storage using to_hdf for writing and read_hdf for reading with query capabilities. The 'where' parameter allows filtering during read operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_167\n\nLANGUAGE: python\nCODE:\n```\ndf_tl = pd.DataFrame({\"A\": list(range(5)), \"B\": list(range(5))})\ndf_tl.to_hdf(\"store_tl.h5\", key=\"table\", append=True)\npd.read_hdf(\"store_tl.h5\", \"table\", where=[\"index>2\"])\n```\n\n----------------------------------------\n\nTITLE: Reading CSVs with Thousands Separators - pandas - Python\nDESCRIPTION: Shows how to read numbers containing thousands separators from a CSV file. Demonstrates both default treatment (as string) and correct parsing using the thousands parameter. Requires pandas and a filesystem.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndata = (\n    \"ID|level|category\\n\"\n    \"Patient1|123,000|x\\n\"\n    \"Patient2|23,000|y\\n\"\n    \"Patient3|1,234,018|z\"\n)\n\nwith open(\"tmp.csv\", \"w\") as fh:\n    fh.write(data)\n\ndf = pd.read_csv(\"tmp.csv\", sep=\"|\")\ndf\n\ndf.level.dtype\n```\n\n----------------------------------------\n\nTITLE: Groupby with Categorical Grouper (observed=True) - pandas Python\nDESCRIPTION: Counts entries grouped by a 'Categorical' variable, but with observed=True, filtering to only observed (appearing) categories. Good for optimizing result size and relevance. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\npd.Series([1, 1, 1]).groupby(\n    pd.Categorical([\"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]), observed=True\n).count()\n```\n\n----------------------------------------\n\nTITLE: Transposing a Pandas DataFrame (Python)\nDESCRIPTION: This snippet shows how to transpose a pandas DataFrame using the T attribute, which flips rows and columns, similar to NumPy ndarray transpose. Prerequisite: pandas. The first 5 rows of df are transposed and displayed; input is a DataFrame, output is its transpose.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# only show the first 5 rows\ndf[:5].T\n```\n\n----------------------------------------\n\nTITLE: Inferring International (Day-First) Date Formats - pandas - Python\nDESCRIPTION: Writes out a short CSV with ambiguous day-first dates, then reads it twice: once with default settings, once with dayfirst=True. Shows the difference in date parsing. Requires pandas and a writable filesystem.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndata = \"date,value,cat\\n1/6/2000,5,a\\n2/6/2000,10,b\\n3/6/2000,15,c\"\nprint(data)\nwith open(\"tmp.csv\", \"w\") as fh:\n    fh.write(data)\n\npd.read_csv(\"tmp.csv\", parse_dates=[0])\npd.read_csv(\"tmp.csv\", dayfirst=True, parse_dates=[0])\n```\n\n----------------------------------------\n\nTITLE: Multi-Column Conditional Assignment in Python\nDESCRIPTION: Assigns values to multiple columns based on a condition. This technique allows for modifying several columns at once when a condition is met.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[df.AAA >= 5, [\"BBB\", \"CCC\"]] = 555\ndf\n```\n\n----------------------------------------\n\nTITLE: Custom NA String Indicators in read_csv - pandas - Python\nDESCRIPTION: Shows how to designate custom NA string values (here \"NA\" and \"0\") while disregarding the default indicators, using keep_default_na=False. Requires pandas and a CSV input file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"NA\", \"0\"])\n```\n\n----------------------------------------\n\nTITLE: Partial Indexing with MultiIndex in Python\nDESCRIPTION: Demonstrates partial indexing where only the first level is specified, allowing selection of all elements matching that first level value in a hierarchical index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[\"bar\"]\n```\n\n----------------------------------------\n\nTITLE: Customizing and Saving a Matplotlib Plot using pandas DataFrame in Python\nDESCRIPTION: This snippet demonstrates advanced plot customization: a figure and axes are created with plt.subplots, then air_quality.plot.area draws the data on the specified axes object. The y-axis label is set using axs.set_ylabel, and the full figure is saved to disk with fig.savefig. plt.show() displays the plot. Requires pandas and Matplotlib, plus a writable filesystem. Outputs both a customized plot and a saved image file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(figsize=(12, 4))\nair_quality.plot.area(ax=axs)\naxs.set_ylabel(\"NO$_2$ concentration\")\nfig.savefig(\"no2_concentrations.png\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Filling and Reindexing a pandas Series using Various Methods (Python)\nDESCRIPTION: Illustrates reindexing a Series to match another's index and filling missing data using forward fill, backward fill, and nearest fill methods. Requires pandas and numpy for random number generation and working with date ranges. Key parameters are the desired method ('ffill', 'bfill', 'nearest') and the target index. Inputs are a Series with a subset index, outputs are reindexed Series with filled values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nrng = pd.date_range(\"1/3/2000\", periods=8)\nts = pd.Series(np.random.randn(8), index=rng)\nts2 = ts.iloc[[0, 3, 6]]\nts\nts2\n\nts2.reindex(ts.index)\nts2.reindex(ts.index, method=\"ffill\")\nts2.reindex(ts.index, method=\"bfill\")\nts2.reindex(ts.index, method=\"nearest\")\n```\n\n----------------------------------------\n\nTITLE: Resetting Index After Aggregation with pandas GroupBy in Python\nDESCRIPTION: Shows how to reset the index on an aggregated DataFrame after groupby aggregation. Useful for converting MultiIndex result to a simple flat DataFrame with group keys as columns. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([\"A\", \"B\"]).agg(\"sum\").reset_index()\n```\n\n----------------------------------------\n\nTITLE: Reading Remote Files with Custom HTTP Headers using Pandas (Python)\nDESCRIPTION: Demonstrates passing custom HTTP headers using the storage_options parameter while reading a remote CSV file. This enables the user to control request headers for authentication or analytics. Requires pandas 1.3.0+, a valid URL, and permits setting custom headers in a dictionary.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nheaders = {\"User-Agent\": \"pandas\"}\ndf = pd.read_csv(\n    \"https://download.bls.gov/pub/time.series/cu/cu.item\",\n    sep=\"\\t\",\n    storage_options=headers\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting DataFrame Subsets with Integer Slicing\nDESCRIPTION: Demonstrates different ways to select parts of a DataFrame using .iloc with integer slices for rows and columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf1.iloc[:3]\ndf1.iloc[1:5, 2:4]\n```\n\n----------------------------------------\n\nTITLE: Aligning DataFrames with DataFrame.align() - pandas - Python\nDESCRIPTION: Aligns two DataFrames on both index and columns using DataFrame.align(), with support for join strategies and axis restriction. Shows both full and axis-specific alignment. Inputs: two DataFrames, join method, optional axis. Outputs: tuple of aligned DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndf.align(df2, join=\"inner\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.align(df2, join=\"inner\", axis=0)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Boolean Mask for DataFrame Filtering in pandas\nDESCRIPTION: This example shows how to create a boolean mask based on a condition, inspect its values, and use it to filter a DataFrame. It creates a mask for dinner entries and applies it to the 'tips' DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/filtering.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nis_dinner = tips[\"time\"] == \"Dinner\"\nis_dinner\nis_dinner.value_counts()\ntips[is_dinner]\n```\n\n----------------------------------------\n\nTITLE: Basic Scatter Plot with Pandas\nDESCRIPTION: This example shows how to create a scatter plot using DataFrame.plot.scatter(). The x and y parameters specify which columns to use for the x and y axes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(50, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\ndf[\"species\"] = pd.Categorical(\n    [\"setosa\"] * 20 + [\"versicolor\"] * 20 + [\"virginica\"] * 10\n)\n\ndf.plot.scatter(x=\"a\", y=\"b\");\n```\n\n----------------------------------------\n\nTITLE: Sorting pandas DataFrame by Values in Single or Multiple Columns (Python)\nDESCRIPTION: Shows how to sort a DataFrame based on one or more columns using the sort_values method and the by parameter. Accepts either a string or a list of column names to determine sorting precedence. Dependencies: pandas. Input: DataFrame. Output: sorted DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(\n    {\"one\": [2, 1, 1, 1], \"two\": [1, 3, 2, 4], \"three\": [5, 4, 3, 2]}\n)\ndf1.sort_values(by=\"two\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf1[[\"one\", \"two\", \"three\"]].sort_values(by=[\"one\", \"two\"])\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Age Grouped by Sex with pandas.groupby() - Python\nDESCRIPTION: This snippet computes the average age for male and female Titanic passengers by first sub-selecting 'Sex' and 'Age' columns, then grouping by 'Sex', and finally applying .mean(). The dependencies are a DataFrame with at least 'Sex' and 'Age' columns. The output is a Series mapping each gender to its average age, ignoring missing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntitanic[[\"Sex\", \"Age\"]].groupby(\"Sex\").mean()\n```\n\n----------------------------------------\n\nTITLE: Custom Converter Function for Missing Values in Excel Read\nDESCRIPTION: Demonstrates using a custom function with the converters parameter to handle missing values while converting data types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_140\n\nLANGUAGE: python\nCODE:\n```\ndef cfun(x):\n    return int(x) if x else -1\n\n\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyInts\": cfun})\n```\n\n----------------------------------------\n\nTITLE: Creating and Inspecting an HDFStore Object in Pandas\nDESCRIPTION: Creates an HDFStore object which provides a dict-like interface for reading and writing pandas objects to HDF5 format using PyTables. The store is initialized empty.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_162\n\nLANGUAGE: python\nCODE:\n```\nstore = pd.HDFStore(\"store.h5\")\nprint(store)\n```\n\n----------------------------------------\n\nTITLE: Joining DataFrames in Pandas (Equivalent to SQL JOIN)\nDESCRIPTION: These snippets demonstrate how to perform various types of joins (INNER, LEFT, RIGHT, FULL) in pandas using the merge function, which is equivalent to SQL JOINs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npd.merge(df1, df2, on=\"key\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge(df1, df2, on=\"key\", how=\"left\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge(df1, df2, on=\"key\", how=\"right\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge(df1, df2, on=\"key\", how=\"outer\")\n```\n\n----------------------------------------\n\nTITLE: Creating a pandas Series from a List - Python\nDESCRIPTION: This snippet creates a pandas Series named 's' from a list of values, including a missing value represented by np.nan. The Series will have a default integer RangeIndex. No special dependencies beyond pandas and numpy are necessary.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 3, 5, np.nan, 6, 8])\\ns\n```\n\n----------------------------------------\n\nTITLE: Creating Series from Dictionary - pandas - Python\nDESCRIPTION: Shows how to construct a Series directly from a Python dictionary, mapping keys to index labels and values to data. This method auto-establishes the index based on dict keys, and assigns corresponding values. No prior dependencies besides pandas and an appropriate dictionary variable.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nd = {\"b\": 1, \"a\": 0, \"c\": 2}\npd.Series(d)\n```\n\n----------------------------------------\n\nTITLE: Chunking Large Datasets with Parquet Files in Python\nDESCRIPTION: Illustrates how to work with large datasets by chunking them into smaller Parquet files. This example creates multiple time series files and demonstrates an out-of-core value_counts operation across all files.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/scale.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pathlib\n\nN = 12\nstarts = [f\"20{i:>02d}-01-01\" for i in range(N)]\nends = [f\"20{i:>02d}-12-13\" for i in range(N)]\n\npathlib.Path(\"data/timeseries\").mkdir(exist_ok=True)\n\nfor i, (start, end) in enumerate(zip(starts, ends)):\n    ts = make_timeseries(start=start, end=end, freq=\"1min\", seed=i)\n    ts.to_parquet(f\"data/timeseries/ts-{i:0>2d}.parquet\")\n\n# Out-of-core value_counts\nfiles = pathlib.Path(\"data/timeseries/\").glob(\"ts*.parquet\")\ncounts = pd.Series(dtype=int)\nfor path in files:\n    df = pd.read_parquet(path)\n    counts = counts.add(df[\"name\"].value_counts(), fill_value=0)\ncounts.astype(int)\n```\n\n----------------------------------------\n\nTITLE: Selecting a Group with Multiple Keys using get_group with pandas in Python\nDESCRIPTION: Demonstrates how to retrieve a group by a tuple of key values after grouping by multiple columns (['A', 'B']). The tuple (\"bar\", \"one\") is used to extract the corresponding group. Requires a DataFrame and pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([\"A\", \"B\"]).get_group((\"bar\", \"one\"))\n```\n\n----------------------------------------\n\nTITLE: Reading Excel File with Pandas\nDESCRIPTION: Shows how to read an Excel file into a pandas DataFrame using the read_excel function. This is similar to opening an Excel file in spreadsheet software.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntips_df = pd.read_excel(\"./tips.xlsx\", index_col=0)\n```\n\n----------------------------------------\n\nTITLE: DataFrame Column Swapping\nDESCRIPTION: Demonstrates different methods of swapping column values in a DataFrame, including correct and incorrect approaches.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf[['B', 'A']] = df[['A', 'B']]\ndf.loc[:, ['B', 'A']] = df[['A', 'B']].to_numpy()\ndf.iloc[:, [1, 0]] = df[['A', 'B']]\n```\n\n----------------------------------------\n\nTITLE: Performing Joins with pandas DataFrame.merge in Python\nDESCRIPTION: These Python code snippets showcase how to perform various SQL-style joins (inner, left, right, outer) between two pandas DataFrames (df1 and df2) using the .merge method. The how keyword specifies the type of join to execute, and the on keyword indicates the joining key column(s), in this case, [\"key\"]. No data sorting is required for these operations. The output is a DataFrame containing the result of the specified join type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/merge.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ninner_join = df1.merge(df2, on=[\"key\"], how=\"inner\")\ninner_join\n```\n\nLANGUAGE: python\nCODE:\n```\nleft_join = df1.merge(df2, on=[\"key\"], how=\"left\")\nleft_join\n```\n\nLANGUAGE: python\nCODE:\n```\nright_join = df1.merge(df2, on=[\"key\"], how=\"right\")\nright_join\n```\n\nLANGUAGE: python\nCODE:\n```\nouter_join = df1.merge(df2, on=[\"key\"], how=\"outer\")\nouter_join\n```\n\n----------------------------------------\n\nTITLE: Iterating through DataFrame columns in Python\nDESCRIPTION: This code demonstrates the semantic approach to iterating through columns in a pandas DataFrame. This pattern enables more readable code by leveraging pandas' intuitive column-based structure instead of using axis numbers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/overview.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfor col in df.columns:\n    series = df[col]\n    # do something with series\n```\n\n----------------------------------------\n\nTITLE: Custom Describe Aggregation with Partial Functions - pandas - Python\nDESCRIPTION: Defines custom quantile partial functions for 25% and 75% quantiles, then aggregates a DataFrame with a list of standard and custom statistical functions using .agg(). Enables creating a custom 'describe' output tailored to user needs. Dependencies: pandas, functools.partial. Inputs: list of functions, including partials. Output: DataFrame with each statistic as a row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom functools import partial\n\nq_25 = partial(pd.Series.quantile, q=0.25)\nq_25.__name__ = \"25%\"\nq_75 = partial(pd.Series.quantile, q=0.75)\nq_75.__name__ = \"75%\"\n\ntsdf.agg([\"count\", \"mean\", \"std\", \"min\", q_25, \"median\", q_75, \"max\"])\n```\n\n----------------------------------------\n\nTITLE: Constructing DataFrame from Records with from_records() - Pandas - Python\nDESCRIPTION: Initializes a DataFrame from a list of tuples or a numpy ndarray with structured dtype, optionally setting an index field. This is analogous to DataFrame constructor but allows for explicit indexing by record field. Depends on prior definition of 'data' variable and requires pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndata\npd.DataFrame.from_records(data, index=\"C\")\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame with NA values for Groupby - pandas Python\nDESCRIPTION: Constructs a DataFrame 'df' with both float and NA values to demonstrate how groupby handles missing group keys using the dropna parameter. Useful for understanding how NA groups are included or excluded. Dependencies: pandas, numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"key\": [1.0, 1.0, np.nan, 2.0, np.nan], \"A\": [1, 2, 3, 4, 5]})\ndf\n```\n\n----------------------------------------\n\nTITLE: Replacing Values In-Place in pandas DataFrame (Python)\nDESCRIPTION: Illustrates usage of the inplace parameter with the replace method in pandas to modify a DataFrame without creating a new object. Requires pandas. The example replaces all occurrences of the value 5 in df and applies the changes directly to df (since inplace=True). No new variable is needed; the operation mutates the original object and returns None.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/copies.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf.replace(5, inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Reading XML Data from URL into DataFrame - pandas - Python\nDESCRIPTION: Shows how to read XML data directly from a remote URL using pandas 'read_xml', producing a DataFrame. This snippet fetches an example XML from W3Schools and converts it to a DataFrame. Internet access and a working pandas installation (>=1.3.0) are required; the URL must be publicly accessible and return valid XML.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_113\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_xml(\"https://www.w3schools.com/xml/books.xml\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Assigning New Columns with pandas DataFrame.assign (Python)\nDESCRIPTION: Demonstrates using the DataFrame.assign method to add new columns by inserting precomputed values or applying functions to an existing DataFrame. Dependencies include pandas as pd and access to the Iris dataset file (data/iris.data). The method returns a new DataFrame including both original and new columns, without modifying the original. Key parameters include column names as keyword arguments, accepting either arrays, Series, or functions applied to the DataFrame. Outputs are DataFrames with additional computed columns. Limitations: DataFrame.assign does not alter the original DataFrame in place.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\niris = pd.read_csv('data/iris.data')\niris.head()\n\niris.assign(sepal_ratio=iris['SepalWidth'] / iris['SepalLength']).head()\n```\n\n----------------------------------------\n\nTITLE: Simplifying MultiIndex Slicing with IndexSlice - pandas - Python\nDESCRIPTION: Uses pandas.IndexSlice for cleaner, more pythonic selection syntax in MultiIndex DataFrames, particularly when using ':' instead of explicit slice(None). This approach increases code readability for complex indexing. Expects valid DataFrame and pandas installation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.IndexSlice\ndfmi.loc[idx[:, :, [\"C1\", \"C3\"]], idx[:, \"foo\"]]\n```\n\n----------------------------------------\n\nTITLE: Slicing a MultiIndex with xs in Python\nDESCRIPTION: Demonstrates how to slice a MultiIndex using the xs method. This example extracts cross-sections from different levels of a hierarchical index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ncoords = [(\"AA\", \"one\"), (\"AA\", \"six\"), (\"BB\", \"one\"), (\"BB\", \"two\"), (\"BB\", \"six\")]\nindex = pd.MultiIndex.from_tuples(coords)\ndf = pd.DataFrame([11, 22, 33, 44, 55], index, [\"MyData\"])\ndf\n\n# Note : level and axis are optional, and default to zero\ndf.xs(\"BB\", level=0, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Determining the Type and Shape of a Selected Column in pandas (Python)\nDESCRIPTION: These snippets show how to check the type and shape of a column selected from a DataFrame in pandas. The first uses type() to verify that selecting a single column returns a Series, while the second accesses the shape attribute to get the number of rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntype(titanic[\"Age\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Age\"].shape\n```\n\n----------------------------------------\n\nTITLE: Merging with Nearest Key Using merge_asof - pandas - Python\nDESCRIPTION: These snippets exemplify pandas.merge_asof, which matches each left DataFrame row with the nearest (not necessarily equal) key in a right DataFrame. Examples include group-wise merges using the by parameter, tolerance windows to limit key differences, and options for allowing/excluding exact matches. Both DataFrames must be sorted by the on key, and pandas is required. Output is a DataFrame where each left row is augmented with the most recent matching right row within specified constraints.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ntrades = pd.DataFrame(\n    {\n        \"time\": pd.to_datetime(\n            [\n                \"20160525 13:30:00.023\",\n                \"20160525 13:30:00.038\",\n                \"20160525 13:30:00.048\",\n                \"20160525 13:30:00.048\",\n                \"20160525 13:30:00.048\",\n            ]\n        ),\n        \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n        \"price\": [51.95, 51.95, 720.77, 720.92, 98.00],\n        \"quantity\": [75, 155, 100, 100, 100],\n    },\n    columns=[\"time\", \"ticker\", \"price\", \"quantity\"],\n)\n\nquotes = pd.DataFrame(\n    {\n        \"time\": pd.to_datetime(\n            [\n                \"20160525 13:30:00.023\",\n                \"20160525 13:30:00.023\",\n                \"20160525 13:30:00.030\",\n                \"20160525 13:30:00.041\",\n                \"20160525 13:30:00.048\",\n                \"20160525 13:30:00.049\",\n                \"20160525 13:30:00.072\",\n                \"20160525 13:30:00.075\",\n            ]\n        ),\n        \"ticker\": [\"GOOG\", \"MSFT\", \"MSFT\", \"MSFT\", \"GOOG\", \"AAPL\", \"GOOG\", \"MSFT\"],\n        \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n        \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],\n    },\n    columns=[\"time\", \"ticker\", \"bid\", \"ask\"],\n)\ntrades\nquotes\npd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\", tolerance=pd.Timedelta(\"2ms\"))\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge_asof(\n    trades,\n    quotes,\n    on=\"time\",\n    by=\"ticker\",\n    tolerance=pd.Timedelta(\"10ms\"),\n    allow_exact_matches=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Label-Aligned Arithmetic Between Different Series - pandas - Python\nDESCRIPTION: Shows addition between partially overlapping slices of a Series, with automatic alignment based on labels and handling of missing data with NaN when indexes do not match. Demonstrates one of pandas' core features: index-aware data operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ns.iloc[1:] + s.iloc[:-1]\n```\n\n----------------------------------------\n\nTITLE: Calculating Median Age and Fare in Titanic Data with pandas - Python\nDESCRIPTION: This snippet computes the median values for both the 'Age' and 'Fare' columns from the Titanic DataFrame. By selecting these columns as a DataFrame slice and using the .median() method, it returns a Series with the median for each column. Input requirements are a DataFrame with 'Age' and 'Fare', and missing data is excluded by default.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntitanic[[\"Age\", \"Fare\"]].median()\n```\n\n----------------------------------------\n\nTITLE: Timedelta Arithmetic Operations on Series/DataFrames - pandas - Python\nDESCRIPTION: Demonstrates subtracting and adding Timedelta and datetime.timedelta objects to pandas Series of datetime64[ns] values, and creating DataFrames from such Series. Requires pandas, datetime, and numpy. Shows the creation of new columns through arithmetic and the resulting dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\ntd = pd.Series([pd.Timedelta(days=i) for i in range(3)])\ndf = pd.DataFrame({\"A\": s, \"B\": td})\ndf\ndf[\"C\"] = df[\"A\"] + df[\"B\"]\ndf\ndf.dtypes\n\ns - s.max()\ns - datetime.datetime(2011, 1, 1, 3, 5)\ns + datetime.timedelta(minutes=5)\ns + pd.offsets.Minute(5)\ns + pd.offsets.Minute(5) + pd.offsets.Milli(5)\n```\n\n----------------------------------------\n\nTITLE: Using String Accessor Methods with StringDtype Series (Python)\nDESCRIPTION: Demonstrates the use of .str.count() on a StringDtype Series, showing that results are of pandas' nullable Int64 dtype regardless of NA presence. Highlights .str accessor functions acting consistently for string dtype and difference in handling missing data. Input is a Series of strings (with a missing value); outputs are counts per element before and after dropping NAs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", None, \"b\"], dtype=\"string\")\ns\ns.str.count(\"a\")\ns.dropna().str.count(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Using Numpy Nullable Data Types with Pandas CSV Reader\nDESCRIPTION: Shows how to use the dtype_backend='numpy_nullable' parameter to ensure all columns are parsed with nullable dtypes, providing better type safety and representation of missing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndata = \"\"\"a,b,c,d,e,f,g,h,i,j\n1,2.5,True,a,,,,,12-31-2019,\n3,4.5,False,b,6,7.5,True,a,12-31-2019,\n\"\"\"\n\ndf = pd.read_csv(StringIO(data), dtype_backend=\"numpy_nullable\", parse_dates=[\"i\"])\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Concatenating Dictionary of DataFrames in Python\nDESCRIPTION: Example of passing a dictionary to concat where the dict keys are used as the keys argument for creating a MultiIndex in the result.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npieces = {\"x\": df1, \"y\": df2, \"z\": df3}\nresult = pd.concat(pieces)\nresult\n```\n\n----------------------------------------\n\nTITLE: Merging Ordered DataFrames with merge_ordered - pandas - Python\nDESCRIPTION: This example applies pandas.merge_ordered to combine DataFrames containing order-based or time-series data. It demonstrates optional missing-data filling using the fill_method parameter and left_by to group result rows. pandas is required, and the input DataFrames should be appropriately sorted or structured. The output is a merged and forward-filled DataFrame honoring groupings if left_by is provided.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame(\n    {\"k\": [\"K0\", \"K1\", \"K1\", \"K2\"], \"lv\": [1, 2, 3, 4], \"s\": [\"a\", \"b\", \"c\", \"d\"]}\n)\n\nright = pd.DataFrame({\"k\": [\"K1\", \"K2\", \"K4\"], \"rv\": [1, 2, 3]})\n\npd.merge_ordered(left, right, fill_method=\"ffill\", left_by=\"s\")\n```\n\n----------------------------------------\n\nTITLE: Binning Continuous Data in Python\nDESCRIPTION: Shows how to use pandas.cut() to transform continuous variables into discrete or categorical variables.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nages = np.array([10, 15, 13, 12, 23, 25, 28, 59, 60])\n\npd.cut(ages, bins=3)\n\npd.cut(ages, bins=[0, 18, 35, 70])\n```\n\n----------------------------------------\n\nTITLE: Creating a Series with Data and Index - pandas - Python\nDESCRIPTION: Demonstrates creating a pandas Series object by supplying generic 'data' and an 'index'. 'data' can be various types (ndarray, dict, or scalar), and 'index' is a list of axis labels matching the data length. The resulting Series supports label-based indexing and serves as the fundamental 1D data structure in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(data, index=index)\n```\n\n----------------------------------------\n\nTITLE: Callable Replacement Function in str.replace (Python)\nDESCRIPTION: Demonstrates passing a Python function (callable) as the replacement argument in str.replace, enabling dynamic replacement logic such as reversing each matched word or working with regex groups. Exemplifies advanced text transformations using custom logic inside regex replacement. Input is Series of strings; output reflects callable transformation. Requires pandas and Python's re module if using regex groups.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Reverse every lowercase alphabetic word\npat = r\"[a-z]+\"\n\ndef repl(m):\n    return m.group(0)[::-1]\n\npd.Series([\"foo 123\", \"bar baz\", np.nan], dtype=\"string\").str.replace(\n    pat, repl, regex=True\n)\n\n# Using regex groups\npat = r\"(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)\"\n\ndef repl(m):\n    return m.group(\"two\").swapcase()\n\npd.Series([\"Foo Bar Baz\", np.nan], dtype=\"string\").str.replace(\n    pat, repl, regex=True\n)\n```\n\n----------------------------------------\n\nTITLE: Merging DataFrames with Indicator Column using pandas - Python\nDESCRIPTION: Displays how to use the 'indicator' argument in pd.merge to add an origin-tracking column. DataFrames with overlapping and unique keys are merged, and the _merge column records the source(s) for each row. Requires pandas; input are two DataFrames and merge args. Output is the merged DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame({\"col1\": [0, 1], \"col_left\": [\"a\", \"b\"]})\ndf2 = pd.DataFrame({\"col1\": [1, 2, 2], \"col_right\": [2, 2, 2]})\npd.merge(df1, df2, on=\"col1\", how=\"outer\", indicator=True)\n```\n\n----------------------------------------\n\nTITLE: Iterating Over DataFrame Items as (Label, Series) Pairs (Python)\nDESCRIPTION: Shows how to use the items method to iterate over a DataFrame, returning pairs of (column label, Series). Each iteration yields the column name and its corresponding Series. Requires pandas, and is suitable for key-value style traversal.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nfor label, ser in df.items():\n    print(label)\n    print(ser)\n```\n\n----------------------------------------\n\nTITLE: Merging on Duplicate Keys with Outer Join in pandas Python\nDESCRIPTION: Shows performing an outer join with DataFrames having duplicate join keys, resulting in a larger DataFrame due to the cartesian product of matching keys. Highlights potential for increased memory usage. Requires pandas. Inputs: DataFrames with duplicate keys; Output: merged DataFrame with all key matches.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame({\"A\": [1, 2], \"B\": [2, 2]})\n\nright = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [2, 2, 2]})\n\nresult = pd.merge(left, right, on=\"B\", how=\"outer\")\nresult\n```\n\n----------------------------------------\n\nTITLE: Pivot Table with Implicit Values - pandas - Python\nDESCRIPTION: This example demonstrates creating a pivot table from a DataFrame with multiple value columns, omitting the 'values' argument. The result includes all available data as an extra hierarchical level in the columns. Especially useful when you want to see all numeric data summarizations. Requires pandas and a DataFrame with relevant columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\npd.pivot_table(df[[\"A\", \"B\", \"C\", \"D\", \"E\"]], index=[\"A\", \"B\"], columns=[\"C\"])\n```\n\n----------------------------------------\n\nTITLE: Multiple Aggregations with Expanding Window\nDESCRIPTION: Shows how to apply multiple aggregation functions to an expanding window.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": range(5), \"B\": range(10, 15)})\ndf.expanding().agg([\"sum\", \"mean\", \"std\"])\n```\n\n----------------------------------------\n\nTITLE: Seeding Random Data and Line Plotting with Series.plot (Python)\nDESCRIPTION: Shows how to seed NumPy's random generator, construct a cumulative sum Series with a date index, and plot it using pandas' Series.plot(). The Series holds 1000 data points indexed by dates. Output is a simple time series line plot. Pandas, numpy, and matplotlib are required dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(123456)\\n\\nts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\\nts = ts.cumsum()\\n\\n@savefig series_plot_basic.png\\nts.plot();\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame Rows to Remove Duplicated Indexes - pandas - Python\nDESCRIPTION: Shows how to drop DataFrame rows where the index is duplicated by using the negated result of Index.duplicated as a boolean filter. Produces a DataFrame containing only the first occurrence of each index label. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf2.loc[~df2.index.duplicated(), :]\n```\n\n----------------------------------------\n\nTITLE: Aggregating by Multiple Keys with as_index=False using pandas GroupBy.agg in Python\nDESCRIPTION: Demonstrates aggregating by multiple columns using groupby with as_index=False, so grouping keys appear as columns. Shows both groupby followed by agg and column selection then aggregation. Requires pandas. Output DataFrame has group keys as columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby([\"A\", \"B\"], as_index=False)\ngrouped.agg(\"sum\")\n\ndf.groupby(\"A\", as_index=False)[[\"C\", \"D\"]].agg(\"sum\")\n```\n\n----------------------------------------\n\nTITLE: Reading JSON into pandas DataFrame with custom dtypes - Python\nDESCRIPTION: This snippet reads a JSON file into a pandas DataFrame, specifying custom dtypes for columns using the dtype parameter. The resulting DataFrame includes the specified column types. Requires pandas, and the 'test.json' file with expected structure. Input is a JSON file path and desired dtype mapping; output is a DataFrame with dtypes as specified.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_76\n\nLANGUAGE: python\nCODE:\n```\npd.read_json(\"test.json\", dtype={\"A\": \"float32\", \"bools\": \"int8\"}).dtypes\n```\n\n----------------------------------------\n\nTITLE: Localizing and Accessing Timezones with pandas Series (Python)\nDESCRIPTION: This snippet demonstrates how to localize a datetime Series to the 'US/Eastern' timezone using the .dt.tz_localize method and how to access the timezone information using the .dt.tz accessor. The Series must have a datetime-like dtype, and pandas must be imported. Inputs: Series of datetime values. Outputs: Localized Series and timezone object. Raises TypeError for non-datetime dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nstz = s.dt.tz_localize(\"US/Eastern\")\nstz\nstz.dt.tz\n```\n\n----------------------------------------\n\nTITLE: Comparing Group Means and NA Counts Before and After Groupwise Imputation with pandas in Python\nDESCRIPTION: This snippet compares group means and non-NA counts between the original and groupwise imputed datasets after filling missing values with group mean. It uses the earlier data_df, grouped, transformed, and key variables. Outputs include means and counts before and after imputation to verify correctness.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ngrouped_trans = transformed.groupby(key)\n\ngrouped.mean()  # original group means\ngrouped_trans.mean()  # transformation did not change group means\n\ngrouped.count()  # original has some missing data points\ngrouped_trans.count()  # counts after transformation\ngrouped_trans.size()  # Verify non-NA count equals group size\n```\n\n----------------------------------------\n\nTITLE: Filtering Groups Based on Aggregate Criteria with pandas DataFrameGroupBy.filter in Python\nDESCRIPTION: These snippets demonstrate advanced filtration using pandas' groupby filter, allowing selection of groups based on group-wise aggregate criteria. Examples include filtering groups by sum or count, controlling dropna behavior, and specifying columns for filtering. Input objects include Series or DataFrame grouped by various keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nsf = pd.Series([1, 1, 2, 3, 3, 3])\nsf.groupby(sf).filter(lambda x: x.sum() > 2)\n```\n\nLANGUAGE: python\nCODE:\n```\ndff = pd.DataFrame({\"A\": np.arange(8), \"B\": list(\"aabbbbcc\")})\ndff.groupby(\"B\").filter(lambda x: len(x) > 2)\n```\n\nLANGUAGE: python\nCODE:\n```\ndff.groupby(\"B\").filter(lambda x: len(x) > 2, dropna=False)\n```\n\nLANGUAGE: python\nCODE:\n```\ndff[\"C\"] = np.arange(8)\ndff.groupby(\"B\").filter(lambda x: len(x[\"C\"]) > 2)\n```\n\n----------------------------------------\n\nTITLE: Basic Series Operations with .loc Label-Based Indexing\nDESCRIPTION: Creates a Series with alphabetic labels and demonstrates .loc for accessing elements by label, slicing, and setting values based on labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series(np.random.randn(6), index=list('abcdef'))\ns1\ns1.loc['c':]\ns1.loc['b']\n```\n\n----------------------------------------\n\nTITLE: Reindexing with Limit and Tolerance Constraints in pandas Series (Python)\nDESCRIPTION: Demonstrates using the limit parameter to restrict consecutive forward fills and the tolerance parameter to constrain filling based on index distance. The tolerance string is converted to a Timedelta for a DatetimeIndex. Dependencies are pandas and Series objects with time-based indices. Inputs include filling method, limit, and tolerance; outputs are Series with conditional fills.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nts2.reindex(ts.index, method=\"ffill\", limit=1)\n```\n\nLANGUAGE: python\nCODE:\n```\nts2.reindex(ts.index, method=\"ffill\", tolerance=\"1 day\")\n```\n\n----------------------------------------\n\nTITLE: Chaining Datetime Localization and Conversion in pandas Series (Python)\nDESCRIPTION: This code snippet shows how to chain .dt.tz_localize and .dt.tz_convert on a pandas Series, first localizing as 'UTC' and then converting to 'US/Eastern'. Inputs: Series with datetime values. No explicit dependencies are shown but assumes pandas and suitable datetime dtype. Output: timezone-converted Series object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ns.dt.tz_localize(\"UTC\").dt.tz_convert(\"US/Eastern\")\n```\n\n----------------------------------------\n\nTITLE: Numeric Type Conversion\nDESCRIPTION: Shows usage of pd.to_numeric() for converting objects to numeric data types with examples of error handling and downcasting options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_86\n\nLANGUAGE: python\nCODE:\n```\nm = [\"1.1\", 2, 3]\npd.to_numeric(m)\n\nm = [\"apple\", 2, 3]\npd.to_numeric(m, errors=\"coerce\")\n\nm = [\"1\", 2, 3]\npd.to_numeric(m, downcast=\"integer\")\npd.to_numeric(m, downcast=\"signed\")\npd.to_numeric(m, downcast=\"unsigned\")\npd.to_numeric(m, downcast=\"float\")\n```\n\n----------------------------------------\n\nTITLE: Assigning New Columns in Chained Method Calls with assign() - Pandas - Python\nDESCRIPTION: Shows how to use DataFrame.assign to create new columns within method chains without mutating the original DataFrame. Demonstrates assigning both precomputed values and callable functions; supports dependent assignments using kwargs order preservation. Requires pandas as pd and optionally uses read_csv for loading example data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\niris = pd.read_csv(\"data/iris.data\")\niris.head()\niris.assign(sepal_ratio=iris[\"SepalWidth\"] / iris[\"SepalLength\"]).head()\n```\n\nLANGUAGE: python\nCODE:\n```\niris.assign(sepal_ratio=lambda x: (x[\"SepalWidth\"] / x[\"SepalLength\"])).head()\n```\n\nLANGUAGE: python\nCODE:\n```\n@savefig basics_assign.png\n(\n    iris.query(\"SepalLength > 5\")\n    .assign(\n        SepalRatio=lambda x: x.SepalWidth / x.SepalLength,\n        PetalRatio=lambda x: x.PetalWidth / x.PetalLength,\n    )\n    .plot(kind=\"scatter\", x=\"SepalRatio\", y=\"PetalRatio\")\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndfa = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\ndfa.assign(C=lambda x: x[\"A\"] + x[\"B\"], D=lambda x: x[\"A\"] + x[\"C\"])\n```\n\n----------------------------------------\n\nTITLE: Reindexing to Align with Another Object Using reindex_like - pandas - Python\nDESCRIPTION: Aligns a DataFrame to have the same index and columns as another DataFrame using .reindex_like(). Shows chaining of operations and mean-centering for demonstration. Input: DataFrames; Output: reindexed DataFrame matching the template.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.reindex([\"a\", \"b\", \"c\"], columns=[\"one\", \"two\"])\ndf3 = df2 - df2.mean()\ndf2\ndf3\ndf.reindex_like(df2)\n```\n\n----------------------------------------\n\nTITLE: Specifying Custom Column Names When Reading CSV Data\nDESCRIPTION: Shows how to provide custom column names using the names parameter and how to control whether the original header row is used or skipped with the header parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprint(data)\npd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=0)\npd.read_csv(StringIO(data), names=[\"foo\", \"bar\", \"baz\"], header=None)\n```\n\n----------------------------------------\n\nTITLE: Defining a Sample Function That Returns a Random Float in Python\nDESCRIPTION: This snippet defines a simple Python function named sample that generates and returns a random float between 0 and 1, using NumPy's random.random(). The docstring includes a detailed description of the function and specifies the type and purpose of the returned value using the 'Returns' section in standard numpy-style documentation. Dependencies: numpy (typically imported as np). Input: none. Output: a single random float. This example demonstrates correct return annotation in docstrings and systematic function documentation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef sample():\n    \"\"\"\n    Generate and return a random number.\n\n    The value is sampled from a continuous uniform distribution between\n    0 and 1.\n\n    Returns\n    -------\n    float\n        Random number generated.\n    \"\"\"\n    return np.random.random()\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Nullable Float dtypes in pandas Series (Python)\nDESCRIPTION: Illustrates the behavior of the new nullable float dtypes (Float32, Float64) that use pd.NA as missing indicator instead of np.nan. Shows both default and new behaviors, comparing how operations handle missing values. Dependencies: pandas. Inputs: Series with None/NA values; Output: Series with nullable float types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# the default NumPy float64 dtype\ns1 = pd.Series([1.5, None])\ns1\ns1 > 1\n```\n\nLANGUAGE: python\nCODE:\n```\n# the new nullable float64 dtype\ns2 = pd.Series([1.5, None], dtype=\"Float64\")\ns2\ns2 > 1\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([1.5, None], dtype=pd.Float32Dtype())\n```\n\n----------------------------------------\n\nTITLE: Parsing Fixed Width Files with Pandas\nDESCRIPTION: Demonstrates reading fixed-width format data using pandas read_fwf function with specified column widths and specifications\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ndata1 = (\n    \"id8141    360.242940   149.910199   11950.7\\n\"\n    \"id1594    444.953632   166.985655   11788.4\\n\"\n    \"id1849    364.136849   183.628767   11806.2\\n\"\n    \"id1230    413.836124   184.375703   11916.8\\n\"\n    \"id1948    502.953953   173.237159   12468.3\"\n)\nwith open(\"bar.csv\", \"w\") as f:\n    f.write(data1)\n\n# Column specifications are a list of half-intervals\ncolspecs = [(0, 6), (8, 20), (21, 33), (34, 43)]\ndf = pd.read_csv(\"bar.csv\", colspecs=colspecs, header=None, index_col=0)\n```\n\n----------------------------------------\n\nTITLE: Using Python DB-API with sqlite3 and pandas (Python)\nDESCRIPTION: Presents direct usage of sqlite3 for creating in-memory or file-based SQLite connections, and running pandas SQL I/O without SQLAlchemy. Dependencies are the standard library's sqlite3 and pandas. The code creates a connection, writes a DataFrame to SQL, and retrieves it via raw query; suitable for lightweight, file- or memory-based SQLite operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_234\n\nLANGUAGE: python\nCODE:\n```\nimport sqlite3\n\ncon = sqlite3.connect(\":memory:\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndata.to_sql(\"data\", con)\npd.read_sql_query(\"SELECT * FROM data\", con)\n```\n\n----------------------------------------\n\nTITLE: Aggregating DataFrame by Group Key with pandas GroupBy.sum in Python\nDESCRIPTION: Shows how to aggregate (sum) DataFrame columns after grouping by a column, here 'kind', using pandas groupby and sum. The DataFrame 'animals' is constructed with animal types, heights, and weights, which are then grouped and summed. Input is a DataFrame, output is a grouped, aggregated DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nanimals = pd.DataFrame(\n    {\n        \"kind\": [\"cat\", \"dog\", \"cat\", \"dog\"],\n        \"height\": [9.1, 6.0, 9.5, 34.0],\n        \"weight\": [7.9, 7.5, 9.9, 198.0],\n    }\n)\nanimals\nanimals.groupby(\"kind\").sum()\n```\n\n----------------------------------------\n\nTITLE: Anonymous Public S3 Data Access using Pandas (Python)\nDESCRIPTION: Illustrates reading public S3 data without credentials by passing {'anon': True} to the storage_options parameter in read_csv. This is useful for downloading open datasets from Amazon S3, relying on s3fs/fsspec. File path must refer to a public S3 bucket.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(\n    \"s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/SaKe2013\"\n    \"-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"anon\": True},\n)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating groupby behavior with observed=False in pandas 3.0.0\nDESCRIPTION: This snippet shows how the behavior of groupby with observed=False has been improved in pandas 3.0.0, particularly for multiple groupings. It now passes unobserved groups to the provided function consistently.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v3.0.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"key1\": pd.Categorical(list(\"aabb\"), categories=list(\"abc\")),\n        \"key2\": [1, 1, 1, 2],\n        \"values\": [1, 2, 3, 4],\n    }\n)\ndf\ngb = df.groupby(\"key1\", observed=False)\ngb[[\"values\"]].apply(lambda x: x.sum())\n\ngb = df.groupby([\"key1\", \"key2\"], observed=False)\ngb[[\"values\"]].apply(lambda x: x.sum())\n```\n\n----------------------------------------\n\nTITLE: Conditional Column Assignment with numpy.where - Python\nDESCRIPTION: Demonstrates assigning a new column in a DataFrame based on conditionally computed values using numpy.where. Requires pandas and numpy. Sets 'color' to 'green' if 'col2' is 'Z', otherwise 'red', for each row. Inputs: DataFrame with 'col1' and 'col2'; Output: DataFrame with new 'color' column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'col1': list('ABBC'), 'col2': list('ZZXY')})\ndf['color'] = np.where(df['col2'] == 'Z', 'green', 'red')\ndf\n```\n\n----------------------------------------\n\nTITLE: Loading Titanic Dataset from CSV with pandas - Python\nDESCRIPTION: This snippet demonstrates how to read the Titanic dataset stored in a CSV file named 'data/titanic.csv' into a pandas DataFrame and display its first few rows with .head(). It requires the pandas library and assumes the CSV file exists at the specified path. The main output is the display of the first five rows for initial data inspection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntitanic = pd.read_csv(\"data/titanic.csv\")\ntitanic.head()\n```\n\n----------------------------------------\n\nTITLE: Reading JSON while Disabling Data Conversion - pandas - Python\nDESCRIPTION: Reads a JSON file using pd.read_json, setting dtype=object to prevent automatic data conversion, but still converting axes and date columns. Returns the data types of resulting DataFrame columns. Inputs: file path. Outputs: Series of dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_75\n\nLANGUAGE: python\nCODE:\n```\npd.read_json(\"test.json\", dtype=object).dtypes\n```\n\n----------------------------------------\n\nTITLE: Grouping by Index Level Name with pandas GroupBy in Python\nDESCRIPTION: Demonstrates how to group a pandas Series by an index level name using the GroupBy API in Python. The snippet assumes 's' is a MultiIndex Series with a level named 'second'. It calls groupby with level=\"second\" and then sums the grouped values. Requires pandas and an appropriately constructed Series with MultiIndex. The input is 's', and the output is a Series grouped and summed by the specified level.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ns.groupby(level=\"second\").sum()\n```\n\n----------------------------------------\n\nTITLE: Grouping a Single Series via GroupBy after Column Selection with pandas in Python\nDESCRIPTION: Shows an alternative approach to grouping a DataFrame column using groupby by directly selecting the 'C' column, then grouping by column 'A'. It explains a less efficient, but explicit alternative, and highlights avoidance of recalculating group information. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf[\"C\"].groupby(df[\"A\"])\n```\n\n----------------------------------------\n\nTITLE: Timedelta Arithmetic with Scalars - pandas - Python\nDESCRIPTION: Illustrates arithmetic with timedelta64[ns] Series and scalars (differences between Series elements). The variable y demonstrates time difference between Series elements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ny = s - s[0]\ny\n```\n\n----------------------------------------\n\nTITLE: Converting pyarrow Table to pandas DataFrame (Python)\nDESCRIPTION: Demonstrates how to convert a pyarrow.Table into a pandas DataFrame while preserving Arrow-based data types by using the types_mapper=pd.ArrowDtype argument in to_pandas. Dependencies are pandas and pyarrow. The input is a pyarrow.Table; output is a pandas DataFrame with ArrowExtensionArray dtypes. Outputs are both the DataFrame and its dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntable = pa.table([pa.array([1, 2, 3], type=pa.int64())], names=[\"a\"])\n\ndf = table.to_pandas(types_mapper=pd.ArrowDtype)\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Creating and Indexing with IntervalIndex\nDESCRIPTION: Shows how to create a DataFrame with an IntervalIndex and demonstrates label-based indexing with intervals, including selecting exact intervals and handling interval containment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"A\": [1, 2, 3, 4]}, index=pd.IntervalIndex.from_breaks([0, 1, 2, 3, 4])\n)\ndf\n\ndf.loc[2]\ndf.loc[[2, 3]]\n\ndf.loc[2.5]\ndf.loc[[2.5, 3.5]]\n\ndf.loc[pd.Interval(1, 2)]\n```\n\n----------------------------------------\n\nTITLE: Specifying Categorical Data Types in Pandas CSV Reader\nDESCRIPTION: Demonstrates how to parse columns as categorical data by using dtype='category' parameter, which automatically converts unique values in the columns to categories.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndata = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\n\npd.read_csv(StringIO(data))\npd.read_csv(StringIO(data)).dtypes\npd.read_csv(StringIO(data), dtype=\"category\").dtypes\n```\n\n----------------------------------------\n\nTITLE: Handling NULL Values in Pandas (Equivalent to SQL IS NULL and IS NOT NULL)\nDESCRIPTION: These snippets demonstrate how to handle NULL (NaN in pandas) values using isna() and notna() methods, which are equivalent to IS NULL and IS NOT NULL in SQL.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nframe = pd.DataFrame(\n    {\"col1\": [\"A\", \"B\", np.nan, \"C\", \"D\"], \"col2\": [\"F\", np.nan, \"G\", \"H\", \"I\"]}\n)\nframe[frame[\"col2\"].isna()]\n```\n\nLANGUAGE: python\nCODE:\n```\nframe[frame[\"col1\"].notna()]\n```\n\n----------------------------------------\n\nTITLE: Reindexing Series with Specific Labels\nDESCRIPTION: Demonstrates the idiomatic way to select potentially not-found elements using .reindex() which can change the order and add missing labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3])\ns.reindex([1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Transforming DataFrame Columns Selectively with Dict - pandas - Python\nDESCRIPTION: Uses a dictionary to specify column-wise transformation functions with .transform(), allowing for selective application of NumPy or custom functions by column. Supports lists for multi-function per column, yielding MultiIndexed columns in output. Inputs: DataFrame and dict of functions/lists. Output: DataFrame of transformed results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ntsdf.transform({\"A\": np.abs, \"B\": lambda x: x + 1})\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf.transform({\"A\": np.abs, \"B\": [lambda x: x + 1, \"sqrt\"]})\n```\n\n----------------------------------------\n\nTITLE: Assigning Sorted DataFrame to a New Variable with pandas (Python)\nDESCRIPTION: Demonstrates how to retain the result of pandas DataFrame sorting by assigning the output of sort_values to a new variable. No external dependencies are required beyond pandas. The main parameter is the column name by which to sort (here, \\\"col1\\\"). This produces a new DataFrame with sorted rows, leaving the original DataFrame unchanged.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/copies.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsorted_df = df.sort_values(\"col1\")\n```\n\n----------------------------------------\n\nTITLE: Initializing DataFrame and Performing GroupBy Boxplot in Python\nDESCRIPTION: Creates a DataFrame with random data and generates a boxplot grouped by a categorical column. The result is a dictionary of boxplots for each group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ndf[\"g\"] = np.random.choice([\"A\", \"B\"], size=50)\ndf.loc[df[\"g\"] == \"B\", 1] += 3\n\n@savefig groupby_boxplot.png\ndf.groupby(\"g\").boxplot()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Time Series Index Frequency with pandas - Python\nDESCRIPTION: This snippet demonstrates how to access the frequency attribute (freq) of a DatetimeIndex, providing insight into the regularity of the time series data. No external dependencies are required apart from pandas; the variable 'monthly_max' is expected to be a pandas object with a datetime-like index. The output reveals the frequency string or object, such as 'M' for month-end, or None if the frequency is not set.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/09_timeseries.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmonthly_max.index.freq\n```\n\n----------------------------------------\n\nTITLE: Drawing a Box Plot from a pandas DataFrame in Python\nDESCRIPTION: This snippet calls air_quality.plot.box() to create a boxplot visualizing the distribution of NO2 measurements per column, and plt.show() to render the plot. No parameters are required beyond the DataFrame. Outputs a box plot that highlights the spread and outliers in the data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nair_quality.plot.box()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Building Table Schemas from pandas Series with build_table_schema - Python\nDESCRIPTION: Shows the use of pandas.io.json.build_table_schema to derive a Table Schema-compliant description from a pandas Series, including handling of datetime fields, timezones, periods (with frequency), categoricals (with ordered/enum), and primary key information. Dependencies: pandas; inputs: Series with different types/indices; output: schema as Python dict. Useful for validation and interoperability.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_86\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.io.json import build_table_schema\n\ns = pd.Series(pd.date_range(\"2016\", periods=4))\nbuild_table_schema(s)\n```\n\nLANGUAGE: python\nCODE:\n```\ns_tz = pd.Series(pd.date_range(\"2016\", periods=12, tz=\"US/Central\"))\nbuild_table_schema(s_tz)\n```\n\nLANGUAGE: python\nCODE:\n```\ns_per = pd.Series(1, index=pd.period_range(\"2016\", freq=\"Y-DEC\", periods=4))\nbuild_table_schema(s_per)\n```\n\nLANGUAGE: python\nCODE:\n```\ns_cat = pd.Series(pd.Categorical([\"a\", \"b\", \"a\"]))\nbuild_table_schema(s_cat)\n```\n\nLANGUAGE: python\nCODE:\n```\ns_dupe = pd.Series([1, 2], index=[1, 1])\nbuild_table_schema(s_dupe)\n```\n\nLANGUAGE: python\nCODE:\n```\ns_multi = pd.Series(1, index=pd.MultiIndex.from_product([(\"a\", \"b\"), (0, 1)]))\nbuild_table_schema(s_multi)\n```\n\n----------------------------------------\n\nTITLE: Raising DuplicateLabelError upon Renaming with Disallowed Duplicates - pandas - Python\nDESCRIPTION: Attempts to rename DataFrame rows to uppercase when allows_duplicate_labels is False; if this results in duplicates, a DuplicateLabelError is raised. Demonstrates how strict label policy interacts with mutative operations. Input is DataFrame with allows_duplicate_labels False; output is exception if duplicates are created.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf.rename(str.upper)\n```\n\n----------------------------------------\n\nTITLE: Coercing Mixed Types to StringDtype in pandas Series (Python)\nDESCRIPTION: Shows how to initialize a pandas Series from a list containing both strings, numbers, and missing (np.nan) values while coercing all entries to strings with StringDtype. The output Series holds all data as string dtype, also demonstrates element-level type using type(s[1]). Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", 2, np.nan], dtype=\"string\")\ns\ntype(s[1])\n```\n\n----------------------------------------\n\nTITLE: Reading Data from Clipboard in Pandas\nDESCRIPTION: Uses pandas.read_clipboard() to import data that has been copied to the system clipboard, returning a DataFrame. This is useful for quickly importing small datasets.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_153\n\nLANGUAGE: python\nCODE:\n```\n>>> clipdf = pd.read_clipboard()\n>>> clipdf\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n```\n\n----------------------------------------\n\nTITLE: Data Type Specification in CSV Reading\nDESCRIPTION: Shows different approaches to specify data types when reading CSV data, including using object type for all columns and mixed dtype specifications.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\ndata = \"a,b,c,d\\n1,2,3,4\\n5,6,7,8\\n9,10,11\"\nprint(data)\n\ndf = pd.read_csv(StringIO(data), dtype=object)\ndf\ndf[\"a\"][0]\ndf = pd.read_csv(StringIO(data), dtype={\"b\": object, \"c\": np.float64, \"d\": \"Int64\"})\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Showing Enhanced DataFrame.info Output with Verbose Mode in pandas 1.0.0 (Python)\nDESCRIPTION: Displays creation of a sample DataFrame and invokes the info method in verbose mode, which now includes line numbers for columns in pandas 1.0.0. Useful for inspecting DataFrame structure. Dependencies: pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"int_col\": [1, 2, 3],\n                   \"text_col\": [\"a\", \"b\", \"c\"],\n                   \"float_col\": [0.0, 0.1, 0.2]})\ndf.info(verbose=True)\n```\n\n----------------------------------------\n\nTITLE: Finding Index of Min/Max Values in Python with Pandas\nDESCRIPTION: This snippet demonstrates how to use the 'idxmin' and 'idxmax' methods to find the index labels of minimum and maximum values in a Series or DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series(np.random.randn(5))\ns1.idxmin(), s1.idxmax()\ndf1 = pd.DataFrame(np.random.randn(5, 3), columns=[\"A\", \"B\", \"C\"])\ndf1.idxmin(axis=0)\ndf1.idxmax(axis=1)\n```\n\n----------------------------------------\n\nTITLE: Comparing two DataFrames using the new compare method\nDESCRIPTION: Example of using the new DataFrame.compare() method to compare two DataFrames and summarize their differences. This method shows a row-by-row comparison of values that differ between the DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"],\n        \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0],\n        \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0]\n    },\n    columns=[\"col1\", \"col2\", \"col3\"],\n)\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.copy()\ndf2.loc[0, 'col1'] = 'c'\ndf2.loc[2, 'col3'] = 4.0\ndf2\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.compare(df2)\n```\n\n----------------------------------------\n\nTITLE: Creating and Merging DataFrames with pandas in Python\nDESCRIPTION: Illustrates creation of two DataFrames and merging them on a single key column using pandas' merge function, resulting in a many-to-many or cartesian join. The left and right DataFrames are created with matching keys and merged on the 'key' column. Requires pandas. Input: two DataFrames with a 'key' column; Output: merged DataFrame with all combinations for matching keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame(\n    {\n        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n    }\n)\n\nright = pd.DataFrame(\n    {\n        \"key\": [\"K0\", \"K1\", \"K2\", \"K3\"],\n        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n    }\n)\nresult = pd.merge(left, right, on=\"key\")\nresult\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing to Different SQL Schemas Using pandas (Python)\nDESCRIPTION: Illustrates the use of the schema keyword in pandas DataFrame.to_sql() and pandas.read_sql_table() for explicitly reading from or writing to SQL schemas other than the default. Requires an engine/connection to a database that supports schemas (e.g., PostgreSQL). Parameters allow setting both schema and table; results in tables or DataFrames from or to the specified schema.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_230\n\nLANGUAGE: python\nCODE:\n```\ndf.to_sql(name=\"table\", con=engine, schema=\"other_schema\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_table(\"table\", engine, schema=\"other_schema\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Age by Sex Using Grouped Selection - pandas.groupby() - Python\nDESCRIPTION: This snippet calculates the average (mean) age for each gender directly after grouping by 'Sex', using the grouped DataFrame's column selection and .mean(). It demonstrates pandas' ability to chain column selection after grouping for specific aggregate computations. Assumes the DataFrame has 'Sex' and 'Age' columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntitanic.groupby(\"Sex\")[\"Age\"].mean()\n```\n\n----------------------------------------\n\nTITLE: Comparing pandas.DataFrame.query and Pure Python Selection Syntax - Python\nDESCRIPTION: Compares various query string forms and their pure Python equivalents for selecting DataFrame rows by boolean logic, including the use of operators, chaining, and different expression styles. Requires pandas and numpy. Inputs: DataFrame; Output: Filtered DataFrames using both string and Python boolean expressions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=list('abc'))\ndf\ndf.query('(a < b) & (b < c)')\ndf[(df['a'] < df['b']) & (df['b'] < df['c'])]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.query('a < b & b < c')\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.query('a < b and b < c')\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.query('a < b < c')\n```\n\n----------------------------------------\n\nTITLE: Merging Series with MultiIndex and DataFrame in pandas Python\nDESCRIPTION: Demonstrates how to merge a DataFrame and a Series with a MultiIndex by resetting the Series index and joining on matching named columns. This allows matching on multiple levels of an index. Requires pandas. Inputs: DataFrame and Series with common MultiIndex/column keys; Output: merged DataFrame with corresponding Series values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"Let\": [\"A\", \"B\", \"C\"], \"Num\": [1, 2, 3]})\ndf\n\nser = pd.Series(\n    [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"],\n    index=pd.MultiIndex.from_arrays(\n        [[\"A\", \"B\", \"C\"] * 2, [1, 2, 3, 4, 5, 6]], names=[\"Let\", \"Num\"]\n    ),\n)\nser\n\npd.merge(df, ser.reset_index(), on=[\"Let\", \"Num\"])\n```\n\n----------------------------------------\n\nTITLE: Measuring String Lengths with pandas Series in Python\nDESCRIPTION: This snippet demonstrates how to calculate the number of characters in a column of a pandas DataFrame using the Series.str.len() accessor. It also shows chaining Series.str.rstrip() to remove trailing blanks before measuring length, ensuring accurate results for strings with potential trailing spaces. Requires pandas to be installed and a DataFrame 'tips' with at least a 'time' column containing string values; returns integer-valued Series objects representing string lengths before and after stripping trailing spaces.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/length.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips[\"time\"].str.len()\ntips[\"time\"].str.rstrip().str.len()\n```\n\n----------------------------------------\n\nTITLE: Unstacking by Named Levels - pandas - Python\nDESCRIPTION: Shows how to unstack a Series or DataFrame by specifying the name of the level in the MultiIndex instead of level number, improving code readability. Operates on a previously stacked object with appropriately named levels. Returns DataFrame with the target level as column index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nstacked.unstack(\"second\")\n```\n\n----------------------------------------\n\nTITLE: Applying String Accessor Methods with StringDtype - pandas - Python\nDESCRIPTION: This snippet demonstrates pandas string accessor methods on Series with the string extension dtype. It converts all strings to uppercase and then splits the strings on the character 'b', using the expand parameter. The dtypes attribute is printed to show the resulting dtypes after the split. Requires pandas 1.0.0+; demonstrates compatibility of string accessor methods with the new dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns.str.upper()\\ns.str.split('b', expand=True).dtypes\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Memory Usage Using pandas - Python\nDESCRIPTION: Demonstrates how to inspect DataFrame memory usage using the pandas DataFrame.info and DataFrame.memory_usage methods, including object dtype limitations and using the 'deep' memory reporting mode. Requires pandas and numpy to be installed. Shows initialization of multiple dtype columns, conversion to categorical, and memory inspection. Output consists of memory readouts for columns and DataFrame as a whole.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/gotchas.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndtypes = [\n    \"int64\",\n    \"float64\",\n    \"datetime64[ns]\",\n    \"timedelta64[ns]\",\n    \"complex128\",\n    \"object\",\n    \"bool\",\n]\nn = 5000\ndata = {t: np.random.randint(100, size=n).astype(t) for t in dtypes}\ndf = pd.DataFrame(data)\ndf[\"categorical\"] = df[\"object\"].astype(\"category\")\n\ndf.info()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.info(memory_usage=\"deep\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.memory_usage()\n\n# total memory usage of dataframe\ndf.memory_usage().sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.memory_usage(index=False)\n```\n\n----------------------------------------\n\nTITLE: Renaming and Setting Categories, CSV I/O - pandas - Python\nDESCRIPTION: Shows how to rename categories and set new category orders for a pandas Series with categorical dtype, export the DataFrame to a CSV via StringIO, and reload from CSV. Demonstrates the re-application of categories after import to restore categorical properties. Dependencies: pandas, io. Key parameters: category labels, category order. Input: DataFrame with categoricals; Output: DataFrame with restored categorical columns. Limitation: Categories must be reapplied when reading from CSV, as CSV does not retain categorical dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ns = s.cat.rename_categories([\"very good\", \"good\", \"bad\"])\n# reorder the categories and add missing categories\ns = s.cat.set_categories([\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"])\ndf = pd.DataFrame({\"cats\": s, \"vals\": [1, 2, 3, 4, 5, 6]})\ncsv = io.StringIO()\ndf.to_csv(csv)\ndf2 = pd.read_csv(io.StringIO(csv.getvalue()))\ndf2.dtypes\ndf2[\"cats\"]\n# Redo the category\ndf2[\"cats\"] = df2[\"cats\"].astype(\"category\")\ndf2[\"cats\"] = df2[\"cats\"].cat.set_categories(\n    [\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"]\n)\ndf2.dtypes\ndf2[\"cats\"]\n```\n\n----------------------------------------\n\nTITLE: Installing pandas via pip in Shell\nDESCRIPTION: Installs the stable release of the pandas library using the pip package manager, retrieving the package from PyPI. pip and Python must be installed and accessible in the shell environment. The command installs pandas and its dependencies, resulting in pandas being available in the Python environment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install pandas\n```\n\n----------------------------------------\n\nTITLE: Using OR Conditions for Selection in Python\nDESCRIPTION: Selects rows based on OR conditions using the | operator between criteria. This allows for selecting data that meets at least one of multiple conditions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= -40), \"AAA\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Values in a Series Using .loc\nDESCRIPTION: Shows how to modify Series values using label-based indexing with .loc by setting all values from 'c' onwards to 0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ns1.loc['c':] = 0\ns1\n```\n\n----------------------------------------\n\nTITLE: Handling Bad Lines with Custom Function in Pandas CSV Parser\nDESCRIPTION: Demonstrates using a callable function to handle bad lines when reading CSV data with the Python engine. The function processes lines with too many fields and returns the last three elements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nexternal_list = []\ndef bad_lines_func(line):\n    external_list.append(line)\n    return line[-3:]\npd.read_csv(StringIO(data), on_bad_lines=bad_lines_func, engine=\"python\")\nexternal_list\n```\n\n----------------------------------------\n\nTITLE: Basic Indexing with MultiIndex in Python\nDESCRIPTION: Demonstrates partial label selection with MultiIndex, showing how to select data by specifying values for one or more levels of the hierarchical index in DataFrames and Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf[\"bar\"]\ndf[\"bar\", \"one\"]\ndf[\"bar\"][\"one\"]\ns[\"qux\"]\n```\n\n----------------------------------------\n\nTITLE: Column-wise dtype Mapping with astype using a Dictionary - Python\nDESCRIPTION: Illustrates explicit conversion of specific columns to targeted dtypes by passing a dictionary to DataFrame.astype. Columns 'a' and 'c' are converted to boolean and float64, respectively. Dependencies are pandas and NumPy. Output includes the updated DataFrame and its dtypes after conversion.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_83\n\nLANGUAGE: python\nCODE:\n```\ndft1 = pd.DataFrame({\"a\": [1, 0, 1], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\ndft1 = dft1.astype({\"a\": np.bool_, \"c\": np.float64})\ndft1\ndft1.dtypes\n```\n\n----------------------------------------\n\nTITLE: Parsing and Constructing Timedeltas - pandas - Python\nDESCRIPTION: Demonstrates various ways to construct pandas Timedelta objects from strings, keyword arguments, integers with units, datetime.timedelta, np.timedelta64, and ISO 8601 duration strings. Requires pandas, numpy, and datetime modules. Shows usage of both positive and negative values, construction using different input types, and handling of NaT (Not a Time) values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\n# strings\npd.Timedelta(\"1 days\")\npd.Timedelta(\"1 days 00:00:00\")\npd.Timedelta(\"1 days 2 hours\")\npd.Timedelta(\"-1 days 2 min 3us\")\n\n# like datetime.timedelta\n# note: these MUST be specified as keyword arguments\npd.Timedelta(days=1, seconds=1)\n\n# integers with a unit\npd.Timedelta(1, unit=\"D\")\n\n# from a datetime.timedelta/np.timedelta64\npd.Timedelta(datetime.timedelta(days=1, seconds=1))\npd.Timedelta(np.timedelta64(1, \"ms\"))\n\n# negative Timedeltas have this string repr\n# to be more consistent with datetime.timedelta conventions\npd.Timedelta(\"-1us\")\n\n# a NaT\npd.Timedelta(\"nan\")\npd.Timedelta(\"nat\")\n\n# ISO 8601 Duration strings\npd.Timedelta(\"P0DT0H1M0S\")\npd.Timedelta(\"P0DT0H0M0.000000123S\")\n```\n\n----------------------------------------\n\nTITLE: Handling NA and Type Promotion for Integer Series in pandas - Python\nDESCRIPTION: Shows how pandas promotes dtype to float64 when introducing NA (missing values) into an integer Series, and illustrates usage of pandas nullable-integer and Arrow extension dtypes for missing integer support. Requires pandas. Outputs Series objects and their dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/gotchas.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3, 4, 5], index=list(\"abcde\"))\ns\ns.dtype\n\ns2 = s.reindex([\"a\", \"b\", \"c\", \"f\", \"u\"])\ns2\ns2.dtype\n```\n\nLANGUAGE: python\nCODE:\n```\ns_int = pd.Series([1, 2, 3, 4, 5], index=list(\"abcde\"), dtype=pd.Int64Dtype())\ns_int\ns_int.dtype\n\ns2_int = s_int.reindex([\"a\", \"b\", \"c\", \"f\", \"u\"])\ns2_int\ns2_int.dtype\n\ns_int_pa = pd.Series([1, 2, None], dtype=\"int64[pyarrow]\")\ns_int_pa\n```\n\n----------------------------------------\n\nTITLE: Performing DataFrame isin Comparison and Filtering (Python)\nDESCRIPTION: Shows how to use the DataFrame isin method for boolean masking across multiple columns against another DataFrame, followed by row selection where any condition matches. Dependencies: pandas. Inputs: two DataFrames ('dfi', 'other'). Outputs: boolean mask and filtered DataFrame. Enables expressive, vectorized subsetting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndfi = pd.DataFrame({'A': [1, 2, 3, 4], 'B': ['a', 'b', 'f', 'n']})\ndfi\nother = pd.DataFrame({'A': [1, 3, 3, 7], 'B': ['e', 'f', 'f', 'e']})\nmask = dfi.isin(other)\nmask\ndfi[mask.any(axis=1)]\n```\n\n----------------------------------------\n\nTITLE: Importing pandas and Reading CSV Data with pandas (Python)\nDESCRIPTION: This snippet imports the pandas library and reads the Titanic dataset from a CSV file into a pandas DataFrame. Users must have pandas installed (e.g., with pip install pandas) and ensure that the 'data/titanic.csv' file is present in the correct path. Inputs are the CSV filepath, and outputs are a DataFrame with Titanic passenger information.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\nLANGUAGE: python\nCODE:\n```\ntitanic = pd.read_csv(\"data/titanic.csv\")\ntitanic.head()\n```\n\n----------------------------------------\n\nTITLE: Interaction Between header and comment Parameters\nDESCRIPTION: Demonstrates how header uses row numbers (ignoring commented/empty lines) while parsing CSV data with comments.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndata = \"#comment\\na,b,c\\nA,B,C\\n1,2,3\"\npd.read_csv(StringIO(data), comment=\"#\", header=1)\n```\n\n----------------------------------------\n\nTITLE: Removing Unused Levels from MultiIndex in Python\nDESCRIPTION: Shows how to create a new MultiIndex with only the levels that are actually used in the data, using the remove_unused_levels method to eliminate unused level values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnew_mi = df[[\"foo\", \"qux\"]].columns.remove_unused_levels()\nnew_mi.levels\n```\n\n----------------------------------------\n\nTITLE: Writing Attribute-Centric XML with pandas.to_xml (Python)\nDESCRIPTION: Exports a DataFrame where all columns are rendered as XML attributes using the attr_cols argument. Useful for producing XML with attribute-driven semantics. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_126\n\nLANGUAGE: python\nCODE:\n```\nprint(geom_df.to_xml(attr_cols=geom_df.columns.tolist()))\n```\n\n----------------------------------------\n\nTITLE: Series Operations with .iloc Position-Based Indexing\nDESCRIPTION: Creates a Series with a non-contiguous integer index and demonstrates .iloc for positional indexing, which is 0-based regardless of the index values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series(np.random.randn(5), index=list(range(0, 10, 2)))\ns1\ns1.iloc[:3]\ns1.iloc[3]\n```\n\n----------------------------------------\n\nTITLE: Selective Column Type Conversion in a DataFrame Using astype and Slicing - Python\nDESCRIPTION: This snippet converts a subset of DataFrame columns (columns 'a' and 'b') to np.uint8 using .astype with slicing. Shows both the transformed DataFrame and its dtypes, requiring pandas and NumPy. Key input is a DataFrame with columns of convertible types. Outputs confirm partial transformation without affecting all columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_82\n\nLANGUAGE: python\nCODE:\n```\ndft = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\ndft[[\"a\", \"b\"]] = dft[[\"a\", \"b\"]].astype(np.uint8)\ndft\ndft.dtypes\n```\n\n----------------------------------------\n\nTITLE: Creating Categorical Data with pd.cut() in Python\nDESCRIPTION: Illustrates the use of pd.cut() to create categorical data by grouping continuous data into discrete bins. This method is useful for creating ordinal categorical data from numeric values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"value\": np.random.randint(0, 100, 20)})\nlabels = [\"{0} - {1}\".format(i, i + 9) for i in range(0, 100, 10)]\n\ndf[\"group\"] = pd.cut(df.value, range(0, 105, 10), right=False, labels=labels)\ndf.head(10)\n```\n\n----------------------------------------\n\nTITLE: Merge Validation with the validate Argument in pandas Python\nDESCRIPTION: Demonstrates use of the 'validate' argument in pandas' merge function to enforce key uniqueness, preventing unintentional cartesian products. Throws an error for duplicated keys if using 'one_to_one'. Requires pandas. Inputs: two DataFrames, join keys, validate argument; Output: merged DataFrame or error if validation fails.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame({\"A\": [1, 2], \"B\": [1, 2]})\nright = pd.DataFrame({\"A\": [4, 5, 6], \"B\": [2, 2, 2]})\nresult = pd.merge(left, right, on=\"B\", how=\"outer\", validate=\"one_to_one\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge(left, right, on=\"B\", how=\"outer\", validate=\"one_to_many\")\n```\n\n----------------------------------------\n\nTITLE: Rolling Apply Returning Scalar (VWAP) on DataFrame - Pandas Python\nDESCRIPTION: Shows how to perform a rolling calculation where the function returns a scalar, specifically the Volume Weighted Average Price (VWAP) across rolling windows of financial data. The snippet generates random financial data, defines the vwap function, and outputs a rounded Series representing the VWAP for each window. Dependencies: pandas, numpy. Input is a DataFrame with 'Open', 'Close', and 'Volume' columns; outputs a Series of rounded VWAP values for each window.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nrng = pd.date_range(start=\"2014-01-01\", periods=100)\ndf = pd.DataFrame(\n    {\n        \"Open\": np.random.randn(len(rng)),\n        \"Close\": np.random.randn(len(rng)),\n        \"Volume\": np.random.randint(100, 2000, len(rng)),\n    },\n    index=rng,\n)\ndf\n\ndef vwap(bars):\n    return (bars.Close * bars.Volume).sum() / bars.Volume.sum()\n\nwindow = 5\ns = pd.concat(\n    [\n        (pd.Series(vwap(df.iloc[i: i + window]), index=[df.index[i + window]]))\n        for i in range(len(df) - window)\n    ]\n)\ns.round(2)\n```\n\n----------------------------------------\n\nTITLE: Applying String Methods to a Series (str.lower) - Python\nDESCRIPTION: Creates a Series of strings, including NaN, and applies the .str.lower() method to convert all string elements to lowercase. Non-string (NaN) values remain unchanged. Demonstrates vectorized string operations built into pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"])\\ns.str.lower()\n```\n\n----------------------------------------\n\nTITLE: Basic Categorical Series Operations - Python\nDESCRIPTION: Demonstrates creating and inspecting categorical series including categories and unique values using pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(list(\"babc\")).astype(CategoricalDtype(list(\"abcd\")))\ns\n\n# categories\ns.cat.categories\n\n# uniques\ns.unique()\n```\n\n----------------------------------------\n\nTITLE: Getting and Setting Options using pandas.get_option and pandas.set_option - Python\nDESCRIPTION: Illustrates retrieving and updating DataFrame display threshold options using the function-based API. Requires only the pandas library. The code retrieves, updates, and verifies the value of \\\"display.chop_threshold\\\" using both the full option name and an unambiguous substring, showcasing the regex-like matching feature of the API. The input is the name or substring of the option; outputs are the current or updated option values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npd.get_option(\"display.chop_threshold\")\npd.set_option(\"display.chop_threshold\", 2)\npd.get_option(\"display.chop_threshold\")\npd.set_option(\"chop\", 4)\npd.get_option(\"display.chop_threshold\")\n```\n\n----------------------------------------\n\nTITLE: Sorting MultiIndex DataFrames by Both Index Levels and Columns (pandas, Python)\nDESCRIPTION: This snippet builds a MultiIndex DataFrame and sorts it by both an index level and a column using the by parameter. Inputs: DataFrame with MultiIndex, Output: DataFrame sorted by named index and column. Dependencies: pandas, numpy. Demonstrates the by parameter working for index levels and columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\n# Build MultiIndex\nidx = pd.MultiIndex.from_tuples(\n    [(\"a\", 1), (\"a\", 2), (\"a\", 2), (\"b\", 2), (\"b\", 1), (\"b\", 1)]\n)\nidx.names = [\"first\", \"second\"]\n\n# Build DataFrame\ndf_multi = pd.DataFrame({\"A\": np.arange(6, 0, -1)}, index=idx)\ndf_multi\n```\n\nLANGUAGE: python\nCODE:\n```\ndf_multi.sort_values(by=[\"second\", \"A\"])\n```\n\n----------------------------------------\n\nTITLE: Binning Data with IntervalIndex in Pandas\nDESCRIPTION: This snippet demonstrates how to use pd.cut with an IntervalIndex to bin age data into specified intervals. It creates bins from 0 to 40 and 40 to 70.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\npd.cut(ages, bins=pd.IntervalIndex.from_breaks([0, 40, 70]))\n```\n\n----------------------------------------\n\nTITLE: Accessing Datetime Properties with Series.dt Accessor (Python)\nDESCRIPTION: Demonstrates the use of the .dt accessor to extract datetime features (hour, second, day) from a Series with datetime values. Requires pandas; input is a datetime-like Series. Outputs are Series of extracted datetime components. Also shows how to filter by a datetime property.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\n# datetime\ns = pd.Series(pd.date_range(\"20130101 09:10:12\", periods=4))\ns\ns.dt.hour\ns.dt.second\ns.dt.day\n```\n\nLANGUAGE: python\nCODE:\n```\ns[s.dt.day == 2]\n```\n\n----------------------------------------\n\nTITLE: Multiple Bar Plot for Entire DataFrame (Python)\nDESCRIPTION: Creates a DataFrame of random values with four columns, then visualizes these as grouped bar plots using DataFrame.plot.bar(). Every column is represented with a set of grouped bars. Requires pandas, numpy, and matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf2 = pd.DataFrame(np.random.rand(10, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\\n\\n@savefig bar_plot_multi_ex.png\\ndf2.plot.bar();\n```\n\n----------------------------------------\n\nTITLE: Reading XML with StringIO or BytesIO into DataFrame - pandas - Python\nDESCRIPTION: Demonstrates how to load XML data from a file as either a StringIO or BytesIO object, useful for in-memory processing and compatibility with APIs expecting file-like objects. Each snippet reads file contents and passes the in-memory stream to 'read_xml' to produce a DataFrame. Requires 'from io import StringIO, BytesIO' and the XML file to exist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_115\n\nLANGUAGE: python\nCODE:\n```\nwith open(file_path, \"r\") as f:\n    sio = StringIO(f.read())\n\ndf = pd.read_xml(sio)\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\nwith open(file_path, \"rb\") as f:\n    bio = BytesIO(f.read())\n\ndf = pd.read_xml(bio)\ndf\n```\n\n----------------------------------------\n\nTITLE: Calculating Group Size with pandas GroupBy.size in Python\nDESCRIPTION: Demonstrates the use of the size method to obtain the number of entries in each group after grouping by multiple columns in a DataFrame. The result is a Series indexed by group keys with counts as values. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby([\"A\", \"B\"])\ngrouped.size()\n```\n\n----------------------------------------\n\nTITLE: Melting a Numpy Array with Index Enumeration - pandas - Python\nDESCRIPTION: This Python code provides the equivalent operation to R's melt for a numpy array. It reshapes a 1D sequence into a 3D numpy array, enumerates its indices and values with np.ndenumerate, then constructs a DataFrame from tuples of indices and values. Requires numpy and pandas. The result is a DataFrame with each row representing one element's indices and value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\na = np.array(list(range(1, 24)) + [np.nan]).reshape(2, 3, 4)\npd.DataFrame([tuple(list(x) + [val]) for x, val in np.ndenumerate(a)])\n```\n\n----------------------------------------\n\nTITLE: Creating Multiple Aggregated Columns with Custom Functions in Resample/GroupBy - Pandas - Python\nDESCRIPTION: Demonstrates resampling a time series by 5-minute intervals and applying multiple aggregation functions, including a custom function to provide a user-defined value for each window if it contains sufficient entries. Inputs include a datetime-indexed Series and custom aggregation logic. Outputs a DataFrame with columns for mean, max, and custom aggregations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nrng = pd.date_range(start=\"2014-10-07\", periods=10, freq=\"2min\")\nts = pd.Series(data=list(range(10)), index=rng)\n\ndef MyCust(x):\n    if len(x) > 2:\n        return x.iloc[1] * 1.234\n    return pd.NaT\n\nmhc = {\"Mean\": \"mean\", \"Max\": \"max\", \"Custom\": MyCust}\nts.resample(\"5min\").apply(mhc)\nts\n```\n\n----------------------------------------\n\nTITLE: Applying Conditional Replacement with DataFrame.apply and where - Python\nDESCRIPTION: Provides a functional alternative to DataFrame.where using DataFrame.apply and lambda functions. Requires pandas. Applies a lambda that conditionally replaces non-positive values in each column with corresponding values from 'A'. Demonstrates row-wise conditional filling. Input: DataFrame; Output: DataFrame after per-column conditional replacement.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.copy()\ndf.apply(lambda x, y: x.where(x > 0, y), y=df['A'])\n```\n\n----------------------------------------\n\nTITLE: Ignoring Comments and Empty Lines in CSV Data\nDESCRIPTION: Demonstrates how to use the comment parameter to ignore commented lines and how pandas by default skips completely blank lines when parsing CSV data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndata = \"\\na,b,c\\n  \\n# commented line\\n1,2,3\\n\\n4,5,6\"\nprint(data)\npd.read_csv(StringIO(data), comment=\"#\")\n```\n\n----------------------------------------\n\nTITLE: Groupby with Standard Deviation on Numeric Columns Only - pandas Python\nDESCRIPTION: Shows how to use groupby with the std aggregation, excluding non-numeric columns by specifying numeric_only=True. Requires pandas >= 1.1. Inputs: a DataFrame with mixed column types. Outputs: standard deviation of numeric columns for each group. Limitation: non-numeric columns are ignored.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"A\").std(numeric_only=True)\n```\n\n----------------------------------------\n\nTITLE: Determining the Shape after Filtering Not-Null Values in pandas (Python)\nDESCRIPTION: This snippet retrieves the shape of the filtered DataFrame age_no_na after removing rows with missing 'Age' values, displaying the number of rows and columns remaining. Useful for quantifying data loss from filtering.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nage_no_na.shape\n```\n\n----------------------------------------\n\nTITLE: Multiple Column Assignments via Multi-line eval String - Python\nDESCRIPTION: Uses a multi-line string with DataFrame.eval to assign several columns at once, including new columns and an overwritten column. Requires pandas. Simplifies complex calculations and batch column assignments. The DataFrame is modified in one step but a copy is returned.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf.eval(\n    \"\"\"\nc = a + b\nd = a + b + c\na = 1\"\"\",\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Column Width for DataFrame Display - Python\nDESCRIPTION: Demonstrates controlling column width truncation in pandas output using 'max_colwidth'. Cells exceeding this threshold are replaced with ellipses. Requires pandas, numpy, and a sample DataFrame with varied string lengths. Useful for keeping table outputs concise. Changes are reverted at end.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    np.array(\n        [\n            [\"foo\", \"bar\", \"bim\", \"uncomfortably long string\"],\n            [\"horse\", \"cow\", \"banana\", \"apple\"],\n        ]\n    )\n)\npd.set_option(\"max_colwidth\", 40)\ndf\npd.set_option(\"max_colwidth\", 6)\ndf\npd.reset_option(\"max_colwidth\")\n```\n\n----------------------------------------\n\nTITLE: Reading XML from AWS S3 with XPath - pandas - Python\nDESCRIPTION: Example of reading XML from an AWS S3 bucket using 'read_xml' with a specified XPath. The code targets biomedical XML data, specifying which top-level elements to parse and returning a DataFrame of selected metadata fields. Requires network access, AWS S3 public access (or credentials), and pandas; 'xpath' enables targeted, efficient parsing. The code uses multi-line string and ellipsis for brevity.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_116\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.read_xml(\n...    \"s3://pmc-oa-opendata/oa_comm/xml/all/PMC1236943.xml\",\n...    xpath=\".//journal-meta\",\n...)\n>>> df\n      journal-id  journal-title  issn  publisher\n0 Cardiovasc Ultrasound Cardiovascular Ultrasound 1476-7120 NaN\n```\n\n----------------------------------------\n\nTITLE: Histogram Plot for Differenced Data (Python)\nDESCRIPTION: Displays a histogram for the one-step difference of column \"A\" in DataFrame df using Series.hist(). This highlights the distribution of differences between consecutive data points. Requires pandas and matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\\n\\n@savefig hist_plot_ex.png\\ndf[\"A\"].diff().hist();\n```\n\n----------------------------------------\n\nTITLE: Selecting the nth Row(s) of Each Group - pandas Python\nDESCRIPTION: Demonstrates the nth, first, and last selection from groupby objects, supporting integers, negative integers, and lists/slices. Shows filtration for unavailable indices and dropna handling. Inputs: DataFrame with groups, output: DataFrame(s) with nth-row(s) of each group. Requires pandas, numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=[\"A\", \"B\"])\ng = df.groupby(\"A\")\n\ng.nth(0)\ng.nth(-1)\ng.nth(1)\n```\n\nLANGUAGE: python\nCODE:\n```\ng.nth(5)\n```\n\n----------------------------------------\n\nTITLE: Slicing a DataFrame with DatetimeIndex Using String Date Labels\nDESCRIPTION: Demonstrates how string dates can be converted to the type of the DatetimeIndex for natural slicing with .loc, using date strings as slice bounds.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndfl.loc['20130102':'20130104']\n```\n\n----------------------------------------\n\nTITLE: Pivoting DataFrame from Long to Wide Format\nDESCRIPTION: Converting long format data to wide format using pivot() function for visualization purposes\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/07_reshape_table_layout.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nno2_subset.pivot(columns=\"location\", values=\"value\")\n```\n\n----------------------------------------\n\nTITLE: Initializing a Series with Custom Index - pandas (Python)\nDESCRIPTION: This code initializes a pandas Series with values [1, 2, 3, 4, 5] and assigns a custom integer index [2, 3, 5, 7, 11]. It demonstrates how user-defined indices can affect Series slicing, an important consideration with upcoming label-based slicing changes for Series with Int64Index or RangeIndex. pandas (imported as pd) is required for this snippet.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, 3, 4, 5], index=[2, 3, 5, 7, 11])\n```\n\n----------------------------------------\n\nTITLE: Multi-column Factorization with GroupBy in Pandas\nDESCRIPTION: Uses the ngroup method to perform multi-column factorization, which can be useful for categorical-like processing or as input to algorithms that require integer encoding.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_72\n\nLANGUAGE: python\nCODE:\n```\ndfg = pd.DataFrame({\"A\": [1, 1, 2, 3, 2], \"B\": list(\"aaaba\")})\n\ndfg\n\ndfg.groupby([\"A\", \"B\"]).ngroup()\n\ndfg.groupby([\"A\", [0, 0, 0, 1, 1]]).ngroup()\n```\n\n----------------------------------------\n\nTITLE: Out of Bounds Slicing in Python Lists Compared to Pandas\nDESCRIPTION: Demonstrates how pandas handles out of bounds slice indexes similarly to Python lists, gracefully returning empty or partial results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# these are allowed in Python/NumPy.\nx = list('abcdef')\nx\nx[4:10]\nx[8:10]\ns = pd.Series(x)\ns\ns.iloc[4:10]\ns.iloc[8:10]\n```\n\n----------------------------------------\n\nTITLE: Invalid Option Shorthand in pandas.get_option - Python\nDESCRIPTION: Shows that providing a non-specific, ambiguous option substring to get_option can cause an error due to multiple matches. Relies on pandas; here \\\"max\\\" matches multiple options such as 'display.max_colwidth', 'display.max_rows', and 'display.max_columns'. Results in an exception or failure. This demonstrates the limitation of using broad regex substrings when retrieving options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npd.get_option(\"max\")\n```\n\n----------------------------------------\n\nTITLE: Pivot Table with pd.Grouper for Time-Based Grouping - pandas - Python\nDESCRIPTION: Utilizes pandas' Grouper class in the pivot_table index, grouping data by monthly frequency from the 'F' datetime column and pivoting columns by 'C'. Demonstrates advanced grouping capabilities of pd.pivot_table for time series data. Requires correct column types and pd.Grouper usage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\npd.pivot_table(df, values=\"D\", index=pd.Grouper(freq=\"ME\", key=\"F\"), columns=\"C\")\n```\n\n----------------------------------------\n\nTITLE: Aggregating DataFrames with Multiple Functions - pandas - Python\nDESCRIPTION: Aggregates DataFrame columns using a list of functions, producing a DataFrame with rows named by the aggregation functions. Supports lambda or standard function names. Inputs: list of aggregation function names/callables; Output: DataFrame with each aggregation as a separate row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ntsdf.agg([\"sum\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf.agg([\"sum\", \"mean\"])\n```\n\n----------------------------------------\n\nTITLE: Inspecting Column Data Types of a pandas DataFrame in Python\nDESCRIPTION: Retrieves and displays the data types for each column in the 'titanic' DataFrame via the 'dtypes' attribute. This is essential for confirming correct type inference after data import and prior to analysis. Returns a Series object with columns and their respective data types (e.g., int64, float64, object).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntitanic.dtypes\n```\n\n----------------------------------------\n\nTITLE: Reading a Remote Tab-Separated CSV via URL using Pandas (Python)\nDESCRIPTION: Reads a remote CSV file using pandas' read_csv, specifying a URL as the source and '\\t' as the separator. Requires internet access and uses a publicly-available resource; output is a DataFrame. Supports remote file handling via built-in pandas capabilities.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Handling Unicode Data with Encoding Parameter\nDESCRIPTION: Demonstrates how to use the encoding parameter to correctly read CSV files containing Unicode data encoded in specific character sets.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\n\ndata = b\"word,length\\n\" b\"Tr\\xc3\\xa4umen,7\\n\" b\"Gr\\xc3\\xbc\\xc3\\x9fe,5\"\ndata = data.decode(\"utf8\").encode(\"latin-1\")\ndf = pd.read_csv(BytesIO(data), encoding=\"latin-1\")\ndf\ndf[\"word\"][1]\n```\n\n----------------------------------------\n\nTITLE: Querying with Conflicting Variable and Column Names in pandas (ipython)\nDESCRIPTION: Shows how pandas resolves queries involving local variables and DataFrame columns with the same name, using the '@' syntax to disambiguate. Demonstrates both standard Python and query string approaches, illustrating equivalence. Requires pandas/numpy. Inputs: a numeric variable and DataFrame with matching column name. Outputs: boolean-masked/filter results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_18\n\nLANGUAGE: ipython\nCODE:\n```\na = np.random.randn()\ndf.query(\"@a < a\")\ndf.loc[a < df[\"a\"]]  # same as the previous expression\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Enumerated List - pandas - Python\nDESCRIPTION: This Python code mimics R's melt by enumerating a list of numbers (including np.nan) and passing it as tuples to pandas.DataFrame. It creates a two-column DataFrame of list index and value. Dependencies are numpy and pandas; input is a list with numerical elements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\na = list(enumerate(list(range(1, 5)) + [np.nan]))\npd.DataFrame(a)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Series with Multiple Functions and Callables - pandas - Python\nDESCRIPTION: Aggregates a Series with multiple functions, including strings and lambda/user-defined functions, returning a Series indexed by function names. Showcases custom function naming as well as lambda-generated <lambda> key. Input: aggregation function list; Output: Series of result per function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ntsdf[\"A\"].agg([\"sum\", \"mean\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf[\"A\"].agg([\"sum\", lambda x: x.mean()])\n```\n\nLANGUAGE: python\nCODE:\n```\ndef mymean(x):\n    return x.mean()\n\ntsdf[\"A\"].agg([\"sum\", mymean])\n```\n\n----------------------------------------\n\nTITLE: Dynamically Combining Boolean Criteria in Python\nDESCRIPTION: Combines multiple boolean criteria using functools.reduce with a lambda function. This approach allows for dynamically building complex filtering conditions from a list of criteria.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n)\ndf\n\nCrit1 = df.AAA <= 5.5\nCrit2 = df.BBB == 10.0\nCrit3 = df.CCC > -40.0\n\nAllCrit = Crit1 & Crit2 & Crit3\n```\n\n----------------------------------------\n\nTITLE: Plotting DataFrame with Subplots in Single and Multi-Axis Layouts - pandas/matplotlib - Python\nDESCRIPTION: These snippets illustrate creating multiple subplots from a DataFrame using the 'subplots' and 'layout' keywords to control the subplot grid, both explicitly and with automatic size inference (using -1), and plotting onto multiple pre-defined axes. Requires pandas, matplotlib, numpy, and a DataFrame 'df'. Ensures that the plot layout matches the intended arrangement and supports complex multi-axes plotting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\n@savefig frame_plot_subplots.png\ndf.plot(subplots=True, figsize=(6, 6));\n```\n\nLANGUAGE: python\nCODE:\n```\n@savefig frame_plot_subplots_layout.png\ndf.plot(subplots=True, layout=(2, 3), figsize=(6, 6), sharex=False);\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.plot(subplots=True, layout=(2, -1), figsize=(6, 6), sharex=False);\n```\n\nLANGUAGE: python\nCODE:\n```\nfig, axes = plt.subplots(4, 4, figsize=(9, 9))\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\ntarget1 = [axes[0][0], axes[1][1], axes[2][2], axes[3][3]]\ntarget2 = [axes[3][0], axes[2][1], axes[1][2], axes[0][3]]\n\ndf.plot(subplots=True, ax=target1, legend=False, sharex=False, sharey=False);\n@savefig frame_plot_subplots_multi_ax.png\n(-df).plot(subplots=True, ax=target2, legend=False, sharex=False, sharey=False);\n```\n\n----------------------------------------\n\nTITLE: Selecting Data from Pivoted DataFrames - pandas - Python\nDESCRIPTION: This snippet selects the 'value2' column from a DataFrame with MultiIndex columns, returning the corresponding data. Assumes the DataFrame 'pivoted' was previously constructed with multiple value columns via pivoting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\npivoted[\"value2\"]\n```\n\n----------------------------------------\n\nTITLE: Constructing DataFrame from List of Dicts - Pandas - Python\nDESCRIPTION: Creates a pandas DataFrame by passing a list where each element is a dict representing a row. Optional arguments include index labels and custom columns. Demonstrates how missing elements are handled as NaN, and how index/column selection can modify output. Requires pandas to be imported as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndata2 = [{\"a\": 1, \"b\": 2}, {\"a\": 5, \"b\": 10, \"c\": 20}]\npd.DataFrame(data2)\npd.DataFrame(data2, index=[\"first\", \"second\"])\npd.DataFrame(data2, columns=[\"a\", \"b\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Data Subset for NO2 Analysis\nDESCRIPTION: Filtering and grouping air quality data to create a subset of NO2 measurements\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/07_reshape_table_layout.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nno2 = air_quality[air_quality[\"parameter\"] == \"no2\"]\nno2_subset = no2.sort_index().groupby([\"location\"]).head(2)\n```\n\n----------------------------------------\n\nTITLE: Creating Pivot Table with Aggregation\nDESCRIPTION: Using pivot_table() to create summary statistics with mean aggregation and margins\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/07_reshape_table_layout.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nair_quality.pivot_table(\n    values=\"value\",\n    index=\"location\",\n    columns=\"parameter\",\n    aggfunc=\"mean\",\n    margins=True\n)\n```\n\n----------------------------------------\n\nTITLE: Slicing with MultiIndex in Python\nDESCRIPTION: Shows different slicing approaches with MultiIndex, including simple level-based slicing and more complex range-based slicing with tuples as start and end points.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[\"baz\":\"foo\"]\n```\n\n----------------------------------------\n\nTITLE: Joining Parameter Descriptions using pandas.merge in Python\nDESCRIPTION: This code merges the main air_quality DataFrame with parameter descriptions. It uses a left join, matching 'parameter' in air_quality with 'id' in the parameter metadata table, enabling the addition of readable descriptions and full names to each measurement. The result is previewed with head(). Dependencies: pandas, both DataFrames loaded previously.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nair_quality = pd.merge(air_quality, air_quality_parameters,\n                           how='left', left_on='parameter', right_on='id')\nair_quality.head()\n```\n\n----------------------------------------\n\nTITLE: Timing I/O Read Operations in IPython\nDESCRIPTION: This code block measures the execution time of various read operations using IPython's %timeit magic command. It compares the performance of different file formats and compression methods for reading data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_246\n\nLANGUAGE: ipython\nCODE:\n```\n%timeit test_sql_read()\n%timeit test_hdf_fixed_read()\n%timeit test_hdf_fixed_read_compress()\n%timeit test_hdf_table_read()\n%timeit test_hdf_table_read_compress()\n%timeit test_csv_read()\n%timeit test_feather_read()\n%timeit test_pickle_read()\n%timeit test_pickle_read_compress()\n%timeit test_parquet_read()\n```\n\n----------------------------------------\n\nTITLE: Getting Used Values from MultiIndex in Python\nDESCRIPTION: Demonstrates how to see only the levels actually used in a MultiIndex after slicing, using the to_numpy method or get_level_values for specific levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf[[\"foo\", \"qux\"]].columns.to_numpy()\n\n# for a specific level\ndf[[\"foo\", \"qux\"]].columns.get_level_values(0)\n```\n\n----------------------------------------\n\nTITLE: Counting Unique Values in a Series (value_counts) - Python\nDESCRIPTION: Shows histogramming with s.value_counts() by first creating a random integer Series. Results display the unique values and their respective counts, facilitating quick insights into data distributions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.random.randint(0, 7, size=10))\\ns\\ns.value_counts()\n```\n\n----------------------------------------\n\nTITLE: Inspecting and Understanding Category Codes Post-Union - Pandas Python\nDESCRIPTION: Shows that union_categoricals may reassign category codes upon merging, with categorical value code assignments varying between source and result. Includes example of inspecting the .codes attribute to illustrate this mapping. Key for understanding low-level behavior and debugging.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nc1 = pd.Categorical([\"b\", \"c\"])\nc2 = pd.Categorical([\"a\", \"b\"])\n\nc1\n# \"b\" is coded to 0\nc1.codes\n\nc2\n# \"b\" is coded to 1\nc2.codes\n\nc = union_categoricals([c1, c2])\nc\n# \"b\" is coded to 0 throughout, same as c1, different from c2\nc.codes\n```\n\n----------------------------------------\n\nTITLE: Handling Ambiguity in Integer Indices with loc and iloc in Python\nDESCRIPTION: Illustrates how loc and iloc behave differently with integer indices. This example clarifies the distinction between position-based and label-based indexing when the index consists of integers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndata = {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\ndf2 = pd.DataFrame(data=data, index=[1, 2, 3, 4])  # Note index starts at 1.\ndf2.iloc[1:3]  # Position-oriented\ndf2.loc[1:3]  # Label-oriented\n```\n\n----------------------------------------\n\nTITLE: Retrieving Level Values from MultiIndex in Python\nDESCRIPTION: Shows how to extract values from a specific level of a MultiIndex using the get_level_values method, which can use either level position or level name as identifiers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nindex.get_level_values(0)\nindex.get_level_values(\"second\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Grouping DataFrames in pandas (Python)\nDESCRIPTION: Illustrates creating a DataFrame of animal speeds and demonstrates grouping by 'class' and by both 'class' and 'order'. Requires pandas (as pd) and numpy (as np) to be imported. The code highlights the construction of a DataFrame and creation of GroupBy objects, foundational for subsequent split-apply-combine operations. The variables 'speeds' and 'grouped' represent the grouped data structure for further analysis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nspeeds = pd.DataFrame(\n    [\n        (\"bird\", \"Falconiformes\", 389.0),\n        (\"bird\", \"Psittaciformes\", 24.0),\n        (\"mammal\", \"Carnivora\", 80.2),\n        (\"mammal\", \"Primates\", np.nan),\n        (\"mammal\", \"Carnivora\", 58),\n    ],\n    index=[\"falcon\", \"parrot\", \"lion\", \"monkey\", \"leopard\"],\n    columns=(\"class\", \"order\", \"max_speed\"),\n)\nspeeds\n\ngrouped = speeds.groupby(\"class\")\ngrouped = speeds.groupby([\"class\", \"order\"])\n```\n\n----------------------------------------\n\nTITLE: Chaining String Methods for DataFrame Column Normalization (Python)\nDESCRIPTION: Demonstrates typical column name normalization by chaining .str.strip(), .str.lower(), and .str.replace() to remove spaces, convert case, and replace spaces with underscores. Assigns clean names back to DataFrame. Input is DataFrame with messy column names; after processing, all columns are normalized as lowercase and underscored.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Reduction and GroupBy Operations with Nullable Integer Data (pandas, Python)\nDESCRIPTION: Illustrates aggregation methods such as sum and grouped sum operating correctly on DataFrames and Series with nullable integer columns. Shows that standard reduction and groupby workflows are preserved. Requires pandas DataFrame df with nullable integer columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.sum(numeric_only=True)\ndf.sum()\ndf.groupby(\"B\").A.sum()\n```\n\n----------------------------------------\n\nTITLE: Multiple Function Aggregation on Series\nDESCRIPTION: Demonstrates applying multiple aggregation functions to a grouped Series, producing a DataFrame with function names as columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby(\"A\")\ngrouped[\"C\"].agg([\"sum\", \"mean\", \"std\"])\n```\n\n----------------------------------------\n\nTITLE: GroupBy Aggregation with Dict of Functions - Pandas - Python\nDESCRIPTION: Demonstrates using groupby with .agg(), supplying a dictionary that maps DataFrame columns to aggregation functions. Outputs a DataFrame with a MultiIndex on columns for multiple aggregations. No future deprecation applies for DataFrames with dict-to-list or dict-to-scalar. Requires DataFrame df, groups by 'A', and aggregates 'B' with 'sum' and 'C' with 'min'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby('A').agg({'B': 'sum', 'C': 'min'})\n```\n\n----------------------------------------\n\nTITLE: Assigning Columns Using DataFrame.eval - Python\nDESCRIPTION: Shows how to create or modify columns in a DataFrame directly within an eval string expression. Each assignment returns a new DataFrame with updated columns; original is unchanged. Requires pandas. The snippet chains multiple eval calls to add columns 'c', 'd', and overwrite 'a'. Input is an integer-based DataFrame; output is a DataFrame with new columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(dict(a=range(5), b=range(5, 10)))\ndf = df.eval(\"c = a + b\")\ndf = df.eval(\"d = a + b + c\")\ndf = df.eval(\"a = 1\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Dynamic Column Creation with map in Python\nDESCRIPTION: Creates new columns by mapping values from existing columns through a dictionary. This technique enables efficient conversion of numeric codes to categorical labels across multiple columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"AAA\": [1, 2, 1, 3], \"BBB\": [1, 1, 2, 2], \"CCC\": [2, 1, 3, 1]})\ndf\n\nsource_cols = df.columns  # Or some subset would work too\nnew_cols = [str(x) + \"_cat\" for x in source_cols]\ncategories = {1: \"Alpha\", 2: \"Beta\", 3: \"Charlie\"}\n\ndf[new_cols] = df[source_cols].map(categories.get)\ndf\n```\n\n----------------------------------------\n\nTITLE: Parsing Datetimes with Mixed Time Zones Using utc=True in Pandas (python)\nDESCRIPTION: This code demonstrates best practice for handling mixed time zone datetimes by passing utc=True to pd.to_datetime. It converts all provided datetime strings to UTC, returning a Series or Index of normalized UTC timestamps. Requires pandas and expects a list of time zone-aware datetime strings as input.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n    data = [\"2020-01-01 00:00:00+06:00\", \"2020-01-01 00:00:00+01:00\"]\n    pd.to_datetime(data, utc=True)\n```\n\n----------------------------------------\n\nTITLE: Generating Interval Ranges with Different Parameters\nDESCRIPTION: Demonstrates creating IntervalIndex objects using interval_range function with various parameters like start, end, periods, and frequency for both numeric and datetime intervals.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\npd.interval_range(start=0, end=5)\n\npd.interval_range(start=pd.Timestamp(\"2017-01-01\"), periods=4)\n\npd.interval_range(end=pd.Timedelta(\"3 days\"), periods=3)\n\npd.interval_range(start=0, periods=5, freq=1.5)\n\npd.interval_range(start=pd.Timestamp(\"2017-01-01\"), periods=4, freq=\"W\")\n\npd.interval_range(start=pd.Timedelta(\"0 days\"), periods=3, freq=\"9h\")\n\npd.interval_range(start=0, end=4, closed=\"both\")\n\npd.interval_range(start=0, end=4, closed=\"neither\")\n\npd.interval_range(start=0, end=6, periods=4)\n\npd.interval_range(pd.Timestamp(\"2018-01-01\"), pd.Timestamp(\"2018-02-28\"), periods=3)\n```\n\n----------------------------------------\n\nTITLE: Assigning Categoricals to Numeric and Object Columns - Pandas Python\nDESCRIPTION: Illustrates assignment of categorical values to standard numerical and object DataFrame columns. Shows that pandas extracts values when target column type is not already categorical, affecting dtypes as reflected in DataFrame.dtypes output. Useful for seeing type conversion side-effects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": [1, 1, 1, 1, 1], \"b\": [\"a\", \"a\", \"a\", \"a\", \"a\"]})\ndf.loc[1:2, \"a\"] = pd.Categorical([2, 2], categories=[2, 3])\ndf.loc[2:3, \"b\"] = pd.Categorical([\"b\", \"b\"], categories=[\"a\", \"b\"])\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Connecting to SQLite using ADBC driver in pandas\nDESCRIPTION: Shows how to connect to an in-memory SQLite database using the ADBC driver interface. This approach leverages Apache Arrow for improved performance, null handling, and type detection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_221\n\nLANGUAGE: python\nCODE:\n```\nimport adbc_driver_sqlite.dbapi as sqlite_dbapi\n\n# Create the connection\nwith sqlite_dbapi.connect(\"sqlite:///:memory:\") as conn:\n     df = pd.read_sql_table(\"data\", conn)\n```\n\n----------------------------------------\n\nTITLE: Boolean Reductions on DataFrames in Pandas Python\nDESCRIPTION: These snippets demonstrate column-wise and overall reductions using the .all(), .any() methods on DataFrames of boolean values, and the .empty property for checking if a DataFrame is empty. Outputs are boolean Series or single booleans, used for summarizing or validating data. Suitable for sanity checks and input validation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n(df > 0).all()\n(df > 0).any()\n```\n\nLANGUAGE: python\nCODE:\n```\n(df > 0).any().any()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.empty\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with Skipped Rows and Custom Header - Python\nDESCRIPTION: Demonstrates two options for reading complex CSV files with pandas.read_csv when there are non-data rows between header and data. Option 1 skips rows explicitly, option 2 first extracts column names then reads data. Dependencies: pandas, io.StringIO. Inputs: multiline CSV-like string with non-uniform structure. Outputs: DataFrames with correctly parsed data and column headers. Key parameters include sep, skiprows, header, index_col, parse_dates, names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\n\npd.read_csv(\n    StringIO(data),\n    sep=\";\",\n    skiprows=[11, 12],\n    index_col=0,\n    parse_dates=True,\n    header=10,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\ncolumns = pd.read_csv(StringIO(data), sep=\";\", header=10, nrows=10).columns\npd.read_csv(\n    StringIO(data), sep=\";\", index_col=0, header=12, parse_dates=True, names=columns\n)\n```\n\n----------------------------------------\n\nTITLE: Stacked Histogram Plot for DataFrame (Python)\nDESCRIPTION: Plots a stacked histogram for all columns in a DataFrame, specifying the bin size and stacking via DataFrame.plot.hist(stacked=True, bins=20). Each column's frequency distribution is stacked. Requires pandas, numpy, matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\\n\\n@savefig hist_new_stacked.png\\ndf4.plot.hist(stacked=True, bins=20);\n```\n\n----------------------------------------\n\nTITLE: Renaming DataFrame Columns with a Dictionary in pandas (Python)\nDESCRIPTION: This snippet demonstrates how to rename multiple columns in a pandas DataFrame by supplying a dictionary to the rename() method. Each key-value pair maps an old column name to a desired new name (following OpenAQ identifiers), and the result is stored in a new DataFrame 'air_quality_renamed'. This is useful for aligning dataset column names with external standards. Dependencies include pandas and the DataFrame columns to rename. No input from the user is needed, and the renamed DataFrame is returned. The limitation is that the provided mapping must match existing column names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/05_add_columns.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nair_quality_renamed = air_quality.rename(\n    columns={\n        \"station_antwerp\": \"BETR801\",\n        \"station_paris\": \"FR04014\",\n        \"station_london\": \"London Westminster\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: GroupBy Aggregations for Selected Columns with pandas max and mean in Python\nDESCRIPTION: Provides examples of aggregating specific columns ('C', 'D') using max and mean functions after grouping a DataFrame by one or more columns. Demonstrates both single-key and multi-key grouping and aggregation. Input is a DataFrame, output is an aggregated DataFrame by maximum or mean values. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"A\")[[\"C\", \"D\"]].max()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([\"A\", \"B\"]).mean()\n```\n\n----------------------------------------\n\nTITLE: Testing Index Monotonicity\nDESCRIPTION: Shows how to check if a DataFrame's index is monotonically increasing, which affects how label-based slicing works with out-of-bounds labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(index=[2, 3, 3, 4, 5], columns=[\"data\"], data=list(range(5)))\ndf.index.is_monotonic_increasing\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table from String or File in Python\nDESCRIPTION: Demonstrates how to read an HTML table from a string or file. The example writes an HTML string to a file and then reads it back using pandas.read_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_90\n\nLANGUAGE: python\nCODE:\n```\nhtml_str = \"\"\"\n        <table>\n            <tr>\n                <th>A</th>\n                <th colspan=\"1\">B</th>\n                <th rowspan=\"1\">C</th>\n            </tr>\n            <tr>\n                <td>a</td>\n                <td>b</td>\n                <td>c</td>\n            </tr>\n        </table>\n    \"\"\"\n\nwith open(\"tmp.html\", \"w\") as f:\n    f.write(html_str)\ndf = pd.read_html(\"tmp.html\")\ndf[0]\n```\n\n----------------------------------------\n\nTITLE: Creating and Serializing a DataFrame to JSON - pandas - Python\nDESCRIPTION: This snippet creates a DataFrame with random values using NumPy and serializes it to a JSON string via the to_json() method. Requires both pandas and NumPy to be installed, and no arguments are passed so defaults are used (column orientation, decimal encoding, etc). The output is a JSON string representing the DataFrame's data. Inputs: none but uses pd and np. Output: JSON string. No advanced formatting or file output is shown here.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ndfj = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\njson = dfj.to_json()\njson\n```\n\n----------------------------------------\n\nTITLE: Performing Left Join with DataFrame Reindexing in Python\nDESCRIPTION: Example of performing an effective left join by concatenating DataFrames and then reindexing to maintain the original DataFrame's index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.concat([df1, df4], axis=1).reindex(df1.index)\nresult\n```\n\n----------------------------------------\n\nTITLE: Accessing List Elements After String Split in pandas Series (Python)\nDESCRIPTION: Demonstrates retrieving a specific item from each sub-list produced by .str.split(), using both .str.get(index) and .str[index] notations. Useful for extracting fields after splitting. Input is Series of delimited strings; outputs are Series of second list elements or NaNs where missing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ns2.str.split(\"_\").str.get(1)\ns2.str.split(\"_\").str[1]\n```\n\n----------------------------------------\n\nTITLE: Rolling Window with Min Periods\nDESCRIPTION: Demonstrates rolling window operations with different minimum period requirements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([np.nan, 1, 2, np.nan, np.nan, 3])\ns.rolling(window=3, min_periods=1).sum()\ns.rolling(window=3, min_periods=2).sum()\ns.rolling(window=3, min_periods=None).sum()\n```\n\n----------------------------------------\n\nTITLE: Colorized Box Plot for DataFrame (Python)\nDESCRIPTION: Shows how to apply custom colors and outlier symbols to box plots by passing a dictionary to the color parameter and specifying sym for outliers. Each part of the box plot (boxes, whiskers, medians, caps) can be assigned different colors. Dependencies: pandas, matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ncolor = {\\n    \"boxes\": \"DarkGreen\",\\n    \"whiskers\": \"DarkOrange\",\\n    \"medians\": \"DarkBlue\",\\n    \"caps\": \"Gray\",\\n}\\n\\n@savefig box_new_colorize.png\\ndf.plot.box(color=color, sym=\"r+\");\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with Date Parsing and Inferring Format - pandas - Python\nDESCRIPTION: Reads a CSV file using pandas, specifying the first column as the index and enabling parsing of dates. Demonstrates format inference for date columns. Requires that 'foo.csv' exists and pandas is installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\n    \"foo.csv\",\n    index_col=0,\n    parse_dates=True,\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Reading CSVs with Datetime Parsing - pandas - Python\nDESCRIPTION: Shows how to write a simple CSV file and then read it with pandas, using the first column as index and automatically parsing it as dates. Demonstrates that the index is a DatetimeIndex of Python datetime objects. Requires the pandas library and an available 'foo.csv' in the same directory.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"foo.csv\", mode=\"w\") as f:\n    f.write(\"date,A,B,C\\n20090101,a,1,2\\n20090102,b,3,4\\n20090103,c,4,5\")\n\n# Use a column as an index, and parse it as dates.\ndf = pd.read_csv(\"foo.csv\", index_col=0, parse_dates=True)\ndf\n\n# These are Python datetime objects\ndf.index\n```\n\n----------------------------------------\n\nTITLE: Using pd.Grouper with Frequency and Key - pandas Python\nDESCRIPTION: Demonstrates grouping by both time frequency with pd.Grouper and another categorical column. Shows sum aggregation over specified time intervals, simulating resampling on DataFrame. Dependencies: pandas, datetime. Key parameters: freq for frequency, key for target column, and additional group columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\ndf = pd.DataFrame(\n    {\n        \"Branch\": \"A A A A A A A B\".split(),\n        \"Buyer\": \"Carl Mark Carl Carl Joe Joe Joe Carl\".split(),\n        \"Quantity\": [1, 3, 5, 1, 8, 1, 9, 3],\n        \"Date\": [\n            datetime.datetime(2013, 1, 1, 13, 0),\n            datetime.datetime(2013, 1, 1, 13, 5),\n            datetime.datetime(2013, 10, 1, 20, 0),\n            datetime.datetime(2013, 10, 2, 10, 0),\n            datetime.datetime(2013, 10, 1, 20, 0),\n            datetime.datetime(2013, 10, 2, 10, 0),\n            datetime.datetime(2013, 12, 2, 12, 0),\n            datetime.datetime(2013, 12, 2, 14, 0),\n        ],\n    }\n)\n\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([pd.Grouper(freq=\"1ME\", key=\"Date\"), \"Buyer\"])[[\"Quantity\"]].sum()\n```\n\n----------------------------------------\n\nTITLE: Creating PyArrow-backed Series in Pandas\nDESCRIPTION: Demonstrates creating Series objects backed by PyArrow arrays using both simple and complex data types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nser_float = pd.Series([1.0, 2.0, None], dtype=\"float32[pyarrow]\")\nser_float\n\nlist_of_int_type = pd.ArrowDtype(pa.list_(pa.int64()))\nser_list = pd.Series([[1, 2], [3, None]], dtype=list_of_int_type)\nser_list\n\nser_list.take([1, 0])\nser_float * 5\nser_float.mean()\nser_float.dropna()\n```\n\n----------------------------------------\n\nTITLE: Raising DuplicateLabelError with set_flags on DataFrame Columns - pandas - Python\nDESCRIPTION: Shows enforcement of column label uniqueness by calling set_flags(allows_duplicate_labels=False) on a DataFrame with unique column names. If columns were duplicated, this would raise DuplicateLabelError. Demonstrates application of label checks to both rows and columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[\"A\", \"B\", \"C\"],).set_flags(\n    allows_duplicate_labels=False\n)\n```\n\n----------------------------------------\n\nTITLE: Getting a Cross Section with .iloc Integer Position\nDESCRIPTION: Shows how to retrieve a cross-section (entire row) of a DataFrame using .iloc with a single integer position, equivalent to df.xs(1).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf1.iloc[1]\n```\n\n----------------------------------------\n\nTITLE: Accessing First Element of Series for int and float Conversion in Python\nDESCRIPTION: Demonstrates the new recommended way to convert a Series to int or float by accessing the first element using iloc[0].\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v3.0.0.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nint(Series.iloc[0])\nfloat(Series.iloc[0])\n```\n\n----------------------------------------\n\nTITLE: Sorting pandas Series and DataFrame by Values Using Key Functions (Python)\nDESCRIPTION: Shows sorting Series and DataFrames by value with a lambda key (e.g., string lowercasing). Demonstrates the key parameter, which expects a callable transforming the values for custom sort order. Assumes Series of strings or string columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([\"B\", \"a\", \"C\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ns1.sort_values()\ns1.sort_values(key=lambda x: x.str.lower())\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": [\"B\", \"a\", \"C\"], \"b\": [1, 2, 3]})\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.sort_values(by=\"a\")\ndf.sort_values(by=\"a\", key=lambda col: col.str.lower())\n```\n\n----------------------------------------\n\nTITLE: Resetting Display Maximum Rows Option in pandas - Python\nDESCRIPTION: Suppresses and resets the 'display.max_rows' option to its default value. Useful for cleaning up after tests or interactive sessions. Only requires the option name as input. There is no output; the effect is global for the current session.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npd.reset_option(\"display.max_rows\")\n```\n\n----------------------------------------\n\nTITLE: Side Effects of Series Creation from Categorical - pandas - Python\nDESCRIPTION: Shows that constructing a Series from a Categorical passes a reference, so modifying the Series affects the original Categorical. Highlights the importance of using copy=True or not reusing Categoricals when isolation is required. Dependencies: pandas. Key parameters: categories in Categorical. Input: Categorical object, series assignment; Output: Modified categorical contents. Limitation: In-place changes unless copy is set.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ncat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10])\ns = pd.Series(cat, name=\"cat\")\ncat\ns.iloc[0:2] = 10\ncat\n```\n\n----------------------------------------\n\nTITLE: Previewing the First Rows of a pandas DataFrame in Python\nDESCRIPTION: Displays the first five rows of the reloaded 'titanic' DataFrame using the default call to '.head()'. No parameters are needed; the method simply outputs a preview for data validation after reading from Excel.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntitanic.head()\n```\n\n----------------------------------------\n\nTITLE: Searching for Insertion Points in pandas Series using searchsorted (Python)\nDESCRIPTION: Explains how to use the Series.searchsorted method to find index positions to insert new values while maintaining sort order. Demonstrates use of the side argument and sorting with a custom sorter. Input: Series and values to search. Output: List of insertion indices. Dependencies: pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, 3])\nser.searchsorted([0, 3])\nser.searchsorted([0, 4])\nser.searchsorted([1, 3], side=\"right\")\nser.searchsorted([1, 3], side=\"left\")\nser = pd.Series([3, 1, 2])\nser.searchsorted([0, 3], sorter=np.argsort(ser))\n```\n\n----------------------------------------\n\nTITLE: GroupBy Aggregation with Released GIL in pandas (Python)\nDESCRIPTION: This snippet demonstrates creating a large DataFrame and performing a groupby aggregation, which, from pandas 0.17.0 onward, releases Python's Global Interpreter Lock (GIL) in certain Cython operations (e.g., during factorization and summation). Dependencies: pandas, numpy. Inputs include random integer and float data; output is a group-by sum of the 'data' column per key. This optimization allows multi-threading gains in user code relying on pandas operations with large data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nN = 1000000\nngroups = 10\ndf = DataFrame(\n    {\"key\": np.random.randint(0, ngroups, size=N), \"data\": np.random.randn(N)}\n)\ndf.groupby(\"key\")[\"data\"].sum()\n```\n\n----------------------------------------\n\nTITLE: Applying Reduce Functions Over Expanding Window with Pandas - Python\nDESCRIPTION: Creates a pandas Series and applies an expanding window with a custom reduce function for cumulative return calculation. Uses functools.reduce and requires pandas and functools. Inputs are a numeric Series; outputs are the expanding application of the custom reduction across the Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nS = pd.Series([i / 100.0 for i in range(1, 11)])\n\ndef cum_ret(x, y):\n    return x * (1 + y)\n\ndef red(x):\n    return functools.reduce(cum_ret, x, 1.0)\n\nS.expanding().apply(red, raw=True)\n```\n\n----------------------------------------\n\nTITLE: Using idxmin with groupby in Python\nDESCRIPTION: Finds the rows with the minimum value in a column for each group. This method uses idxmin() to get the indices of minimum values within groups and then selects these rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [1, 1, 1, 2, 2, 2, 3, 3], \"BBB\": [2, 1, 3, 4, 5, 1, 2, 3]}\n)\ndf\n\ndf.loc[df.groupby(\"AAA\")[\"BBB\"].idxmin()]\n```\n\n----------------------------------------\n\nTITLE: Applying Elementwise Functions with map() - pandas - Python\nDESCRIPTION: Applies a custom function f to individual values of a DataFrame column or the whole DataFrame using .map(), returning the length of each value's string representation. Useful for non-vectorizable functions. Demonstrates .map() for both Series and DataFrame. Input: function f; Output: Series or DataFrame of function outputs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndf4 = df.copy()\ndf4\n```\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    return len(str(x))\n```\n\nLANGUAGE: python\nCODE:\n```\ndf4[\"one\"].map(f)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf4.map(f)\n```\n\n----------------------------------------\n\nTITLE: Error with Duplicate Index after Intersection and Reindex\nDESCRIPTION: Demonstrates how the combination of intersection and reindex still raises an error if the result has duplicate indices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nlabels = ['a', 'd']\ns.loc[s.index.intersection(labels)].reindex(labels)\n```\n\n----------------------------------------\n\nTITLE: Timedelta Frequency Conversion with astype - pandas - Python\nDESCRIPTION: Converts Timedelta Series to a specific frequency (seconds) using astype. Also demonstrates mutation of individual elements and missing value assignment. Useful for normalizing time differences to standard units.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndecember = pd.Series(pd.date_range(\"20121201\", periods=4))\njanuary = pd.Series(pd.date_range(\"20130101\", periods=4))\ntd = january - december\n\ntd[2] += datetime.timedelta(minutes=5, seconds=3)\ntd[3] = np.nan\ntd\n\n# to seconds\ntd.astype(\"timedelta64[s]\")\n```\n\n----------------------------------------\n\nTITLE: Converting Existing Series to Categorical in Python\nDESCRIPTION: Shows how to convert an existing Series or DataFrame column to a categorical data type using the astype() method. This approach allows for post-creation conversion to categorical.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [\"a\", \"b\", \"c\", \"a\"]})\ndf[\"B\"] = df[\"A\"].astype(\"category\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Pivoting with Hierarchical Columns - pandas - Python\nDESCRIPTION: Here, after adding a new 'value2' column to the DataFrame, pivoting is performed without specifying the 'values' argument. This leads to hierarchical columns, where each variable contains both 'value' and 'value2'. This relies on pandas' ability to output MultiIndex columns if multiple value columns are present. Requires pandas and DataFrame with suitable columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndf[\"value2\"] = df[\"value\"] * 2\npivoted = df.pivot(index=\"date\", columns=\"variable\")\npivoted\n```\n\n----------------------------------------\n\nTITLE: Creating a MultiIndex from Product in Python\nDESCRIPTION: Shows how to create a MultiIndex by generating all pairwise combinations from input iterables using the from_product method, which is useful when you need every possible combination of elements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\niterables = [[\"bar\", \"baz\", \"foo\", \"qux\"], [\"one\", \"two\"]]\npd.MultiIndex.from_product(iterables, names=[\"first\", \"second\"])\n```\n\n----------------------------------------\n\nTITLE: Slicing a Series with Duplicate Labels\nDESCRIPTION: Demonstrates how .loc slicing works with duplicate labels in the index, which can raise KeyError in certain cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(list('abcdef'), index=[0, 3, 2, 5, 4, 2])\ns.loc[3:5]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating DataFrame.join order preservation in Pandas\nDESCRIPTION: This example shows how DataFrame.join with inner join now preserves the order of the left DataFrame's index rather than the right DataFrame, which is a breaking change in Pandas 0.20.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_39\n\nLANGUAGE: ipython\nCODE:\n```\nleft = pd.DataFrame({'a': [20, 10, 0]}, index=[2, 1, 0])\nleft\nright = pd.DataFrame({'b': [100, 200, 300]}, index=[1, 2, 3])\nright\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: left.join(right, how='inner')\nOut[4]:\n   a    b\n1  10  100\n2  20  200\n```\n\nLANGUAGE: ipython\nCODE:\n```\nleft.join(right, how='inner')\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data into Pandas DataFrame\nDESCRIPTION: This snippet demonstrates how to load CSV data from a URL into a pandas DataFrame. It uses pd.read_csv() to read the data from a GitHub raw file URL.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nurl = (\n    \"https://raw.githubusercontent.com/pandas-dev\"\n    \"/pandas/main/pandas/tests/io/data/csv/tips.csv\"\n)\ntips = pd.read_csv(url)\ntips\n```\n\n----------------------------------------\n\nTITLE: Dropping Duplicates by Index Value in Pandas DataFrame\nDESCRIPTION: This code shows how to drop duplicates by index value using Index.duplicated() and slicing. It demonstrates different keep options for handling duplicate index values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\ndf3 = pd.DataFrame({'a': np.arange(6),\n                   'b': np.random.randn(6)},\n                  index=['a', 'a', 'b', 'c', 'b', 'a'])\ndf3\ndf3.index.duplicated()\ndf3[~df3.index.duplicated()]\ndf3[~df3.index.duplicated(keep='last')]\ndf3[~df3.index.duplicated(keep=False)]\n```\n\n----------------------------------------\n\nTITLE: Reading an SPSS .sav File into pandas DataFrame - Python\nDESCRIPTION: This example loads a full SPSS .sav file into a pandas DataFrame using pandas.read_spss. By default, categorical columns are automatically converted into pd.Categorical. Inputs: .sav file path. Output: DataFrame containing all columns. Requires pandas library and assumes the specified .sav file is available.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_241\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_spss(\"spss_data.sav\")\n```\n\n----------------------------------------\n\nTITLE: Accessing GroupBy Object Attributes and Cardinality (Python)\nDESCRIPTION: Demonstrates the use of the 'groups' attribute to access the underlying group indices in DataFrameGroupBy objects, and uses len() to get the number of groups. Includes examples of grouping by columns and applying a custom function using groupby. Requires pandas (as pd) and DataFrame 'df' to be available.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"A\").groups\ndf.T.groupby(get_letter_type).groups\n```\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby([\"A\", \"B\"])\ngrouped.groups\nlen(grouped)\n```\n\n----------------------------------------\n\nTITLE: Specifying Categorical Data Type for Individual Columns\nDESCRIPTION: Shows how to apply categorical data types to specific columns when reading CSV data using a dictionary specification with column names as keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(StringIO(data), dtype={\"col1\": \"category\"}).dtypes\n```\n\n----------------------------------------\n\nTITLE: Explicitly Setting Index Column When Reading CSV\nDESCRIPTION: Demonstrates how to explicitly specify which column to use as the index using the index_col parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndata = \"index,a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\npd.read_csv(StringIO(data), index_col=0)\n```\n\n----------------------------------------\n\nTITLE: New Behavior: DataFrame.all with bool_only=True (ipython)\nDESCRIPTION: This code demonstrates the new behavior, where DataFrame.all with bool_only=True produces consistent and predictable results for both the entire DataFrame and selected columns, respecting each column's suitability for boolean operations. This output format is identical between subset and full DataFrame usages. Output is a Series with booleans per column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nIn [5]: df.all(bool_only=True)\n\nIn [6]: df[[\"B\", \"C\"]].all(bool_only=True)\n```\n\n----------------------------------------\n\nTITLE: Making Row Headers Sticky in a Wide DataFrame with Pandas Styler (Python)\nDESCRIPTION: This code demonstrates how to make index (row) headers sticky in a wide DataFrame using Styler.set_sticky, improving navigation for large tables. Requires pandas and numpy for data creation. The DataFrame (bigdf) is generated with random values and displayed in a notebook with horizontal scrolling. set_sticky maintains the visibility of row headers as users scroll.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nbigdf = pd.DataFrame(np.random.default_rng().standard_normal((16, 100)))\nbigdf.style.set_sticky(axis=\"index\")\n```\n\n----------------------------------------\n\nTITLE: Joining with MultiIndexes on Multiple Levels - pandas - Python\nDESCRIPTION: These snippets illustrate joining DataFrames with MultiIndexes on a subset of levels. The first part shows joining when the right MultiIndex is a subset of the left's; the second part demonstrates resetting indexes and merging on a common column, then restoring the MultiIndex. pandas and MultiIndex manipulation knowledge are required. Output DataFrames reflect aligned multi-level keys, and unknown levels are removed unless converted to columns before joining.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nleftindex = pd.MultiIndex.from_product(\n    [list(\"abc\"), list(\"xy\"), [1, 2]], names=[\"abc\", \"xy\", \"num\"]\n)\nleft = pd.DataFrame({\"v1\": range(12)}, index=leftindex)\nleft\n\nrightindex = pd.MultiIndex.from_product(\n    [list(\"abc\"), list(\"xy\")], names=[\"abc\", \"xy\"]\n)\nright = pd.DataFrame({\"v2\": [100 * i for i in range(1, 7)]}, index=rightindex)\nright\n\nleft.join(right, on=[\"abc\", \"xy\"], how=\"inner\")\n```\n\nLANGUAGE: python\nCODE:\n```\nleftindex = pd.MultiIndex.from_tuples(\n    [(\"K0\", \"X0\"), (\"K0\", \"X1\"), (\"K1\", \"X2\")], names=[\"key\", \"X\"]\n)\nleft = pd.DataFrame(\n    {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]}, index=leftindex\n)\n\nrightindex = pd.MultiIndex.from_tuples(\n    [(\"K0\", \"Y0\"), (\"K1\", \"Y1\"), (\"K2\", \"Y2\"), (\"K2\", \"Y3\")], names=[\"key\", \"Y\"]\n)\nright = pd.DataFrame(\n    {\"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"]}, index=rightindex\n)\n\nresult = pd.merge(\n    left.reset_index(), right.reset_index(), on=[\"key\"], how=\"inner\"\n).set_index([\"key\", \"X\", \"Y\"])\nresult\n```\n\n----------------------------------------\n\nTITLE: Reindexing Series with Missing Data using pandas (Python)\nDESCRIPTION: These snippets illustrate how pandas uses sentinel values for missing data when reindexing Series objects of various dtypes. The effect is shown for integer, boolean, timedelta64, datetime64, and period dtypes, with missing values represented by np.nan, pd.NaT, or <NA> depending on dtype. There are no external dependencies beyond pandas and numpy; key parameters are the dtype and the new index being used for reindexing. Output is a Series with coerced, dtype-appropriate missing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npd.Series([1, 2], dtype=np.int64).reindex([0, 1, 2])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([True, False], dtype=np.bool_).reindex([0, 1, 2])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([1, 2], dtype=np.dtype(\"timedelta64[ns]\")).reindex([0, 1, 2])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([1, 2], dtype=np.dtype(\"datetime64[ns]\")).reindex([0, 1, 2])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([\"2020\", \"2020\"], dtype=pd.PeriodDtype(\"D\")).reindex([0, 1, 2])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([1, 2], dtype=\"Int64\").reindex([0, 1, 2])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([True, False], dtype=\"boolean[pyarrow]\").reindex([0, 1, 2])\n```\n\n----------------------------------------\n\nTITLE: Reshaping DataFrames with Stack and MultiIndex in pandas Python\nDESCRIPTION: Explains how to create MultiIndex DataFrames and reshape them using stack and unstack. Dependencies are pandas and numpy. Shows construction of a DataFrame with hierarchical row indices, use of stack to compress columns into the index, and unstack to reverse this process. Inputs are arrays/lists for index creation and random data. Outputs are DataFrames or Series with different index structures.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\narrays = [\\n   [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\\n   [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\\n]\\nindex = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\\ndf = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"])\\ndf2 = df[:4]\\ndf2\n```\n\nLANGUAGE: python\nCODE:\n```\nstacked = df2.stack()\\nstacked\n```\n\nLANGUAGE: python\nCODE:\n```\nstacked.unstack()\\nstacked.unstack(1)\\nstacked.unstack(0)\n```\n\n----------------------------------------\n\nTITLE: Formatting pandas Series with PeriodIndex to Strings (Python)\nDESCRIPTION: This example demonstrates formatting a Series indexed by a PeriodIndex as strings. The .dt.strftime method is applied similarly to datetime Series, assuming s is period-typed. Dependencies: pandas. Input: Series, Output: Series of formatted strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(pd.period_range(\"20130101\", periods=4))\ns\ns.dt.strftime(\"%Y/%m/%d\")\n```\n\n----------------------------------------\n\nTITLE: Melting a DataFrame in Python\nDESCRIPTION: Shows how to use pandas.melt() to reshape a DataFrame from wide to long format.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ncheese = pd.DataFrame(\n    {\n        \"first\": [\"John\", \"Mary\"],\n        \"last\": [\"Doe\", \"Bo\"],\n        \"height\": [5.5, 6.0],\n        \"weight\": [130, 150],\n    }\n)\ncheese\ncheese.melt(id_vars=[\"first\", \"last\"])\ncheese.melt(id_vars=[\"first\", \"last\"], var_name=\"quantity\")\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple CSV Files with pandas - Python\nDESCRIPTION: This snippet demonstrates how to concatenate several CSV files into a single pandas DataFrame by reading each file separately and combining them using pd.concat. Dependencies: pandas library and a list of files. Input: list of CSV file paths. Output: a single DataFrame with all data concatenated. The ignore_index=True option ensures the resulting index is consecutive. Requires each CSV to have compatible columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n```\n\n----------------------------------------\n\nTITLE: Sorting Categorical Data in Pandas\nDESCRIPTION: Demonstrates sorting operations on ordered and unordered categorical data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(pd.Categorical([\"a\", \"b\", \"c\", \"a\"], ordered=False))\ns = s.sort_values()\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"]).astype(CategoricalDtype(ordered=True))\ns = s.sort_values()\ns\ns.min(), s.max()\n```\n\n----------------------------------------\n\nTITLE: Importing pandas and numpy for Data Analysis - Python\nDESCRIPTION: The code snippet imports the pandas and numpy libraries, commonly aliased as 'pd' and 'np' respectively. These imports are prerequisites for all subsequent pandas and numpy operations in the guide. Both libraries must be installed and available in the Python environment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Propagating NA in Arithmetic and Comparison Operations in pandas (Python)\nDESCRIPTION: These snippets demonstrate how arithmetic and comparison operators propagate NA in pandas. Addition, multiplication, exponentiation, and equality operations are shown, indicating propagation of NA with different operands. There are no external dependencies besides pandas. Output matches three-valued logic for NA propagation, except for special cases such as exponentiation with 0 or 1.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.NA + 1\n\"a\" * pd.NA\n```\n\nLANGUAGE: python\nCODE:\n```\npd.NA ** 0\n1 ** pd.NA\n```\n\nLANGUAGE: python\nCODE:\n```\npd.NA == 1\npd.NA == pd.NA\npd.NA < 2.5\n```\n\nLANGUAGE: python\nCODE:\n```\npd.isna(pd.NA)\n```\n\n----------------------------------------\n\nTITLE: Leveraging Styler.from_custom_template for Easier Customization (Python)\nDESCRIPTION: Illustrates the use of Styler.from_custom_template, a convenience method that returns a new sub-class of Styler set up to use a provided custom template directory and file. It then creates a styled DataFrame and renders it to HTML with a title. Requires pandas, the custom template, and a DataFrame df3.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nEasyStyler = Styler.from_custom_template(\"templates\", \"myhtml.tpl\")\nHTML(EasyStyler(df3).to_html(table_title=\"Another Title\"))\n```\n\n----------------------------------------\n\nTITLE: Customizing MultiIndex Levels in DataFrame Concatenation in Python\nDESCRIPTION: Example of specifying custom levels and names for the MultiIndex created during DataFrame concatenation with keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.concat(\n    pieces, keys=[\"x\", \"y\", \"z\"], levels=[[\"z\", \"y\", \"x\", \"w\"]], names=[\"group_key\"]\n)\nresult\n```\n\n----------------------------------------\n\nTITLE: Reading and Displaying a CSV File as a Pandas DataFrame (Python)\nDESCRIPTION: Demonstrates reading a CSV file into a DataFrame and displaying the DataFrame or summary info. Requires pandas, a CSV file at data/baseball.csv, and optionally setting display.max_rows. print(baseball) prints the full/truncated DataFrame to console, baseball.info() shows concise summary.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nbaseball = pd.read_csv(\"data/baseball.csv\")\nprint(baseball)\nbaseball.info()\n```\n\n----------------------------------------\n\nTITLE: Comparing Accessor Application Results on Series and Categoricals - Pandas Python\nDESCRIPTION: Verifies that applying .str accessor methods to a Series and its categorical-cast version yields identical results and types. Illustrates performance and correctness using .str.contains and dtype checks. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nret_s = str_s.str.contains(\"a\")\nret_cat = str_cat.str.contains(\"a\")\nret_s.dtype == ret_cat.dtype\nret_s == ret_cat\n```\n\n----------------------------------------\n\nTITLE: Applying Lambda with groupby in pandas - Python\nDESCRIPTION: Demonstrates how to use the groupby method with the group_keys argument to control whether grouped columns appear in the result index. Useful for customizing the structure of aggregated results when grouping by a column. Inputs include a DataFrame 'df' and the key column; output is a grouped and applied DataFrame. No external dependencies beyond pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"A\", group_keys=True).apply(lambda x: x)\n```\n\n----------------------------------------\n\nTITLE: Creating SQLAlchemy Engine Connections to Common Databases (Python)\nDESCRIPTION: Provides example code for creating SQLAlchemy engine connections for PostgreSQL, MySQL, Oracle, MSSQL, and SQLite. Requires SQLAlchemy and corresponding DB API drivers for the specific backend. Each example uses a different database URI scheme; engines created are used for subsequent pandas operations or raw SQL queries.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_232\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\"postgresql://scott:tiger@localhost:5432/mydatabase\")\n\nengine = create_engine(\"mysql+mysqldb://scott:tiger@localhost/foo\")\n\nengine = create_engine(\"oracle://scott:tiger@127.0.0.1:1521/sidname\")\n\nengine = create_engine(\"mssql+pyodbc://mydsn\")\n\n# sqlite://<nohostname>/<path>\n# where <path> is relative:\nengine = create_engine(\"sqlite:///foo.db\")\n\n# or absolute, starting with a slash:\nengine = create_engine(\"sqlite:////absolute/path/to/foo.db\")\n```\n\n----------------------------------------\n\nTITLE: Demonstration: Max/Min Rows and Truncated DataFrame Output - Python\nDESCRIPTION: Illustrates the effect of 'display.max_rows' and 'display.min_rows' on DataFrame repr output. Shows output when displayed rows are below and above the maximum. Requires pandas and numpy. Useful for large DataFrames, ensuring summarized but informative outputs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.max_rows\", 8)\npd.set_option(\"display.min_rows\", 4)\n# below max_rows -> all rows shown\ndf = pd.DataFrame(np.random.randn(7, 2))\ndf\n# above max_rows -> only min_rows (4) rows shown\ndf = pd.DataFrame(np.random.randn(9, 2))\ndf\npd.reset_option(\"display.max_rows\")\npd.reset_option(\"display.min_rows\")\n```\n\n----------------------------------------\n\nTITLE: Standardizing Data by Group with pandas GroupBy in Python\nDESCRIPTION: This snippet creates a time series and standardizes the data within each group by year, applying a z-score transformation. It requires pandas and numpy as dependencies. The transformation centers and scales each group so that the resulting data within a group has mean zero and standard deviation one.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.date_range(\"10/1/1999\", periods=1100)\nts = pd.Series(np.random.normal(0.5, 2, 1100), index)\nts = ts.rolling(window=100, min_periods=100).mean().dropna()\n\nts.head()\nts.tail()\n\ntransformed = ts.groupby(lambda x: x.year).transform(\n    lambda x: (x - x.mean()) / x.std()\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing Groups in a DataFrame Using groupby.get_group - Pandas - Python\nDESCRIPTION: Groups a DataFrame by the 'animal' column and retrieves all rows where 'animal' is 'cat' using get_group. Expects a grouping object as input (result of groupby) and returns the subgroup DataFrame. No external dependencies beyond pandas and the existing DataFrame/group object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ngb = df.groupby(\"animal\")\ngb.get_group(\"cat\")\n```\n\n----------------------------------------\n\nTITLE: Importing CSV Data with pandas in Python\nDESCRIPTION: This snippet demonstrates using pandas' read_csv function to import CSV data from a remote URL into a DataFrame named 'tips'. The url variable holds the location of the dataset, and pd.read_csv(url) reads the file into a DataFrame. It requires pandas to be imported as pd and internet access for the URL. The output is a pandas DataFrame containing the dataset. Limitations include the need for a valid URL and that the dataset must be reachable from the user's environment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spss.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n   url = (\n       \"https://raw.githubusercontent.com/pandas-dev\"\n       \"/pandas/main/pandas/tests/io/data/csv/tips.csv\"\n   )\n   tips = pd.read_csv(url)\n   tips\n```\n\n----------------------------------------\n\nTITLE: Converting Dummy Variables to Categorical Data\nDESCRIPTION: Demonstrates the new from_dummies() function that converts dummy-coded columns into categorical variables.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame({\"col1_a\": [1, 0, 1], \"col1_b\": [0, 1, 0],\n                   \"col2_a\": [0, 1, 0], \"col2_b\": [1, 0, 0],\n                   \"col2_c\": [0, 0, 1]})\n\npd.from_dummies(df, sep=\"_\")\n```\n\n----------------------------------------\n\nTITLE: Importing Tab-Delimited Data without Headers using pandas in Python\nDESCRIPTION: This snippet illustrates how to use pd.read_csv with custom parameters (sep and header) to read a tab-delimited file with no column names. The function pd.read_csv(\"tips.csv\", sep=\"\\t\", header=None) reads the data, and pd.read_table provides an equivalent alias. It assumes pandas is imported as pd. The primary parameters are the file path, delimiter ('\\t'), and the absence of headers ('header=None'). The output is a DataFrame with generic integer column labels. This method requires the input file to exist and be formatted as expected.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spss.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n   tips = pd.read_csv(\"tips.csv\", sep=\"\\t\", header=None)\n\n   # alternatively, read_table is an alias to read_csv with tab delimiter\n   tips = pd.read_table(\"tips.csv\", header=None)\n```\n\n----------------------------------------\n\nTITLE: Handling Unexpected Values in Categorical Data Types\nDESCRIPTION: Demonstrates how values that are not in the predefined category list are treated as missing values when using CategoricalDtype with read_csv.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndtype = CategoricalDtype([\"a\", \"b\", \"d\"])  # No 'c'\npd.read_csv(StringIO(data), dtype={\"col1\": dtype}).col1\n```\n\n----------------------------------------\n\nTITLE: Creating and Using CategoricalIndex in DataFrame\nDESCRIPTION: Demonstrates how to create a DataFrame with a categorical column and convert it to a CategoricalIndex, which is useful for efficiently indexing data with duplicates.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import CategoricalDtype\n\ndf = pd.DataFrame({\"A\": np.arange(6), \"B\": list(\"aabbca\")})\ndf[\"B\"] = df[\"B\"].astype(CategoricalDtype(list(\"cab\")))\ndf\ndf.dtypes\ndf[\"B\"].cat.categories\n```\n\n----------------------------------------\n\nTITLE: Value Replacement in Pandas\nDESCRIPTION: Shows different methods of replacing values in DataFrames, including using mappings and regular expressions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.eye(3))\ndf_missing = df.replace(0, np.nan)\ndf_filled = df_missing.replace(np.nan, 2)\ndf_filled.replace([1, 44], [2, 28])\ndf_filled.replace({1: 44, 2: 28})\n```\n\n----------------------------------------\n\nTITLE: Resample API Usage Examples\nDESCRIPTION: Shows the new groupby-like API for resampling operations including downsampling and column-specific operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(1234)\ndf = pd.DataFrame(np.random.rand(10,4),\n                     columns=list('ABCD'),\n                     index=pd.date_range('2010-01-01 09:00:00',\n                                         periods=10, freq='s'))\nr = df.resample('2s')\nr.mean()\nr.sum()\nr[['A','C']].mean()\nr.agg({'A' : 'mean', 'B' : 'sum'})\nr[['A','B']].agg(['mean','sum'])\n```\n\n----------------------------------------\n\nTITLE: Combining DataFrames in Python with Pandas\nDESCRIPTION: This snippet shows how to combine two DataFrames using the 'combine_first' method, which fills missing values in one DataFrame with values from another DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(\n    {\"A\": [1.0, np.nan, 3.0, 5.0, np.nan], \"B\": [np.nan, 2.0, 3.0, np.nan, 6.0]}\n)\ndf2 = pd.DataFrame(\n    {\n        \"A\": [5.0, 2.0, 4.0, np.nan, 3.0, 7.0],\n        \"B\": [np.nan, np.nan, 3.0, 4.0, 6.0, 8.0],\n    }\n)\ndf1.combine_first(df2)\n```\n\n----------------------------------------\n\nTITLE: Grouping Transposed DataFrame Using a Custom Function (Python)\nDESCRIPTION: Demonstrates grouping the transposed (rows become columns) DataFrame by applying a custom function on labels. The get_letter_type function classifies letters as vowels or consonants. This shows how to use arbitrary Python functions as grouping keys in pandas. The snippet presumes the existence of a DataFrame 'df' and relies on pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_letter_type(letter):\n    if letter.lower() in 'aeiou':\n        return 'vowel'\n    else:\n        return 'consonant'\n\ngrouped = df.T.groupby(get_letter_type)\n```\n\n----------------------------------------\n\nTITLE: Describing Categorical Data in Python\nDESCRIPTION: Shows how to use the describe() method on categorical data, which produces output similar to string-type Series or DataFrames. This provides summary statistics for categorical data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncat = pd.Categorical([\"a\", \"c\", \"c\", np.nan], categories=[\"b\", \"a\", \"c\"])\ndf = pd.DataFrame({\"cat\": cat, \"s\": [\"a\", \"c\", \"c\", np.nan]})\ndf.describe()\ndf[\"cat\"].describe()\n```\n\n----------------------------------------\n\nTITLE: Transposing a DataFrame (df.T) - Python\nDESCRIPTION: The code uses 'df.T' to transpose the DataFrame, swapping rows and columns. Input is the DataFrame object; output is a new transposed DataFrame. Useful for flipping axes for analysis or visualization.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.T\n```\n\n----------------------------------------\n\nTITLE: Discretization using cut() and qcut() in Pandas\nDESCRIPTION: Shows how to discretize continuous values using cut() for value-based bins and qcut() for quantile-based bins.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\narr = np.random.randn(20)\nfactor = pd.cut(arr, 4)\nfactor\n\nfactor = pd.cut(arr, [-5, -1, 0, 1, 5])\nfactor\n\narr = np.random.randn(30)\nfactor = pd.qcut(arr, [0, 0.25, 0.5, 0.75, 1])\nfactor\n\narr = np.random.randn(20)\nfactor = pd.cut(arr, [-np.inf, 0, np.inf])\nfactor\n```\n\n----------------------------------------\n\nTITLE: Getting a Cross Section of a DataFrame with .loc\nDESCRIPTION: Shows how to retrieve a cross-section (entire row) of a DataFrame using .loc with a single label, equivalent to df.xs('a').\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf1.loc['a']\n```\n\n----------------------------------------\n\nTITLE: Reading Specific Columns from an SPSS File Without Converting Categoricals - Python\nDESCRIPTION: This snippet demonstrates selective reading of columns from an SPSS .sav file into a pandas DataFrame using pandas.read_spss. It specifies usecols to limit which columns are loaded and sets convert_categoricals=False to return data without converting categorical columns to pd.Categorical. Inputs: file path, columns to select, flag for categorical conversion. Output: DataFrame with subset of columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_242\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_spss(\n    \"spss_data.sav\",\n    usecols=[\"foo\", \"bar\"],\n    convert_categoricals=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Data in a Time Series with Backward Fill - Pandas - Python\nDESCRIPTION: Creates a DataFrame with random numbers and a business day date index, introduces a missing value, then fills missing data using backwards fill (bfill). Requires pandas and numpy. Inputs are the number of rows, random data, and intentional NaN; outputs are the original and filled DataFrames. Assumes NaN is introduced at a specific location.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    np.random.randn(6, 1),\n    index=pd.date_range(\"2013-08-01\", periods=6, freq=\"B\"),\n    columns=list(\"A\"),\n)\ndf.loc[df.index[3], \"A\"] = np.nan\ndf\ndf.bfill()\n```\n\n----------------------------------------\n\nTITLE: Aggregating with String Aliases using DataFrameGroupBy.aggregate/agg with pandas in Python\nDESCRIPTION: Shows how to use string aliases for aggregation with the aggregate (or alias agg) method after grouping columns 'C' and 'D' by 'A' or by multiple columns ('A', 'B'). Demonstrates both single and multi-key grouping with aggregate summation. Input is a DataFrame, output is an aggregated DataFrame with groups as indices. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby(\"A\")\ngrouped[[\"C\", \"D\"]].aggregate(\"sum\")\n\ngrouped = df.groupby([\"A\", \"B\"])\ngrouped.agg(\"sum\")\n```\n\n----------------------------------------\n\nTITLE: Dropping Missing Data using dropna in pandas (Python)\nDESCRIPTION: These examples illustrate removing rows or columns containing missing data using DataFrame.dropna and Series.dropna. Different axes and dtypes are used for flexibility. Only pandas is required for execution. The axis parameter determines if rows or columns are dropped, and dtype affects missing value handling. Output is the cleaned DataFrame or Series with missing entries removed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[np.nan, 1, 2], [1, 2, np.nan], [1, 2, 3]])\ndf\ndf.dropna()\ndf.dropna(axis=1)\n```\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, pd.NA], dtype=\"int64[pyarrow]\")\nser.dropna()\n```\n\n----------------------------------------\n\nTITLE: Incorrect Label-Based Slicing with Arithmetic on Labels - pandas - Python\nDESCRIPTION: Demonstrates a failed attempt to use arithmetic on string labels during a .loc slice operation in pandas, which raises an error. The :okexcept: marker signals exception handling. Illustrates that incrementing string labels does not yield a valid next index label. Requires Series 's' to have string labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ns.loc['c':'e' + 1]\n```\n\n----------------------------------------\n\nTITLE: Viewing the Head of a Renamed DataFrame in pandas (Python)\nDESCRIPTION: This snippet displays the first few rows of the DataFrame produced from the previous rename operation, using the head() method. It assumes 'air_quality_renamed' already exists and allows verification of the renaming process. The primary dependency is the existence of the 'air_quality_renamed' DataFrame. Inputs and outputs are not modified, and no side effects occur.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/05_add_columns.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nair_quality_renamed.head()\n```\n\n----------------------------------------\n\nTITLE: Combining DataFrames with combine_first - pandas - Python\nDESCRIPTION: This snippet uses DataFrame.combine_first to update missing (NaN) values in one DataFrame with corresponding values from another DataFrame. It sets up sample data with NaNs and demonstrates how combine_first fills missing values positionally, requiring pandas and potentially numpy for NaN generation. The function expects compatible DataFrame shapes or aligned indexes and outputs a DataFrame with gaps filled where possible.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(\n    [[np.nan, 3.0, 5.0], [-4.6, np.nan, np.nan], [np.nan, 7.0, np.nan]]\n)\ndf2 = pd.DataFrame([[-42.6, np.nan, -8.2], [-5.0, 1.6, 4]], index=[1, 2])\nresult = df1.combine_first(df2)\nresult\n```\n\n----------------------------------------\n\nTITLE: Merging on Columns and Index Levels - pandas - Python\nDESCRIPTION: This example merges DataFrames on a combination of a column and an index level using DataFrame.merge. It illustrates how to refer to columns and index level names simultaneously for merging without resetting the index. Required dependencies are pandas and indexed DataFrames/columns; merging can preserve or drop index levels depending on parameter usage. The merged result aligns rows using both a named index and a specified column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nleft_index = pd.Index([\"K0\", \"K0\", \"K1\", \"K2\"], name=\"key1\")\n\nleft = pd.DataFrame(\n    {\n        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n        \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n    },\n    index=left_index,\n)\n\nright_index = pd.Index([\"K0\", \"K1\", \"K2\", \"K2\"], name=\"key1\")\n\nright = pd.DataFrame(\n    {\n        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n        \"key2\": [\"K0\", \"K0\", \"K0\", \"K1\"],\n    },\n    index=right_index,\n)\n\nresult = left.merge(right, on=[\"key1\", \"key2\"])\nresult\n```\n\n----------------------------------------\n\nTITLE: Setting Output Decimal Precision for DataFrame Display - Python\nDESCRIPTION: Shows how to control decimal display precision by adjusting the 'display.precision' option for DataFrame output in pandas. Demonstrates both a high precision (7) and lower precision (4) with example output for a sample DataFrame. Useful for financial/numeric data reporting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(5, 5))\npd.set_option(\"display.precision\", 7)\ndf\npd.set_option(\"display.precision\", 4)\ndf\n```\n\n----------------------------------------\n\nTITLE: Stacking MultiIndex Pivot Table - pandas - Python\nDESCRIPTION: Demonstrates stacking a pivoted DataFrame to add multi-level indexing on the row axis, transforming columns into hierarchical row index levels. Useful for preparing data for further groupbased computations or tidy data outputs. Requires a previously constructed pivot table DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ntable.stack()\n```\n\n----------------------------------------\n\nTITLE: Loading Specific Columns from Parquet File in Python\nDESCRIPTION: Shows two methods for loading specific columns from a Parquet file: one that loads all data and then filters, and another that only loads the requested columns. This demonstrates efficient data loading techniques.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/scale.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncolumns = [\"id_0\", \"name_0\", \"x_0\", \"y_0\"]\n\n# Option 1: Load all data and filter\npd.read_parquet(\"timeseries_wide.parquet\")[columns]\n\n# Option 2: Only load specified columns\npd.read_parquet(\"timeseries_wide.parquet\", columns=columns)\n```\n\n----------------------------------------\n\nTITLE: Creating a pandas DataFrame with East Asian Unicode Characters - Python\nDESCRIPTION: This snippet initializes a pandas DataFrame containing Unicode columns and values, including Japanese characters. The DataFrame showcases how differing character widths can affect output alignment in pandas terminal display. No options are modified at this stage; the primary dependency is pandas, and inputs are hardcoded as dictionary items mapped to DataFrame columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"国籍\": [\"UK\", \"日本\"], \"名前\": [\"Alice\", \"しのぶ\"]})\ndf\n```\n\n----------------------------------------\n\nTITLE: Selecting DataFrame Subsets with Integer Lists\nDESCRIPTION: Shows how to select specific rows and columns from a DataFrame using .iloc with lists of integer positions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf1.iloc[[1, 3, 5], [1, 3]]\n```\n\n----------------------------------------\n\nTITLE: Implementing ExtensionArray with Operator Support in Python\nDESCRIPTION: Example demonstrating how to implement an ExtensionArray subclass with operator support using ExtensionScalarOpsMixin. Shows the setup for adding arithmetic and comparison operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.extensions import ExtensionArray, ExtensionScalarOpsMixin\n\n\nclass MyExtensionArray(ExtensionArray, ExtensionScalarOpsMixin):\n    pass\n\n\nMyExtensionArray._add_arithmetic_ops()\nMyExtensionArray._add_comparison_ops()\n```\n\n----------------------------------------\n\nTITLE: Using Explicit Compression Type with to_pickle in Pandas\nDESCRIPTION: Demonstrates serializing a DataFrame with explicit gzip compression using to_pickle and reading it back with read_pickle. The compression type must be specified in both operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_158\n\nLANGUAGE: python\nCODE:\n```\ndf.to_pickle(\"data.pkl.compress\", compression=\"gzip\")\nrt = pd.read_pickle(\"data.pkl.compress\", compression=\"gzip\")\nrt\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Data from URL - pandas - Python\nDESCRIPTION: This Python code uses pandas' 'read_csv' to download and read a CSV file from a URL directly into a DataFrame named 'tips'. Requires pandas to be imported (typically as pd). The 'url' variable stores the address, split for readability. Inputs: Internet access and the referenced URL. Output: DataFrame containing the CSV's contents.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nurl = (\\n    \"https://raw.githubusercontent.com/pandas-dev\"\\n    \"/pandas/main/pandas/tests/io/data/csv/tips.csv\"\\n)\\ntips = pd.read_csv(url)\\ntips\n```\n\n----------------------------------------\n\nTITLE: Joining DataFrames on a Key Column with DataFrame.join in pandas Python\nDESCRIPTION: Shows how to join DataFrames using DataFrame.join with the 'on' argument to align rows using a key column rather than the index. The right DataFrame is indexed by the join key. Requires pandas. Input: DataFrames with a common key; Output: joined DataFrame with columns from both tables aligned via key.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame(\n    {\n        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n        \"key\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n    }\n)\n\nright = pd.DataFrame({\"C\": [\"C0\", \"C1\"], \"D\": [\"D0\", \"D1\"]}, index=[\"K0\", \"K1\"])\n\nresult = left.join(right, on=\"key\")\nresult\n```\n\n----------------------------------------\n\nTITLE: Sorting a DataFrame by Index and Values - Python\nDESCRIPTION: These snippets demonstrate two sorting methods: df.sort_index(axis=1, ascending=False) sorts DataFrame columns in descending order, and df.sort_values(by=\"B\") sorts the DataFrame rows by the values in column 'B'. Both return a sorted copy by default.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf.sort_index(axis=1, ascending=False)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.sort_values(by=\"B\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating New DataFrameGroupBy.value_counts Behavior in Pandas 3.0\nDESCRIPTION: This snippet illustrates the new behavior of DataFrameGroupBy.value_counts in Pandas 3.0. It shows how the method now maintains the order of non-grouping columns within groups when sort=False is specified.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v3.0.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"a\", sort=True).value_counts(sort=False)\n```\n\n----------------------------------------\n\nTITLE: Counting Substrings with Nullable Output - pandas - Python\nDESCRIPTION: This snippet counts occurrences of the substring 'a' in a string Series with the experimental string dtype, and outputs the result which is an Int64 dtype (nullable integer) Series. Demonstrates integration of string operations and nullable integer output in pandas 1.0.0. Requires pandas 1.0.0 or higher.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ns.str.count(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Pivot Tables in Pandas - Python\nDESCRIPTION: Demonstrates how to construct a Categorical, create a DataFrame, and use pandas.pivot_table with categorical columns. Requires pandas. The input is a DataFrame with categorical and standard columns; the output is a pivot table aggregating data based on index and categories. The 'observed' parameter controls treatment of unused categories.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nraw_cat = pd.Categorical([\"a\", \"a\", \"b\", \"b\"], categories=[\"a\", \"b\", \"c\"])\ndf = pd.DataFrame({\"A\": raw_cat, \"B\": [\"c\", \"d\", \"c\", \"d\"], \"values\": [1, 2, 3, 4]})\npd.pivot_table(df, values=\"values\", index=[\"A\", \"B\"], observed=False)\n```\n\n----------------------------------------\n\nTITLE: Concatenating DataFrames and Handling Index - Pandas Python\nDESCRIPTION: Demonstrates concatenation of two DataFrames with overlapping or identical index, highlighting the use of the ignore_index parameter to produce a new continuous index. Dependencies: pandas, numpy. The code creates two DataFrames, optionally concatenates them while resetting the index, and displays the result.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nrng = pd.date_range(\"2000-01-01\", periods=6)\ndf1 = pd.DataFrame(np.random.randn(6, 3), index=rng, columns=[\"A\", \"B\", \"C\"])\ndf2 = df1.copy()\n\ndf = pd.concat([df1, df2], ignore_index=True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Working with NA in Nullable Series using pandas (Python)\nDESCRIPTION: These snippets show how NA values behave in pandas Series with nullable dtypes such as Int64. The code demonstrates Series construction with None, element access, and identity comparison with pd.NA. Dependencies are pandas (1.0+) with support for nullable dtypes. Indexing and type assertions are shown, outputting NA as the value for missing entries.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, None], dtype=\"Int64\")\ns\ns[2]\ns[2] is pd.NA\n```\n\n----------------------------------------\n\nTITLE: Sorting with keys in pandas DataFrames and Series\nDESCRIPTION: Examples of using the new key parameter in sorting methods to apply a function to the values before sorting. This allows for custom sorting logic like case-insensitive string sorting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['C', 'a', 'B'])\ns\n```\n\nLANGUAGE: python\nCODE:\n```\ns.sort_values()\n```\n\nLANGUAGE: python\nCODE:\n```\ns.sort_values(key=lambda x: x.str.lower())\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': ['C', 'C', 'a', 'a', 'B', 'B'],\n                  'b': [1, 2, 3, 4, 5, 6]})\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.sort_values(by=['a'], key=lambda col: col.str.lower())\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table from URL in Python with pandas\nDESCRIPTION: Demonstrates using pandas.read_html() to read an HTML table from a URL. This example reads bank failure data from the FDIC website and returns a list of DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_88\n\nLANGUAGE: python\nCODE:\n```\nurl = \"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list\"\n\npd.read_html(url)\n```\n\n----------------------------------------\n\nTITLE: Creating and Transforming DataFrames with Functions - pandas - Python\nDESCRIPTION: Demonstrates use of .transform() to apply a NumPy function, string function name, and a lambda over all DataFrame elements, yielding a DataFrame of same shape. Shows equivalence between numpy function and .transform(), and supports arbitrary user functions. Output DataFrame has same index and columns as input.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ntsdf = pd.DataFrame(\n    np.random.randn(10, 3),\n    columns=[\"A\", \"B\", \"C\"],\n    index=pd.date_range(\"1/1/2000\", periods=10),\n)\ntsdf.iloc[3:7] = np.nan\ntsdf\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf.transform(np.abs)\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf.transform(\"abs\")\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf.transform(lambda x: x.abs())\n```\n\nLANGUAGE: python\nCODE:\n```\nnp.abs(tsdf)\n```\n\n----------------------------------------\n\nTITLE: Concatenating DataFrames in Pandas (Equivalent to SQL UNION)\nDESCRIPTION: These snippets show how to concatenate DataFrames in pandas using concat, which is equivalent to SQL's UNION and UNION ALL operations. It also demonstrates how to remove duplicates.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npd.concat([df1, df2])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.concat([df1, df2]).drop_duplicates()\n```\n\n----------------------------------------\n\nTITLE: Assigning NA to DataFrame Column and Checking Dtypes (Python)\nDESCRIPTION: This snippet illustrates assigning pd.NA to a new DataFrame column, resulting in an 'object' dtype, and then displays the resulting data types. This can impact performance negatively compared to explicitly specifying 'boolean' dtype. Input is an empty DataFrame; output lists column data types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/boolean.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame()\ndf['objects'] = pd.NA\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Setting Row Count Threshold for DataFrame.info Null Count Display - Python\nDESCRIPTION: Demonstrates the effect of the 'max_info_rows' option, which limits null checking and reporting in DataFrame.info for performance reasons. Shows default, raised, and reduced settings with a sample DataFrame with missing values. Required dependencies: pandas, numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.choice([0, 1, np.nan], size=(10, 10)))\ndf\npd.set_option(\"max_info_rows\", 11)\ndf.info()\npd.set_option(\"max_info_rows\", 5)\ndf.info()\npd.reset_option(\"max_info_rows\")\n```\n\n----------------------------------------\n\nTITLE: Using Series.get() for Safe Label Lookup - pandas - Python\nDESCRIPTION: Demonstrates safe retrieval of Series values with the 'get' method, which returns None or a specified default value (such as np.nan) for missing labels, instead of raising an exception. Helps prevent errors when accessing possibly-missing entries.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ns.get(\"f\")\n\ns.get(\"f\", np.nan)\n```\n\n----------------------------------------\n\nTITLE: Appending Series as Row to DataFrame in Python\nDESCRIPTION: Example of appending a Series as a single row to a DataFrame by first converting the Series to a DataFrame with transpose and then concatenating.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ns2 = pd.Series([\"X0\", \"X1\", \"X2\", \"X3\"], index=[\"A\", \"B\", \"C\", \"D\"])\nresult = pd.concat([df1, s2.to_frame().T], ignore_index=True)\nresult\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns in Pandas (Equivalent to SQL SELECT)\nDESCRIPTION: This snippet shows how to select specific columns from a pandas DataFrame, which is equivalent to the SQL SELECT statement. It uses square bracket notation with a list of column names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntips[[\"total_bill\", \"tip\", \"smoker\", \"time\"]]\n```\n\n----------------------------------------\n\nTITLE: Pivoting DataFrames Using index/columns/values - pandas - Python\nDESCRIPTION: This snippet uses pandas' DataFrame.pivot method to reshape data by turning unique 'variable' values into columns, with 'date' as the index and associated 'value' as entries. Requires pandas and a pre-existing DataFrame as input. Produces a reshaped DataFrame where columns are unique values from the 'variable' field.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npivoted = df.pivot(index=\"date\", columns=\"variable\", values=\"value\")\npivoted\n```\n\n----------------------------------------\n\nTITLE: Formatting DataFrame Display with Styler - pandas - Python\nDESCRIPTION: Demonstrates using pandas' Styler to format and relabel DataFrame display values, including floating-point precision, thousands and decimal separators, and index/column label formatting. Dependencies: pandas, numpy. Parameters include 'precision', 'thousands', 'decimal', 'axis', and relabeling; input is a small DataFrame, output is a styled DataFrame ready for display in Jupyter or as HTML/Excel/LaTeX. Does not alter original data, only display representation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\"strings\": [\"Adam\", \"Mike\"], \"ints\": [1, 3], \"floats\": [1.123, 1000.23]}\n)\ndf.style.format(precision=3, thousands=\".\", decimal=\",\").format_index(\n    str.upper, axis=1\n).relabel_index([\"row 1\", \"row 2\"], axis=0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom DataFrame Accessor in Python\nDESCRIPTION: Example showing how to create a custom 'geo' accessor for pandas DataFrames using the register_dataframe_accessor decorator. The accessor validates geographic data and provides methods for calculating center points and plotting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@pd.api.extensions.register_dataframe_accessor(\"geo\")\nclass GeoAccessor:\n    def __init__(self, pandas_obj):\n        self._validate(pandas_obj)\n        self._obj = pandas_obj\n\n    @staticmethod\n    def _validate(obj):\n        # verify there is a column latitude and a column longitude\n        if \"latitude\" not in obj.columns or \"longitude\" not in obj.columns:\n            raise AttributeError(\"Must have 'latitude' and 'longitude'.\")\n\n    @property\n    def center(self):\n        # return the geographic center point of this DataFrame\n        lat = self._obj.latitude\n        lon = self._obj.longitude\n        return (float(lon.mean()), float(lat.mean()))\n\n    def plot(self):\n        # plot this array's data on a map, e.g., using Cartopy\n        pass\n```\n\n----------------------------------------\n\nTITLE: New Behavior: Groupby Aggregate Relabeling Preserves Results - python\nDESCRIPTION: With pandas 1.1.0, using .groupby().agg with as_index=False and relabeled output columns correctly preserves result data, showing expected aggregation output. Demonstrates a groupby, named aggregation, and output listing the group and corresponding calculated values. Requires pandas 1.1.0+.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby(\"key\", as_index=False)\\nresult = grouped.agg(min_val=pd.NamedAgg(column=\"val\", aggfunc=\"min\"))\\nresult\n```\n\n----------------------------------------\n\nTITLE: Combine Multiple CSV Files into a Single DataFrame - Pandas Python\nDESCRIPTION: Demonstrates reading multiple CSV files and concatenating their contents into a single DataFrame. Two approaches are shown: explicit construction of filenames and use of glob to match patterns. Dependencies: pandas, numpy, glob, os (for file creation and pattern matching). Inputs are multiple CSV file paths; output is a combined DataFrame. Useful for batch data ingestion from external files.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nfor i in range(3):\n    data = pd.DataFrame(np.random.randn(10, 4))\n    data.to_csv(\"file_{}.csv\".format(i))\n\nfiles = [\"file_0.csv\", \"file_1.csv\", \"file_2.csv\"]\nresult = pd.concat([pd.read_csv(f) for f in files], ignore_index=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport glob\nimport os\n\nfiles = glob.glob(\"file_*.csv\")\n```\n\n----------------------------------------\n\nTITLE: Creating Categorical Series in Python\nDESCRIPTION: Demonstrates how to create a categorical Series by specifying dtype=\"category\" when constructing a Series. This method automatically infers categories from the data and sets them as unordered.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\ns\n```\n\n----------------------------------------\n\nTITLE: Creating Example DataFrame for Parquet Serialization using pandas (Python)\nDESCRIPTION: This block creates a pandas DataFrame with various dtypes, including datetime with timezones and ordered categoricals, for Parquet examples. Requires pandas and numpy. The output displays the DataFrame and its dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_206\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"a\": list(\"abc\"),\n        \"b\": list(range(1, 4)),\n        \"c\": np.arange(3, 6).astype(\"u1\"),\n        \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n        \"e\": [True, False, True],\n        \"f\": pd.date_range(\"20130101\", periods=3),\n        \"g\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n        \"h\": pd.Categorical(list(\"abc\")),\n        \"i\": pd.Categorical(list(\"abc\"), ordered=True),\n    }\n)\n\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Querying SQL Databases Using pandas.read_sql_query (Python)\nDESCRIPTION: Shows querying SQL databases directly from pandas using raw SQL statements (with read_sql_query) and specifying various query forms. Assumes a valid pandas DataFrame is written to SQL, and an engine or connection is provided. Examples include reading all data from a table, selecting filtered columns, writing data in chunks, and iterating over query results using chunksize for memory management. Outputs iterators or DataFrames depending on the function usage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_231\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_query(\"SELECT * FROM data\", engine)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_query(\"SELECT id, Col_1, Col_2 FROM data WHERE id = 42;\", engine)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(20, 3), columns=list(\"abc\"))\ndf.to_sql(name=\"data_chunks\", con=engine, index=False)\n```\n\nLANGUAGE: python\nCODE:\n```\nfor chunk in pd.read_sql_query(\"SELECT * FROM data_chunks\", engine, chunksize=5):\n    print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame Columns to Best Supported dtypes with convert_dtypes - pandas - Python\nDESCRIPTION: This snippet creates a DataFrame with columns containing text (with None), numbers (with np.nan), and booleans, and demonstrates the dtypes before and after using the convert_dtypes() method. convert_dtypes attempts to apply the most appropriate extension dtype (like string, Int64, boolean) for each column considering missing values. The code outputs both the original and converted DataFrames and their dtypes. Requires pandas 1.0.0 or later, and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'x': ['abc', None, 'def'],\\n               'y': [1, 2, np.nan],\\n               'z': [True, False, True]})\\ndf\\ndf.dtypes\n```\n\nLANGUAGE: python\nCODE:\n```\nconverted = df.convert_dtypes()\\nconverted\\nconverted.dtypes\n```\n\n----------------------------------------\n\nTITLE: Resetting All Options in pandas - Python\nDESCRIPTION: Demonstrates resetting all customizable pandas options to their default values. Uses pandas.reset_option with the 'all' argument. Useful for ensuring a clean environment state, especially in test or notebook contexts. No parameters beyond the string 'all'; affects all subsequent pandas API operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.reset_option(\"all\")\n```\n\n----------------------------------------\n\nTITLE: Merging DataFrames on Multiple Keys with pandas in Python\nDESCRIPTION: Shows how to merge two DataFrames on multiple keys using the 'how' argument in pandas' merge function. Demonstrates left, right, outer, and inner joins on columns 'key1' and 'key2', controlling which key combinations appear in the result. Requires pandas. Inputs: two DataFrames, keys to join on, join type; Output: merged DataFrame with keys determined by join type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame(\n   {\n      \"key1\": [\"K0\", \"K0\", \"K1\", \"K2\"],\n      \"key2\": [\"K0\", \"K1\", \"K0\", \"K1\"],\n      \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n      \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n   }\n)\nright = pd.DataFrame(\n   {\n      \"key1\": [\"K0\", \"K1\", \"K1\", \"K2\"],\n      \"key2\": [\"K0\", \"K0\", \"K0\", \"K0\"],\n      \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n      \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n   }\n)\nresult = pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"])\nresult\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.merge(left, right, how=\"right\", on=[\"key1\", \"key2\"])\nresult\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.merge(left, right, how=\"outer\", on=[\"key1\", \"key2\"])\nresult\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.merge(left, right, how=\"inner\", on=[\"key1\", \"key2\"])\nresult\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.merge(left, right, how=\"cross\")\nresult\n```\n\n----------------------------------------\n\nTITLE: Stacked Bar Plot for DataFrame (Python)\nDESCRIPTION: Plots a DataFrame as a stacked bar chart using DataFrame.plot.bar(stacked=True). All columns' values for each row are stacked in vertical bars, showing subtotal contributions. Dependencies: pandas, numpy, matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@savefig bar_plot_stacked_ex.png\\ndf2.plot.bar(stacked=True);\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table with Header Specification in Python\nDESCRIPTION: Shows how to specify which row should be used as the header when reading HTML tables with pandas.read_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_93\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, header=0)\n```\n\n----------------------------------------\n\nTITLE: Initializing DataFrames with MultiIndex for Stack/Unstack - pandas - Python\nDESCRIPTION: Shows how to create a MultiIndex from arrays, and then construct a DataFrame with random data indexed by this MultiIndex and labeled columns ['A', 'B']. Also slices the DataFrame to get the first four rows as df2. Sets up structures for demonstrating stack and unstack methods. Requires pandas and numpy libraries.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ntuples = [\n   [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n   [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n]\nindex = pd.MultiIndex.from_arrays(tuples, names=[\"first\", \"second\"])\ndf = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"])\ndf2 = df[:4]\ndf2\n```\n\n----------------------------------------\n\nTITLE: Modifying and Dropping DataFrame Columns with pandas - Python\nDESCRIPTION: This snippet demonstrates how to modify values in an existing DataFrame column, create a new column via vectorized arithmetic operations, and remove a column using pandas in Python. It requires the pandas library and assumes 'tips' is a pre-existing DataFrame. The snippet assigns a recalculated 'total_bill', adds a 'new_bill' column as half the total, outputs the modified DataFrame, then removes the 'new_bill' column. Inputs are column names and transformation logic; the output is the mutated DataFrame with specified columns updated or dropped.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/column_operations.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips[\"total_bill\"] = tips[\"total_bill\"] - 2\n    tips[\"new_bill\"] = tips[\"total_bill\"] / 2\n    tips\n\n    tips = tips.drop(\"new_bill\", axis=1)\n```\n\n----------------------------------------\n\nTITLE: Constructing TimedeltaIndex from Mixed Inputs - pandas - Python\nDESCRIPTION: Illustrates construction of TimedeltaIndex from a heterogeneous list of strings, np.timedelta64, and datetime.timedelta instances. Supports inclusion of missing values via np.nan/pd.NaT. Returns a pandas Index typed for timedeltas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\npd.TimedeltaIndex(\n    [\n        \"1 days\",\n        \"1 days, 00:00:05\",\n        np.timedelta64(2, \"D\"),\n        datetime.timedelta(days=2, seconds=2),\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing PeriodIndex and Checking dtypes (ipython, Python)\nDESCRIPTION: Displays creation and inspection of a PeriodIndex, specifically focusing on dtype behavior before and after the introduction of 'period' dtype extension. Includes use of pandas type checking utilities. Requires pandas; outputs include PeriodIndex objects and boolean checks. Before change, PeriodIndex had integer dtype; now it uses a period extension dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_33\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pi = pd.PeriodIndex(['2016-08-01'], freq='D')\n\nIn [2]: pi\nOut[2]: PeriodIndex(['2016-08-01'], dtype='int64', freq='D')\n\nIn [3]: pd.api.types.is_integer_dtype(pi)\nOut[3]: True\n\nIn [4]: pi.dtype\nOut[4]: dtype('int64')\n```\n\nLANGUAGE: python\nCODE:\n```\npi = pd.PeriodIndex([\"2016-08-01\"], freq=\"D\")\npi\npd.api.types.is_integer_dtype(pi)\npd.api.types.is_period_dtype(pi)\npi.dtype\ntype(pi.dtype)\n```\n\n----------------------------------------\n\nTITLE: Describing All Available pandas Options - Python\nDESCRIPTION: Shows how to print a listing and descriptions of all options supported by pandas using the describe_option function. Only requires pandas. No arguments fetches descriptions for every available option. Output is sent directly to the standard output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.describe_option()\n```\n\n----------------------------------------\n\nTITLE: Error on Supplying Flags with Compiled Regex Patterns (Python)\nDESCRIPTION: Demonstrates an error scenario where calling str.replace() with both a compiled regex object and a flags argument results in a ValueError. Illustrates correct and incorrect API usage when working with precompiled patterns in pandas. Useful as documentation of API constraints.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n@verbatim\nIn [1]: s3.str.replace(regex_pat, 'XX-XX ', flags=re.IGNORECASE)\n---------------------------------------------------------------------------\nValueError: case and flags cannot be set when pat is a compiled regex\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Rolling Window Apply with Numba Engine in pandas (ipython)\nDESCRIPTION: Demonstrates how to use pandas' rolling apply with a custom function using the Numba JIT engine for significant performance improvements, including initial compilation overhead and subsequent speed gains. Requires pandas, numpy, a large Series, and Numba installed. The code benchmarks Numba and Cython engines for a rolling calculation, showing that using Numba is beneficial with large datasets. Inputs: pandas Series; function to apply. Outputs: processed Series with rolling calculations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: data = pd.Series(range(1_000_000))  # noqa: E225\n\nIn [2]: roll = data.rolling(10)\n\nIn [3]: def f(x):\n   ...:     return np.sum(x) + 5\n# Run the first time, compilation time will affect performance\nIn [4]: %timeit -r 1 -n 1 roll.apply(f, engine='numba', raw=True)\n1.23 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n# Function is cached and performance will improve\nIn [5]: %timeit roll.apply(f, engine='numba', raw=True)\n188 ms ± 1.93 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nIn [6]: %timeit roll.apply(f, engine='cython', raw=True)\n3.92 s ± 59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n\n----------------------------------------\n\nTITLE: Using .take for Efficient DataFrame, Series, and Index Retrieval - pandas - Python\nDESCRIPTION: Shows how to use the .take method to retrieve rows or columns by position using integers or integer arrays, supporting negative values and providing a faster alternative to fancy indexing in many scenarios. Not suitable for boolean indices, and may differ from selection with boolean arrays. Works for pandas Index, Series, and DataFrame objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.Index(np.random.randint(0, 1000, 10))\nindex\npositions = [0, 9, 3]\nindex[positions]\nindex.take(positions)\n```\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(np.random.randn(10))\nser.iloc[positions]\nser.take(positions)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrm = pd.DataFrame(np.random.randn(5, 3))\nfrm.take([1, 4, 3])\nfrm.take([0, 2], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Set Operations on Pandas Index Objects\nDESCRIPTION: This snippet shows how to perform set operations like difference(), union(), and symmetric_difference() on Pandas Index objects. It also demonstrates type casting for unions between indexes with different dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\na = pd.Index(['c', 'b', 'a'])\nb = pd.Index(['c', 'e', 'd'])\na.difference(b)\n\nidx1 = pd.Index([1, 2, 3, 4])\nidx2 = pd.Index([2, 3, 4, 5])\nidx1.symmetric_difference(idx2)\n\nidx1 = pd.Index([0, 1, 2])\nidx2 = pd.Index([0.5, 1.5])\nidx1.union(idx2)\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table with Combined Parameters in Python\nDESCRIPTION: Shows how to combine multiple parameters when reading HTML tables with pandas.read_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_99\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, match=\"Metcalf Bank\", index_col=0)\n```\n\n----------------------------------------\n\nTITLE: Initializing DataFrame Column with NA Values and Dtype Warning (pandas, Python)\nDESCRIPTION: Demonstrates that assigning pd.NA directly to a DataFrame column (df['objects'] = pd.NA) results in the column taking the object dtype, which is sub-optimal for performance. Shows how to check resulting column dtypes. Requires pandas as pd and a DataFrame df.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame()\ndf['objects'] = pd.NA\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Basic CSV Data Reading Example\nDESCRIPTION: Simple example showing how to read a CSV string containing basic columnar data using pandas read_csv function with StringIO.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\npd.read_csv(StringIO(data))\n```\n\n----------------------------------------\n\nTITLE: Importing Matplotlib for Plotting (Python)\nDESCRIPTION: Initializes matplotlib's pyplot interface as plt, which is used throughout for figure management and display in pandas' visualizations. Closing all plots ensures a clean state before generating new figures. Dependencies include matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\\n\\nplt.close(\"all\")\n```\n\n----------------------------------------\n\nTITLE: Indexing Series with Scalar Indexer (Deprecation Example) - pandas ipython\nDESCRIPTION: Exposes a deprecation warning when a scalar floating point indexer is used to access a pandas Series with an Int64Index, showing a future warning about required integer indexing. Demonstrates forward compatibility and the new recommended usage for Series indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_22\n\nLANGUAGE: ipython\nCODE:\n```\n# non-floating point indexes can only be indexed by integers / labels\nIn [1]: pd.Series(1, np.arange(5))[3.0]\n        pandas/core/index.py:469: FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point\nOut[1]: 1\n```\n\n----------------------------------------\n\nTITLE: Styling and Formatting a Large DataFrame with Magnification in Pandas (Python)\nDESCRIPTION: This code creates a large DataFrame with cumulative sums of random values and applies gradient background coloring and custom styles for magnified display on hover. It uses seaborn for colormap creation and numpy for data generation. Required dependencies: pandas, numpy, seaborn. The styled DataFrame is configured for small default font size, increased font and cell size on hover, a caption, custom precision, and uses the previously defined magnify function for table styles.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ncmap = sns.diverging_palette(5, 250, as_cmap=True)\nbigdf = pd.DataFrame(np.random.default_rng(25).standard_normal((20, 25))).cumsum()\n\nbigdf.style.background_gradient(cmap, axis=1).set_properties(\n    **{\"max-width\": \"80px\", \"font-size\": \"1pt\"}\n).set_caption(\"Hover to magnify\").format(precision=2).set_table_styles(magnify())\n```\n\n----------------------------------------\n\nTITLE: Selecting Smallest and Largest Values in pandas Series and DataFrames (Python)\nDESCRIPTION: Demonstrates use of nsmallest and nlargest methods to quickly access the smallest or largest n values in a Series or DataFrame. Particularly efficient for large datasets. Inputs: Series or DataFrame, parameters: n, columns. Outputs: data with top n or bottom n by value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.random.permutation(10))\ns\ns.sort_values()\ns.nsmallest(3)\ns.nlargest(3)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"a\": [-2, -1, 1, 10, 8, 11, -1],\n        \"b\": list(\"abdceff\"),\n        \"c\": [1.0, 2.0, 4.0, 3.2, np.nan, 3.0, 4.0],\n    }\n)\ndf.nlargest(3, \"a\")\ndf.nlargest(5, [\"a\", \"c\"])\ndf.nsmallest(3, \"a\")\ndf.nsmallest(5, [\"a\", \"c\"])\n```\n\n----------------------------------------\n\nTITLE: Customizing S3 Endpoint Access when Reading JSON with Pandas (Python)\nDESCRIPTION: Demonstrates how to specify a custom S3 endpoint (such as MinIO or a local S3-compatible store) for pandas read_json by passing custom client_kwargs in the storage_options dictionary. Requires s3fs/fsspec, applicable when non-standard endpoints are used.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nstorage_options = {\"client_kwargs\": {\"endpoint_url\": \"http://127.0.0.1:5555\"}}\ndf = pd.read_json(\"s3://pandas-test/test-1\", storage_options=storage_options)\n```\n\n----------------------------------------\n\nTITLE: Applying String Case Methods in pandas DataFrame - Python\nDESCRIPTION: This snippet creates a pandas DataFrame with a single string column and applies str.upper(), str.lower(), and str.title() to generate new columns with transformed values. It requires the pandas library and assumes input column data of type string. The result shows the original and transformed case representations for each name; inputs must be strings, and non-string inputs may result in errors or unexpected results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/case.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfirstlast = pd.DataFrame({\"string\": [\"John Smith\", \"Jane Cook\"]})\\nfirstlast[\"upper\"] = firstlast[\"string\"].str.upper()\\nfirstlast[\"lower\"] = firstlast[\"string\"].str.lower()\\nfirstlast[\"title\"] = firstlast[\"string\"].str.title()\\nfirstlast\n```\n\n----------------------------------------\n\nTITLE: Renaming a Column in pandas DataFrame using Python\nDESCRIPTION: This snippet demonstrates how to rename the 'total_bill' column to 'total_bill_2' in the 'tips' DataFrame by passing a dictionary to the 'columns' argument of the rename method. Requires the pandas library and a DataFrame named 'tips'. The rename operation is not performed in place unless 'inplace=True' is provided. Returns a DataFrame with the renamed column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/column_selection.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntips.rename(columns={\"total_bill\": \"total_bill_2\"})\n```\n\n----------------------------------------\n\nTITLE: String Operations on Arrow-Backed Series (pandas Python API)\nDESCRIPTION: Demonstrates use of string accessors such as .str.startswith on Series with ArrowDtype(pa.string()), which leverages optimized Arrow string kernels for performance. Requires pandas and pyarrow, with expected outputs being boolean Series indicating results of the string operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nser_str = pd.Series([\"a\", \"b\", None], dtype=pd.ArrowDtype(pa.string()))\nser_str.str.startswith(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Unstacking Index Levels to Columns - pandas - Python\nDESCRIPTION: Illustrates the use of stacked.unstack() to pivot the deepest row index back into columns, restoring or altering the table's axis hierarchy. Also demonstrates unstacking by level number: 1 (second), or 0 (first), returning DataFrames with corresponding column levels. Assumes 'stacked' was created from previous stacking operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nstacked.unstack()\nstacked.unstack(1)\nstacked.unstack(0)\n```\n\n----------------------------------------\n\nTITLE: Subsetting Data in pandas\nDESCRIPTION: Shows how to subset data in pandas using the query method, boolean indexing, and loc accessor.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": np.random.randn(10), \"b\": np.random.randn(10)})\ndf.query(\"a <= b\")\ndf[df[\"a\"] <= df[\"b\"]]\ndf.loc[df[\"a\"] <= df[\"b\"]]\n```\n\n----------------------------------------\n\nTITLE: Series Struct Accessor Methods\nDESCRIPTION: Struct accessor methods for Arrow struct-dtype Series including dtypes, field and explode operations\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/series.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nSeries.struct.dtypes\nSeries.struct.field\nSeries.struct.explode\n```\n\n----------------------------------------\n\nTITLE: Quickly Plotting Air Quality Data with pandas and Matplotlib in Python\nDESCRIPTION: This snippet generates a default line plot for each numeric column in the DataFrame using the plot() method, then displays the plot with plt.show(). No parameters are required, but the DataFrame must be loaded. The output is a multi-line plot, one for each measurement location, enabling a visual overview of trends.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nair_quality.plot()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Checking DataFrame Emptiness in Python with Pandas\nDESCRIPTION: This snippet demonstrates how to check if a Pandas DataFrame is empty using the 'empty' attribute. It creates a DataFrame with columns 'A', 'B', and 'C', and checks its emptiness.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame(columns=list(\"ABC\")).empty\n```\n\n----------------------------------------\n\nTITLE: Summarizing Data with describe() in Python using Pandas\nDESCRIPTION: This snippet shows how to use the 'describe' method to compute summary statistics for a Series or DataFrame. It demonstrates usage with both numerical and non-numerical data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nseries = pd.Series(np.random.randn(1000))\nseries[::2] = np.nan\nseries.describe()\nframe = pd.DataFrame(np.random.randn(1000, 5), columns=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nframe.iloc[::2] = np.nan\nframe.describe()\nseries.describe(percentiles=[0.05, 0.25, 0.75, 0.95])\n```\n\n----------------------------------------\n\nTITLE: Inferring Compression Type from File Extension in Pandas\nDESCRIPTION: Shows how to use 'infer' to automatically determine the compression type from the file extension (.xz in this case) when pickling and reading a DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_159\n\nLANGUAGE: python\nCODE:\n```\ndf.to_pickle(\"data.pkl.xz\", compression=\"infer\")\nrt = pd.read_pickle(\"data.pkl.xz\", compression=\"infer\")\nrt\n```\n\n----------------------------------------\n\nTITLE: Creating Series from Dictionary with Explicit Index - pandas - Python\nDESCRIPTION: This snippet creates a pandas Series from a dictionary where an explicit index is provided, possibly including labels not present in the dictionary. The values are aligned to index labels, with missing mappings filled with NaN. It highlights pandas' ability to reindex and handle missing data automatically.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nd = {\"a\": 0.0, \"b\": 1.0, \"c\": 2.0}\npd.Series(d)\npd.Series(d, index=[\"b\", \"c\", \"d\", \"a\"])\n```\n\n----------------------------------------\n\nTITLE: Comparing StringDtype and ArrowDtype with PyArrow in pandas (Python)\nDESCRIPTION: Illustrates the difference between using pandas' StringDtype(\\\"pyarrow\\\") and explicitly specifying ArrowDtype with a pyarrow string type for pandas Series construction. Explains that StringDtype(\\\"pyarrow\\\") and ArrowDtype(pa.string()) may not be strictly equivalent, particularly in their returned types and backend behavior. Requires pandas and pyarrow, with Series constructed from a list of strings, and demonstrates dtype comparison and string method usage on both.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\ndata = list(\"abc\")\nser_sd = pd.Series(data, dtype=\"string[pyarrow]\")\nser_ad = pd.Series(data, dtype=pd.ArrowDtype(pa.string()))\nser_ad.dtype == ser_sd.dtype\nser_sd.str.contains(\"a\")\nser_ad.str.contains(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Categorical Properties in Python\nDESCRIPTION: Demonstrates how to access the categories and ordered properties of categorical data using the .cat accessor. This allows inspection and manipulation of the underlying categorical structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\ns.cat.categories\ns.cat.ordered\n\ns = pd.Series(pd.Categorical([\"a\", \"b\", \"c\", \"a\"], categories=[\"c\", \"b\", \"a\"]))\ns.cat.categories\ns.cat.ordered\n```\n\n----------------------------------------\n\nTITLE: Handling nanosecond precision datetime roundtrip in pandas JSON I/O - Python\nDESCRIPTION: This snippet illustrates how to ensure nanosecond precision for datetime values in JSON serialization and deserialization, using pandas' to_json and read_json functions. It shows attempts with date_unit='ms' (fails for ns precision), pandas' automatic detection, and explicit date_unit='ns'. Dependencies: pandas, io.StringIO; inputs: DataFrame with datetime columns, outputs: DataFrame after round-trip with correct precision.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_78\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\njson = dfj2.to_json(date_format=\"iso\", date_unit=\"ns\")\n\n# Try to parse timestamps as milliseconds -> Won't Work\ndfju = pd.read_json(StringIO(json), date_unit=\"ms\")\ndfju\n\n# Let pandas detect the correct precision\ndfju = pd.read_json(StringIO(json))\ndfju\n\n# Or specify that all timestamps are in nanoseconds\ndfju = pd.read_json(StringIO(json), date_unit=\"ns\")\ndfju\n```\n\n----------------------------------------\n\nTITLE: Joining Multiple DataFrames on Index - pandas - Python\nDESCRIPTION: This code joins multiple DataFrames at once by passing a list to DataFrame.join, automatically joining on their indexes. It demonstrates combining two right-hand DataFrames into a left DataFrame's context using pandas. Input DataFrames should have compatible or aligned indexes; the result is a DataFrame extended with columns from all joined sources based on index matching.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nright2 = pd.DataFrame({\"v\": [7, 8, 9]}, index=[\"K1\", \"K1\", \"K2\"])\nresult = left.join([right, right2])\n```\n\n----------------------------------------\n\nTITLE: Using divmod with Pandas Series and Index in Python\nDESCRIPTION: These snippets demonstrate use of Python's built-in divmod for elementwise floor division and modulo with both Series and Index objects. They also show broadcasting when the divisor is an array. This is useful for partitioning or discretizing data within pandas. Results are returned as the same type (Series or Index) for div and rem.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.arange(10))\ns\ndiv, rem = divmod(s, 3)\ndiv\nrem\n```\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index(np.arange(10))\nidx\ndiv, rem = divmod(idx, 3)\ndiv\nrem\n```\n\nLANGUAGE: python\nCODE:\n```\ndiv, rem = divmod(s, [2, 2, 3, 3, 4, 4, 5, 5, 6, 6])\ndiv\nrem\n```\n\n----------------------------------------\n\nTITLE: GroupBy Aggregation and Separate Renaming - Pandas - Python\nDESCRIPTION: Shows the new idiomatic approach to aggregation and column renaming: use groupby.agg(dict) to aggregate, then .rename(columns=dict) to relabel columns. Input: DataFrame df; output: DataFrame with aggregations and explicit column names. Required for forward compatibility.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\n(df.groupby('A')\n   .agg({'B': 'sum', 'C': 'min'})\n   .rename(columns={'B': 'foo', 'C': 'bar'})\n )\n```\n\n----------------------------------------\n\nTITLE: Testing exceptions in Python using pytest\nDESCRIPTION: Example of how to test for specific exceptions using pytest.raises context manager.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nwith pytest.raises(ValueError, match=\"an error\"):\n    raise ValueError(\"an error\")\n```\n\n----------------------------------------\n\nTITLE: Reading HTML with Custom HTTP Headers in Python using pandas\nDESCRIPTION: Shows how to pass custom HTTP headers with pandas.read_html() when making a request to a URL. This allows for setting User-Agent, Authentication tokens, and other HTTP request headers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_89\n\nLANGUAGE: python\nCODE:\n```\nurl = 'https://www.sump.org/notes/request/' # HTTP request reflector\n\npd.read_html(url)\n```\n\nLANGUAGE: python\nCODE:\n```\nheaders = {\n   'User-Agent':'Mozilla Firefox v14.0',\n   'Accept':'application/json',\n   'Connection':'keep-alive',\n   'Auth':'Bearer 2*/f3+fe68df*4'\n}\n\npd.read_html(url, storage_options=headers)\n```\n\n----------------------------------------\n\nTITLE: Memory-Efficient XML Parsing with pandas.read_xml and iterparse (Python)\nDESCRIPTION: Illustrates usage of pandas.read_xml with the iterparse argument for handling very large XML files efficiently. The snippet demonstrates how to specify repeating nodes and desired descendant fields, leveraging lxml's or ElementTree's iterparse to read massive datasets without excess memory usage. Requires pandas, a physical XML file on disk, and lxml or xml.etree.ElementTree.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_123\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_xml(\n    \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\",\n    iterparse = {\"page\": [\"title\", \"ns\", \"id\"]}\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Groupby with dropna True and False - pandas Python\nDESCRIPTION: Performs groupby with the dropna parameter both True and False to demonstrate the effect on NA groups. dropna=True (default) excludes NA in grouping, while dropna=False includes them. Shows resulting summaries for each mode. Inputs: DataFrame with NA values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"key\", dropna=True).sum()\n\ndf.groupby(\"key\", dropna=False).sum()\n```\n\n----------------------------------------\n\nTITLE: loc Query Edge Cases with Non-Monotonic Indices - pandas - Python\nDESCRIPTION: Illustrates the failure cases when using .loc for slice-based access on a non-monotonic index in pandas. Shows errors when index bounds are not present or not unique. Requires a pandas DataFrame 'df' with a non-monotonic index to exist. Use of the :okexcept: directive indicates that exceptions are expected and handled in the surrounding documentation context.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\n# 0 is not in the index\ndf.loc[0:4, :]\n\n# 3 is not a unique label\ndf.loc[2:3, :]\n```\n\n----------------------------------------\n\nTITLE: Combining Criteria with functools.reduce in Python\nDESCRIPTION: Uses functools.reduce to dynamically combine multiple boolean criteria. This method provides a more flexible approach for combining an arbitrary number of filtering conditions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport functools\n\nCritList = [Crit1, Crit2, Crit3]\nAllCrit = functools.reduce(lambda x, y: x & y, CritList)\n\ndf[AllCrit]\n```\n\n----------------------------------------\n\nTITLE: Timing DataFrame Arithmetic Using pandas.eval - Python\nDESCRIPTION: Measures execution time for optimally adding four large DataFrames using pandas.eval. This can be significantly faster with the numexpr engine for large data sizes. The command is run in an IPython environment. Inputs are the same large DataFrames; output is the timing for eval operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n%timeit pd.eval(\"df1 + df2 + df3 + df4\")\n```\n\n----------------------------------------\n\nTITLE: Creating Cross-tabulations in Python\nDESCRIPTION: Demonstrates how to use pandas.crosstab() to compute cross-tabulations of two or more factors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\na = np.array([\"foo\", \"foo\", \"bar\", \"bar\", \"foo\", \"foo\"], dtype=object)\nb = np.array([\"one\", \"one\", \"two\", \"one\", \"two\", \"one\"], dtype=object)\nc = np.array([\"dull\", \"dull\", \"shiny\", \"dull\", \"dull\", \"shiny\"], dtype=object)\npd.crosstab(a, [b, c], rownames=[\"a\"], colnames=[\"b\", \"c\"])\n\ndf = pd.DataFrame(\n    {\"A\": [1, 2, 2, 2, 2], \"B\": [3, 3, 4, 4, 4], \"C\": [1, 1, np.nan, 1, 1]}\n)\ndf\n\npd.crosstab(df[\"A\"], df[\"B\"])\n\nfoo = pd.Categorical([\"a\", \"b\"], categories=[\"a\", \"b\", \"c\"])\nbar = pd.Categorical([\"d\", \"e\"], categories=[\"d\", \"e\", \"f\"])\npd.crosstab(foo, bar)\n\npd.crosstab(foo, bar, dropna=False)\n\npd.crosstab(df[\"A\"], df[\"B\"], normalize=True)\n\npd.crosstab(df[\"A\"], df[\"B\"], normalize=\"columns\")\n\npd.crosstab(df[\"A\"], df[\"B\"], values=df[\"C\"], aggfunc=\"sum\")\n\npd.crosstab(\n    df[\"A\"], df[\"B\"], values=df[\"C\"], aggfunc=\"sum\", normalize=True, margins=True\n)\n```\n\n----------------------------------------\n\nTITLE: Joining on Index and MultiIndex - pandas - Python\nDESCRIPTION: This code joins a DataFrame with a simple Index to another DataFrame with a MultiIndex, aligning on a named index level. It constructs sample DataFrames with explicit names and demonstrates an inner join using the DataFrame.join method. The code requires pandas and clear index naming for proper key alignment. The output is a DataFrame where rows are matched via corresponding index levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame(\n    {\"A\": [\"A0\", \"A1\", \"A2\"], \"B\": [\"B0\", \"B1\", \"B2\"]},\n    index=pd.Index([\"K0\", \"K1\", \"K2\"], name=\"key\"),\n)\n\nindex = pd.MultiIndex.from_tuples(\n    [(\"K0\", \"Y0\"), (\"K1\", \"Y1\"), (\"K2\", \"Y2\"), (\"K2\", \"Y3\")],\n    names=[\"key\", \"Y\"],\n)\nright = pd.DataFrame(\n    {\"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"]},\n    index=index,\n)\n\nresult = left.join(right, how=\"inner\")\nresult\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table with Column Converters in Python\nDESCRIPTION: Demonstrates how to use converters to control the data type of columns when reading HTML tables with pandas.read_html(). This is useful for preserving leading zeros in numerical text data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_98\n\nLANGUAGE: python\nCODE:\n```\nurl_mcc = \"https://en.wikipedia.org/wiki/Mobile_country_code?oldid=899173761\"\ndfs = pd.read_html(\n    url_mcc,\n    match=\"Telekom Albania\",\n    header=0,\n    converters={\"MNC\": str},\n)\n```\n\n----------------------------------------\n\nTITLE: Reading XML String into DataFrame - pandas - Python\nDESCRIPTION: Demonstrates reading an XML string into a pandas DataFrame using 'read_xml' and a StringIO wrapper for in-memory string processing. The snippet provides a complete XML example with multiple book entries, shows how to pass the string to the reader, and outputs the resulting DataFrame. Requires pandas (>=1.3.0) and 'from io import StringIO'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_112\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\nxml = \"\"\"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\n<bookstore>\n  <book category=\\\"cooking\\\">\n    <title lang=\\\"en\\\">Everyday Italian</title>\n    <author>Giada De Laurentiis</author>\n    <year>2005</year>\n    <price>30.00</price>\n  </book>\n  <book category=\\\"children\\\">\n    <title lang=\\\"en\\\">Harry Potter</title>\n    <author>J K. Rowling</author>\n    <year>2005</year>\n    <price>29.99</price>\n  </book>\n  <book category=\\\"web\\\">\n    <title lang=\\\"en\\\">Learning XML</title>\n    <author>Erik T. Ray</author>\n    <year>2003</year>\n    <price>39.95</price>\n  </book>\n</bookstore>\"\"\"\n\ndf = pd.read_xml(StringIO(xml))\ndf\n```\n\n----------------------------------------\n\nTITLE: Accessing DataFrame Rows Using loc with Slices - pandas - Python\nDESCRIPTION: Demonstrates retrieving DataFrame rows using the .loc accessor with label-based slice notation in Python pandas. Highlights behavior when slice bounds are partially or wholly outside the DataFrame index range. Assumes that 'df' is a predefined pandas DataFrame. Returns all rows within the inclusive endpoint labels if present; otherwise, produces an empty DataFrame if no bounds match an index label.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\n# no rows 0 or 1, but still returns rows 2, 3 (both of them), and 4:\ndf.loc[0:4, :]\n\n# slice is are outside the index, so empty DataFrame is returned\ndf.loc[13:15, :]\n```\n\n----------------------------------------\n\nTITLE: Setting Values in a Series Using .iloc\nDESCRIPTION: Shows how to modify Series values using position-based indexing with .iloc by setting the first three elements to 0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ns1.iloc[:3] = 0\ns1\n```\n\n----------------------------------------\n\nTITLE: Statistical Operations (mean, sub) on DataFrames - Python\nDESCRIPTION: Summarizes DataFrame statistics by column and row using df.mean() and df.mean(axis=1). The snippet also demonstrates alignment-aware arithmetic: df.sub(s, axis=\"index\") subtracts a Series from the DataFrame based on index label matching, broadcasting across rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf.mean()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.mean(axis=1)\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)\\ns\\ndf.sub(s, axis=\"index\")\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Structured NumPy Array - pandas - Python\nDESCRIPTION: Initializes a structured NumPy array with named fields, fills it with tuples, then converts it to a DataFrame. Different constructors show specifying row or column labels. This approach is helpful when working with structured scientific or tabular data formats.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndata = np.zeros((2,), dtype=[(\"A\", \"i4\"), (\"B\", \"f4\"), (\"C\", \"S10\")])\ndata[:] = [(1, 2.0, \"Hello\"), (2, 3.0, \"World\")]\n\npd.DataFrame(data)\npd.DataFrame(data, index=[\"first\", \"second\"])\npd.DataFrame(data, columns=[\"C\", \"A\", \"B\"])\n```\n\n----------------------------------------\n\nTITLE: Index.astype() with copy Parameter - Pandas - Python\nDESCRIPTION: Mentions support for an optional 'copy' boolean parameter in Index.astype() to control whether a new object is returned only if the dtype is different. This feature is useful for optimizing memory usage when type conversion is not needed. Requires pandas; parameters: target dtype and copy boolean flag.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_14\n\n\n\n----------------------------------------\n\nTITLE: Creating and Manipulating Series of timedelta64[ns] via Date Ranges - Pandas - Python\nDESCRIPTION: Shows construction and manipulation of a Series of timedelta64[ns] by subtracting two date_range objects and assigning NaN or incrementing. Also demonstrates type casting and inplace operations. Prerequisites: pandas, numpy, and datetime modules. Inputs: date_range start dates; Outputs: timedelta Series with mixed values and NaN.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\\ntd = pd.Series(pd.date_range('20130101', periods=4)) - pd.Series(\\n    pd.date_range('20121201', periods=4))\\ntd[2] += np.timedelta64(datetime.timedelta(minutes=5, seconds=3))\\ntd[3] = np.nan\\ntd\n```\n\n----------------------------------------\n\nTITLE: Iterating Over HDF5 File Chunks via pd.read_hdf with Chunksize - Python\nDESCRIPTION: This snippet shows how to use pandas.read_hdf as an iterator to read an HDF5 file in chunked DataFrames, with automatic resource cleanup. It opens 'store.h5', reads the 'df' dataset in chunks of three rows, and prints each chunk. Dependencies: pandas, existing HDF5 file with group/key 'df'. Inputs: file path, key, chunksize. Outputs: batch DataFrames. Limitation: file must exist with the specified content; chunk shape may be unequal if the row count isn't divisible by the chunksize.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_189\n\nLANGUAGE: python\nCODE:\n```\nfor df in pd.read_hdf(\"store.h5\", \"df\", chunksize=3):\n    print(df)\n```\n\n----------------------------------------\n\nTITLE: Iterative Reading of CSV with Specified Iterator using Pandas (Python)\nDESCRIPTION: Shows how to open a CSV as an iterator by setting iterator=True in read_csv and retrieve a chunk of data using get_chunk. The file is managed by a context manager. Requires pandas and a valid 'tmp.csv'; chunk size is set to 5. Prints the chunk output as a DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nwith pd.read_csv(\"tmp.csv\", iterator=True) as reader:\n    print(reader.get_chunk(5))\n```\n\n----------------------------------------\n\nTITLE: Checking for Constant Values in Series - Treating NA as Distinct\nDESCRIPTION: Approach to check for constant values while considering missing values as distinct. This method handles edge cases where a Series might contain only NA values or mixed values with NA.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\nv = s.to_numpy()\nis_constant = v.shape[0] == 0 or (s[0] == s).all() or not pd.notna(v).any()\n```\n\n----------------------------------------\n\nTITLE: Reading Excel with Calamine Engine - pandas Python\nDESCRIPTION: This snippet demonstrates how to read an Excel (including .xlsb, .xls, .xlsx, .ods) file into a pandas DataFrame using the new 'calamine' engine. It requires the python-calamine package and supports multiple spreadsheet formats, offering performance benefits for many files. The input is a file path, and the output is a DataFrame containing the spreadsheet data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npd.read_excel(\"path_to_file.xlsb\", engine=\"calamine\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Processing Query Coordinate Chunks for Batched HDFStore Reads - pandas HDFStore - Python\nDESCRIPTION: This snippet constructs a DataFrame 'dfeq', appends it to an HDFStore with a data column, builds a helper function to split query coordinate arrays into equal-sized chunks, retrieves rows matching a query using select_as_coordinates, and processes these in batches. Dependencies: pandas, numpy, open HDFStore. Inputs: DataFrame, set of target values, chunk size. Outputs: Batched retrievals of filtered DataFrames. The approach is useful for balancing load and memory usage for large query results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_190\n\nLANGUAGE: python\nCODE:\n```\ndfeq = pd.DataFrame({\"number\": np.arange(1, 11)})\ndfeq\n\nstore.append(\"dfeq\", dfeq, data_columns=[\"number\"])\n\ndef chunks(l, n):\n    return [l[i: i + n] for i in range(0, len(l), n)]\n\nevens = [2, 4, 6, 8, 10]\ncoordinates = store.select_as_coordinates(\"dfeq\", \"number=evens\")\nfor c in chunks(coordinates, 2):\n    print(store.select(\"dfeq\", where=c))\n```\n\n----------------------------------------\n\nTITLE: Creating Parallel Coordinates Plot for Multivariate Data\nDESCRIPTION: Generates a parallel coordinates plot for the Iris dataset, using the 'Name' column for coloring. This visualization technique represents multivariate data as connected line segments across parallel axes, helping identify clusters and patterns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.plotting import parallel_coordinates\n\ndata = pd.read_csv(\"data/iris.data\")\n\nplt.figure();\n\nparallel_coordinates(data, \"Name\");\n```\n\n----------------------------------------\n\nTITLE: Basic GroupBy UDF Aggregation in Python\nDESCRIPTION: Example showing how to use a lambda function as a UDF to perform custom aggregation on grouped data, returning a set of values for each group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nanimals.groupby(\"kind\")[[\"height\"]].agg(lambda x: set(x))\n```\n\n----------------------------------------\n\nTITLE: Sorting pandas Series with NA Values (Python)\nDESCRIPTION: Highlights sorting Series with possibly NA values, controlling NA placement using the na_position parameter. Assumes s is a pandas Series with at least three elements. Outputs: Series with NA placed either last (default) or first. Requires numpy for np.nan.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ns[2] = np.nan\ns.sort_values()\ns.sort_values(na_position=\"first\")\n```\n\n----------------------------------------\n\nTITLE: Outputting Parsed HTML Table Result - pandas (Python)\nDESCRIPTION: Shows accessing the parsed HTML table as a DataFrame after using read_html. Requires result from a prior read_html call. Output demonstrates improved handling of colspan/rowspan in parsed data, presenting merged cell results accurately. Useful for data acquisition workflows ingesting complex HTML tables.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nresult\n```\n\n----------------------------------------\n\nTITLE: Handling Malformed CSV Lines (Too Many Fields) - pandas - Python\nDESCRIPTION: Reads CSV string data with some lines having more fields than the header. By default, this will raise an error. The :okexcept: indicates the code is expected to show an error without interrupting the documentation build. Requires pandas and StringIO.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,c\\n1,2,3\\n4,5,6,7\\n8,9,10\"\npd.read_csv(StringIO(data))\n```\n\n----------------------------------------\n\nTITLE: Creating Pivot Tables in Pandas\nDESCRIPTION: Demonstrates how to create a pivot table in pandas using the pivot_table function, which is equivalent to Excel's PivotTable feature. This example calculates average tip by party size and server gender.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npd.pivot_table(\n    tips, values=\"tip\", index=[\"size\"], columns=[\"sex\"], aggfunc=np.average\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing DataFrames with Aggregation for Pivot Tables - pandas - Python\nDESCRIPTION: This snippet creates a pandas DataFrame 'df' with categorical and datetime columns (A, B, C, D, E, F), generating random numeric data for D and E and datetime values for F, using numpy and Python's datetime module. The structure is set for demonstrating pivot table aggregations. Input requires numpy, pandas, and datetime libraries.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport datetime\n\ndf = pd.DataFrame(\n    {\n        \"A\": [\"one\", \"one\", \"two\", \"three\"] * 6,\n        \"B\": [\"A\", \"B\", \"C\"] * 8,\n        \"C\": [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"] * 4,\n        \"D\": np.random.randn(24),\n        \"E\": np.random.randn(24),\n        \"F\": [datetime.datetime(2013, i, 1) for i in range(1, 13)]\n        + [datetime.datetime(2013, i, 15) for i in range(1, 13)],\n    }\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Indexing pandas Series with Nullable Boolean Mask (Python)\nDESCRIPTION: This example shows how to use a pandas Series and apply a nullable BooleanArray mask containing True, False, and NA values. NA values are treated as False when used for indexing. Requires pandas 'boolean' dtype support. Input is a Series and a Boolean mask; output is the filtered Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/boolean.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3])\nmask = pd.array([True, False, pd.NA], dtype=\"boolean\")\ns[mask]\n```\n\n----------------------------------------\n\nTITLE: Displaying Large DataFrames as Truncated or Info - Python\nDESCRIPTION: Shows how changing 'large_repr' affects display formatting for DataFrames that exceed max_rows or max_columns. Uses values 'truncate' and 'info' to control verbosity. Involves adjusting 'display.max_rows' and 'large_repr' options, displaying a large DataFrame each time. Important for summarizing or expanding outputs in large data scenarios.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 10))\npd.set_option(\"display.max_rows\", 5)\npd.set_option(\"large_repr\", \"truncate\")\ndf\npd.set_option(\"large_repr\", \"info\")\ndf\npd.reset_option(\"large_repr\")\npd.reset_option(\"display.max_rows\")\n```\n\n----------------------------------------\n\nTITLE: Reduction Operations (mean, median, quantile, sum) on Timedelta Series - pandas - Python\nDESCRIPTION: Uses pandas mean, median, quantile, and sum aggregation methods on a Series of Timedeltas (with possible NaT values). Returns computed Timedelta results. NaT values are skipped by default.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ny2 = pd.Series(\n    pd.to_timedelta([\"-1 days +00:00:05\", \"nat\", \"-1 days +00:00:05\", \"1 days\"])\n)\ny2\ny2.mean()\ny2.median()\ny2.quantile(0.1)\ny2.sum()\n```\n\n----------------------------------------\n\nTITLE: Renaming Categories in Pandas Series\nDESCRIPTION: Shows how to rename categories in a categorical series using both list comprehension and dictionary mapping.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"], dtype=\"category\")\ns\nnew_categories = [\"Group %s\" % g for g in s.cat.categories]\ns = s.cat.rename_categories(new_categories)\ns\n# You can also pass a dict-like object to map the renaming\ns = s.cat.rename_categories({1: \"x\", 2: \"y\", 3: \"z\"})\ns\n```\n\n----------------------------------------\n\nTITLE: Initializing a DataFrame with a Dictionary in Pandas - Python\nDESCRIPTION: This snippet shows how to create a pandas DataFrame by passing a Python dictionary, where each key is mapped to a column name and its corresponding value is a list of data for that column. Requires the pandas library (imported as pd). The function pd.DataFrame() takes the dictionary as input and produces a DataFrame object; displaying the object outputs the tabular data. Input columns and row values must be of equal length; suitable for small, structured datasets.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/construct_dataframe.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"x\": [1, 3, 5], \"y\": [2, 4, 6]})\ndf\n```\n\n----------------------------------------\n\nTITLE: Adding Timedeltas to Dates in DataFrame\nDESCRIPTION: Example of creating a DataFrame with datetime and timedelta columns, then adding them to create a new column of dates. Shows how to perform date arithmetic across DataFrame columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": s, \"B\": deltas})\ndf\n\ndf[\"New Dates\"] = df[\"A\"] + df[\"B\"]\n\ndf[\"Delta\"] = df[\"A\"] - df[\"New Dates\"]\ndf\n\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Exporting DataFrame to HTML with Rendered Links - pandas - Python\nDESCRIPTION: Illustrates how to use the 'render_links' argument to automatically turn URLs in DataFrame cells into clickable hyperlinks in the HTML output. The snippet first creates a DataFrame with 'name' and 'url' columns and then exports it to an HTML string with links rendered, finally displaying the output with an HTML renderer. Requires pandas and assumes an environment where 'display' and 'HTML' are available, such as Jupyter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_108\n\nLANGUAGE: python\nCODE:\n```\nurl_df = pd.DataFrame(\n    {\n        \"name\": [\"Python\", \"pandas\"],\n        \"url\": [\"https://www.python.org/\", \"https://pandas.pydata.org\"],\n    }\n)\nhtml = url_df.to_html(render_links=True)\nprint(html)\ndisplay(HTML(html))\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Function with Rolling Apply - pandas - Python\nDESCRIPTION: This code defines a custom Mean Absolute Deviation (MAD) function and applies it to a rolling window using Rolling.apply with raw=True. It demonstrates functional aggregation over rolling windows for a Series. Required: pandas, numpy. Inputs: Series, rolling window specification, function (mad); Output: Series with rolling MAD. Note: raw=True passes ndarray windows, speeding up computation for numeric data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef mad(x):\n    return np.fabs(x - x.mean()).mean()\n\ns = pd.Series(range(10))\ns.rolling(window=4).apply(mad, raw=True)\n```\n\n----------------------------------------\n\nTITLE: Resolving Index/Column Name Ambiguity in pandas.DataFrame.query - Python\nDESCRIPTION: Illustrates how DataFrame.query gives precedence to column names over index names, and how to use the special keyword 'index' for referring to the index, or 'ilevel_0' when a column named 'index' exists. Requires pandas and numpy. Shows queries using 'a', 'index', and 'ilevel_0' for disambiguation. Inputs: DataFrame with potential for name overlap; Output: Filtered DataFrame as per disambiguation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': np.random.randint(5, size=5)})\ndf.index.name = 'a'\ndf.query('a > 2')  # uses the column 'a', not the index\ndf.query('index > 2')\n```\n\n----------------------------------------\n\nTITLE: List-based Selection with MultiIndex in Python\nDESCRIPTION: Shows how to select data by providing a list of labels or tuples to the loc accessor, which works similar to reindexing with specific keys in the hierarchical structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[[(\"bar\", \"two\"), (\"qux\", \"one\")]]\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to Markdown - pandas - Python\nDESCRIPTION: This snippet demonstrates how to create a simple pandas DataFrame with columns A and B, indexed by two 'a' and one 'b' labels, and print its markdown-formatted output using the .to_markdown() method. Requires pandas version 1.0.0 or later and the optional tabulate library for markdown output. It illustrates how the enhanced DataFrame IO features work, and outputs the data in markdown (text) format for use in documentation or markdown files.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [1, 2, 3]}, index=['a', 'a', 'b'])\\nprint(df.to_markdown())\n```\n\n----------------------------------------\n\nTITLE: Cross-section Selection with xs and Slicers - pandas - Python\nDESCRIPTION: Explores access of data at a specific level using DataFrame.xs, specifying the level (and optionally axis), in both row and column orientation, and compares this to .loc slicers. Supports both single and multi-key selection and retaining levels with drop_level=False. Requires appropriate DataFrame structures.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf\ndf.xs(\"one\", level=\"second\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# using the slicers\ndf.loc[(slice(None), \"one\"), :]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = df.T\ndf.xs(\"one\", level=\"second\", axis=1)\n```\n\nLANGUAGE: python\nCODE:\n```\n# using the slicers\ndf.loc[:, (slice(None), \"one\")]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.xs((\"one\", \"bar\"), level=(\"second\", \"first\"), axis=1)\n```\n\nLANGUAGE: python\nCODE:\n```\n# using the slicers\ndf.loc[:, (\"bar\", \"one\")]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.xs(\"one\", level=\"second\", axis=1, drop_level=False)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.xs(\"one\", level=\"second\", axis=1, drop_level=True)\n```\n\n----------------------------------------\n\nTITLE: Setting Custom HTTP Headers for Remote CSV Files in Python\nDESCRIPTION: Demonstrates how to set custom HTTP headers when reading CSV files from remote URLs using pandas. This allows controlling the User-Agent or sending other custom headers within the storage_options parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nheaders = {\"User-Agent\": \"pandas\"}\ndf = pd.read_csv(\n    \"https://download.bls.gov/pub/time.series/cu/cu.item\",\n    sep=\"\\t\",\n    storage_options=headers\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas and Loading Data\nDESCRIPTION: Basic setup importing pandas and loading sample datasets (Titanic and air quality data)\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/07_reshape_table_layout.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ntitanic = pd.read_csv(\"data/titanic.csv\")\nair_quality = pd.read_csv(\"data/air_quality_long.csv\", index_col=\"date.utc\", parse_dates=True)\n```\n\n----------------------------------------\n\nTITLE: Timedelta Conversion\nDESCRIPTION: Shows pd.to_timedelta() usage for converting objects to timedelta types with error handling examples.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_88\n\nLANGUAGE: python\nCODE:\n```\nm = [\"5us\", pd.Timedelta(\"1day\")]\npd.to_timedelta(m)\n\nm = [\"apple\", pd.Timedelta(\"1day\")]\npd.to_timedelta(m, errors=\"coerce\")\n```\n\n----------------------------------------\n\nTITLE: Reversed Order Arithmetic with Timedelta Series - pandas - Python\nDESCRIPTION: Shows that arithmetic operations with Series and scalars (datetime or timedelta) can appear with reversed operand order. Useful for broad operations in analysis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ns.max() - s\ndatetime.datetime(2011, 1, 1, 3, 5) - s\ndatetime.timedelta(minutes=5) + s\n```\n\n----------------------------------------\n\nTITLE: Demonstration: Setting and Resetting Displayed Rows in DataFrame Output - Python\nDESCRIPTION: Shows how adjusting the 'display.max_rows' setting influences how many rows are shown when pretty-printing a DataFrame in pandas. Requires pandas, numpy, and a DataFrame 'df' with sample data. Modifies the global row display threshold, displaying a DataFrame before and after each setting, and resets the option at the end.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(7, 2))\npd.set_option(\"display.max_rows\", 7)\ndf\npd.set_option(\"display.max_rows\", 5)\ndf\npd.reset_option(\"display.max_rows\")\n```\n\n----------------------------------------\n\nTITLE: Querying with Boolean Expressions on DataFrames in HDF5 - pandas - Python\nDESCRIPTION: Uses pandas HDFStore's select to retrieve rows from a DataFrame based on boolean expressions on the index and columns, including inline function evaluation. Demonstrates advanced query capabilities using string expressions. Requires pandas; input is a DataFrame in HDFStore; output is a filtered DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_179\n\nLANGUAGE: python\nCODE:\n```\nstore.select(\"dfq\", \"index>pd.Timestamp('20130104') & columns=['A', 'B']\")\n```\n\nLANGUAGE: python\nCODE:\n```\nstore.select(\"dfq\", where=\"A>0 or C>0\")\n```\n\n----------------------------------------\n\nTITLE: Using Current Timestamp and Date Offsets with Pandas - Python\nDESCRIPTION: Illustrates how to obtain the current timestamp and add a date offset (here, a year) using pandas.tseries.offsets. Requires pandas and import of pandas.tseries.offsets as offsets. Input is no argument for now(), output is current datetime, whereas adding DateOffset increases the year by 1. Useful for time arithmetic without parsing from strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport pandas.tseries.offsets as offsets\n\npd.Timestamp.now()\npd.Timestamp.now() + offsets.DateOffset(years=1)\n```\n\n----------------------------------------\n\nTITLE: Managing SQLAlchemy connection with context manager\nDESCRIPTION: Shows how to manage a SQLAlchemy database connection using a Python context manager. This ensures the connection is properly closed after operations are completed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_223\n\nLANGUAGE: python\nCODE:\n```\nwith engine.connect() as conn, conn.begin():\n    data = pd.read_sql_table(\"data\", conn)\n```\n\n----------------------------------------\n\nTITLE: Extracting Links from HTML Tables in Python\nDESCRIPTION: Demonstrates how to extract links along with text from HTML tables using the extract_links parameter in pandas.read_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_102\n\nLANGUAGE: python\nCODE:\n```\nhtml_table = \"\"\"\n<table>\n  <tr>\n    <th>GitHub</th>\n  </tr>\n  <tr>\n    <td><a href=\"https://github.com/pandas-dev/pandas\">pandas</a></td>\n  </tr>\n</table>\n\"\"\"\n\ndf = pd.read_html(\n    StringIO(html_table),\n    extract_links=\"all\"\n)[0]\ndf\ndf[(\"GitHub\", None)]\ndf[(\"GitHub\", None)].str[1]\n```\n\n----------------------------------------\n\nTITLE: Multiple Column Histogram Plot with Custom Parameters (Python)\nDESCRIPTION: Visualizes the histograms of the differences for all columns in df in a single plot using DataFrame.diff().hist(color=\"k\", alpha=0.5, bins=50). All columns are drawn in black with some transparency. Dependencies: pandas and matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\\n\\n@savefig frame_hist_ex.png\\ndf.diff().hist(color=\"k\", alpha=0.5, bins=50);\n```\n\n----------------------------------------\n\nTITLE: Advanced get_dummies Usage in Python\nDESCRIPTION: Shows advanced features of pandas.get_dummies(), including drop_first and handling single-level columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(list(\"abcaa\"))\n\npd.get_dummies(s)\n\npd.get_dummies(s, drop_first=True)\n\ndf = pd.DataFrame({\"A\": list(\"aaaaa\"), \"B\": list(\"ababc\")})\n\npd.get_dummies(df)\n\npd.get_dummies(df, drop_first=True)\n\ndf = pd.DataFrame({\"A\": list(\"abc\"), \"B\": [1.1, 2.2, 3.3]})\n\npd.get_dummies(df, dtype=np.float32).dtypes\n```\n\n----------------------------------------\n\nTITLE: Summing a DataFrame Column with Missing Values in pandas - Python\nDESCRIPTION: This snippet computes the sum of the value_x column of the outer_join DataFrame, ignoring any NaN entries by default. It demonstrates that pandas aggregation functions treat missing data as non-contributing, returning the sum of available (non-missing) values. This snippet requires pandas and assumes that value_x is a numeric column containing possible NaN values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/missing_intro.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nouter_join[\"value_x\"].sum()\n```\n\n----------------------------------------\n\nTITLE: Setting PyArrow as Default String Storage with Context Manager\nDESCRIPTION: Demonstrates how to use pandas option_context to set PyArrow as the default storage backend for string data types within a specific context.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nwith pd.option_context(\"string_storage\", \"pyarrow\"):\n    s = pd.Series(['abc', None, 'def'], dtype=\"string\")\ns\n```\n\n----------------------------------------\n\nTITLE: Accessing the Underlying Array of a Series - pandas - Python\nDESCRIPTION: Accesses the backing array of a Series using the '.array' attribute, which returns a pandas ExtensionArray representing the data without the index. This is helpful for lower-level array operations, bypassing the automatic alignment and index handling steps. Requires pandas Series 's'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ns.array\n```\n\n----------------------------------------\n\nTITLE: Creating Sparse Series with pandas - Python\nDESCRIPTION: Demonstrates how to create a pandas Series from a NumPy array, converting it to a SparseArray to save memory where fill values (e.g., NaN) are prevalent. NumPy is used for random data generation and pandas handles the conversion. The output Series contains only non-NaN values in its compressed storage, resulting in a sparse dtype (e.g., Sparse[float64, nan]). Required dependencies: numpy, pandas. The Series must be constructed from an array, and missing data is omitted from the storage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\narr = np.random.randn(10)\narr[2:-2] = np.nan\nts = pd.Series(pd.arrays.SparseArray(arr))\nts\n```\n\n----------------------------------------\n\nTITLE: Resampling Data using GroupBy in Pandas\nDESCRIPTION: Demonstrates how to use GroupBy to resample data by aggregating samples into bins. This example downsamples by consolidating samples and calculating their standard deviation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 2))\ndf\ndf.index // 5\ndf.groupby(df.index // 5).std()\n```\n\n----------------------------------------\n\nTITLE: Reindexing a DataFrame and Setting Missing Values - Python\nDESCRIPTION: The snippet creates a new DataFrame 'df1' by reindexing 'df' with a subset of rows and columns, adding new columns as well. It then sets some values in the new column 'E' and displays the resulting DataFrame. Useful for reshaping or partially filling data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [\"E\"])\\ndf1.loc[dates[0] : dates[1], \"E\"] = 1\\ndf1\n```\n\n----------------------------------------\n\nTITLE: String Interpolation and Safe Query Construction for HDFStore - pandas - Python\nDESCRIPTION: Explains pitfalls of interpolating user-supplied strings into HDFStore queries and provides the recommended pattern: assign the string first, then use the variable in the query. Also demonstrates the correct and incorrect ways to interpolate using Python format specifiers. Emphasizes that passing a string with quotes improperly causes SyntaxError; the '%r' specifier ensures the string is quoted. No output produced except from select().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_177\n\nLANGUAGE: python\nCODE:\n```\nstring = \"HolyMoly'\"\nstore.select(\"df\", \"index == string\")\n```\n\nLANGUAGE: python\nCODE:\n```\nstring = \"HolyMoly'\"\nstore.select('df', f'index == {string}')\n```\n\nLANGUAGE: python\nCODE:\n```\nstore.select(\"df\", \"index == %r\" % string)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment for Pandas on Unix/macOS\nDESCRIPTION: Steps to create and activate a Python virtual environment for Pandas development on Unix/macOS systems, and install required dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Create a virtual environment\n# Use an ENV_DIR of your choice. We'll use ~/virtualenvs/pandas-dev\n# Any parent directories should already exist\npython3 -m venv ~/virtualenvs/pandas-dev\n\n# Activate the virtualenv\n. ~/virtualenvs/pandas-dev/bin/activate\n\n# Install the build dependencies\npython -m pip install -r requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: Applying Rolling and Expanding Window Operations Per Group with pandas in Python\nDESCRIPTION: This set of snippets demonstrates the application of rolling and expanding window operations within groups using pandas groupby. It shows calculation of rolling means, expanding sums, and resampling with ffill per group. Dependencies include pandas and numpy, and the examples assume dataframes as shown in the snippet.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndf_re = pd.DataFrame({\"A\": [1] * 10 + [5] * 10, \"B\": np.arange(20)})\ndf_re\n\ndf_re.groupby(\"A\").rolling(4).B.mean()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf_re.groupby(\"A\").expanding().sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf_re = pd.DataFrame(\n    {\n        \"date\": pd.date_range(start=\"2016-01-01\", periods=4, freq=\"W\"),\n        \"group\": [1, 1, 2, 2],\n        \"val\": [5, 6, 7, 8],\n    }\n).set_index(\"date\")\ndf_re\n\ndf_re.groupby(\"group\").resample(\"1D\").ffill()\n```\n\n----------------------------------------\n\nTITLE: DataFrame Alignment and Arithmetic Operations - Pandas - Python\nDESCRIPTION: Demonstrates automatic alignment of DataFrames and Series along indices and columns when performing arithmetic. Also shows broadcasting and scalar operations. Requires pandas as pd, numpy as np, and properly shaped DataFrame(s).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 4), columns=[\"A\", \"B\", \"C\", \"D\"])\ndf2 = pd.DataFrame(np.random.randn(7, 3), columns=[\"A\", \"B\", \"C\"])\ndf + df2\n```\n\nLANGUAGE: python\nCODE:\n```\ndf - df.iloc[0]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf * 5 + 2\n1 / df\ndf ** 4\n```\n\n----------------------------------------\n\nTITLE: Reading, Deduplicating, and Enforcing Unique Labels on DataFrames - pandas - Python\nDESCRIPTION: Represents a common ETL workflow: ingest data with possibly duplicated labels, deduplicate via groupby and first(), and ensure future operations disallow duplicates by setting allows_duplicate_labels to False. This code is intended for cleanup and validation in data pipelines. Requires pandas; deduplicated is a DataFrame with unique index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> raw = pd.read_csv(\"...\")\n>>> deduplicated = raw.groupby(level=0).first()  # remove duplicates\n>>> deduplicated.flags.allows_duplicate_labels = False  # disallow going forward\n```\n\n----------------------------------------\n\nTITLE: Type Inference with DataFrame Transposition\nDESCRIPTION: Demonstrates how DataFrame.infer_objects() can correct data types after transposition when columns are initially stored as objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_85\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\ndf = pd.DataFrame(\n    [\n        [1, 2],\n        [\"a\", \"b\"],\n        [datetime.datetime(2016, 3, 2), datetime.datetime(2016, 3, 2)],\n    ]\n)\ndf = df.T\ndf\ndf.dtypes\n\ndf.infer_objects().dtypes\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame for Groupby Consistency Demonstrations - Python\nDESCRIPTION: Creates a test DataFrame with grouping and value columns to illustrate changes in behavior for groupby operations. The DataFrame is the basis for showing how aggregation functions and result structures have changed in recent pandas versions. No dependencies apart from pandas required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [1, 1, 2, 3]})\\ndf\n```\n\n----------------------------------------\n\nTITLE: Computing Rolling Correlation Between DataFrame and Series - pandas - Python\nDESCRIPTION: This code calculates the rolling correlation between each column of a DataFrame and a selected Series using Rolling.corr. It first cumsums a random DataFrame, selects the first four rows, and computes rolling correlation of each column with column 'B' over a window of 2. Dependencies: pandas, numpy. Inputs: DataFrame, Series for correlation; Output: DataFrame of rolling correlations. Supports missing values and flexible window sizes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    np.random.randn(10, 4),\n    index=pd.date_range(\"2020-01-01\", periods=10),\n    columns=[\"A\", \"B\", \"C\", \"D\"],\n)\ndf = df.cumsum()\n\ndf2 = df[:4]\ndf2.rolling(window=2).corr(df2[\"B\"])\n```\n\n----------------------------------------\n\nTITLE: Writing a pandas DataFrame to an Excel File in Python\nDESCRIPTION: Exports the 'titanic' DataFrame to an Excel file named 'titanic.xlsx', placing the data in a worksheet named 'passengers' and omitting the row index ('index=False'). Requires pandas, the 'openpyxl' or compatible Excel writer library, and a writable filesystem. Key parameters: file path, sheet name, and index exclusion. Outputs an Excel spreadsheet suitable for sharing or further processing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntitanic.to_excel(\"titanic.xlsx\", sheet_name=\"passengers\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Handling NA-like Values in Nullable Integer Arrays (pandas, Python)\nDESCRIPTION: Illustrates construction of a nullable integer array from a list containing various NA-like values (np.nan, None, and pandas.NA), resulting in all missing entries being standardized as pandas.NA. Requires pandas (pd), NumPy (np), and handles multiple NA types seamlessly by converting them to pandas.NA.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npd.array([1, 2, np.nan, None, pd.NA], dtype=\"Int64\")\n```\n\n----------------------------------------\n\nTITLE: Converting Data Types to Nullable Types with convert_dtypes in pandas (Python)\nDESCRIPTION: These snippets illustrate using pd.read_csv to read a CSV file and then using convert_dtypes to convert all columns to nullable types supporting pd.NA. The code requires pandas (v1.0+) and io for StringIO. Parameters include CSV data, DataFrame, and columns. Output is a DataFrame with dtypes changed to nullable equivalents. This is useful after data import for robust missing value handling.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport io\ndata = io.StringIO(\"a,b\\n,True\\n2,\")\ndf = pd.read_csv(data)\ndf.dtypes\ndf_conv = df.convert_dtypes()\ndf_conv\ndf_conv.dtypes\n```\n\n----------------------------------------\n\nTITLE: MultiIndex Slicing with Lists and Slices - pandas - Python\nDESCRIPTION: Performs advanced slicing on MultiIndex DataFrame using a combination of slice and explicit label lists for index levels, enabling precise subsetting of data. Depends on pandas and the prior creation of a MultiIndex DataFrame. Input expects level names or positions and label lists; output is a filtered DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndfmi.loc[(slice(\"A1\", \"A3\"), slice(None), [\"C1\", \"C3\"]), :]\n```\n\n----------------------------------------\n\nTITLE: Applying Vectorized String Lowercase Method to pandas Series (Python)\nDESCRIPTION: Demonstrates usage of the .str.lower() vectorized string method on a pandas Series of string values with automatic NA-handling. The input Series s contains string and possibly nan values. Dependency: pandas and optionally numpy for np.nan. Output: Series of lowercase strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(\n    [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\"\n)\ns.str.lower()\n```\n\n----------------------------------------\n\nTITLE: Creating Stacked Area Plot in Pandas\nDESCRIPTION: This example shows how to create a stacked area plot using DataFrame.plot.area(). By default, the areas are stacked on top of each other with random data from multiple columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(10, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\ndf.plot.area();\n```\n\n----------------------------------------\n\nTITLE: Applying VariableOffsetWindowIndexer for Rolling Sums over BusinessDay Offsets - pandas - Python\nDESCRIPTION: This snippet demonstrates the use of VariableOffsetWindowIndexer to define rolling windows based on business day offsets. It constructs a DataFrame indexed by dates, creates a VariableOffsetWindowIndexer with a 1-business-day offset, and computes the rolling sum. Dependencies: pandas and pandas.offsets. Inputs: time-indexed DataFrame, offset specifier; Output: DataFrame with business-day rolling sum. Enables flexible non-fixed date-based rolling windows, suitable for time series aligned on business calendars.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.indexers import VariableOffsetWindowIndexer\n\ndf = pd.DataFrame(range(10), index=pd.date_range(\"2020\", periods=10))\noffset = pd.offsets.BDay(1)\nindexer = VariableOffsetWindowIndexer(index=df.index, offset=offset)\ndf\n\ndf.rolling(indexer).sum()\n```\n\n----------------------------------------\n\nTITLE: Using Keys in DataFrame Concatenation to Create MultiIndex in Python\nDESCRIPTION: Demonstrates using the keys argument to add another level to the resulting index, creating a MultiIndex that associates specific keys with each original DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.concat(frames, keys=[\"x\", \"y\", \"z\"])\nresult\nresult.loc[\"y\"]\n```\n\n----------------------------------------\n\nTITLE: Creating Categorical Series from pandas.Categorical in Python\nDESCRIPTION: Demonstrates how to create a categorical Series by passing a pandas.Categorical object. This method allows for explicit specification of categories and order.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nraw_cat = pd.Categorical(\n    [\"a\", \"b\", \"c\", \"a\"], categories=[\"b\", \"c\", \"d\"], ordered=False\n)\ns = pd.Series(raw_cat)\ns\ndf = pd.DataFrame({\"A\": [\"a\", \"b\", \"c\", \"a\"]})\ndf[\"B\"] = raw_cat\ndf\n```\n\n----------------------------------------\n\nTITLE: Comparing Memory Usage of Dense and Sparse DataFrames - Python\nDESCRIPTION: Calculates and compares the memory usage (in kilobytes) of a standard dense pandas DataFrame versus its sparse representation. Demonstrates the significant space savings achievable via sparsity for datasets with many fill values. Requires pandas and numpy. The output is formatted strings indicating memory consumption for both cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n'dense : {:0.2f} bytes'.format(df.memory_usage().sum() / 1e3)\n'sparse: {:0.2f} bytes'.format(sdf.memory_usage().sum() / 1e3)\n```\n\n----------------------------------------\n\nTITLE: Constructing Timedelta Ranges with pandas (Python)\nDESCRIPTION: Demonstrates how to construct TimedeltaIndex ranges using pd.timedelta_range with various combinations of 'start', 'end', 'periods', and 'freq' parameters. Requires pandas to be installed. 'start' and 'end' define the endpoints, 'periods' sets the number of periods, and 'freq' specifies the frequency of generated timedeltas. The resulting TimedeltaIndex contains evenly spaced timedeltas in the specified configuration. Inputs must be parseable as timedelta strings, and frequency can be a string like '30min' or '2D5h'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\npd.timedelta_range(start=\"1 days\", end=\"5 days\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.timedelta_range(end=\"10 days\", periods=4)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.timedelta_range(start=\"1 days\", end=\"2 days\", freq=\"30min\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.timedelta_range(start=\"1 days\", periods=5, freq=\"2D5h\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.timedelta_range(\"0 days\", \"4 days\", periods=5)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.timedelta_range(\"0 days\", \"4 days\", periods=10)\n```\n\n----------------------------------------\n\nTITLE: Using Nullable Integer Types in DataFrame - pandas (Python)\nDESCRIPTION: Illustrates assigning a Series with nullable integer (Int64 dtype) as a DataFrame column alongside other columns, and viewing data types. Requires pandas >=0.24 and NumPy. Input is a DataFrame constructed with mixed standard and extension dtypes. Output shows the DataFrame and its column dtypes. This supports downstream operations and ensures the DataFrame recognizes the extension type. Some pandas methods may not yet fully support all extension dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': s, 'B': [1, 1, 3], 'C': list('aab')})\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Determining the Shape of a Filtered DataFrame in pandas (Python)\nDESCRIPTION: This snippet retrieves the shape (number of rows and columns) for the DataFrame above_35 after filtering. It helps to see how many rows satisfy the filtering criteria 'Age > 35'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nabove_35.shape\n```\n\n----------------------------------------\n\nTITLE: Returning Named Series for Reshaping in Pandas GroupBy\nDESCRIPTION: Demonstrates how to group DataFrame columns, compute metrics, and return a named Series. This is useful for reshaping operations like stacking, where the column index name is used as the name of the inserted column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"a\": [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],\n        \"b\": [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1],\n        \"c\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n        \"d\": [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],\n    }\n)\n\ndef compute_metrics(x):\n    result = {\"b_sum\": x[\"b\"].sum(), \"c_mean\": x[\"c\"].mean()}\n    return pd.Series(result, name=\"metrics\")\n\nresult = df.groupby(\"a\").apply(compute_metrics)\n\nresult\n\nresult.stack()\n```\n\n----------------------------------------\n\nTITLE: Slicing and Accessing Data with Categoricals using .iloc and .loc - Pandas Python\nDESCRIPTION: Shows advanced slicing, subsetting, and category type preservation in pandas DataFrames/Series indexed by categorical data. Requires pandas. Demonstrates that slicing preserves the category dtype when possible and contrasts with single-row extraction where type is coerced to object. Includes selection, filtering, dtype introspection, and comparison with R.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([\"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\"])\ncats = pd.Series([\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\"], dtype=\"category\", index=idx)\nvalues = [1, 2, 2, 2, 3, 4, 5]\ndf = pd.DataFrame({\"cats\": cats, \"values\": values}, index=idx)\ndf.iloc[2:4, :]\ndf.iloc[2:4, :].dtypes\ndf.loc[\"h\":\"j\", \"cats\"]\ndf[df[\"cats\"] == \"b\"]\n```\n\n----------------------------------------\n\nTITLE: Using pandas.plotting.table to Add Data Tables to Matplotlib Axes in Python\nDESCRIPTION: This snippet shows how to use the pandas.plotting.table helper with Matplotlib axes to display summary statistics (via describe) as a table next to a plot. Dependencies: pandas, numpy, matplotlib. Key parameters include passing summarized data to the table helper and specifying location/column width. Inputs: a DataFrame; outputs: a plot and a table added to the axes. Limitations: further customization is available via the returned table instance and matplotlib's table API.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.plotting import table\n\nfig, ax = plt.subplots(1, 1)\n\ntable(ax, np.round(df.describe(), 2), loc=\"upper right\", colWidths=[0.2, 0.2, 0.2]);\ndf.plot(ax=ax, ylim=(0, 2), legend=None);\n```\n\n----------------------------------------\n\nTITLE: Converting Series to NumPy ndarray - pandas - Python\nDESCRIPTION: Converts a Series object to a NumPy ndarray using the 'to_numpy()' method. This returns just the raw data values, excluding all metadata such as index or name. This is useful when working with libraries or APIs that require ndarrays. Requires an existing pandas Series 's'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ns.to_numpy()\n```\n\n----------------------------------------\n\nTITLE: Selecting Subsets of Groups with nth in pandas GroupBy Filtration (Python)\nDESCRIPTION: This snippet demonstrates basic filtration by selecting the nth row of each group using pandas groupby 'nth' method. It is applicable to DataFrames with a 'class' column and shows how to select specific group members for further analysis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nspeeds\nspeeds.groupby(\"class\").nth(1)\n```\n\n----------------------------------------\n\nTITLE: Applying Arbitrary Functions with pandas GroupBy.apply in Python\nDESCRIPTION: This code set demonstrates the use of the flexible groupby apply method in pandas to run arbitrary functions on grouped data. Examples include generating groupwise descriptions, returning custom DataFrames, and mapping functions producing Series outputs. Python, pandas, and knowledge of DataFrames are required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ndf\ngrouped = df.groupby(\"A\")\n\n# could also just call .describe()\ngrouped[\"C\"].apply(lambda x: x.describe())\n```\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby('A')['C']\n\ndef f(group):\n    return pd.DataFrame({'original': group,\n                         'demeaned': group - group.mean()})\n\ngrouped.apply(f)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    return pd.Series([x, x ** 2], index=[\"x\", \"x^2\"])\n\n\ns = pd.Series(np.random.rand(5))\ns\ns.apply(f)\n```\n\n----------------------------------------\n\nTITLE: Reading and writing line-delimited JSON files in pandas - Python\nDESCRIPTION: Shows how to read line-delimited JSON strings into a pandas DataFrame and serialize it back to line-delimited JSON format. Also demonstrates creating a reader iterator to process large files in chunks with the chunksize parameter. Dependencies: pandas, io.StringIO; input: multi-line JSON string; output: DataFrame, line-delimited records, and iteration over chunked DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_83\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\njsonl = \"\"\"\n    {\"a\": 1, \"b\": 2}\n    {\"a\": 3, \"b\": 4}\n\"\"\"\ndf = pd.read_json(StringIO(jsonl), lines=True)\ndf\ndf.to_json(orient=\"records\", lines=True)\n\n# reader is an iterator that returns ``chunksize`` lines each iteration\nwith pd.read_json(StringIO(jsonl), lines=True, chunksize=1) as reader:\n    reader\n    for chunk in reader:\n        print(chunk)\n```\n\n----------------------------------------\n\nTITLE: Checking Index and Column Uniqueness - pandas - Python\nDESCRIPTION: Demonstrates how to check for uniqueness in a DataFrame's index and columns using the is_unique attribute. No input beyond a DataFrame with potentially duplicate labels; outputs two booleans indicating uniqueness of the index and columns respectively.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf2\ndf2.index.is_unique\ndf2.columns.is_unique\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrames with pandas sort_values in Python\nDESCRIPTION: This snippet demonstrates the usage of pandas DataFrame.sort_values for sorting the 'tips' DataFrame by the columns 'sex' and 'total_bill'. The pandas library must be installed, and a DataFrame object named 'tips' should already exist in the environment. The sort_values method takes a list of column names, rearranges the rows based on those columns, and returns a new, sorted DataFrame. Inputs are the list of column names for sorting, and the output is the sorted DataFrame with order updated according to the specified columns. This code is typically used in data analysis workflows that require data ordering prior to aggregation or visualization.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/sorting.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntips = tips.sort_values([\"sex\", \"total_bill\"])\ntips\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing XML Documents with Pandas\nDESCRIPTION: Shows how to read XML documents into pandas DataFrames using read_xml() and convert DataFrames back to XML using to_xml(). Uses lxml as the parser, supporting XPath 1.0 and XSLT 1.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nxml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n<data>\n <row>\n    <shape>square</shape>\n    <degrees>360</degrees>\n    <sides>4.0</sides>\n </row>\n <row>\n    <shape>circle</shape>\n    <degrees>360</degrees>\n    <sides/>\n </row>\n <row>\n    <shape>triangle</shape>\n    <degrees>180</degrees>\n    <sides>3.0</sides>\n </row>\n </data>\"\"\"\n\ndf = pd.read_xml(xml)\ndf\n\ndf.to_xml()\n```\n\n----------------------------------------\n\nTITLE: Interaction Between usecols and index_col Parameters\nDESCRIPTION: Demonstrates how index_col specification is based on the subset of columns selected by usecols, not the original data columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,c\\n4,apple,bat,\\n8,orange,cow,\"\nprint(data)\npd.read_csv(StringIO(data), usecols=[\"b\", \"c\"])\npd.read_csv(StringIO(data), usecols=[\"b\", \"c\"], index_col=0)\n```\n\n----------------------------------------\n\nTITLE: DataFrame Column Replacement with CoW\nDESCRIPTION: Demonstrates correct ways to replace values in DataFrame columns with Copy-on-Write enabled.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/copy_on_write.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})\ndf.replace({\"foo\": {1: 5}}, inplace=True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating an IntervalIndex from Tuples - Pandas - Python\nDESCRIPTION: Demonstrates constructing an IntervalIndex using pd.IntervalIndex.from_tuples, showing the intervals included. This snippet does not require any dependencies beyond pandas. The example interval tuples span several ranges and will be used for lookup demonstrations in later snippets.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nii = pd.IntervalIndex.from_tuples([(0, 4), (1, 5), (5, 8)])\\nii\n```\n\n----------------------------------------\n\nTITLE: Filling Cells with Sequential Numbers in Pandas\nDESCRIPTION: Demonstrates how to create a sequence of numbers and assign them to specific cells in a pandas DataFrame, similar to using Excel's fill handle functionality.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"AAA\": [1] * 8, \"BBB\": list(range(0, 8))})\ndf\n\nseries = list(range(1, 5))\nseries\n\ndf.loc[2:5, \"AAA\"] = series\n\ndf\n```\n\n----------------------------------------\n\nTITLE: Viewing Head and Tail of Pandas Series in Python\nDESCRIPTION: This snippet illustrates how to view subsets of a long pandas Series using the head() and tail() methods. By default, head() returns the first five rows, while tail(n) allows for viewing the last n rows. Dependencies include pandas and numpy; input is a Series of length 1000, and output is a subset of the Series for quick inspection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlong_series = pd.Series(np.random.randn(1000))\nlong_series.head()\nlong_series.tail(3)\n```\n\n----------------------------------------\n\nTITLE: Applying NumPy Binary Universal Functions to Series and Index (Python)\nDESCRIPTION: Shows that when a NumPy ufunc (np.maximum) is applied to a pandas Series and Index, the Series behavior takes precedence and the result is a Series. Dependencies: pandas, NumPy. Inputs: ser (Series) and idx (Index); output: Series of element-wise maxima.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, 3])\nidx = pd.Index([4, 5, 6])\n\nnp.maximum(ser, idx)\n```\n\n----------------------------------------\n\nTITLE: Concatenating DataFrames with Ignored Indexes in Python\nDESCRIPTION: Using ignore_index=True to concatenate DataFrames while ignoring their original indexes, creating a new consecutive integer index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.concat([df1, df4], ignore_index=True, sort=False)\nresult\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Values in Categorical Series - pandas - Python\nDESCRIPTION: Illustrates use of methods such as pd.isna and fillna on Series with categorical dtype containing missing values. Ensures missing values can be detected and imputed within categoricals. Dependencies: pandas, numpy. Key parameters: missing values, fill value. Input: Series with categoricals and NaNs; Output: Boolean mask, filled Series. Limitation: fillna value must be an existing category.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"b\", np.nan], dtype=\"category\")\ns\npd.isna(s)\ns.fillna(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Enhanced String Methods in Pandas\nDESCRIPTION: Demonstrates new string operation capabilities including str accessor for Index and expanded string methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.1.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([\" jack\", \"jill \", \" jesse \", \"frank\"])\nidx.str.strip()\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a,b\", \"a,c\", \"b,c\"])\n\n# return Series\ns.str.split(\",\")\n\n# return DataFrame\ns.str.split(\",\", expand=True)\n```\n\n----------------------------------------\n\nTITLE: Writing XML Without Declaration or Pretty Print using pandas.to_xml (Python)\nDESCRIPTION: Removes XML declaration and applies compact (non-pretty) formatting to the XML output by setting xml_declaration=False and pretty_print=False. Useful for embedding or minimizing XML. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_131\n\nLANGUAGE: python\nCODE:\n```\nprint(\n    geom_df.to_xml(xml_declaration=False,\n                   pretty_print=False)\n)\n```\n\n----------------------------------------\n\nTITLE: Basic NA Value Replacement in Pandas DataFrame\nDESCRIPTION: Demonstrates filling NA values with a scalar value using fillna() method. Shows handling of different NA types (np.nan, pd.NA, None) with object and non-object dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndata = {\"np\": [1.0, np.nan, np.nan, 2], \"arrow\": pd.array([1.0, pd.NA, pd.NA, 2], dtype=\"float64[pyarrow]\")}  \ndf = pd.DataFrame(data)\ndf.fillna(0)\n```\n\n----------------------------------------\n\nTITLE: Masking Negative Values with pandas.Series.mask and DataFrame.mask - Python\nDESCRIPTION: Demonstrates the use of mask as the inverse of where: replaces values where a condition is True with NaN. Requires a pandas DataFrame or Series. Shows usage with a Series and a DataFrame, masking values >= 0. Input: Series/DataFrame; Output: Series/DataFrame with masked values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\ns.mask(s >= 0)\ndf.mask(df >= 0)\n```\n\n----------------------------------------\n\nTITLE: Loading Air Quality Station Coordinates with pandas in Python\nDESCRIPTION: This snippet reads a metadata CSV file containing measurement station coordinates into a pandas DataFrame and displays its head. This table maps station identifiers to their geographical positions, necessary for enriching the measurement records. Dependency: the CSV file must exist at the specified location.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nstations_coord = pd.read_csv(\"data/air_quality_stations.csv\")\nstations_coord.head()\n```\n\n----------------------------------------\n\nTITLE: Querying and Writing to HDFStore with Advanced Query Syntax - Pandas - Python\nDESCRIPTION: Presents new HDFStore query formats, creation, and storage for DataFrames, as well as advanced query strings for filtering on read. Shows writing using format='table', boolean query expressions, and inline column references. Requires pandas and numpy, and HDF5 file access. Inputs: DataFrame saved to HDF5; Outputs: filtered DataFrames from store. Includes cleanup of file as a best practice.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npath = 'test.h5'\\ndfq = pd.DataFrame(np.random.randn(10, 4),\\n                   columns=list('ABCD'),\\n                   index=pd.date_range('20130101', periods=10))\\ndfq.to_hdf(path, key='dfq', format='table', data_columns=True)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_hdf(path, 'dfq',\\n            where=\"index>Timestamp('20130104') & columns=['A', 'B']\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_hdf(path, 'dfq',\\n            where=\"A>0 or C>0\")\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\\nos.remove(path)\n```\n\n----------------------------------------\n\nTITLE: Renaming DataFrame Axis Labels in pandas 1.0.0 (Python)\nDESCRIPTION: Demonstrates the correct use of DataFrame.rename with a single positional argument, and how to specify axis via keyword. Shows how to rename either row or column labels as intended.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.rename({0: 1})\ndf.rename({0: 1}, axis=1)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Categorical.unique Maintains dtype - Pandas Python\nDESCRIPTION: This snippet demonstrates the updated behavior of pd.Categorical.unique preserving the original categorical dtype. It constructs a pandas Categorical with an explicit ordered dtype, shows how the unique method was previously changing the dtype (by removing unused categories), and compares the outcomes. Requires pandas; set up a CategoricalDtype and use Series.unique. Inputs: list of category values and CategoricalDtype. Outputs: unique values array with original dtype preserved.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndtype = pd.CategoricalDtype(['bad', 'neutral', 'good'], ordered=True)\ncat = pd.Categorical(['good', 'good', 'bad', 'bad'], dtype=dtype)\noriginal = pd.Series(cat)\nunique = original.unique()\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: unique\n['good', 'bad']\nCategories (2, object): ['bad' < 'good']\nIn [2]: original.dtype == unique.dtype\nFalse\n```\n\nLANGUAGE: python\nCODE:\n```\nunique\noriginal.dtype == unique.dtype\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Styling Pipeline to DataFrame Slice - pandas - Python\nDESCRIPTION: Applies previously defined stylistic formatting pipeline to a subset of the weather DataFrame. Uses .pipe to modularize style application. Dependencies: pandas, numpy, defined make_pretty function. Input is a DataFrame slice (rows by date range), output is a styled DataFrame reflecting both functional and presentational styling; works in Jupyter or HTML rendering environments.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nweather_df.loc[\"2021-01-04\":\"2021-01-08\"].style.pipe(make_pretty)\n```\n\n----------------------------------------\n\nTITLE: DataFrame where Operation with Assignment - Python\nDESCRIPTION: Creates a copy of 'df' named 'df2' and negates all values greater than zero using boolean masking and assignment. This pattern is useful for conditional value replacement or transformation in bulk.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.copy()\\ndf2[df2 > 0] = -df2\\ndf2\n```\n\n----------------------------------------\n\nTITLE: Renaming a Series Object - pandas - Python\nDESCRIPTION: Renames a Series, creating a new object with the updated 'name' attribute, leaving the original unchanged. Uses the '.rename()' method, and demonstrates that names are distinct between objects and not shared. Input is an existing Series; returns a new Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ns2 = s.rename(\"different\")\ns2.name\n```\n\n----------------------------------------\n\nTITLE: Filtering Columns with usecols Parameter by Position\nDESCRIPTION: Shows how to select columns based on their position (index) using the usecols parameter when reading CSV data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(StringIO(data), usecols=[0, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Installing SQL Database Dependencies for Pandas\nDESCRIPTION: Command to install traditional database drivers for PostgreSQL, MySQL, and other SQL databases for use with pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pandas[postgresql, mysql, sql-other]\"\n```\n\n----------------------------------------\n\nTITLE: Timing I/O Write Operations in IPython\nDESCRIPTION: This code block measures the execution time of various write operations using IPython's %timeit magic command. It compares the performance of different file formats and compression methods for writing data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_245\n\nLANGUAGE: ipython\nCODE:\n```\n%timeit test_sql_write(df)\n%timeit test_hdf_fixed_write(df)\n%timeit test_hdf_fixed_write_compress(df)\n%timeit test_hdf_table_write(df)\n%timeit test_hdf_table_write_compress(df)\n%timeit test_csv_write(df)\n%timeit test_feather_write(df)\n%timeit test_pickle_write(df)\n%timeit test_pickle_write_compress(df)\n%timeit test_parquet_write(df)\n```\n\n----------------------------------------\n\nTITLE: Styling Index and Column Headers\nDESCRIPTION: Demonstrates styling index and column headers using map_index() and apply_index() methods with conditional coloring.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ns2.map_index(lambda v: \"color:pink;\" if v > 4 else \"color:darkblue;\", axis=0)\ns2.apply_index(\n    lambda s: np.where(s.isin([\"A\", \"B\"]), \"color:pink;\", \"color:darkblue;\"), axis=1\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Text to Lowercase with Pandas str Accessor\nDESCRIPTION: Demonstrates how to convert all characters in the 'Name' column to lowercase using the str accessor and the lower method, which applies the operation element-wise to each value in the Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/10_text_data.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Name\"].str.lower()\n```\n\n----------------------------------------\n\nTITLE: Applying Scalar String Methods via .str Accessor (Python)\nDESCRIPTION: Shows transformation of entire Series of string values to lowercase, uppercase, and computing string length using .str methods. Demonstrates that methods ignore NA values automatically. Input is a string Series, output is the transformed Series. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(\n    [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\"\n)\ns.str.lower()\ns.str.upper()\ns.str.len()\n```\n\n----------------------------------------\n\nTITLE: Creating pandas DataFrame from scipy.sparse Matrix - Python\nDESCRIPTION: Demonstrates how to initialize a pandas DataFrame backed by sparse storage from a SciPy csr_matrix, enabling seamless interoperability for large sparse datasets. All scipy.sparse formats are supported; if not in COO format, conversion may occur. Dependencies: numpy, pandas, scipy.sparse. The output is a sparse DataFrame and associated metadata.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy.sparse import csr_matrix\n\narr = np.random.random(size=(1000, 5))\narr[arr < .9] = 0\n\nsp_arr = csr_matrix(arr)\nsp_arr\n\nsdf = pd.DataFrame.sparse.from_spmatrix(sp_arr)\nsdf.head()\nsdf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Reading Tab-Delimited Data and Using read_table - pandas - Python\nDESCRIPTION: This Python snippet demonstrates how to read tab-delimited data with pandas by specifying 'sep=\"\\t\"' and 'header=None' options in 'read_csv', or by using 'read_table', which is functionally equivalent to 'read_csv' with tab as the delimiter. Inputs are file paths; outputs are DataFrames without inferred headers. Dependencies: pandas required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntips = pd.read_csv(\"tips.csv\", sep=\"\\t\", header=None)\\n\\n# alternatively, read_table is an alias to read_csv with tab delimiter\\ntips = pd.read_table(\"tips.csv\", header=None)\n```\n\n----------------------------------------\n\nTITLE: Converting Cell Values with Converters in Excel Read\nDESCRIPTION: Shows how to transform individual cell values during the read operation using the converters parameter, with an example of converting a column to boolean values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_139\n\nLANGUAGE: python\nCODE:\n```\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", converters={\"MyBools\": bool})\n```\n\n----------------------------------------\n\nTITLE: Removing HDF5 File from Disk Using os.remove - Python\nDESCRIPTION: This snippet demonstrates clean-up by removing an HDF5 file from the file system using Python's built-in os.remove function. It is typically used at the end of a test or example to avoid leaving temporary files. Requires: import os and the existence of a file named 'appends.h5'. No input/output beyond file system side effects; it raises an exception if the file does not exist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_186\n\nLANGUAGE: python\nCODE:\n```\nos.remove(\"appends.h5\")\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with Column Selection using Lambda\nDESCRIPTION: Example demonstrating how to read a CSV file with column selection using a lambda function in the usecols parameter. Shows filtering columns whose uppercase names match specific values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom io import StringIO\n\ndata = \"col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3\"\npd.read_csv(StringIO(data))\npd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"COL1\", \"COL3\"])\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas and Loading Titanic Dataset\nDESCRIPTION: Imports the pandas library and loads the Titanic dataset from a CSV file, displaying the first few rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/10_text_data.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\nLANGUAGE: python\nCODE:\n```\ntitanic = pd.read_csv(\"data/titanic.csv\")\ntitanic.head()\n```\n\n----------------------------------------\n\nTITLE: DataFrame Sum Reductions with Preserved Dtypes - pandas - Python\nDESCRIPTION: Illustrates new reduction behavior with pandas 2.1 where DataFrame.sum() preserves extension dtypes. Requires pandas, NumPy, and (for PyArrow dtypes) PyArrow installed. By constructing DataFrames with 'Int64' and 'int64[pyarrow]' dtypes, then summing across columns, the output retains extension dtype instead of reverting to base NumPy dtype. Input is a DataFrame with a specific extension dtype, expected output is a sum Series with the same extension dtype; helps users transition to enhanced dtype preservation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": [1, 1, 2, 1], \"b\": [np.nan, 2.0, 3.0, 4.0]}, dtype=\"Int64\")\ndf.sum()\ndf = df.astype(\"int64[pyarrow]\")\ndf.sum()\n```\n\n----------------------------------------\n\nTITLE: Parsing ISO8601 Date Formats - pandas - Python\nDESCRIPTION: Reads a CSV string with dates in ISO8601 format, then uses pandas.to_datetime with format='ISO8601' for efficient, robust parsing. Requires pandas and StringIO.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndata = StringIO(\"date\\n2020-01-01\\n2020-01-01 03:00\\n\")\ndf = pd.read_csv(data)\ndf['date'] = pd.to_datetime(df['date'], format='ISO8601')\ndf\n```\n\n----------------------------------------\n\nTITLE: Series, DataFrame, and Index Map with NA Action - pandas - Python\nDESCRIPTION: Demonstrates the new pandas 2.1 behavior using 'map' with 'na_action=\"ignore\"' for categorical Series, DataFrames, and Indices. Requires pandas, NumPy, and categorical dtype support. Callable is applied only to non-NA elements, with extension arrays now fully supporting this option. Inputs are constructed categorical Series, DataFrame, and Index; expected outputs are transformed objects with NA values preserved.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([\"a\", \"b\", np.nan], dtype=\"category\")\nser.map(str.upper, na_action=\"ignore\")\ndf = pd.DataFrame(ser)\ndf.map(str.upper, na_action=\"ignore\")\nidx = pd.Index(ser)\nidx.map(str.upper, na_action=\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Using the New pipe Method in Pandas for Cleaner Function Chains\nDESCRIPTION: Shows how to use the new pipe method to create cleaner, more readable function chains where both code and logic flow from top to bottom, with keyword arguments next to their respective functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.2.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n(\n    df.pipe(h)  # noqa F821\n    .pipe(g, arg1=1)  # noqa F821\n    .pipe(f, arg2=2, arg3=3)  # noqa F821\n)\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Lower Dimension Group Results Using pandas transform in Python\nDESCRIPTION: This snippet shows how pandas broadcasts the result of a groupby transformation that returns a single value per group across the input's full shape. It calculates, for each group of years, the range (max-min) of the group and broadcasts the result to the group's shape. Requires the time series ts and pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nts.groupby(lambda x: x.year).transform(lambda x: x.max() - x.min())\n```\n\n----------------------------------------\n\nTITLE: Vectorized Arithmetic and NumPy Operations on Series - pandas - Python\nDESCRIPTION: Performs vectorized operations like addition and multiplication, as well as universal NumPy functions, on Series objects. These operations execute elementwise and are efficient, matching NumPy's broadcasting behavior. Inputs must be valid Series; outputs new Series with the same labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ns + s\ns * 2\nnp.exp(s)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Expressions within a DataFrame using eval Method (Python)\nDESCRIPTION: Demonstrates using DataFrame.eval to perform arithmetic on columns by column name as variable, within the DataFrame's own namespace. Requires pandas. Inputs: DataFrame with columns 'a' and 'b'. Output: Series of element-wise sums. Simplifies concise, readable, and efficient column manipulations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 2), columns=['a', 'b'])\ndf.eval('a + b')\n```\n\n----------------------------------------\n\nTITLE: Plotting with Table Display using pandas DataFrame and Matplotlib in Python\nDESCRIPTION: This snippet demonstrates plotting a DataFrame with a transposed table below the plot by specifying table=True. It sets a random seed for reproducibility and configures the axes. Required dependencies: numpy, pandas, matplotlib. The key parameters include the DataFrame to plot and the table argument. Input is a DataFrame; output is a line plot with a table. Limitations: table will be displayed transposed to match matplotlib's rendering.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(123456)\nfig, ax = plt.subplots(1, 1, figsize=(7, 6.5))\ndf = pd.DataFrame(np.random.rand(5, 3), columns=[\"a\", \"b\", \"c\"])\nax.xaxis.tick_top()  # Display x-axis ticks on top.\ndf.plot(table=True, ax=ax);\n```\n\n----------------------------------------\n\nTITLE: Timedelta Division to Other Units - pandas - Python\nDESCRIPTION: Shows how to convert timedelta Series to days by dividing by np.timedelta64(1, 'D'). Demonstrates both true division via NumPy and the difference from floor division methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# to days\ntd / np.timedelta64(1, \"D\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Timedelta and TimedeltaIndex Attributes - pandas - Python\nDESCRIPTION: Demonstrates direct access to several component attributes (days, seconds) of a TimedeltaIndex via the .dt accessor. May require Series of timedeltas. Shows how to extract numeric time parts from the objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntd.dt.days\ntd.dt.seconds\n```\n\n----------------------------------------\n\nTITLE: Adding Additional NA Indicators - pandas - Python\nDESCRIPTION: Reads a CSV file and adds a custom NA string indicator (here \"Nope\") to pandas' default list of NA indicators. Requires pandas and a CSV input file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(\"path_to_file.csv\", na_values=[\"Nope\"])\n```\n\n----------------------------------------\n\nTITLE: Controlling DataFrame dtype backend when reading JSON - Python\nDESCRIPTION: This snippet reads a JSON string into a pandas DataFrame while specifying the dtype_backend argument to use pyarrow types for backend storage. It demonstrates how column types change depending on the backend specified, and inspects resulting dtypes. Dependencies: pandas, pyarrow, io.StringIO; inputs: string-formatted JSON data, outputs: DataFrame with pyarrow dtype columns. pyarrow must be installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_79\n\nLANGUAGE: python\nCODE:\n```\ndata = (\n '{\"a\":{\"0\":1,\"1\":3},\"b\":{\"0\":2.5,\"1\":4.5},\"c\":{\"0\":true,\"1\":false},\"d\":{\"0\":\"a\",\"1\":\"b\",}'\n '\"e\":{\"0\":null,\"1\":6.0},\"f\":{\"0\":null,\"1\":7.5},\"g\":{\"0\":null,\"1\":true},\"h\":{\"0\":null,\"1\":\"a\"},'\n '\"i\":{\"0\":\"12-31-2019\",\"1\":\"12-31-2019\"},\"j\":{\"0\":null,\"1\":null}}'\n)\ndf = pd.read_json(StringIO(data), dtype_backend=\"pyarrow\")\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Creating and Sorting DataFrames with CategoricalIndex - pandas - Python\nDESCRIPTION: Demonstrates creating a pandas DataFrame with a CategoricalIndex, specifying a category order, and using sort_index to reorder rows accordingly. Highlights the efficiency benefits for duplicate-heavy indices. Dependencies: pandas. Key parameters: categories/order for index. Input: Categorical series for index; Output: DataFrame with CategoricalIndex and sorted rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ncats = pd.Categorical([1, 2, 3, 4], categories=[4, 2, 3, 1])\nstrings = [\"a\", \"b\", \"c\", \"d\"]\nvalues = [4, 2, 3, 1]\ndf = pd.DataFrame({\"strings\": strings, \"values\": values}, index=cats)\ndf.index\n# This now sorts by the categories order\ndf.sort_index()\n```\n\n----------------------------------------\n\nTITLE: Inner Join Merge Result Comparison - pandas Python\nDESCRIPTION: This snippet presents an inner join merge example with non-unique keys to illustrate the behavioral change in pandas' sort policy. It requires pandas, constructs two DataFrames, merges them on a common column, and shows old and new result order under sort=False, aligned with the documented sort behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame({\"a\": [1, 2, 1]})\nright = pd.DataFrame({\"a\": [1, 2]})\nresult = pd.merge(left, right, how=\"inner\", on=\"a\", sort=False)\n```\n\n----------------------------------------\n\nTITLE: Alternate Row/Column Labeling for Series-to-COO Conversion - pandas - Python\nDESCRIPTION: Demonstrates conversion of a sparse Series with a MultiIndex to a COO matrix using custom combinations of index levels for rows and columns, and without label sorting. This allows fine-tuned structuring of the resulting sparse matrix. Dependencies: pandas, numpy. The output includes the COO matrix and the new axis arrangements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nA, rows, columns = ss.sparse.to_coo(\n    row_levels=[\"A\", \"B\", \"C\"], column_levels=[\"D\"], sort_labels=False\n)\n\nA\nA.todense()\nrows\ncolumns\n```\n\n----------------------------------------\n\nTITLE: to_timedelta: Parsing Strings and Lists - pandas - Python\nDESCRIPTION: Demonstrates conversion of scalar strings and lists of time strings to Timedelta or TimedeltaIndex using pd.to_timedelta. Requires pandas and numpy. Shows input string parsing, and conversion of arrays/lists of mixed values, with support for NaN.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.to_timedelta(\"1 days 06:05:01.00003\")\npd.to_timedelta(\"15.5us\")\n```\n\n----------------------------------------\n\nTITLE: Plotting DataFrame Columns on Secondary Y-Axis - pandas/matplotlib - Python\nDESCRIPTION: These snippets illustrate plotting columns 'A' and 'B' from DataFrame 'df' on separate y-axes, including use of the 'secondary_y=True' parameter to overlay 'B' on a secondary y-axis, and plotting multiple columns on the secondary axis using 'secondary_y=[\"A\",\"B\"]'. Graphical output distinguishes between axes and allows different scales per variable. Requires pandas, matplotlib, and pre-existing DataFrame 'df'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ndf[\"A\"].plot();\n\n@savefig series_plot_secondary_y.png\ndf[\"B\"].plot(secondary_y=True, style=\"g\");\n```\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\nax = df.plot(secondary_y=[\"A\", \"B\"])\nax.set_ylabel(\"CD scale\");\n@savefig frame_plot_secondary_y.png\nax.right_ax.set_ylabel(\"AB scale\");\n```\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\n\n@savefig frame_plot_secondary_y_no_right.png\ndf.plot(secondary_y=[\"A\", \"B\"], mark_right=False);\n```\n\n----------------------------------------\n\nTITLE: Creating Nullable Integer Series with pd.NA - pandas - Python\nDESCRIPTION: This code demonstrates how to create a pandas Series containing nullable integers by specifying dtype=\"Int64\" and using None to represent missing values. The snippet shows behavior when accessing a missing entry, which is represented as the experimental scalar pd.NA, introduced in pandas 1.0.0. Requires numpy and pandas 1.0.0 or higher; s[2] will output pd.NA indicating a missing value in a nullable integer column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, None], dtype=\"Int64\")\\ns\\ns[2]\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Custom BaseIndexer for Rolling Windows - pandas - Python\nDESCRIPTION: This snippet defines a custom window indexer by subclassing pandas.api.indexers.BaseIndexer to determine window bounds based on a boolean mask (use_expanding). The get_window_bounds method utilizes the use_expanding parameter to build expanding or fixed windows for each position. Dependencies include pandas and numpy. It creates a DataFrame, instantiates the custom indexer, and applies a rolling sum with this indexer. Inputs: the DataFrame, custom indexer, and use_expanding boolean list; Output: DataFrame with rolling sum. This technique allows custom dynamic windowing, though advanced users must ensure get_window_bounds signature compatibility with pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nuse_expanding = [True, False, True, False, True]\ndf = pd.DataFrame({\"values\": range(5)})\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.indexers import BaseIndexer\n\nclass CustomIndexer(BaseIndexer):\n     def get_window_bounds(self, num_values, min_periods, center, closed, step):\n         start = np.empty(num_values, dtype=np.int64)\n         end = np.empty(num_values, dtype=np.int64)\n         for i in range(num_values):\n             if self.use_expanding[i]:\n                 start[i] = 0\n                 end[i] = i + 1\n             else:\n                 start[i] = i\n                 end[i] = i + self.window_size\n         return start, end\n\nindexer = CustomIndexer(window_size=1, use_expanding=use_expanding)\n\ndf.rolling(indexer).sum()\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Values in Pandas Index\nDESCRIPTION: This code demonstrates how to handle missing values (NaN and NaT) in Pandas Index objects using the fillna() method for both numeric and datetime indexes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\nidx1 = pd.Index([1, np.nan, 3, 4])\nidx1\nidx1.fillna(2)\n\nidx2 = pd.DatetimeIndex([pd.Timestamp('2011-01-01'),\n                        pd.NaT,\n                        pd.Timestamp('2011-01-03')])\nidx2\nidx2.fillna(pd.Timestamp('2011-01-02'))\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame with Duplicate Row Labels and Label-Based Indexing - pandas - Python\nDESCRIPTION: This example builds a DataFrame with duplicate index (row) labels and shows how .loc reacts differently for unique and duplicated labels. df2.loc['b', 'A'] returns a scalar because 'b' is unique; df2.loc['a', 'A'] returns a Series because 'a' is duplicated. Highlights the dimensionality shift due to duplicates.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf2 = pd.DataFrame({\"A\": [0, 1, 2]}, index=[\"a\", \"a\", \"b\"])\ndf2\ndf2.loc[\"b\", \"A\"]  # a scalar\ndf2.loc[\"a\", \"A\"]  # a Series\n```\n\n----------------------------------------\n\nTITLE: Testing network operations in Python using pytest\nDESCRIPTION: Example of how to mock network interactions for testing using the httpserver fixture from pytest-localserver.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.network\n@pytest.mark.single_cpu\ndef test_network(httpserver):\n    httpserver.serve_content(content=\"content\")\n    result = pd.read_html(httpserver.url)\n```\n\n----------------------------------------\n\nTITLE: Comparison Operations on IntegerArray Yielding BooleanArray (Python, pandas 1.0.0)\nDESCRIPTION: Shows that comparisons involving pandas IntegerArray now yield a BooleanArray, allowing missing values propagation in logical operations. Requires pandas and variable 'a' as an IntegerArray.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\na = pd.array([1, 2, None], dtype=\"Int64\")\na > 1\n```\n\n----------------------------------------\n\nTITLE: Computing the Maximum Value of a pandas Series in Python\nDESCRIPTION: This code applies the max() method to the ages Series, returning the highest value. Input is a Series object; output is a scalar (numerical value). Assumes the Series consists of comparable numerical data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/01_table_oriented.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nages.max()\n```\n\n----------------------------------------\n\nTITLE: Slicing a Series with Unsorted Index Using Labels\nDESCRIPTION: Demonstrates how .loc slicing works with unsorted indices, returning elements located between the start and stop labels (inclusive).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(list('abcde'), index=[0, 3, 2, 5, 4])\ns.loc[3:5]\n```\n\n----------------------------------------\n\nTITLE: Initializing SparseArray with Missing Values - pandas - Python\nDESCRIPTION: Creates a SparseArray from a NumPy array that includes explicit slices of NaN values. Demonstrates storage of only non-fill values. Dependencies: numpy, pandas. The resulting SparseArray omits NaNs from internal storage and provides compressed 1D array behavior. Intended for selective missing value handling where storage optimization is important.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\narr = np.random.randn(10)\narr[2:5] = np.nan\narr[7:8] = np.nan\nsparr = pd.arrays.SparseArray(arr)\nsparr\n```\n\n----------------------------------------\n\nTITLE: Finding and Replacing Text in Pandas\nDESCRIPTION: Demonstrates how to perform find and replace operations in pandas, similar to Excel's Find and Replace feature. The example shows conditional filtering and text replacement in a DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntips\ntips == \"Sun\"\ntips[\"day\"].str.contains(\"S\")\n\ntips.replace(\"Thu\", \"Thursday\")\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to a Binary File Object in CSV Format - pandas - Python\nDESCRIPTION: Demonstrates writing a pandas DataFrame to a binary buffer (using BytesIO), with utf-8 encoding and gzip compression. Shows how pandas.to_csv can work with binary file objects. Requires pandas and io.BytesIO.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\ndata = pd.DataFrame([0, 1, 2])\nbuffer = io.BytesIO()\ndata.to_csv(buffer, encoding=\"utf-8\", compression=\"gzip\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Ambiguous Unicode Characters as Wide in pandas - Python\nDESCRIPTION: This snippet sets the pandas 'display.unicode.ambiguous_as_wide' option to True, causing ambiguous-unicode characters to render as double-width when 'display.unicode.east_asian_width' is also enabled. This impacts the column alignment in DataFrame and Series display, and is primarily effective for environments where terminal encodings require wide rendering of ambiguous characters. Both pandas options must be set correctly for the output to align as intended.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.unicode.ambiguous_as_wide\", True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Removing Duplicate Rows in Pandas\nDESCRIPTION: Shows how to use the drop_duplicates method in pandas to remove duplicate rows, similar to Excel's remove duplicates functionality. Also demonstrates filtering duplicates based on specific columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"class\": [\"A\", \"A\", \"A\", \"B\", \"C\", \"D\"],\n        \"student_count\": [42, 35, 42, 50, 47, 45],\n        \"all_pass\": [\"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\"],\n    }\n)\n\ndf.drop_duplicates()\n\ndf.drop_duplicates([\"class\", \"student_count\"])\n```\n\n----------------------------------------\n\nTITLE: List Comparison in pandas.DataFrame.query and isin Equivalents - Python\nDESCRIPTION: Depicts how comparing lists to columns in query strings works like Series.isin(), supporting equality, inequality, and membership testing ('in', 'not in'), plus demonstrating the difference in syntax when using pure Python. Requires pandas and numpy. Inputs: DataFrame; Output: Filtered DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ndf.query('b == [\"a\", \"b\", \"c\"]')\n\ndf[df['b'].isin([\"a\", \"b\", \"c\"])]\n\ndf.query('c == [1, 2]')\n\ndf.query('c != [1, 2]')\n\ndf.query('[1, 2] in c')\n\ndf.query('[1, 2] not in c')\n\ndf[df['c'].isin([1, 2])]\n```\n\n----------------------------------------\n\nTITLE: Handling Duplicate Column Names in CSV Data\nDESCRIPTION: Shows how pandas automatically handles duplicate column names by adding numbered suffixes (.1, .2, etc.) to make them unique and prevent data overwriting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,a\\n0,1,2\\n3,4,5\"\npd.read_csv(StringIO(data))\n```\n\n----------------------------------------\n\nTITLE: Numpy Platform-Dependent Array dtype Example - Python\nDESCRIPTION: Demonstrates that when creating pandas DataFrames from NumPy arrays, the dtype of the DataFrame may depend on the platform (int32 on 32-bit, int64 on 64-bit). Requires pandas and NumPy. Inputs must be NumPy arrays. Output is the DataFrame itself. Highlights a subtle interoperability nuance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_78\n\nLANGUAGE: python\nCODE:\n```\nframe = pd.DataFrame(np.array([1, 2]))\n```\n\n----------------------------------------\n\nTITLE: Conversion and Scalar Operations on TimedeltaIndex in pandas (Python)\nDESCRIPTION: Performs type conversion and scalar arithmetic on TimedeltaIndex using pandas and numpy. Includes division by NumPy timedeltas, casting to other timedelta dtypes, and demonstrates that addition/subtraction with Timestamp outputs a DatetimeIndex, while timedelta + timedelta returns TimedeltaIndex. Integer division returns TimedeltaIndex, but division by a Timedelta can produce float indices. Inputs must match expected types and operations are element-wise.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ntdi / np.timedelta64(1, \"s\")\ntdi.astype(\"timedelta64[s]\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# adding or timedelta and date -> datelike\ntdi + pd.Timestamp(\"20130101\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# subtraction of a date and a timedelta -> datelike\n# note that trying to subtract a date from a Timedelta will raise an exception\n(pd.Timestamp(\"20130101\") - tdi).to_list()\n```\n\nLANGUAGE: python\nCODE:\n```\n# timedelta + timedelta -> timedelta\ntdi + pd.Timedelta(\"10 days\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# division can result in a Timedelta if the divisor is an integer\ntdi / 2\n```\n\nLANGUAGE: python\nCODE:\n```\n# or a float64 Index if the divisor is a Timedelta\ntdi / tdi[0]\n```\n\n----------------------------------------\n\nTITLE: GroupBy Multiple Lambda Aggregations in Python using Pandas\nDESCRIPTION: Shows how to use multiple lambda functions in a GroupBy aggregation operation, which was previously not supported and would raise a SpecificationError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nanimals.groupby('kind').height.agg([\n    lambda x: x.iloc[0], lambda x: x.iloc[-1]\n])\n\nanimals.groupby('kind').agg([\n    lambda x: x.iloc[0] - x.iloc[1],\n    lambda x: x.iloc[0] + x.iloc[1]\n])\n```\n\n----------------------------------------\n\nTITLE: Interaction Between skiprows and comment Parameters\nDESCRIPTION: Shows how skiprows uses line numbers (including commented/empty lines) when parsing CSV data, which differs from how header counts rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndata = \"A,B,C\\n#comment\\na,b,c\\n1,2,3\"\npd.read_csv(StringIO(data), comment=\"#\", skiprows=2)\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to HTML in Python\nDESCRIPTION: Shows basic usage of the to_html() method to convert a DataFrame to an HTML table. The example creates a simple DataFrame and converts it to HTML.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_103\n\nLANGUAGE: python\nCODE:\n```\nfrom IPython.display import display, HTML\n\ndf = pd.DataFrame(np.random.randn(2, 2))\ndf\nhtml = df.to_html()\nprint(html)  # raw html\ndisplay(HTML(html))\n```\n\n----------------------------------------\n\nTITLE: Enabling East Asian Unicode Width Alignment in pandas Display - Python\nDESCRIPTION: This snippet sets the pandas option 'display.unicode.east_asian_width' to True to enable checking for East Asian Character Widths during DataFrame rendering. This ensures proper alignment of CJK (Chinese, Japanese, Korean) and similar characters, potentially at the cost of longer render times. This setting requires pandas to use each character's Unicode property for width calculation and only affects display output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.unicode.east_asian_width\", True)\ndf\n```\n\n----------------------------------------\n\nTITLE: NumPy Compatibility with Categorical Dtype - pandas - Python\nDESCRIPTION: Attempts to construct a NumPy dtype directly from the string 'category' and from a pandas Categorical dtype. Demonstrates that NumPy does not recognize pandas categoricals and raises TypeError. Dependencies: pandas, numpy. Key parameters: dtype inputs. Input: dtype string or pandas CategoricalDtype; Output: TypeError message. Limitation: Categorical dtype is not supported natively by NumPy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    np.dtype(\"category\")\nexcept TypeError as e:\n    print(\"TypeError:\", str(e))\n\ndtype = pd.Categorical([\"a\"]).dtype\ntry:\n    np.dtype(dtype)\nexcept TypeError as e:\n    print(\"TypeError:\", str(e))\n```\n\n----------------------------------------\n\nTITLE: Pivot Table with Multiple Aggregation Functions - pandas - Python\nDESCRIPTION: Performs a pivot_table, aggregating 'E' over indices ('B', 'C'), columnized by 'A', applying both sum and mean as aggregation functions. Demonstrates the ability to use multiple functions for value aggregation. The result is a DataFrame with a MultiIndex on columns for both aggregation type and variable.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\npd.pivot_table(\n    df, values=\"E\",\n    index=[\"B\", \"C\"],\n    columns=[\"A\"],\n    aggfunc=[\"sum\", \"mean\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Groups with Multiple Keys with pandas GroupBy in Python\nDESCRIPTION: Shows how to iterate through a DataFrame grouped by multiple columns (e.g., ['A', 'B']) using pandas GroupBy. For each (name, group) pair, both the tuple of group keys and group data are printed. This highlights that group names are tuples when using multiple keys. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfor name, group in df.groupby(['A', 'B']):\n    print(name)\n    print(group)\n```\n\n----------------------------------------\n\nTITLE: Weighted Sampling in Pandas\nDESCRIPTION: Shows how to perform weighted sampling using custom probability weights for different elements in a Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([0, 1, 2, 3, 4, 5])\nexample_weights = [0, 0, 0.2, 0.2, 0.2, 0.4]\ns.sample(n=3, weights=example_weights)\n\n# Weights will be re-normalized automatically\nexample_weights2 = [0.5, 0, 0, 0, 0, 0]\ns.sample(n=1, weights=example_weights2)\n```\n\n----------------------------------------\n\nTITLE: Extracting Days, Seconds, and Components from Timedelta Series in pandas (Python)\nDESCRIPTION: Using a timedelta Series, this snippet extracts the number of days, seconds, and all components using the .dt accessor. Demonstrates support for timedelta64 data. Inputs: Series of timedeltas. Outputs: Series of integers for days/seconds and a DataFrame for components. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(pd.timedelta_range(\"1 day 00:00:05\", periods=4, freq=\"s\"))\ns\ns.dt.days\ns.dt.seconds\ns.dt.components\n```\n\n----------------------------------------\n\nTITLE: Creating Pivot Tables and Partial Sums in Pandas - Python\nDESCRIPTION: Builds a DataFrame of sales by province and city, constructs a pivot table to sum sales by city and province, and then stacks the city level. Includes 'margins' to compute partial sums. Inputs are categorical region columns and sales data. Outputs the multi-index pivot table and a stacked version. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    data={\n        \"Province\": [\"ON\", \"QC\", \"BC\", \"AL\", \"AL\", \"MN\", \"ON\"],\n        \"City\": [\n            \"Toronto\",\n            \"Montreal\",\n            \"Vancouver\",\n            \"Calgary\",\n            \"Edmonton\",\n            \"Winnipeg\",\n            \"Windsor\",\n        ],\n        \"Sales\": [13, 6, 16, 8, 4, 3, 1],\n    }\n)\ntable = pd.pivot_table(\n    df,\n    values=[\"Sales\"],\n    index=[\"Province\"],\n    columns=[\"City\"],\n    aggfunc=\"sum\",\n    margins=True,\n)\ntable.stack(\"City\")\n```\n\n----------------------------------------\n\nTITLE: Assigning with Categorical Data and Enforcing Category Consistency - Pandas Python\nDESCRIPTION: Shows assignment of pd.Categorical objects to column slices and error handling when categories mismatch between assigned data and the DataFrame column. Useful for illustrating pandas' enforcement of consistent categorical metadata.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[\"j\":\"k\", \"cats\"] = pd.Categorical([\"a\", \"a\"], categories=[\"a\", \"b\"])\ndf\ntry:\n    df.loc[\"j\":\"k\", \"cats\"] = pd.Categorical([\"b\", \"b\"], categories=[\"a\", \"b\", \"c\"])\nexcept TypeError as e:\n    print(\"TypeError:\", str(e))\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to HTML with Column Selection in Python\nDESCRIPTION: Demonstrates how to select specific columns when converting a DataFrame to HTML using the columns parameter of to_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_104\n\nLANGUAGE: python\nCODE:\n```\nhtml = df.to_html(columns=[0])\nprint(html)\ndisplay(HTML(html))\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame by Columns and Index Levels - Pandas - Python\nDESCRIPTION: Shows how to sort a pandas DataFrame by both index levels and column names with sort_values. Demonstrates the use of MultiIndex, building a DataFrame, and sorting it by a combination of index ('second') and data column ('A'). Requires pandas and NumPy. The key parameters are the index and columns to sort by; output is a DataFrame sorted according to those keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Build MultiIndex\nidx = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('a', 2),\n                                 ('b', 2), ('b', 1), ('b', 1)])\nidx.names = ['first', 'second']\n\n# Build DataFrame\ndf_multi = pd.DataFrame({'A': np.arange(6, 0, -1)},\n                        index=idx)\ndf_multi\n\n# Sort by 'second' (index) and 'A' (column)\ndf_multi.sort_values(by=['second', 'A'])\n\n```\n\n----------------------------------------\n\nTITLE: Colored Scatter Plot using Column Values\nDESCRIPTION: This example shows how to color a scatter plot based on values from another column. The 'c' parameter takes a column name whose values determine the colors of the points.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf.plot.scatter(x=\"a\", y=\"b\", c=\"c\", s=50);\n```\n\n----------------------------------------\n\nTITLE: Filtering Products by Cumulative Volume Share in pandas GroupBy (Python)\nDESCRIPTION: This snippet identifies the largest products in each group that together comprise no more than 90% of total group volume. It sorts products by volume, calculates cumulative percentages, and selects products meeting the cumulative threshold. Assumes a DataFrame named product_volumes. Useful for market share or Pareto analyses.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nproduct_volumes = pd.DataFrame(\n    {\n        \"group\": list(\"xxxxyyy\"),\n        \"product\": list(\"abcdefg\"),\n        \"volume\": [10, 30, 20, 15, 40, 10, 20],\n    }\n)\nproduct_volumes\n\n# Sort by volume to select the largest products first\nproduct_volumes = product_volumes.sort_values(\"volume\", ascending=False)\ngrouped = product_volumes.groupby(\"group\")[\"volume\"]\ncumpct = grouped.cumsum() / grouped.transform(\"sum\")\ncumpct\nsignificant_products = product_volumes[cumpct <= 0.9]\nsignificant_products.sort_values([\"group\", \"product\"])\n```\n\n----------------------------------------\n\nTITLE: Selecting a Single Group via get_group with pandas DataFrameGroupBy in Python\nDESCRIPTION: Illustrates retrieving a specific group from a pandas GroupBy object using the get_group method. Here, 'bar' is passed to get_group to select that group. Works with single-key groupings. Requires a DataFrame grouped by 'A'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ngrouped.get_group(\"bar\")\n```\n\n----------------------------------------\n\nTITLE: Specifying SQL column types when writing DataFrames\nDESCRIPTION: Shows how to override default SQL type inference when writing a DataFrame to a database by using the dtype parameter. This example specifies that 'Col_1' should use the String type instead of Text.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_227\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy.types import String\n\ndata.to_sql(\"data_dtype\", con=engine, dtype={\"Col_1\": String})\n```\n\n----------------------------------------\n\nTITLE: Multiple Aggregations with pandas DataFrames - Python\nDESCRIPTION: This code shows how to aggregate a DataFrame using a list of functions ('sum', 'min') for each column, returning a DataFrame with results for each aggregation. It requires pandas and a DataFrame, and outputs a DataFrame where rows indicate the aggregation function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf.agg(['sum', 'min'])\n```\n\n----------------------------------------\n\nTITLE: Autosummary for pandas RangeIndex, CategoricalIndex, and IntervalIndex - reStructuredText\nDESCRIPTION: This collection of autosummary snippets registers the RangeIndex, CategoricalIndex, and IntervalIndex classes, along with their key properties and methods, for generation in the pandas Sphinx documentation. Custom templates are used for class documentation. Methods and data attributes (such as 'RangeIndex.start', 'CategoricalIndex.categories', etc.) are listed individually for autosummary page creation. This is dependent on Sphinx, pandas, and correct module paths.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/indexing.rst#_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n   :template: autosummary/class_without_autosummary.rst\\n\\n   RangeIndex\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   RangeIndex.start\\n   RangeIndex.stop\\n   RangeIndex.step\\n   RangeIndex.from_range\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n   :template: autosummary/class_without_autosummary.rst\\n\\n   CategoricalIndex\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   CategoricalIndex.codes\\n   CategoricalIndex.categories\\n   CategoricalIndex.ordered\\n   CategoricalIndex.rename_categories\\n   CategoricalIndex.reorder_categories\\n   CategoricalIndex.add_categories\\n   CategoricalIndex.remove_categories\\n   CategoricalIndex.remove_unused_categories\\n   CategoricalIndex.set_categories\\n   CategoricalIndex.as_ordered\\n   CategoricalIndex.as_unordered\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   CategoricalIndex.map\\n   CategoricalIndex.equals\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n   :template: autosummary/class_without_autosummary.rst\\n\\n   IntervalIndex\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   IntervalIndex.from_arrays\\n   IntervalIndex.from_tuples\\n   IntervalIndex.from_breaks\\n   IntervalIndex.left\\n   IntervalIndex.right\\n   IntervalIndex.mid\\n   IntervalIndex.closed\\n   IntervalIndex.length\\n   IntervalIndex.values\\n   IntervalIndex.is_empty\\n   IntervalIndex.is_non_overlapping_monotonic\\n   IntervalIndex.is_overlapping\\n   IntervalIndex.get_loc\\n   IntervalIndex.get_indexer\\n   IntervalIndex.set_closed\\n   IntervalIndex.contains\\n   IntervalIndex.overlaps\\n   IntervalIndex.to_tuples\\n\n```\n\n----------------------------------------\n\nTITLE: Splitting a DataFrame Based on Boolean Criteria in Python\nDESCRIPTION: Divides a DataFrame into subsets based on boolean conditions. This method creates separate DataFrames by filtering rows that satisfy or don't satisfy a specific condition.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n)\ndf\n\ndf[df.AAA <= 5]\ndf[df.AAA > 5]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Chained Assignment Warnings and Migration - pandas - Python\nDESCRIPTION: This set of examples demonstrates instances of chained assignment on DataFrames and highlights associated warnings and future behavioral incompatibilities in pandas 3.0. It shows not only deprecated patterns but also explains and demonstrates the correct single-step assignment using .loc. Requires the pandas library. Inputs are DataFrames/Series with selection criteria; outputs are warning messages or correctly updated DataFrames. Users should avoid chained assignment to ensure DataFrame updates are persistent.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})\n\n# first selecting rows with a mask, then assigning values to a column\n# -> this has never worked and raises a SettingWithCopyWarning\ndf[df[\"bar\"] > 5][\"foo\"] = 100\n\n# first selecting the column, and then assigning to a subset of that column\n# -> this currently works\ndf[\"foo\"][df[\"bar\"] > 5] = 100\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> df[\"foo\"][df[\"bar\"] > 5] = 100\nFutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[df[\"bar\"] > 5, \"foo\"] = 100\n```\n\n----------------------------------------\n\nTITLE: Calculating Prices with GroupBy.pipe in Pandas\nDESCRIPTION: Shows how to use GroupBy.pipe to calculate prices (revenue divided by quantity) per store and product in a single readable operation, then unstack the results for tabular display.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n(df.groupby(['Store', 'Product'])\n   .pipe(lambda grp: grp.Revenue.sum() / grp.Quantity.sum())\n   .unstack().round(2))\n```\n\n----------------------------------------\n\nTITLE: Indexing Series with TimedeltaIndex in pandas (Python)\nDESCRIPTION: Shows how to use TimedeltaIndex as the index of a pandas Series. Requires pandas and numpy installed. Constructs a Series 's' of 100 elements, indexed by hourly timedeltas starting from '1 days'. The TimedeltaIndex enables time-based selection and slicing, similar to DatetimeIndex and PeriodIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(\n    np.arange(100),\n    index=pd.timedelta_range(\"1 days\", periods=100, freq=\"h\"),\n)\ns\n```\n\n----------------------------------------\n\nTITLE: Grouping by Index Level Names via Keys with pandas GroupBy in Python\nDESCRIPTION: Illustrates how to use groupby with MultiIndex Series by passing the index level names directly as keys. This code groups the Series 's' by the 'first' and 'second' levels and computes the sum. Requires pandas and a MultiIndex Series with appropriate level names. The input is the Series 's', and output is the Series grouped and summed by the specified keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ns.groupby([\"first\", \"second\"]).sum()\n```\n\n----------------------------------------\n\nTITLE: Returning Multiple Values (Length and Random Letters) in Python\nDESCRIPTION: This snippet defines a Python function named random_letters that returns a tuple: a random integer (length) and a random string of letters. It uses NumPy for random number and character selection, and Python's string module for character choices. The docstring follows numpy-style, annotating each component of the tuple in the 'Returns' section. Dependencies: numpy as np, string. Inputs: none. Outputs: tuple (length:int, letters:str). Demonstrates documenting multiple return values with types and descriptions for each.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport string\n\ndef random_letters():\n    \"\"\"\n    Generate and return a sequence of random letters.\n\n    The length of the returned string is also random, and is also\n    returned.\n\n    Returns\n    -------\n    length : int\n        Length of the returned string.\n    letters : str\n        String of random letters.\n    \"\"\"\n    length = np.random.randint(1, 10)\n    letters = ''.join(np.random.choice(string.ascii_lowercase)\n                      for i in range(length))\n    return length, letters\n\n```\n\n----------------------------------------\n\nTITLE: Creating Series with 'string' Alias for StringDtype - pandas - Python\nDESCRIPTION: This snippet creates a pandas Series of strings using dtype=\"string\", which is an alias for StringDtype, and stores missing values using None. The code then outputs the Series to illustrate how string extension arrays appear. Requires pandas 1.0.0 or later for support of the string extension type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['abc', None, 'def'], dtype=\"string\")\\ns\n```\n\n----------------------------------------\n\nTITLE: Shifting Values Within Groups Based on Index Level Using groupby.shift - Pandas - Python\nDESCRIPTION: Creates a DataFrame with custom row indices and groups by the first index level. Shifts the 'beyer' values within each group by 1, assigning the result to a new column. Inputs require a specific MultiIndex structure. Outputs the DataFrame with the additional shifted column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"line_race\": [10, 10, 8, 10, 10, 8], \"beyer\": [99, 102, 103, 103, 88, 100]},\n    index=[\n        \"Last Gunfighter\",\n        \"Last Gunfighter\",\n        \"Last Gunfighter\",\n        \"Paynter\",\n        \"Paynter\",\n        \"Paynter\",\n    ],\n)\ndf\ndf[\"beyer_shifted\"] = df.groupby(level=0)[\"beyer\"].shift(1)\ndf\n```\n\n----------------------------------------\n\nTITLE: Conditional Setting with Alignment using pandas.DataFrame.where - Python\nDESCRIPTION: Shows how pandas.DataFrame.where aligns the boolean mask to the DataFrame, enabling partial selection and assignment. Requires pandas and numpy. Demonstrates setting positive values to 3 based on a boolean mask on a slice of the DataFrame. Input: DataFrame; Output: DataFrame with modified elements where the condition is met.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.copy()\ndf2[df2[1:4] > 0] = 3\ndf2\n```\n\n----------------------------------------\n\nTITLE: Exporting DataFrame to HTML with Custom CSS Classes - pandas - Python\nDESCRIPTION: Demonstrates how to export a pandas DataFrame to HTML with custom CSS classes appended. The snippet uses the 'to_html' method with the 'classes' argument, which adds specified class names to the rendered HTML table in addition to the default 'dataframe' class. No additional dependencies are required; the DataFrame (df) must be instantiated, and the method outputs a string of HTML representing the table.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_107\n\nLANGUAGE: python\nCODE:\n```\nprint(df.to_html(classes=[\"awesome_table_class\", \"even_more_awesome_class\"]))\n```\n\n----------------------------------------\n\nTITLE: Creating a Sparse Array Using String Alias Syntax - pandas - Python\nDESCRIPTION: Demonstrates an alternative, concise way of specifying a SparseDtype using a string alias (e.g., 'Sparse[int]') when constructing a pandas array. This approach streamlines type specification for sparse structures. Dependencies: pandas. Takes a list of values and creates a SparseArray with specified type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npd.array([1, 0, 0, 2], dtype='Sparse[int]')\n```\n\n----------------------------------------\n\nTITLE: Reading a SAS7BDAT File into a pandas DataFrame - Python\nDESCRIPTION: This snippet shows how to load a SAS7BDAT (.sas7bdat) file into a pandas DataFrame using pandas.read_sas. This method supports reading ASCII and floating-point columns, with automatic type conversions depending on format and file contents. Input: filename (typically .sas7bdat), output: DataFrame. Requires pandas library and the input file must exist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_239\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_sas(\"sas_data.sas7bdat\")\n```\n\n----------------------------------------\n\nTITLE: Counting Unique Values in Groups with pandas GroupBy.nunique in Python\nDESCRIPTION: Illustrates how to count the number of unique values in each group after grouping by column 'A'. A small DataFrame is created and grouped, with nunique applied to column 'B'. Input is a DataFrame, output is a Series of counts of unique values per group. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nll = [['foo', 1], ['foo', 2], ['foo', 2], ['bar', 1], ['bar', 1]]\ndf4 = pd.DataFrame(ll, columns=[\"A\", \"B\"])\ndf4\ndf4.groupby(\"A\")[\"B\"].nunique()\n```\n\n----------------------------------------\n\nTITLE: Setting CategoricalIndex and Demonstrating Properties\nDESCRIPTION: Shows how to set a categorical column as index and demonstrates indexing operations, sorting, and how the CategoricalIndex is preserved after operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.set_index(\"B\")\ndf2.index\n\ndf2.loc[\"a\"]\n\ndf2.loc[\"a\"].index\n\ndf2.sort_index()\n\ndf2.groupby(level=0, observed=True).sum()\ndf2.groupby(level=0, observed=True).sum().index\n```\n\n----------------------------------------\n\nTITLE: Dropping Labels from a pandas DataFrame Axis (Python)\nDESCRIPTION: Shows removal of specific row and column labels from a DataFrame using drop, as well as using reindex with index.difference. This requires a DataFrame; axis=0 for rows, axis=1 for columns. Inputs are the labels to be dropped. Returns a new DataFrame without the specified labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndf\ndf.drop([\"a\", \"d\"], axis=0)\ndf.drop([\"one\"], axis=1)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.reindex(df.index.difference([\"a\", \"d\"]))\n```\n\n----------------------------------------\n\nTITLE: Boolean Operations Error Handling in pandas\nDESCRIPTION: Shows how boolean operations on entire pandas objects raise ValueError to prevent ambiguous comparisons, and demonstrates the new .bool() method for single-element evaluation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame({'A': np.random.randn(10),\n...                    'B': np.random.randn(10),\n...                    'C': pd.date_range('20130101', periods=10)\n...                    })\n...\n>>> if df:\n...     pass\n...\nTraceback (most recent call last):\n    ...\nValueError: The truth value of a DataFrame is ambiguous.  Use a.empty,\na.bool(), a.item(), a.any() or a.all().\n\n>>> df1 = df\n>>> df2 = df\n>>> df1 and df2\nTraceback (most recent call last):\n    ...\nValueError: The truth value of a DataFrame is ambiguous.  Use a.empty,\na.bool(), a.item(), a.any() or a.all().\n\n>>> d = [1, 2, 3]\n>>> s1 = pd.Series(d)\n>>> s2 = pd.Series(d)\n>>> s1 and s2\nTraceback (most recent call last):\n    ...\nValueError: The truth value of a DataFrame is ambiguous.  Use a.empty,\na.bool(), a.item(), a.any() or a.all().\n\n>>> pd.Series([True]).bool()\n True\n>>> pd.Series([False]).bool()\n False\n>>> pd.DataFrame([[True]]).bool()\n True\n>>> pd.DataFrame([[False]]).bool()\n False\n```\n\n----------------------------------------\n\nTITLE: Flattening Nested XML with XSLT and Reading with pandas.read_xml (Python/XML)\nDESCRIPTION: Shows how to flatten a nested XML structure using an XSLT stylesheet for preprocessing prior to parsing with pandas.read_xml. The workflow involves defining an XML document with nested tags, an XSLT string to transform its structure, and then using pandas.read_xml with a stylesheet argument. Requires pandas, StringIO from io, and lxml for XSLT support.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_122\n\nLANGUAGE: python\nCODE:\n```\nxml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n <response>\n  <row>\n    <station id=\\\"40850\\\" name=\\\"Library\\\"/>\n    <month>2020-09-01T00:00:00</month>\n    <rides>\n      <avg_weekday_rides>864.2</avg_weekday_rides>\n      <avg_saturday_rides>534</avg_saturday_rides>\n      <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n    </rides>\n  </row>\n  <row>\n    <station id=\\\"41700\\\" name=\\\"Washington/Wabash\\\"/>\n    <month>2020-09-01T00:00:00</month>\n    <rides>\n      <avg_weekday_rides>2707.4</avg_weekday_rides>\n      <avg_saturday_rides>1909.8</avg_saturday_rides>\n      <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n    </rides>\n  </row>\n  <row>\n    <station id=\\\"40380\\\" name=\\\"Clark/Lake\\\"/>\n    <month>2020-09-01T00:00:00</month>\n    <rides>\n      <avg_weekday_rides>2949.6</avg_weekday_rides>\n      <avg_saturday_rides>1657</avg_saturday_rides>\n      <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n    </rides>\n  </row>\n </response>\"\"\"\n\nxsl = \"\"\"<xsl:stylesheet version=\\\"1.0\\\" xmlns:xsl=\\\"http://www.w3.org/1999/XSL/Transform\\\">\n   <xsl:output method=\\\"xml\\\" omit-xml-declaration=\\\"no\\\" indent=\\\"yes\\\"/>\n   <xsl:strip-space elements=\\\"*\\\"/>\n   <xsl:template match=\\\"/response\\\">\\n      <xsl:copy>\\n        <xsl:apply-templates select=\\\"row\\\"/>\\n      </xsl:copy>\\n   </xsl:template>\\n   <xsl:template match=\\\"row\\\">\\n      <xsl:copy>\\n        <station_id><xsl:value-of select=\\\"station/@id\\\"/></station_id>\\n        <station_name><xsl:value-of select=\\\"station/@name\\\"/></station_name>\\n        <xsl:copy-of select=\\\"month|rides/*\\\"/>\\n      </xsl:copy>\\n   </xsl:template>\\n </xsl:stylesheet>\"\"\"\n\noutput = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n <response>\n   <row>\n      <station_id>40850</station_id>\n      <station_name>Library</station_name>\n      <month>2020-09-01T00:00:00</month>\n      <avg_weekday_rides>864.2</avg_weekday_rides>\n      <avg_saturday_rides>534</avg_saturday_rides>\n      <avg_sunday_holiday_rides>417.2</avg_sunday_holiday_rides>\n   </row>\n   <row>\n      <station_id>41700</station_id>\n      <station_name>Washington/Wabash</station_name>\n      <month>2020-09-01T00:00:00</month>\n      <avg_weekday_rides>2707.4</avg_weekday_rides>\n      <avg_saturday_rides>1909.8</avg_saturday_rides>\n      <avg_sunday_holiday_rides>1438.6</avg_sunday_holiday_rides>\n   </row>\n   <row>\n      <station_id>40380</station_id>\n      <station_name>Clark/Lake</station_name>\n      <month>2020-09-01T00:00:00</month>\n      <avg_weekday_rides>2949.6</avg_weekday_rides>\n      <avg_saturday_rides>1657</avg_saturday_rides>\n      <avg_sunday_holiday_rides>1453.8</avg_sunday_holiday_rides>\n   </row>\n </response>\"\"\"\n\ndf = pd.read_xml(StringIO(xml), stylesheet=StringIO(xsl))\ndf\n```\n\nLANGUAGE: xml\nCODE:\n```\n<xsl:stylesheet version=\\\"1.0\\\" xmlns:xsl=\\\"http://www.w3.org/1999/XSL/Transform\\\">\n   <xsl:output method=\\\"xml\\\" omit-xml-declaration=\\\"no\\\" indent=\\\"yes\\\"/>\n   <xsl:strip-space elements=\\\"*\\\"/>\n   <xsl:template match=\\\"/response\\\">\n      <xsl:copy>\n        <xsl:apply-templates select=\\\"row\\\"/>\n      </xsl:copy>\n   </xsl:template>\n   <xsl:template match=\\\"row\\\">\n      <xsl:copy>\n        <station_id><xsl:value-of select=\\\"station/@id\\\"/></station_id>\n        <station_name><xsl:value-of select=\\\"station/@name\\\"/></station_name>\n        <xsl:copy-of select=\\\"month|rides/*\\\"/>\n      </xsl:copy>\n   </xsl:template>\n </xsl:stylesheet>\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame Values\nDESCRIPTION: Examples of sorting DataFrame rows by single and multiple columns using sort_values()\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/07_reshape_table_layout.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntitanic.sort_values(by=\"Age\").head()\ntitanic.sort_values(by=['Pclass', 'Age'], ascending=False).head()\n```\n\n----------------------------------------\n\nTITLE: Slicing a Sorted Series with Labels\nDESCRIPTION: Shows how .loc slicing with labels works on a sorted index, selecting elements that rank between the start and stop labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ns.sort_index()\ns.sort_index().loc[1:6]\n```\n\n----------------------------------------\n\nTITLE: MultiIndexSelection with IndexSlice Helper - pandas Python\nDESCRIPTION: Demonstrates shorthand usage for MultiIndex selection using the pandas.IndexSlice helper. By assigning idx = pd.IndexSlice, it enables concise selction over multiple index and column levels in a highly readable form.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.IndexSlice\ndf.loc[idx[:, :, ['C1', 'C3']], idx[:, 'foo']]\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Age of Titanic Passengers with pandas - Python\nDESCRIPTION: This snippet calculates the average age of all Titanic passengers by accessing the 'Age' column of the DataFrame and invoking its .mean() method. The operation excludes missing data by default and returns a single float representing the mean age. Requires the 'titanic' DataFrame loaded with an 'Age' column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Age\"].mean()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Various pandas Column Doubling Methods (ipython)\nDESCRIPTION: Compares the performance of using apply with a non-Numba function, direct vectorized assignment, and Numba-vectorized function for doubling the values in a pandas DataFrame column. Benchmarks each method to illustrate performance differences. Inputs: pandas DataFrame/Series; Outputs: timing results. Highlights that pandas' built-in operations are fast, but Numba vectorization can be faster.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_16\n\nLANGUAGE: ipython\nCODE:\n```\n# Custom function without numba\nIn [5]: %timeit df[\"col1_doubled\"] = df[\"a\"].apply(double_every_value_nonumba)  # noqa E501\n1000 loops, best of 3: 797 us per loop\n\n# Standard implementation (faster than a custom function)\nIn [6]: %timeit df[\"col1_doubled\"] = df[\"a\"] * 2\n1000 loops, best of 3: 233 us per loop\n\n# Custom function with numba\nIn [7]: %timeit df[\"col1_doubled\"] = double_every_value_withnumba(df[\"a\"].to_numpy())\n1000 loops, best of 3: 145 us per loop\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Monotonic DataFrame Index and Querying with loc - pandas - Python\nDESCRIPTION: Shows how to create a pandas DataFrame with a custom, non-monotonic index and how to check its monotonicity using is_monotonic_increasing. Demonstrates querying the DataFrame with .loc for a slice where both bounds are unique members of the index. Requires pandas to be imported as pd. This snippet validates index constraints for non-monotonic indices before slicing, ensuring proper label-based access.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(index=[2, 3, 1, 4, 3, 5], columns=[\"data\"], data=list(range(6)))\ndf.index.is_monotonic_increasing\n\n# OK because 2 and 4 are in the index\ndf.loc[2:4, :]\n```\n\n----------------------------------------\n\nTITLE: Reading XML with Selective XPath Query - pandas - Python\nDESCRIPTION: Shows how to limit parsed nodes using the 'xpath' argument. Parses only '<book>' elements where '<year>=2005' from the example XML file and returns them as a DataFrame. This method leverages lxml's full XPath support for conditional selection. Prerequisites are pandas with lxml installed and the target XML file present.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_117\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_xml(file_path, xpath=\"//book[year=2005]\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Closing and Using Context Manager with HDFStore in Pandas\nDESCRIPTION: Demonstrates closing an HDFStore manually and using a context manager for automatic closure. The context manager is the recommended approach for safely handling HDF5 files.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_166\n\nLANGUAGE: python\nCODE:\n```\nstore.close()\nstore\nstore.is_open\n\n# Working with, and automatically closing the store using a context manager\nwith pd.HDFStore(\"store.h5\") as store:\n    store.keys()\n```\n\n----------------------------------------\n\nTITLE: Setting Column Count Threshold for DataFrame.info Output - Python\nDESCRIPTION: Shows usage of the 'max_info_columns' display option. Adjusts how many columns trigger shortened output in DataFrame.info(). Example sets an 11-column threshold, displays info, then sets threshold to 5 and shows the compacted output for wide DataFrames. Useful for managing verbosity. Reset at the end.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 10))\npd.set_option(\"max_info_columns\", 11)\ndf.info()\npd.set_option(\"max_info_columns\", 5)\ndf.info()\npd.reset_option(\"max_info_columns\")\n```\n\n----------------------------------------\n\nTITLE: Comparing DataFrames While Retaining Original Shape Using pandas in Python\nDESCRIPTION: This snippet illustrates using pandas DataFrame.compare() with the keep_shape=True argument, which retains all original rows and columns in the result, even if all corresponding values are equal. This is useful when a full representation is needed instead of compacting the output. Required: pandas, two DataFrames for comparison. The keep_shape argument is set to True. Inputs are two DataFrames, and output is a difference DataFrame with the same shape as the inputs, with NaNs where values match.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndf.compare(df2, keep_shape=True)\n```\n\n----------------------------------------\n\nTITLE: Aligning Series Objects with align() - pandas - Python\nDESCRIPTION: Uses Series.align() to simultaneously align two Series objects on their index, supporting multiple join strategies ('outer', 'left', 'right', 'inner'). Returns a tuple of aligned Series. Inputs: two Series, join method. Output: tuple of reindexed Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\ns1 = s[:4]\ns2 = s[1:]\ns1.align(s2)\ns1.align(s2, join=\"inner\")\ns1.align(s2, join=\"left\")\n```\n\n----------------------------------------\n\nTITLE: Joining Single-Index and MultiIndex DataFrames in Python\nDESCRIPTION: Shows how to join a single-indexed DataFrame with a MultiIndexed DataFrame using household and portfolio data example.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nhousehold = pd.DataFrame({'household_id': [1, 2, 3],\n                           'male': [0, 1, 0],\n                           'wealth': [196087.3, 316478.7, 294750]\n                           },\n                          columns=['household_id', 'male', 'wealth']\n                          ).set_index('household_id')\n\nportfolio = pd.DataFrame({'household_id': [1, 2, 2, 3, 3, 3, 4],\n                           'asset_id': [\"nl0000301109\",\n                                        \"nl0000289783\",\n                                        \"gb00b03mlx29\",\n                                        \"gb00b03mlx29\",\n                                        \"lu0197800237\",\n                                        \"nl0000289965\",\n                                        np.nan],\n                           'name': [\"ABN Amro\",\n                                    \"Robeco\",\n                                    \"Royal Dutch Shell\",\n                                    \"Royal Dutch Shell\",\n                                    \"AAB Eastern Europe Equity Fund\",\n                                    \"Postbank BioTech Fonds\",\n                                    np.nan],\n                           'share': [1.0, 0.4, 0.6, 0.15, 0.6, 0.25, 1.0]\n                           },\n                          columns=['household_id', 'asset_id', 'name', 'share']\n                          ).set_index(['household_id', 'asset_id'])\n\nhousehold.join(portfolio, how='inner')\n```\n\n----------------------------------------\n\nTITLE: Using origin and offset parameters in resample\nDESCRIPTION: Example of using the new origin parameter in resample to control the timestamp on which to adjust the grouping. This provides more control over bin alignment for different frequency resample operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstart, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00'\nmiddle = '2000-10-02 00:00:00'\nrng = pd.date_range(start, end, freq='7min')\nts = pd.Series(np.arange(len(rng)) * 3, index=rng)\nts\n```\n\nLANGUAGE: python\nCODE:\n```\nts.resample('17min').sum()\nts.resample('17min', origin='start_day').sum()\n```\n\nLANGUAGE: python\nCODE:\n```\nts.resample('17min', origin='epoch').sum()\nts.resample('17min', origin='2000-01-01').sum()\n```\n\n----------------------------------------\n\nTITLE: Mean Normalization by Group in SAS\nDESCRIPTION: Calculates the mean of 'total_bill' for each 'smoker' group and merges it back to the original frame to compute an adjusted 'total_bill'. Requires: 'tips' table with a 'smoker' column. Outputs new version of 'tips' with an added field 'adj_total_bill.' Multi-step transformation emulates pandas groupby-transform operation in SAS.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_22\n\nLANGUAGE: sas\nCODE:\n```\nproc summary data=tips missing nway;\n    class smoker;\n    var total_bill;\n    output out=smoker_means mean(total_bill)=group_bill;\nrun;\n\nproc sort data=tips;\n    by smoker;\nrun;\n\ndata tips;\n    merge tips(in=a) smoker_means(in=b);\n    by smoker;\n    adj_total_bill = total_bill - group_bill;\n    if a and b;\nrun;\n```\n\n----------------------------------------\n\nTITLE: Writing Hierarchical DataFrame Columns to Flattened XML with pandas.to_xml (Python)\nDESCRIPTION: Generates XML from a DataFrame with hierarchical (MultiIndex) columns, which are flattened using underscore separators in the XML element names. DataFrame is derived from a pivot table on shape and type, aggregating degrees and sides. Requires pandas and NumPy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_128\n\nLANGUAGE: python\nCODE:\n```\next_geom_df = pd.DataFrame(\n    {\n        \"type\": [\"polygon\", \"other\", \"polygon\"],\n        \"shape\": [\"square\", \"circle\", \"triangle\"],\n        \"degrees\": [360, 360, 180],\n        \"sides\": [4, np.nan, 3],\n    }\n)\n\npvt_df = ext_geom_df.pivot_table(index='shape',\n                                 columns='type',\n                                 values=['degrees', 'sides'],\n                                 aggfunc='sum')\npvt_df\n\nprint(pvt_df.to_xml())\n```\n\n----------------------------------------\n\nTITLE: Grouping and Counting Records using Pandas groupby - Python\nDESCRIPTION: Demonstrates the use of the groupby method in Pandas to aggregate counts across multiple columns, with the option to control inclusion of observed values only. Requires pandas. The groupby method takes columns (A, B, C) and uses the observed argument to specify whether to show only observed combinations or all possible groupings; output is a DataFrame with count per group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(['A', 'B', 'C'], observed=False).count()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(['A', 'B', 'C'], observed=True).count()\n```\n\n----------------------------------------\n\nTITLE: Getting Technical Summary Information of a pandas DataFrame in Python\nDESCRIPTION: Provides a concise, technical summary of the 'titanic' DataFrame with '.info()'. The method outputs details such as row count, column count, non-null entries, data types, index range, and approximate memory usage. Useful for quick data audits and checking for missing data or data type issues.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntitanic.info()\n```\n\n----------------------------------------\n\nTITLE: DataFrame Explode Method Example in Python using Pandas\nDESCRIPTION: Demonstrates the new explode method for transforming comma-separated strings in a column to individual rows in a long-form DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([{'var1': 'a,b,c', 'var2': 1},\n                   {'var1': 'd,e,f', 'var2': 2}])\ndf.assign(var1=df.var1.str.split(',')).explode('var1')\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Styling Functions with Styler.pipe in pandas\nDESCRIPTION: Demonstrates the new Styler.pipe method for applying custom styling functions to DataFrames. This allows reusable styling to be applied more easily.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'N': [1250, 1500, 1750], 'X': [0.25, 0.35, 0.50]})\n\ndef format_and_align(styler):\n    return (styler.format({'N': '{:,}', 'X': '{:.1%}'})\n                  .set_properties(**{'text-align': 'right'}))\n\ndf.style.pipe(format_and_align).set_caption('Summary of results.')\n```\n\n----------------------------------------\n\nTITLE: Rounding/Flooring/Ceiling on DatetimeIndex, Timestamp, TimedeltaIndex, and Timedelta - pandas - Python\nDESCRIPTION: Demonstrates usage of .round(), .floor() and .ceil() methods for datetime and timedelta-like pandas objects. Requires pandas and numpy (for np.arange). Shows both naive and tz-aware datetime rounding, as well as Timedelta operations, highlighting input periods, strings as freq specifiers, and result types (scalar and index). Outputs rounded/floored/ceiled versions in expected frequency, with support through the .dt accessor for Series objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndr = pd.date_range('20130101 09:12:56.1234', periods=3)\ndr\ndr.round('s')\n\n# Timestamp scalar\ndr[0]\ndr[0].round('10s')\n```\n\nLANGUAGE: python\nCODE:\n```\ndr = dr.tz_localize('US/Eastern')\ndr\ndr.round('s')\n```\n\nLANGUAGE: python\nCODE:\n```\nt = pd.timedelta_range('1 days 2 hr 13 min 45 us', periods=3, freq='d')\nt\n# TimedeltaIndex before rounding\n# TimedeltaIndex(['1 days 02:13:00.000045', ...])\nt.round('10min')\n# TimedeltaIndex after rounding\n\n# Timedelta scalar\nt[0]\nt[0].round('2h')\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(dr)\ns\ns.dt.round('D')\n```\n\n----------------------------------------\n\nTITLE: Logical Operations with NA using pandas (Python)\nDESCRIPTION: These snippets show how logical AND (&) and OR (|) operations behave with NA values following three-valued (Kleene) logic. The code covers different boolean operand combinations with NA. There are no dependencies except for pandas. Results vary based on the non-NA operand, with NA propagating only when the result is ambiguous.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nTrue | False\nTrue | pd.NA\npd.NA | True\n```\n\nLANGUAGE: python\nCODE:\n```\nFalse | True\nFalse | False\nFalse | pd.NA\n```\n\nLANGUAGE: python\nCODE:\n```\nFalse & True\nFalse & False\nFalse & pd.NA\n```\n\nLANGUAGE: python\nCODE:\n```\nTrue & True\nTrue & False\nTrue & pd.NA\n```\n\n----------------------------------------\n\nTITLE: Bar Plotting a Single Series from DataFrame (Python)\nDESCRIPTION: Demonstrates how to produce a bar plot from a single row of a DataFrame using Series.plot(kind=\"bar\"). Plots values from the sixth row as vertical bars. Dependencies are pandas, numpy, and matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\\n\\n@savefig bar_plot_ex.png\\ndf.iloc[5].plot(kind=\"bar\");\n```\n\n----------------------------------------\n\nTITLE: Constructing DataFrame from Dict of Tuples (MultiIndex) - Pandas - Python\nDESCRIPTION: Illustrates creation of a DataFrame with MultiIndex columns and rows by passing a dict whose keys and nested dict keys are tuples. This leads pandas to build hierarchical indices both for columns and rows. Requires pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame(\n    {\n        (\"a\", \"b\"): {(\"A\", \"B\"): 1, (\"A\", \"C\"): 2},\n        (\"a\", \"a\"): {(\"A\", \"C\"): 3, (\"A\", \"B\"): 4},\n        (\"a\", \"c\"): {(\"A\", \"B\"): 5, (\"A\", \"C\"): 6},\n        (\"b\", \"a\"): {(\"A\", \"C\"): 7, (\"A\", \"B\"): 8},\n        (\"b\", \"b\"): {(\"A\", \"D\"): 9, (\"A\", \"B\"): 10},\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Functions Returning Series in Series.apply and Upcasting to DataFrame - python\nDESCRIPTION: Shows how Series.apply() functions that themselves return Series are upcasted, resulting in a DataFrame. Defines a function f(x) returning a two-element Series, then applies it to a random Series. Clarifies new broadcasting behavior and the automatic conversion of scalar-to-Series transformations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    return pd.Series([x, x ** 2], index=[\"x\", \"x^2\"])\n\ns = pd.Series(np.random.rand(5))\ns\ns.apply(f)\n```\n\n----------------------------------------\n\nTITLE: Querying on DataFrame with Named Index Fallback - Python\nDESCRIPTION: Demonstrates DataFrame.query's ability to use index name as a fallback when a column of the same name does not exist. Requires pandas and numpy. After setting the index name to 'a', a query can select rows based on the index. Inputs: DataFrame with named index; Output: Filtered DataFrame using index in query string.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randint(n / 2, size=(n, 2)), columns=list('bc'))\ndf.index.name = 'a'\ndf\ndf.query('a < b and b < c')\n```\n\n----------------------------------------\n\nTITLE: Writing a pandas DataFrame to Stata Format - Python\nDESCRIPTION: This snippet demonstrates how to create a pandas DataFrame with random data and export it to a Stata .dta file using DataFrame.to_stata. The method generates a file in Stata 12 format (version 115), supporting only specific data types and fixed-width strings up to 244 characters. Prerequisite: pandas (imported as pd) and numpy (imported as np) must be installed. The input is a DataFrame and output is a .dta file saved to disk.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_235\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\ndf.to_stata(\"stata.dta\")\n```\n\n----------------------------------------\n\nTITLE: Series List Accessor Methods\nDESCRIPTION: List accessor methods for Arrow list-dtype Series including flatten, len, and __getitem__\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/series.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSeries.list.flatten\nSeries.list.len\nSeries.list.__getitem__\n```\n\n----------------------------------------\n\nTITLE: Interpolating Missing Values in DataFrames using interpolate (Python)\nDESCRIPTION: Demonstrates interpolating missing data in a DataFrame using interpolate. Supports several methods (default is linear), requires pandas and optionally scipy (for extended methods). Inputs: DataFrame with NaN values. Output: filled/interpolated DataFrame. Useful for time series and continuous variable restoration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': [1, 2.1, np.nan, 4.7, 5.6, 6.8],\n                  'B': [.25, np.nan, np.nan, 4, 12.2, 14.4]})\ndf.interpolate()\n```\n\n----------------------------------------\n\nTITLE: Escaping HTML in DataFrames with the Styler.format Method (Python)\nDESCRIPTION: This snippet applies the Styler.format method with escape='html' to a DataFrame to prevent the rendering of HTML code in its cell contents. Requires pandas and a DataFrame (df4) with HTML strings. The main parameter, escape, determines how contents are sanitized, and the output is a displayed DataFrame with HTML tags treated as plain text, increasing security.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf4.style.format(escape=\"html\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Boolean Series for Conditional Filtering with pandas (Python)\nDESCRIPTION: This snippet evaluates the boolean Series resulting from the condition 'titanic[\"Age\"] > 35', which is used to filter rows. The output is a Series of True/False values, one for each row in the DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Age\"] > 35\n```\n\n----------------------------------------\n\nTITLE: Pivot Table with Multiple Values and Aggregation - pandas - Python\nDESCRIPTION: Performs a pivot_table operation aggregating both 'D' and 'E' columns by 'B' index and ('A', 'C') columns, applying the 'sum' function. Shows how to aggregate multiple fields across multi-level columns and index. Requires pandas, numpy, and appropriate DataFrame setup.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\npd.pivot_table(\n    df, values=[\"D\", \"E\"],\n    index=[\"B\"],\n    columns=[\"A\", \"C\"],\n    aggfunc=\"sum\",\n)\n```\n\n----------------------------------------\n\nTITLE: String Replacement Optimization\nDESCRIPTION: Shows how to optimize HTML output by customizing CSS class names and removing unnecessary HTML elements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmy_css = {\n    \"row_heading\": \"\",\n    \"col_heading\": \"\",\n    \"index_name\": \"\",\n    \"col\": \"c\",\n    \"row\": \"r\",\n    \"col_trim\": \"\",\n    \"row_trim\": \"\",\n    \"level\": \"l\",\n    \"data\": \"\",\n    \"blank\": \"\",\n}\nhtml = Styler(df4, uuid_len=0, cell_ids=False)\nhtml.set_table_styles(\n    [\n        {\"selector\": \"td\", \"props\": props},\n        {\"selector\": \".c1\", \"props\": \"color:green;\"},\n        {\"selector\": \".l0\", \"props\": \"color:blue;\"},\n    ],\n    css_class_names=my_css,\n)\nprint(html.to_html())\n```\n\n----------------------------------------\n\nTITLE: Concatenating Series into DataFrame using pandas.concat - Python\nDESCRIPTION: Demonstrates the concatenation of multiple Series, both named and unnamed, into a single DataFrame with axis=1 using pandas.concat. The result now preserves Series names where available as the column headers. Input is a list of Series, output is a DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npd.concat([foo, bar, baz], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Accessing Series elements by index using loc in Python\nDESCRIPTION: Demonstrates using .loc to access specific elements of a pandas Series by their index labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ns.loc[[1, 2]]\n```\n\n----------------------------------------\n\nTITLE: Sampling Data in Pandas Series\nDESCRIPTION: Demonstrates sampling from a pandas Series with and without replacement, including weight-based sampling and random state control.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([0, 1, 2, 3, 4, 5])\n\n# Without replacement (default):\ns.sample(n=6, replace=False)\n\n# With replacement:\ns.sample(n=6, replace=True)\n```\n\n----------------------------------------\n\nTITLE: Series Serialization Methods\nDESCRIPTION: Methods for converting Series to various formats including pickle, csv, dict, excel, frame, xarray, hdf, sql, json, string, clipboard, latex, and markdown\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/series.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nSeries.to_pickle\nSeries.to_csv\nSeries.to_dict\nSeries.to_excel\nSeries.to_frame\nSeries.to_xarray\nSeries.to_hdf\nSeries.to_sql\nSeries.to_json\nSeries.to_string\nSeries.to_clipboard\nSeries.to_latex\nSeries.to_markdown\n```\n\n----------------------------------------\n\nTITLE: Plotting DataFrame with Custom X and Y Columns (Python)\nDESCRIPTION: Constructs a DataFrame with two randomly generated columns and adds a third as a range. Demonstrates how to plot one column versus another using DataFrame.plot(x=..., y=...). The result is a line plot of column \"B\" versus \"A\". Dependencies are pandas, numpy, and matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf3 = pd.DataFrame(np.random.randn(1000, 2), columns=[\"B\", \"C\"]).cumsum()\\ndf3[\"A\"] = pd.Series(list(range(len(df))))\\n\\n@savefig df_plot_xy.png\\ndf3.plot(x=\"A\", y=\"B\");\n```\n\n----------------------------------------\n\nTITLE: Calculating the Ratio Between DataFrame Columns in pandas (Python)\nDESCRIPTION: This snippet adds a new column called 'ratio_paris_antwerp' to the 'air_quality' DataFrame, computing the element-wise ratio of the 'station_paris' column to the 'station_antwerp' column. This operation is used to compare measured NO2 concentrations between two stations. The only dependencies are pandas and the presence of input columns. Expected inputs are numeric columns for 'station_paris' and 'station_antwerp'; the output is a new column with their ratios for each row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/05_add_columns.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nair_quality[\"ratio_paris_antwerp\"] = (\n    air_quality[\"station_paris\"] / air_quality[\"station_antwerp\"]\n)\nair_quality.head()\n```\n\n----------------------------------------\n\nTITLE: Inserting Missing Data by Assignment in pandas (Python)\nDESCRIPTION: These snippets demonstrate how to assign missing values (None, np.nan) into Series of different dtypes, and show the resulting propagation and representation of missing data. Both labeled and positional indexing are illustrated. Dependencies are pandas and numpy. The output reflects the dtype-specific sentinel for missing values (e.g., np.nan, pd.NaT, pd.NA, or direct None for object).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1., 2., 3.])\nser.loc[0] = None\nser\n```\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([pd.Timestamp(\"2021\"), pd.Timestamp(\"2021\")])\nser.iloc[0] = np.nan\nser\n```\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([True, False], dtype=\"boolean[pyarrow]\")\nser.iloc[0] = None\nser\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"b\", \"c\"], dtype=object)\ns.loc[0] = None\ns.loc[1] = np.nan\ns\n```\n\n----------------------------------------\n\nTITLE: Boxplot with GroupBy Operation\nDESCRIPTION: This example creates boxplots using the GroupBy.boxplot() method. It creates a dataset with two numeric columns and a grouping variable, then plots boxplots grouped by that variable.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(1234)\ndf_box = pd.DataFrame(np.random.randn(50, 2))\ndf_box[\"g\"] = np.random.choice([\"A\", \"B\"], size=50)\ndf_box.loc[df_box[\"g\"] == \"B\", 1] += 3\n\nbp = df_box.boxplot(by=\"g\")\n```\n\n----------------------------------------\n\nTITLE: Using Element-plus-Class Selectors for Higher CSS Specificity in pandas Styling (Python)\nDESCRIPTION: This snippet demonstrates increasing CSS specificity with a combination of element and class in set_table_styles. It assigns the 'cls-1' class to a cell and adds a 'td.data' selector to boost specificity in the CSS. Requires pandas, and shows how specificity scoring alters which style takes effect. The main inputs are the DataFrame and the class assignment, with output in yellow style if the specificity rules dictate.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndf4.style.set_uuid(\"c_\").set_table_styles(\n    [\n        {\"selector\": \"td\", \"props\": \"color:red;\"},\n        {\"selector\": \".cls-1\", \"props\": \"color:blue;\"},\n        {\"selector\": \"td.data\", \"props\": \"color:yellow;\"},\n    ]\n).map(lambda x: \"color:green;\").set_td_classes(pd.DataFrame([[\"cls-1\"]]))\n```\n\n----------------------------------------\n\nTITLE: Initializing StringDtype Series in pandas (Python)\nDESCRIPTION: Shows explicit creation of a pandas Series with StringDtype by specifying dtype, either as the string 'string' or via pd.StringDtype(). Requires pandas; demonstrates conversion of lists to Series for text processing where the result will be a Series of string dtype. This allows for more predictable string operations and type selection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npd.Series([\"a\", \"b\", \"c\"], dtype=\"string\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([\"a\", \"b\", \"c\"], dtype=pd.StringDtype())\n```\n\n----------------------------------------\n\nTITLE: Comparing DataFrame Equality in Python with Pandas\nDESCRIPTION: This snippet shows how to compare two DataFrames for equality using the 'equals' method. It demonstrates that direct comparison with '==' may not work as expected due to NaN values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf + df == df * 2\n(df + df == df * 2).all()\n(df + df).equals(df * 2)\n```\n\n----------------------------------------\n\nTITLE: Table Styles Implementation - Sub-optimal vs Optimized\nDESCRIPTION: Shows how to efficiently apply styles using table-wide CSS rather than cell-by-cell styling.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprops = 'font-family: \"Times New Roman\", Times, serif; color: #e83e8c; font-size:1.3em;'\ndf4.style.map(lambda x: props, subset=[1])\n```\n\nLANGUAGE: python\nCODE:\n```\ndf4.style.set_table_styles([{\"selector\": \"td.col1\", \"props\": props}])\n```\n\n----------------------------------------\n\nTITLE: Selecting Data by Position with .iloc and .iat - Python\nDESCRIPTION: These code examples demonstrate integer-based selection using .iloc (full slices, lists, or single positions) and .iat (single value fast access) to select data by row and column number instead of label. Useful for efficient position-based operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf.iloc[3]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.iloc[3:5, 0:2]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.iloc[[1, 2, 4], [0, 2]]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.iloc[1:3, :]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.iloc[:, 1:3]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.iloc[1, 1]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.iat[1, 1]\n```\n\n----------------------------------------\n\nTITLE: Creating MultiIndex Series and DataFrame in Python\nDESCRIPTION: Demonstrates creating Series and DataFrame objects with MultiIndex using dictionary of tuples.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\npd.Series({('a', 'b'): 1, ('a', 'a'): 0, \n         ('a', 'c'): 2, ('b', 'a'): 3, ('b', 'b'): 4})\n\npd.DataFrame({('a', 'b'): {('A', 'B'): 1, ('A', 'C'): 2},\n              ('a', 'a'): {('A', 'C'): 3, ('A', 'B'): 4},\n              ('a', 'c'): {('A', 'B'): 5, ('A', 'C'): 6},\n              ('b', 'a'): {('A', 'C'): 7, ('A', 'B'): 8},\n              ('b', 'b'): {('A', 'D'): 9, ('A', 'B'): 10}})\n```\n\n----------------------------------------\n\nTITLE: Assigning Array to DataFrame via iloc (New/Future Behavior) - pandas, numpy (Python)\nDESCRIPTION: Shows the future behavior where assigning a NumPy array to a DataFrame column through iloc updates both the DataFrame and any original Series references, ensuring inplace modification. After assignment, both df and original_prices reflect the updated, float-converted values. Dependencies: pandas, numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: df.iloc[:, 0] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]:\nbook1    98.0\nbook2    99.0\nName: price, dtype: float64\nIn [5]: original_prices\nOut[5]:\nbook1    98.0\nbook2    99.0\nName: price, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Serializing DataFrame with ISO Date Formatting to JSON - pandas - Python\nDESCRIPTION: Shows how to add a date column to a DataFrame, sort columns, and then serialize using to_json with ISO8601 date formatting. pandas and NumPy are required. Key parameters: date_format set to \"iso\". Input: DataFrame with dates. Output: JSON string with dates in ISO format.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\ndfd = pd.DataFrame(np.random.randn(5, 2), columns=list(\"AB\"))\ndfd[\"date\"] = pd.Timestamp(\"20130101\")\ndfd = dfd.sort_index(axis=1, ascending=False)\njson = dfd.to_json(date_format=\"iso\")\njson\n```\n\n----------------------------------------\n\nTITLE: Using pyarrow as dtype_backend for SQL data type preservation\nDESCRIPTION: Demonstrates reading from a SQL database with Arrow type system preservation by using the dtype_backend=\"pyarrow\" parameter. This helps maintain database-specific types that might be lost in the pandas/NumPy type system.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_226\n\nLANGUAGE: python\nCODE:\n```\n# for roundtripping\nwith pg_dbapi.connect(uri) as conn:\n    df2 = pd.read_sql(\"pandas_table\", conn, dtype_backend=\"pyarrow\")\n```\n\n----------------------------------------\n\nTITLE: Importing pandas and Matplotlib in Python\nDESCRIPTION: This snippet demonstrates how to import the core libraries, pandas and Matplotlib, which are prerequisites for all subsequent plotting functionality. Importing pandas as pd and matplotlib.pyplot as plt is standard in data science workflows for analysis and visualization. No parameters or inputs are needed; this just establishes the namespace for later use.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Using Named Arguments with DataFrame.rename and reindex in Python\nDESCRIPTION: Shows how to use the traditional \"index, columns\" style with named arguments for rename and reindex methods, which continues to work as before.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.rename(index=id, columns=str.lower)\ndf.reindex(index=[0, 1, 3], columns=['A', 'B', 'C'])\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table from StringIO in Python\nDESCRIPTION: Shows how to read HTML content from a StringIO object using pandas.read_html(). This is useful when the HTML content is already in memory rather than in a file or URL.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_91\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(StringIO(html_str))\ndfs[0]\n```\n\n----------------------------------------\n\nTITLE: Setting pandas Plotting Backend via Global Option in Python\nDESCRIPTION: This snippet illustrates how to set the plotting backend globally using set_option or the options attribute in pandas. Afterwards, all plot calls use the desired backend until the option is changed. Dependencies: pandas and a compatible backend. Inputs: none; outputs: affects plotting behavior globally. Limitation: affects all subsequent plots until reset.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\n>>> pd.set_option(\"plotting.backend\", \"backend.module\")\n>>> pd.Series([1, 2, 3]).plot()\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> pd.options.plotting.backend = \"backend.module\"\n>>> pd.Series([1, 2, 3]).plot()\n```\n\n----------------------------------------\n\nTITLE: Handling NaN values in string columns with Python engine\nDESCRIPTION: Demonstrates the new behavior in read_csv and read_excel where NaN values in string columns are properly converted to np.nan instead of the string 'nan'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndata = 'a,b,c\\n1,,3\\n4,5,6'\ndf = pd.read_csv(StringIO(data), engine='python', dtype=str, na_filter=True)\ndf.loc[0, 'b']\n```\n\n----------------------------------------\n\nTITLE: Strict Comparison Operators for pandas Series and DataFrame - Python\nDESCRIPTION: Illustrates strict behavior of comparison operators for Series and DataFrames: a ValueError is now raised if indexes do not match, rather than comparing by position. Requires pandas. Input: two Series/DataFrames with non-identical indexes. Output: raises ValueError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ns1.values == s2.values\n```\n\nLANGUAGE: python\nCODE:\n```\ns1.eq(s2)\n```\n\n----------------------------------------\n\nTITLE: New Behavior: DataFrame.mean with Mixed Dtypes and numeric_only=None (ipython)\nDESCRIPTION: This snippet portrays new semantics: DataFrame.mean now properly outputs mean values for all numeric columns, even when run on the entire DataFrame, ensuring users do not have to manually subset numeric columns. Output is a Series with means for numeric columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: df.mean()\nOut[3]:\nA    1.0\ndtype: float64\n\nIn [4]: df[[\"A\"]].mean()\nOut[4]:\nA    1.0\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Selecting Rows by Condition Using .loc and Boolean Indexing - pandas - Python\nDESCRIPTION: This example shows the recommended way to select DataFrame rows by condition using df.loc and index.map, replacing the deprecated select method. The lambda function tests if the index label is in ['bar', 'baz'], returning the filtered DataFrame. Inputs are the DataFrame and a boolean indexer; no special limitations, but the code requires pandas and an existing DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[df.index.map(lambda x: x in ['bar', 'baz'])]\n```\n\n----------------------------------------\n\nTITLE: Filtering Groups in Series and DataFrames with groupby.filter - Python\nDESCRIPTION: These examples demonstrate using groupby().filter to return subsets of a Series or DataFrame where the group satisfies a predicate, such as group sum or group size. Also shown is the use of dropna=False to retain group membership with NaN. Dependencies: pandas, numpy. Inputs include group key or lambda function; output is filtered data. Applied to both Series and DataFrames to illustrate flexibility.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsf = pd.Series([1, 1, 2, 3, 3, 3])\nsf.groupby(sf).filter(lambda x: x.sum() > 2)\n```\n\nLANGUAGE: python\nCODE:\n```\ndff = pd.DataFrame({\"A\": np.arange(8), \"B\": list(\"aabbbbcc\")})\ndff.groupby(\"B\").filter(lambda x: len(x) > 2)\n```\n\nLANGUAGE: python\nCODE:\n```\ndff.groupby(\"B\").filter(lambda x: len(x) > 2, dropna=False)\n```\n\n----------------------------------------\n\nTITLE: Combining set_uuid and set_table_styles with map in pandas DataFrame Styling (Python)\nDESCRIPTION: This example explains how setting a unique uuid on the Styler, defining table-wide styles with set_table_styles, and applying a cell-specific style with map interact via CSS specificity. It requires pandas and demonstrates the interaction of ID and element selectors in CSS specificity calculation. The function showcases that a rule with both ID and element is more specific, even if the map is applied later.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndf4.style.set_uuid(\"a_\").set_table_styles(\n    [{\"selector\": \"td\", \"props\": \"color:red;\"}]\n).map(lambda x: \"color:green;\")\n```\n\n----------------------------------------\n\nTITLE: Using Rolling Window Operations with GroupBy in Pandas\nDESCRIPTION: Demonstrates the new syntax for applying rolling window operations to grouped data in Pandas, simplifying the process compared to previous versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1] * 20 + [2] * 12 + [3] * 8, \"B\": np.arange(40)})\ndf\n\ndf.groupby(\"A\").rolling(4).B.mean()\n```\n\n----------------------------------------\n\nTITLE: Examining MultiIndex Levels in Python\nDESCRIPTION: Shows how MultiIndex keeps all defined levels even when slicing, and demonstrates methods to view only the used levels with get_level_values or remove_unused_levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.columns.levels  # original MultiIndex\n\ndf[[\"foo\",\"qux\"]].columns.levels  # sliced\n```\n\n----------------------------------------\n\nTITLE: Getting and Setting Display Options with pandas API - Python\nDESCRIPTION: Demonstrates how to retrieve and modify the global setting for the maximum number of rows displayed in pandas DataFrames using the options attribute. Requires the pandas library. The parameter \\\"display.max_rows\\\" controls DataFrame display height, and can be accessed and updated directly; changes affect subsequent DataFrame display operations. No inputs other than setting the value; output is the current maximum row display value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\npd.options.display.max_rows\npd.options.display.max_rows = 999\npd.options.display.max_rows\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Head with Pandas in Python\nDESCRIPTION: This code snippet uses the DataFrame.head method from pandas to display the first 5 rows of the 'tips' DataFrame. Requires the pandas library and an already initialized DataFrame named 'tips' in the current Python environment. Accepts an optional integer argument to specify the number of top rows to return (default is 5). Outputs the resulting subset as a reduced DataFrame, truncated if the original DataFrame is large. Applicable for quickly inspecting the structure or contents of datasets.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/limit.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips.head(5)\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns by Name in Excel Read Operation\nDESCRIPTION: Demonstrates selecting specific columns by their names when reading an Excel file. Names can be provided by the user or inferred from the header row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_136\n\nLANGUAGE: python\nCODE:\n```\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[\"foo\", \"bar\"])\n```\n\n----------------------------------------\n\nTITLE: Adding Absolute DateOffsets to Timedelta Series (Python)\nDESCRIPTION: Demonstrates using pandas' offsets module to add Minute and Milli offsets to a timedelta Series, treating DateOffsets as equivalent to timedeltas. Dependencies: pandas. Inputs: 'td', pandas.offsets. Outputs: Timedelta arithmetic with offsets. Useful for time-based augmentation and feature engineering.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas import offsets\ntd + offsets.Minute(5) + offsets.Milli(5)\n```\n\n----------------------------------------\n\nTITLE: DataFrame Multi-axis Selection with .loc\nDESCRIPTION: Shows how to select specific rows and columns from a DataFrame using .loc accessor with label-based indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.arange(25).reshape(5, 5), index=list(\"abcde\"), columns=list(\"abcde\"))\ndf.loc[[\"a\", \"c\", \"e\"], [\"b\", \"d\"]]\n```\n\n----------------------------------------\n\nTITLE: Get First Day of Month for Each DatetimeIndex Entry - Pandas Python\nDESCRIPTION: Calculates the first day of the month for each entry in a DatetimeIndex, converting datetime values to monthly periods and back to timestamps. Dependencies: pandas. The input is a DatetimeIndex; the output is an Index of Timestamp values for the first day of each month. Useful for grouping or resampling data by month.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndates = pd.date_range(\"2000-01-01\", periods=5)\ndates.to_period(freq=\"M\").to_timestamp()\n```\n\n----------------------------------------\n\nTITLE: Indexing DatetimeIndex with datetime.date in Python\nDESCRIPTION: Fixed regression when indexing a Series or DataFrame indexed by DatetimeIndex with a slice containing a datetime.date object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nseries_or_df[datetime.date(2020, 1, 1):]\n```\n\n----------------------------------------\n\nTITLE: Styler Functional Formatting and Gradient Highlighting - pandas - Python\nDESCRIPTION: Shows advanced display formatting of a weather DataFrame using Styler. Demonstrates functional value formatting, format_index with date formatting, applying a background color gradient, and setting a caption. Dependencies: pandas, numpy. Core parameters include a display mapping function (rain_condition), column/index formatters, gradients with vmin/vmax, and the caption string. The styled DataFrame, when rendered, applies all styles for enhanced visual summary; works best in Jupyter/HTML. Inputs: random float values, pandas date index; Outputs: formatted and styled DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nweather_df = pd.DataFrame(\n    np.random.default_rng().random((10, 2)) * 5,\n    index=pd.date_range(start=\"2021-01-01\", periods=10),\n    columns=[\"Tokyo\", \"Beijing\"],\n)\n\n\ndef rain_condition(v):\n    if v < 1.75:\n        return \"Dry\"\n    elif v < 2.75:\n        return \"Rain\"\n    return \"Heavy Rain\"\n\n\ndef make_pretty(styler):\n    styler.set_caption(\"Weather Conditions\")\n    styler.format(rain_condition)\n    styler.format_index(lambda v: v.strftime(\"%A\"))\n    styler.background_gradient(axis=None, vmin=1, vmax=5, cmap=\"YlGnBu\")\n    return styler\n\n\nweather_df\n```\n\n----------------------------------------\n\nTITLE: Maintaining Shape-Based Addition by Converting DataFrame to NumPy Array - Python\nDESCRIPTION: Illustrates how to achieve shape-based addition using np.add by converting one pandas DataFrame to a NumPy array. This ensures indices are ignored and values are aligned by position. Requires pandas and NumPy imported as pd and np. Inputs: DataFrames df1 and df2, with df2 converted to a NumPy array via np.asarray(df2); output is a DataFrame whose values are summed positionally.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nIn [7]: np.add(df1, np.asarray(df2))\nOut[7]:\n   a  b\n0  2  6\n1  4  8\n```\n\n----------------------------------------\n\nTITLE: get_dummies New Behavior Data Types - Pandas - Python\nDESCRIPTION: Demonstrates the new behavior after the enhancement where pd.get_dummies produces columns of integer dtype, reducing memory usage. Only pandas is needed. The input is a list of categorical values and the output will show Series of integer dtypes for each dummy variable.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\npd.get_dummies([\"a\", \"b\", \"a\", \"c\"]).dtypes\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns in DataFrameGroupBy Objects with pandas (ipython)\nDESCRIPTION: This snippet demonstrates both deprecated and recommended practices for selecting columns from a pandas DataFrameGroupBy object. It constructs a DataFrame, performs a groupby operation, and then illustrates various selection styles: single key (SeriesGroupBy), single-key tuple, multi-key tuple (deprecated), direct multi-key (deprecated), and the preferred list of keys (returns DataFrameGroupBy). Dependencies: pandas and NumPy must be imported. Key parameters include the groupby key ('A') and selection keys ('B', 'C'). Expected input is a DataFrame; outputs are groupby objects. Deprecated forms now raise warnings and may not be supported in future releases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_29\n\nLANGUAGE: ipython\nCODE:\n```\ndf = pd.DataFrame({\n    \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n    \"B\": np.random.randn(8),\n    \"C\": np.random.randn(8),\n})\ng = df.groupby('A')\n\n# single key, returns SeriesGroupBy\ng['B']\n\n# tuple of single key, returns SeriesGroupBy\ng[('B',)]\n\n# tuple of multiple keys, returns DataFrameGroupBy, raises FutureWarning\ng[('B', 'C')]\n\n# multiple keys passed directly, returns DataFrameGroupBy, raises FutureWarning\n# (implicitly converts the passed strings into a single tuple)\ng['B', 'C']\n\n# proper way, returns DataFrameGroupBy\ng[['B', 'C']]\n\n```\n\n----------------------------------------\n\nTITLE: Advanced SQLAlchemy Querying with pandas.read_sql (Python)\nDESCRIPTION: Demonstrates advanced query constructs using SQLAlchemy within pandas.read_sql() calls. Examples include parameterized queries with sqlalchemy.text, creating SQL table metadata, using SQLAlchemy expressions in selects/filters, and binding external parameters. Requires SQLAlchemy and pandas with an active engine. Parameters shown include named placeholders and dynamic date filtering. Outputs DataFrames from complex SQLAlchemy-based queries.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_233\n\nLANGUAGE: python\nCODE:\n```\nimport sqlalchemy as sa\n\npd.read_sql(\n    sa.text(\"SELECT * FROM data where Col_1=:col1\"), engine, params={\"col1\": \"X\"}\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nmetadata = sa.MetaData()\ndata_table = sa.Table(\n    \"data\",\n    metadata,\n    sa.Column(\"index\", sa.Integer),\n    sa.Column(\"Date\", sa.DateTime),\n    sa.Column(\"Col_1\", sa.String),\n    sa.Column(\"Col_2\", sa.Float),\n    sa.Column(\"Col_3\", sa.Boolean),\n)\n\npd.read_sql(sa.select(data_table).where(data_table.c.Col_3 is True), engine)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport datetime as dt\n\nexpr = sa.select(data_table).where(data_table.c.Date > sa.bindparam(\"date\"))\npd.read_sql(expr, engine, params={\"date\": dt.datetime(2010, 10, 18)})\n```\n\n----------------------------------------\n\nTITLE: Constructing DataFrame from List of Namedtuples - Pandas - Python\nDESCRIPTION: Shows how to construct a DataFrame from a list of namedtuples, where field names determine DataFrame columns. Describes handling of missing and extra tuple fields: shorter tuples result in NaN for missing values, longer ones raise ValueError. Requires pandas as pd and Python's collections.namedtuple.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import namedtuple\n\nPoint = namedtuple(\"Point\", \"x y\")\n\npd.DataFrame([Point(0, 0), Point(0, 3), (2, 3)])\n\nPoint3D = namedtuple(\"Point3D\", \"x y z\")\n\npd.DataFrame([Point3D(0, 0, 0), Point3D(0, 3, 5), Point(2, 3)])\n```\n\n----------------------------------------\n\nTITLE: Arithmetic with MultiIndex Broadcasting in Python\nDESCRIPTION: Performs arithmetic operations on a MultiIndex DataFrame with proper broadcasting. This example divides values across the DataFrame by corresponding values in a specific column, using level-based alignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ncols = pd.MultiIndex.from_tuples(\n    [(x, y) for x in [\"A\", \"B\", \"C\"] for y in [\"O\", \"I\"]]\n)\ndf = pd.DataFrame(np.random.randn(2, 6), index=[\"n\", \"m\"], columns=cols)\ndf\ndf = df.div(df[\"C\"], level=1)\ndf\n```\n\n----------------------------------------\n\nTITLE: Applying Exported Styler to Another DataFrame in Pandas (Python)\nDESCRIPTION: This example uses a previously exported style object and applies it to a new DataFrame using Styler's export and use methods. Dependencies are pandas and the pre-existing style1. The intended effect is to propagate format and style mappings, ensuring consistent presentation across multiple DataFrames. The process handles data-aware styles and gracefully re-evaluates them for the new DataFrame context.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nstyle2 = df3.style\nstyle2.use(style1.export())\nstyle2\n```\n\n----------------------------------------\n\nTITLE: Parsing timezone-aware dates from different timezones with to_datetime\nDESCRIPTION: Example of using to_datetime with the utc parameter to parse dates with different timezone offsets and convert them all to UTC. This returns a DatetimeIndex with timezone at UTC.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntz_strs = [\"2010-01-01 12:00:00 +0100\", \"2010-01-01 12:00:00 -0100\",\n           \"2010-01-01 12:00:00 +0300\", \"2010-01-01 12:00:00 +0400\"]\npd.to_datetime(tz_strs, format='%Y-%m-%d %H:%M:%S %z', utc=True)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrames with Explicit dtypes and Inspecting Schema - Python\nDESCRIPTION: Creates DataFrames with specified column dtypes using dtype parameters and Series initialization, then displays each DataFrame and inspects their dtypes. Illustrates coexistence of different (and non-combined) numeric types. Dependencies include pandas and NumPy. Key parameters are dtype specifications. Outputs confirm pandas' retention of explicit types per column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_76\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(np.random.randn(8, 1), columns=[\"A\"], dtype=\"float64\")\ndf1\ndf1.dtypes\ndf2 = pd.DataFrame(\n    {\n        \"A\": pd.Series(np.random.randn(8), dtype=\"float32\"),\n        \"B\": pd.Series(np.random.randn(8)),\n        \"C\": pd.Series(np.random.randint(0, 255, size=8), dtype=\"uint8\"),  # [0,255] (range of uint8)\n    }\n)\ndf2\ndf2.dtypes\n```\n\n----------------------------------------\n\nTITLE: Extracting Multiple Regex Groups from Series Strings with DataFrame Output (Python)\nDESCRIPTION: Demonstrates extracting two groups from strings in a Series using a regex with parentheses, returning a two-column DataFrame. Dependencies: pandas. Inputs: Series of strings. Output: DataFrame of groups; NaN where no match. Enables advanced data cleaning and feature extraction from heterogeneous string columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\npd.Series(['a1', 'b2', 'c3']).str.extract('([ab])(\\d)')\n```\n\n----------------------------------------\n\nTITLE: Inspecting the dtype of a SparseArray - pandas - Python\nDESCRIPTION: Retrieves the dtype property of a SparseArray, which is typically a SparseDtype that encodes both base type and the fill value. Useful for introspection and verification of sparse object characteristics. Dependency: pandas. The output is a dtype object summarizing both stored value type and fill value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsparr.dtype\n```\n\n----------------------------------------\n\nTITLE: Accessing Struct Field in Series with struct.field - pandas Python\nDESCRIPTION: This snippet illustrates how to access an individual field of a struct-typed pandas Series using the struct.field accessor. It requires a Series with a pyarrow struct dtype, and returns a new Series containing the values of the specified field for each row. The accessor simplifies extracting nested fields without manual iteration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nseries.struct.field(\"project\")\n```\n\n----------------------------------------\n\nTITLE: Reading Multiple Excel Sheets and Returning Dictionary of DataFrames with pandas.read_excel (Python)\nDESCRIPTION: Explains how to read selected sheets from an Excel file by passing a list to the sheetname argument in read_excel. Dependencies: pandas. Inputs: file path string and a list of sheet names or indices. Output: dictionary mapping sheet names to DataFrames for each requested sheet. This supports efficient multi-sheet data import for batch processing or comparative analysis. Limitation: File must exist and be readable by pandas Excel engines.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Returns the 1st and 4th sheet, as a dictionary of DataFrames.\npd.read_excel('path_to_file.xls', sheetname=['Sheet1', 3])\n```\n\n----------------------------------------\n\nTITLE: Setting Columns in pandas DataFrame in Python Without Spurious Warnings\nDESCRIPTION: This snippet exhibits a DataFrame column-setting operation that formerly generated a SettingWithCopy warning, now corrected in this release. It showcases creating two DataFrames from the same source, then adding a new column to the second one. It depends on pandas and proper DataFrame/Series construction. The inputs are lists used to create Series and DataFrames, and the operation sets a new column by assignment. The key effect is that such assignments no longer incorrectly warn the user. Ensure pandas is imported as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame({'x': pd.Series(['a', 'b', 'c']),\n                     'y': pd.Series(['d', 'e', 'f'])})\ndf2 = df1[['x']]\ndf2['y'] = ['g', 'h', 'i']\n```\n\n----------------------------------------\n\nTITLE: Handling Infinity in Null Operations and Filling Values in pandas - ipython\nDESCRIPTION: Demonstrates the updated behavior regarding infinity and negative infinity when using pd.isnull() and filling missing values with fillna(). The snippet shows how the use_inf_as_null option changes handling of inf values, affecting detection of NA and fill operations. Useful for ensuring correct missing data treatment in calculations, especially after behavior changes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_2\n\nLANGUAGE: ipython\nCODE:\n```\nIn [6]: s = pd.Series([1.5, np.inf, 3.4, -np.inf])\n\nIn [7]: pd.isnull(s)\nOut[7]:\n0    False\n1    False\n2    False\n3    False\nLength: 4, dtype: bool\n\nIn [8]: s.fillna(0)\nOut[8]:\n0    1.500000\n1         inf\n2    3.400000\n3        -inf\nLength: 4, dtype: float64\n\nIn [9]: pd.set_option('use_inf_as_null', True)\n\nIn [10]: pd.isnull(s)\nOut[10]:\n0    False\n1     True\n2    False\n3     True\nLength: 4, dtype: bool\n\nIn [11]: s.fillna(0)\nOut[11]:\n0    1.5\n1    0.0\n2    3.4\n3    0.0\nLength: 4, dtype: float64\n\nIn [12]: pd.reset_option('use_inf_as_null')\n```\n\n----------------------------------------\n\nTITLE: Serializing DataFrame to JSON with Microsecond Date Precision - pandas - Python\nDESCRIPTION: Serializes a DataFrame (prepared with date columns) to JSON using ISO date formatting with microseconds as the precision unit. The date_unit parameter is set to \"us\". Requires previous DataFrame setup (dfd), pandas, and NumPy. Input: DataFrame. Output: JSON string with date fields formatted to microsecond precision.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_70\n\nLANGUAGE: python\nCODE:\n```\njson = dfd.to_json(date_format=\"iso\", date_unit=\"us\")\njson\n```\n\n----------------------------------------\n\nTITLE: IntegerArray with pandas.NA and Effects on ndarray Conversion (Python, pandas 1.0.0)\nDESCRIPTION: Demonstrates use of pandas.NA as the missing value in IntegerArray and shows that converting such an array to a numpy float array raises an error. Shows the correct way to explicitly set the na_value during conversion. Requires numpy as np and pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\na = pd.array([1, 2, None], dtype=\"Int64\")\na\na[2]\n```\n\nLANGUAGE: python\nCODE:\n```\nnp.asarray(a, dtype=\"float\")\n```\n\nLANGUAGE: python\nCODE:\n```\na.to_numpy(dtype=\"float\", na_value=np.nan)\n```\n\n----------------------------------------\n\nTITLE: Constructing Categorical from Codes and Categories in Pandas (ipython)\nDESCRIPTION: This ipython cell shows the preferred post-0.15.0 method for creating a Categorical from integer codes and explicit categories using 'from_codes'. Dependencies: pandas. Inputs: code list, categories. Output: Categorical array. Ensures forward compatibility with newer Pandas versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_19\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: pd.Categorical.from_codes([0,1,0,2,1], categories=['a', 'b', 'c'])\nOut[2]:\n[a, b, a, c, b]\nCategories (3, object): [a, b, c]\n```\n\n----------------------------------------\n\nTITLE: Setting DataFrame Values with Slice in Python\nDESCRIPTION: Fixed regression in DataFrame where setting values with a slice (e.g. df[-4:] = 1) was indexing by label instead of position.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndf[-4:] = 1\n```\n\n----------------------------------------\n\nTITLE: Constructing MultiIndex DataFrame - pandas - Python\nDESCRIPTION: Builds a MultiIndex using combinations from labeling function 'mklbl' to create levels for DataFrame rows and columns, and fills the DataFrame with consecutive integers. Demonstrates hierarchical DataFrame setup, requiring pandas and numpy (as pd, np) and creation of dense MultiIndex. No parameters beyond the number of levels and library imports; output is a sample DataFrame sorted along both axes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef mklbl(prefix, n):\n    return [\"%s%s\" % (prefix, i) for i in range(n)]\n\nmiindex = pd.MultiIndex.from_product(\n    [mklbl(\"A\", 4), mklbl(\"B\", 2), mklbl(\"C\", 4), mklbl(\"D\", 2)]\n)\nmicolumns = pd.MultiIndex.from_tuples(\n    [(\"a\", \"foo\"), (\"a\", \"bar\"), (\"b\", \"foo\"), (\"b\", \"bah\")], names=[\"lvl0\", \"lvl1\"]\n)\ndfmi = (\n    pd.DataFrame(\n        np.arange(len(miindex) * len(micolumns)).reshape(\n            (len(miindex), len(micolumns))\n        ),\n        index=miindex,\n        columns=micolumns,\n    )\n    .sort_index()\n    .sort_index(axis=1)\n)\ndfmi\n```\n\n----------------------------------------\n\nTITLE: Creating Ordered Categorical Series in Pandas\nDESCRIPTION: Demonstrates the new behavior for creating ordered Categorical Series. The 'ordered' parameter now defaults to False and must be explicitly set to True if ordering is desired.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([0, 1, 2], dtype='category')\ns\ns.cat.ordered\ns = s.cat.as_ordered()\ns\ns.cat.ordered\n\n# you can set in the constructor of the Categorical\ns = pd.Series(pd.Categorical([0, 1, 2], ordered=True))\ns\ns.cat.ordered\n```\n\n----------------------------------------\n\nTITLE: Applying Weighted and Standard Rolling Windows - pandas (Scipy Dependent) - Python\nDESCRIPTION: This snippet shows normal and weighted rolling window computations using .rolling with the win_type parameter. It demonstrates standard mean, triangular-weighted mean, and Gaussian-weighted mean windows, passing supplementary parameters (std) where required. Dependencies: pandas, scipy >=1.4 for signal.windows, numpy. Inputs: Series, window size, (optionally) window type and its arguments (e.g., std for gaussian); Outputs: Series with rolling means. Weighted windows require scipy to be installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(range(10))\ns.rolling(window=5).mean()\ns.rolling(window=5, win_type=\"triang\").mean()\n# Supplementary Scipy arguments passed in the aggregation function\ns.rolling(window=5, win_type=\"gaussian\").mean(std=0.1)\n```\n\n----------------------------------------\n\nTITLE: Customizing Pie Charts with Series in pandas\nDESCRIPTION: Demonstrates how to customize a pie chart using a pandas Series with specific labels, colors, autopct (percentage format), fontsize, and figsize parameters. This snippet shows how to control the visual appearance of pie chart elements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nseries.plot.pie(\n    labels=[\"AA\", \"BB\", \"CC\", \"DD\"],\n    colors=[\"r\", \"g\", \"b\", \"c\"],\n    autopct=\"%.2f\",\n    fontsize=20,\n    figsize=(6, 6),\n);\n```\n\n----------------------------------------\n\nTITLE: Normalizing semi-structured JSON data to flat tables using pandas.json_normalize - Python\nDESCRIPTION: This example shows how to use pandas' json_normalize function to convert a list of dictionaries containing nested structures into a flat DataFrame. Helpful for transforming semi-structured or deeply nested JSON into tabular format. Requires pandas. Input is a list of dicts, some of which contain nested dicts; output is a flat DataFrame where keys are normalized into columns. Handles missing keys flexibly.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_80\n\nLANGUAGE: python\nCODE:\n```\ndata = [\n    {\"id\": 1, \"name\": {\"first\": \"Coleen\", \"last\": \"Volk\"}},\n    {\"name\": {\"given\": \"Mark\", \"family\": \"Regner\"}},\n    {\"id\": 2, \"name\": \"Faye Raker\"},\n]\npd.json_normalize(data)\n```\n\n----------------------------------------\n\nTITLE: Transforming DataFrames with Broadcasting Functions Using pandas - Python\nDESCRIPTION: Demonstrates the .transform() method on DataFrames with a combination of absolute value and a lambda to subtract the minimum of each column. The result is a broadcasted DataFrame with the specified transformations. Requires pandas and a numeric DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.transform(['abs', lambda x: x - x.min()])\n```\n\n----------------------------------------\n\nTITLE: Handling Mixed Data Types in CSV Files with Pandas\nDESCRIPTION: Demonstrates how pandas can end up with mixed data types in columns when reading CSV files, resulting in an overall object dtype for the column despite containing both integers and strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncol_1 = list(range(500000)) + [\"a\", \"b\"] + list(range(500000))\ndf = pd.DataFrame({\"col_1\": col_1})\ndf.to_csv(\"foo.csv\")\nmixed_df = pd.read_csv(\"foo.csv\")\nmixed_df[\"col_1\"].apply(type).value_counts()\nmixed_df[\"col_1\"].dtype\n```\n\n----------------------------------------\n\nTITLE: Evaluating Math Functions in DataFrame.eval with pandas - Python\nDESCRIPTION: Shows how pandas.eval enables math functions directly in the expression string. Demonstrates creation of a DataFrame with random numbers, then uses eval to create a new column computed by applying the sin function. Requires pandas and numpy; input is a DataFrame and expression string, output modifies or expands the DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": np.random.randn(10)})\ndf.eval(\"b = sin(a)\")\n```\n\n----------------------------------------\n\nTITLE: Concatenating DataFrames with Inner Join in Python\nDESCRIPTION: Example of concatenating DataFrames with the 'inner' join option that takes only the intersection of index values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.concat([df1, df4], axis=1, join=\"inner\")\nresult\n```\n\n----------------------------------------\n\nTITLE: Applying GroupBy and Custom Functions with Pandas - Python\nDESCRIPTION: Demonstrates grouping a DataFrame by the 'animal' column, then applying a custom lambda function to each group to list the size of the animal with the highest weight. Inputs are multiple columns with categorical and numeric data. Requires pandas and a DataFrame structured as shown. Outputs the largest-sized animal per group. Uses groupby.apply for maximum flexibility.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"animal\": \"cat dog cat fish dog cat cat\".split(),\n        \"size\": list(\"SSMMMLL\"),\n        \"weight\": [8, 10, 11, 1, 20, 12, 12],\n        \"adult\": [False] * 5 + [True] * 2,\n    }\n)\ndf\n\n# List the size of the animals with the highest weight.\ndf.groupby(\"animal\").apply(lambda subf: subf[\"size\"][subf[\"weight\"].idxmax()])\n```\n\n----------------------------------------\n\nTITLE: Querying by Anonymous Index in pandas.DataFrame.query - Python\nDESCRIPTION: Shows how to use the 'index' keyword in query strings when the index has no name, supporting queries not reliant on index naming. Requires pandas and numpy. Filters rows where index value is less than column 'b' and less than column 'c'. Inputs: DataFrame with unnamed index; Output: Filtered DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randint(n, size=(n, 2)), columns=list('bc'))\ndf\ndf.query('index < b < c')\n```\n\n----------------------------------------\n\nTITLE: Applying NumPy Universal Functions to SparseArray - pandas - Python\nDESCRIPTION: Shows that NumPy's ufuncs, such as np.abs, operate directly on pandas SparseArray objects, preserving the sparse structure in the result. This supports efficient mathematical operations on sparse data without conversion to dense forms. Requires numpy, pandas. The input is a SparseArray and the output is a SparseArray transformed by the ufunc.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\narr = pd.arrays.SparseArray([1., np.nan, np.nan, -2., np.nan])\nnp.abs(arr)\n```\n\n----------------------------------------\n\nTITLE: Replacing Missing Values with Column Mean Using pandas.Series.fillna - Python\nDESCRIPTION: This snippet fills missing values in the 'value_x' column of a pandas DataFrame with the mean of the same column, using the fillna() method. The method requires the pandas library and that 'value_x' is a numeric column. Takes the mean of non-missing values, replacing NaNs; returns a pandas Series with imputed values. The output Series retains the index from the original DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/missing.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nouter_join[\"value_x\"].fillna(outer_join[\"value_x\"].mean())\n```\n\n----------------------------------------\n\nTITLE: Converting Sparse Series with MultiIndex to COO Matrix - pandas - Python\nDESCRIPTION: Extracts a scipy.sparse.coo_matrix representation from a pandas Series with a MultiIndex, specifying row and column label grouping and sorting. Returns the COO matrix and the row/column indices, permitting arbitrary axis label mapping. Requires pandas, numpy. Inputs are the sparse Series and axis groupings; outputs are the COO matrix and axis labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nA, rows, columns = ss.sparse.to_coo(\n    row_levels=[\"A\", \"B\"], column_levels=[\"C\", \"D\"], sort_labels=True\n)\n\nA\nA.todense()\nrows\ncolumns\n```\n\n----------------------------------------\n\nTITLE: Working with CategoricalDtype in Python\nDESCRIPTION: Illustrates the creation and usage of CategoricalDtype objects, which fully describe a categorical's type with categories and ordering information. This is useful for explicitly defining categorical data types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import CategoricalDtype\n\nCategoricalDtype([\"a\", \"b\", \"c\"])\nCategoricalDtype([\"a\", \"b\", \"c\"], ordered=True)\nCategoricalDtype()\n```\n\n----------------------------------------\n\nTITLE: Converting NumPy Array Byte Order for pandas Series Construction - Python\nDESCRIPTION: This snippet demonstrates converting a NumPy array with big-endian byte order to the system-native byte order before creating a pandas Series. Requires Python, NumPy, and pandas. The np.array function is used to create an array with explicitly big-endian integer type (\"'>i4'\"), then .byteswap() and .view() methods are used to convert the byte order. The resulting array can be safely passed into pandas.Series for cross-platform data compatibility. Inputs: original array in a non-native byte order. Outputs: pandas Series with elements in native byte order.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/gotchas.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nx = np.array(list(range(10)), \">i4\")  # big endian\\nnewx = x.byteswap().view(x.dtype.newbyteorder())  # force native byteorder\\ns = pd.Series(newx)\n```\n\n----------------------------------------\n\nTITLE: Grouping Change-Runs Using groupby and cumsum - Pandas - Python\nDESCRIPTION: Demonstrates grouping consecutive equal values in a column and performing group-wise cumulative sum. Groups are determined by identifying where values differ from their shifted versions. Outputs the group dictionary and the cumulative sum for each group. No external dependencies beyond pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([0, 1, 0, 1, 1, 1, 0, 1, 1], columns=[\"A\"])\ndf[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).groups\ndf[\"A\"].groupby((df[\"A\"] != df[\"A\"].shift()).cumsum()).cumsum()\n```\n\n----------------------------------------\n\nTITLE: Using Binary File Handles with pandas.to_csv (Python)\nDESCRIPTION: Demonstrates exporting a DataFrame to a binary buffer using pandas to_csv with encoding and gzip compression. Dependencies: pandas, io. Key parameters are encoding, compression, and buffer object opened in binary mode. Inputs: DataFrame; Output: gzipped CSV to buffer.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\ndata = pd.DataFrame([0, 1, 2])\nbuffer = io.BytesIO()\ndata.to_csv(buffer, encoding=\"utf-8\", compression=\"gzip\")\n```\n\n----------------------------------------\n\nTITLE: Writing large DataFrames to SQL with chunking\nDESCRIPTION: Shows how to write a large DataFrame to a SQL database in chunks to avoid packet size limitations. The chunksize parameter controls the number of rows in each batch.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_225\n\nLANGUAGE: python\nCODE:\n```\ndata.to_sql(\"data_chunked\", con=engine, chunksize=1000)\n```\n\n----------------------------------------\n\nTITLE: Working with Timezone Removal and Localization on Timestamps and DateTimeIndex - pandas - Python\nDESCRIPTION: Illustrates how to use tz_localize(None) to remove timezone information for Timestamp and DatetimeIndex objects, and shows new options and error handling. Requires pandas. Output is a tz-naive version of the same dates. Code assumes input is tz-aware and may raise in prior versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nts = pd.Timestamp('2014-08-01 09:00', tz='US/Eastern')\\nts\n```\n\nLANGUAGE: python\nCODE:\n```\nts.tz_localize(None)\n```\n\nLANGUAGE: python\nCODE:\n```\ndidx = pd.date_range(start='2014-08-01 09:00', freq='H',\\n                   periods=10, tz='US/Eastern')\n```\n\nLANGUAGE: python\nCODE:\n```\ndidx\n```\n\nLANGUAGE: python\nCODE:\n```\ndidx.tz_localize(None)\n```\n\n----------------------------------------\n\nTITLE: Displaying MultiIndex with Non-Sparse Format in Python\nDESCRIPTION: Shows how to control the display of MultiIndex using the multi_sparse option, which can toggle between default sparse display and non-sparse display where all index levels are shown for each row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith pd.option_context(\"display.multi_sparse\", False):\n    df\n```\n\n----------------------------------------\n\nTITLE: Querying with timedelta64[ns] Columns in HDFStore - pandas - Python\nDESCRIPTION: Shows how to create a DataFrame containing timedelta64[ns] data, store it with data_columns enabled, and perform a query comparing a timedelta column to a value string (e.g., '<-3.5D'). Requires pandas and the datetime module. Inputs include a DataFrame with timedelta columns and queries using timedelta string expressions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_181\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import timedelta\n\ndftd = pd.DataFrame(\n    {\n        \"A\": pd.Timestamp(\"20130101\"),\n        \"B\": [\n            pd.Timestamp(\"20130101\") + timedelta(days=i, seconds=10)\n            for i in range(10)\n        ],\n    }\n)\ndftd[\"C\"] = dftd[\"A\"] - dftd[\"B\"]\ndftd\nstore.append(\"dftd\", dftd, data_columns=True)\nstore.select(\"dftd\", \"C<'-3.5D'\")\n```\n\n----------------------------------------\n\nTITLE: Selecting Rows with Multiple Column Criteria in Python\nDESCRIPTION: Demonstrates selecting rows that meet conditions across multiple columns using the & (and) operator. This technique allows for complex filtering based on multiple conditions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n)\ndf\n\ndf.loc[(df[\"BBB\"] < 25) & (df[\"CCC\"] >= -40), \"AAA\"]\n```\n\n----------------------------------------\n\nTITLE: Selecting Series from DataFrame with Unique Column Label - pandas - Python\nDESCRIPTION: Demonstrates how selecting a unique column label from a DataFrame that has duplicate column names returns a Series. This example shows df1['B'], where 'B' is unique, resulting in a Series. No special dependencies beyond pandas. Input is a DataFrame with duplicate columns; output is a Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf1[\"B\"]  # a series\n```\n\n----------------------------------------\n\nTITLE: Renaming Columns after Grouped Series Aggregation - Pandas - Python\nDESCRIPTION: Provides a preferred method for renaming columns after aggregation. Aggregates grouped Series, then uses .rename(columns=...) to relabel the result. Input: DataFrame df. Output: DataFrame with appropriately renamed columns. Encouraged over using dicts for renaming in .agg().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby('A').B.agg(['count']).rename(columns={'count': 'foo'})\n```\n\n----------------------------------------\n\nTITLE: Creating Sparse DataFrames and Inspecting Density with pandas - Python\nDESCRIPTION: Shows how to create a large pandas DataFrame with mostly missing values and convert it to sparse representation with pd.SparseDtype, then access its density attribute. Useful for cases where most values are 'compressed' (unused fill value), significantly reducing memory usage. Dependencies: numpy, pandas. Key parameters: df (original DataFrame), sdf (sparse DataFrame). Outputs include DataFrame's head, dtypes, and density metric.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10000, 4))\ndf.iloc[:9998] = np.nan\nsdf = df.astype(pd.SparseDtype(\"float\", np.nan))\nsdf.head()\nsdf.dtypes\nsdf.sparse.density\n```\n\n----------------------------------------\n\nTITLE: Extracting Optional Groups and Named Matches from Series Strings (Python)\nDESCRIPTION: Applies str.extract with optional groups for flexible regex parsing, handling cases where 'letter' may be missing but 'digit' always present. Dependencies: pandas. Inputs: Series of variable-formatted strings. Output: DataFrame with NaN for unmatched groups. Facilitates semi-structured data ingestion.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\npd.Series(['a1', 'b2', '3']).str.extract(\n    '(?P<letter>[ab])?(?P<digit>\\d)')\n```\n\n----------------------------------------\n\nTITLE: Assigning Series Name Attribute and Accessing It - pandas - Python\nDESCRIPTION: Creates a Series with a name attribute and illustrates how to assign, access, and verify the 'name'. Assigns a string as the Series name during creation and retrieves it through the name property. This attribute is commonly used when a Series is the result of DataFrame operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.random.randn(5), name=\"something\")\ns\ns.name\n```\n\n----------------------------------------\n\nTITLE: Demonstrating New DataFrame.value_counts Behavior in Pandas 3.0\nDESCRIPTION: This code snippet shows the new behavior of DataFrame.value_counts with sort=False in Pandas 3.0. It now maintains the order of the input instead of sorting by row labels as in previous versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v3.0.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"a\": [2, 2, 2, 2, 1, 1, 1, 1],\n        \"b\": [2, 1, 3, 1, 2, 3, 1, 1],\n    }\n)\ndf\n\ndf.value_counts(sort=False)\n```\n\n----------------------------------------\n\nTITLE: Error Handling with Duplicated Index in Reindexing\nDESCRIPTION: Demonstrates how .reindex() raises an error when the Series has a duplicated index, showing the limitation of this method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.arange(4), index=['a', 'a', 'b', 'c'])\nlabels = ['c', 'd']\ns.reindex(labels)\n```\n\n----------------------------------------\n\nTITLE: Creating Non-Identical DataFrames via Indexing in pandas (Python)\nDESCRIPTION: The snippet clarifies that creating new DataFrames via slicing or indexing produces distinct objects; their identities (`id`) are different and mutations on one do not affect the other. Dependencies: pandas. Input: DataFrame, slicing operations. Output: demonstration of object identity and mutation independence. Useful to highlight how to produce a true copy or non-identical object in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> df1 = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n>>> df2 = df1[:]  # or df1.loc[...] with some indexer\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> id(df1) == id(df2)  # or df1 is df2\nFalse\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> df1.iloc[0, 0]\n1\n>>> df2.iloc[0, 0] = 10\n>>> df1.iloc[0, 0]  # not changed\n1\n```\n\n----------------------------------------\n\nTITLE: Enabling Copy-on-Write Mode in pandas - Python\nDESCRIPTION: This snippet sets the Copy-on-Write mode in pandas by updating a global options flag. Setting this flag to True changes the data mutation semantics: all subset creations (even slices or masks) behave like copies rather than views, matching the Copy-on-Write approach described. Required dependency: pandas (imported as pd). Place this command after importing pandas, or set the environment variable PANDAS_COPY_ON_WRITE=1 before import. No input besides pandas is needed; it modifies global pandas behavior for the user process.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> pd.options.mode.copy_on_write = True\n```\n\n----------------------------------------\n\nTITLE: Calculating Time Differences with Shift in Pandas\nDESCRIPTION: Demonstrates calculating the difference between consecutive datetime values using shift() and setting a value to NaT using np.nan. Shows how to handle time series lag operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ny = s - s.shift()\ny\n\ny[1] = np.nan\ny\n```\n\n----------------------------------------\n\nTITLE: Timedelta Multiplication and Division with Integer Series - pandas - Python\nDESCRIPTION: Performs element-wise multiplication and multiplication with scalar/integer Series on a timedelta Series. Useful for scaling time differences. Shows negative and variable-length scaling.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntd * -1\ntd * pd.Series([1, 2, 3, 4])\n```\n\n----------------------------------------\n\nTITLE: Exporting DataFrame to HTML with and without Character Escaping - pandas - Python\nDESCRIPTION: Shows how to control escaping of special characters ('<', '>', '&') in the exported HTML using the 'escape' argument of the 'to_html' method. The first snippet exports with default escaping (escape=True), while the second disables escaping (escape=False), affecting how HTML tags and entities in the data are rendered in the output. Requires a DataFrame with such characters; outputs HTML as a string, demonstrated with both printing and HTML display.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_109\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": list(\"&<>\"), \"b\": np.random.randn(3)})\n```\n\nLANGUAGE: python\nCODE:\n```\nhtml = df.to_html()\nprint(html)\ndisplay(HTML(html))\n```\n\nLANGUAGE: python\nCODE:\n```\nhtml = df.to_html(escape=False)\nprint(html)\ndisplay(HTML(html))\n```\n\n----------------------------------------\n\nTITLE: Integer and Float Dtype Inference in Series (pandas, Python)\nDESCRIPTION: Demonstrates the default dtype inference in pandas.Series, indicating that [1, None] will create a series of float dtype by default, while [1, 2] results in an integer dtype. Illustrates the importance of dtype explicitness for nullable integer support. Requires pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.Series([1, None])\npd.Series([1, 2])\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in DataFrame Reductions with ExtensionArrays\nDESCRIPTION: Addresses a regression in DataFrame reductions using numeric_only=True and ExtensionArrays.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame.reduce(numeric_only=True)\n```\n\n----------------------------------------\n\nTITLE: Extracting Words from DataFrame Columns Using pandas String Methods in Python\nDESCRIPTION: Demonstrates how to use pandas string splitting methods to extract the first and last names from a column containing full names in a DataFrame. Requires the pandas library (imported as pd) and assumes a DataFrame with a column named 'String'. Inputs are strings separated by spaces, and the code outputs two new columns: 'First_Name' and 'Last_Name', extracted using str.split and str.rsplit with the expand=True argument. Limitations: Assumes names have exactly two words separated by a single space.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/nth_word.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfirstlast = pd.DataFrame({\"String\": [\"John Smith\", \"Jane Cook\"]})\nfirstlast[\"First_Name\"] = firstlast[\"String\"].str.split(\" \", expand=True)[0]\nfirstlast[\"Last_Name\"] = firstlast[\"String\"].str.rsplit(\" \", expand=True)[1]\nfirstlast\n```\n\n----------------------------------------\n\nTITLE: Integrating Nullable Integer Series with DataFrames (pandas, Python)\nDESCRIPTION: Shows how a nullable integer Series can be added as a column to a DataFrame along with regular Python/numpy arrays. Demonstrates DataFrame construction, direct viewing, and printing of dtypes, ensuring integration of nullable types with other standard data structures. Requires pandas as pd and Series s.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": s, \"B\": [1, 1, 3], \"C\": list(\"aab\")})\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Aggregating DataFrames with Single Functions - pandas - Python\nDESCRIPTION: Aggregates all columns in a DataFrame using a single function, either as a lambda or as a string, resulting in a Series of aggregated results. Shows equivalence to DataFrame.apply and demonstrates function aliasing. Key parameter: the aggregation function (as callable or string). Output: Series of aggregate values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ntsdf.agg(lambda x: np.sum(x))\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf.agg(\"sum\")\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf.sum()\n```\n\n----------------------------------------\n\nTITLE: Range Slicing with Tuples in MultiIndex\nDESCRIPTION: Demonstrates how to slice with a range of values in a MultiIndex by providing slice endpoints as tuples, allowing for precise selection of hierarchical data ranges.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[(\"baz\", \"two\"):(\"qux\", \"one\")]\ndf.loc[(\"baz\", \"two\"):\"foo\"]\n```\n\n----------------------------------------\n\nTITLE: Selecting a DataFrame Column as a Series using pandas in Python\nDESCRIPTION: This snippet selects the 'Age' column from the previously defined DataFrame 'df', returning it as a pandas Series object. The column label (a string) is accessed using square brackets. The output is a Series containing the numerical Age data, retaining original row labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/01_table_oriented.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf[\"Age\"]\n```\n\n----------------------------------------\n\nTITLE: Interpolation Methods in Pandas\nDESCRIPTION: Demonstrates various interpolation methods including linear, time-based, and scipy-based methods like barycentric, pchip, and akima.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\n    \"A\": [1, 2.1, np.nan, 4.7, 5.6, 6.8],\n    \"B\": [0.25, np.nan, np.nan, 4, 12.2, 14.4]\n})\ndf.interpolate()\ndf.interpolate(method=\"barycentric\")\ndf.interpolate(method=\"pchip\")\ndf.interpolate(method=\"akima\")\n```\n\n----------------------------------------\n\nTITLE: Initializing DataFrame with Assignment and dtype Behavior Pandas Python\nDESCRIPTION: Shows behavioral differences when assigning values to DataFrame columns via frame[keys] = values, particularly illustrating that new arrays replace existing arrays, preventing dtype casting to existing columns. Requires pandas. When assigning a scalar or array to multiple columns, pandas creates arrays with the dtype of the assigned value(s) rather than casting to the column dtype, and preserves type. Demonstrates via sample DataFrame creation and assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(range(3), columns=[\"A\"], dtype=\"float64\")\ndf[[\"A\"]] = 5\n```\n\n----------------------------------------\n\nTITLE: Determining dtype of a Homogenized NumPy Array from a DataFrame - Python\nDESCRIPTION: Extracts the underlying NumPy array from a DataFrame using the .to_numpy method and checks its dtype property, illustrating the 'lowest-common-denominator' type promotion across columns. Requires a prior DataFrame (e.g., 'df3'). The output is the dtype of the resulting ndarray, ensuring uniformity required by NumPy matrices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_80\n\nLANGUAGE: python\nCODE:\n```\ndf3.to_numpy().dtype\n```\n\n----------------------------------------\n\nTITLE: Reshaping Panel Data from Wide to Long Format using wide_to_long (Python)\nDESCRIPTION: Uses pandas' wide_to_long utility to melt a multi-column DataFrame with year-coded suffixes into a long format suitable for panel data analysis. Dependencies: pandas, numpy. Inputs: DataFrame with columns like 'A1970', 'B1980', etc. Outputs: multi-indexed long DataFrame. Aids in preparing data for statistical modeling.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(123)\ndf = pd.DataFrame({\"A1970\" : {0 : \"a\", 1 : \"b\", 2 : \"c\"},\n                   \"A1980\" : {0 : \"d\", 1 : \"e\", 2 : \"f\"},\n                   \"B1970\" : {0 : 2.5, 1 : 1.2, 2 : .7},\n                   \"B1980\" : {0 : 3.2, 1 : 1.3, 2 : .1},\n                   \"X\"     : dict(zip(range(3), np.random.randn(3)))\n                  })\ndf[\"id\"] = df.index\ndf\npd.wide_to_long(df, [\"A\", \"B\"], i=\"id\", j=\"year\")\n```\n\n----------------------------------------\n\nTITLE: Exploding Struct Series with pyarrow - pandas Python\nDESCRIPTION: This snippet demonstrates how to create a pandas Series holding struct records using pyarrow's ArrowDtype and explode the struct to flatten the Series. Dependencies include pandas and pyarrow. It shows the creation of complex dtypes and how struct-explode expands each struct record for further processing. The Series must use an ArrowDtype with a struct type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nseries = pd.Series(\n    [\n        {\"project\": \"pandas\", \"version\": \"2.2.0\"},\n        {\"project\": \"numpy\", \"version\": \"1.25.2\"},\n        {\"project\": \"pyarrow\", \"version\": \"13.0.0\"},\n    ],\n    dtype=pd.ArrowDtype(\n        pa.struct([\n            (\"project\", pa.string()),\n            (\"version\", pa.string()),\n        ])\n    ),\n)\nseries.struct.explode()\n```\n\n----------------------------------------\n\nTITLE: NA in Boolean Context in pandas (Python)\nDESCRIPTION: This snippet demonstrates the ambiguous behavior when attempting to convert pd.NA to a Python boolean. It shows that bool(pd.NA) raises an exception rather than returning a boolean. There are no external dependencies but requires pandas with NA support. This highlights that NA cannot be used directly in conditional statements; use pd.isna instead.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbool(pd.NA)\n```\n\n----------------------------------------\n\nTITLE: Creating MultiIndex Series and DataFrame with Arrays in Python\nDESCRIPTION: Shows how to directly pass a list of arrays to Series or DataFrame constructors to automatically create a MultiIndex without explicitly using MultiIndex constructors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\narrays = [\n    np.array([\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"]),\n    np.array([\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"]),\n]\ns = pd.Series(np.random.randn(8), index=arrays)\ns\ndf = pd.DataFrame(np.random.randn(8, 4), index=arrays)\ndf\n```\n\n----------------------------------------\n\nTITLE: Using Table Schema Output in Pandas JSON Export\nDESCRIPTION: This code showcases the new 'table' orient option for DataFrame.to_json() which generates a Table Schema compatible representation of the data with type information and metadata.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nIn [38]: df = pd.DataFrame(\n   ....: {'A': [1, 2, 3],\n   ....:  'B': ['a', 'b', 'c'],\n   ....:  'C': pd.date_range('2016-01-01', freq='d', periods=3)},\n   ....: index=pd.Index(range(3), name='idx'))\nIn [39]: df\nOut[39]:\n     A  B          C\nidx\n0    1  a 2016-01-01\n1    2  b 2016-01-02\n2    3  c 2016-01-03\n\n[3 rows x 3 columns]\n\nIn [40]: df.to_json(orient='table')\nOut[40]:\n'{\"schema\":{\"fields\":[{\"name\":\"idx\",\"type\":\"integer\"},{\"name\":\"A\",\"type\":\"integer\"},{\"name\":\"B\",\"type\":\"string\"},{\"name\":\"C\",\"type\":\"datetime\"}],\"primaryKey\":[\"idx\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"idx\":0,\"A\":1,\"B\":\"a\",\"C\":\"2016-01-01T00:00:00.000\"},{\"idx\":1,\"A\":2,\"B\":\"b\",\"C\":\"2016-01-02T00:00:00.000\"},{\"idx\":2,\"A\":3,\"B\":\"c\",\"C\":\"2016-01-03T00:00:00.000\"}]}'\n```\n\n----------------------------------------\n\nTITLE: Using Tuples as Index Labels in Python\nDESCRIPTION: Demonstrates how to use tuples directly as labels for pandas Series indices without explicitly creating a MultiIndex object, which is a valid alternative approach.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npd.Series(np.random.randn(8), index=tuples)\n```\n\n----------------------------------------\n\nTITLE: Current Error for Positional Indexing by Float in pandas Series - IPython\nDESCRIPTION: Shows current error message thrown when using a float for positional indexing with .iloc, which now always raises TypeError. Useful for diagnostics and code migration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_24\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: s.iloc[2.0]\nTypeError: cannot do label indexing on <class 'pandas.indexes.numeric.Int64Index'> with these indexers [2.0] of <type 'float'>\n```\n\n----------------------------------------\n\nTITLE: Tab-Completion for DataFrame Plot Submethods in pandas (Python)\nDESCRIPTION: This snippet demonstrates tab-completion on the plot accessor in IPython, listing the available plot kinds as methods. It improves user productivity by exposing only valid submethods, each corresponding to a plot type. Input: DataFrame; context: IPython/Jupyter environment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_7\n\nLANGUAGE: ipython\nCODE:\n```\nIn [15]: df.plot.<TAB>  # noqa: E225, E999\ndf.plot.area     df.plot.barh     df.plot.density  df.plot.hist     df.plot.line     df.plot.scatter\ndf.plot.bar      df.plot.box      df.plot.hexbin   df.plot.kde      df.plot.pie\n```\n\n----------------------------------------\n\nTITLE: Referencing DataFrame After Outer Join in pandas - Python\nDESCRIPTION: This snippet displays the result of an outer join in a pandas DataFrame, labeled as outer_join. It assumes that outer_join was previously created, often through functions like pd.merge with how=\\\"outer\\\". The code outputs the DataFrame for inspection. The DataFrame may contain NaN in columns where records did not match, illustrating the way pandas represents missing entries. This snippet requires the pandas package and a pre-existing DataFrame named outer_join.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/missing_intro.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nouter_join\n```\n\n----------------------------------------\n\nTITLE: Expanding and Collapsing DataFrame Display Across Pages - Python\nDESCRIPTION: Demonstrates toggling the 'expand_frame_repr' option in pandas, which determines whether DataFrames stretch across pages or wrap columns. Setting True/False shows visual differences in console output. Useful for wide DataFrames. Requires pandas, numpy, and a DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(5, 10))\npd.set_option(\"expand_frame_repr\", True)\ndf\npd.set_option(\"expand_frame_repr\", False)\ndf\npd.reset_option(\"expand_frame_repr\")\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Tables into DataFrames Using pd.read_html and DataFrame.to_html - Pandas - Python\nDESCRIPTION: Demonstrates how to serialize a DataFrame to HTML and then parse it back using 'pd.read_html', which accepts file-like objects and returns a list of DataFrames. Shows equivalence checking for round-trip conversion. This process relies on the 'BeautifulSoup4' and 'html5lib' libraries for parsing. Warns that round-trip is not a guaranteed inverse, as 'read_html' returns a list rather than a single DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport io\ndf = pd.DataFrame({\"a\": range(3), \"b\": list(\"abc\")})\nprint(df)\nhtml = df.to_html()\nalist = pd.read_html(io.StringIO(html), index_col=0)\nprint(df == alist[0])\n```\n\n----------------------------------------\n\nTITLE: Pivoting Data via Melt and Pivot Table - pandas - Python\nDESCRIPTION: The snippet illustrates how to reshape and aggregate a pandas DataFrame similarly to R's acast by first melting the data and then using pivot_table. It aggregates the 'value' column by 'variable', 'week', and 'month', computing the mean. Requires pandas and numpy; suitable for multi-indexed aggregation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"x\": np.random.uniform(1.0, 168.0, 12),\n        \"y\": np.random.uniform(7.0, 334.0, 12),\n        \"z\": np.random.uniform(1.7, 20.7, 12),\n        \"month\": [5, 6, 7] * 4,\n        \"week\": [1, 2] * 6,\n    }\n)\n\nmdf = pd.melt(df, id_vars=[\"month\", \"week\"])\npd.pivot_table(\n    mdf,\n    values=\"value\",\n    index=[\"variable\", \"week\"],\n    columns=[\"month\"],\n    aggfunc=\"mean\",\n)\n```\n\n----------------------------------------\n\nTITLE: Applying pandas.DataFrame.where with Axis and Level Alignment - Python\nDESCRIPTION: Illustrates using the 'axis' parameter in DataFrame.where to align condition evaluation along the specified axis. Requires pandas. Replaces negative or zero entries in a DataFrame with values from column 'A', aligned by index. Input: DataFrame; Output: DataFrame with selected replacements along index axis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.copy()\ndf2.where(df2 > 0, df2['A'], axis='index')\n```\n\n----------------------------------------\n\nTITLE: get_dummies with sparse=True DataFrame type output (legacy pandas, Python)\nDESCRIPTION: Shows legacy pandas behavior where get_dummies with sparse=True returned either a DataFrame or a SparseDataFrame depending on which columns were dummy-encoded, and demonstrates the type checking of returned objects. Assumes pandas is imported as pd and DataFrame with columns 'A', 'B', 'C' is constructed. Inputs are DataFrame and its subset columns; outputs are type DataFrame/SparseDataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: df = pd.DataFrame({\"A\": [1, 2], \"B\": ['a', 'b'], \"C\": ['a', 'a']})\n\nIn [3]: type(pd.get_dummies(df, sparse=True))\nOut[3]: pandas.DataFrame\n\nIn [4]: type(pd.get_dummies(df[['B', 'C']], sparse=True))\nOut[4]: pandas.core.sparse.frame.SparseDataFrame\n```\n\n----------------------------------------\n\nTITLE: Viewing Top and Bottom Rows of a DataFrame - Python\nDESCRIPTION: The methods df.head() and df.tail(3) fetch the first and last rows of the DataFrame, respectively. df.head() gives the top 5 rows by default, while df.tail(3) gives the last 3 rows. Useful for data inspection; requires a DataFrame in memory.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.head()\\ndf.tail(3)\n```\n\n----------------------------------------\n\nTITLE: Label-Based Slicing Using .loc - pandas (Python, future behavior)\nDESCRIPTION: This example illustrates label-based slicing using ser.loc[2:4] on a pandas Series with a custom integer index. In pandas 1.5+ and future versions, slicing like ser[2:4] will behave this way natively. The output shows values corresponding to the label-based range 2 to 4. Prior to this change, .loc was the explicit approach to achieve label-based slicing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nIn [4]: ser.loc[2:4]\nOut[4]:\n2    1\n3    2\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Generating Dummy Variables with get_dummies - Pandas Python\nDESCRIPTION: Shows the use of pd.get_dummies on a DataFrame, encoding categorical columns as one-hot (dummy) variables while leaving other columns untouched. Assumes pandas is imported and a DataFrame is created from a specified dictionary. Output is a new DataFrame with original categorical columns replaced by their dummy variable representations. Requires pandas; input is any DataFrame with mixed column types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['c', 'c', 'b'],\n                'C': [1, 2, 3]})\npd.get_dummies(df)\n```\n\n----------------------------------------\n\nTITLE: Aggregating with Nullable Integer Columns (sum, groupby) - pandas (Python)\nDESCRIPTION: Shows groupby and reduction operations (such as sum) on DataFrames containing nullable integer extension types. Requires pandas >=0.24.0. Input is a DataFrame with such columns; output includes summing across columns and grouped sums preserving NAs. Ensures standard reductions and groupby respect missing values in extension dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.sum()\ndf.groupby('B').A.sum()\n```\n\n----------------------------------------\n\nTITLE: Explicit Index-Wise Addition Between DataFrame and Series with add(axis=\"index\") in pandas (Python)\nDESCRIPTION: This snippet demonstrates the recommended approach for aligning and broadcasting a Series across a DataFrame's index during arithmetic operations. By using the 'add' method with 'axis=\"index\"', users can explicitly control broadcasting behavior and avoid warnings or future incompatibilities. Dependencies: pandas. Input: DataFrame and Series; Output: new DataFrame with element-wise sum along the specified axis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndf.add(df.A, axis=\"index\")\n```\n\n----------------------------------------\n\nTITLE: TypeError Raised for Float Indexing on Int64Index - Pandas - ipython\nDESCRIPTION: Shows examples where using scalar or slice float indexers on a pandas Series with an Int64Index triggers TypeError. Demonstrates pandas error messages for both scalar and slice float indexers, essential for understanding and debugging type mismatches in indexing. No dependencies beyond pandas required. Output: TypeError exceptions with clear messages.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_8\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pd.Series(range(5))[3.5]\\nTypeError: the label [3.5] is not a proper indexer for this index type (Int64Index)\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pd.Series(range(5))[3.5:4.5]\\nTypeError: the slice start [3.5] is not a proper indexer for this index type (Int64Index)\n```\n\n----------------------------------------\n\nTITLE: Reading a Custom Table Template from Disk in Python\nDESCRIPTION: This snippet opens and reads a user-defined HTML template file from the local 'templates' directory, intending to use it as a custom Jinja template for DataFrame rendering. It expects the 'templates/myhtml.tpl' file to exist on disk, and can be used to verify or inspect the template's contents before integration into a Styler subclass.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"templates/myhtml.tpl\") as f_html:\n    print(f_html.read())\n```\n\n----------------------------------------\n\nTITLE: Accessing Timedelta Components in Python\nDESCRIPTION: Demonstrates the new behavior for accessing components of a pandas Timedelta object, including days, seconds, and microseconds. Also shows how to access all components using the .components attribute.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nt = pd.Timedelta('1 day, 10:11:12.100123')\nt.days\nt.seconds\nt.microseconds\n\nt.components\nt.components.seconds\n```\n\n----------------------------------------\n\nTITLE: Indexing PyArrow List Series with list accessor - pandas Python\nDESCRIPTION: This example demonstrates the creation of a pandas Series whose elements are lists of integers stored with a pyarrow list dtype, and then indexing into those lists using the Series.list accessor. Prerequisites are pandas and pyarrow with ArrowDtype support. The snippet shows how to retrieve the first element from each pyarrow list in the Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nseries = pd.Series(\n    [\n        [1, 2, 3],\n        [4, 5],\n        [6],\n    ],\n    dtype=pd.ArrowDtype(\n        pa.list_(pa.int64())\n    ),\n)\nseries.list[0]\n```\n\n----------------------------------------\n\nTITLE: Literal String Replacement with regex=False (Python)\nDESCRIPTION: Highlights how to force literal matching in replacements by setting regex=False, which avoids need to escape regex characters. Both methods shown are equivalent for simple literal replacement. Useful for data cleaning with special characters. Input is string Series with dollar amounts.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndollars = pd.Series([\"12\", \"-$10\", \"$10,000\"], dtype=\"string\")\n\n# These lines are equivalent\ndollars.str.replace(r\"-\\$\", \"-\", regex=True)\ndollars.str.replace(\"-$\", \"-\", regex=False)\n```\n\n----------------------------------------\n\nTITLE: Timing Chained DataFrame Comparisons with Standard Python - Python\nDESCRIPTION: Benchmarks the time taken to compute a chained boolean AND operation across four large DataFrames using standard Python. Useful to compare against the pandas.eval method. Input: df1, df2, df3, df4; output: timing of the boolean operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n%timeit (df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\n```\n\n----------------------------------------\n\nTITLE: Selecting DataFrame from DataFrame with Duplicate Column Label - pandas - Python\nDESCRIPTION: Illustrates that selecting a column from a DataFrame when the label is duplicated returns a DataFrame rather than a Series. In this snippet, df1['A'] returns a DataFrame with both 'A' columns. Requires pandas; behavior is specific to duplicated columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf1[\"A\"]  # a DataFrame\n```\n\n----------------------------------------\n\nTITLE: Prohibiting normalize=True for Tick DateOffset, and associativity fix (pandas >=0.24, Python)\nDESCRIPTION: Demonstrates the new restriction: creating a Tick offset with normalize=True is forbidden. Shows standard time addition without normalization and confirms associativity. Inputs: Timestamp and standard Offset; output: boolean True for associativity.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nts = pd.Timestamp('2018-06-11 18:01:14')\ntic = pd.offsets.Hour(n=2)\nts + tic + tic + tic == ts + (tic + tic + tic)\n```\n\n----------------------------------------\n\nTITLE: Transforming Geometry Data with XSLT in Pandas\nDESCRIPTION: An XSLT stylesheet example that transforms geometry data by creating object elements with polygon type attributes for non-circle shapes. The stylesheet copies shape information and includes degree and sides properties in a nested property element.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_132\n\nLANGUAGE: xml\nCODE:\n```\n<xsl:apply-templates select=\"row\"/>\n        </geometry>\n      </xsl:template>\n      <xsl:template match=\"row\">\n        <object index=\"{index}\">\n          <xsl:if test=\"shape!='circle'\">\n              <xsl:attribute name=\"type\">polygon</xsl:attribute>\n          </xsl:if>\n          <xsl:copy-of select=\"shape\"/>\n          <property>\n              <xsl:copy-of select=\"degrees|sides\"/>\n          </property>\n        </object>\n      </xsl:template>\n    </xsl:stylesheet>\"\n```\n\n----------------------------------------\n\nTITLE: If-Then-Else Using NumPy's where Function in Python\nDESCRIPTION: Creates a new column based on a condition using NumPy's where function. This function provides a vectorized if-then-else operation, returning values chosen from two arrays based on a condition.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n)\ndf\ndf[\"logic\"] = np.where(df[\"AAA\"] > 5, \"high\", \"low\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Applying EWM (Exponentially Weighted Mean) on GroupBy Objects (Python)\nDESCRIPTION: Demonstrates how to use the new direct support for EWM (Exponentially Weighted Mean) aggregation on DataFrameGroupBy objects. Supports additional arguments for engine and engine_kwargs (e.g., for Numba). Dependencies: pandas (and Numba for engine='numba'). Inputs: DataFrame with grouping column; Output: GroupBy object with EWM mean applied.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': ['a', 'b', 'a', 'b'], 'B': range(4)})\ndf\ndf.groupby('A').ewm(com=1.0).mean()\n```\n\n----------------------------------------\n\nTITLE: Selecting Rows with Maximum Value from Each Group - Pandas - Python\nDESCRIPTION: Uses a DataFrame indexed by 'host' and 'service', groups by the first index, and selects the row with the maximum 'no' value from each group. Outputs a DataFrame with the selected rows as flat records. Useful for extracting extremum entries per group from a hierarchical index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"host\": [\"other\", \"other\", \"that\", \"this\", \"this\"],\n        \"service\": [\"mail\", \"web\", \"mail\", \"mail\", \"web\"],\n        \"no\": [1, 2, 1, 2, 1],\n    }\n).set_index([\"host\", \"service\"])\nmask = df.groupby(level=0).agg(\"idxmax\")\ndf_count = df.loc[mask[\"no\"]].reset_index()\ndf_count\n```\n\n----------------------------------------\n\nTITLE: Formatting and Exporting DataFrame to LaTeX with Styler - pandas - Python\nDESCRIPTION: Applies value formatting to a DataFrame using the 'Styler.format' method and then exports the styled DataFrame to LaTeX. In this example, each value is formatted as a Euro currency string. Useful for applying custom string formatting prior to LaTeX export; outputs a LaTeX string for further document integration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_111\n\nLANGUAGE: python\nCODE:\n```\nprint(df.style.format(\"\\u20ac {}\").to_latex())\n```\n\n----------------------------------------\n\nTITLE: Assigning and Manipulating Datetime Columns with pandas - Python\nDESCRIPTION: This snippet creates new DataFrame columns using pandas Timestamp and performs various datetime manipulations, such as extracting the year or month from Timestamps, adding month offsets, and calculating the number of months between two dates by converting them to period objects. It requires the pandas library and an existing DataFrame named 'tips'. The input includes hard-coded dates and column names; the output is the display of the selected newly created columns, illustrating the effect of each datetime operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/time_date.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips[\"date1\"] = pd.Timestamp(\"2013-01-15\")\\ntips[\"date2\"] = pd.Timestamp(\"2015-02-15\")\\ntips[\"date1_year\"] = tips[\"date1\"].dt.year\\ntips[\"date2_month\"] = tips[\"date2\"].dt.month\\ntips[\"date1_next\"] = tips[\"date1\"] + pd.offsets.MonthBegin()\\ntips[\"months_between\"] = tips[\"date2\"].dt.to_period(\"M\") - tips[\\n    \"date1\"\\n].dt.to_period(\"M\")\\n\\ntips[\\n    [\"date1\", \"date2\", \"date1_year\", \"date2_month\", \"date1_next\", \"months_between\"]\\n]\\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating String-based Datetime Resolution Inference in Pandas 3.0\nDESCRIPTION: This snippet illustrates how Pandas 3.0 infers datetime resolution from string inputs, matching the precision of the input strings. It shows examples with different levels of precision in the datetime strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v3.0.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: pd.to_datetime([\"2024-03-22 11:43:01\"]).dtype\nIn [3]: pd.to_datetime([\"2024-03-22 11:43:01.002\"]).dtype\nIn [4]: pd.to_datetime([\"2024-03-22 11:43:01.002003\"]).dtype\nIn [5]: pd.to_datetime([\"2024-03-22 11:43:01.002003004\"]).dtype\n```\n\n----------------------------------------\n\nTITLE: Retrieving NumPy Arrays from Index and Series - pandas (Python)\nDESCRIPTION: Demonstrates converting Index or Series objects to NumPy arrays explicitly using .to_numpy(), recommended for interoperability. Inputs are PeriodIndex or Series objects; outputs are NumPy arrays. Ensures that array data is unambiguous and well-typed, suitable for use with other NumPy functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nidx.to_numpy()\npd.Series(idx).to_numpy()\n```\n\n----------------------------------------\n\nTITLE: Ranking within Groups using SQL RANK() Analytic Function - SQL\nDESCRIPTION: Implements group-wise ranking on the 'tips' SQL table using the RANK() analytic function, partitioning by 'sex' and ordering by 'tip'. Filters to select records with 'tip' less than 2 and 'rank' less than 3. The subquery approach aligns with scenarios where identical tips receive the same rank, as in Oracle's RANK(). Requires an SQL-compatible RDBMS (e.g., Oracle) and a 'tips' table with 'sex' and 'tip' columns. Outputs a result set ordered by 'sex' and rank; duplicate tip values yield the same rank.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_11\n\nLANGUAGE: sql\nCODE:\n```\n-- Oracle's RANK() analytic function\nSELECT * FROM (\n  SELECT\n    t.,\n    RANK() OVER(PARTITION BY sex ORDER BY tip) AS rnk\n  FROM tips t\n  WHERE tip < 2\n)\nWHERE rnk < 3\nORDER BY sex, rnk;\n```\n\n----------------------------------------\n\nTITLE: Pivoting DataFrames and Controlling NaN Representation with dropna - Python\nDESCRIPTION: Demonstrates creation of categorical data using Pandas Categorical, building a DataFrame, and performing pivot_table operations with the dropna keyword. Requires pandas. By toggling dropna, the pivot_table includes either all category combinations or only those observed, illustrating control over inclusion of missing data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncat1 = pd.Categorical([\"a\", \"a\", \"b\", \"b\"],\n                     categories=[\"a\", \"b\", \"z\"], ordered=True)\ncat2 = pd.Categorical([\"c\", \"d\", \"c\", \"d\"],\n                     categories=[\"c\", \"d\", \"y\"], ordered=True)\ndf = pd.DataFrame({\"A\": cat1, \"B\": cat2, \"values\": [1, 2, 3, 4]})\ndf\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pd.pivot_table(df, values='values', index=['A', 'B'], dropna=True)\n\nOut[1]:\n     values\nA B       \na c     1.0\n  d     2.0\nb c     3.0\n  d     4.0\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: pd.pivot_table(df, values='values', index=['A', 'B'], dropna=False)\n\nOut[2]:\n     values\nA B           \na c     1.0\n  d     2.0\n  y     NaN\nb c     3.0\n  d     4.0\n  y     NaN\nz c     NaN\n  d     NaN\n  y     NaN\n```\n\n----------------------------------------\n\nTITLE: Creating Sparse Series with MultiIndex for COO Conversion - pandas - Python\nDESCRIPTION: Creates a pandas Series with a MultiIndex and converts it to a SparseDtype ('Sparse'). Prepares for transformation to a COO sparse matrix based on hierarchical labels. Dependencies: pandas, numpy. Output is a sparse Series whose index encodes multi-level axes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([3.0, np.nan, 1.0, 3.0, np.nan, np.nan])\ns.index = pd.MultiIndex.from_tuples(\n    [\n        (1, 2, \"a\", 0),\n        (1, 2, \"a\", 1),\n        (1, 1, \"b\", 0),\n        (1, 1, \"b\", 1),\n        (2, 1, \"b\", 0),\n        (2, 1, \"b\", 1),\n    ],\n    names=[\"A\", \"B\", \"C\", \"D\"],\n)\nss = s.astype('Sparse')\nss\n```\n\n----------------------------------------\n\nTITLE: Setting Names and Levels on MultiIndex - Pandas Python\nDESCRIPTION: Demonstrates how to set names and levels on a MultiIndex in pandas using set_names and set_levels, with support for multiple levels and different input formats (strings or lists). Requires pandas and a MultiIndex constructed from specified components. Parameters allow targeting of specific levels by integer or name. Inputs are the MultiIndex and new names or level values; outputs are new or modified MultiIndexes. Useful for managing hierarchical indexes in DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.MultiIndex.from_product([['a'], range(3), list(\"pqr\")],\n                                   names=['foo', 'bar', 'baz'])\nidx.set_names('qux', level=0)\nidx.set_names(['qux', 'corge'], level=[0, 1])\nidx.set_levels(['a', 'b', 'c'], level='bar')\nidx.set_levels([['a', 'b', 'c'], [1, 2, 3]], level=[1, 2])\n```\n\n----------------------------------------\n\nTITLE: Rolling Window Iteration Example\nDESCRIPTION: Shows how to iterate over rolling windows to view the partitions of data used in calculations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfor window in s.rolling(window=2):\n    print(window)\n```\n\n----------------------------------------\n\nTITLE: Boolean Negation in pandas.DataFrame.query and Python - Python\nDESCRIPTION: Explains negating boolean expressions in DataFrame.query using not and ~, showing equivalence to Python boolean array indexing. Requires pandas and numpy. Chains boolean conditions for DataFrame selection. Inputs: DataFrame; Output: Filtered DataFrames by negated boolean column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\ndf['bools'] = np.random.rand(len(df)) > 0.5\ndf.query('~bools')\ndf.query('not bools')\ndf.query('not bools') == df[~df['bools']]\n```\n\n----------------------------------------\n\nTITLE: Passing Compression Options to to_pickle in Pandas\nDESCRIPTION: Shows how to pass additional options to the compression protocol by using a dictionary with 'method' and other parameters. This example uses gzip with a lower compression level for faster performance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_161\n\nLANGUAGE: python\nCODE:\n```\ndf.to_pickle(\"data.pkl.gz\", compression={\"method\": \"gzip\", \"compresslevel\": 1})\n```\n\n----------------------------------------\n\nTITLE: Using subset parameter instead of cols in duplicated and drop_duplicates\nDESCRIPTION: Example showing the parameter name change in DataFrame.duplicated() and DataFrame.drop_duplicates() methods, where 'subset' should be used instead of the deprecated 'cols' keyword.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.duplicated(subset=[...])\nDataFrame.drop_duplicates(subset=[...])\n```\n\n----------------------------------------\n\nTITLE: Element-wise Comparison in Python with Pandas\nDESCRIPTION: This snippet demonstrates element-wise comparison between a Pandas Series or Index and a scalar value, as well as between different array-like objects of the same length.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npd.Series([\"foo\", \"bar\", \"baz\"]) == \"foo\"\npd.Index([\"foo\", \"bar\", \"baz\"]) == \"foo\"\npd.Series([\"foo\", \"bar\", \"baz\"]) == pd.Index([\"foo\", \"bar\", \"qux\"])\npd.Series([\"foo\", \"bar\", \"baz\"]) == np.array([\"foo\", \"bar\", \"qux\"])\n```\n\n----------------------------------------\n\nTITLE: Walking Through HDF5 Hierarchical Groups - pandas - Python\nDESCRIPTION: Traverses the hierarchical group structure within HDFStore using walk, prints both groups and leaf keys, and retrieves the stored DataFrames for each key. Useful for exploring and debugging the contents of an HDFStore. Requires pandas; outputs are paths and contents printed to standard output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_173\n\nLANGUAGE: python\nCODE:\n```\nfor (path, subgroups, subkeys) in store.walk():\n    for subgroup in subgroups:\n        print(\"GROUP: {}/{}\".format(path, subgroup))\n    for subkey in subkeys:\n        key = \"/\".join([path, subkey])\n        print(\"KEY: {}\".format(key))\n        print(store.get(key))\n```\n\n----------------------------------------\n\nTITLE: setuptools Version Exclusion in setup.py - Python\nDESCRIPTION: Illustrates how to exclude pandas 0.21.x from dependencies in a Python setup.py file using install_requires to avoid API-inconsistent versions. Intended for library maintainers requiring consistent sum/prod behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ninstall_requires=['pandas!=0.21.*', ...]\n```\n\n----------------------------------------\n\nTITLE: Upsampling with Resample and Sum in pandas 0.22.0 - Python\nDESCRIPTION: Shows that, in pandas 0.22.0, upsampling and summing results in empty (inserted) bins being filled with 0 instead of NaN. Inputs: Series with DateTimeIndex, 12H rebinning. Dependencies: pandas as pd, numpy as np.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nIn [14]: idx = pd.DatetimeIndex([\"2017-01-01\", \"2017-01-02\"])\nIn [15]: pd.Series([1, 2], index=idx).resample(\"12H\").sum()\nOut[15]:\n2017-01-01 00:00:00    1\n2017-01-01 12:00:00    0\n2017-01-02 00:00:00    2\nFreq: 12H, Length: 3, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Melting and Stacking DataFrame - pandas - Python\nDESCRIPTION: This Python snippet demonstrates reshaping a DataFrame from wide to long format using pandas.melt, with 'first' and 'last' as id_vars. It further shows an alternative using set_index and stack. Requires pandas. Input is a DataFrame structured similarly to the R example; output is a long-format DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ncheese = pd.DataFrame(\n    {\n        \"first\": [\"John\", \"Mary\"],\n        \"last\": [\"Doe\", \"Bo\"],\n        \"height\": [5.5, 6.0],\n        \"weight\": [130, 150],\n    }\n)\n\npd.melt(cheese, id_vars=[\"first\", \"last\"])\ncheese.set_index([\"first\", \"last\"]).stack()  # alternative way\n```\n\n----------------------------------------\n\nTITLE: Preserving string indices and structure during JSON serialization/deserialization - Python\nDESCRIPTION: This snippet demonstrates how to serialize a pandas DataFrame with string indices to JSON, and then deserialize it, preserving the string indices and column labels. It utilizes the StringIO class for in-memory operations and shows index/column roundtripping. Dependencies: pandas, numpy, io.StringIO. Inputs: DataFrame with string index; outputs: roundtripped DataFrame, with preserved indices and column structure. Note: the convert_axes parameter is deprecated in recent pandas versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_77\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\nsi = pd.DataFrame(\n    np.zeros((4, 4)), columns=list(range(4)), index=[str(i) for i in range(4)]\n)\nsi\nsi.index\nsi.columns\njson = si.to_json()\n\nsij = pd.read_json(StringIO(json), convert_axes=False)\nsij\nsij.index\nsij.columns\n```\n\n----------------------------------------\n\nTITLE: Writing Categorical Data to CSV and Loss of Metadata - Pandas Python\nDESCRIPTION: Begins to show how categorical data may be written to CSV, noting that information about category levels or ordering is lost and must be re-established upon reading. Requires pandas and io module. Demonstrates creation of a series with categorical dtype and renaming its categories.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\ns = pd.Series(pd.Categorical([\"a\", \"b\", \"b\", \"a\", \"a\", \"d\"]))\n# rename the categories\n```\n\n----------------------------------------\n\nTITLE: Creating and Manipulating Pandas Index Objects\nDESCRIPTION: This section shows how to create Pandas Index objects, including specifying data types and names. It also demonstrates basic operations and attribute setting on Index objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.Index(['e', 'd', 'a', 'b'])\nindex\n'd' in index\n\nindex = pd.Index([1, 5, 12])\nindex\n5 in index\n\nindex = pd.Index(['e', 'd', 'a', 'b'], dtype=\"string\")\nindex\nindex = pd.Index([1, 5, 12], dtype=\"int8\")\nindex\nindex = pd.Index([1, 5, 12], dtype=\"float32\")\nindex\n\nindex = pd.Index(['e', 'd', 'a', 'b'], name='something')\nindex.name\n\nindex = pd.Index(list(range(5)), name='rows')\ncolumns = pd.Index(['A', 'B', 'C'], name='cols')\ndf = pd.DataFrame(np.random.randn(5, 3), index=index, columns=columns)\ndf\ndf['A']\n```\n\n----------------------------------------\n\nTITLE: Writing OpenDocument Spreadsheets with Pandas\nDESCRIPTION: Shows how to write a pandas DataFrame to an OpenDocument spreadsheet (.ods) file using the odf engine.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_149\n\nLANGUAGE: python\nCODE:\n```\n# Writes DataFrame to a .ods file\ndf.to_excel(\"path_to_file.ods\", engine=\"odf\")\n```\n\n----------------------------------------\n\nTITLE: Caveats of .take with Boolean Indices - pandas - Python\nDESCRIPTION: Highlights that .take is designed for integer indices and using boolean indices may return unexpected results, unlike boolean mask-based selection methods. Offers a comparison between array-style and iloc/indexing access. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\narr = np.random.randn(10)\narr.take([False, False, True, True])\narr[[0, 1]]\nser = pd.Series(np.random.randn(10))\nser.take([False, False, True, True])\nser.iloc[[0, 1]]\n```\n\n----------------------------------------\n\nTITLE: Defining and Applying Custom Numba-JIT Functions in pandas Workflows (python)\nDESCRIPTION: Defines several Numba JIT-compiled functions and demonstrates integrating them into a pandas DataFrame workflow. Highlights how to decorate Python numerical functions for efficiency and combine them to run on NumPy array columns, returning a Series efficiently. Needs pandas, numpy, numba; inputs are DataFrame columns as arrays. Outputs a Series with custom computation results. Shows both how to design custom JIT-suitable functions and apply them at scale.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport numba\n\n\n@numba.jit\ndef f_plain(x):\n    return x * (x - 1)\n\n\n@numba.jit\ndef integrate_f_numba(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_plain(a + i * dx)\n    return s * dx\n\n\n@numba.jit\ndef apply_integrate_f_numba(col_a, col_b, col_N):\n    n = len(col_N)\n    result = np.empty(n, dtype=\"float64\")\n    assert len(col_a) == len(col_b) == n\n    for i in range(n):\n        result[i] = integrate_f_numba(col_a[i], col_b[i], col_N[i])\n    return result\n\n\ndef compute_numba(df):\n    result = apply_integrate_f_numba(\n        df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy()\n    )\n    return pd.Series(result, index=df.index, name=\"result\")\n```\n\n----------------------------------------\n\nTITLE: Iterating Over String Series with pandas.Series.str - Python\nDESCRIPTION: This snippet demonstrates the iteration over strings in a pandas Series, yielding a new Series at each iteration with characters or NaN as appropriate. Shows actual and expected output, including extracting the nth character across all elements, and testing string comparisons. Dependencies: pandas. Input is a string Series; output is a sequence of Series, each representing characters at a given position; illustrates new iteration support in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nstrs = \"go\", \"bow\", \"joe\", \"slow\"\nds = pd.Series(strs)\nfor s in ds.str:\n    print(s)\ns.dropna().values.item() == \"w\"\n```\n\n----------------------------------------\n\nTITLE: Using NA values in groupby keys with the dropna parameter\nDESCRIPTION: Example of using the new dropna parameter in DataFrame.groupby() to allow NA values in groupby keys. Setting dropna=False includes NA values as a separate group in the groupby operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf_list = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\ndf_dropna = pd.DataFrame(df_list, columns=[\"a\", \"b\", \"c\"])\n\ndf_dropna\n```\n\nLANGUAGE: python\nCODE:\n```\n# Default ``dropna`` is set to True, which will exclude NaNs in keys\ndf_dropna.groupby(by=[\"b\"], dropna=True).sum()\n\n# In order to allow NaN in keys, set ``dropna`` to False\ndf_dropna.groupby(by=[\"b\"], dropna=False).sum()\n```\n\n----------------------------------------\n\nTITLE: Reading Binary C Structs into pandas DataFrame - Python\nDESCRIPTION: Reads a binary file containing an array of C structs into a pandas DataFrame by specifying the correct dtype (names, offsets, formats). Dependencies: pandas, numpy. Input: 'binary.dat' file generated from C. Output: DataFrame with columns matching struct fields. Offsets and alignment must match binary layout, may not be portable across platforms.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nnames = \"count\", \"avg\", \"scale\"\n\n# note that the offsets are larger than the size of the type because of\n# struct padding\noffsets = 0, 8, 16\nformats = \"i4\", \"f8\", \"f4\"\ndt = np.dtype({\"names\": names, \"offsets\": offsets, \"formats\": formats}, align=True)\ndf = pd.DataFrame(np.fromfile(\"binary.dat\", dt))\n```\n\n----------------------------------------\n\nTITLE: Reindexing Series and DataFrame - pandas - Python\nDESCRIPTION: Reindexes a Series and DataFrame to new label sets, handling missing data (NaN) for new indices and supports reindexing both index and columns. Shows how to realign data containers for label-based alignment. Inputs: target labels; Output: reindexed Series/DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\ns\ns.reindex([\"e\", \"b\", \"f\", \"d\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ndf\ndf.reindex(index=[\"c\", \"f\", \"b\"], columns=[\"three\", \"two\", \"one\"])\n```\n\n----------------------------------------\n\nTITLE: Comparing DataFrames with Flexible Comparison Methods in Pandas Python\nDESCRIPTION: This snippet leverages the gt (greater than) and ne (not equal) methods for elementwise comparison between DataFrames. The resulting objects are DataFrames of boolean dtype, useful for subsequent filtering, masking, or conditional calculations. Key parameters are the comparison partners (DataFrames of the same shape).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.gt(df2)\ndf2.ne(df)\n```\n\n----------------------------------------\n\nTITLE: to_numeric Downcast Parameter for Efficient Dtype - Pandas - Python\nDESCRIPTION: Showcases pd.to_numeric() with the 'downcast' parameter, which allows conversion to the smallest unsigned or integer dtype possible. The snippets demonstrate converting a mixed-type list to numeric and downcasting to 'unsigned' and 'integer' dtypes, respectively. Only pandas is required; parameters are the data list and the downcast type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ns = [\"1\", 2, 3]\npd.to_numeric(s, downcast=\"unsigned\")\npd.to_numeric(s, downcast=\"integer\")\n```\n\n----------------------------------------\n\nTITLE: Boxplot with Column Subset and Multiple Grouping Variables\nDESCRIPTION: This example demonstrates how to create boxplots for a subset of columns grouped by multiple categorical variables. Only Col1 and Col2 are plotted, grouped by both X and Y columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(10, 3), columns=[\"Col1\", \"Col2\", \"Col3\"])\ndf[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\ndf[\"Y\"] = pd.Series([\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"])\n\nplt.figure();\n\nbp = df.boxplot(column=[\"Col1\", \"Col2\"], by=[\"X\", \"Y\"])\n```\n\n----------------------------------------\n\nTITLE: Exporting DataFrame to Stata - pandas - Python\nDESCRIPTION: Uses pandas' 'to_stata' DataFrame method to write 'tips' to a Stata-compatible .dta file ('tips2.dta'). Dependencies: pandas. Input: DataFrame. Output: Stata binary file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntips.to_stata(\"tips2.dta\")\n```\n\n----------------------------------------\n\nTITLE: IntervalIndex Membership Test using the 'in' Operator - Pandas - Python\nDESCRIPTION: Shows how the 'in' operator checks for interval containment in an IntervalIndex. This demonstrates two intervals being tested for containment, illustrating the updated exact-match behavior. Requires pandas, and the IntervalIndex instance 'ii' created in the prior example.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npd.Interval(1, 2, closed='neither') in ii\\npd.Interval(-10, 10, closed='both') in ii\n```\n\n----------------------------------------\n\nTITLE: Direct String Indexing in Pandas Series - Python\nDESCRIPTION: Demonstrates accessing specific characters in each string of a pandas Series using the .str accessor with integer-based indexing. If the specified position is out of bounds for a string, NaN is returned. Requires pandas and numpy, accepts integer indices, and returns a new Series with the extracted characters or NaN where not applicable.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(\n    [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\"\n)\n\ns.str[0]\ns.str[1]\n```\n\n----------------------------------------\n\nTITLE: Cumulative Sum Line Plotting for DataFrame (Python)\nDESCRIPTION: Initializes a DataFrame of random numbers with the same date index as ts and four columns, computes cumulative sum, then plots all columns using DataFrame.plot. Generates one figure displaying all columns' lines. Requires pandas, numpy, and matplotlib; the seed is set for reproducibility.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\"))\\ndf = df.cumsum()\\n\\nplt.figure();\\n@savefig frame_plot_basic.png\\ndf.plot();\n```\n\n----------------------------------------\n\nTITLE: New Behavior: Indexing Series Raises Consistent KeyError - ipython\nDESCRIPTION: Illustrates pandas 1.1.0's updated consistent error handling: all incompatible or missing key lookups on Series now raise KeyError, regardless of the key's type or the use of direct indexing or .loc. This extends to DatetimeIndex and unexpected key types, with clearer exceptions for users. No extra dependencies beyond pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: ser1[1.5]\\n...\\nKeyError: 1.5\\n\\nIn [4] ser1[\"foo\"]\\n...\\nKeyError: 'foo'\\n\\nIn [5]: ser1.loc[1.5]\\n...\\nKeyError: 1.5\\n\\nIn [6]: ser1.loc[\"foo\"]\\n...\\nKeyError: 'foo'\\n\\nIn [7]: ser2.loc[1]\\n...\\nKeyError: 1\\n\\nIn [8]: ser2.loc[pd.Timestamp(0)]\\n...\\nKeyError: Timestamp('1970-01-01 00:00:00')\n```\n\n----------------------------------------\n\nTITLE: Datetime Arithmetic with Explicit Frequency Multiples - Pandas - IPython\nDESCRIPTION: These snippets illustrate the new recommended method to shift pandas Timestamp, TimedeltaIndex, and DatetimeIndex objects using integer multiples of their frequency attribute. Rather than using raw integers for arithmetic, objects should be incremented or decremented using expression like `N * obj.freq`. Inputs are datetimelike objects and integer multiples, outputs are appropriately shifted objects. Standard pandas and numpy prerequisites apply. The approach ensures arithmetic is explicit and less error-prone, with consistent handling of frequency. Outputs match previous results but with improved clarity and future compatibility.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_45\n\nLANGUAGE: ipython\nCODE:\n```\nIn [108]: ts = pd.Timestamp('1994-05-06 12:15:16', freq=pd.offsets.Hour())\n\nIn[109]: ts + 2 * ts.freq\nOut[109]: Timestamp('1994-05-06 14:15:16', freq='H')\n\nIn [110]: tdi = pd.timedelta_range('1D', periods=2)\n\nIn [111]: tdi - np.array([2 * tdi.freq, 1 * tdi.freq])\nOut[111]: TimedeltaIndex(['-1 days', '1 days'], dtype='timedelta64[ns]', freq=None)\n\nIn [112]: dti = pd.date_range('2001-01-01', periods=2, freq='7D')\n\nIn [113]: dti + pd.Index([1 * dti.freq, 2 * dti.freq])\nOut[113]: DatetimeIndex(['2001-01-08', '2001-01-22'], dtype='datetime64[ns]', freq=None)\n```\n\n----------------------------------------\n\nTITLE: Using .values vs .array for Index Data - pandas (Python)\nDESCRIPTION: Highlights that Index.values returns a (possibly new) ndarray of objects (or array-specific type), while .array may yield an extension array or thin PandasArray wrapper. Shows object identity to illustrate .values returns a new object each time for some types, such as PeriodIndex. This is relevant for users migrating from .values to .array/.to_numpy() and underscores efficiency and semantics differences.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nidx.values\nid(idx.values)\nid(idx.values)\n```\n\n----------------------------------------\n\nTITLE: Three-Valued Logic with pd.NA in Logical Operations - pandas - Python\nDESCRIPTION: This snippet demonstrates the behavior of pandas' pd.NA scalar in boolean logical operations, specifically using logical OR with True. The result is pd.NA, reflecting three-valued logic (Kleene logic) support for missing values. Requires pandas 1.0.0 or higher, and shows propagation of 'unknown' in boolean expressions containing missing data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.NA | True\n```\n\n----------------------------------------\n\nTITLE: Implementing Else Logic with Conditional Assignment in Python\nDESCRIPTION: Implements the else part of an if-then-else construct by applying another condition. This technique complements the previous if-then example with alternative logic.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[df.AAA < 5, [\"BBB\", \"CCC\"]] = 2000\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating Sample DataFrames for Pandas Merge Examples in Python\nDESCRIPTION: This code snippet initializes two Pandas DataFrames (df1 and df2) with 'key' and 'value' columns. The 'key' column contains string values, while the 'value' column is populated with random numbers using numpy's random.randn() function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/merge_setup.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame({\"key\": [\"A\", \"B\", \"C\", \"D\"], \"value\": np.random.randn(4)})\ndf1\ndf2 = pd.DataFrame({\"key\": [\"B\", \"D\", \"D\", \"E\"], \"value\": np.random.randn(4)})\ndf2\n```\n\n----------------------------------------\n\nTITLE: DataFrame Sum Reductions with Extension Dtypes - pandas - IPython\nDESCRIPTION: Compares the behavior of DataFrame sum reductions before and after using PyArrow extension dtypes in pandas. Initially, the sum is performed on DataFrames with 'Int64' dtype, then with 'int64[pyarrow]'. Requires pandas, NumPy, and optional PyArrow. Demonstrates how summed Series results now preserve original extension dtypes rather than defaulting to NumPy types; input is DataFrame with extension dtypes, output is sum with preserved dtype. Shows both pre-2.1 and post-2.1 behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_1\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: df = pd.DataFrame({\"a\": [1, 1, 2, 1], \"b\": [np.nan, 2.0, 3.0, 4.0]}, dtype=\"Int64\")\nIn [2]: df.sum()\nOut[2]:\na    5\nb    9\ndtype: int64\nIn [3]: df = df.astype(\"int64[pyarrow]\")\nIn [4]: df.sum()\nOut[4]:\na    5\nb    9\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Custom Boolean Value Mapping in read_csv - pandas - Python\nDESCRIPTION: Reads CSV-formatted data where Yes and No are mapped to True and False using true_values and false_values parameters. The first output shows default reading, the second demonstrates custom mapping. Requires pandas and StringIO.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,c\\n1,Yes,2\\n3,No,4\"\nprint(data)\npd.read_csv(StringIO(data))\npd.read_csv(StringIO(data), true_values=[\"Yes\"], false_values=[\"No\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Series from NumPy Array and Index - pandas - Python\nDESCRIPTION: Illustrates creating a Series from a NumPy array with explicit index labels, and another Series with default integer index. 'np.random.randn(5)' generates five random numbers from the standard normal distribution. The index parameter provides custom labels, while omitting it results in default integer-based indexing. Outputs Series objects and displays their indices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\ns\ns.index\n\npd.Series(np.random.randn(5))\n```\n\n----------------------------------------\n\nTITLE: Using XML Stylesheet with to_xml Method in Pandas\nDESCRIPTION: Python code that demonstrates how to apply the XSLT stylesheet to a DataFrame using the to_xml method with StringIO to pass the stylesheet as a parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_133\n\nLANGUAGE: python\nCODE:\n```\nprint(geom_df.to_xml(stylesheet=StringIO(xsl)))\n```\n\n----------------------------------------\n\nTITLE: Method Chaining Without Redundant Copies Using Copy-on-Write in pandas (Python)\nDESCRIPTION: The code showcases method chaining and functional transformations with DataFrames, where methods such as `rename` and `set_index` return new objects. Under Copy-on-Write, these operations can avoid unnecessary data copying internally, optimizing performance. Dependencies: pandas. Inputs: DataFrame and string manipulation functions. Outputs: transformed DataFrame objects. No mutation of parent unless explicitly requested.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> df2 = df.rename(columns=str.lower)\n>>> df3 = df2.set_index(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to Excel File\nDESCRIPTION: Basic example of writing a pandas DataFrame to an Excel file using the to_excel method, specifying the file path and sheet name.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_142\n\nLANGUAGE: python\nCODE:\n```\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\")\n```\n\n----------------------------------------\n\nTITLE: MultiIndex Intersection with Preserved Order Using sort=False in Pandas (Python)\nDESCRIPTION: This snippet shows how to construct two MultiIndex objects from arrays and then intersect them with the intersection method, explicitly passing sort=False. After the fix, the order of elements in the result follows the left-hand side index, guaranteeing order preservation when requested. Requires pandas. Inputs are string and integer label arrays; output is their intersection in left-side order.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.MultiIndex.from_arrays([[\"b\", \"a\"], [2, 1]])\nright = pd.MultiIndex.from_arrays([[\"a\", \"b\", \"c\"], [1, 2, 3]])\n# Common elements are now guaranteed to be ordered by the left side\nleft.intersection(right, sort=False)\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Aggregation Logic per Group Using groupby.apply - Pandas - Python\nDESCRIPTION: Defines a 'GrowUp' function to compute a custom size, weight, and adult status per group, using weighted sums depending on size category. The groupby object applies this function, and outputs a DataFrame with the aggregated results. Inputs are animal size and weight data; output is a summarized series per group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndef GrowUp(x):\n    avg_weight = sum(x[x[\"size\"] == \"S\"].weight * 1.5)\n    avg_weight += sum(x[x[\"size\"] == \"M\"].weight * 1.25)\n    avg_weight += sum(x[x[\"size\"] == \"L\"].weight)\n    avg_weight /= len(x)\n    return pd.Series([\"L\", avg_weight, True], index=[\"size\", \"weight\", \"adult\"])\n\n\nexpected_df = gb.apply(GrowUp)\nexpected_df\n```\n\n----------------------------------------\n\nTITLE: Engineering Float Formatting for Numeric Series - Python\nDESCRIPTION: Demonstrates formatting floating point Series and DataFrame values in engineering notation using set_eng_float_format. Shows output scaling with engineering prefixes and specified accuracy. Useful for scientific applications where SI units and scaling are relevant. Requires pandas, numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\npd.set_eng_float_format(accuracy=3, use_eng_prefix=True)\ns = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\ns / 1.0e3\ns / 1.0e6\n```\n\n----------------------------------------\n\nTITLE: Creating and Appending DataFrames for Query Examples - pandas - Python\nDESCRIPTION: Sets up a DataFrame with a date index and multiple columns, and appends it to an HDFStore with table format and data_columns enabled, making it ready for querying with select. Prepares data for subsequent conditional selection examples. Requires pandas and numpy; outputs a stored table with full query support.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_178\n\nLANGUAGE: python\nCODE:\n```\ndfq = pd.DataFrame(\n    np.random.randn(10, 4),\n    columns=list(\"ABCD\"),\n    index=pd.date_range(\"20130101\", periods=10),\n)\nstore.append(\"dfq\", dfq, format=\"table\", data_columns=True)\n```\n\n----------------------------------------\n\nTITLE: Dropping Rows with Missing Values Using pandas.DataFrame.dropna - Python\nDESCRIPTION: This code demonstrates how to remove all rows from a pandas DataFrame that contain at least one missing value (NaN), using the dropna() method. The input is a DataFrame ('outer_join'); the method returns a new DataFrame that contains only complete rows, with missing data rows excluded. Dependency: pandas library. Ensure the method is called on a DataFrame object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/missing.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nouter_join.dropna()\n```\n\n----------------------------------------\n\nTITLE: Unstacking a DataFrame in Python\nDESCRIPTION: Demonstrates how to unstack a DataFrame and fill missing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf3\ndf3.unstack()\n\ndf3.unstack(fill_value=-1e9)\n```\n\n----------------------------------------\n\nTITLE: Documenting Function Parameters in Python Docstrings\nDESCRIPTION: Outlines the correct methodology for documenting function parameters using a structured Parameters section, adhering to pandas and NumPy conventions. The example shows the method 'plot' in a 'Series' class, complete with short and extended summaries, descriptions for each parameter and use of the appropriate keywords to indicate defaults and arbitrary keyword arguments. No external dependencies, but assumes user will pass valid types. Useful for ensuring generated documentation is clear and precise.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass Series:\n    def plot(self, kind, color='blue', **kwargs):\n        \"\"\"\n        Generate a plot.\n\n        Render the data in the Series as a matplotlib plot of the\n        specified kind.\n\n        Parameters\n        ----------\n        kind : str\n            Kind of matplotlib plot.\n        color : str, default 'blue'\n            Color name or rgb code.\n        **kwargs\n            These parameters will be passed to the matplotlib plotting\n            function.\n        \"\"\"\n        pass\n```\n\n----------------------------------------\n\nTITLE: Resetting Multiple pandas Options using Regex - Python\nDESCRIPTION: Resets all pandas options matching the regular expression '^display' to their defaults. This is useful for bulk resetting related options. Use with caution as it affects multiple settings that control DataFrame/Series display.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npd.reset_option(\"^display\")\n```\n\n----------------------------------------\n\nTITLE: Forward and Backward Fill in Pandas\nDESCRIPTION: Shows how to fill NA values using forward fill (ffill) and backward fill (bfill) methods, including limiting the number of fills.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf.ffill()\ndf.bfill()\ndf.ffill(limit=1)\n```\n\n----------------------------------------\n\nTITLE: Grouping Data Using SQL Syntax (SQL)\nDESCRIPTION: Demonstrates a SQL query that uses GROUP BY to compute aggregated statistics (mean and sum) by two columns. This sets the conceptual foundation for similar operations in pandas, facilitating users' understanding of GroupBy functionality by analogy. No dependencies; it illustrates SQL syntax relevant to later pandas examples.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT Column1, Column2, mean(Column3), sum(Column4)\nFROM SomeTable\nGROUP BY Column1, Column2\n```\n\n----------------------------------------\n\nTITLE: New Behavior: DataFrame Right Merge Preserves Right Frame Order - python\nDESCRIPTION: Shows that, with the updated pandas, right merge now preserves the right DataFrame's row order when combining with the left on specified columns. This snippet merges left_df and right_df and retains the right_df's row order in the output. Assumes both DataFrames are defined as above.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nleft_df.merge(right_df, on=['animal', 'max_speed'], how=\"right\")\n```\n\n----------------------------------------\n\nTITLE: Reindexing DataFrames Using Axis Argument - pandas - Python\nDESCRIPTION: Demonstrates axis-style reindexing, allowing reindexing by labels along either rows or columns. Input: labels and axis; Output: DataFrame reordered and/or with additional rows/columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndf.reindex([\"c\", \"f\", \"b\"], axis=\"index\")\ndf.reindex([\"three\", \"two\", \"one\"], axis=\"columns\")\n```\n\n----------------------------------------\n\nTITLE: Reading Data from an Excel File into a pandas DataFrame in Python\nDESCRIPTION: Imports data from the 'titanic.xlsx' Excel file, specifically from the 'passengers' worksheet, back into a pandas DataFrame named 'titanic'. Requires pandas and an Excel-supporting library (e.g., openpyxl). Key parameters: file path and sheet name. The DataFrame will mirror the spreadsheet's data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntitanic = pd.read_excel(\"titanic.xlsx\", sheet_name=\"passengers\")\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to HTML with Row Label Formatting in Python\nDESCRIPTION: Demonstrates how to control the formatting of row labels when converting a DataFrame to HTML using the bold_rows parameter of to_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_106\n\nLANGUAGE: python\nCODE:\n```\nhtml = df.to_html(bold_rows=False)\nprint(html)\ndisplay(HTML(html))\n```\n\n----------------------------------------\n\nTITLE: Listing Available Plot Methods for a pandas DataFrame in Python\nDESCRIPTION: This snippet uses a list comprehension with the built-in dir() function to retrieve all public methods of the DataFrame.plot accessor, excluding any method names starting with an underscore. The result is a list of available plot types (e.g., 'box', 'area', etc.). No external dependencies beyond pandas; input is any DataFrame, output is a list of method strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n[\n    method_name\n    for method_name in dir(air_quality.plot)\n    if not method_name.startswith(\"_\")\n]\n```\n\n----------------------------------------\n\nTITLE: Using in and not in Operators in pandas.DataFrame.query - Python\nDESCRIPTION: Displays the use of 'in' and 'not in' operators in query strings for succinct membership testing, equivalent to Series.isin(), including comparison with their pure Python equivalents. Demonstrates combining with other expressions for more complex row filtering. Requires pandas and numpy. Inputs: DataFrame; Output: Filtered DataFrame(s).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': list('aabbccddeeff'), 'b': list('aaaabbbbcccc'),\n                   'c': np.random.randint(5, size=12),\n                   'd': np.random.randint(9, size=12)})\ndf\ndf.query('a in b')\n\ndf[df['a'].isin(df['b'])]\n\ndf.query('a not in b')\n\ndf[~df['a'].isin(df['b'])]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.query('a in b and c < d')\n\ndf[df['b'].isin(df['a']) & (df['c'] < df['d'])]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating DataFrame.apply Output Consistency in pandas (Python)\nDESCRIPTION: Shows how pandas DataFrame.apply produces different result types depending on the return value of the user-supplied function. Highlights the change whereby list-like returns now always produce a Series, unless result_type is set to 'expand' or 'broadcast'. Demonstrates key behavior using sample DataFrame transformations, including returning Series to control output structure. Requires pandas and numpy. Input is a DataFrame and a lambda function; output depends on the type of value the lambda returns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.tile(np.arange(3), 6).reshape(6, -1) + 1,\n                  columns=['A', 'B', 'C'])\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.apply(lambda x: [1, 2, 3], axis=1)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.apply(lambda x: [1, 2], axis=1)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.apply(lambda x: [1, 2, 3], axis=1, result_type='expand')\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.apply(lambda x: [1, 2, 3], axis=1, result_type='broadcast')\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.apply(lambda x: pd.Series([1, 2, 3], index=['D', 'E', 'F']), axis=1)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating pivot_table consistent DataFrame return in Pandas\nDESCRIPTION: This example shows how pivot_table now always returns a DataFrame even in cases where it previously returned a Series, fixing inconsistent behavior as documented in issue #4386.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_40\n\nLANGUAGE: ipython\nCODE:\n```\ndf = pd.DataFrame({'col1': [3, 4, 5],\n                 'col2': ['C', 'D', 'E'],\n                 'col3': [1, 3, 9]})\ndf\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: df.pivot_table('col1', index=['col3', 'col2'], aggfunc=\"sum\")\nOut[2]:\ncol3  col2\n1     C       3\n3     D       4\n9     E       5\nName: col1, dtype: int64\n```\n\nLANGUAGE: ipython\nCODE:\n```\ndf.pivot_table('col1', index=['col3', 'col2'], aggfunc=\"sum\")\n```\n\n----------------------------------------\n\nTITLE: Creating Unstacked Area Plot in Pandas\nDESCRIPTION: This example demonstrates how to create an unstacked area plot by setting stacked=False. Each area is plotted independently with an alpha transparency of 0.5 by default.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf.plot.area(stacked=False);\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Pandas Development\nDESCRIPTION: Commands to create and activate a conda environment for Pandas development using the provided environment.yml file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda env create --file environment.yml\nconda activate pandas-dev\n```\n\n----------------------------------------\n\nTITLE: Assigning Period end_time consistently with Series.dt.end_time in pandas (Python)\nDESCRIPTION: Shows new, aligned behavior where both Series.dt.end_time and Period.end_time use '23:59:59.999999999', matching pandas Period implementations for improved consistency. Dependencies: pandas imported as pd. Inputs are Period and PeriodIndex; output is a Timestamp with nanosecond precision.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\np = pd.Period('2017-01-01', 'D')\npi = pd.PeriodIndex([p])\n\npd.Series(pi).dt.end_time[0]\n\np.end_time\n```\n\n----------------------------------------\n\nTITLE: Series Default Dtype Deprecation in Pandas 1.0.0\nDESCRIPTION: Example showing the deprecation warning for empty Series dtype changing from float64 to object in future releases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_26\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pd.Series()\nOut[2]:\nDeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\nSeries([], dtype: float64)\n```\n\n----------------------------------------\n\nTITLE: Accelerating Numeric and Logical Operations on Arrow-Backed Series (pandas Python API)\nDESCRIPTION: Provides examples of arithmetic, aggregation, and missing data handling on Arrow-backed Series in pandas, showing how operations like mean, addition, comparison, and missing data treatment are accelerated by underlying Arrow compute functions when PyArrow dtypes are used. Requires pandas and pyarrow. Code operates on numeric Series with PyArrow dtype and outputs results such as computed means, sums, conditions, and filled/cleaned Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nser = pd.Series([-1.545, 0.211, None], dtype=\"float32[pyarrow]\")\nser.mean()\nser + ser\nser > (ser + 1)\n\nser.dropna()\nser.isna()\nser.fillna(0)\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for numeric data functions\nDESCRIPTION: RestructuredText directive listing Pandas numeric data conversion function\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/general_functions.rst#_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n\n   to_numeric\n```\n\n----------------------------------------\n\nTITLE: Using pandas_gbq for Google BigQuery Operations in Python\nDESCRIPTION: Demonstrates the new way to read from and write to Google BigQuery using the pandas_gbq library instead of the removed read_gbq and to_gbq methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v3.0.0.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\npandas_gbq.read_gbq\npandas_gbq.to_gbq\n```\n\n----------------------------------------\n\nTITLE: Exclusively Defining NA Indicator as Empty Strings - pandas - Python\nDESCRIPTION: Reads a CSV file and only treats empty strings as NaN, overriding pandas' default list of missing value indicators, by using keep_default_na=False. Requires pandas and a CSV input file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(\"path_to_file.csv\", keep_default_na=False, na_values=[\"\"])\n```\n\n----------------------------------------\n\nTITLE: Renaming Labels and Axes (rename, rename_axis, set_names) - pandas - Python\nDESCRIPTION: Demonstrates renaming columns, index labels, and axis names with rename, rename_axis, and Index.set_names. Supports dictionary or function mappings, affects both index and column levels. Useful for standardizing outputs or preparing for flattening MultiIndex with reset_index. Requires pandas DataFrames or MultiIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf.rename(columns={0: \"col0\", 1: \"col1\"})\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.rename(index={\"one\": \"two\", \"y\": \"z\"})\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.rename_axis(index=[\"abc\", \"def\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.rename_axis(columns=\"Cols\").columns\n```\n\nLANGUAGE: python\nCODE:\n```\nmi = pd.MultiIndex.from_product([[1, 2], [\"a\", \"b\"]], names=[\"x\", \"y\"])\nmi.names\nmi2 = mi.rename(\"new name\", level=0)\nmi2\n```\n\nLANGUAGE: python\nCODE:\n```\nmi.levels[0].name = \"name via level\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrating GroupBy Tab Completion in pandas (Python)\nDESCRIPTION: Constructs a demo DataFrame with random numerical and categorical data, groups it by 'gender', and assigns the groupby object to 'gb'. This sets up the context for showing which GroupBy attributes, columns, and methods tab-complete in an interactive session. pandas (as pd) and numpy (as np) must be imported. Code is suitable for use in a Python shell or notebook.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nn = 10\nweight = np.random.normal(166, 20, size=n)\nheight = np.random.normal(60, 10, size=n)\ntime = pd.date_range(\"1/1/2000\", periods=n)\ngender = np.random.choice([\"male\", \"female\"], size=n)\ndf = pd.DataFrame(\n    {\"height\": height, \"weight\": weight, \"gender\": gender}, index=time\n)\ndf\ngb = df.groupby(\"gender\")\n```\n\n----------------------------------------\n\nTITLE: Creating SparseDtype with Explicit Fill Value - pandas - Python\nDESCRIPTION: Initializes a SparseDtype for datetime64[ns], explicitly setting the fill value to a custom pandas Timestamp (e.g., '2017-01-01'). Useful for applications where a non-default or sentinel fill value is required for missing data. Requires pandas. The result is a SparseDtype configured for downstream sparse data structures.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npd.SparseDtype(np.dtype('datetime64[ns]'),\n                 fill_value=pd.Timestamp('2017-01-01'))\n```\n\n----------------------------------------\n\nTITLE: Importing pandas Library - Python\nDESCRIPTION: This snippet imports the pandas library as pd, which is the standard alias used for pandas in Python data analysis workflows. No external dependencies are required besides having pandas installed. This is a prerequisite step for using pandas DataFrame and Series objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Indexing DataFrames with MultiIndex Using .loc in Pandas (Python)\nDESCRIPTION: This snippet demonstrates how to index a Pandas DataFrame with a MultiIndex composed of string and integer labels using the .loc accessor. It shows that rows are now returned in the order of the provided keys, reflecting the bugfix that ensures requested ordering. Requires pandas and numpy libraries. The input lists control the MultiIndex structure, and the .loc selection outputs rows in the specified key order.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.arange(4),\n                  index=[[\"a\", \"a\", \"b\", \"b\"], [1, 2, 1, 2]])\n# Rows are now ordered as the requested keys\ndf.loc[(['b', 'a'], [2, 1]), :]\n```\n\n----------------------------------------\n\nTITLE: Loading Pickled Objects in Pandas\nDESCRIPTION: Uses the pandas.read_pickle function to load a previously pickled pandas object from file. This restores the entire data structure exactly as it was saved.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_156\n\nLANGUAGE: python\nCODE:\n```\npd.read_pickle(\"foo.pkl\")\n```\n\n----------------------------------------\n\nTITLE: Performing Boolean and Bitwise Operations on Pandas DataFrames (Python)\nDESCRIPTION: Demonstrates the creation of boolean-typed pandas DataFrames and the use of bitwise logical operations such as AND, OR, XOR, and NOT. Dependencies: pandas. df1 and df2 serve as input DataFrames, and the operations df1 & df2, df1 | df2, df1 ^ df2, and -df1 yield DataFrames resulting from element-wise logical operations. All inputs and outputs are pandas DataFrames with boolean dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame({\"a\": [1, 0, 1], \"b\": [0, 1, 1]}, dtype=bool)\ndf2 = pd.DataFrame({\"a\": [0, 1, 1], \"b\": [1, 1, 0]}, dtype=bool)\ndf1 & df2\ndf1 | df2\ndf1 ^ df2\n-df1\n```\n\n----------------------------------------\n\nTITLE: Optimizing DataFrame Memory by Reassigning with CoW in pandas (Python)\nDESCRIPTION: This snippet illustrates improved performance by reassigning the result of reset_index to the same DataFrame variable, thus invalidating the old reference and avoiding unnecessary copies on modification. It shows that when df is reassigned, no other objects hold a reference to the original data, so subsequent inplace changes do not require a copy. Prerequisite: pandas imported as pd. Key aspects: the use of reset_index (drop=True) and iloc for mutation. This encourages best practices for minimizing memory overhead in workflows relying on Copy-on-Write.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/copy_on_write.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\ndf = df.reset_index(drop=True)\ndf.iloc[0, 0] = 100\n```\n\n----------------------------------------\n\nTITLE: Comparing DataFrames with compare - pandas - Python\nDESCRIPTION: This snippet sets up two DataFrames with minor differences and suggests using DataFrame.compare to highlight and summarize the differences between them. It creates example DataFrames containing small value/label variations for demonstration. pandas is required, and the expected output is a succinct DataFrame showing where and how the inputs differ.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"],\n        \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0],\n        \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0],\n    },\n    columns=[\"col1\", \"col2\", \"col3\"],\n)\ndf\ndf2 = df.copy()\ndf2.loc[0, \"col1\"] = \"c\"\n```\n\n----------------------------------------\n\nTITLE: Type Conversion and Mixed Type Handling - Pandas - Python\nDESCRIPTION: Converts DataFrame df3 to a specified dtype ('float32') for all columns using astype, then demonstrates column-specific conversions (e.g., float16, int32). Also includes adding string columns and performing numeric object conversion using convert_objects with parameters such as convert_numeric=True. Highlights DataFrame/Series conversion options and how they affect final dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf3.astype('float32').dtypes\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [12]: df3['D'] = '1.'\n\nIn [13]: df3['E'] = '1'\n\nIn [14]: df3.convert_objects(convert_numeric=True).dtypes\nOut[14]:\nA    float32\nB    float64\nC    float64\nD    float64\nE      int64\ndtype: object\n\n# same, but specific dtype conversion\nIn [15]: df3['D'] = df3['D'].astype('float16')\n\nIn [16]: df3['E'] = df3['E'].astype('int32')\n\nIn [17]: df3.dtypes\nOut[17]:\nA    float32\nB    float64\nC    float64\nD    float16\nE      int32\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Dropping Duplicates in Series with Keep Argument using pandas - Python\nDESCRIPTION: Demonstrates use of the keep argument in Series.drop_duplicates to retain first, last, or no duplicates in the result. This replaces the previous take_last keyword. Requires pandas; input is a Series possibly with duplicates. Output is a Series filtered per the chosen keep strategy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"A\", \"B\", \"C\", \"A\", \"B\", \"D\"])\ns.drop_duplicates()\ns.drop_duplicates(keep=\"last\")\ns.drop_duplicates(keep=False)\n```\n\n----------------------------------------\n\nTITLE: Default Compression Inference in Pandas Pickle Operations\nDESCRIPTION: Demonstrates that the default behavior for to_pickle and read_pickle is to infer compression from file extension for both DataFrame and Series objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_160\n\nLANGUAGE: python\nCODE:\n```\ndf.to_pickle(\"data.pkl.gz\")\nrt = pd.read_pickle(\"data.pkl.gz\")\nrt\n\ndf[\"A\"].to_pickle(\"s1.pkl.bz2\")\nrt = pd.read_pickle(\"s1.pkl.bz2\")\nrt\n```\n\n----------------------------------------\n\nTITLE: Batch Querying Multiple DataFrames with Shared Columns - Python\nDESCRIPTION: Shows a use case for DataFrame.query across multiple DataFrames with overlapping columns. Uses map and query to apply a filter expression on multiple frames simultaneously. Requires pandas and numpy. Filters frames where 'a', 'c' satisfy 0.0 <= a <= c <= 0.5. Inputs: Multiple DataFrames; Output: Iterator of DataFrames filtered by a common query.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\ndf\ndf2 = pd.DataFrame(np.random.rand(n + 2, 3), columns=df.columns)\ndf2\nexpr = '0.0 <= a <= c <= 0.5'\nmap(lambda frame: frame.query(expr), [df, df2])\n```\n\n----------------------------------------\n\nTITLE: Calculating DataFrame Memory Usage with info() and memory_usage() - pandas - Python\nDESCRIPTION: Shows creation of a DataFrame with various dtypes and demonstrates methods to check memory usage via info() and memory_usage(). Requires pandas and numpy. Key parameters include data types and row count; outputs are printed memory diagnostics. Assumes presence of columns specified in the dtype list and pandas 0.15.0 or later for memory usage options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndtypes = ['int64', 'float64', 'datetime64[ns]', 'timedelta64[ns]',\\n          'complex128', 'object', 'bool']\\nn = 5000\\ndata = {t: np.random.randint(100, size=n).astype(t) for t in dtypes}\\ndf = pd.DataFrame(data)\\ndf['categorical'] = df['object'].astype('category')\\n\\ndf.info()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.memory_usage(index=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Extension Arrays with pandas.array API - pandas (Python)\nDESCRIPTION: Demonstrates the new pandas.array top-level method for constructing 1D extension arrays, allowing explicit control over extension types (e.g., Int64, category). Requires pandas >=0.24.0 and NumPy for NaN. Inputs are standard Python sequences and the dtype; outputs are extension or categorical arrays. Highlights potential differences in result depending on provided dtype and handling of NaN.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npd.array([1, 2, np.nan], dtype='Int64')\npd.array(['a', 'b', 'c'], dtype='category')\n```\n\n----------------------------------------\n\nTITLE: Fixing TimedeltaIndex Division\nDESCRIPTION: Corrects the behavior of TimedeltaIndex division to return a Float64Index instead of raising a TypeError when dividing by another TimedeltaIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_55\n\nLANGUAGE: Python\nCODE:\n```\ntimedelta_index1 / timedelta_index2\n```\n\n----------------------------------------\n\nTITLE: Profiling DataFrame Apply with Typed Cython Function - Python\nDESCRIPTION: Runs a timeit benchmark for the DataFrame.apply approach using 'integrate_f_typed', which is the C-typed Cython implementation. This demonstrates the performance achieved by type annotations. Requires previous successful compilation. Outputs execution time for further comparison.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%timeit df.apply(lambda x: integrate_f_typed(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n```\n\n----------------------------------------\n\nTITLE: Inserting and Reading via ADBC Driver with pandas to_sql/read_sql (Python)\nDESCRIPTION: Provides an end-to-end example of using Apache Arrow ADBC DBAPI driver for round-trip database operations with pandas. Demonstrates DataFrame creation, inserting into PostgreSQL, and reading with round-trip support. Requires the adbc_driver_postgresql package, a running PostgreSQL server, and pandas. Inputs are DataFrame and SQL table name; outputs are data persisted into and loaded from the database.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_2\n\nLANGUAGE: ipython\nCODE:\n```\nimport adbc_driver_postgresql.dbapi as pg_dbapi\n\ndf = pd.DataFrame(\n    [\n        [1, 2, 3],\n        [4, 5, 6],\n    ],\n    columns=['a', 'b', 'c']\n)\nuri = \"postgresql://postgres:postgres@localhost/postgres\"\nwith pg_dbapi.connect(uri) as conn:\n    df.to_sql(\"pandas_table\", conn, index=False)\n\n# for round-tripping\nwith pg_dbapi.connect(uri) as conn:\n    df2 = pd.read_sql(\"pandas_table\", conn)\n```\n\n----------------------------------------\n\nTITLE: Calling Maximum Performance Cython ndarray Function from Python - Python\nDESCRIPTION: Measures the performance of the fully optimized, boundscheck- and wraparound-disabled Cython function on DataFrame numpy arrays. This demonstrates the fastest implementation using explicit C array types. Inputs must be validated to prevent segfaults due to unchecked indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%timeit apply_integrate_f_wrap(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy())\n```\n\n----------------------------------------\n\nTITLE: Right-Side String Splitting with rsplit (Python)\nDESCRIPTION: Uses .str.rsplit() with parameters expand=True and n=1 to split starting from the right, useful for extracting last fields. Input is delimited string Series; output is DataFrame split at rightmost occurrence. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ns2.str.rsplit(\"_\", expand=True, n=1)\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to Parquet File using pandas with pyarrow/fastparquet (Python)\nDESCRIPTION: Writes a pandas DataFrame to Parquet files using both 'pyarrow' and 'fastparquet' engines. Each call produces a Parquet file. Requires pandas, and either pyarrow or fastparquet installed. Ensures compatibility and allows for benchmarking or comparison across both engines.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_207\n\nLANGUAGE: python\nCODE:\n```\ndf.to_parquet(\"example_pa.parquet\", engine=\"pyarrow\")\ndf.to_parquet(\"example_fp.parquet\", engine=\"fastparquet\")\n```\n\n----------------------------------------\n\nTITLE: Reading Specific Columns from Parquet File using pandas (Python)\nDESCRIPTION: Reads only selected columns ('a' and 'b') from Parquet files using pandas with both 'fastparquet' and 'pyarrow' engines. Output includes just these columns from the DataFrame and their dtypes. Requires the relevant engine library and the respective Parquet file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_210\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.read_parquet(\n    \"example_fp.parquet\",\n    engine=\"fastparquet\",\n    columns=[\"a\", \"b\"],\n)\nresult = pd.read_parquet(\n    \"example_pa.parquet\",\n    engine=\"pyarrow\",\n    columns=[\"a\", \"b\"],\n)\nresult.dtypes\n```\n\n----------------------------------------\n\nTITLE: Cleanup: Removing Parquet Files using os (Python)\nDESCRIPTION: Deletes the files 'example_pa.parquet' and 'example_fp.parquet' to clean up after Parquet examples. Uses os.remove and will raise exceptions if files are missing. Requires the os Python module.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_211\n\nLANGUAGE: python\nCODE:\n```\nos.remove(\"example_pa.parquet\")\nos.remove(\"example_fp.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Including Grouping Columns in Aggregated Output with pandas GroupBy in Python\nDESCRIPTION: Illustrates how to include the grouping columns themselves ('A' and 'B') in the aggregated results when using pandas GroupBy and summing. This can be used to retain group keys in the output. Requires pandas and a DataFrame containing 'A' and 'B'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ngrouped[[\"A\", \"B\"]].sum()\n```\n\n----------------------------------------\n\nTITLE: Column Filtering on Data Retrieval - pandas - Python\nDESCRIPTION: Demonstrates how to retrieve a subset of columns from a stored DataFrame in an HDFStore using the columns parameter or an equivalent query string. Useful for efficiently loading only selected columns. Requires pandas; outputs a DataFrame with filtered columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_180\n\nLANGUAGE: python\nCODE:\n```\nstore.select(\"df\", \"columns=['A', 'B']\")\n```\n\n----------------------------------------\n\nTITLE: Replacing Values Using Dictionary Mapping\nDESCRIPTION: Demonstrates how to replace categorical values using a dictionary mapping with the replace method. The example converts 'male' to 'M' and 'female' to 'F' in the Sex column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/10_text_data.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Sex_short\"] = titanic[\"Sex\"].replace({\"male\": \"M\", \"female\": \"F\"})\ntitanic[\"Sex_short\"]\n```\n\n----------------------------------------\n\nTITLE: Exporting DataFrame to CSV with to_csv - pandas - Python\nDESCRIPTION: Uses the pandas DataFrame method 'to_csv' to write the 'tips' DataFrame to a CSV file ('tips2.csv'). If the file exists it will be overwritten. Input: DataFrame; Output: CSV file on disk. pandas is required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ntips.to_csv(\"tips2.csv\")\n```\n\n----------------------------------------\n\nTITLE: Interactive Styler with IPython Widgets in Pandas (Python)\nDESCRIPTION: This snippet integrates the pandas Styler with ipywidgets to create an interactive function for adjusting gradient color mapping in a DataFrame. It imports widgets from ipywidgets and uses seaborn's diverging_palette for dynamic color mapping. Required dependencies include pandas, seaborn, and ipywidgets. Inputs are controlled interactively (h_neg, h_pos, s, l_post), and the output is an updated styled DataFrame with a background gradient.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom ipywidgets import widgets\n\n\n@widgets.interact\ndef f(h_neg=(0, 359, 1), h_pos=(0, 359), s=(0.0, 99.9), l_post=(0.0, 99.9)):\n    return df2.style.background_gradient(\n        cmap=sns.palettes.diverging_palette(\n            h_neg=h_neg, h_pos=h_pos, s=s, l=l_post, as_cmap=True\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Boolean Arithmetic Operations Warning Example\nDESCRIPTION: Demonstrates new warnings and errors for arithmetic operations with boolean dtypes in Pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> x = pd.Series(np.random.rand(10) > 0.5)\n>>> y = True\n>>> x + y  # warning generated: should do x | y instead\nUserWarning: evaluating in Python space because the '+' operator is not\nsupported by numexpr for the bool dtype, use '|' instead\n>>> x / y  # this raises because it doesn't make sense\nNotImplementedError: operator '/' not implemented for bool dtypes\n```\n\n----------------------------------------\n\nTITLE: Timing Arithmetic Operations on DataFrames - Python\nDESCRIPTION: Benchmarks the performance of adding four large DataFrames using standard Python operators. Uses the %timeit IPython magic to measure the execution time. Requires an IPython environment, pandas, and numpy. The sample demonstrates how native Python arithmetic performs as a baseline for comparison with pandas.eval.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n%timeit df1 + df2 + df3 + df4\n```\n\n----------------------------------------\n\nTITLE: Comparing Expanding and Full-Length Rolling Means - pandas - Python\nDESCRIPTION: This code compares expanding versus full-length rolling means for a DataFrame of size 5, demonstrating that both approaches yield identical results when min_periods=1. Dependencies: pandas. Inputs: DataFrame, window size, rolling/expanding method; Output: DataFrame with mean values calculated up to each index position. Useful for understanding the relationship between expanding and rolling window semantics.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(range(5))\ndf.rolling(window=len(df), min_periods=1).mean()\ndf.expanding(min_periods=1).mean()\n```\n\n----------------------------------------\n\nTITLE: Creating Series with Dense Index from COO Matrix using dense_index=True - pandas - Python\nDESCRIPTION: Creates a pandas Series from a scipy.sparse.coo_matrix with dense_index=True, resulting in a Series indexed by the full cartesian product of row and column labels. This may consume significantly more memory for highly sparse matrices. Dependencies: pandas, scipy. Output is a densely indexed Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nss_dense = pd.Series.sparse.from_coo(A, dense_index=True)\nss_dense\n```\n\n----------------------------------------\n\nTITLE: Querying DataFrame dtypes Pandas Python\nDESCRIPTION: Shows how to inspect a DataFrame's dtypes to observe column data types before and after assignment or behavioral changes. Requires pandas. This prints or queries the data types of all columns in df via df.dtypes, reporting the dtype (e.g., float64, int64). Used to show dtype changes or retention.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Managing and Assigning Datetime Columns with NaN/NaT - Pandas - Python\nDESCRIPTION: Demonstrates creating DataFrames with datetime columns, setting specific values to np.nan (which internally become NaT), and inspecting dtype assignments. Shows usage of pd.Timestamp for consistent datetime columns and the default handling of missing/not-a-time values. Requires pandas, NumPy, and demonstrates property .dtypes usage and inspection of dtype frequencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(6, 2), pd.date_range('20010102', periods=6),\n                 columns=['A', ' B'])\ndf['timestamp'] = pd.Timestamp('20010103')\ndf\n\n# datetime64[ns] out of the box\ndf.dtypes.value_counts()\n\n# use the traditional nan, which is mapped to NaT internally\ndf.loc[df.index[2:4], ['A', 'timestamp']] = np.nan\ndf\n```\n\n----------------------------------------\n\nTITLE: Examining MultiIndex Levels after Concatenation in Python\nDESCRIPTION: Illustrates how to inspect the levels of a MultiIndex that was created during DataFrame concatenation with keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresult.index.levels\n```\n\n----------------------------------------\n\nTITLE: Index Metadata Descriptor for RangeIndex in pandas-Parquet - Python\nDESCRIPTION: This Python snippet provides the schema for storing a pandas RangeIndex as metadata in Parquet files, eliminating the need for serialization as a data column. It includes properties such as kind, name, start, stop, and step to fully describe the index. Requires pandas and a RangeIndex instance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/developer.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n   index = pd.RangeIndex(0, 10, 2)\n   {\n       \"kind\": \"range\",\n       \"name\": index.name,\n       \"start\": index.start,\n       \"stop\": index.stop,\n       \"step\": index.step,\n   }\n```\n\n----------------------------------------\n\nTITLE: Renaming Categorical Categories with Dict in Python\nDESCRIPTION: Demonstrates the new ability to use a dict-like argument for new_categories when renaming Categorical categories. The previous categories are looked up in the dictionary's keys and replaced if found.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nc = pd.Categorical(['a', 'a', 'b'])\nc.rename_categories({\"a\": \"eh\", \"b\": \"bee\"})\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Docstringed Function in Python\nDESCRIPTION: Demonstrates how to properly use a multi-line docstring to document a function, following the pandas and NumPy conventions. Dependencies include standard Python and understanding of docstring sections such as Parameters, Returns, See Also, and Examples. The function 'add' takes two integers and returns their sum, providing clear parameter and return documentation, as well as cross-references to related functions and usage examples. Inputs: two integers; Output: their sum. Shows format suitable for automatic documentation generation with Sphinx.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef add(num1, num2):\n    \"\"\"\n    Add up two integer numbers.\n\n    This function simply wraps the ``+`` operator, and does not\n    do anything interesting, except for illustrating what\n    the docstring of a very simple function looks like.\n\n    Parameters\n    ----------\n    num1 : int\n        First number to add.\n    num2 : int\n        Second number to add.\n\n    Returns\n    -------\n    int\n        The sum of ``num1`` and ``num2``.\n\n    See Also\n    --------\n    subtract : Subtract one integer from another.\n\n    Examples\n    --------\n    >>> add(2, 2)\n    4\n    >>> add(25, 0)\n    25\n    >>> add(10, -10)\n    0\n    \"\"\"\n    return num1 + num2\n```\n\n----------------------------------------\n\nTITLE: Converting SparseArray to Dense ndarray with numpy - Python\nDESCRIPTION: Converts a pandas SparseArray object back into a dense NumPy ndarray using numpy.asarray. This is useful when dense computation or export is needed after working efficiently with sparse objects. Dependencies: numpy, pandas. Input: a pandas SparseArray. Output: a regular NumPy ndarray containing all data, including fill values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnp.asarray(sparr)\n```\n\n----------------------------------------\n\nTITLE: Data Alignment with MultiIndex in Python\nDESCRIPTION: Demonstrates operations between differently-indexed Series with MultiIndex, showing how data alignment works with hierarchical indices in addition and reindexing operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ns + s[:-2]\ns + s[::2]\n```\n\n----------------------------------------\n\nTITLE: Storing Attributes in HDFStore with pandas - Python\nDESCRIPTION: Shows how to store arbitrary Python objects as attributes in an HDFStore object with pandas. Dependencies: pandas, numpy. Input: DataFrame to be stored and attribute to attach. Output: attribute can be accessed through HDFStore's storer. Note: requires HDF5 support, file IO, and works with pickle for arbitrary objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(8, 3))\nstore = pd.HDFStore(\"test.h5\")\nstore.put(\"df\", df)\n\n# you can store an arbitrary Python object via pickle\nstore.get_storer(\"df\").attrs.my_attribute = {\"A\": 10}\nstore.get_storer(\"df\").attrs.my_attribute\n```\n\n----------------------------------------\n\nTITLE: Writing Concise Short Summaries in Python Function Docstrings\nDESCRIPTION: Exemplifies the correct approach to providing a short summary in a function docstring: starting with an infinitive verb, in one line, ending with a period. The function 'astype' demonstrates minimum-compliant documentation, in line with pandas conventions. Requirements: none beyond standard Python. Purpose is stylistic, focusing on summary style rather than implementation. No functional body provided.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef astype(dtype):\n    \"\"\"\n    Cast Series type.\n\n    This section will provide further details.\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Inspecting PeriodIndex.values Output (ipython, Python)\nDESCRIPTION: Shows change in PeriodIndex.values property: it now returns an array of Period objects instead of integers. Requires pandas; key parameter is the list of periods and frequency. Output changes from integer array to Period array.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_35\n\nLANGUAGE: ipython\nCODE:\n```\nIn [6]: pi = pd.PeriodIndex(['2011-01', '2011-02'], freq='M')\nIn [7]: pi.values\nOut[7]: array([492, 493])\n```\n\nLANGUAGE: python\nCODE:\n```\npi = pd.PeriodIndex([\"2011-01\", \"2011-02\"], freq=\"M\")\npi.values\n```\n\n----------------------------------------\n\nTITLE: Defining a Class Method with Parameter Documentation in a Docstring (Python)\nDESCRIPTION: This snippet shows the beginning definition of the head method for a Series class in Python, focusing on the Parameters section within the docstring that documents the parameter n. It demonstrates the preferred use of Python type annotations (e.g., int) within the Parameters section and begins the structure for method documentation according to pandas guidelines. Inputs: n (int), default 5. Outputs: not shown. This code lays out initial docstring parameter documentation structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass Series:\n\n    def head(self, n=5):\n        \"\"\"\n        Return the first elements of the Series.\n\n        This function is mainly useful to preview the values of the\n        Series without displaying all of it.\n\n        Parameters\n        ----------\n        n : int\n\n```\n\n----------------------------------------\n\nTITLE: Reading SAS XPORT Files with pandas.read_sas (Python)\nDESCRIPTION: This snippet illustrates reading a SAS XPORT format file into a pandas DataFrame using the newly added read_sas method. The only required parameter is the file path to an .xpt file. Output is a DataFrame with data parsed from the SAS file. Dependency: pandas 0.17.0+; note that reading other SAS file formats may require different arguments or formats.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_sas(\"sas_xport.xpt\")\n```\n\n----------------------------------------\n\nTITLE: Applying Functions to Groups in pandas\nDESCRIPTION: Shows how to use the pivot_table method in pandas to apply a function to groups, similar to R's tapply function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport random\nimport string\n\nbaseball = pd.DataFrame(\n    {\n        \"team\": [\"team %d\" % (x + 1) for x in range(5)] * 5,\n        \"player\": random.sample(list(string.ascii_lowercase), 25),\n        \"batting avg\": np.random.uniform(0.200, 0.400, 25),\n    }\n)\n\nbaseball.pivot_table(values=\"batting avg\", columns=\"team\", aggfunc=\"max\")\n```\n\n----------------------------------------\n\nTITLE: Standard Python Assignment Equivalent for DataFrame Columns - Python\nDESCRIPTION: Provides a reference example of how multi-step column assignments and arithmetic can be performed without eval, using direct DataFrame column assignment. Produces the same result as previous multi-line eval but relies on standard syntax. Helps compare succinctness and clarity of eval versus standard methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(dict(a=range(5), b=range(5, 10)))\ndf[\"c\"] = df[\"a\"] + df[\"b\"]\ndf[\"d\"] = df[\"a\"] + df[\"b\"] + df[\"c\"]\ndf[\"a\"] = 1\ndf\n```\n\n----------------------------------------\n\nTITLE: Counting Records by Group Using groupby and count - pandas.groupby() - Python\nDESCRIPTION: This snippet groups the DataFrame by 'Pclass' and counts non-missing entries in 'Pclass' for each group using .count(). It illustrates how value_counts() can be implemented via a groupby and count combination. Requires the DataFrame to contain the 'Pclass' column and excludes rows with missing 'Pclass' values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntitanic.groupby(\"Pclass\")[\"Pclass\"].count()\n```\n\n----------------------------------------\n\nTITLE: Indexing Series with Nullable Boolean Mask (New behavior in pandas 1.0.2) - pandas (Python)\nDESCRIPTION: In pandas 1.0.2, this snippet shows successful indexing of a Series with a nullable Boolean mask, treating NA as False. It demonstrates the resolved regression where previously such indexing would raise an error. Assumes pandas 1.0.2+, Series s, and mask are already defined. The result is a filtered Series excluding elements where the mask is False or NA, with NA treated as False. There are no external dependencies beyond standard pandas imports.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.2.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ns[mask]\n```\n\n----------------------------------------\n\nTITLE: Plotting DataFrames and Series with pandas and matplotlib in Python\nDESCRIPTION: Shows plotting techniques using pandas' built-in plot functions and matplotlib. Demonstrates plotting Series and DataFrames and adjusting legends. Inputs are random Series or DataFrames; outputs are plotted charts. Requires pandas and matplotlib (imported as plt). When in a non-interactive environment, matplotlib's show/savefig can be used to render or save plots.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\\n\\nplt.close(\"all\")\n```\n\nLANGUAGE: python\nCODE:\n```\nts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\\nts = ts.cumsum()\\n\\n@savefig series_plot_basic.png\\nts.plot();\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\\n    np.random.randn(1000, 4), index=ts.index, columns=[\"A\", \"B\", \"C\", \"D\"]\\n)\\n\\ndf = df.cumsum()\\n\\nplt.figure();\\ndf.plot();\\n@savefig frame_plot_basic.png\\nplt.legend(loc='best');\n```\n\n----------------------------------------\n\nTITLE: Categorical Construction with numpy.str_ in Python\nDESCRIPTION: Fixed regression in Categorical construction with numpy.str_ categories.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\npd.Categorical(['a', 'b'], categories=np.array(['a', 'b'], dtype=np.str_))\n```\n\n----------------------------------------\n\nTITLE: Using get_dummies with cut in Python\nDESCRIPTION: Shows how to combine pandas.get_dummies() with pandas.cut() for binning and encoding.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nvalues = np.random.randn(10)\nvalues\n\nbins = [0, 0.2, 0.4, 0.6, 0.8, 1]\n\npd.get_dummies(pd.cut(values, bins))\n```\n\n----------------------------------------\n\nTITLE: Constructing a Data Set - Stata\nDESCRIPTION: This snippet demonstrates how to create a new Stata data set by specifying column names and rows of values directly after an 'input' statement. No external dependencies are required; column names (x and y) are provided followed by 3 rows of values. Ends the data entry with the 'end' keyword. Input: None. Output: In-memory Stata data set.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_0\n\nLANGUAGE: stata\nCODE:\n```\ninput x y\\n1 2\\n3 4\\n5 6\\nend\n```\n\n----------------------------------------\n\nTITLE: Assign Behavior Change for Callables in assign - Pandas - Python\nDESCRIPTION: Compares old and new behaviors in DataFrame.assign when updating existing columns with callables. Requires pandas and explains that previously, callables referenced the old column value, but with the update, callables can reference new, assigned values. Includes both code block and IPython-style demonstrations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1, 2, 3]})\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.assign(A=lambda df: df.A + 1, C=lambda df: df.A * -1)\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.assign(A=df.A + 1, C=lambda df: df.A * -1)\n\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Rolling Means with Numba in pandas DataFrames (ipython)\nDESCRIPTION: Illustrates leveraging multi-CPU hardware by configuring Numba thread count and enabling parallel execution via engine_kwargs. Shows how changing numba.set_num_threads can further increase performance. Requires pandas, numpy, and numba. Demonstrates with a large DataFrame, rolling window operation, and timing results for single vs multi-threaded execution. Inputs: DataFrame; code changes thread count and performs rolling means. Outputs: timing measurements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_12\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: import numba\n\nIn [2]: numba.set_num_threads(1)\n\nIn [3]: df = pd.DataFrame(np.random.randn(10_000, 100))\n\nIn [4]: roll = df.rolling(100)\n\nIn [5]: %timeit roll.mean(engine=\"numba\", engine_kwargs={\"parallel\": True})\n347 ms ± 26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nIn [6]: numba.set_num_threads(2)\n\nIn [7]: %timeit roll.mean(engine=\"numba\", engine_kwargs={\"parallel\": True})\n201 ms ± 2.97 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n\n----------------------------------------\n\nTITLE: Creating a Scatter Plot to Compare Two Columns in pandas and Matplotlib (Python)\nDESCRIPTION: This snippet generates a scatter plot comparing NO2 values measured in London and Paris using pandas' DataFrame.plot.scatter method, specifying x and y column names and setting alpha=0.5 for point transparency. plt.show() displays the plot. DataFrame must contain numeric 'station_london' and 'station_paris' columns. The output visually compares value correlation between the two locations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nair_quality.plot.scatter(x=\"station_london\", y=\"station_paris\", alpha=0.5)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Reading Only Elements or Only Attributes from XML - pandas - Python\nDESCRIPTION: Demonstrates use of the 'elems_only' and 'attrs_only' options in 'read_xml' to parse either only element nodes or only attribute values, respectively. Each usage produces a DataFrame tailored to the chosen data subset. Helpful for extracting specific XML structures; assumes the XML file is in scope.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_118\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_xml(file_path, elems_only=True)\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_xml(file_path, attrs_only=True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Renaming MultiIndex Levels in pandas\nDESCRIPTION: Shows how to use the new index and columns arguments in rename_axis to change specific level names in a MultiIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nmi = pd.MultiIndex.from_product([list('AB'), list('CD'), list('EF')],\n                                names=['AB', 'CD', 'EF'])\ndf = pd.DataFrame(list(range(len(mi))), index=mi, columns=['N'])\ndf\ndf.rename_axis(index={'CD': 'New'})\n```\n\n----------------------------------------\n\nTITLE: Casting Series to float64 Before Assignment in Pandas (python)\nDESCRIPTION: This snippet demonstrates how to explicitly cast a Series to 'float64' dtype, allowing for assignment of float values. It showcases the recommended pre-cast approach when the intended assignment requires supporting floating-point numbers. The input is an integer list, and the Series is updated with a float value at index 0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n  ser = pd.Series([1, 2, 3])\n  ser = ser.astype('float64')\n  ser[0] = 1.1\n  ser\n```\n\n----------------------------------------\n\nTITLE: Deprecated Dict-of-Dicts Aggregation with Renaming - Pandas - Python\nDESCRIPTION: Demonstrates deprecated use of dict-of-dicts for both aggregation and renaming in groupby.agg(). Produces nested columns and is subject to removal. Requires df, returns a DataFrame where column levels reflect renamed aggregations. FutureWarning will be issued.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nIn [23]: (df.groupby('A')\n   ...:    .agg({'B': {'foo': 'sum'}, 'C': {'bar': 'min'}})\n   ...:  )\nFutureWarning: using a dict with renaming is deprecated and\nwill be removed in a future version\n\nOut[23]:\n     B   C\n   foo bar\nA        \n1   3   0\n2   7   3\n```\n\n----------------------------------------\n\nTITLE: Displaying a DataFrame - pandas Python\nDESCRIPTION: Displays the content of an example DataFrame 'df'. This is foundational for subsequent groupby operations. Assumes prior definition of 'df'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\ndf\n```\n\n----------------------------------------\n\nTITLE: Calculating Duration in Seconds with Series.dt.total_seconds in pandas (Python)\nDESCRIPTION: Demonstrates the use of the new .dt.total_seconds() accessor for Series of type timedelta64, which computes the duration of each element in seconds. Input: Series of timedeltas; output: Series of corresponding second values. Useful for numeric analysis or conversion from timedelta representations. Requires pandas 0.17.0 or newer.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# TimedeltaIndex\ns = pd.Series(pd.timedelta_range(\"1 minutes\", periods=4))\ns\ns.dt.total_seconds()\n```\n\n----------------------------------------\n\nTITLE: Creating MultiIndex from product in Pandas 0.13.1\nDESCRIPTION: Shows how to use the new MultiIndex.from_product() convenience function to create a MultiIndex from the cartesian product of a set of iterables.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.1.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nshades = [\"light\", \"dark\"]\ncolors = [\"red\", \"green\", \"blue\"]\n\npd.MultiIndex.from_product([shades, colors], names=[\"shade\", \"color\"])\n```\n\n----------------------------------------\n\nTITLE: Chaining Style Maps with Different Order in pandas DataFrame (Python)\nDESCRIPTION: This snippet shows the reverse order of applying two style maps on a pandas DataFrame, highlighting how the order of style.map invocations changes the resulting CSS due to how pandas renders CSS rules. The snippet depends on pandas and involves rendering a DataFrame with sequentially applied color styles. The main difference from the previous example is the order of mapping, serving as a pedagogical comparison.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf4.style.map(lambda x: \"color:red;\").map(lambda x: \"color:green;\")\n```\n\n----------------------------------------\n\nTITLE: Installing pandas Development Version via pip in Shell\nDESCRIPTION: Installs the latest development version of pandas from the scientific-python-nightly-wheels index using pip. The '--pre' flag permits installation of pre-releases, and the additional index URL fetches nightly wheels. Must uninstall any current pandas version if a conflicting install exists. Requires pip and an internet connection. This command is for trying unreleased features or bugfixes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install --pre --extra-index https://pypi.anaconda.org/scientific-python-nightly-wheels/simple pandas\n```\n\n----------------------------------------\n\nTITLE: DateTime Conversion\nDESCRIPTION: Demonstrates pd.to_datetime() usage for converting objects to datetime, including handling of invalid values with error coercion.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_87\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nm = [\"2016-07-09\", datetime.datetime(2016, 3, 2)]\npd.to_datetime(m)\n\nm = [\"apple\", datetime.datetime(2016, 3, 2)]\npd.to_datetime(m, errors=\"coerce\")\n```\n\n----------------------------------------\n\nTITLE: Creating a MultiIndex from DataFrame in Python\nDESCRIPTION: Demonstrates constructing a MultiIndex directly from a DataFrame using the from_frame method, which serves as a complement to the to_frame method for MultiIndex objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    [[\"bar\", \"one\"], [\"bar\", \"two\"], [\"foo\", \"one\"], [\"foo\", \"two\"]],\n    columns=[\"first\", \"second\"],\n)\npd.MultiIndex.from_frame(df)\n```\n\n----------------------------------------\n\nTITLE: Creating SparseArray from Timezone-aware Dtype (Python)\nDESCRIPTION: Now issues a warning before dropping timezone information when creating a SparseArray from timezone-aware dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\npd.arrays.SparseArray(data, dtype='datetime64[ns, UTC]')\n```\n\n----------------------------------------\n\nTITLE: Sharing Index Objects between Series and DataFrames - pandas - Python\nDESCRIPTION: Reindexes a Series to match a DataFrame's index and demonstrates index sharing by comparing objects via 'is'. Shows low-level object identity between data container indices. Inputs: DataFrame and Series. Output: reindexed Series and comparison result (True/False).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nrs = s.reindex(df.index)\nrs\nrs.index is df.index\n```\n\n----------------------------------------\n\nTITLE: Accessing PandasArray from Series of Integers - pandas (Python)\nDESCRIPTION: Shows that .array on a normal Series of ints yields a PandasArray, a no-copy wrapper around the underlying NumPy array. Also demonstrates .to_numpy() for standard conversion to ndarray. This is relevant if custom extension array compatibility is needed or for advanced array processing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, 3])\nser.array\nser.to_numpy()\n```\n\n----------------------------------------\n\nTITLE: Forward Filling Missing Data with pandas.DataFrame.ffill - Python\nDESCRIPTION: This code shows how to fill missing values in a pandas DataFrame by propagating the last valid observation forward to the next missing value, using the ffill() method. The input is a DataFrame ('outer_join'), and the output is a DataFrame where NaNs are replaced by the previous row's value in the same column. Assumes pandas is installed. Edge cases occur if the first row(s) are missing, as these remain NaN.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/missing.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nouter_join.ffill()\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame with Duplicate Column Labels - pandas - Python\nDESCRIPTION: This code constructs a DataFrame with duplicated column names to highlight behavior changes in indexing when duplicate columns are present. A DataFrame with columns ['A', 'A', 'B'] and two rows is created. This setup is used for illustrating subsequent indexing examples.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[\"A\", \"A\", \"B\"])\ndf1\n```\n\n----------------------------------------\n\nTITLE: Computing Lower Triangular Correlation Matrix with Boolean Mask - Python\nDESCRIPTION: Calculates a correlation matrix for a DataFrame and masks it to show only the lower triangle using numpy's tril to build the mask. Dependencies: pandas, numpy. Input: random DataFrame. Output: masked correlation matrix with upper triangle as NaN. Useful for visualizing and reducing redundancy in correlation matrices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.random(size=(100, 5)))\n\ncorr_mat = df.corr()\nmask = np.tril(np.ones_like(corr_mat, dtype=np.bool_), k=-1)\n\ncorr_mat.where(mask)\n```\n\n----------------------------------------\n\nTITLE: Resetting pandas Unicode Display Options to Default - Python\nDESCRIPTION: This snippet resets both 'display.unicode.east_asian_width' and 'display.unicode.ambiguous_as_wide' to their default (False) values. This is useful for ensuring consistent behavior or cleaning up state in an interactive or test environment. No user parameters are required as the reset is performed globally.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.unicode.east_asian_width\", False)\npd.set_option(\"display.unicode.ambiguous_as_wide\", False)\n```\n\n----------------------------------------\n\nTITLE: RangeIndex Usage in pandas\nDESCRIPTION: Shows the new RangeIndex implementation which provides memory savings compared to Int64Index for common use cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(range(1000))\ns.index\ns.index.nbytes\n```\n\n----------------------------------------\n\nTITLE: Limiting normalization depth in pandas.json_normalize with max_level - Python\nDESCRIPTION: This snippet demonstrates the max_level argument of pandas' json_normalize to control how deep the flattening of nested data should go. With max_level=1, only the outermost layer is normalized. Requires pandas; input is a list of dicts with nested dictionaries; output is a DataFrame normalized to specified nested level.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_82\n\nLANGUAGE: python\nCODE:\n```\ndata = [\n    {\n        \"CreatedBy\": {\"Name\": \"User001\"},\n        \"Lookup\": {\n            \"TextField\": \"Some text\",\n            \"UserField\": {\"Id\": \"ID001\", \"Name\": \"Name001\"},\n        },\n        \"Image\": {\"a\": \"b\"},\n    }\n]\npd.json_normalize(data, max_level=1)\n```\n\n----------------------------------------\n\nTITLE: Time Series Operations: Resampling, Timezone Localization and Offsetting in pandas Python\nDESCRIPTION: Demonstrates time series functionality in pandas, including resampling, timezone localization, timezone conversion, and adding business day offsets. Dependencies: pandas, numpy. Inputs are Series or DatetimeIndex with date/time data. Outputs are resampled or time-adjusted Series with aggregation as specified.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nrng = pd.date_range(\"1/1/2012\", periods=100, freq=\"s\")\\nts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\\nts.resample(\"5Min\").sum()\n```\n\nLANGUAGE: python\nCODE:\n```\nrng = pd.date_range(\"3/6/2012 00:00\", periods=5, freq=\"D\")\\nts = pd.Series(np.random.randn(len(rng)), rng)\\nts\\nts_utc = ts.tz_localize(\"UTC\")\\nts_utc\n```\n\nLANGUAGE: python\nCODE:\n```\nts_utc.tz_convert(\"US/Eastern\")\n```\n\nLANGUAGE: python\nCODE:\n```\nrng\\nrng + pd.offsets.BusinessDay(5)\n```\n\n----------------------------------------\n\nTITLE: Grouping by Named Index Level and Column with pandas Grouper in Python\nDESCRIPTION: Shows grouping a MultiIndex DataFrame by index level (specified by name 'second') and a column ('A') using Grouper. Demonstrates the versatility of pandas grouping when index level names are used. Requires pandas. Input is the DataFrame 'df', output is a DataFrame grouped and summed by the second level and 'A'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([pd.Grouper(level=\"second\"), \"A\"]).sum()\n```\n\n----------------------------------------\n\nTITLE: Accessing Timedelta Fields in pandas - Python\nDESCRIPTION: This snippet demonstrates field access on a pandas Timedelta object for seconds, microseconds, and nanoseconds. Requires pandas >= 0.15.0. The Timedelta object is instantiated from a string and values of key fields are retrieved as outputs. Note: prior to v0.16.0, field semantics differ from Python's standard timedelta.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntd = pd.Timedelta('1 hour 3m 15.5us')\ntd.seconds\ntd.microseconds\ntd.nanoseconds\n```\n\n----------------------------------------\n\nTITLE: Identifying Duplicated Index Labels - pandas - Python\nDESCRIPTION: Uses the Index.duplicated method to create a boolean array marking duplicated index entries. This can be used for filtering out duplicates or further analytical steps. Input is a DataFrame (df2); output is a numpy boolean array.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf2.index.duplicated()\n```\n\n----------------------------------------\n\nTITLE: Grouped Histogram for DataFrame using by Keyword (Python)\nDESCRIPTION: Shows how to create grouped histograms for a DataFrame with categorical columns using DataFrame.plot.hist(by=[...]). The plot separates the continuous columns according to the unique combinations of values in categorical columns 'a' and 'b'. Requires pandas, numpy, and matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndata = pd.DataFrame(\\n    {\\n        \"a\": np.random.choice([\"x\", \"y\", \"z\"], 1000),\\n        \"b\": np.random.choice([\"e\", \"f\", \"g\"], 1000),\\n        \"c\": np.random.randn(1000),\\n        \"d\": np.random.randn(1000) - 1,\\n    },\\n)\\n\\n@savefig grouped_hist_by.png\\ndata.plot.hist(by=[\"a\", \"b\"], figsize=(10, 5));\n```\n\n----------------------------------------\n\nTITLE: Calculating Exponential Moving Average with Timedelta Halflife using Pandas (Python)\nDESCRIPTION: This snippet demonstrates computing an exponentially weighted mean on a pandas DataFrame column 'B' using a time-aware exponential moving window, where the 'halflife' parameter is specified as a string timedelta ('4 days'). The code depends on 'pandas' (imported as pd) and 'numpy' (imported as np) and utilizes 'pd.DatetimeIndex' to set the time sequence for decay calculation. The expected input is a DataFrame with NaN values and a sequence of datetime strings; the output is a DataFrame column containing the exponentially weighted means, allowing for more accurate decay over irregular samples. The method 'ewm' supports other parameters, but here 'mean()' is shown for aggregation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"B\": [0, 1, 2, np.nan, 4]})\ndf\ntimes = [\"2020-01-01\", \"2020-01-03\", \"2020-01-10\", \"2020-01-15\", \"2020-01-17\"]\ndf.ewm(halflife=\"4 days\", times=pd.DatetimeIndex(times)).mean()\n```\n\n----------------------------------------\n\nTITLE: Setting Values with loc and IndexSlice - pandas - Python\nDESCRIPTION: Illustrates assignment to selected slices of a DataFrame using .loc(axis=0) and .loc with IndexSlice, including both scalar and array-based assignment (alignable object on the right). Requires DataFrame copy to avoid modifying originals. Inputs are selection patterns and right-hand-side values; outputs are updated DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf2 = dfmi.copy()\ndf2.loc(axis=0)[:, :, [\"C1\", \"C3\"]] = -10\ndf2\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2 = dfmi.copy()\ndf2.loc[idx[:, :, [\"C1\", \"C3\"]], :] = df2 * 1000\ndf2\n```\n\n----------------------------------------\n\nTITLE: Aggregating DataFrames with Single Function Using pandas - Python\nDESCRIPTION: This snippet shows the use of the .agg() method with a single function string 'sum' to calculate column sums. It demonstrates the equivalence to .apply and outputs a Series or DataFrame of sums per column. Requires pandas, expects a numerical DataFrame as input.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf.agg('sum')\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame for Assignment to Multiple Columns Demo - Python\nDESCRIPTION: Constructs a simple DataFrame with columns 'a' and 'b' for demonstrating the corrected behavior when assigning to multiple columns, where some columns may not yet exist. Uses a dictionary with lists to initialize the DataFrame. No special dependencies beyond pandas are required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': [0, 1, 2], 'b': [3, 4, 5]})\\ndf\n```\n\n----------------------------------------\n\nTITLE: Reading pandas to_html Output Back into DataFrame\nDESCRIPTION: Demonstrates how to convert a DataFrame to HTML using to_html() and then read it back with read_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_100\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(2, 2))\ns = df.to_html(float_format=\"{0:.40g}\".format)\ndfin = pd.read_html(s, index_col=0)\n```\n\n----------------------------------------\n\nTITLE: Substring Extraction with Regular Expressions using extract and extractall - Pandas Python\nDESCRIPTION: Covers various usages of pandas Series.str.extract and extractall, extracting regex capture groups as DataFrames or Series, handling named and optional groups, and outlining differences based on the expand argument and Subject type (Series or Index). Demonstrates handling unmatched patterns (returns NaN), and using extractall for all matches (returns DataFrame with MultiIndex). Requires pandas and Python re module understanding. Returns Series, DataFrame, or Index depending on usage. Limitations include mandatory presence of at least one capture group and possible ValueError for unsupported cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\npd.Series(\n    [\"a1\", \"b2\", \"c3\"],\n    dtype=\"string\",\n).str.extract(r\"([ab])(\\d)\", expand=False)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([\"a1\", \"b2\", \"c3\"], dtype=\"string\").str.extract(\n    r\"(?P<letter>[ab])(?P<digit>\\d)\", expand=False\n)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series(\n    [\"a1\", \"b2\", \"3\"],\n    dtype=\"string\",\n).str.extract(r\"([ab])?(\\d)\", expand=False)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([\"a1\", \"b2\", \"c3\"], dtype=\"string\").str.extract(r\"[ab](\\d)\", expand=True)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([\"a1\", \"b2\", \"c3\"], dtype=\"string\").str.extract(r\"[ab](\\d)\", expand=False)\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a1\", \"b2\", \"c3\"], [\"A11\", \"B22\", \"C33\"], dtype=\"string\")\ns\ns.index.str.extract(\"(?P<letter>[a-zA-Z])\", expand=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ns.index.str.extract(\"(?P<letter>[a-zA-Z])\", expand=False)\n```\n\nLANGUAGE: python\nCODE:\n```\ns.index.str.extract(\"(?P<letter>[a-zA-Z])([0-9]+)\", expand=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ns.index.str.extract(\"(?P<letter>[a-zA-Z])([0-9]+)\", expand=False)\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"], dtype=\"string\")\ns\ntwo_groups = \"(?P<letter>[a-z])(?P<digit>[0-9])\"\ns.str.extract(two_groups, expand=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ns.str.extractall(two_groups)\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a3\", \"b3\", \"c2\"], dtype=\"string\")\ns\n```\n\nLANGUAGE: python\nCODE:\n```\nextract_result = s.str.extract(two_groups, expand=True)\nextract_result\nextractall_result = s.str.extractall(two_groups)\nextractall_result\nextractall_result.xs(0, level=\"match\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index([\"a1a2\", \"b1\", \"c1\"]).str.extractall(two_groups)\n\npd.Series([\"a1a2\", \"b1\", \"c1\"], dtype=\"string\").str.extractall(two_groups)\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for hashing functions\nDESCRIPTION: RestructuredText directive listing Pandas object hashing utility functions\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/general_functions.rst#_snippet_7\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n\n   util.hash_array\n   util.hash_pandas_object\n```\n\n----------------------------------------\n\nTITLE: Parsing Dates from Multi-Column DataFrames with Format - Python\nDESCRIPTION: This group of snippets shows creating a DataFrame from date parts, constructing date strings, and benchmarking efficient datetime parsing with pandas. Dependencies: pandas, numpy. Input: a DataFrame with columns for year, month, day. Output: performance comparisons and parsed datetime series. Faster date parsing is achieved using the format argument in pd.to_datetime.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\ni = pd.date_range(\"20000101\", periods=10000)\ndf = pd.DataFrame({\"year\": i.year, \"month\": i.month, \"day\": i.day})\ndf.head()\n\n%timeit pd.to_datetime(df.year * 10000 + df.month * 100 + df.day, format='%Y%m%d')\nds = df.apply(lambda x: \"%04d%02d%02d\" % (x[\"year\"], x[\"month\"], x[\"day\"]), axis=1)\nds.head()\n%timeit pd.to_datetime(ds)\n```\n\n----------------------------------------\n\nTITLE: Reading a Stata .dta File into a pandas DataFrame - Python\nDESCRIPTION: This snippet uses pandas.read_stata to read a Stata .dta file into a DataFrame. The function supports multiple .dta format versions. The primary input is the filename (here 'stata.dta'), and the output is a DataFrame containing the file's data. Requires pandas to be installed and assumes 'stata.dta' is present in the working directory.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_236\n\nLANGUAGE: python\nCODE:\n```\npd.read_stata(\"stata.dta\")\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregation with DataFrame.groupby - pandas - Python\nDESCRIPTION: This Python snippet shows how to mimic R's ddply using pandas. It creates a DataFrame with random values, groups by 'month' and 'week', and calculates the mean and standard deviation for column 'x'. Dependencies include pandas and numpy. Expected output is a DataFrame with aggregated statistics; ensure input columns exist and numerical data is provided.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"x\": np.random.uniform(1.0, 168.0, 120),\n        \"y\": np.random.uniform(7.0, 334.0, 120),\n        \"z\": np.random.uniform(1.7, 20.7, 120),\n        \"month\": [5, 6, 7, 8] * 30,\n        \"week\": np.random.randint(1, 4, 120),\n    }\n)\n\ngrouped = df.groupby([\"month\", \"week\"])\ngrouped[\"x\"].agg([\"mean\", \"std\"])\n```\n\n----------------------------------------\n\nTITLE: Constructing Series with Parameterized PyArrow Types in pandas (Python)\nDESCRIPTION: Shows how to construct pandas Series with complex parameterized PyArrow types, such as lists or custom Decimals, by passing ArrowDtype with a configured pyarrow data type. Dependencies are pandas and pyarrow. The code demonstrates constructing Series with Arrow list and decimal types, and Index with Arrow time types. Input data must match the parameterization; output is a pandas Series or Index with the Arrow-backed extension type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pyarrow as pa\nlist_str_type = pa.list_(pa.string())\nser = pd.Series([[\"hello\"], [\"there\"]], dtype=pd.ArrowDtype(list_str_type))\nser\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import time\nidx = pd.Index([time(12, 30), None], dtype=pd.ArrowDtype(pa.time64(\"us\")))\nidx\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom decimal import Decimal\ndecimal_type = pd.ArrowDtype(pa.decimal128(3, scale=2))\ndata = [[Decimal(\"3.19\"), None], [None, Decimal(\"-1.23\")]]\ndf = pd.DataFrame(data, dtype=decimal_type)\ndf\n```\n\n----------------------------------------\n\nTITLE: Converting Nullable and Arrow Types to NumPy Dtype with to_numpy (Python)\nDESCRIPTION: Illustrates the behavior of Series.to_numpy() for NumPy nullable dtypes and PyArrow-backed columns, showing conversion results before and after the enhancement. For Int64 and timestamp[ns][pyarrow] Series, demonstrates dtype conversion rules (e.g., presence of missing values leading to float type; datetime/timedelta compatibility). Requires pandas 2.2.0+ and pyarrow if using Arrow-backed types. Input is pandas Series; output is a NumPy array with appropriate dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_5\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: ser = pd.Series([1, 2, 3], dtype=\"Int64\")\nIn [2]: ser.to_numpy()\nOut[2]: array([1, 2, 3], dtype=object)\n```\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, 3], dtype=\"Int64\")\nser.to_numpy()\n\nser = pd.Series([1, 2, 3], dtype=\"timestamp[ns][pyarrow]\")\nser.to_numpy()\n```\n\n----------------------------------------\n\nTITLE: Logical AND with np.nan in pandas Series: object vs boolean dtype (Python)\nDESCRIPTION: This code demonstrates the logical AND operation on a Series with np.nan values, comparing object and boolean dtypes. For object dtype, np.nan is always False; for boolean dtype, nullable Kleene logic applies. Inputs are Series with different dtypes; outputs are Series results of the AND operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/boolean.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.Series([True, False, np.nan], dtype=\"object\") & True\npd.Series([True, False, np.nan], dtype=\"boolean\") & True\n```\n\n----------------------------------------\n\nTITLE: Plotting Daily Mean Values in Time Series with pandas - Python\nDESCRIPTION: This snippet demonstrates resampling a pandas DataFrame or Series ('no_2') to compute daily mean ('D') values and plotting the result. It requires pandas and matplotlib; the 'no_2' variable must have a datetime index. The plot visualizes daily averaged NO₂ values for each station, using circles and lines for style, and sets the figure size to 10x5. The output is a plot image; all input data must support resampling, and any missing dates may affect the plot continuity.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/09_timeseries.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@savefig 09_resample_mean.png\nno_2.resample(\"D\").mean().plot(style=\"-o\", figsize=(10, 5));\n```\n\n----------------------------------------\n\nTITLE: Timedelta Arithmetic: Using mod (%) and divmod with pd.Timedelta - Python\nDESCRIPTION: Demonstrates the new support for mod and divmod operations on pandas Timedelta objects (using either Timedelta or scalars as the divisor). Requires pandas. The sample computes the remainder of dividing one Timedelta by another, yielding a Timedelta object indicating the leftover time.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntd = pd.Timedelta(hours=37)\ntd % pd.Timedelta(minutes=45)\n```\n\n----------------------------------------\n\nTITLE: Merging and Type Conversion with Nullable Integer Columns - pandas (Python)\nDESCRIPTION: Demonstrates merging DataFrame columns with extension integer types, inferring dtypes after concat, and converting extension Int64 column to float. Requires pandas >=0.24. Inputs include mixed DataFrame columns with extension types; outputs include displayed dtypes and data type conversion. Highlights ease of combining columns and explicit type casting using astype method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.concat([df[['A']], df[['B', 'C']]], axis=1).dtypes\ndf['A'].astype(float)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating New Series and Index Data-Dtype Incompatibilities in Python\nDESCRIPTION: This snippet shows how Series and Index constructors now raise errors when data is incompatible with a specified dtype in pandas 0.24.0. It attempts to create a Series with negative values and uint64 dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\npd.Series([-1], dtype=\"uint64\")\n# Raises OverflowError: Trying to coerce negative values to unsigned integers\n```\n\n----------------------------------------\n\nTITLE: Creating Indicator Variables from String Index in Pandas\nDESCRIPTION: Shows how to use the str.get_dummies() method on a pandas Index object containing strings, which returns a MultiIndex object with dummy variables.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([\"a\", \"a|b\", np.nan, \"a|c\"])\nidx.str.get_dummies(sep=\"|\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Year and Day Components from PeriodIndex Series (Python)\nDESCRIPTION: This snippet shows how to use the .dt accessor with period dtype Series to extract the year and day fields. Inputs: Series with period dtype. Dependencies: pandas. Outputs: Series of years or days corresponding to each entry.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(pd.period_range(\"20130101\", periods=4, freq=\"D\"))\ns\ns.dt.year\ns.dt.day\n```\n\n----------------------------------------\n\nTITLE: Creating Pie Subplots with pandas DataFrame\nDESCRIPTION: Creates a DataFrame with random data and generates pie charts as subplots for each column. The subplots parameter is set to True to generate multiple pie charts, and figsize is specified to control the output dimensions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    3 * np.random.rand(4, 2), index=[\"a\", \"b\", \"c\", \"d\"], columns=[\"x\", \"y\"]\n)\n\ndf.plot.pie(subplots=True, figsize=(8, 4));\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Unique Function Behavior with Categorical Data in Python\nDESCRIPTION: Illustrates the change in behavior of the unique function when applied to Series with category dtype. Now only categories that occur in the array are returned.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.2.rst#_snippet_1\n\nLANGUAGE: ipython\nCODE:\n```\ncat = pd.Categorical(['a', 'b', 'a'], categories=['a', 'b', 'c'])\ncat.unique()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Boolean Expressions with pandas.eval Python Engine - Python\nDESCRIPTION: Demonstrates how to use pandas.eval with the 'python' parser/engine to evaluate chained boolean expressions involving multiple DataFrames. This requires pandas and numpy to be installed. Inputs include large DataFrames (df1, df2, df3, df4) with random numbers. The output is a DataFrame of booleans showing where all DataFrames have values greater than zero. Parentheses are used to ensure correct operator precedence.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nnrows, ncols = 20000, 100\n# Four large random DataFrames\ndf1, df2, df3, df4 = [pd.DataFrame(np.random.randn(nrows, ncols)) for _ in range(4)]\n\nexpr = \"(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\"\nx = pd.eval(expr, parser=\"python\")\n```\n\n----------------------------------------\n\nTITLE: Extracting Underlying Extension Arrays from Series and Index - pandas (Python)\nDESCRIPTION: Demonstrates the new .array attribute for directly accessing the backing extension array (or PandasArray) from a Series or Index. Requires pandas 0.24.0 or higher. Inputs are a PeriodIndex and a Series constructed from it; outputs are the underlying array representations, distinguishing from .values which may return an object-typed ndarray. Useful for advanced cases needing explicit control over internal array structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.period_range('2000', periods=4)\nidx.array\npd.Series(idx).array\n```\n\n----------------------------------------\n\nTITLE: Datetime Operations in Python with Pandas\nDESCRIPTION: Examples of datetime operations including NaT handling, Timedelta division, and Series operations with Timestamps.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npd.Series([pd.NaT], dtype='<m8[ns]') + pd.Series([pd.NaT], dtype='<m8[ns]')\npd.Timedelta('1s') / 2.0\nser = pd.Series(pd.timedelta_range('1 day', periods=3))\nser\npd.Timestamp('2012-01-01') - ser\n```\n\n----------------------------------------\n\nTITLE: Generating Random String Series for Performance Comparison\nDESCRIPTION: This code creates two Series objects with random strings, one using the current object dtype and another using PyArrow's string type for performance comparison.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0010-required-pyarrow-dependency.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport string\nimport random\n\nimport pandas as pd\n\n\ndef random_string() -> str:\n    return \"\".join(random.choices(string.printable, k=random.randint(10, 100)))\n\n\nser_object = pd.Series([random_string() for _ in range(1_000_000)])\nser_string = ser_object.astype(\"string[pyarrow]\")\n```\n\n----------------------------------------\n\nTITLE: Using Keys to Override Column Names in Series Concatenation in Python\nDESCRIPTION: Example of using the keys argument to override column names when concatenating Series objects to create a new DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ns3 = pd.Series([0, 1, 2, 3], name=\"foo\")\ns4 = pd.Series([0, 1, 2, 3])\ns5 = pd.Series([0, 1, 4, 5])\n\npd.concat([s3, s4, s5], axis=1)\npd.concat([s3, s4, s5], axis=1, keys=[\"red\", \"blue\", \"yellow\"])\n```\n\n----------------------------------------\n\nTITLE: Listing Contributors with the contributors Directive - reStructuredText\nDESCRIPTION: This snippet uses the custom reStructuredText directive '.. contributors:: v2.1.2..v2.1.3|HEAD' to automatically list contributors between the specified git tags (v2.1.2 to v2.1.3, up to HEAD). The dependencies include Sphinx and any custom extensions supporting the contributors directive, which parses git history for relevant commits. Outputs a formatted list of contributors in the built documentation; limitations include dependence on build context and working git repository.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.3.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. contributors:: v2.1.2..v2.1.3|HEAD\n```\n\n----------------------------------------\n\nTITLE: Setting with Enlargement in Pandas\nDESCRIPTION: Demonstrates how to enlarge Series and DataFrame objects by setting values at non-existent indices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nse = pd.Series([1, 2, 3])\nse[5] = 5.\n\ndfi = pd.DataFrame(np.arange(6).reshape(3, 2),\n                  columns=['A', 'B'])\ndfi.loc[:, 'C'] = dfi.loc[:, 'A']\ndfi.loc[3] = 5\n```\n\n----------------------------------------\n\nTITLE: Selecting Random Samples from a Series\nDESCRIPTION: Shows how to use the sample method to randomly select elements from a Series, with options for specifying number of samples or fraction.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([0, 1, 2, 3, 4, 5])\n\n# When no arguments are passed, returns 1 row.\ns.sample()\n\n# One may specify either a number of rows:\ns.sample(n=3)\n\n# Or a fraction of the rows:\ns.sample(frac=0.5)\n```\n\n----------------------------------------\n\nTITLE: Tab Completion for DataFrame Attributes (IPython) - Python\nDESCRIPTION: By typing 'df2.' followed by the Tab key in an IPython shell, users can see all available column names and public attributes on the DataFrame, aiding discoverability. Output includes both columns and methods. No runtime dependencies, but requires IPython for Tab-completion, not standard Python.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: df2.<TAB>  # noqa: E225, E999\\ndf2.A                  df2.bool\\ndf2.abs                df2.boxplot\\ndf2.add                df2.C\\ndf2.add_prefix         df2.clip\\ndf2.add_suffix         df2.columns\\ndf2.align              df2.copy\\ndf2.all                df2.count\\ndf2.any                df2.combine\\ndf2.append             df2.D\\ndf2.apply              df2.describe\\ndf2.B                  df2.duplicated\\ndf2.diff\n```\n\n----------------------------------------\n\nTITLE: Initializing Series and Demonstrating Assignment with Boolean Dtype Casting Pandas Python\nDESCRIPTION: Illustrates how pandas consistently casts or changes Series dtype when assigning non-boolean values to a boolean-typed Series. Requires pandas and numpy. Creates original Series of bool, copies it, assigns np.nan or a numeric value, and demonstrates how pandas converts the resulting Series to object dtype as needed. Used to clarify new casting behavior when modifying boolean Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\norig = pd.Series([True, False])\nser = orig.copy()\nser.iloc[1] = np.nan\nser2 = orig.copy()\nser2.iloc[1] = 2.0\n```\n\n----------------------------------------\n\nTITLE: GroupBy Sum with Observed=False and Dropna=False - pandas - IPython\nDESCRIPTION: This snippet shows incorrect groupby behavior in pandas 1.5.0 where using observed=False and dropna=False leads to only observed categories being present. It relies on DataFrame 'df' and the pandas library. Input: DataFrame 'df', groupby with observed=False and dropna=False; Output: grouped result missing unobserved categories—contrary to expected inclusive behavior. This helps demonstrate regression for diagnostic purposes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.1.rst#_snippet_2\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: # Incorrect behavior, only observed categories present\n        df.groupby(\"x\", observed=False, dropna=False).sum()\nOut[4]:\n     y\nx\n1    3\nNaN  4\n```\n\n----------------------------------------\n\nTITLE: Selecting Multiple DataFrame Columns in pandas\nDESCRIPTION: Shows how to select multiple columns by name and integer location in pandas using iloc and numpy.r_.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 3), columns=list(\"abc\"))\ndf[[\"a\", \"c\"]]\ndf.loc[:, [\"a\", \"c\"]]\n```\n\nLANGUAGE: python\nCODE:\n```\nnamed = list(\"abcdefg\")\nn = 30\ncolumns = named + np.arange(len(named), n).tolist()\ndf = pd.DataFrame(np.random.randn(n, n), columns=columns)\n\ndf.iloc[:, np.r_[:10, 24:30]]\n```\n\n----------------------------------------\n\nTITLE: Axis-specific Slicing with .loc(axis=0) - pandas - Python\nDESCRIPTION: Demonstrates pandas' ability to perform selection on a specific axis, useful for targeting only rows or columns in MultiIndex DataFrames, using the 'axis' argument in .loc. Expects a valid DataFrame, pandas, and labels to select.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndfmi.loc(axis=0)[:, :, [\"C1\", \"C3\"]]\n```\n\n----------------------------------------\n\nTITLE: Taking Elements from SparseArray Using take Method in Pandas (Python)\nDESCRIPTION: Shows the updated SparseArray.take method that returns a scalar for a single position or a SparseArray for multiple. Requires Pandas and NumPy. Demonstrates both scalar and list index access. Input is a SparseArray and either a scalar or list of indices; output is a scalar or SparseArray, with proper NaN/fill handling.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ns = pd.SparseArray([np.nan, np.nan, 1, 2, 3, np.nan, 4, 5, np.nan, 6])\ns.take(0)\ns.take([1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Regex Pattern Replacement Using re.compile (Python)\nDESCRIPTION: Shows usage of compiled regex pattern (re.compile) for replacement in Series.str.replace, which allows flags to be set at regex compile time. Allows for more complex regex logic and readability. Raises ValueError if flags are passed at function call time. Requires Python's re module, pandas, and a text Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport re\n\nregex_pat = re.compile(r\"^.a|dog\", flags=re.IGNORECASE)\ns3.str.replace(regex_pat, \"XX-XX \", regex=True)\n```\n\n----------------------------------------\n\nTITLE: Defining a test function in Python using pytest\nDESCRIPTION: Example of the preferred functional style for writing tests using pytest in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef test_really_cool_feature():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Exporting Data to CSV with PROC EXPORT - SAS\nDESCRIPTION: Demonstrates exporting a SAS dataset ('tips') to a CSV file ('tips2.csv') using 'PROC EXPORT'. The 'dbms=csv' option specifies the format. Inputs: SAS dataset; output: CSV file written to specified location.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_5\n\nLANGUAGE: SAS\nCODE:\n```\nproc export data=tips outfile='tips2.csv' dbms=csv;\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Selecting Coordinates Based on Conditional Query and Reading Rows by Index - pandas HDFStore - Python\nDESCRIPTION: This snippet demonstrates how to retrieve index coordinates of rows matching a query with select_as_coordinates, then fetch corresponding rows. It first creates and stores a DataFrame in an HDFStore, queries coordinates for 'index > 20020101', and finally retrieves the selected rows. Requires pandas, numpy, pd.date_range, and an open HDFStore. Inputs: DataFrame, query string. Outputs: coordinates (Index object), selected DataFrame subset.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_192\n\nLANGUAGE: python\nCODE:\n```\ndf_coord = pd.DataFrame(\n    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n)\nstore.append(\"df_coord\", df_coord)\nc = store.select_as_coordinates(\"df_coord\", \"index > 20020101\")\nc\nstore.select(\"df_coord\", where=c)\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to XML with pandas.to_xml (Python)\nDESCRIPTION: Shows basic usage of DataFrame.to_xml to export pandas DataFrames to XML format. The example creates a DataFrame of shapes and outputs standard XML. Only root-level namespaces are supported, and special XML features like DTD, CData, or schema definitions are not handled. Requires pandas and NumPy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_124\n\nLANGUAGE: python\nCODE:\n```\ngeom_df = pd.DataFrame(\n    {\n        \"shape\": [\"square\", \"circle\", \"triangle\"],\n        \"degrees\": [360, 360, 180],\n        \"sides\": [4, np.nan, 3],\n    }\n)\n\nprint(geom_df.to_xml())\n```\n\n----------------------------------------\n\nTITLE: Replacing Patterns and Values in DataFrames with pandas.replace - Python\nDESCRIPTION: These two examples illustrate advanced use of DataFrame.replace in pandas, first using a regular expression to replace dots (with optional surrounding whitespace) with NaN and second replacing all exact '.' values with NaN. Dependencies: pandas, numpy. The main parameter features are the 'regex' argument and flexible handling of DataFrame/Series content. Returns a new DataFrame with replacements, original data is unchanged unless assigned back.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": list(\"ab..\"), \"b\": [1, 2, 3, 4]})\ndf.replace(regex=r\"\\s*\\.\\s*\", value=np.nan)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.replace(\".\", np.nan)\n```\n\n----------------------------------------\n\nTITLE: Controlling GroupBy Sorting Behavior (Python)\nDESCRIPTION: Illustrates how to group a DataFrame by column 'X' and control group key sorting using the 'sort' parameter in groupby. With sort=False, results appear in the order of first appearance in the DataFrame. Sum is computed for each group. This demonstrates performance tuning and output ordering options. pandas (as pd) must be imported.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf2 = pd.DataFrame({\"X\": [\"B\", \"B\", \"A\", \"A\"], \"Y\": [1, 2, 3, 4]})\ndf2.groupby([\"X\"]).sum()\ndf2.groupby([\"X\"], sort=False).sum()\n```\n\n----------------------------------------\n\nTITLE: Defining I/O Test Functions in Python\nDESCRIPTION: This code block defines various functions for testing I/O operations with different file formats and compression methods. It includes functions for SQL, HDF5, CSV, Feather, Pickle, and Parquet formats.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_244\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport os\n\nsz = 1000000\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\nsz = 1000000\nnp.random.seed(42)\ndf = pd.DataFrame({\"A\": np.random.randn(sz), \"B\": [1] * sz})\n\n\ndef test_sql_write(df):\n    if os.path.exists(\"test.sql\"):\n        os.remove(\"test.sql\")\n    sql_db = sqlite3.connect(\"test.sql\")\n    df.to_sql(name=\"test_table\", con=sql_db)\n    sql_db.close()\n\n\ndef test_sql_read():\n    sql_db = sqlite3.connect(\"test.sql\")\n    pd.read_sql_query(\"select * from test_table\", sql_db)\n    sql_db.close()\n\n\ndef test_hdf_fixed_write(df):\n    df.to_hdf(\"test_fixed.hdf\", key=\"test\", mode=\"w\")\n\n\ndef test_hdf_fixed_read():\n    pd.read_hdf(\"test_fixed.hdf\", \"test\")\n\n\ndef test_hdf_fixed_write_compress(df):\n    df.to_hdf(\"test_fixed_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\")\n\n\ndef test_hdf_fixed_read_compress():\n    pd.read_hdf(\"test_fixed_compress.hdf\", \"test\")\n\n\ndef test_hdf_table_write(df):\n    df.to_hdf(\"test_table.hdf\", key=\"test\", mode=\"w\", format=\"table\")\n\n\ndef test_hdf_table_read():\n    pd.read_hdf(\"test_table.hdf\", \"test\")\n\n\ndef test_hdf_table_write_compress(df):\n    df.to_hdf(\n        \"test_table_compress.hdf\", key=\"test\", mode=\"w\", complib=\"blosc\", format=\"table\"\n    )\n\n\ndef test_hdf_table_read_compress():\n    pd.read_hdf(\"test_table_compress.hdf\", \"test\")\n\n\ndef test_csv_write(df):\n    df.to_csv(\"test.csv\", mode=\"w\")\n\n\ndef test_csv_read():\n    pd.read_csv(\"test.csv\", index_col=0)\n\n\ndef test_feather_write(df):\n    df.to_feather(\"test.feather\")\n\n\ndef test_feather_read():\n    pd.read_feather(\"test.feather\")\n\n\ndef test_pickle_write(df):\n    df.to_pickle(\"test.pkl\")\n\n\ndef test_pickle_read():\n    pd.read_pickle(\"test.pkl\")\n\n\ndef test_pickle_write_compress(df):\n    df.to_pickle(\"test.pkl.compress\", compression=\"xz\")\n\n\ndef test_pickle_read_compress():\n    pd.read_pickle(\"test.pkl.compress\", compression=\"xz\")\n\n\ndef test_parquet_write(df):\n    df.to_parquet(\"test.parquet\")\n\n\ndef test_parquet_read():\n    pd.read_parquet(\"test.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Initializing SparseArrays with Specific Dtypes in pandas - Python\nDESCRIPTION: Shows new-style usage of SparseArray from pandas.arrays module, initializing with int64 and bool type sequences. Illustrates that as of v0.19.0, pandas preserves input dtype and assigns more appropriate default fill_values (0 for int64, False for bool). Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\npd.arrays.SparseArray([1, 2, 0, 0], dtype=np.int64)\npd.arrays.SparseArray([True, False, False, False])\n```\n\n----------------------------------------\n\nTITLE: Creating Dataframe from Value Combinations with expand_grid\nDESCRIPTION: A function to create a DataFrame from all combinations of given values, similar to R's expand.grid(). This utility helps generate test data with a Cartesian product of input values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\ndef expand_grid(data_dict):\n    rows = itertools.product(*data_dict.values())\n    return pd.DataFrame.from_records(rows, columns=data_dict.keys())\n\n\ndf = expand_grid(\n    {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]}\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Previewing Combined Air Quality Data with pandas in Python\nDESCRIPTION: This snippet simply shows the head (first few rows) of the main air_quality DataFrame as currently constructed. Useful for quick inspection or debugging after transformations, prior to merging additional metadata.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nair_quality.head()\n```\n\n----------------------------------------\n\nTITLE: Reading SAS Files into pandas DataFrames in Python\nDESCRIPTION: Demonstrates reading SAS XPORT and SAS7BDAT binary files in pandas using the pd.read_sas function. Requires pandas library (with 'read_sas' support). Accepts both file types, inferring format by extension or by explicit format argument. Input: SAS file path; output: pandas DataFrame. Some files may load slowly depending on underlying format.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_sas(\"transport-file.xpt\")\ndf = pd.read_sas(\"binary-file.sas7bdat\")\n```\n\n----------------------------------------\n\nTITLE: Importing CSV Data - Stata\nDESCRIPTION: This snippet shows how to import a CSV file ('tips.csv') located in the current working directory into Stata using the 'import delimited' command. No extra dependencies are needed. Input: 'tips.csv' must be present in the directory. Output: Stata data set in memory corresponding to the CSV.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_1\n\nLANGUAGE: stata\nCODE:\n```\nimport delimited tips.csv\n```\n\n----------------------------------------\n\nTITLE: Preserving or Discarding Timezones When Exporting Datetime Series to NumPy in Pandas Python\nDESCRIPTION: These two snippets illustrate conversion of a timezone-aware pandas Series of datetimes to a NumPy array either as dtype 'object', preserving timezones, or as 'datetime64[ns]', discarding timezones for compatibility. This is essential for controlling how datetime data is represented when integrating with external code or performing calculations. Inputs are a timezone-aware Series 'ser' and chosen dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(pd.date_range(\"2000\", periods=2, tz=\"CET\"))\nser.to_numpy(dtype=object)\n```\n\nLANGUAGE: python\nCODE:\n```\nser.to_numpy(dtype=\"datetime64[ns]\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Table Schema Output for pandas DataFrame and Series - Python\nDESCRIPTION: This snippet sets the 'display.html.table_schema' option to True so pandas DataFrame and Series objects will publish a Table Schema (JSON format) representation by default. This affects how pandas objects are serialized in environments that support rich output (e.g., Jupyter), and only serializes up to the 'display.max_rows' limit. Requires pandas 0.20.0 (or later) with appropriate display backends.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.html.table_schema\", True)\n```\n\n----------------------------------------\n\nTITLE: Using pandas.option_context for Temporary Option Changes - Python\nDESCRIPTION: Demonstrates the use of the option_context context manager to temporarily set global display options within a code block. On exiting the context, changes are reverted. Useful for ensuring only certain code executes with custom settings. Inputs are option names and values; outputs are the set values within and outside the context block.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwith pd.option_context(\"display.max_rows\", 10, \"display.max_columns\", 5):\n    print(pd.get_option(\"display.max_rows\"))\n    print(pd.get_option(\"display.max_columns\"))\nprint(pd.get_option(\"display.max_rows\"))\nprint(pd.get_option(\"display.max_columns\"))\n```\n\n----------------------------------------\n\nTITLE: Detecting Categorical Series via hasattr - pandas - Python\nDESCRIPTION: Shows how to check if a pandas Series contains categorical data by testing for the presence of the 'cat' attribute. Useful for run-time introspection. Dependencies: pandas. Key parameters: Series dtype. Input: Series objects; Output: Boolean results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nhasattr(pd.Series([\"a\"], dtype=\"category\"), \"cat\")\nhasattr(pd.Series([\"a\"]), \"cat\")\n```\n\n----------------------------------------\n\nTITLE: Bollinger Bands Plotting with Matplotlib using pandas Series in Python\nDESCRIPTION: This sequence shows manual construction of a time series plot with Bollinger bands (mean plus/minus two standard deviations) using rolling windows, direct Matplotlib calls, and a pandas Series with a DateTime index. Dependencies are numpy, pandas, matplotlib. Inputs include a simulated random-walk series; outputs are a line plot of the series and its rolling mean, with a filled region for confidence intervals. Suitable for custom decorative plots or cases not directly supported by pandas plotting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(123456)\nprice = pd.Series(\n    np.random.randn(150).cumsum(),\n    index=pd.date_range(\"2000-1-1\", periods=150, freq=\"B\"),\n)\nma = price.rolling(20).mean()\nmstd = price.rolling(20).std()\n\nplt.figure();\n\nplt.plot(price.index, price, \"k\");\nplt.plot(ma.index, ma, \"b\");\nplt.fill_between(mstd.index, ma - 2 * mstd, ma + 2 * mstd, color=\"b\", alpha=0.2);\n```\n\n----------------------------------------\n\nTITLE: Selecting with .loc and MultiIndex - pandas - Python\nDESCRIPTION: Demonstrates usage of .loc with tuple-based slicers in pandas DataFrames with MultiIndex. It is best practice to specify both the index and columns axes when slicing, to avoid ambiguity—especially in cases where indexes could be interpreted as referencing both axes. Requires pandas loaded as pd and appropriate DataFrame structures. Expects MultiIndex DataFrame inputs and returns sliced DataFrame outputs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[(slice(\"A1\", \"A3\"), ...), :]  # noqa: E999\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[(slice(\"A1\", \"A3\"), ...)]  # noqa: E999\n```\n\n----------------------------------------\n\nTITLE: Initializing String Series with StringDtype - pandas - Python\nDESCRIPTION: This snippet demonstrates the creation of a pandas Series of strings using the new StringDtype extension type and explicit null support (with None as missing value). Prior to pandas 1.0.0, strings were stored as object dtype. The snippet shows safer type enforcement and improved handling of missing data. Requires pandas 1.0.0 or later.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.Series(['abc', None, 'def'], dtype=pd.StringDtype())\n```\n\n----------------------------------------\n\nTITLE: Integer Indexing Behavior in Pandas\nDESCRIPTION: Demonstrates that integer-based label indexing with negative indices is not supported in pandas Series and DataFrame objects, preventing position-based indexing ambiguities.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(range(5))\ns[-1]\ndf = pd.DataFrame(np.random.randn(5, 4))\ndf\ndf.loc[-2:]\n```\n\n----------------------------------------\n\nTITLE: Selection with Callable Functions in DataFrame\nDESCRIPTION: Demonstrates using callable functions as indexers with .loc and .iloc to dynamically select rows and columns based on conditions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(np.random.randn(6, 4),\n                  index=list('abcdef'),\n                  columns=list('ABCD'))\ndf1\n\ndf1.loc[lambda df: df['A'] > 0, :]\ndf1.loc[:, lambda df: ['A', 'B']]\n\ndf1.iloc[:, lambda df: [0, 1]]\n\ndf1[lambda df: df.columns[0]]\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Values in Timedelta Series using fillna (Python)\nDESCRIPTION: Shows two methods for filling NaT (missing) values in a timedelta Series using zero or a custom Timedelta. Requires pandas (and optionally datetime). Uses 'td.fillna(pd.Timedelta(0))' and 'td.fillna(datetime.timedelta(days=1, seconds=5))'. Outputs: a filled Series. Important for robust time computations without missing data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntd.fillna(pd.Timedelta(0))\ntd.fillna(datetime.timedelta(days=1, seconds=5))\n```\n\n----------------------------------------\n\nTITLE: Logical Operator Alignment in pandas Series and DataFrame - Python\nDESCRIPTION: Shows that logical operators (&) between Series or DataFrames now align both .index labels, filling missing results with False. Also demonstrates preserving legacy behavior via reindex_like. Requires pandas, with input Series/DataFrame boolean arrays. Output aligns on all indexes, producing boolean results or NaN for missing data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([True, False, True], index=list(\"ABC\"))\ns2 = pd.Series([True, True, True], index=list(\"ABD\"))\ns1 & s2\n```\n\nLANGUAGE: python\nCODE:\n```\ns1 & s2.reindex_like(s1)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame([True, False, True], index=list(\"ABC\"))\ndf2 = pd.DataFrame([True, True, True], index=list(\"ABD\"))\ndf1 & df2\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Error with Incompatible CategoricalIndex Categories\nDESCRIPTION: Shows that combining DataFrames with CategoricalIndex instances having different categories will raise a TypeError during concatenation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndf4 = pd.DataFrame({\"A\": np.arange(2), \"B\": list(\"ba\")})\ndf4[\"B\"] = df4[\"B\"].astype(CategoricalDtype(list(\"ab\")))\ndf4 = df4.set_index(\"B\")\ndf4.index\n\ndf5 = pd.DataFrame({\"A\": np.arange(2), \"B\": list(\"bc\")})\ndf5[\"B\"] = df5[\"B\"].astype(CategoricalDtype(list(\"bc\")))\ndf5 = df5.set_index(\"B\")\ndf5.index\n```\n\n----------------------------------------\n\nTITLE: Styling DataFrames for Excel Export\nDESCRIPTION: This code demonstrates the new experimental feature to export styled DataFrames to Excel using the openpyxl engine, with conditional formatting and cell highlighting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(24)\ndf = pd.DataFrame({'A': np.linspace(1, 10, 10)})\ndf = pd.concat([df, pd.DataFrame(np.random.RandomState(24).randn(10, 4),\n                                 columns=list('BCDE'))],\n               axis=1)\ndf.iloc[0, 2] = np.nan\ndf\nstyled = (df.style\n          .map(lambda val: 'color:red;' if val < 0 else 'color:black;')\n          .highlight_max())\nstyled.to_excel('styled.xlsx', engine='openpyxl')\n```\n\n----------------------------------------\n\nTITLE: Converting Integer Series with Nullable Dtype to StringDtype (Python)\nDESCRIPTION: Demonstrates converting a pandas Series of nullable Int64 dtype to StringDtype using astype. Shows original Series, the result after conversion, and inspects the datatype of an element in the converted Series. Useful for migrating numeric data to string format for consistent text operations. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([1, 2, np.nan], dtype=\"Int64\")\ns1\ns2 = s1.astype(\"string\")\ns2\ntype(s2[0])\n```\n\n----------------------------------------\n\nTITLE: Reading DataFrame from ORC File using pandas (Python)\nDESCRIPTION: Reads a DataFrame previously saved as an ORC file ('example_pa.orc') using pandas. Requires pyarrow installed and working. Outputs the reloaded DataFrame and its dtypes, which may not perfectly match the original if certain dtypes are unsupported or if datetimes contained timezones.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_219\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.read_orc(\"example_pa.orc\")\n\nresult.dtypes\n```\n\n----------------------------------------\n\nTITLE: Querying GroupBy Rolling Window Sum Pandas Python\nDESCRIPTION: Performs a groupby operation on column A with a subsequent rolling window sum, highlighting that the grouped-by column is now excluded from the output. Requires pandas. Calls df.groupby(\"A\").rolling(2).sum() to compute rolling window sums for each group; the result is a DataFrame with rolling sums for B (and potentially A if not excluded), omitting the group-by column in the reported values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"A\").rolling(2).sum()\n```\n\n----------------------------------------\n\nTITLE: Conditional Assignment with OR Operation in Python\nDESCRIPTION: Assigns values to a column based on OR conditions. This example modifies the AAA column where either BBB > 25 or CCC >= 75.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[(df[\"BBB\"] > 25) | (df[\"CCC\"] >= 75), \"AAA\"] = 999\ndf\n```\n\n----------------------------------------\n\nTITLE: Comparing Dtypes Between pandas and NumPy - pandas - Python\nDESCRIPTION: Demonstrates that direct comparison (==) between pandas categorical dtype and NumPy's string dtype works without error. Facilitates type checking when working with mixed pandas/NumPy objects. Dependencies: pandas, numpy. Key parameters: dtype objects. Input: dtype comparisons; Output: Boolean results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndtype == np.str_\nnp.str_ == dtype\n```\n\n----------------------------------------\n\nTITLE: Chained DataFrame Filtering, Assignment, and Plotting with pandas (Python)\nDESCRIPTION: Illustrates chaining multiple DataFrame operations: querying rows, assigning multiple calculated columns, and plotting with matplotlib. Dependencies: pandas, matplotlib, and the iris data file. This chained approach enables concise and expressive data transformations and visualizations. Inputs: the 'iris' DataFrame. Outputs: a scatter plot of calculated sepal and petal ratios. Requires plot backend to display figures. Note: Assignment is not in-place; data is filtered before assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\niris = pd.read_csv('data/iris.data')\n(iris.query('SepalLength > 5')\n     .assign(SepalRatio=lambda x: x.SepalWidth / x.SepalLength,\n             PetalRatio=lambda x: x.PetalWidth / x.PetalLength)\n     .plot(kind='scatter', x='SepalRatio', y='PetalRatio'))\n```\n\n----------------------------------------\n\nTITLE: DataFrame Column Cleaning with .str Accessor (Python)\nDESCRIPTION: Utilizes the string accessor on DataFrame columns, showing how to strip whitespace and convert names to lowercase using .str.strip() and .str.lower(). Common data-cleaning steps when handling messy column names. Input is a DataFrame with whitespace-padded columns. Output is a cleaned list of column names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    np.random.randn(3, 2), columns=[\" Column A \", \" Column B \"], index=range(3)\n)\ndf\ndf.columns.str.strip()\ndf.columns.str.lower()\n```\n\n----------------------------------------\n\nTITLE: Creating Series with Conditional Values via case_when (Python)\nDESCRIPTION: Demonstrates the new Series.case_when API, which creates a pandas Series with values determined by evaluating multiple conditions. Each tuple in caselist contains a boolean array condition and a corresponding replacement value, with a default value provided. Requires pandas 2.2.0+. Inputs are condition-replacement pairs and a default series; output is a Series where each element reflects the first matched condition or default.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame(dict(a=[1, 2, 3], b=[4, 5, 6]))\ndefault=pd.Series('default', index=df.index)\ndefault.case_when(\n     caselist=[\n         (df.a == 1, 'first'),                              # condition, replacement\n         (df.a.gt(1) & df.b.eq(5), 'second'),  # condition, replacement\n     ],\n)\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in DataFrame.describe\nDESCRIPTION: Addresses a regression in DataFrame.describe raising TypeError: unhashable type: 'dict'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame.describe()\n```\n\n----------------------------------------\n\nTITLE: Generating Regular Ranges of Timedeltas - pandas - Python\nDESCRIPTION: Uses pd.timedelta_range to produce a regularly spaced range of Timedeltas, starting from a specific value and for a given period count. The default frequency is one calendar day. Useful for time-based indexing or simulation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\npd.timedelta_range(start=\"1 days\", periods=5)\n```\n\n----------------------------------------\n\nTITLE: Selecting Slices from MultiIndexed DataFrame Using loc - pandas Python\nDESCRIPTION: Demonstrates how to slice a MultiIndexed DataFrame using loc, requiring both index and column selectors to avoid ambiguity. Slice objects and ellipsis are combined to target specific index levels explicitly. This example emphasizes the necessity to specify all axes in loc for unambiguous selection and to select ranges from 'A1' to 'A3' across all remaining levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> df.loc[(slice('A1', 'A3'), ...), :]  # noqa: E901\n```\n\n----------------------------------------\n\nTITLE: Reindexing a Series with Duplicated Labels - pandas - Python\nDESCRIPTION: Demonstrates the behavior of reindexing a pandas Series that contains duplicate index labels. Shows that reindexing with duplicate index values raises an exception, as the operation is ambiguous. Requires pandas; s1 is a Series with duplicated index 'b'. Shows raising error on .reindex(['a', 'b', 'c']).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([0, 1, 2], index=[\"a\", \"b\", \"b\"])\ns1.reindex([\"a\", \"b\", \"c\"])\n```\n\n----------------------------------------\n\nTITLE: Creating a Sample DataFrame in pandas\nDESCRIPTION: This code creates a sample DataFrame with various data types including strings, integers, unsigned integers, floating-point numbers, booleans, and dates. It's used as a demonstration dataset for ArcticDB operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"a\": list(\"abc\"),\n        \"b\": list(range(1, 4)),\n        \"c\": np.arange(3, 6).astype(\"u1\"),\n        \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n        \"e\": [True, False, True],\n        \"f\": pd.date_range(\"20130101\", periods=3)\n    }\n)\n\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Parallel Coordinates Plotting with Colormap using pandas and Matplotlib in Python\nDESCRIPTION: This snippet creates a parallel coordinates plot colored using Matplotlib's 'gist_rainbow' colormap via pandas plotting API. It requires pandas, matplotlib, and an appropriately structured DataFrame. The main parameters are the data source, the label column ('Name'), and the colormap. Input: DataFrame with a column for class/label; output: parallel coordinates chart with unique colors for each class. Limitation: dependent on data and explicit label column presence.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\nparallel_coordinates(data, \"Name\", colormap=\"gist_rainbow\");\n```\n\n----------------------------------------\n\nTITLE: Writing XML with Default Namespace using pandas.to_xml (Python)\nDESCRIPTION: Shows how to include a default XML namespace in the root element during DataFrame export using the namespaces argument. Only root-level namespaces are supported. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_129\n\nLANGUAGE: python\nCODE:\n```\nprint(geom_df.to_xml(namespaces={\"\": \"https://example.com\"}))\n```\n\n----------------------------------------\n\nTITLE: Boolean Comparisons and Null Handling in Series - Python\nDESCRIPTION: Demonstrates changes in comparing a pandas.Series containing None to None, and recommends the use of isnull for null checks. Shows how assignments of None to Series elements are handled and how the equality comparison vs. None now works without raising TypeError. Requires pandas and numpy, inputs are Series and None/nan values, outputs are boolean arrays.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(range(3), dtype=\"float\")\ns.iloc[1] = None\ns\n```\n\nLANGUAGE: python\nCODE:\n```\ns == None\n```\n\nLANGUAGE: python\nCODE:\n```\ns.isnull()\n```\n\nLANGUAGE: python\nCODE:\n```\nNone == None\nnp.nan == np.nan\n```\n\n----------------------------------------\n\nTITLE: Named Aggregation with NamedAgg\nDESCRIPTION: Shows how to use named aggregation with pd.NamedAgg to control output column names while applying different aggregation functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nanimals.groupby(\"kind\").agg(\n    min_height=pd.NamedAgg(column=\"height\", aggfunc=\"min\"),\n    max_height=pd.NamedAgg(column=\"height\", aggfunc=\"max\"),\n    average_weight=pd.NamedAgg(column=\"weight\", aggfunc=\"mean\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Obtaining unique values from timezone-aware Series in pandas (Python)\nDESCRIPTION: Shows the legacy result of Series.unique which returned a numpy.ndarray of Timestamp objects for datetime with timezone. Useful for highlighting the migration to DatetimeArray objects in pandas 0.24+. The input is a pandas Series of Timestamps; output is an array.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: ser.unique()\nOut[3]: array([Timestamp('2000-01-01 00:00:00+0000', tz='UTC')], dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Handling NaN Values in String Columns for HDF5 Storage\nDESCRIPTION: This code shows how to handle NaN values in string columns when storing data in HDF5 format. It demonstrates the default behavior and how to specify a custom NaN representation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_201\n\nLANGUAGE: python\nCODE:\n```\ndfss = pd.DataFrame({\"A\": [\"foo\", \"bar\", \"nan\"]})\ndfss\n\nstore.append(\"dfss\", dfss)\nstore.select(\"dfss\")\n\n# here you need to specify a different nan rep\nstore.append(\"dfss2\", dfss, nan_rep=\"_nan_\")\nstore.select(\"dfss2\")\n```\n\n----------------------------------------\n\nTITLE: Joining DataFrames on MultiIndexes - pandas (Python)\nDESCRIPTION: Shows how to join two DataFrames with MultiIndex objects using .join, aligning on the common 'key' level. Requires pandas >=0.24.0. Inputs are multi-level indexes constructed with from_tuples; outputs are the result of joining two DataFrames on overlapping index levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nindex_left = pd.MultiIndex.from_tuples([('K0', 'X0'), ('K0', 'X1'),\n                                       ('K1', 'X2')],\n                                       names=['key', 'X'])\n\nleft = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n                     'B': ['B0', 'B1', 'B2']}, index=index_left)\n\nindex_right = pd.MultiIndex.from_tuples([('K0', 'Y0'), ('K1', 'Y1'),\n                                        ('K2', 'Y2'), ('K2', 'Y3')],\n                                        names=['key', 'Y'])\n\nright = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3']}, index=index_right)\n\nleft.join(right)\n```\n\n----------------------------------------\n\nTITLE: Implementing memory_usage Method for StringArray (Python)\nDESCRIPTION: Added implementation for StringArray.memory_usage method, which was previously missing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_60\n\nLANGUAGE: Python\nCODE:\n```\nStringArray(['a', 'b', 'c']).memory_usage()\n```\n\n----------------------------------------\n\nTITLE: Row Extraction and Single Value Access for Categoricals - Pandas Python\nDESCRIPTION: Provides examples of extracting a single row or value from a categorical DataFrame. Shows that extracting a full row results in an object dtype Series, and scalar access returns the value, not a one-element Categorical. Useful for understanding coercion rules in pandas. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# get the complete \"h\" row as a Series\ndf.loc[\"h\", :]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.iat[0, 0]\ndf[\"cats\"] = df[\"cats\"].cat.rename_categories([\"x\", \"y\", \"z\"])\ndf.at[\"h\", \"cats\"]  # returns a string\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[[\"h\"], \"cats\"]\n```\n\n----------------------------------------\n\nTITLE: Binning Data with IntervalIndex Categories\nDESCRIPTION: This code shows how to use existing interval categories to bin other data, with NaN representing values that don't fall within any interval.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npd.cut([0, 3, 5, 1], bins=c.categories)\n```\n\n----------------------------------------\n\nTITLE: Storing Interval Data in Series - pandas (Python)\nDESCRIPTION: Demonstrates that pandas Series can now store intervals natively with the correct extension dtype, not just object arrays. Requires pandas >=0.24.0. Uses pd.interval_range to construct intervals and creates a Series; outputs show the data and its dtype. Improves performance and semantics when working with interval data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(pd.interval_range(0, 5))\nser\nser.dtype\n```\n\n----------------------------------------\n\nTITLE: Appending and Accessing DataFrames in HDF5 Store - pandas - Python\nDESCRIPTION: Shows how to append partitioned DataFrames into a pandas HDFStore and retrieve them using select. Appending automatically creates tables if not present, and select returns the stored object. Accessing internal PyTables attributes demonstrates type inspection. Requires pandas; inputs are DataFrames, output is persisted and accessed data in HDF5 format.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_171\n\nLANGUAGE: python\nCODE:\n```\nstore = pd.HDFStore(\"store.h5\")\ndf1 = df[0:4]\ndf2 = df[4:]\n\n# append data (creates a table automatically)\nstore.append(\"df\", df1)\nstore.append(\"df\", df2)\nstore\n\n# select the entire object\nstore.select(\"df\")\n\n# the type of stored data\nstore.root.df._v_attrs.pandas_type\n```\n\n----------------------------------------\n\nTITLE: Reading a CSV Without Header Using Pandas - Python\nDESCRIPTION: This snippet demonstrates how to create a pandas DataFrame from CSV-formatted data without headers, using 'header=None'. It shows that, following the 0.9.0 release, default column names are now integer-based ('0', '1', '2', ...) to improve attribute access and Python consistency. Dependencies include the pandas and io packages. Input is a multiline string with comma-separated values; output is a DataFrame with automatically assigned integer column names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.9.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\ndata = \"\"\"\n0,0,1\n1,1,0\n0,1,0\n\"\"\"\ndf = pd.read_csv(io.StringIO(data), header=None)\ndf\n```\n\n----------------------------------------\n\nTITLE: Series tolist() Returns Python Types Instead of NumPy Types - pandas - Python\nDESCRIPTION: Shows the change to Series.tolist() so that it returns entries as native Python types rather than NumPy types. Requires pandas and Series data. Input is a Series; output is a list of Python scalar types, e.g., int instead of numpy.int64.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3])\n```\n\nLANGUAGE: python\nCODE:\n```\ntype(s.tolist()[0])\n```\n\n----------------------------------------\n\nTITLE: Demonstrating New DataFrame Broadcasting Behavior in Python\nDESCRIPTION: This snippet shows how DataFrame broadcasting now works with 2D NumPy arrays in pandas 0.24.0. It creates a DataFrame and demonstrates broadcasting with 1-row and 1-column arrays.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\narr = np.arange(6).reshape(3, 2)\ndf = pd.DataFrame(arr)\ndf\n\ndf + arr[[0], :]   # 1 row, 2 columns\ndf + arr[:, [1]]   # 1 column, 3 rows\n```\n\n----------------------------------------\n\nTITLE: Updating Columns with Conditional Assignment Using .loc in pandas - Python\nDESCRIPTION: Performs an in-place update of the 'tip' column in the 'tips' DataFrame, doubling values where 'tip' is less than 2. This direct assignment ensures only qualifying rows are modified. Requires pandas and a DataFrame named 'tips' with a 'tip' column. No new object is returned; the original DataFrame is modified.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntips.loc[tips[\"tip\"] < 2, \"tip\"] *= 2\n```\n\n----------------------------------------\n\nTITLE: Indexing pandas Series with TimedeltaIndex - Python\nDESCRIPTION: These snippets create a Series indexed by a TimedeltaIndex and demonstrate value selection by exact or range-based indexers using timedelta strings. Inputs: range of timedeltas and integer values; outputs: a Series and results of various time-based selection operations. Dependencies are pandas and NumPy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.arange(5),\n             index=pd.timedelta_range('1 days', periods=5, freq='s'))\ns\n\ns['1 day 00:00:02']\ns['1 day':'1 day 00:00:02']\n```\n\n----------------------------------------\n\nTITLE: Specifying Format in pandas read_sas in Python\nDESCRIPTION: Shows how to explicitly specify the file format ('xport' or 'sas7bdat') when reading SAS files into pandas using read_sas. Requires pandas. Input: SAS file path and format string; output: DataFrame. Ensures correct loader is used; 'format' argument helps avoid ambiguity.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_sas(\"transport-file.xpt\", format=\"xport\")\ndf = pd.read_sas(\"binary-file.sas7bdat\", format=\"sas7bdat\")\n```\n\n----------------------------------------\n\nTITLE: Using String Alias for Nullable Integer Dtype in pandas (Python)\nDESCRIPTION: Demonstrates constructing a nullable integer array using the string alias 'Int64' for the dtype, differentiating it from NumPy's 'int64'. Any NA-like value (including np.nan) will be promoted to pandas.NA in the resulting IntegerArray. Assumes NumPy is imported as np and pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npd.array([1, 2, np.nan], dtype=\"Int64\")\n```\n\n----------------------------------------\n\nTITLE: Splitting Strings and Extracting Surnames\nDESCRIPTION: Shows how to split strings by a delimiter and extract specific parts from the resulting lists. The example splits passenger names by comma and extracts the surname portion.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/10_text_data.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Name\"].str.split(\",\")\n```\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Surname\"] = titanic[\"Name\"].str.split(\",\").str.get(0)\ntitanic[\"Surname\"]\n```\n\n----------------------------------------\n\nTITLE: Checking data types in DataFrame to_dict output in Python\nDESCRIPTION: Demonstrates checking the data type of values in a dictionary created from a pandas DataFrame using to_dict().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nd = {'a': [1], 'b': ['b']}\ndf = pd.DataFrame(d)\ntype(df.to_dict()['a'][0])\n```\n\n----------------------------------------\n\nTITLE: Renaming MultiIndex Levels Using DataFrame/Series.rename_axis (Python)\nDESCRIPTION: Shows how to change names of MultiIndex levels on a DataFrame using index mapping or string transformation functions. Requires pandas, a DataFrame with MultiIndex, and a mapping or function for the level names. Outputs a DataFrame with renamed index levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"x\": [1, 2, 3, 4, 5, 6], \"y\": [10, 20, 30, 40, 50, 60]},\n    index=pd.MultiIndex.from_product(\n        [[\"a\", \"b\", \"c\"], [1, 2]], names=[\"let\", \"num\"]\n    ),\n)\ndf\ndf.rename_axis(index={\"let\": \"abc\"})\ndf.rename_axis(index=str.upper)\n```\n\n----------------------------------------\n\nTITLE: Reading and Caching Data from S3 with fsspec simplecache via Pandas (Python)\nDESCRIPTION: Shows advanced fsspec usage by using 'simplecache::' with S3 URLs, enabling local caching of remote files. The storage_options dictionary specifies configuration for the S3 layer. Requires fsspec and s3fs. The data is returned as a DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(\n    \"simplecache::s3://ncei-wcsd-archive/data/processed/SH1305/18kHz/\"\n    \"SaKe2013-D20130523-T080854_to_SaKe2013-D20130523-T085643.csv\",\n    storage_options={\"s3\": {\"anon\": True}},\n)\n```\n\n----------------------------------------\n\nTITLE: Hierarchical Key Storage and Manipulation in HDFStore - pandas - Python\nDESCRIPTION: Demonstrates usage of hierarchical keys in an HDFStore: storing DataFrames with nested keys, retrieving the list of keys, and removing subgroups. Shows that keys are absolute and support directory-like structure. Requires pandas; inputs are DataFrames, and outputs include the HDFStore structure and manipulated keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_172\n\nLANGUAGE: python\nCODE:\n```\nstore.put(\"foo/bar/bah\", df)\nstore.append(\"food/orange\", df)\nstore.append(\"food/apple\", df)\nstore\n\n# a list of keys are returned\nstore.keys()\n\n# remove all nodes under this level\nstore.remove(\"food\")\nstore\n```\n\n----------------------------------------\n\nTITLE: Setting Metadata for Pandas Index Objects\nDESCRIPTION: This code demonstrates how to set and change the name attribute of Index objects using methods like rename() and set_names(), as well as direct attribute assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_72\n\nLANGUAGE: python\nCODE:\n```\nind = pd.Index([1, 2, 3])\nind.rename(\"apple\")\nind\nind = ind.set_names([\"apple\"])\nind.name = \"bob\"\nind\n\nindex = pd.MultiIndex.from_product([range(3), ['one', 'two']], names=['first', 'second'])\nindex\nindex.levels[1]\nindex.set_levels([\"a\", \"b\"], level=1)\n```\n\n----------------------------------------\n\nTITLE: Joining DataFrames with Multi-Key Join - pandas - Python\nDESCRIPTION: This snippet demonstrates how to join two pandas DataFrames on multiple key columns (\"key1\", \"key2\"). It creates two DataFrames with complex indices and combines them using DataFrame.join, supporting both default (left) and inner join types. Prerequisites include the pandas library and properly structured DataFrames with suitable indices. The expected output is a merged DataFrame aligned on the specified keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nright = pd.DataFrame(\n    {\"C\": [\"C0\", \"C1\", \"C2\", \"C3\"], \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"]}, index=index\n)\nresult = left.join(right, on=[\"key1\", \"key2\"])\nresult\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = left.join(right, on=[\"key1\", \"key2\"], how=\"inner\")\nresult\n```\n\n----------------------------------------\n\nTITLE: Selecting a Single Column from an HDFStore Table with select_column - pandas HDFStore - Python\nDESCRIPTION: This snippet demonstrates retrieving a single indexable or data column from an HDF5 table using store.select_column in pandas' HDFStore. It returns a pandas Series of the column, indexed by row number, and does not support where selectors. Dependencies: pandas, HDFStore with 'df_dc'. Inputs: table and column names. Outputs: Series object for the requested column. Limitation: Only top-level column retrieval without filtering.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_191\n\nLANGUAGE: python\nCODE:\n```\nstore.select_column(\"df_dc\", \"index\")\nstore.select_column(\"df_dc\", \"string\")\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame and Function for apply/applymap Consistency - Python\nDESCRIPTION: Demonstrates creating a small DataFrame and a function for testing pandas' apply/applymap behavior, especially for verifying that the first row or column is processed only once. The function prints and returns its input, serving to record call patterns. Only pandas and core Python needed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': [1, 2], 'b': [3, 6]})\\n\\ndef func(row):\\n    print(row)\\n    return row\n```\n\n----------------------------------------\n\nTITLE: Using String and Datetime Accessors on Categoricals - Pandas Python\nDESCRIPTION: Illustrates use of .str and .dt accessors on Series with categorical dtype, provided the categories are appropriate types. Shows correct handling of string search and datetime properties when underlying categories permit it. Output types match those of equivalent Series of base type. Requires pandas, input is Series of strings or datetimes cast to category.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nstr_s = pd.Series(list(\"aabb\"))\nstr_cat = str_s.astype(\"category\")\nstr_cat\nstr_cat.str.contains(\"a\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndate_s = pd.Series(pd.date_range(\"1/1/2015\", periods=5))\ndate_cat = date_s.astype(\"category\")\ndate_cat\ndate_cat.dt.day\n```\n\n----------------------------------------\n\nTITLE: Boxplot using GroupBy Method\nDESCRIPTION: This example demonstrates creating boxplots using groupby().boxplot(). This splits the data by the grouping variable first, then creates boxplots for each numeric column within each group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nbp = df_box.groupby(\"g\").boxplot()\n```\n\n----------------------------------------\n\nTITLE: Assigning Dependent Columns with DataFrame.assign - Pandas - Python\nDESCRIPTION: Illustrates enhanced .assign() functionality where later keyword arguments can depend on earlier ones if supplied as a callable, available in Python 3.6+. Requires pandas >=0.23 and Python 3.6+. Shows creating a DataFrame, assigning new columns B and C where C is a function of previously computed B. Demonstrates correct ordering and usage patterns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': [1, 2, 3]})\ndf\ndf.assign(B=df.A, C=lambda x: x['A'] + x['B'])\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Copy-on-Write Mode in pandas (Python)\nDESCRIPTION: Demonstrates how to enable the upcoming default Copy-on-Write mode in pandas using pd.options. This impacts the behavior of DataFrame and Series operations with respect to copies and views, potentially surfacing new warnings. Requires pandas 2.0 or later. The 'warn' mode further exposes behavioral changes for migration purposes. No inputs or outputs except change in global behavior, and must be used before creating relevant pandas objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_0\n\nLANGUAGE: ipython\nCODE:\n```\npd.options.mode.copy_on_write = True\n```\n\nLANGUAGE: ipython\nCODE:\n```\npd.options.mode.copy_on_write = \"warn\"\n```\n\n----------------------------------------\n\nTITLE: Time Series Resampling with Updated Behavior in pandas - ipython\nDESCRIPTION: This ipython code snippet shows the updated default behavior for the resample() method when working with time series indexed by dates. The first example uses the new default, while the second specifies the old behavior with explicit closed and label arguments. It helps users understand how aggregation labeling and binning have changed, and ensures correct migration of resampling logic.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_1\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: dates = pd.date_range('1/1/2000', '1/5/2000', freq='4h')\n\nIn [2]: series = pd.Series(np.arange(len(dates)), index=dates)\n\nIn [3]: series\nOut[3]:\n2000-01-01 00:00:00     0\n2000-01-01 04:00:00     1\n2000-01-01 08:00:00     2\n2000-01-01 12:00:00     3\n2000-01-01 16:00:00     4\n2000-01-01 20:00:00     5\n2000-01-02 00:00:00     6\n2000-01-02 04:00:00     7\n2000-01-02 08:00:00     8\n2000-01-02 12:00:00     9\n2000-01-02 16:00:00    10\n2000-01-02 20:00:00    11\n2000-01-03 00:00:00    12\n2000-01-03 04:00:00    13\n2000-01-03 08:00:00    14\n2000-01-03 12:00:00    15\n2000-01-03 16:00:00    16\n2000-01-03 20:00:00    17\n2000-01-04 00:00:00    18\n2000-01-04 04:00:00    19\n2000-01-04 08:00:00    20\n2000-01-04 12:00:00    21\n2000-01-04 16:00:00    22\n2000-01-04 20:00:00    23\n2000-01-05 00:00:00    24\nFreq: 4H, dtype: int64\n\nIn [4]: series.resample('D', how='sum')\nOut[4]:\n2000-01-01     15\n2000-01-02     51\n2000-01-03     87\n2000-01-04    123\n2000-01-05     24\nFreq: D, dtype: int64\n\nIn [5]: # old behavior\nIn [6]: series.resample('D', how='sum', closed='right', label='right')\nOut[6]:\n2000-01-01      0\n2000-01-02     21\n2000-01-03     57\n2000-01-04     93\n2000-01-05    129\nFreq: D, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Using Pivot Table with Grouper in Pandas\nDESCRIPTION: Demonstrates using pivot_table with Grouper objects for date-based grouping on both index and columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\n    'Branch': 'A A A A A B'.split(),\n    'Buyer': 'Carl Mark Carl Carl Joe Joe'.split(),\n    'Quantity': [1, 3, 5, 1, 8, 1],\n    'Date': [datetime.datetime(2013, 11, 1, 13, 0),\n             datetime.datetime(2013, 9, 1, 13, 5),\n             datetime.datetime(2013, 10, 1, 20, 0),\n             datetime.datetime(2013, 10, 2, 10, 0),\n             datetime.datetime(2013, 11, 1, 20, 0),\n             datetime.datetime(2013, 10, 2, 10, 0)],\n    'PayDay': [datetime.datetime(2013, 10, 4, 0, 0),\n               datetime.datetime(2013, 10, 15, 13, 5),\n               datetime.datetime(2013, 9, 5, 20, 0),\n               datetime.datetime(2013, 11, 2, 10, 0),\n               datetime.datetime(2013, 10, 7, 20, 0),\n               datetime.datetime(2013, 9, 5, 10, 0)]})\n```\n\n----------------------------------------\n\nTITLE: DataFrame Query Combining Object and Numeric Columns - Python\nDESCRIPTION: Demonstrates a DataFrame query involving both string (object) and numeric columns, where part of the expression will be evaluated by Python (object comparison) and another part by numexpr (numeric comparison). Requires pandas and numpy. Highlights that only the numeric part benefits from numexpr, while object dtype limits performance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"strings\": np.repeat(list(\"cba\"), 3), \"nums\": np.repeat(range(3), 3)}\n)\ndf\ndf.query(\"strings == 'a' and nums == 1\")\n```\n\n----------------------------------------\n\nTITLE: Obtaining Unique Values from Index-Like Objects (ipython, Python)\nDESCRIPTION: Demonstrates that Index.unique() now consistently returns an Index object of the appropriate dtype, including for standard Index and DatetimeIndex with timezone. Requires pandas; input is Index or DatetimeIndex, output is always an Index of unique values, maintaining necessary metadata.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_38\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pd.Index([1, 2, 3]).unique()\nOut[1]: array([1, 2, 3])\n\nIn [2]: pd.DatetimeIndex(['2011-01-01', '2011-01-02',\n   ...:                   '2011-01-03'], tz='Asia/Tokyo').unique()\nOut[2]:\nDatetimeIndex(['2011-01-01 00:00:00+09:00', '2011-01-02 00:00:00+09:00',\n               '2011-01-03 00:00:00+09:00'],\n              dtype='datetime64[ns, Asia/Tokyo]', freq=None)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index([1, 2, 3]).unique()\npd.DatetimeIndex(\n    [\"2011-01-01\", \"2011-01-02\", \"2011-01-03\"], tz=\"Asia/Tokyo\"\n).unique()\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregating Data in pandas\nDESCRIPTION: Shows how to group data and compute the mean for each group in pandas using the groupby method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"v1\": [1, 3, 5, 7, 8, 3, 5, np.nan, 4, 5, 7, 9],\n        \"v2\": [11, 33, 55, 77, 88, 33, 55, np.nan, 44, 55, 77, 99],\n        \"by1\": [\"red\", \"blue\", 1, 2, np.nan, \"big\", 1, 2, \"red\", 1, np.nan, 12],\n        \"by2\": [\n            \"wet\",\n            \"dry\",\n            99,\n            95,\n            np.nan,\n            \"damp\",\n            95,\n            99,\n            \"red\",\n            99,\n            np.nan,\n            np.nan,\n        ],\n    }\n)\n\ng = df.groupby([\"by1\", \"by2\"])\ng[[\"v1\", \"v2\"]].mean()\n```\n\n----------------------------------------\n\nTITLE: Applying NumPy Binary Universal Functions on Series with Partially Overlapping Indices (Python)\nDESCRIPTION: Demonstrates that np.remainder aligns Series ser1 (with index [\"a\", \"b\", \"c\"]) and ser3 (with index [\"b\", \"c\", \"d\"]) by union of their indices before applying the function. Missing values appear where indices do not overlap. Dependencies: pandas, NumPy. Inputs are two Series; the output is a Series with potentially missing (NaN) elements.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nser3 = pd.Series([2, 4, 6], index=[\"b\", \"c\", \"d\"])\nser3\nnp.remainder(ser1, ser3)\n```\n\n----------------------------------------\n\nTITLE: Replacing Values in Groups with Group-wise Means Using transform - Pandas - Python\nDESCRIPTION: Creates a grouped DataFrame, defines a mask for negative values, and replaces them with the mean of the non-negative group members. This uses groupby.transform with a custom function. Requires pandas, a DataFrame with suitable structure, and returns a transformed DataFrame where certain values are conditionally set to the group's mean.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1, 1, 2, 2], \"B\": [1, -1, 1, 2]})\ngb = df.groupby(\"A\")\n\ndef replace(g):\n    mask = g < 0\n    return g.where(~mask, g[~mask].mean())\n\ngb.transform(replace)\n```\n\n----------------------------------------\n\nTITLE: Subtraction between Period objects returns int (legacy pandas, Python)\nDESCRIPTION: Shows legacy subtraction between pandas Period objects returning an integer (period difference). Inputs: two Period objects (e.g., 'June 2018', 'April 2018'); output: integer. This behavior now returns DateOffset in newer pandas versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: june = pd.Period('June 2018')\n\nIn [3]: april = pd.Period('April 2018')\n\nIn [4]: june - april\nOut [4]: 2\n```\n\n----------------------------------------\n\nTITLE: Initializing a Simple DataFrame with Custom Index - pandas - Python\nDESCRIPTION: This snippet initializes a pandas DataFrame with one column 'A' and three rows, setting a custom index of ['foo', 'bar', 'baz']. This setup is commonly used to demonstrate indexing, selection, and other DataFrame operations. Requires pandas to be imported as pd; input is a dictionary for columns and a list for the index; output is a DataFrame instance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': [1, 2, 3]}, index=['foo', 'bar', 'baz'])\n```\n\n----------------------------------------\n\nTITLE: Accessing DataFrame Subsets with Label Slices\nDESCRIPTION: Demonstrates accessing a subset of a DataFrame using .loc with label slicing for both rows and columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf1.loc['d':, 'A':'C']\n```\n\n----------------------------------------\n\nTITLE: Datetime Operations on Arrow-Backed Series (pandas Python API)\nDESCRIPTION: Shows datetime accessor usage (e.g., .dt.strftime) on Series with Arrow-backed timestamp dtypes, enabling efficient datetime formatting. Required dependencies are pandas and pyarrow; expected input is a Series of datetimes with ArrowDtype, and output is the formatted string Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\npa_type = pd.ArrowDtype(pa.timestamp(\"ns\"))\nser_dt = pd.Series([datetime(2022, 1, 1), None], dtype=pa_type)\nser_dt.dt.strftime(\"%Y-%m\")\n```\n\n----------------------------------------\n\nTITLE: Using CustomBusinessDay Offsets for Date Arithmetic - Python\nDESCRIPTION: This snippet shows how to create and use a CustomBusinessDay offset in pandas for regions with custom weekends and holidays, such as Egypt's Friday-Saturday weekend plus annual Workers' Day. Requires: pandas, numpy (>=1.7.0) for busdaycalendar support, and datetime. Inputs are weekmask and list of holidays, outputs are date arithmetic results and weekday mapping. Useful for business date calculations in locale-specific calendars.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.tseries.offsets import CustomBusinessDay\nfrom datetime import datetime\n\n# As an interesting example, let's look at Egypt where\n# a Friday-Saturday weekend is observed.\nweekmask_egypt = \"Sun Mon Tue Wed Thu\"\n# They also observe International Workers' Day so let's\n# add that for a couple of years\nholidays = [\"2012-05-01\", datetime(2013, 5, 1), np.datetime64(\"2014-05-01\")]\nbday_egypt = CustomBusinessDay(holidays=holidays, weekmask=weekmask_egypt)\ndt = datetime(2013, 4, 30)\nprint(dt + 2 * bday_egypt)\ndts = pd.date_range(dt, periods=5, freq=bday_egypt)\nprint(pd.Series(dts.weekday, dts).map(pd.Series(\"Mon Tue Wed Thu Fri Sat Sun\".split())))\n```\n\n----------------------------------------\n\nTITLE: Setting MultiIndex Levels and Labels in Python\nDESCRIPTION: Demonstrates the new API for handling MultiIndex metadata, including setting levels, labels and names using dedicated methods rather than direct assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# previously, you would have set levels or labels directly\n>>> pd.index.levels = [[1, 2, 3, 4], [1, 2, 4, 4]]\n\n# now, you use the set_levels or set_labels methods\n>>> index = pd.index.set_levels([[1, 2, 3, 4], [1, 2, 4, 4]])\n\n# similarly, for names, you can rename the object\n# but setting names is not deprecated\n>>> index = pd.index.set_names([\"bob\", \"cranberry\"])\n\n# and all methods take an inplace kwarg - but return None\n>>> pd.index.set_names([\"bob\", \"cranberry\"], inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Comparing Series Indexing Performance\nDESCRIPTION: Compares the performance of accessing Series elements using iloc indexing versus the take method with a shuffled array of indices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(arr[:, 0])\n%timeit ser.iloc[indexer]\n%timeit ser.take(indexer)\n```\n\n----------------------------------------\n\nTITLE: GroupBy nth Method Consistency Demonstrations in Pandas (Python)\nDESCRIPTION: Demonstrates new and previous behaviors of DataFrame GroupBy nth method with as_index argument. Requires Pandas. Shows DataFrame groupby by column 'A' and retrieving the nth value from another column. Input is a DataFrame grouped by a key; output is a Series showing the selected elements, demonstrating both as_index=True and as_index=False cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [\"a\", \"b\", \"a\"], \"B\": [1, 2, 3]})\ndf\ndf.groupby(\"A\", as_index=True)[\"B\"].nth(0)\ndf.groupby(\"A\", as_index=False)[\"B\"].nth(0)\n```\n\n----------------------------------------\n\nTITLE: IPython Startup Script for Setting pandas Display Options - Python\nDESCRIPTION: An example startup script for IPython or Python environments that preconfigures pandas display options on startup. Used by placing this script in the IPython startup directory. Sets commonly tailored options like 'display.max_rows' and 'display.precision'. Only pandas is required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\npd.set_option(\"display.max_rows\", 999)\npd.set_option(\"display.precision\", 5)\n```\n\n----------------------------------------\n\nTITLE: Selecting from MultiIndex DataFrames with Named and Unnamed Levels - pandas - Python\nDESCRIPTION: Demonstrates querying MultiIndex DataFrames in HDFStore using both level names and default 'level_n' keyword when names are None. Covers both the case with named levels and with unnamed levels, showing flexibility in hierarchical queries. Requires pandas and numpy; outputs are indexes for selection and filtered DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_182\n\nLANGUAGE: python\nCODE:\n```\ndf_mi.index.names\nstore.select(\"df_mi\", \"foo=baz and bar=two\")\n```\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.MultiIndex(\n    levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n    codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n)\ndf_mi_2 = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\ndf_mi_2\n\nstore.append(\"df_mi_2\", df_mi_2)\n\n# the levels are automatically included as data columns with keyword level_n\nstore.select(\"df_mi_2\", \"level_0=foo and level_1=two\")\n```\n\n----------------------------------------\n\nTITLE: Subtraction between Period objects returns DateOffset (pandas >=0.24, Python)\nDESCRIPTION: Shows updated pandas Period subtraction now yields DateOffset rather than an integer, improving type correctness. Inputs: two Period objects. Output: a DateOffset instance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\njune = pd.Period('June 2018')\napril = pd.Period('April 2018')\njune - april\n```\n\n----------------------------------------\n\nTITLE: Grouping by Categorical Fields with the observed Keyword - Pandas - Python\nDESCRIPTION: Prepares a DataFrame with two categorical columns (with unused categories included) and a data column to demonstrate the effects of the new observed parameter in groupby. Shows how inclusion or exclusion of unobserved category combinations affects groupby results. Requires pandas >=0.23. Inputs are categorical columns with categories defined; outputs change depending on observed parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncat1 = pd.Categorical([\"a\", \"a\", \"b\", \"b\"],\n                      categories=[\"a\", \"b\", \"z\"], ordered=True)\ncat2 = pd.Categorical([\"c\", \"d\", \"c\", \"d\"],\n                      categories=[\"c\", \"d\", \"y\"], ordered=True)\ndf = pd.DataFrame({\"A\": cat1, \"B\": cat2, \"values\": [1, 2, 3, 4]})\ndf['C'] = ['foo', 'bar'] * 2\ndf\n\n```\n\n----------------------------------------\n\nTITLE: New Behavior: Groupby with as_index=True Only in Index - python\nDESCRIPTION: Highlights that now, the same groupby operation places only the grouping column(s) in the index, with columns containing reduction results. Demonstrates greater consistency across groupby methods and result formatting. Input: DataFrame, groupby column, as_index parameter, and aggregation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"a\", as_index=True).nunique()\n```\n\n----------------------------------------\n\nTITLE: Working with Factors and Categorical Data - base::cut/factor - R\nDESCRIPTION: This R snippet illustrates the use of cut to discretize numeric vectors into factors and factor to explicitly create categorical variables. Input is a numeric vector; output is a factor (categorical) variable. Useful in statistical modeling and plotting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_24\n\nLANGUAGE: R\nCODE:\n```\ncut(c(1,2,3,4,5,6), 3)\nfactor(c(1,2,3,2,2,3))\n```\n\n----------------------------------------\n\nTITLE: GroupBy Resample with Apply in Pandas (Python)\nDESCRIPTION: Demonstrates the output types of GroupBy resampling when using the apply method with a time-based grouper. Requires Pandas and datetime support. In the new behavior, groupby is performed on a 'date' column with monthly frequency, applying aggregation/lambda functions that either sum values or sum within sub-DataFrames. Inputs are a DataFrame and a TimeGrouper for the 'date' column; outputs are Series or DataFrame depending on the aggregation function used.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"date\": pd.to_datetime([\"10/10/2000\", \"11/10/2000\"]), \"value\": [10, 13]}\n)\ndf\ndf.groupby(pd.TimeGrouper(key='date', freq='M')).apply(lambda x: x.value.sum())\ndf.groupby(pd.TimeGrouper(key='date', freq='M')).apply(lambda x: x[['value']].sum())\n```\n\n----------------------------------------\n\nTITLE: Converting Panel to Xarray - pandas/xarray - Python\nDESCRIPTION: Demonstrates the new .to_xarray() method to seamlessly convert Panel (3D NDFrame) objects to xarray DataArray, preparing for Panel deprecation. Requires pandas (with Panel support) and xarray installed, input is a 3D Panel of integers, output is a structured xarray.DataArray with coordinates and values. Key parameter is data shape; output is a multidimensional xarray suitable for advanced analytics.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\np = Panel(np.arange(2*3*4).reshape(2,3,4))\np.to_xarray()\n```\n\n----------------------------------------\n\nTITLE: Creating Series and Demonstrating Inclusive Label-Based Slicing - pandas - Python\nDESCRIPTION: Creates a pandas Series with custom string labels and demonstrates retrieving the Series object. Inputs are random numbers as values and a string list as the index. Requires pandas and numpy to be imported (as pd and np, respectively). Used as a setup for demonstrating slicing behaviors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.random.randn(6), index=list(\"abcdef\"))\ns\n```\n\n----------------------------------------\n\nTITLE: Exporting Data to CSV - Stata\nDESCRIPTION: The 'export delimited tips2.csv' command exports the current Stata data set to a CSV file named 'tips2.csv' in the current directory. Input: Existing data in memory. Output: Created/overwritten file 'tips2.csv'. No additional dependencies required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_6\n\nLANGUAGE: stata\nCODE:\n```\nexport delimited tips2.csv\n```\n\n----------------------------------------\n\nTITLE: Exporting DataFrame to LaTeX with Styler - pandas - Python\nDESCRIPTION: Exports a DataFrame to LaTeX format using the preferred 'Styler.to_latex' method, which allows for more flexible styling options compared to 'DataFrame.to_latex'. The snippet creates a DataFrame, applies the style export, and prints the resulting LaTeX code. Recommended for users who need conditional formatting or advanced LaTeX table customization.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_110\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[1, 2], [3, 4]], index=[\"a\", \"b\"], columns=[\"c\", \"d\"])\nprint(df.style.to_latex())\n```\n\n----------------------------------------\n\nTITLE: min, max, idxmin, idxmax Operations on DataFrame of Timedeltas - pandas - Python\nDESCRIPTION: Illustrates use of min, max, idxmin, idxmax reduction operations on DataFrames of timedeltas constructed from Timestamps and Timedeltas. Shows axis-wise aggregation and index retrieval for min/max operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nA = s - pd.Timestamp(\"20120101\") - pd.Timedelta(\"00:05:05\")\nB = s - pd.Series(pd.date_range(\"2012-1-2\", periods=3, freq=\"D\"))\n\ndf = pd.DataFrame({\"A\": A, \"B\": B})\ndf\n\ndf.min()\ndf.min(axis=1)\n\ndf.idxmin()\ndf.idxmax()\n```\n\n----------------------------------------\n\nTITLE: Handling Special Index Names with JSON Round-Trip - Pandas - Python\nDESCRIPTION: Shows the behavior when the DataFrame index is named 'index' and serialized using 'orient=table'. Highlights specific limitations due to the reserved status of 'index' in this context. The example demonstrates assignment of index name, serialization, deserialization, and output of the resulting DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf.index.name = 'index'\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.to_json('test.json', orient='table')\nnew_df = pd.read_json('test.json', orient='table')\nnew_df\nnew_df.dtypes\n\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.remove('test.json')\n\n```\n\n----------------------------------------\n\nTITLE: Raising DuplicateLabelError with set_flags on Series - pandas - Python\nDESCRIPTION: Attempts to set allows_duplicate_labels=False on a Series with duplicate labels, which raises a DuplicateLabelError. Demonstrates explicit enforcement of label uniqueness. The Series is constructed with duplicate labels and then set_flags is used.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npd.Series([0, 1, 2], index=[\"a\", \"b\", \"b\"]).set_flags(allows_duplicate_labels=False)\n```\n\n----------------------------------------\n\nTITLE: Comparing Array Access Performance in NumPy\nDESCRIPTION: Demonstrates performance comparison between direct array indexing with an array of indices versus using NumPy's take method for accessing elements from a random array.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\narr = np.random.randn(10000, 5)\nindexer = np.arange(10000)\nrandom.shuffle(indexer)\n\n%timeit arr[indexer]\n%timeit arr.take(indexer, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with Custom True/False Value Interpretation in pandas - ipython\nDESCRIPTION: Demonstrates the use of true_values and false_values parameters in pd.read_csv() to coerce specific string literals (e.g., 'Yes', 'No') into boolean values, instead of the default string treatment. Useful for customizing file loading behavior for mixed-type data and ensuring correct dtype assignment during ingest.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_4\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: print(data)\n\n    a,b,c\n    1,Yes,2\n    3,No,4\n\nIn [5]: pd.read_csv(io.StringIO(data))\nOut[5]:\n       a    b  c\n0      1  Yes  2\n1      3   No  4\n\nIn [6]: pd.read_csv(io.StringIO(data), true_values=[\"Yes\"], false_values=[\"No\"])\nOut[6]:\n       a      b  c\n0      1   True  2\n1      3  False  4\n```\n\n----------------------------------------\n\nTITLE: Parsing datetime strings with timezone offsets in Pandas\nDESCRIPTION: Shows how to_datetime now preserves UTC offsets in the tz attribute when all datetime strings have the same UTC offset. This aligns the behavior with Timestamp.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime(\"2015-11-18 15:30:00+05:30\")\npd.Timestamp(\"2015-11-18 15:30:00+05:30\")\n```\n\n----------------------------------------\n\nTITLE: Deprecated Indexing with Missing Labels in Python\nDESCRIPTION: Shows the deprecated behavior of indexing a Series with a list containing missing labels, which now raises a FutureWarning. In the future, this will raise a KeyError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3])\ns\ns.loc[[1, 2, 3]]\n```\n\n----------------------------------------\n\nTITLE: Deprecated Grouped Series Aggregation with Dict (Renaming) - Pandas - Python\nDESCRIPTION: Illustrates a deprecated syntax for aggregating and renaming with groupby on a Series using .agg() with a dict, which will be removed in the future. Input: DataFrame df. Output: DataFrame with custom column name. Use is discouraged; future versions will require separate renaming.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nIn [6]: df.groupby('A').B.agg({'foo': 'count'})\nFutureWarning: using a dict on a Series for aggregation\nis deprecated and will be removed in a future version\n\nOut[6]:\n   foo\nA    \n1   3\n2   2\n```\n\n----------------------------------------\n\nTITLE: Handling KeyError in .loc Access with MultiIndex in Pandas (python)\nDESCRIPTION: This Python example demonstrates MultiIndex Series construction and exception handling for missing index access. Dependencies: pandas, numpy. Inputs: Series values, MultiIndex created from product. Output: prints KeyError message if index not found. Useful for robust code accessing multi-level indexes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.arange(3, dtype='int64'),\n              index=pd.MultiIndex.from_product([['A'],\n                                               ['foo', 'bar', 'baz']],\n                                               names=['one', 'two'])\n              ).sort_index()\ns\ntry:\n    s.loc[['D']]\nexcept KeyError as e:\n    print(\"KeyError: \" + str(e))\n```\n\n----------------------------------------\n\nTITLE: Reading and Displaying a Custom Table Structure Template in Python\nDESCRIPTION: Reads a custom HTML table structure template from a file and then renders it as raw HTML using IPython's HTML function. Requires the 'html_table_structure.html' file in the 'templates' folder, and is intended for verifying or previewing template structure before linking to pandas' Styler rendering logic.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"templates/html_table_structure.html\") as f_table_struct:\n    table_structure = f_table_struct.read()\n```\n\nLANGUAGE: python\nCODE:\n```\nHTML(table_structure)\n```\n\n----------------------------------------\n\nTITLE: Handling MultiIndex Columns in melt Function (Python)\nDESCRIPTION: Fixed a bug where melting MultiIndex columns with col_level > 0 would raise a KeyError on id_vars.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\npd.melt(df, id_vars=['A'], value_vars=['B'])\n```\n\n----------------------------------------\n\nTITLE: Generating and Printing HTML Representation of a Styled DataFrame in Pandas (Python)\nDESCRIPTION: This example generates a DataFrame with index and column labels, styles it, and prints the generated HTML using the Styler.to_html method. Requires pandas. Inputs are a simple numeric matrix with named rows and columns; output is the raw HTML string representing the styled table, elucidating how internal classes and ids are embedded in the output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nprint(\n    pd.DataFrame(\n        [[1, 2], [3, 4]], index=[\"i1\", \"i2\"], columns=[\"c1\", \"c2\"]\n    ).style.to_html()\n)\n```\n\n----------------------------------------\n\nTITLE: Melting a DataFrame with MultiIndex in Python\nDESCRIPTION: Demonstrates melting a DataFrame with a MultiIndex, showing how to preserve or ignore the index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.MultiIndex.from_tuples([(\"person\", \"A\"), (\"person\", \"B\")])\ncheese = pd.DataFrame(\n    {\n        \"first\": [\"John\", \"Mary\"],\n        \"last\": [\"Doe\", \"Bo\"],\n        \"height\": [5.5, 6.0],\n        \"weight\": [130, 150],\n    },\n    index=index,\n)\ncheese\ncheese.melt(id_vars=[\"first\", \"last\"])\ncheese.melt(id_vars=[\"first\", \"last\"], ignore_index=False)\n```\n\n----------------------------------------\n\nTITLE: Advanced MultiIndex Selection with xs in Python\nDESCRIPTION: Creates a complex MultiIndex DataFrame and demonstrates various selection techniques. This example shows how to use slices and the slice() function for advanced selection from hierarchical indices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport itertools\n\nindex = list(itertools.product([\"Ada\", \"Quinn\", \"Violet\"], [\"Comp\", \"Math\", \"Sci\"]))\nheadr = list(itertools.product([\"Exams\", \"Labs\"], [\"I\", \"II\"]))\nindx = pd.MultiIndex.from_tuples(index, names=[\"Student\", \"Course\"])\ncols = pd.MultiIndex.from_tuples(headr)  # Notice these are un-named\ndata = [[70 + x + y + (x * y) % 3 for x in range(4)] for y in range(9)]\ndf = pd.DataFrame(data, indx, cols)\ndf\n\nAll = slice(None)\ndf.loc[\"Violet\"]\ndf.loc[(All, \"Math\"), All]\ndf.loc[(slice(\"Ada\", \"Quinn\"), \"Math\"), All]\ndf.loc[(All, \"Math\"), (\"Exams\")]\ndf.loc[(All, \"Math\"), (All, \"II\")]\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying a Large DataFrame with Option Context - pandas - Python\nDESCRIPTION: Demonstrates how to create a large DataFrame and control its displayed output using pandas display settings within a context. Uses np.arange to fill the DataFrame, then leverages pd.option_context to temporarily set display.max_rows, display.max_columns, and display.show_dimensions for controlled truncation. Outputs the DataFrame to the console using print; the display will reflect truncation and optionally display the DataFrame's dimensions based on set options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndfd = pd.DataFrame(np.arange(25).reshape(-1, 5),\\n                   index=[0, 1, 2, 3, 4],\\n                   columns=[0, 1, 2, 3, 4])\n```\n\nLANGUAGE: python\nCODE:\n```\n# show dimensions since this is truncated\\nwith pd.option_context('display.max_rows', 2, 'display.max_columns', 2,\\n                       'display.show_dimensions', 'truncate'):\\n    print(dfd)\n```\n\nLANGUAGE: python\nCODE:\n```\n# will not show dimensions since it is not truncated\\nwith pd.option_context('display.max_rows', 10, 'display.max_columns', 40,\\n                       'display.show_dimensions', 'truncate'):\\n    print(dfd)\n```\n\n----------------------------------------\n\nTITLE: Deleting a File using os.remove - Python\nDESCRIPTION: Demonstrates use of the os module to programmatically remove a file, here cleaning up an Excel file created earlier. Requires the os module; input is the target file path as a string. No output is produced unless an error occurs upon failure to remove the file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.remove(\"test.xlsx\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Consistent Behavior with Conflicting Attribute/Column Names in Python\nDESCRIPTION: Shows how conflicting attribute and column names now behave consistently between getting and setting in NDFrame objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.2.rst#_snippet_2\n\nLANGUAGE: ipython\nCODE:\n```\ndata = pd.DataFrame({'x': [1, 2, 3]})\ndata.y = 2\ndata['y'] = [2, 4, 6]\ndata\n\n# this assignment was inconsistent\ndata.y = 5\n\ndata.y\ndata['y'].values\n```\n\n----------------------------------------\n\nTITLE: Demonstrating GroupBy dropna Parameter (Python)\nDESCRIPTION: Creates a DataFrame containing NaN values and performs grouping with groupby using 'dropna=True' and 'dropna=False' to show how NA values in keys are handled. This lets users understand how missing data can be included or excluded in group keys with pandas groupby. Requires pandas (as pd) to be imported.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf_list = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]]\ndf_dropna = pd.DataFrame(df_list, columns=[\"a\", \"b\", \"c\"])\n\ndf_dropna\n```\n\nLANGUAGE: python\nCODE:\n```\n# Default ``dropna`` is set to True, which will exclude NaNs in keys\ndf_dropna.groupby(by=[\"b\"], dropna=True).sum()\n\n# In order to allow NaN in keys, set ``dropna`` to False\ndf_dropna.groupby(by=[\"b\"], dropna=False).sum()\n```\n\n----------------------------------------\n\nTITLE: Overriding Dictionary Keys in DataFrame Concatenation in Python\nDESCRIPTION: Shows how to override the dictionary keys by explicitly specifying the keys argument when concatenating a dictionary of DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.concat(pieces, keys=[\"z\", \"y\"])\nresult\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows by Condition using pandas in Python\nDESCRIPTION: This snippet shows how to filter rows in a DataFrame using boolean indexing, selecting all entries in 'tips' where the 'total_bill' column is greater than 10. It assumes the DataFrame 'tips' is already loaded and available. The expression inside the brackets returns a boolean Series used to index the DataFrame. The resulting output is a subset DataFrame. No external dependencies beyond pandas are required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spss.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    tips[tips[\"total_bill\"] > 10]\n```\n\n----------------------------------------\n\nTITLE: Chaining Data Selection Operations in Pandas\nDESCRIPTION: Demonstrates how to chain multiple data selection operations in Pandas without using temporary variables, improving code readability and efficiency.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbb = pd.read_csv(\"data/baseball.csv\", index_col=\"id\")\n(bb.groupby([\"year\", \"team\"]).sum(numeric_only=True).loc[lambda df: df.r > 100])\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in DataFrame.loc and Series.loc for Timezone-aware Datetimes\nDESCRIPTION: Addresses a regression in DataFrame.loc and Series.loc throwing an error when a datetime64[ns, tz] value is provided.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame.loc[datetime64[ns, tz]]\n```\n\nLANGUAGE: Python\nCODE:\n```\nSeries.loc[datetime64[ns, tz]]\n```\n\n----------------------------------------\n\nTITLE: Selecting Rows Using a Mask Based on DatetimeIndex Month - pandas HDFStore - Python\nDESCRIPTION: This snippet performs masked selection of table rows from HDFStore based on a boolean mask over a DateTimeIndex. It stores a random DataFrame, selects index values as a Series, builds a mask for a specific month, and fetches rows where the mask is true. Requires pandas and numpy. Inputs: DataFrame, mask. Outputs: Filtered DataFrame. Limitation: assumes the index is of datetime type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_193\n\nLANGUAGE: python\nCODE:\n```\ndf_mask = pd.DataFrame(\n    np.random.randn(1000, 2), index=pd.date_range(\"20000101\", periods=1000)\n)\nstore.append(\"df_mask\", df_mask)\nc = store.select_column(\"df_mask\", \"index\")\nwhere = c[pd.DatetimeIndex(c).month == 5].index\nstore.select(\"df_mask\", where=where)\n```\n\n----------------------------------------\n\nTITLE: DataFrame Dictionary Assignment\nDESCRIPTION: Demonstrates how to assign dictionary values to a DataFrame row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nx = pd.DataFrame({'x': [1, 2, 3], 'y': [3, 4, 5]})\nx.iloc[1] = {'x': 9, 'y': 99}\nx\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame for GroupBy Operations (Python)\nDESCRIPTION: This snippet creates a DataFrame with columns 'A', 'B', 'C', and 'D', filling 'C' and 'D' using random data from numpy. It sets up data for demonstrating column-based grouping and further illustrates how pandas structures data for split-apply-combine operations. pandas (as pd) and numpy (as np) must be imported. The resulting 'df' variable is a DataFrame ready for grouping.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n        \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n        \"C\": np.random.randn(8),\n        \"D\": np.random.randn(8),\n    }\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Memory Usage Comparison with Categoricals - pandas - Python\nDESCRIPTION: Compares memory usage (nbytes) of object dtype versus categorical dtype in pandas Series. Shows how categoricals can reduce memory usage when the number of categories is much less than data length. Dependencies: pandas. Key parameters: input size, number of unique values. Input: Large Series; Output: Byte sizes of different dtypes. Limitation: For high-cardinality data, categoricals may not offer savings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"foo\", \"bar\"] * 1000)\n\n# object dtype\ns.nbytes\n\n# category dtype\ns.astype(\"category\").nbytes\n```\n\n----------------------------------------\n\nTITLE: Resampling DataFrame Using 'on' and 'level' Parameters - Pandas - Python\nDESCRIPTION: Shows two ways to resample data by specifying either a column ('on=') or a MultiIndex level ('level=') for time-based grouping, followed by summing a column. The examples show the monthly aggregation of column 'a' by different time axes. Requires pandas and a previously constructed compliant DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nIn [74]: df.resample(\"M\", on=\"date\")[[\"a\"]].sum()\nOut[74]:\n            a\ndate        \n2015-01-31  6\n2015-02-28  4\n\n[2 rows x 1 columns]\n\nIn [75]: df.resample(\"M\", level=\"d\")[[\"a\"]].sum()\nOut[75]:\n            a\n d         \n2015-01-31  6\n2015-02-28  4\n\n[2 rows x 1 columns]\n```\n\n----------------------------------------\n\nTITLE: Using Membership Operations with pandas Series and DataFrame - Python\nDESCRIPTION: Explains how the Python 'in' operator works on pandas Series and DataFrames: checking membership among index or columns instead of values, and how to check for value membership using 'isin' and chaining with .any(). Requires pandas. Outputs boolean Series or boolean scalars.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/gotchas.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(range(5), index=list(\"abcde\"))\n2 in s\n'b' in s\n```\n\nLANGUAGE: python\nCODE:\n```\ns.isin([2])\ns.isin([2]).any()\n```\n\n----------------------------------------\n\nTITLE: Creating a MultiIndex from a Labeled DataFrame in Python\nDESCRIPTION: Transforms a regular DataFrame into one with hierarchical columns (MultiIndex). This example splits column names on underscores to create a two-level column index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"row\": [0, 1, 2],\n        \"One_X\": [1.1, 1.1, 1.1],\n        \"One_Y\": [1.2, 1.2, 1.2],\n        \"Two_X\": [1.11, 1.11, 1.11],\n        \"Two_Y\": [1.22, 1.22, 1.22],\n    }\n)\ndf\n\n# As Labelled Index\ndf = df.set_index(\"row\")\ndf\n# With Hierarchical Columns\ndf.columns = pd.MultiIndex.from_tuples([tuple(c.split(\"_\")) for c in df.columns])\ndf\n# Now stack & Reset\ndf = df.stack(0).reset_index(1)\ndf\n# And fix the labels (Notice the label 'level_1' got added automatically)\ndf.columns = [\"Sample\", \"All_X\", \"All_Y\"]\ndf\n```\n\n----------------------------------------\n\nTITLE: Comprehensive test example in Python using pytest and pandas\nDESCRIPTION: A complete example demonstrating various testing techniques including parametrization, fixtures, and pandas-specific assertions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport pytest\nimport numpy as np\nimport pandas as pd\n\n\n@pytest.mark.parametrize('dtype', ['int8', 'int16', 'int32', 'int64'])\ndef test_dtypes(dtype):\n    assert str(np.dtype(dtype)) == dtype\n\n\n@pytest.mark.parametrize(\n    'dtype', ['float32', pytest.param('int16', marks=pytest.mark.skip),\n              pytest.param('int32', marks=pytest.mark.xfail(\n                  reason='to show how it works'))])\ndef test_mark(dtype):\n    assert str(np.dtype(dtype)) == 'float32'\n\n\n@pytest.fixture\ndef series():\n    return pd.Series([1, 2, 3])\n\n\n@pytest.fixture(params=['int8', 'int16', 'int32', 'int64'])\ndef dtype(request):\n    return request.param\n\n\ndef test_series(series, dtype):\n    # GH <issue_number>\n    result = series.astype(dtype)\n    assert result.dtype == dtype\n\n    expected = pd.Series([1, 2, 3], dtype=dtype)\n    tm.assert_series_equal(result, expected)\n```\n\n----------------------------------------\n\nTITLE: Series and DataFrame Renaming in pandas\nDESCRIPTION: Demonstrates new rename functionality allowing scalar or list-like arguments for altering Series or axis names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.random.randn(5))\ns.rename('newname')\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(5, 2))\n(df.rename_axis(\"indexname\")\n   .rename_axis(\"columns_name\", axis=\"columns\"))\n```\n\n----------------------------------------\n\nTITLE: Broadcasting with reindex and align on Level - pandas - Python\nDESCRIPTION: Shows use of the level parameter in reindex and align to broadcast or align values by MultiIndex level, supporting reshaping and aligning DataFrames based on hierarchical groupings. Expects pandas, numpy, and multi-level structures as input.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nmidx = pd.MultiIndex(\n    levels=[[\"zero\", \"one\"], [\"x\", \"y\"]], codes=[[1, 1, 0, 0], [1, 0, 1, 0]]\n)\ndf = pd.DataFrame(np.random.randn(4, 2), index=midx)\ndf\ndf2 = df.groupby(level=0).mean()\ndf2\ndf2.reindex(df.index, level=0)\n# aligning\ndf_aligned, df2_aligned = df.align(df2, level=0)\ndf_aligned\ndf2_aligned\n```\n\n----------------------------------------\n\nTITLE: Reading XML with Prefixed Namespace using XPath - pandas - Python\nDESCRIPTION: Illustrates parsing XML nodes qualified by a prefixed namespace, requiring explicit namespace dictionary mapping in the 'namespaces' argument. The example XML defines a 'doc' prefix; the snippet uses XPath ('//doc:row') and the 'namespaces' mapping to correctly locate the elements. Requires pandas, StringIO, and a compatible XML string.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_119\n\nLANGUAGE: python\nCODE:\n```\nxml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n<doc:data xmlns:doc=\\\"https://example.com\\\">\n  <doc:row>\n    <doc:shape>square</doc:shape>\n    <doc:degrees>360</doc:degrees>\n    <doc:sides>4.0</doc:sides>\n  </doc:row>\n  <doc:row>\n    <doc:shape>circle</doc:shape>\n    <doc:degrees>360</doc:degrees>\n    <doc:sides/>\n  </doc:row>\n  <doc:row>\n    <doc:shape>triangle</doc:shape>\n    <doc:degrees>180</doc:degrees>\n    <doc:sides>3.0</doc:sides>\n  </doc:row>\n</doc:data>\"\"\"\n\ndf = pd.read_xml(StringIO(xml),\n                 xpath=\"//doc:row\",\n                 namespaces={\"doc\": \"https://example.com\"})\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating Series from Scalar Value - pandas - Python\nDESCRIPTION: Demonstrates instantiating a Series with a scalar value and explicit index, resulting in repeated values across all labels. Useful for initializing a data structure before filling it with actual data. Requires pandas and an iterable label index; outputs a Series of given length with the same value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.Series(5.0, index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n```\n\n----------------------------------------\n\nTITLE: Using wide_to_long in Python\nDESCRIPTION: Shows how to use pandas.wide_to_long() to reshape data from wide to long format with more customization.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndft = pd.DataFrame(\n    {\n        \"A1970\": {0: \"a\", 1: \"b\", 2: \"c\"},\n        \"A1980\": {0: \"d\", 1: \"e\", 2: \"f\"},\n        \"B1970\": {0: 2.5, 1: 1.2, 2: 0.7},\n        \"B1980\": {0: 3.2, 1: 1.3, 2: 0.1},\n        \"X\": dict(zip(range(3), np.random.randn(3))),\n    }\n)\ndft[\"id\"] = dft.index\ndft\npd.wide_to_long(dft, [\"A\", \"B\"], i=\"id\", j=\"year\")\n```\n\n----------------------------------------\n\nTITLE: Finding Position of a Substring with FINDW in SAS\nDESCRIPTION: Uses SAS's 'FINDW' function to locate the position of a substring within a string column ('sex'). Outputs the position via 'put' to the SAS log. Inputs: 'tips' dataset, target string/substring; outputs: start position integer in log.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_14\n\nLANGUAGE: SAS\nCODE:\n```\ndata _null_;\\nset tips;\\nput(FINDW(sex,'ale'));\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Reading Tab-delimited or Headerless CSV - pandas - Python\nDESCRIPTION: Demonstrates how to read a tab-delimited CSV (or one lacking headers) using pandas' 'read_csv' and 'read_table'. By specifying 'sep=\"\\t\"' and 'header=None', the code adapts to non-standard CSVs. Additionally, 'read_table' is shown as an alias for tab-delimited data. Dependencies: pandas. Inputs: a local file 'tips.csv' formatted as described. Outputs: DataFrame with default headers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntips = pd.read_csv(\"tips.csv\", sep=\"\\t\", header=None)\\n\\n# alternatively, read_table is an alias to read_csv with tab delimiter\\ntips = pd.read_table(\"tips.csv\", header=None)\n```\n\n----------------------------------------\n\nTITLE: Using isin with Levels in MultiIndex - Pandas Python\nDESCRIPTION: Shows enhanced usage of the Index.isin method in pandas, now accepting a level argument to perform membership tests against a specific level of a MultiIndex. Requires pandas and MultiIndex instantiation. Code inputs are the MultiIndex and the values to check; outputs are boolean arrays indicating membership per row. Useful for filtering or masking based on hierarchical index components.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.MultiIndex.from_product([[0, 1], ['a', 'b', 'c']])\n\nidx.values\n# Out: array([(0, 'a'), (0, 'b'), (0, 'c'), (1, 'a'), (1, 'b'), (1, 'c')], dtype=object)\n\nidx.isin(['a', 'c', 'e'], level=1)\n# Out: array([ True, False,  True,  True, False,  True], dtype=bool)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Generated HDF5 Files after Testing with os (Python)\nDESCRIPTION: This snippet demonstrates removing an HDF5 file created for testing purposes using Python's os.remove. This helps to prevent clutter and ensures a clean test environment after serialization tests. Dependencies: os module. Input is the filename as a string, and there is no output if the file is successfully deleted. A FileNotFoundError may be raised if the file does not exist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.remove(\"file.h5\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Derived DataFrame Column with Arithmetic in pandas (Python)\nDESCRIPTION: This snippet creates a new column in the 'air_quality' DataFrame, named 'london_mg_per_cubic', calculated as the element-wise product of the 'station_london' column and the conversion factor 1.882. The purpose is to convert NO2 concentrations for London from an original unit to mg/m^3 based on assumed environmental conditions. Dependencies include a pandas DataFrame with a 'station_london' column. The input is an existing numeric column, and the output is a new column with the converted values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/05_add_columns.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nair_quality[\"london_mg_per_cubic\"] = air_quality[\"station_london\"] * 1.882\nair_quality.head()\n```\n\n----------------------------------------\n\nTITLE: Setting Display Options in Pandas (Python)\nDESCRIPTION: This snippet demonstrates how to revert the pandas display.max_columns option to its previous default value by explicitly setting it to 20. It requires the pandas library to be imported (usually as pd). The key parameter is max_columns, which controls the maximum number of columns displayed when printing DataFrames. Input: none; Output: changes the global display option for maximum columns shown. There are no external constraints, but this change affects only the local Python session or script where it is run.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n  pd.options.display.max_columns = 20\n```\n\n----------------------------------------\n\nTITLE: GroupBy Aggregation on Object-dtype Columns in Python\nDESCRIPTION: Fixed regression in .groupby().agg() that was raising an AssertionError for some reductions like min on object-dtype columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndf.groupby('column').agg('min')\n```\n\n----------------------------------------\n\nTITLE: Distributed Data Processing with Cylon\nDESCRIPTION: This code shows how to use Cylon for distributed memory parallel processing with a pandas-like API. It initializes a distributed environment and performs a join operation that can scale across thousands of cores.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom pycylon import read_csv, DataFrame, CylonEnv\nfrom pycylon.net import MPIConfig\n\n# Initialize Cylon distributed environment\nconfig: MPIConfig = MPIConfig()\nenv: CylonEnv = CylonEnv(config=config, distributed=True)\n\ndf1: DataFrame = read_csv('/tmp/csv1.csv')\ndf2: DataFrame = read_csv('/tmp/csv2.csv')\n\n# Using 1000s of cores across the cluster to compute the join\ndf3: Table = df1.join(other=df2, on=[0], algorithm=\"hash\", env=env)\n\nprint(df3)\n```\n\n----------------------------------------\n\nTITLE: Converting and Aligning TimedeltaIndex and DateRange - pandas - Python\nDESCRIPTION: Demonstrates use of TimedeltaIndex and DateRange in pandas, converting indexes to lists, and aligning them via arithmetic operations. Requires pandas and numpy installations. Inputs are arrays of timedelta strings or date strings; output is lists of corresponding Python objects or datetime results. Dependent on correct input formats and length matching for arithmetic.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntdi = pd.TimedeltaIndex(['1 days', pd.NaT, '2 days'])\\ntdi.tolist()\\ndti = pd.date_range('20130101', periods=3)\\ndti.tolist()\\n\\n(dti + tdi).tolist()\\n(dti - tdi).tolist()\n```\n\n----------------------------------------\n\nTITLE: Resetting Table Schema Output Option in pandas - Python\nDESCRIPTION: This snippet resets the 'display.html.table_schema' pandas option to its default value, effectively disabling the Table Schema auto-publication feature for DataFrame and Series. This is commonly used in tests or tutorials to restore prior settings. No user arguments are needed; dependencies are limited to pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\npd.reset_option(\"display.html.table_schema\")\n```\n\n----------------------------------------\n\nTITLE: Creating Categorical from Codes in Python\nDESCRIPTION: Shows how to create a categorical Series using Categorical.from_codes() when codes and categories are already known. This method can be more efficient than the normal constructor in certain situations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nsplitter = np.random.choice([0, 1], 5, p=[0.5, 0.5])\ns = pd.Series(pd.Categorical.from_codes(splitter, categories=[\"train\", \"test\"]))\n```\n\n----------------------------------------\n\nTITLE: Syncing Local Main with Upstream and Creating Feature Branches - Shell\nDESCRIPTION: This snippet ensures your local main branch matches the latest upstream state and then creates and switches to a new feature branch. It assumes you have already configured the upstream remote. The 'git pull upstream main --ff-only' command integrates upstream changes without merge commits. Inputs are branch names; outputs are branch changes in your workspace.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing.rst#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout main\\ngit pull upstream main --ff-only\n```\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout -b shiny-new-feature\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Silent Upcasting with Series.setitem Operations in Pandas (ipython)\nDESCRIPTION: This code illustrates the previous behavior of pandas Series when an incompatible data type assignment (e.g., assigning a string to an integer Series) was silently upcast to object dtype. It requires the pandas library and demonstrates modification of Series elements and the resulting dtype change. The snippet highlights how assigning incompatible types affected the series prior to deprecation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_6\n\nLANGUAGE: ipython\nCODE:\n```\n  In [1]: ser = pd.Series([1, 2, 3])\n\n  In [2]: ser\n  Out[2]:\n  0    1\n  1    2\n  2    3\n  dtype: int64\n\n  In [3]: ser[0] = 'not an int64'\n\n  In [4]: ser\n  Out[4]:\n  0    not an int64\n  1               2\n  2               3\n  dtype: object\n```\n\n----------------------------------------\n\nTITLE: Handling Mixed Data Types in to_datetime() with 'errors=coerce' - pandas - Python\nDESCRIPTION: Demonstrates that pd.to_datetime now converts integers/floats with the default unit of 'ns' when supplied alongside non-datetime strings and errors='coerce'. Requires pandas. Input: list of mixed numeric and non-time strings. Output: DatetimeIndex with converted and NaT results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime([1, \"foo\"], errors=\"coerce\")\n```\n\n----------------------------------------\n\nTITLE: MultiIndex DataFrame Construction and Initialization - pandas Python\nDESCRIPTION: Defines utility functions for label creation and demonstrates the initialization of MultiIndex objects for both index and columns. Builds a DataFrame with a product MultiIndex on both axes and sorts for further selection examples. Dependencies include pandas (pd) and numpy (np).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef mklbl(prefix, n):\n    return [\"%s%s\" % (prefix, i) for i in range(n)]\n\nindex = pd.MultiIndex.from_product([mklbl('A', 4),\n                                    mklbl('B', 2),\n                                    mklbl('C', 4),\n                                    mklbl('D', 2)])\ncolumns = pd.MultiIndex.from_tuples([('a', 'foo'), ('a', 'bar'),\n                                     ('b', 'foo'), ('b', 'bah')],\n                                    names=['lvl0', 'lvl1'])\ndf = pd.DataFrame(np.arange(len(index) * len(columns)).reshape((len(index),\n                  len(columns))),\n                  index=index,\n                  columns=columns).sort_index().sort_index(axis=1)\ndf\n```\n\n----------------------------------------\n\nTITLE: Recommended DataFrame Column Update Using fillna - pandas - Python\nDESCRIPTION: This concise code shows the correct way to update a single column in a DataFrame using the fillna method on the DataFrame, avoiding chained assignments. It requires pandas as a dependency. Input is a DataFrame with NA values; output is the DataFrame with the specified column's NA values filled. Ensures future compatibility with pandas 3.0 by recommending DataFrame-level operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf.fillna({\"foo\": 0}, inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Creating Kernel Density Plot with pandas Series\nDESCRIPTION: Creates a kernel density estimation (KDE) plot using a pandas Series containing random data. Uses the plot.kde() method to generate a smooth curve representing the probability density function of the data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(np.random.randn(1000))\n\nser.plot.kde();\n```\n\n----------------------------------------\n\nTITLE: Plotting a Specific Column from a pandas DataFrame in Python\nDESCRIPTION: This snippet selects the 'station_paris' column from the air_quality DataFrame using column indexing, then plots it using the Series.plot() method. plt.show() renders the plot. Requires that the DataFrame includes a 'station_paris' column. Result is a single line plot representing Paris NO2 values over time.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nair_quality[\"station_paris\"].plot()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Selecting Rows with .loc Indexing - Pandas - Python\nDESCRIPTION: Uses the .loc indexer to select rows by label and columns by label, which is the recommended replacement for .ix when accessing by label. Expects a DataFrame df; selects the first and third index labels for column 'A'. Returns a Series with the selected entries.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[df.index[[0, 2]], 'A']\n```\n\n----------------------------------------\n\nTITLE: Date Range Creation with dateutil Timezones in Pandas - Python\nDESCRIPTION: This example shows how to generate a pandas date_range with time zone support using dateutil time zones. It creates a range of dates and inspects the associated timezone, leveraging the new dateutil/Europe/London support added in this release. Requirements include pandas installed and imported as pd; the code is to be run in a Python or IPython environment capable of handling dateutil timezones.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.1.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrng = pd.date_range(\n    \"3/6/2012 00:00\", periods=10, freq=\"D\", tz=\"dateutil/Europe/London\"\n)\nrng.tz\n```\n\n----------------------------------------\n\nTITLE: GroupBy with Expanding Window\nDESCRIPTION: Shows how to combine groupby operations with expanding windows for grouped cumulative calculations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': ['a', 'b', 'a', 'b', 'a'], 'B': range(5)})\ndf.groupby('A').expanding().sum()\n```\n\n----------------------------------------\n\nTITLE: Series and DataFrame Rank Method Updates\nDESCRIPTION: Demonstrates the updated signature for rank methods in Series and DataFrame objects with consistent parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npd.Series([0,1]).rank(axis=0, method='average', numeric_only=False,\n                         na_option='keep', ascending=True, pct=False)\npd.DataFrame([0,1]).rank(axis=0, method='average', numeric_only=False,\n                            na_option='keep', ascending=True, pct=False)\n```\n\n----------------------------------------\n\nTITLE: Creating SparseArrays with Explicit or Default Dtype in pandas - ipython\nDESCRIPTION: This collection of snippets show the difference between creating a SparseArray with and without explicitly specifying dtype or fill_value. The first initializes with default float64, the second with int64 (default fill_value being nan), and the third with both int64 and an explicit fill_value of 0. Demonstrates changes to dtype and fill_value handling in pandas v0.19.0. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_42\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pd.SparseArray([1, 2, 0, 0])\nOut[1]:\n[1.0, 2.0, 0.0, 0.0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n```\n\nLANGUAGE: ipython\nCODE:\n```\n# specifying int64 dtype, but all values are stored in sp_values because\n# fill_value default is np.nan\nIn [2]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64)\nOut[2]:\n[1, 2, 0, 0]\nFill: nan\nIntIndex\nIndices: array([0, 1, 2, 3], dtype=int32)\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: pd.SparseArray([1, 2, 0, 0], dtype=np.int64, fill_value=0)\nOut[3]:\n[1, 2, 0, 0]\nFill: 0\nIntIndex\nIndices: array([0, 1], dtype=int32)\n```\n\n----------------------------------------\n\nTITLE: Fixing DatetimeIndex Construction with Boolean Input in Python\nDESCRIPTION: Bug fix for DatetimeIndex constructor incorrectly accepting bool-dtype inputs. This change raises an error for invalid input types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nDatetimeIndex([True, False])  # Now raises TypeError\n```\n\n----------------------------------------\n\nTITLE: Working with Categorical Data in HDF5 using Python\nDESCRIPTION: This snippet shows how to write and query data containing categorical dtypes to an HDF5 store using pandas. It demonstrates creating a DataFrame with categorical data, storing it, and querying specific categories.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_199\n\nLANGUAGE: python\nCODE:\n```\ndfcat = pd.DataFrame(\n    {\"A\": pd.Series(list(\"aabbcdba\")).astype(\"category\"), \"B\": np.random.randn(8)}\n)\ndfcat\ndfcat.dtypes\ncstore = pd.HDFStore(\"cats.h5\", mode=\"w\")\ncstore.append(\"dfcat\", dfcat, format=\"table\", data_columns=[\"A\"])\nresult = cstore.select(\"dfcat\", where=\"A in ['b', 'c']\")\nresult\nresult.dtypes\n```\n\n----------------------------------------\n\nTITLE: New Behavior: Groupby with as_index=False Retains Columns - python\nDESCRIPTION: Demonstrates that after the update, groupby with as_index=False and reductions leaves the grouping column unchanged and displays only the intended results. Improves predictability and compatibility for result DataFrames. Inputs include the DataFrame, method, parameters, and chosen aggregation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"a\", as_index=False).nunique()\n```\n\n----------------------------------------\n\nTITLE: Creating a Regular TimedeltaIndex Range - Python\nDESCRIPTION: This snippet shows how to construct a TimedeltaIndex with regular intervals using pd.timedelta_range, specifying start date, number of periods, and frequency. Inputs are strings for start, integer period count, and string frequency; output is a TimedeltaIndex suited for regular time deltas. Requires pandas >= 0.15.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.timedelta_range('1 days', periods=5, freq='D')\n```\n\n----------------------------------------\n\nTITLE: Accessing Datetime, Period, and Timedelta Properties with Series.dt - pandas - Python\nDESCRIPTION: Demonstrates use of the Series.dt accessor for extracting datetime, period, and timedelta properties. This requires a Series of datetime64, period, or timedelta values and pandas 0.15.0+. Outputs are new Series with extracted components, such as hours, days, or frequency. Key parameters include Series content and optional timezone arguments.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# datetime\\ns = pd.Series(pd.date_range('20130101 09:10:12', periods=4))\\ns\\ns.dt.hour\\ns.dt.second\\ns.dt.day\\ns.dt.freq\n```\n\nLANGUAGE: python\nCODE:\n```\ns[s.dt.day == 2]\n```\n\nLANGUAGE: python\nCODE:\n```\nstz = s.dt.tz_localize('US/Eastern')\\nstz\\nstz.dt.tz\n```\n\nLANGUAGE: python\nCODE:\n```\ns.dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n```\n\nLANGUAGE: python\nCODE:\n```\n# period\\ns = pd.Series(pd.period_range('20130101', periods=4, freq='D'))\\ns\\ns.dt.year\\ns.dt.day\n```\n\nLANGUAGE: python\nCODE:\n```\n# timedelta\\ns = pd.Series(pd.timedelta_range('1 day 00:00:05', periods=4, freq='s'))\\ns\\ns.dt.days\\ns.dt.seconds\\ns.dt.components\n```\n\n----------------------------------------\n\nTITLE: Structuring Titanic Dataset Description with HTML and Bootstrap\nDESCRIPTION: This HTML snippet creates a collapsible section using Bootstrap to display information about the Titanic dataset. It includes a toggle button and a card body with dataset details and a link to the raw data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/includes/titanic.rst#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div data-bs-toggle=\"collapse\" href=\"#collapsedata\" role=\"button\" aria-expanded=\"false\" aria-controls=\"collapsedata\">\n    <span class=\"badge bg-secondary\">Titanic data</span>\n</div>\n<div class=\"collapse\" id=\"collapsedata\">\n    <div class=\"card-body\">\n        <p class=\"card-text\">\n\nThis tutorial uses the Titanic data set, stored as CSV. The data\nconsists of the following data columns:\n\n-  PassengerId: Id of every passenger.\n-  Survived: Indication whether passenger survived. ``0`` for yes and ``1`` for no.\n-  Pclass: One out of the 3 ticket classes: Class ``1``, Class ``2`` and Class ``3``.\n-  Name: Name of passenger.\n-  Sex: Gender of passenger.\n-  Age: Age of passenger in years.\n-  SibSp: Number of siblings or spouses aboard.\n-  Parch: Number of parents or children aboard.\n-  Ticket: Ticket number of passenger.\n-  Fare: Indicating the fare.\n-  Cabin: Cabin number of passenger.\n-  Embarked: Port of embarkation.\n\n        </p>\n        <a href=\"https://github.com/pandas-dev/pandas/raw/main/doc/data/titanic.csv\" class=\"btn btn-dark btn-sm\">To raw data</a>\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Autosummary for MultiIndex and Associated Methods - reStructuredText\nDESCRIPTION: This snippet uses the Sphinx autosummary directive to document the MultiIndex core class, its constructors, property accessors, and manipulation methods in pandas. Each block includes either the main MultiIndex entity or a group of member functions such as from_arrays constructors, properties like codes/levels, and multi-level operations. These patterns enable automated generation of individual API doc pages for hierarchical index handling. Sphinx, pandas, and appropriate class paths are prerequisites.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/indexing.rst#_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n   :template: autosummary/class_without_autosummary.rst\\n\\n   MultiIndex\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   MultiIndex.from_arrays\\n   MultiIndex.from_tuples\\n   MultiIndex.from_product\\n   MultiIndex.from_frame\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   MultiIndex.names\\n   MultiIndex.levels\\n   MultiIndex.codes\\n   MultiIndex.nlevels\\n   MultiIndex.levshape\\n   MultiIndex.dtypes\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   MultiIndex.set_levels\\n   MultiIndex.set_codes\\n   MultiIndex.to_flat_index\\n   MultiIndex.to_frame\\n   MultiIndex.sortlevel\\n   MultiIndex.droplevel\\n   MultiIndex.swaplevel\\n   MultiIndex.reorder_levels\\n   MultiIndex.remove_unused_levels\\n   MultiIndex.drop\\n   MultiIndex.copy\\n   MultiIndex.append\\n   MultiIndex.truncate\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   MultiIndex.get_loc\\n   MultiIndex.get_locs\\n   MultiIndex.get_loc_level\\n   MultiIndex.get_indexer\\n   MultiIndex.get_level_values\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   IndexSlice\\n\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Values in HDF5 Storage with Pandas\nDESCRIPTION: Shows how to control the handling of missing values when writing to HDF5 format. By default, rows with missing values are preserved, but setting dropna=True removes them from storage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_168\n\nLANGUAGE: python\nCODE:\n```\ndf_with_missing = pd.DataFrame(\n    {\n        \"col1\": [0, np.nan, 2],\n        \"col2\": [1, np.nan, np.nan],\n    }\n)\ndf_with_missing\n\ndf_with_missing.to_hdf(\"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\")\n\npd.read_hdf(\"file.h5\", \"df_with_missing\")\n\ndf_with_missing.to_hdf(\n    \"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\", dropna=True\n)\npd.read_hdf(\"file.h5\", \"df_with_missing\")\n```\n\n----------------------------------------\n\nTITLE: Basic Rolling Window Sum in Pandas\nDESCRIPTION: Demonstrates basic rolling window operation with a window size of 2 to calculate sum of consecutive values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(range(5))\ns.rolling(window=2).sum()\n```\n\n----------------------------------------\n\nTITLE: Horizontal Stacked Bar Plot for DataFrame (Python)\nDESCRIPTION: Uses DataFrame.plot.barh(stacked=True) to plot stacked horizontal bar charts, visualizing cumulative row-wise values across columns. Requires pandas and matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@savefig barh_plot_stacked_ex.png\\ndf2.plot.barh(stacked=True);\n```\n\n----------------------------------------\n\nTITLE: Advanced Data Selection in Pandas (Equivalent to SQL Analytic Functions)\nDESCRIPTION: These snippets show how to perform more advanced data selection operations in pandas, such as selecting top N rows with offset and top N rows per group, which are equivalent to SQL analytic functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntips.nlargest(10 + 5, columns=\"tip\").tail(10)\n```\n\nLANGUAGE: python\nCODE:\n```\n(\n    tips.assign(\n        rn=tips.sort_values([\"total_bill\"], ascending=False)\n        .groupby([\"day\"])\n        .cumcount()\n        + 1\n    )\n    .query(\"rn < 3\")\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting First Row by Group with pandas groupby in Python\nDESCRIPTION: Uses pandas' groupby to select the first row from each group defined by 'sex' and 'smoker'. Requires: pandas library. Input: DataFrame 'tips' with 'sex' and 'smoker' columns. Output: DataFrame of first-row representatives for each group. Equivalent to SAS by-group processing using FIRST..\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntips.groupby([\"sex\", \"smoker\"]).first()\n```\n\n----------------------------------------\n\nTITLE: Creating a SQLAlchemy Engine and Writing/Reading DataFrames - pandas & SQLAlchemy - Python\nDESCRIPTION: Shows how to establish a SQL database connection using SQLAlchemy's create_engine, then write a pandas DataFrame to a SQL table and read it back using the new pandas SQL API (read_sql_table, read_sql_query). The code covers engine instantiation for an in-memory SQLite DB, exporting a DataFrame to SQL, and reading data via both table name and direct query. Dependencies include pandas and sqlalchemy installed, with knowledge of SQL URI structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy import create_engine\\n# Create your connection.\\nengine = create_engine('sqlite:///:memory:')\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})\\ndf.to_sql(name='db_table', con=engine, index=False)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_table('db_table', engine)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_query('SELECT * FROM db_table', engine)\n```\n\n----------------------------------------\n\nTITLE: Using Complement Operator for Mask Inversion in Python\nDESCRIPTION: Uses the complement operator (~) to invert a boolean mask for filtering. This technique allows for selecting rows that do not meet specified conditions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n)\ndf\n\ndf[~((df.AAA <= 6) & (df.index.isin([0, 2, 4])))]\n```\n\n----------------------------------------\n\nTITLE: Running Test Files, Test Classes, or Test Methods with Pytest (Bash)\nDESCRIPTION: Shows several command-line examples to run a specific test module, class, or single method using pytest. The sample constructs leverage pytest's CLI syntax. Requires pytest installed and assumes the user has the test file and optional class/method present.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\npytest pandas/tests/[test-module].py\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest pandas/tests/[test-module].py::[TestClass]\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest pandas/tests/[test-module].py::[TestClass]::[test_method]\n```\n\n----------------------------------------\n\nTITLE: Quick Statistical Summary with DataFrame.describe() - Python\nDESCRIPTION: The df.describe() function summarizes count, mean, standard deviation, min, percentiles, and max for each numerical column of the DataFrame. Non-numeric columns are ignored unless include='all' is specified. No arguments are required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf.describe()\n```\n\n----------------------------------------\n\nTITLE: Comparing DataFrames with Row Alignment Using pandas in Python\nDESCRIPTION: This snippet showcases comparing two pandas DataFrames using compare() with the align_axis=0 option, which stacks the differences along rows instead of the default column alignment. Required: pandas library, and two similar DataFrames for comparison. The align_axis parameter must be set to 0 for this behavior. This allows for visualization of differences in a stacked row format, which can be helpful for certain analysis tasks. Input: DataFrames. Output: Row-aligned differences DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndf.compare(df2, align_axis=0)\n```\n\n----------------------------------------\n\nTITLE: Creating Example DataFrame with NaNs - pandas - Python\nDESCRIPTION: Creates a pandas DataFrame (tsdf) with random numbers, sets specific rows to NaN, and assigns date indices and column labels A, B, and C. Demonstrates basic DataFrame creation and manipulation as setup for subsequent aggregation or transformation examples. Dependencies: pandas (pd), NumPy (np). Inputs: shape parameters; Outputs: DataFrame tsdf with shape (10,3).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ntsdf = pd.DataFrame(\n    np.random.randn(10, 3),\n    columns=[\"A\", \"B\", \"C\"],\n    index=pd.date_range(\"1/1/2000\", periods=10),\n)\ntsdf.iloc[3:7] = np.nan\ntsdf\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrames for Concatenation in Python\nDESCRIPTION: Creation of three DataFrames (df1, df2, df3) with similar structure but different values and indexes for use in concatenation examples.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(\n    {\n        \"A\": [\"A0\", \"A1\", \"A2\", \"A3\"],\n        \"B\": [\"B0\", \"B1\", \"B2\", \"B3\"],\n        \"C\": [\"C0\", \"C1\", \"C2\", \"C3\"],\n        \"D\": [\"D0\", \"D1\", \"D2\", \"D3\"],\n    },\n    index=[0, 1, 2, 3],\n)\n\ndf2 = pd.DataFrame(\n    {\n        \"A\": [\"A4\", \"A5\", \"A6\", \"A7\"],\n        \"B\": [\"B4\", \"B5\", \"B6\", \"B7\"],\n        \"C\": [\"C4\", \"C5\", \"C6\", \"C7\"],\n        \"D\": [\"D4\", \"D5\", \"D6\", \"D7\"],\n    },\n    index=[4, 5, 6, 7],\n)\n\ndf3 = pd.DataFrame(\n    {\n        \"A\": [\"A8\", \"A9\", \"A10\", \"A11\"],\n        \"B\": [\"B8\", \"B9\", \"B10\", \"B11\"],\n        \"C\": [\"C8\", \"C9\", \"C10\", \"C11\"],\n        \"D\": [\"D8\", \"D9\", \"D10\", \"D11\"],\n    },\n    index=[8, 9, 10, 11],\n)\n\nframes = [df1, df2, df3]\nresult = pd.concat(frames)\nresult\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Data When Unstacking - pandas - Python\nDESCRIPTION: Initializes MultiIndex DataFrame and subselects it to create non-aligned subgroups, setting up a scenario where unstacking introduces missing values. Demonstrates how the default fill values are applied for missing subgroup labels when reshaping. Includes MultiIndex creation for both columns and index, and DataFrame slicing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ncolumns = pd.MultiIndex.from_tuples(\n    [\n        (\"A\", \"cat\"),\n        (\"B\", \"dog\"),\n        (\"B\", \"cat\"),\n        (\"A\", \"dog\"),\n    ],\n    names=[\"exp\", \"animal\"],\n)\nindex = pd.MultiIndex.from_product(\n    [(\"bar\", \"baz\", \"foo\", \"qux\"), (\"one\", \"two\")], names=[\"first\", \"second\"]\n)\ndf = pd.DataFrame(np.random.randn(8, 4), index=index, columns=columns)\ndf3 = df.iloc[[0, 1, 4, 7], [1, 2]]\n```\n\n----------------------------------------\n\nTITLE: Line Plotting with Explicit Matplotlib Colormap Object in pandas Python\nDESCRIPTION: This code demonstrates how to supply a Matplotlib colormap object instead of its name for line plots in pandas. Dependencies are matplotlib, pandas, numpy. The example uses cm.cubehelix as the colormap. Input: DataFrame with cumulative sums; output: color-differentiated line plot. The use of a colormap object gives more flexibility and direct access to Matplotlib's color maps.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nfrom matplotlib import cm\n\nplt.figure();\ndf.plot(colormap=cm.cubehelix);\n```\n\n----------------------------------------\n\nTITLE: Testing Series GetItem and Loc Methods with pandas - python\nDESCRIPTION: Illustrates a minimal test case (with comments) that checks whether pandas.Series __getitem__ and .loc methods return the expected subset when provided a list of indices. The test constructs a Series, performs two retrieval operations, and validates them with pandas' test utility. Dependencies include pandas and pandas._testing, and the method tm.assert_series_equal is used for assertion. Inputs are integer indices; outputs are compared series objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport pandas._testing as tm\n\ndef test_getitem_listlike_of_ints():\n    ser = pd.Series(range(5))\n\n    result = ser[[3, 4]]\n    expected = pd.Series([2, 3])\n    tm.assert_series_equal(result, expected)\n\n    result = ser.loc[[3, 4]]\n    tm.assert_series_equal(result, expected)\n\n```\n\n----------------------------------------\n\nTITLE: MultiIndex Join Without Level Reordering - pandas Python\nDESCRIPTION: This snippet demonstrates joining two pandas DataFrames with MultiIndex indices of different levels, showing that the newer pandas behavior preserves index level order. After constructing the DataFrames with mismatched MultiIndex levels, the join is performed and the resulting DataFrame reflects the new level ordering policy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame({\"left\": 1}, index=pd.MultiIndex.from_tuples([(\"x\", 1), (\"x\", 2)], names=[\"A\", \"B\"]))\nright = pd.DataFrame({\"right\": 2}, index=pd.MultiIndex.from_tuples([(1, 1), (2, 2)], names=[\"B\", \"C\"]))\nleft\nright\nresult = left.join(right)\n```\n\n----------------------------------------\n\nTITLE: Incorrect Docstring Placement and Formatting in Python (Anti-pattern Example)\nDESCRIPTION: Highlights several anti-patterns in docstring formatting such as incorrect blank lines, improper placement of opening/closing quotes, and poor alignment with convention. The function 'func' is shown with a misaligned docstring and code sections, serving to teach what not to do when writing Python docstrings. No functional dependencies; purpose is to demonstrate invalid style rather than implementation. Inputs/outputs are trivial; not meant for production.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef func():\n\n    \"\"\"Some function.\n\n    With several mistakes in the docstring.\n\n    It has a blank line after the signature ``def func():``.\n\n    The text 'Some function' should go in the line after the\n    opening quotes of the docstring, not in the same line.\n\n    There is a blank line between the docstring and the first line\n    of code ``foo = 1``.\n\n    The closing quotes should be in the next line, not in this one.\"\"\"\n\n    foo = 1\n    bar = 2\n    return foo + bar\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Updated Inplace Operations in Pandas\nDESCRIPTION: Shows how inplace operations now properly update all references to a pandas object, fixing previous inconsistent behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3])\ns2 = s\ns += 1.5\n```\n\n----------------------------------------\n\nTITLE: Creating Sample DataFrame with Random Data\nDESCRIPTION: Creates a DataFrame with random normally distributed values across 10 rows and 4 columns using NumPy's random generator.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf2 = pd.DataFrame(\n    np.random.default_rng(0).standard_normal((10, 4)), columns=[\"A\", \"B\", \"C\", \"D\"]\n)\ndf2.style\n```\n\n----------------------------------------\n\nTITLE: Demonstrating DataFrame.any and DataFrame.all with bool_only Parameter in Pandas (Python)\nDESCRIPTION: These code snippets illustrate the behavior changes for DataFrame.any and DataFrame.all when using bool_only=True. They show the construction of a DataFrame with object dtype consisting of boolean and string columns, alongside assignment of an additional boolean column. The examples reveal the updated behavior for column exclusion logic during reduction, providing a direct demonstration of results before and after the fix. Dependencies: pandas. Key parameters: bool_only (determines which columns participate in operation). Expected inputs are DataFrames with mixed column types. Outputs are Series indicating boolean reduction per-column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [\"foo\", \"bar\"], \"B\": [True, False]}, dtype=object)\ndf[\"C\"] = pd.Series([True, True])\n```\n\n----------------------------------------\n\nTITLE: Ranking Series with inf and NaN Values using Series.rank - Python\nDESCRIPTION: Shows how the rank method now handles inf and NaN values properly, compared to previous versions. Requires pandas and numpy. Examples cover Series creation and ranking, with before-and-after output highlighting changes in treatment of special values. The ranking correctly distinguishes and orders infinite values and NaNs, with options for placement (e.g., na_option='top').\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([-np.inf, 0, 1, np.nan, np.inf])\ns\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [11]: s.rank()\nOut[11]:\n0    1.0\n1    2.0\n2    3.0\n3    NaN\n4    NaN\ndtype: float64\n```\n\nLANGUAGE: python\nCODE:\n```\ns.rank()\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([np.nan, np.nan, -np.inf, -np.inf])\ns\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [15]: s.rank(na_option='top')\nOut[15]:\n0    2.5\n1    2.5\n2    2.5\n3    2.5\ndtype: float64\n```\n\nLANGUAGE: python\nCODE:\n```\ns.rank(na_option='top')\n```\n\n----------------------------------------\n\nTITLE: Retaining Ordered on CategoricalDtype Conversion (Python)\nDESCRIPTION: Shows that, starting pandas 0.23, changing CategoricalDtype categories retains the original ordered state unless explicitly overridden. Demonstrates constructing and converting categorical data, verifying that ordered is preserved unless changed with an explicit ordered argument. Requires pandas. Inputs are Categorical and CategoricalDtype objects; output is a categorized series with updated categories.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import CategoricalDtype\ncat = pd.Categorical(list('abcaba'), ordered=True, categories=list('cba'))\ncat\n```\n\nLANGUAGE: python\nCODE:\n```\ncdt = CategoricalDtype(categories=list('cbad'))\n```\n\nLANGUAGE: python\nCODE:\n```\ncat.astype(cdt)\n```\n\n----------------------------------------\n\nTITLE: Building a Year-Over-Year DataFrame Pivot Table - Pandas - Python\nDESCRIPTION: Creates a DataFrame of values indexed by a date range, then creates a pivot table summarizing the data by month and year using sum as aggregation. Produces a crosstab for year-over-year analysis. Inputs are a numeric Series with a date index; outputs a DataFrame with months as rows and years as columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"value\": np.random.randn(36)},\n    index=pd.date_range(\"2011-01-01\", freq=\"ME\", periods=36),\n)\n\npd.pivot_table(\n    df, index=df.index.month, columns=df.index.year, values=\"value\", aggfunc=\"sum\"\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating a Conda Virtual Environment in Shell\nDESCRIPTION: Creates a new Conda environment named \"name_of_my_env\" and installs Python and pandas within it. Provides commands for activating the environment on Linux/MacOS and Windows. Conda must be installed beforehand. Commands must be run in a terminal. No output except for the setup and activation of the specified environment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda create -c conda-forge -n name_of_my_env python pandas\n# On Linux or MacOS\nsource activate name_of_my_env\n# On Windows\nactivate name_of_my_env\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame with Integer Indices for .iloc Examples\nDESCRIPTION: Creates a DataFrame with non-contiguous integer indices for both rows and columns to demonstrate .iloc's position-based indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(np.random.randn(6, 4),\n                  index=list(range(0, 12, 2)),\n                  columns=list(range(0, 8, 2)))\ndf1\n```\n\n----------------------------------------\n\nTITLE: Creating and Round-Tripping DataFrames with JSON Table Orient - Pandas - Python\nDESCRIPTION: Demonstrates serialization and deserialization of a pandas DataFrame to/from JSON using the 'table' orient to preserve index names and data types. Requires pandas version 0.23.0 or later and Python. The code initializes a DataFrame with various dtypes, writes it to a JSON file, reads it back, and verifies dtype integrity. The 'index' string is reserved and has limitations when used as the index name.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'foo': [1, 2, 3, 4],\n                   'bar': ['a', 'b', 'c', 'd'],\n                   'baz': pd.date_range('2018-01-01', freq='d', periods=4),\n                   'qux': pd.Categorical(['a', 'b', 'c', 'c'])},\n                  index=pd.Index(range(4), name='idx'))\n```\n\nLANGUAGE: python\nCODE:\n```\ndf\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.dtypes\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.to_json('test.json', orient='table')\n\n```\n\nLANGUAGE: python\nCODE:\n```\nnew_df = pd.read_json('test.json', orient='table')\n\n```\n\nLANGUAGE: python\nCODE:\n```\nnew_df\n\n```\n\nLANGUAGE: python\nCODE:\n```\nnew_df.dtypes\n\n```\n\n----------------------------------------\n\nTITLE: Property-based testing in Python using Hypothesis\nDESCRIPTION: An example of using the Hypothesis library for property-based testing, demonstrating how to generate complex inputs for thorough testing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport json\nfrom hypothesis import given, strategies as st\n\nany_json_value = st.deferred(lambda: st.one_of(\n    st.none(), st.booleans(), st.floats(allow_nan=False), st.text(),\n    st.lists(any_json_value), st.dictionaries(st.text(), any_json_value)\n))\n\n\n@given(value=any_json_value)\ndef test_json_roundtrip(value):\n    result = json.loads(json.dumps(value))\n    assert value == result\n```\n\n----------------------------------------\n\nTITLE: Assigning and Inspecting Datetimes with Timezones in pandas (Python)\nDESCRIPTION: This snippet demonstrates constructing a DataFrame with columns containing both naive and timezone-aware datetime objects using pandas. It shows the use of pd.date_range to assign datetimes with specific timezones and checks the resulting DataFrame and column data types. Dependencies: pandas. Key parameters include the date range string ('20130101'), number of periods, and tz; the output is a DataFrame with columns A (naive), B (US/Eastern), and C (CET) timezones, and their respective dtypes. This feature enables memory-efficient and performant handling of datetimes with a single timezone per column in pandas 0.17.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"A\": pd.date_range(\"20130101\", periods=3),\n        \"B\": pd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\"),\n        \"C\": pd.date_range(\"20130101\", periods=3, tz=\"CET\"),\n    }\n)\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Groupby With Grouper Using Index and Key - pandas Python\nDESCRIPTION: Shows grouping by both Grouper on a column and Grouper on an index level. Demonstrates how to handle time-based groupings when date is present in both column and index. Dependencies: pandas, DataFrame with date as column and index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndf = df.set_index(\"Date\")\ndf[\"Date\"] = df.index + pd.offsets.MonthEnd(2)\ndf.groupby([pd.Grouper(freq=\"6ME\", key=\"Date\"), \"Buyer\"])[[\"Quantity\"]].sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([pd.Grouper(freq=\"6ME\", level=\"Date\"), \"Buyer\"])[[\"Quantity\"]].sum()\n```\n\n----------------------------------------\n\nTITLE: Overriding pandas Data Structure Constructors for Subclassing - Python\nDESCRIPTION: Shows how to override special _constructor properties to preserve subclasses during operations on Series and DataFrame. Dependencies: pandas (as pd). Key properties include _constructor (same-type), _constructor_expanddim (Series-to-DataFrame), and _constructor_sliced (DataFrame-to-Series). This implementation retains the correct subclass type after various data manipulations such as slicing and conversion.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass SubclassedSeries(pd.Series):\n    @property\n    def _constructor(self):\n        return SubclassedSeries\n\n    @property\n    def _constructor_expanddim(self):\n        return SubclassedDataFrame\n\n\nclass SubclassedDataFrame(pd.DataFrame):\n    @property\n    def _constructor(self):\n        return SubclassedDataFrame\n\n    @property\n    def _constructor_sliced(self):\n        return SubclassedSeries\n```\n\n----------------------------------------\n\nTITLE: Using String Accessor Methods with Object-Dtype Series (Python)\nDESCRIPTION: Compares behavior of .str.count() on an object-dtype Series, illustrating that return type can be float64 if NAs are present, as opposed to the nullable Int64 in StringDtype. Shows importance of dtype choice for return type predictability in string operations. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns2 = pd.Series([\"a\", None, \"b\"], dtype=\"object\")\ns2.str.count(\"a\")\ns2.dropna().str.count(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Concatenating CSV Chunks with pandas in Python\nDESCRIPTION: This snippet demonstrates how to use pandas to read CSV data in chunks using read_csv with the chunksize parameter, then concatenates the resulting chunked DataFrames using concat. Requires pandas, and data must be a string or object compatible with StringIO. Useful for working with large CSVs in memory-efficient chunks; expects CSV-formatted input via data variable.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\npd.concat(pd.read_csv(StringIO(data), chunksize=2))\n```\n\n----------------------------------------\n\nTITLE: Handling EmptyDataError Exceptions with read_csv in Pandas (Python)\nDESCRIPTION: Shows standardized exception handling for empty columns/header in pd.read_csv with both 'c' and 'python' engines. Requires Pandas and the io module for StringIO. Demonstrates previous behavior where ValueError or StopIteration was raised and the new behavior where pandas.io.common.EmptyDataError is consistently thrown for empty CSV input. Inputs are empty CSV buffers; output is exception (EmptyDataError).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport io\ndf = pd.read_csv(io.StringIO(''), engine='c')\ndf = pd.read_csv(io.StringIO(''), engine='python')\n```\n\n----------------------------------------\n\nTITLE: Upsampling PeriodIndex Data in Python\nDESCRIPTION: Demonstrates how upsampling data with a PeriodIndex results in a higher frequency TimeSeries that spans the original time window in Pandas 0.9.1.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.9.1.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprng = pd.period_range('2012Q1', periods=2, freq='Q')\n\ns = pd.Series(np.random.randn(len(prng)), prng)\n\ns.resample('M')\n```\n\n----------------------------------------\n\nTITLE: Calling DataFrameGroupBy.value_counts with observed=True on Categorical Columns in pandas - Python\nDESCRIPTION: This snippet demonstrates the difference in behavior when using DataFrameGroupBy.value_counts with observed=True on Categorical columns in pandas DataFrames. Shows that new versions preserve all categories, including non-observed, in the output, aligning with expectations for categorical completeness. Requires: pandas. Inputs are DataFrames with categorical dtype; output is a Series with category counts per group. Behavior varies with pandas version.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\nIn [6]: df = pd.DataFrame([\"a\", \"b\", \"c\"], dtype=\"category\").iloc[0:2]\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [7]: df\nOut[7]:\n   0\n0  a\n1  b\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [8]: df.groupby(level=0, observed=True).value_counts()\nOut[8]:\n0  a    1\n1  b    1\ndtype: int64\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [9]: df.groupby(level=0, observed=True).value_counts()\nOut[9]:\n0  a    1\n1  a    0\n   b    1\n0  b    0\n   c    0\n1  c    0\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Advanced Pytest Parallelization and Test Selection with Marks (Bash)\nDESCRIPTION: Shows how to parallelize pytest with '-n', exclude slow/network/database/single_cpu tests using '-m', and request a detailed summary with '-r sxX'. This is useful for excluding expensive or environment-dependent tests and reviewing skip/failure status. Requires pytest-xdist and proper test marks configuration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\npytest pandas -n 4 -m \"not slow and not network and not db and not single_cpu\" -r sxX\n```\n\n----------------------------------------\n\nTITLE: Parsing Mixed Date Formats - pandas - Python\nDESCRIPTION: Reads a CSV-formatted string containing dates in multiple formats, then parses them using pandas.to_datetime with format='mixed'. Useful for columns where dates aren't consistently formatted. Requires pandas and StringIO.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndata = StringIO(\"date\\n12 Jan 2000\\n2000-01-13\\n\")\ndf = pd.read_csv(data)\ndf['date'] = pd.to_datetime(df['date'], format='mixed')\ndf\n```\n\n----------------------------------------\n\nTITLE: Sorting Data by Columns - Stata\nDESCRIPTION: Sorts the data set by columns 'sex' and 'total_bill' using the 'sort' command. Input: Data set with these columns. No dependencies. Output: Data set sorted in memory.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_14\n\nLANGUAGE: stata\nCODE:\n```\nsort sex total_bill\n```\n\n----------------------------------------\n\nTITLE: Sorting MultiIndex and Level-based Sorting - pandas - Python\nDESCRIPTION: Sorts MultiIndex-ed Series and DataFrame objects along specified levels (or named levels), affecting performance and behavior of selection. Sorting is recommended for efficient and reliable subsetting; unsorted indexes may return warnings or copies. Optimizes for read performance in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nimport random\n\nrandom.shuffle(tuples)\ns = pd.Series(np.random.randn(8), index=pd.MultiIndex.from_tuples(tuples))\ns\ns.sort_index()\ns.sort_index(level=0)\ns.sort_index(level=1)\n```\n\nLANGUAGE: python\nCODE:\n```\ns.index = s.index.set_names([\"L1\", \"L2\"])\ns.sort_index(level=\"L1\")\ns.sort_index(level=\"L2\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.T.sort_index(level=1, axis=1)\n```\n\n----------------------------------------\n\nTITLE: Customizing Index Placement in Excel Write Operation\nDESCRIPTION: Shows how to customize the placement of the index label when writing a DataFrame to Excel by disabling cell merging.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_143\n\nLANGUAGE: python\nCODE:\n```\ndf.to_excel(\"path_to_file.xlsx\", index_label=\"label\", merge_cells=False)\n```\n\n----------------------------------------\n\nTITLE: Handling Overlapping Value Columns with suffixes in pandas merge (Python)\nDESCRIPTION: Shows how to handle DataFrames with overlapping column names by appending suffixes using the 'suffixes' argument in pandas' merge. This disambiguates result columns. Requires pandas. Input: DataFrames with overlapping column(s); Output: merged DataFrame with suffixed columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame({\"k\": [\"K0\", \"K1\", \"K2\"], \"v\": [1, 2, 3]})\nright = pd.DataFrame({\"k\": [\"K0\", \"K0\", \"K3\"], \"v\": [4, 5, 6]})\n\nresult = pd.merge(left, right, on=\"k\")\nresult\n```\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.merge(left, right, on=\"k\", suffixes=(\"_l\", \"_r\"))\nresult\n```\n\n----------------------------------------\n\nTITLE: Ambiguous Positional Argument Error in DataFrame.rename (Python, pandas 1.0.0)\nDESCRIPTION: Demonstrates that passing multiple positional arguments to DataFrame.rename will raise a TypeError in pandas 1.0.0 due to changes preventing ambiguity. Assumes existence of a DataFrame 'df'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf.rename({0: 1}, {0: 2})\n```\n\n----------------------------------------\n\nTITLE: Setting Multiple pandas Options Simultaneously with pd.set_option - Python\nDESCRIPTION: This snippet demonstrates retrieving and updating multiple pandas configuration options at once. It showcases getting individual option values, using pd.set_option to set several options in a single command, and verifying that changes take effect. Dependencies: pandas library. The key API is pd.set_option, which accepts multiple option-value pairs, effectively improving configurability for end-users.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npd.get_option('a.b')\npd.get_option('b.c')\npd.set_option('a.b', 1, 'b.c', 4)\npd.get_option('a.b')\npd.get_option('b.c')\n```\n\n----------------------------------------\n\nTITLE: Creating and Using pandas.Series head Method in Python\nDESCRIPTION: Demonstrates how to create a pandas.Series and use the head method with and without the n parameter. Requires pandas to be imported as pd. Shows inputting a list of strings, creation of a Series, and output of the first few elements with default and explicit row count.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(['Ant', 'Bear', 'Cow', 'Dog', 'Falcon',\n               'Lion', 'Monkey', 'Rabbit', 'Zebra'])\nser.head()\n\n# With the ``n`` parameter, we can change the number of returned rows:\nser.head(n=3)\n```\n\n----------------------------------------\n\nTITLE: Using IntervalIndex for Overlapping Interval Selection\nDESCRIPTION: Demonstrates how to select intervals that overlap with a given interval using the overlaps method of IntervalIndex to create a boolean indexer.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nidxr = df.index.overlaps(pd.Interval(0.5, 2.5))\nidxr\ndf[idxr]\n```\n\n----------------------------------------\n\nTITLE: DataFrame Apply with Object dtype in Python\nDESCRIPTION: Fixed regression in DataFrame.apply with object dtype and non-reducing function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndf.apply(non_reducing_function)\n```\n\n----------------------------------------\n\nTITLE: Applying NumPy Universal Functions on Pandas DataFrame (Python)\nDESCRIPTION: Demonstrates how NumPy universal functions such as exp and asarray can be directly applied to a pandas DataFrame. Dependencies: NumPy, pandas, a DataFrame df. The output of np.exp(df) is a DataFrame of element-wise exponents; np.asarray(df) returns the underlying ndarray. Inputs must be compatible with these functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nnp.exp(df)\nnp.asarray(df)\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: Groupby Aggregate with Relabeling Loses Results - ipython\nDESCRIPTION: Previously, when using .agg with as_index=False and output columns relabeled, resulting DataFrames incorrectly lost data, with result values replaced by what used to be the index. This code block shows grouping, aggregating, and displaying the output, which contains grouping keys as data and missing intended columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_29\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: grouped = df.groupby(\"key\", as_index=False)\\nIn [3]: result = grouped.agg(min_val=pd.NamedAgg(column=\"val\", aggfunc=\"min\"))\\nIn [4]: result\\nOut[4]:\\n     min_val\\n 0   x\\n 1   y\\n 2   z\n```\n\n----------------------------------------\n\nTITLE: Constructing DataFrame from List of Dataclasses - Pandas - Python\nDESCRIPTION: Creates a DataFrame from a list of dataclass instances, wherein each dataclass is converted to a row dict. Uses make_dataclass from dataclasses module; all elements must be dataclass instances or a TypeError is raised. Requires Python 3.7+ and pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import make_dataclass\n\nPoint = make_dataclass(\"Point\", [(\"x\", int), (\"y\", int)])\n\npd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n```\n\n----------------------------------------\n\nTITLE: Rendering a Custom-Styled DataFrame to HTML with a Template Title (Python)\nDESCRIPTION: Uses the IPython HTML display function and the custom MyStyler object to render the styled DataFrame, specifying an additional \"table_title\" keyword argument for the template. The output is HTML that includes the customizations defined in the template. Prerequisites are the previous definition of MyStyler and initialization of df3.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nHTML(MyStyler(df3).to_html(table_title=\"Extending Example\"))\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame for Groupby Aggregate Renaming Behavior - Python\nDESCRIPTION: Creates an example DataFrame for exploring the effect of relabeling columns in groupby.agg when as_index is False. DataFrame consists of a grouping key and a value column suitable for aggregations and renaming output columns. Relies on pandas only.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"key\": [\"x\", \"y\", \"z\", \"x\", \"y\", \"z\"],\\n                  \"val\": [1.0, 0.8, 2.0, 3.0, 3.6, 0.75]})\\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating SQLAlchemy engine for SQLite in pandas\nDESCRIPTION: Demonstrates creating a SQLAlchemy engine for connecting to an in-memory SQLite database. The engine object is used for database operations throughout the pandas SQL API.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_222\n\nLANGUAGE: python\nCODE:\n```\nfrom sqlalchemy import create_engine\n\n# Create your engine.\nengine = create_engine(\"sqlite:///:memory:\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Duplicate Label Handling in pandas (Python)\nDESCRIPTION: Shows how to create Series objects with duplicate labels, how to restrict duplicate labels using the allows_duplicate_labels flag, and the resulting DuplicateLabelError when violations occur. Dependencies: pandas package. Key parameters include allows_duplicate_labels, set in set_flags. Inputs: Series data and index; Output: Series with or without duplicates, or raises DuplicateLabelError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: pd.Series([1, 2], index=['a', 'a'])\nOut[1]:\na    1\na    2\nLength: 2, dtype: int64\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: pd.Series([1, 2], index=['a', 'a']).set_flags(allows_duplicate_labels=False)\n...\nDuplicateLabelError: Index has duplicates.\n      positions\nlabel\na        [0, 1]\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: a = (\n   ...:     pd.Series([1, 2], index=['a', 'b'])\n   ...:       .set_flags(allows_duplicate_labels=False)\n   ...: )\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [4]: a\nOut[4]:\na    1\nb    2\nLength: 2, dtype: int64\n```\n\nLANGUAGE: python\nCODE:\n```\n# An operation introducing duplicates\nIn [5]: a.reindex(['a', 'b', 'a'])\n...\nDuplicateLabelError: Index has duplicates.\n      positions\nlabel\na        [0, 2]\n\n[1 rows x 1 columns]\n```\n\n----------------------------------------\n\nTITLE: Querying Memory Sharing with NumPy and Pandas Python\nDESCRIPTION: Demonstrates the use of numpy's shares_memory to check if a pandas DataFrame column shares memory with another numpy array. Requires pandas and numpy. Takes a DataFrame df, and uses np.shares_memory with df[\"A\"].values or the column directly against another array (values/new), returning a boolean. Useful for understanding memory views and copy semantics between DataFrames and numpy arrays.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nnp.shares_memory(df[\"A\"].values, values)\nnp.shares_memory(df[\"A\"], new)\nnp.shares_memory(df[\"A\"], values)\n```\n\n----------------------------------------\n\nTITLE: String Length Calculation - Stata\nDESCRIPTION: Demonstrates use of Stata functions 'strlen' for ASCII strings and 'ustrlen' for Unicode strings in generating new columns reflecting the lengths of values in the 'time' column. Input: 'time' character column present. Output: Two new columns with string lengths. No dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_15\n\nLANGUAGE: stata\nCODE:\n```\ngenerate strlen_time = strlen(time)\\ngenerate ustrlen_time = ustrlen(time)\n```\n\n----------------------------------------\n\nTITLE: Column Selection, Addition, and Deletion in DataFrame - Pandas - Python\nDESCRIPTION: Demonstrates how to select, add, and delete columns using familiar dictionary-style operations in pandas DataFrames. Includes scalar assignment (broadcasting), conformance of Series to index, and raw ndarray insertion. Uses methods like del and pop, as well as assignment to new/existing columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf[\"one\"]\ndf[\"three\"] = df[\"one\"] * df[\"two\"]\ndf[\"flag\"] = df[\"one\"] > 2\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndel df[\"two\"]\nthree = df.pop(\"three\")\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf[\"foo\"] = \"bar\"\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf[\"one_trunc\"] = df[\"one\"][:2]\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.insert(1, \"bar\", df[\"one\"])\ndf\n```\n\n----------------------------------------\n\nTITLE: Converting different data types to StringDtype in pandas\nDESCRIPTION: Example showing how StringDtype now works in situations where astype(str) would work, allowing different types of data including numbers and NaN values to be converted to string dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, \"abc\", np.nan], dtype=\"string\")\nser\nser[0]\npd.Series([1, 2, np.nan], dtype=\"Int64\").astype(\"string\")\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: Groupby with as_index=False Modified Columns - ipython\nDESCRIPTION: Shows that previously, using groupby with as_index=False and certain reductions would incorrectly modify or replace grouping columns. Demonstrates aggregation 'nunique' resulting in columns with unintended values. Input: DataFrame, groupby method, as_index parameter, and aggregation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_24\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: df.groupby(\"a\", as_index=False).nunique()\\nOut[4]:\\n   a  b\\n0  1  1\\n1  1  2\n```\n\n----------------------------------------\n\nTITLE: File Reading Operations - Python\nDESCRIPTION: Fixes for file reading functions including read_fwf(), read_csv(), and read_excel() addressing various regression issues with parameter handling and data type conversion.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.4.3.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nread_fwf()\nread_csv()\nread_excel()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Data from DataFrame as NumPy Array in Pandas Python\nDESCRIPTION: This snippet demonstrates using the to_numpy() method on a DataFrame to retrieve its underlying array. This is suitable when all columns share a common dtype; otherwise, data consersion or copy may occur. It is especially useful for computational tasks requiring NumPy or for exporting data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.to_numpy()\n```\n\n----------------------------------------\n\nTITLE: Reading JSON String as DataFrame using StringIO - pandas - Python\nDESCRIPTION: Shows how to parse a JSON string as a DataFrame using pd.read_json together with StringIO for string buffer integration. Requires pandas and Python's io.StringIO module. Inputs: JSON string (variable json). Outputs: DataFrame reconstructed from JSON.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\npd.read_json(StringIO(json))\n```\n\n----------------------------------------\n\nTITLE: Reading XPORT Files Iteratively with pandas - Python\nDESCRIPTION: Demonstrates incremental reading of a SAS XPORT file by specifying the chunksize parameter in pandas.read_sas. This example yields DataFrame objects of up to 10,000 rows, which can be processed one at a time to save memory. Requires pandas and a valid XPORT file; input is the file path and chunk size, and output is an iterator over DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfor df in pd.read_sas(\"sas_xport.xpt\", chunksize=10000):\n    do_something(df)\n```\n\n----------------------------------------\n\nTITLE: Reading XML Data with iterparse in pandas\nDESCRIPTION: Shows how to use pandas' read_xml method with the iterparse parameter to efficiently parse large XML files without loading the entire tree into memory. The example demonstrates extracting specific elements and attributes from an XML file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_4\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: df = pd.read_xml(\n...      \"/path/to/downloaded/enwikisource-latest-pages-articles.xml\",\n...      iterparse = {\"page\": [\"title\", \"ns\", \"id\"]})\n...  )\ndf\nOut[2]:\n                                                     title   ns        id\n0                                   Gettysburg Address    0     21450\n1                                            Main Page    0     42950\n2                        Declaration by United Nations    0      8435\n3         Constitution of the United States of America    0      8435\n4                 Declaration of Independence (Israel)    0     17858\n...                                                ...  ...       ...\n3578760           Page:Black cat 1897 07 v2 n10.pdf/17  104    219649\n3578761           Page:Black cat 1897 07 v2 n10.pdf/43  104    219649\n3578762           Page:Black cat 1897 07 v2 n10.pdf/44  104    219649\n3578763  The History of Tom Jones, a Foundling/Book IX    0  12084291\n3578764  Page:Shakespeare of Stratford (1926) Yale.djvu/91  104     21450\n\n[3578765 rows x 3 columns]\n```\n\n----------------------------------------\n\nTITLE: Creating Indicator Variables with Index.str.get_dummies in Pandas (Python)\nDESCRIPTION: Illustrates the new Index.str.get_dummies method that returns a MultiIndex corresponding to indicator variables. Requires Pandas. Demonstrates creating a textual Index and applying get_dummies with a separator character. Input is categorical string data; output is a MultiIndex result.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([\"a|b\", \"a|c\", \"b|c\"])\nidx.str.get_dummies(\"|\")\n```\n\n----------------------------------------\n\nTITLE: Performing Asof Merge with Pandas DataFrames in Python\nDESCRIPTION: Demonstrates how to use pandas' merge_asof function to perform asof-style joining of time-series DataFrames. These examples show matching on nearest keys, using the allow_exact_matches parameter for only prior data, and grouping by ticker for typical trade and quote datasets. Requires pandas library and DataFrames as inputs; returns merged DataFrame with nearest matches per key.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nleft = pd.DataFrame({\"a\": [1, 5, 10], \"left_val\": [\"a\", \"b\", \"c\"]})\nright = pd.DataFrame({\"a\": [1, 2, 3, 6, 7], \"right_val\": [1, 2, 3, 6, 7]})\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge_asof(left, right, on=\"a\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge_asof(left, right, on=\"a\", allow_exact_matches=False)\n```\n\nLANGUAGE: python\nCODE:\n```\ntrades = pd.DataFrame(\n    {\n        \"time\": pd.to_datetime(\n            [\n                \"20160525 13:30:00.023\",\n                \"20160525 13:30:00.038\",\n                \"20160525 13:30:00.048\",\n                \"20160525 13:30:00.048\",\n                \"20160525 13:30:00.048\",\n            ]\n        ),\n        \"ticker\": [\"MSFT\", \"MSFT\", \"GOOG\", \"GOOG\", \"AAPL\"],\n        \"price\": [51.95, 51.95, 720.77, 720.92, 98.00],\n        \"quantity\": [75, 155, 100, 100, 100],\n    },\n    columns=[\"time\", \"ticker\", \"price\", \"quantity\"],\n)\n\nquotes = pd.DataFrame(\n    {\n        \"time\": pd.to_datetime(\n            [\n                \"20160525 13:30:00.023\",\n                \"20160525 13:30:00.023\",\n                \"20160525 13:30:00.030\",\n                \"20160525 13:30:00.041\",\n                \"20160525 13:30:00.048\",\n                \"20160525 13:30:00.049\",\n                \"20160525 13:30:00.072\",\n                \"20160525 13:30:00.075\",\n            ]\n        ),\n        \"ticker\": [\"GOOG\", \"MSFT\", \"MSFT\", \"MSFT\", \"GOOG\", \"AAPL\", \"GOOG\", \"MSFT\"],\n        \"bid\": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],\n        \"ask\": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03],\n    },\n    columns=[\"time\", \"ticker\", \"bid\", \"ask\"],\n)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge_asof(trades, quotes, on=\"time\", by=\"ticker\")\n```\n\n----------------------------------------\n\nTITLE: Rolling Sum Centered Calculation with NaN Padding - pandas - Python\nDESCRIPTION: Demonstrates how rolling_sum(center=True) fills the final entries with computed values considering NaN padding, instead of returning trailing NaN. Input is a Series; window and min_periods parameters control calculation. Output is rolling sum Series. Requires pandas and is specific to 0.15.0+ (legacy code included for comparison).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npd.rolling_sum(Series(range(4)), window=3, min_periods=0, center=True)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.rolling_sum(pd.Series(range(4)), window=3,\\n                min_periods=0, center=True)\n```\n\n----------------------------------------\n\nTITLE: Constructing a DataFrame with Datalines - SAS\nDESCRIPTION: This SAS code snippet demonstrates how to create a data set ('df') from manually entered values using the 'datalines' statement. The 'input' statement specifies column names, and values are entered in the section after 'datalines'. The resulting dataset can be used for further analysis and processing. Inputs are the inline values; outputs are a new data set named 'df' with columns x and y.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_0\n\nLANGUAGE: SAS\nCODE:\n```\ndata df;\\n    input x y;\\n    datalines;\\n    1 2\\n    3 4\\n    5 6\\n    ;\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Combining Row Labels and Value Conditions in Python\nDESCRIPTION: Selects rows based on both value conditions and index values. This technique filters rows where both a column condition is met and the index is in a specified list.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n)\ndf\n\ndf[(df.AAA <= 6) & (df.index.isin([0, 2, 4]))]\n```\n\n----------------------------------------\n\nTITLE: Attempting Inplace Assignment with loc/iloc for Entire Columns - Pandas Python\nDESCRIPTION: This snippet shows how assigning an entire column with loc or iloc now prefers inplace modification of the underlying data array. It creates a float DataFrame, saves its values array, assigns an int64 array using loc, and compares dtype and memory sharing before/after. Requires pandas and numpy. Inputs: DataFrame, array to assign. Outputs: DataFrame with modified column and checks for dtype and whether memory is shared between the new column values and the assigned array.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(range(3), columns=[\"A\"], dtype=\"float64\")\nvalues = df.values\nnew = np.array([5, 6, 7], dtype=\"int64\")\ndf.loc[[0, 1, 2], \"A\"] = new\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: df.dtypes\nOut[1]:\nA    int64\ndtype: object\nIn [2]: np.shares_memory(df[\"A\"].values, new)\nOut[2]: False\n```\n\n----------------------------------------\n\nTITLE: Logical OR with np.nan in pandas Series: object vs boolean dtype (Python)\nDESCRIPTION: This code compares the behavior of performing a logical OR operation between a Series containing np.nan and True, with object and boolean dtypes. For object dtype, np.nan is treated as always False. For boolean dtype, Kleene logic applies. Inputs are Series of type object and boolean; outputs are the result of the OR operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/boolean.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.Series([True, False, np.nan], dtype=\"object\") | True\npd.Series([True, False, np.nan], dtype=\"boolean\") | True\n```\n\n----------------------------------------\n\nTITLE: Merging, Reshaping, and Casting DataFrames with Nullable Integer Dtypes (pandas, Python)\nDESCRIPTION: Demonstrates the ability to concatenate DataFrames containing nullable integer columns, and type casting of such columns to standard float. Requires a DataFrame df as prior context; demonstrates pd.concat for merging and astype for explicit casting. Shows compatibility of nullable dtypes with core DataFrame operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npd.concat([df[[\"A\"]], df[[\"B\", \"C\"]]], axis=1).dtypes\ndf[\"A\"].astype(float)\n```\n\n----------------------------------------\n\nTITLE: Creating Example DataFrame and DataFrameGroupBy - pandas - Python\nDESCRIPTION: These snippets demonstrate how to create DataFrame and GroupBy objects in pandas, serving as the foundational context for subsequent examples of reduction operations and their changing behavior with deprecation. The snippet introduces a numeric and a datetime column, and groups data for aggregation. Required dependencies: pandas. Key parameters: data dictionary for DataFrame, group labels for groupby. Outputs are pandas DataFrame objects and GroupBy objects, used as input for prod operations in later examples.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1, 2, 3, 4], \"B\": pd.date_range(\"2016-01-01\", periods=4)})\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1, 2, 3, 4], \"B\": pd.date_range(\"2016-01-01\", periods=4)})\ngb = df.groupby([1, 1, 2, 2])\n```\n\n----------------------------------------\n\nTITLE: Creating and Using UInt64Index in pandas - Python\nDESCRIPTION: Demonstrates creating an unsigned 64-bit integer index (UInt64Index) and assigning it to a DataFrame, then displaying the index type. Highlights improved support for unsigned integer operations in pandas. Requires pandas. Inputs include an integer list for the index, outputs the index with type annotation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: idx = pd.UInt64Index([1, 2, 3])\nIn [2]: df = pd.DataFrame({'A': ['a', 'b', 'c']}, index=idx)\nIn [3]: df.index\nOut[3]: UInt64Index([1, 2, 3], dtype='uint64')\n```\n\n----------------------------------------\n\nTITLE: Converting Sparse DataFrame to Dense Format with .sparse.to_dense - pandas - Python\nDESCRIPTION: Converts a sparse pandas DataFrame (sdf) to its dense representation using the .sparse.to_dense() accessor. This is necessary when downstream operations require standard DataFrames or interoperability with functions that do not support sparse types. Requires pandas. The input is a sparse DataFrame, the output is its dense version.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsdf.sparse.to_dense()\n```\n\n----------------------------------------\n\nTITLE: New Behavior: DataFrameGroupBy.size Returns DataFrame with Columns - python\nDESCRIPTION: Shows that now, .size() on groupby result with as_index=False returns a DataFrame where grouping columns are preserved as columns, not index. Increases uniformity between groupby result types. Inputs: groupby object and size() method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"a\", as_index=False).size()\n```\n\n----------------------------------------\n\nTITLE: Generating TimedeltaIndex with Start/End/Frequency - Python\nDESCRIPTION: This example creates a TimedeltaIndex between two endpoints with a specified frequency (30 minutes), using the pd.timedelta_range function. It shows rich indexing using string dates. Inputs are start and end strings with frequency; outputs are TimedeltaIndex objects. Requires pandas >= 0.15.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.timedelta_range(start='1 days', end='2 days', freq='30T')\n```\n\n----------------------------------------\n\nTITLE: Specifying Column Data Types in Excel Read\nDESCRIPTION: Shows how to set specific data types for columns when reading an Excel file using the dtype parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_141\n\nLANGUAGE: python\nCODE:\n```\npd.read_excel(\"path_to_file.xls\", dtype={\"MyInts\": \"int64\", \"MyText\": str})\n```\n\n----------------------------------------\n\nTITLE: Hiding Rows and Columns in DataFrame Display with Styler - pandas - Python\nDESCRIPTION: Demonstrates selective hiding of rows and columns in a DataFrame display using the Styler.hide method. Dependencies: pandas, numpy. Input: 5x5 DataFrame of random standard normal values; parameters: lists of row/column indices to hide (subset, axis). Output is a DataFrame display with specified rows and columns hidden from view, not affecting underlying data or CSS class indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.default_rng().standard_normal((5, 5)))\ndf.style.hide(subset=[0, 2, 4], axis=0).hide(subset=[0, 2, 4], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Performing Elementwise Bitwise Comparison with pandas Series - Python\nDESCRIPTION: Demonstrates the use of bitwise boolean operators (such as ==) on pandas Series, resulting in Series of per-element boolean outcomes. Requires pandas. Shows Series creation and elementwise equality comparison output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/gotchas.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(range(5))\ns == 4\n```\n\n----------------------------------------\n\nTITLE: Querying Series after Assignment for dtype Pandas Python\nDESCRIPTION: Prints the contents and dtype of Series 'ser' and 'ser2' after assignment to non-boolean values. Requires pandas and numpy. Used to verify and illustrate the effect of setting values (np.nan, 2.0) in a Series of dtype bool, demonstrating how Series dtype transitions to object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nser\nser2\n```\n\n----------------------------------------\n\nTITLE: Selecting Rows with .iloc and get_loc - Pandas - Python\nDESCRIPTION: Demonstrates selecting rows by position and columns by locating the column position, using .iloc and get_loc. Requires df to be defined. Selects the 0th and 2nd row positions in column 'A', outputting a Series of their values. No external dependencies beyond Pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndf.iloc[[0, 2], df.columns.get_loc('A')]\n```\n\n----------------------------------------\n\nTITLE: Finding Substring Position with pandas Series.str.find in Python\nDESCRIPTION: This snippet demonstrates how to use the pandas Series.str.find method to search for the first occurrence of the substring \\\"ale\\\" in the \\\"sex\\\" column of a DataFrame named \\\"tips\\\". The method returns the integer position of the substring if found and \\\"-1\\\" otherwise, respecting Python\\'s zero-based indexing. It requires pandas to be installed and a loaded DataFrame named \\\"tips\\\" with a column \\\"sex\\\" containing string values; the input is the single substring, and the output is a Series of integer positions for each entry in the column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/find_substring.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips[\"sex\"].str.find(\"ale\")\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame Columns by Row Values using pandas sort_values - Python\nDESCRIPTION: Demonstrates how to re-order DataFrame columns based on the values in a given row using the 'axis=1' and 'by' arguments in pd.DataFrame.sort_values. Requires pandas. 'by' specifies the row label whose values will be used for sorting columns. The input is a DataFrame; the output is a DataFrame with columns sorted according to the referenced row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [2, 7], \"B\": [3, 5], \"C\": [4, 8]}, index=[\"row1\", \"row2\"])\ndf\ndf.sort_values(by=\"row2\", axis=1)\n```\n\n----------------------------------------\n\nTITLE: Setting Display Max Column Width in Python\nDESCRIPTION: Fixed regression where setting pd.options.display.max_colwidth was not accepting negative integer. This behavior has been deprecated in favor of using None.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\npd.options.display.max_colwidth = None\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Dictionary of Series - pandas - Python\nDESCRIPTION: Creates a DataFrame from a dictionary whose values are pandas Series with different indexes. Demonstrates DataFrame construction, automatic union of indexes, and missing value handling. Outputs DataFrame objects with potentially NaN values where indexes don't overlap.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nd = {\n    \"one\": pd.Series([1.0, 2.0, 3.0], index=[\"a\", \"b\", \"c\"]),\n    \"two\": pd.Series([1.0, 2.0, 3.0, 4.0], index=[\"a\", \"b\", \"c\", \"d\"]),\n}\ndf = pd.DataFrame(d)\ndf\n\npd.DataFrame(d, index=[\"d\", \"b\", \"a\"])\npd.DataFrame(d, index=[\"d\", \"b\", \"a\"], columns=[\"two\", \"three\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Example DataFrames to Demonstrate Merge Order - Python\nDESCRIPTION: Creates two example pandas DataFrames with columns 'animal' and 'max_speed', designed for use in demonstrating merge behavior and row order preservation. Both DataFrames are initialized with dictionaries; left_df and right_df contain overlap for testing merge semantics. No special dependencies beyond pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nleft_df = pd.DataFrame({'animal': ['dog', 'pig'],\\n                       'max_speed': [40, 11]})\\nright_df = pd.DataFrame({'animal': ['quetzal', 'pig'],\\n                        'max_speed': [80, 11]})\\nleft_df\\nright_df\n```\n\n----------------------------------------\n\nTITLE: Closing and Cleaning Up HDFStore Files - Python\nDESCRIPTION: This snippet ensures resources are released by closing the open HDFStore and deleting the created HDF5 file. Dependencies: pandas, os. Input: open HDFStore. Output: file descriptor closed, file removed from the filesystem. Important for preventing file locks and managing disk space.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nstore.close()\nos.remove(\"test.h5\")\n```\n\n----------------------------------------\n\nTITLE: Generating Nanosecond-Precision Date Ranges with date_range (Python)\nDESCRIPTION: Creates a date range with 5 periods at nanosecond frequency using pd.date_range and '5N' freq. Requires pandas >= 0.13 and numpy >= 1.7. Inputs: start date, periods, freq. Outputs: DatetimeIndex of nanosecond intervals. Extends precision for high-frequency time series use cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\npd.date_range('2013-01-01', periods=5, freq='5N')\n```\n\n----------------------------------------\n\nTITLE: Extension Type Inference with pandas.array and NaN - pandas (Python)\nDESCRIPTION: Demonstrates dtype inference in pandas.array when input contains NaN but no explicit dtype specified, resulting in float dtype. Shows practical implications of NaN being float and thus input list [1, 2, np.nan] resulting in float array rather than extension Int64 array in absence of dtype specification.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npd.array([1, 2, np.nan])\n```\n\n----------------------------------------\n\nTITLE: Using Time-Series Aware Rolling Windows with Pandas in Python\nDESCRIPTION: Illustrates new time-aware capabilities of the DataFrame.rolling() method, accepting both integer window sizes and time-based (offset) windows. Examples show calculations on regular and irregular indices and demonstrate the use of the 'on' parameter to specify a DataFrame column for windowing. Requires the pandas and numpy libraries and suitable time-indexed DataFrame input; outputs DataFrame with rolling sums.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndft = pd.DataFrame(\n    {\"B\": [0, 1, 2, np.nan, 4]},\n    index=pd.date_range(\"20130101 09:00:00\", periods=5, freq=\"s\"),\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndft.rolling(2).sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndft.rolling(2, min_periods=1).sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndft.rolling(\"2s\").sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndft = pd.DataFrame(\n    {\"B\": [0, 1, 2, np.nan, 4]},\n    index=pd.Index(\n        [\n            pd.Timestamp(\"20130101 09:00:00\"),\n            pd.Timestamp(\"20130101 09:00:02\"),\n            pd.Timestamp(\"20130101 09:00:03\"),\n            pd.Timestamp(\"20130101 09:00:05\"),\n            pd.Timestamp(\"20130101 09:00:06\"),\n        ],\n        name=\"foo\",\n    ),\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndft.rolling(2).sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndft.rolling(\"2s\").sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndft = dft.reset_index()\ndft.rolling(\"2s\", on=\"foo\").sum()\n```\n\n----------------------------------------\n\nTITLE: Improving Performance of memory_usage for Object Dtype\nDESCRIPTION: Fixes a performance regression in memory_usage(deep=True) for object dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmemory_usage(deep=True)\n```\n\n----------------------------------------\n\nTITLE: Proper Chained Assignment with CoW\nDESCRIPTION: Demonstrates the correct way to perform conditional assignments using loc instead of chained assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/copy_on_write.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})\ndf.loc[df[\"bar\"] > 5, \"foo\"] = 100\n```\n\n----------------------------------------\n\nTITLE: Targeted Multi-Axis Plotting of DataFrame Columns - pandas/matplotlib - Python\nDESCRIPTION: This snippet demonstrates plotting multiple DataFrame columns on separate axes produced by matplotlib's subplots, explicitly targeting axes via the 'ax' parameter. Each subplot is labeled and displayed with a unique title. Dependencies: pandas, matplotlib, numpy, and a DataFrame 'df'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nfig, axes = plt.subplots(nrows=2, ncols=2)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\ndf[\"A\"].plot(ax=axes[0, 0]);\naxes[0, 0].set_title(\"A\");\ndf[\"B\"].plot(ax=axes[0, 1]);\naxes[0, 1].set_title(\"B\");\ndf[\"C\"].plot(ax=axes[1, 0]);\naxes[1, 0].set_title(\"C\");\ndf[\"D\"].plot(ax=axes[1, 1]);\n@savefig series_plot_multi.png\naxes[1, 1].set_title(\"D\");\n```\n\n----------------------------------------\n\nTITLE: Computing Pairwise Rolling Covariances Using pairwise=True - pandas - Python\nDESCRIPTION: This code computes rolling pairwise covariances between multiple columns of a DataFrame using Rolling.cov with pairwise=True. It demonstrates calculation of a MultiIndexed DataFrame for all column pairs specified. Dependencies: pandas, numpy. Inputs: DataFrame with selected columns, additional DataFrame to specify columns for pairwise comparison, window size; Output: MultiIndexed DataFrame of rolling covariances. Handles missing data with pairwise complete observations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncovs = (\n    df[[\"B\", \"C\", \"D\"]]\n    .rolling(window=4)\n    .cov(df[[\"A\", \"B\", \"C\"]], pairwise=True)\n)\ncovs\n```\n\n----------------------------------------\n\nTITLE: Box Plot for DataFrame Columns (Python)\nDESCRIPTION: Generates a boxplot for each column in a DataFrame using DataFrame.plot.box(), visualizing the distribution, median, quartiles, and outliers. Useful for comparing spread and range across multiple variables. Requires pandas, numpy, matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\\n\\n@savefig box_plot_new.png\\ndf.plot.box();\n```\n\n----------------------------------------\n\nTITLE: New Index.map and MultiIndex.map Return Types - Python\nDESCRIPTION: This snippet exemplifies the new behavior for Index.map and MultiIndex.map: both now return a corresponding Index object, preserving index structure and supporting Index-specific methods. The only dependency is Pandas. Inputs are Index or MultiIndex objects; outputs are Index or MultiIndex. Limitation: these objects are immutable, unlike numpy arrays. Convert to numpy type via np.asarray if needed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\nidx.map(lambda x: x * 2)\nidx.map(lambda x: (x, x * 2))\n\nmi.map(lambda x: x)\n\nmi.map(lambda x: x[0])\n```\n\n----------------------------------------\n\nTITLE: Storing and Selecting MultiIndex DataFrames in HDF5 - pandas - Python\nDESCRIPTION: Constructs a MultiIndex DataFrame, stores it in HDFStore, and demonstrates selecting on index levels. Shows how index levels become queryable columns, and includes an explicit query using index level name. Requires pandas and numpy; input is a MultiIndex and DataFrame, and output is persisted and filtered data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_176\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.MultiIndex(\n   levels=[[\"foo\", \"bar\", \"baz\", \"qux\"], [\"one\", \"two\", \"three\"]],\n   codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n   names=[\"foo\", \"bar\"],\n)\ndf_mi = pd.DataFrame(np.random.randn(10, 3), index=index, columns=[\"A\", \"B\", \"C\"])\ndf_mi\n\nstore.append(\"df_mi\", df_mi)\nstore.select(\"df_mi\")\n\n# the levels are automatically included as data columns\nstore.select(\"df_mi\", \"foo=bar\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Columns with usecols Parameter by Name\nDESCRIPTION: Demonstrates how to select a subset of columns when reading CSV data by specifying column names in the usecols parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,c,d\\n1,2,3,foo\\n4,5,6,bar\\n7,8,9,baz\"\npd.read_csv(StringIO(data))\npd.read_csv(StringIO(data), usecols=[\"b\", \"d\"])\n```\n\n----------------------------------------\n\nTITLE: Writing Extended Summaries in Python Function Docstrings\nDESCRIPTION: Demonstrates using an extended summary in addition to a short one in a docstring, formatted with a blank line in between. The 'unstack' function includes multi-paragraph, detailed explanations of use cases, avoiding parameter-level details. Intended usage: as reference for developing pandas-compliant docstrings documented with Sphinx. No functional dependencies or parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef unstack():\n    \"\"\"\n    Pivot a row index to columns.\n\n    When using a MultiIndex, a level can be pivoted so each value in\n    the index becomes a column. This is especially useful when a subindex\n    is repeated for the main index, and data is easier to visualize as a\n    pivot table.\n\n    The index level will be automatically removed from the index when added\n    as columns.\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Stacking by Level Index Instead of Name - pandas - Python\nDESCRIPTION: Performs the same stacking operation as above but with level indices instead of names. This is often used for programmatic or dynamic stacking of levels in wide DataFrames. Input is a DataFrame with MultiIndex columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n# df.stack(level=['animal', 'hair_length'])\n# from above is equivalent to:\ndf.stack(level=[1, 2])\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Data with Binary Operations and Fill Values in Pandas DataFrames in Python\nDESCRIPTION: This snippet showcases DataFrame arithmetic where missing values (NaN) are handled via the fill_value parameter in binary operations. By copying a DataFrame and altering a value, then performing both + and add(filled) operations, it demonstrates how to substitute NaN with a desired value (here, 0) during computation. Ensuring data consistency during arithmetic with missing data is a major use case.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.copy()\ndf2.loc[\"a\", \"three\"] = 1.0\ndf\ndf2\ndf + df2\ndf.add(df2, fill_value=0)\n```\n\n----------------------------------------\n\nTITLE: Andrews Curves Plotting with Colormap using pandas and Matplotlib in Python\nDESCRIPTION: This code plots Andrews curves using the \"winter\" colormap for class identification via pandas plotting API. Dependencies include pandas, matplotlib, and data containing a label column ('Name'). Key parameters: data, class_column, colormap. Input is a DataFrame; output is an Andrews curves plot, with each class colored distinctly. Limitation is that data must fit the requirements of andrews_curves (no missing values in class_column).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\nandrews_curves(data, \"Name\", colormap=\"winter\");\n```\n\n----------------------------------------\n\nTITLE: Factorizing Data in Pandas\nDESCRIPTION: This example shows how to use pd.factorize to encode a series of mixed data types into integer labels. It demonstrates handling of string values, NaN, and numeric values, including infinity.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nx = pd.Series([\"A\", \"A\", np.nan, \"B\", 3.14, np.inf])\nx\nlabels, uniques = pd.factorize(x)\nlabels\nuniques\n```\n\n----------------------------------------\n\nTITLE: Creating Scatter Matrix Plot in pandas\nDESCRIPTION: Generates a scatter matrix plot using pandas.plotting.scatter_matrix with a DataFrame of random data. The alpha parameter controls transparency, figsize sets the dimensions, and diagonal='kde' adds kernel density plots on the diagonal.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.plotting import scatter_matrix\n\ndf = pd.DataFrame(np.random.randn(1000, 4), columns=[\"a\", \"b\", \"c\", \"d\"])\n\nscatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal=\"kde\");\n```\n\n----------------------------------------\n\nTITLE: Enabling Copy-on-Write Globally in pandas\nDESCRIPTION: Demonstrates how to enable the copy-on-write feature globally in pandas, which ensures that any DataFrame or Series derived from another behaves as a copy rather than a view.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"mode.copy_on_write\", True)\npd.options.mode.copy_on_write = True\n```\n\n----------------------------------------\n\nTITLE: Converting multiple timezones to UTC when parsing datetimes\nDESCRIPTION: Shows how to convert datetime strings with different UTC offsets to UTC timezone using the utc=True parameter in to_datetime.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime([\"2015-11-18 15:30:00+05:30\",\n                \"2015-11-18 16:30:00+06:30\"], utc=True)\n```\n\n----------------------------------------\n\nTITLE: Explicitly Setting Nullable Integer Dtype for Arrays and Series (pandas, Python)\nDESCRIPTION: Shows explicit specification of the 'Int64' nullable integer dtype for both pd.array and pd.Series to ensure missing values are represented as pandas.NA and dtype is consistent throughout. This avoids ambiguity in missing data handling. Requires pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npd.array([1, None], dtype=\"Int64\")\npd.Series([1, None], dtype=\"Int64\")\n```\n\n----------------------------------------\n\nTITLE: Filling NaN in Timedelta Series with fillna - pandas - Python\nDESCRIPTION: Shows usage of fillna on Timedelta Series with various Timedelta values to impute missing entries. Exhibits flexibility in representing missing time differences.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ny.fillna(pd.Timedelta(0))\ny.fillna(pd.Timedelta(10, unit=\"s\"))\ny.fillna(pd.Timedelta(\"-1 days, 00:00:05\"))\n```\n\n----------------------------------------\n\nTITLE: Specifying Excel Writer Engine in to_excel Method\nDESCRIPTION: Demonstrates how to explicitly specify which Excel writer engine to use when writing a DataFrame to an Excel file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_146\n\nLANGUAGE: python\nCODE:\n```\n# By setting the 'engine' in the DataFrame 'to_excel()' methods.\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\", engine=\"xlsxwriter\")\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(\"path_to_file.xlsx\", engine=\"xlsxwriter\")\n\n# Or via pandas configuration.\nfrom pandas import options  # noqa: E402\n\noptions.io.excel.xlsx.writer = \"xlsxwriter\"\n\ndf.to_excel(\"path_to_file.xlsx\", sheet_name=\"Sheet1\")\n```\n\n----------------------------------------\n\nTITLE: Reading Excel Binary Files with pyxlsb Engine in Pandas\nDESCRIPTION: Uses pandas.read_excel with the pyxlsb engine to read Excel Binary Workbook (.xlsb) files. Note that pyxlsb does not recognize datetime types and will return floats instead.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_150\n\nLANGUAGE: python\nCODE:\n```\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xlsb\", engine=\"pyxlsb\")\n```\n\n----------------------------------------\n\nTITLE: Grouped Histogram for Series using by Keyword (Python)\nDESCRIPTION: Illustrates grouping for histograms using Series.hist(by=...) to partition data based on random labels. The resulting plot shows separate histograms for each group in a grid layout. Inputs are a random Series and a grouping array. Dependencies: pandas, numpy, matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndata = pd.Series(np.random.randn(1000))\\n\\n@savefig grouped_hist.png\\ndata.hist(by=np.random.randint(0, 4, 1000), figsize=(6, 4));\n```\n\n----------------------------------------\n\nTITLE: Selecting Multiple nth Values in GroupBy - Pandas Python\nDESCRIPTION: Demonstrates new capabilities of the groupby.nth method in pandas, which can now select multiple nth values from each group by passing a list of desired positions. Requires pandas and a DataFrame with business date indexes. The example groups a DataFrame by year and month, returning the first, fourth, and last date index per group. Inputs are the DataFrame and grouping keys; output is the subset DataFrame matching nth positions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nbusiness_dates = pd.date_range(start='4/1/2014', end='6/30/2014', freq='B')\ndf = pd.DataFrame(1, index=business_dates, columns=['a', 'b'])\n# get the first, 4th, and last date index for each month\ndf.groupby([df.index.year, df.index.month]).nth([0, 3, -1])\n```\n\n----------------------------------------\n\nTITLE: Creating Categorical Variables and Binning - pandas - Python\nDESCRIPTION: This Python snippet shows how to bucketize a pandas Series using pd.cut and convert a Series to the 'category' dtype, equivalent to R's cut and factor functions. Input must be a pandas Series; requires pandas. The output is a new categorical Series suitable for grouping or statistical use.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\npd.cut(pd.Series([1, 2, 3, 4, 5, 6]), 3)\npd.Series([1, 2, 3, 2, 2, 3]).astype(\"category\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Frequency Table by Group in Pandas - Python\nDESCRIPTION: Builds a DataFrame representing exam results, then groups by 'ExamYear' and applies several aggregations: counts of participation, passes, employment, and mean grade. This produces a summary statistics table by year. Inputs are academic records; outputs are aggregated year-wise statistics. Requires pandas and a DataFrame with categorical and numeric columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ngrades = [48, 99, 75, 80, 42, 80, 72, 68, 36, 78]\ndf = pd.DataFrame(\n    {\n        \"ID\": [\"x%d\" % r for r in range(10)],\n        \"Gender\": [\"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\"],\n        \"ExamYear\": [\n            \"2007\",\n            \"2007\",\n            \"2007\",\n            \"2008\",\n            \"2008\",\n            \"2008\",\n            \"2008\",\n            \"2009\",\n            \"2009\",\n            \"2009\",\n        ],\n        \"Class\": [\n            \"algebra\",\n            \"stats\",\n            \"bio\",\n            \"algebra\",\n            \"algebra\",\n            \"stats\",\n            \"stats\",\n            \"algebra\",\n            \"bio\",\n            \"bio\",\n        ],\n        \"Participated\": [\n            \"yes\",\n            \"yes\",\n            \"yes\",\n            \"yes\",\n            \"no\",\n            \"yes\",\n            \"yes\",\n            \"yes\",\n            \"yes\",\n            \"yes\",\n        ],\n        \"Passed\": [\"yes\" if x > 50 else \"no\" for x in grades],\n        \"Employed\": [\n            True,\n            True,\n            True,\n            False,\n            False,\n            False,\n            False,\n            True,\n            True,\n            False,\n        ],\n        \"Grade\": grades,\n    }\n)\n\ndf.groupby(\"ExamYear\").agg(\n    {\n        \"Participated\": lambda x: x.value_counts()[\"yes\"],\n        \"Passed\": lambda x: sum(x == \"yes\"),\n        \"Employed\": lambda x: sum(x),\n        \"Grade\": lambda x: sum(x) / len(x),\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Registering Matplotlib Date Converters with pandas (ipython)\nDESCRIPTION: This snippet shows how to manually register pandas' converters with matplotlib for proper plotting of date-like pandas objects. It sets the pd.options.plotting.matplotlib.register_converters option to True, thus ensuring that matplotlib picks up pandas formatters when plotting. Dependencies: pandas must be imported. Key parameter is the plotting option. Input is setting a pandas plotting option; the effect appears when plotting with matplotlib, particularly for direct matplotlib plot calls. This change is relevant only when using matplotlib directly (not via DataFrame.plot/Series.plot), in recent versions of pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_30\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: import pandas as pd\nIn [2]: pd.options.plotting.matplotlib.register_converters = True\n\n```\n\n----------------------------------------\n\nTITLE: Serializing DataFrame and Series to JSON with Orientation Options - pandas - Python\nDESCRIPTION: Demonstrates the use of the to_json method with various orient options (\"columns\", \"index\", \"records\", \"values\", \"split\") for both DataFrames and Series. These options control the structure of the resulting JSON data. The code also notes which orientations apply to DataFrame vs Series. Inputs: pandas DataFrame/Series. Outputs: JSON string with specified orientation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\ndfjo.to_json(orient=\"columns\")\n# Not available for Series\n```\n\nLANGUAGE: python\nCODE:\n```\ndfjo.to_json(orient=\"index\")\nsjo.to_json(orient=\"index\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndfjo.to_json(orient=\"records\")\nsjo.to_json(orient=\"records\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndfjo.to_json(orient=\"values\")\n# Not available for Series\n```\n\nLANGUAGE: python\nCODE:\n```\ndfjo.to_json(orient=\"split\")\nsjo.to_json(orient=\"split\")\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in Series.aggregate for multiple argument passing\nDESCRIPTION: Addresses a problem where Series.aggregate was attempting to pass args and kwargs multiple times to the user-supplied func in certain cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nSeries.aggregate(func, *args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Column Selection and Renaming - Stata\nDESCRIPTION: This snippet illustrates how to keep, drop, and rename columns in Stata using 'keep', 'drop', and 'rename'. It assumes the presence of the columns referenced (e.g., 'sex', 'total_bill', 'tip'). No dependencies. Input: Data with given columns. Output: Modified data set with columns selected, removed, or renamed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_13\n\nLANGUAGE: stata\nCODE:\n```\nkeep sex total_bill tip\\n\\ndrop sex\\n\\nrename total_bill total_bill_2\n```\n\n----------------------------------------\n\nTITLE: Searching for Function Usages with Git Grep - shell\nDESCRIPTION: Shows a shell command using git grep to locate calls to a particular function (by name) across the repository. This aids contributors in finding relevant test locations or understanding function usage. It requires git to be installed and a local clone of the repository. The function_name parameter should be replaced with the actual function being searched for. Output is a list of occurrences in code with their locations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ngit grep \"function_name(\"\n```\n\n----------------------------------------\n\nTITLE: Plotting Group Means with Asymmetrical Error Bars using pandas and Matplotlib in Python\nDESCRIPTION: This snippet extends the previous example to calculate and display asymmetrical error bars representing the min and max range for grouped data. Dependencies include pandas, matplotlib, and numpy. Key variables are DataFrames of group means, minima, and maxima, and errors is a two-element list defining lower and upper bounds. Input is grouped DataFrames; output is a bar plot where each bar's error is not necessarily symmetrical. Limitations: error arrays must be properly shaped as required by matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nmins = gp3.min()\nmaxs = gp3.max()\n\n# errors should be positive, and defined in the order of lower, upper\nerrors = [[means[c] - mins[c], maxs[c] - means[c]] for c in df3.columns]\n\n# Plot\nfig, ax = plt.subplots()\nmeans.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying a Wide DataFrame in Pandas (Python)\nDESCRIPTION: Constructs a DataFrame with 3 rows and 12 columns of random values, illustrating how wide DataFrames are displayed across multiple rows by default. Dependencies: pandas, NumPy. Input: output of np.random.randn(3, 12).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame(np.random.randn(3, 12))\n```\n\n----------------------------------------\n\nTITLE: Comparing DataFrames While Keeping All Values Using pandas in Python\nDESCRIPTION: This snippet presents using pandas DataFrame.compare() with both keep_shape=True and keep_equal=True, causing the comparison to retain all original rows and columns and also to show all original values, even if they are equal. This results in a full DataFrame showing both changed and unchanged cell values for each input DataFrame. Required: pandas library. Inputs are the DataFrames to compare. Output is a comprehensive DataFrame including all corresponding values from both inputs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndf.compare(df2, keep_shape=True, keep_equal=True)\n```\n\n----------------------------------------\n\nTITLE: Concatenating DataFrames with Sparse Values (Python)\nDESCRIPTION: Demonstrates that when concatenating DataFrames with sparse column values, the result is now a DataFrame or Series with sparse values, as opposed to the previously returned SparseDataFrame. Requires Pandas and a DataFrame with pd.arrays.SparseArray. Shows updated behavior for type inference on concatenation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_6\n\nLANGUAGE: ipython\nCODE:\n```\ndf = pd.DataFrame({\"A\": pd.arrays.SparseArray([0, 1])})\n```\n\n----------------------------------------\n\nTITLE: Fixing DatetimeIndex Localization with Integer Data\nDESCRIPTION: Corrects the localization of integer data when constructing a DatetimeIndex with a timezone.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\nDatetimeIndex([1, 2, 3], tz='UTC')\n```\n\n----------------------------------------\n\nTITLE: Using a Function to Filter Columns in Excel Read\nDESCRIPTION: Shows how to use a callable function with the usecols parameter to select columns based on a condition applied to their names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_137\n\nLANGUAGE: python\nCODE:\n```\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=lambda x: x.isalpha())\n```\n\n----------------------------------------\n\nTITLE: Using IntervalIndex.overlaps to Replicate Previous Overlap Behavior - Pandas - Python\nDESCRIPTION: Shows how to create a boolean indexer using the overlaps method to select overlapping intervals, providing a way to regain the previous behavior where overlapping intervals are matched. Requires pandas and a Series 's' indexed by IntervalIndex. Returns a boolean mask and the filtered rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nidxr = s.index.overlaps(pd.Interval(2, 3))\\nidxr\\ns[idxr]\\ns.loc[idxr]\n```\n\n----------------------------------------\n\nTITLE: Describing All Columns in a DataFrame - Pandas Python\nDESCRIPTION: Demonstrates how to use the DataFrame.describe method in pandas with the 'include=\"all\"' argument to produce a summary of all columns, including non-numeric types. Requires pandas to be imported as pd and an existing DataFrame df. The key parameter 'include' determines which columns are included in the output summary. Output is a descriptive statistics table for all columns in the DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndf.describe(include='all')\n```\n\n----------------------------------------\n\nTITLE: Indexing DataFrames with .loc and Missing Indices in Pandas (ipython)\nDESCRIPTION: These code cells demonstrate changes to the '.loc' accessor when selecting rows by a list of indices, including cases where some or all are missing. Prior to 0.15.0, the results differed between row-only and row/column selection; now, both return a reindexed frame with NaN for missing values. Dependencies: pandas, DataFrame 'df'. Inputs: index list. Output: DataFrame with reindexed rows. Clarifies consistent selection behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_20\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: df.loc[[1, 3]]\nOut[3]:\n     0\n1    a\n3  NaN\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: df.loc[[1, 3], :]\nOut[4]:\n     0\n1    a\n3  NaN\n```\n\n----------------------------------------\n\nTITLE: Adding Timedelta to Datetime in Pandas\nDESCRIPTION: Example showing how to add a timedelta object to a datetime series. This is a basic operation used when working with time-based data to perform time arithmetic.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\ndatetime.timedelta(minutes=5) + s\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Consistency: DataFrame.any and Series.any with Category Dtype (Python)\nDESCRIPTION: This snippet shows how reductions like any interact with DataFrames and Series containing categorical datatypes. It constructs a Series of type 'category', converts it to a DataFrame, and sets context for examining prior and updated reduction behavior. Dependencies: pandas. Key parameter: dtype (\"category\"). Output for DataFrame.any may now be empty if the reduction does not apply to the category dtype, matching Series logic.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([0, 1], dtype=\"category\", name=\"A\")\ndf = ser.to_frame()\n```\n\n----------------------------------------\n\nTITLE: Handling Duplicate Keys in DataFrame and Series Concatenation (Python)\nDESCRIPTION: The concat function now allows concatenation of DataFrame and Series with duplicate keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\npd.concat([df, series])\n```\n\n----------------------------------------\n\nTITLE: Defining and Applying Numerical Integration Functions - Python\nDESCRIPTION: Defines two functions: 'f(x)' for a quadratic calculation, and 'integrate_f(a, b, N)' for numerical integration using a for-loop. Demonstrates how to apply 'integrate_f' row-wise to a DataFrame using DataFrame.apply with a lambda function. Dependencies include pandas and numpy, with expected inputs being numeric columns. Outputs are the computed integral per row, but performance is limited due to Python loops.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    return x * (x - 1)\n\ndef integrate_f(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f(a + i * dx)\n    return s * dx\n```\n\n----------------------------------------\n\nTITLE: Timedelta Scalar Arithmetic - pandas - Python\nDESCRIPTION: Illustrates arithmetic operations with Timedelta scalars constructed via pd.offsets.Day and pd.offsets.Second, and combining with a string duration. Requires pandas offsets support. The sum of multiple Timedelta objects is exemplified.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npd.Timedelta(pd.offsets.Day(2)) + pd.Timedelta(pd.offsets.Second(2)) + pd.Timedelta(\n    \"00:00:00.000123\"\n)\n```\n\n----------------------------------------\n\nTITLE: Series Plot Methods\nDESCRIPTION: Series plotting methods including area, bar, barh, box, density, hist, kde, line, and pie plots\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/series.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nSeries.plot.area\nSeries.plot.bar\nSeries.plot.barh\nSeries.plot.box\nSeries.plot.density\nSeries.plot.hist\nSeries.plot.kde\nSeries.plot.line\nSeries.plot.pie\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in DataFrame.convert_dtypes for byte string conversion\nDESCRIPTION: Addresses a regression where DataFrame.convert_dtypes incorrectly converts byte strings to strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.convert_dtypes()\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame Rows Conditionally to Mimic SQL DELETE in pandas - Python\nDESCRIPTION: Filters the 'tips' DataFrame to retain only those rows where 'tip' is less than or equal to 9, mimicking the effect of SQL DELETE. The filtered DataFrame is assigned back to the variable 'tips', replacing its contents. Requires pandas and a DataFrame named 'tips' with a 'tip' column. Returns a DataFrame with only qualifying rows; original data is overwritten.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntips = tips.loc[tips[\"tip\"] <= 9]\n```\n\n----------------------------------------\n\nTITLE: Resample and Sum of Series with NA bins in pandas 0.22.0 - Python\nDESCRIPTION: In pandas 0.22.0, resampling and summing over all-NA bins now returns 0 instead of NaN for those bins. Demonstrates change in output for time-based groupings. Dependencies: pandas as pd, numpy as np.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nIn [11]: s = pd.Series([1, 1, np.nan, np.nan],\n   ....:               index=pd.date_range(\"2017\", periods=4))\n\nIn [12]: s.resample(\"2d\").sum()\nOut[12]:\n2017-01-01    2.0\n2017-01-03    0.0\nFreq: 2D, Length: 2, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Resampling Grouped Data in Pandas\nDESCRIPTION: Shows how to use the new syntax for resampling grouped data in Pandas, which simplifies the process compared to previous versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"date\": pd.date_range(start=\"2016-01-01\", periods=4, freq=\"W\"),\n        \"group\": [1, 1, 2, 2],\n        \"val\": [5, 6, 7, 8],\n    }\n).set_index(\"date\")\n\ndf\n\ndf.groupby(\"group\").resample(\"1D\").ffill()\n```\n\n----------------------------------------\n\nTITLE: Constructing TimedeltaIndex from Multiple Input Types - Python\nDESCRIPTION: This code demonstrates creating a TimedeltaIndex from a mix of string inputs, NumPy timedelta64, and Python's datetime.timedelta. The TimedeltaIndex can be used as an index in DataFrames or Series. Requires pandas, NumPy, and optionally datetime module. Inputs are a list of time specifications; output is a TimedeltaIndex object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.TimedeltaIndex(['1 days', '1 days, 00:00:05',\n                  np.timedelta64(2, 'D'),\n                  datetime.timedelta(days=2, seconds=2)])\n```\n\n----------------------------------------\n\nTITLE: Extracting the Nth Word from a String Using SCAN in SAS\nDESCRIPTION: Applies SAS's 'scan' function to retrieve first and last names from a string column in a dataset. Values are input using 'datalines2'. Outputs new columns for first and last name, parsed by word order. Input: strings; outputs: separated name columns for each row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_16\n\nLANGUAGE: SAS\nCODE:\n```\ndata firstlast;\\ninput String $60.;\\nFirst_Name = scan(string, 1);\\nLast_Name = scan(string, -1);\\ndatalines2;\\nJohn Smith;\\nJane Cook;\\n;;;\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Data Binning with cut and IntervalIndex\nDESCRIPTION: Shows how pandas cut function returns a Categorical with an IntervalIndex as its categories, and demonstrates using these intervals for binning additional data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nc = pd.cut(range(4), bins=2)\nc\nc.categories\n\npd.cut([0, 3, 5, 1], bins=c.categories)\n```\n\n----------------------------------------\n\nTITLE: Custom Dialect Configuration for CSV Parsing\nDESCRIPTION: Shows how to handle unenclosed quotes in CSV data by creating a custom dialect that disables quote handling\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nimport csv\n\ndia = csv.excel()\ndia.quoting = csv.QUOTE_NONE\npd.read_csv(StringIO(data), dialect=dia)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating DataFrame.mean Consistency with Mixed Dtypes and numeric_only=None in Pandas (Python)\nDESCRIPTION: This snippet creates a DataFrame with an integer and string column and demonstrates previous versus new behavior for DataFrame.mean when numeric_only=None. It shows how, in the updated version, non-numeric columns are properly excluded, ensuring mean calculation only considers relevant columns. Dependencies: pandas. Key parameters: numeric_only (default None). Input is a mixed-dtype DataFrame; output is a Series or DataFrame containing only mean values for numeric columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [0, 1, 2], \"B\": [\"a\", \"b\", \"c\"]}, dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Aggregating Data by Factors with dcast - reshape2 - R\nDESCRIPTION: This R snippet aggregates a data.frame ('df') by 'Animal' and 'FeedType' using dcast to create a wide-format summary, filling missing data with NaN. It also demonstrates a base R alternative using tapply. Input must contain categorical and numeric columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_22\n\nLANGUAGE: R\nCODE:\n```\ndf <- data.frame(\n  Animal = c('Animal1', 'Animal2', 'Animal3', 'Animal2', 'Animal1',\n             'Animal2', 'Animal3'),\n  FeedType = c('A', 'B', 'A', 'A', 'B', 'B', 'A'),\n  Amount = c(10, 7, 4, 2, 5, 6, 2)\n)\n\ndcast(df, Animal ~ FeedType, sum, fill=NaN)\n# Alternative method using base R\nwith(df, tapply(Amount, list(Animal, FeedType), sum))\n```\n\n----------------------------------------\n\nTITLE: Flexible Comparison Methods for pandas Series with Non-matching Indexes - Python\nDESCRIPTION: Demonstrates use of flexible methods (eq, ge) that align indexes for elementwise comparison across Series with different indexes. Requires pandas. Inputs: two Series with possibly non-overlapping indexes. Outputs: boolean Series aligned on all unique indexes from both inputs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\ns2 = pd.Series([2, 2, 2], index=[\"b\", \"c\", \"d\"])\ns1.eq(s2)\ns1.ge(s2)\n```\n\n----------------------------------------\n\nTITLE: New Behavior: DataFrame.any with Category Dtype (ipython)\nDESCRIPTION: Illustrates that in the new version, DataFrame.any does not return a value for the categorical column, aligning with Series.any semantics for unsupported types. Output is an empty Series of type bool.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_14\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: df.any()\nOut[5]: Series([], dtype: bool)\n```\n\n----------------------------------------\n\nTITLE: Taking Elements with Index.take Method in Pandas (Python)\nDESCRIPTION: Demonstrates usage of the pd.Index.take method, including the new behavior concerning allow_fill and fill_value. Requires Pandas library. Shows both default take and call with fill_value=True; index positions are specified to retrieve elements from a float-indexed Index. Input consists of positions to take; output is a new Index or filled values depending on parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([1.0, 2.0, 3.0, 4.0], dtype=\"float\")\n\n# default, allow_fill=True, fill_value=None\nidx.take([2, -1])\nidx.take([2, -1], fill_value=True)\n```\n\n----------------------------------------\n\nTITLE: Categorical.argsort Previous Behavior - Pandas - Python\nDESCRIPTION: Documents the prior ordering of missing values when using Categorical.argsort, which previously sorted missing values to the start. Shows instantiation and use of argsort on a categorical object with missing values. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: cat = pd.Categorical(['b', None, 'a'], categories=['a', 'b'], ordered=True)\\n\\nIn [3]: cat.argsort()\\nOut[3]: array([1, 2, 0])\\n\\nIn [4]: cat[cat.argsort()]\\nOut[4]:\\n[NaN, a, b]\\ncategories (2, object): [a < b]\n```\n\n----------------------------------------\n\nTITLE: Concatenating with Keys for Hierarchical Indexing using pandas.concat in Python\nDESCRIPTION: This snippet demonstrates advanced concatenation by adding keys to each source DataFrame ('PM25' and 'NO2'), resulting in a MultiIndex on rows. The head of the resulting DataFrame is shown. This approach helps to distinguish different sources explicitly within the combined DataFrame. Dependencies: pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nair_quality_ = pd.concat([air_quality_pm25, air_quality_no2], keys=[\"PM25\", \"NO2\"])\nair_quality_.head()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating chained indexing bug fix in Pandas 0.13.1\nDESCRIPTION: Shows the recommended way to assign values to a DataFrame column to avoid a segmentation fault that could occur with numpy < 1.8 and chained assignment on string-like arrays.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": np.array([\"foo\", \"bar\", \"bah\", \"foo\", \"bar\"])})\ndf.loc[0, \"A\"] = np.nan\ndf\n```\n\n----------------------------------------\n\nTITLE: Filtering Columns with usecols Parameter Using a Callable\nDESCRIPTION: Demonstrates how to use a callable function with the usecols parameter to dynamically select columns based on conditions applied to column names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(StringIO(data), usecols=lambda x: x.upper() in [\"A\", \"C\"])\n```\n\n----------------------------------------\n\nTITLE: Categorical.argsort and Sorting Output (New Behavior) - Pandas - Python\nDESCRIPTION: Shows new behavior for Categorical.argsort where missing values are sorted to the end. The result demonstrates the argsort output and extracted sorted values. Requires pandas and a Categorical object with missing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ncat.argsort()\\ncat[cat.argsort()]\n```\n\n----------------------------------------\n\nTITLE: Ranking within Groups using pandas rank() - Python\nDESCRIPTION: Ranks entries in the 'tips' DataFrame within the 'day' group, sorting by descending 'total_bill' using the 'first' ranking method, then filters for the top 2 ranked items in each group. Requires pandas and an existing DataFrame named 'tips' with 'day' and 'total_bill' columns. Returns a new DataFrame sorted by 'day' and rank; inputs are filtered and annotated by rank, with the key constraint that ties are broken by order of appearance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n(\n    tips.assign(\n        rnk=tips.groupby([\"day\"])[\"total_bill\"].rank(\n            method=\"first\", ascending=False\n        )\n    )\n    .query(\"rnk < 3\")\n    .sort_values([\"day\", \"rnk\"])\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting and Slicing with TimedeltaIndex in pandas (Python)\nDESCRIPTION: Demonstrates time-based selections using string representations and slices with a Series indexed by TimedeltaIndex. Requires pandas. Shows selection by interval, by exact match (string-parsable), and by Timedelta object. String-based intervals and partial strings are automatically parsed/coerced, allowing intuitive access.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ns[\"1 day\":\"2 day\"]\n```\n\nLANGUAGE: python\nCODE:\n```\ns[\"1 day 01:00:00\"]\n```\n\nLANGUAGE: python\nCODE:\n```\ns[pd.Timedelta(\"1 day 1h\")]\n```\n\nLANGUAGE: python\nCODE:\n```\ns[\"1 day\":\"1 day 5 hours\"]\n```\n\n----------------------------------------\n\nTITLE: Restoring NaN for Unobserved Groups with min_count - Python\nDESCRIPTION: Demonstrates how to revert to returning NaN for categories with no observations in groupby-sum operations by setting min_count=1. Useful for codebases expecting the legacy behavior. Dependencies: pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npd.Series([1, 2]).groupby(grouper, observed=False).sum(min_count=1)\n```\n\n----------------------------------------\n\nTITLE: Setting Floating Point Display Precision in pandas (Python)\nDESCRIPTION: Changing the 'display.precision' option in pandas is demonstrated here, which affects the number of decimal places displayed for floating point numbers when DataFrames are printed. The key parameter is the integer passed to 'display.precision', representing the desired precision. This influences both regular and scientific formats, aligning with numpy's printing. The DataFrame constructor is shown with a single float value. Dependencies: pandas. Input is a float or list of floats; output is a DataFrame formatted to two decimal places.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\npd.set_option('display.precision', 2)\n\npd.DataFrame({'x': [123.456789]})\n```\n\n----------------------------------------\n\nTITLE: Assigning None to Numeric and Object Series in Pandas (ipython)\nDESCRIPTION: These ipython cells show assignment of None to a numeric Series and an object Series, respectively. In numeric containers, assigning None now introduces NaN and preserves dtype, while in object containers None is preserved. Dependencies: pandas. Inputs: Series, scalar assignment. Output: Series with replaced values. Ensures correct missing value semantics for recent Pandas versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_23\n\nLANGUAGE: ipython\nCODE:\n```\ns = pd.Series([1., 2., 3.])\ns.loc[0] = None\ns\n```\n\nLANGUAGE: ipython\nCODE:\n```\ns = pd.Series([\"a\", \"b\", \"c\"])\ns.loc[0] = None\ns\n```\n\n----------------------------------------\n\nTITLE: Inspecting Datetime Dtype Objects in pandas (Python)\nDESCRIPTION: These short snippets demonstrate how to check the dtype of a DataFrame column containing timezone-aware datetimes and how to reveal the underlying dtype class. It emphasizes the new custom dtype objects for timezones in pandas, aligning them closely with numpy conventions. Input is a pandas Series; output is its dtype and repr type. Useful for debugging and validating the internal type system in pandas 0.17.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf[\"B\"].dtype\ntype(df[\"B\"].dtype)\n```\n\n----------------------------------------\n\nTITLE: Index dropna() Method for Excluding Missing Values - Pandas - Python\nDESCRIPTION: Demonstrates usage of Index.dropna() to remove missing values (NaN) from a pandas Index. The first example shows applying dropna to a single-level Index, while in MultiIndex, dropna removes rows where any or all levels are missing, depending on 'how' parameter. Requires pandas and optionally numpy for constructing Indexes; parameters include the missing value policy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nidx = pd.Index([1, 2, np.nan, 4])\nidx.dropna()\n```\n\nLANGUAGE: Python\nCODE:\n```\nmidx = pd.MultiIndex.from_arrays([[1, 2, np.nan, 4], [1, 2, np.nan, np.nan]])\nmidx\nmidx.dropna()\nmidx.dropna(how=\"all\")\n```\n\n----------------------------------------\n\nTITLE: Per-Column Aggregations Using Dictionary with pandas - Python\nDESCRIPTION: This snippet demonstrates passing a dictionary to .agg(), enabling specific aggregation functions per column. The output DataFrame has one column per unique function, with NaNs in unused function/column positions. Requires pandas, a DataFrame, and outputs a DataFrame with mixed aggregations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf.agg({'A': ['sum', 'min'], 'B': ['min', 'max']})\n```\n\n----------------------------------------\n\nTITLE: Combining TimedeltaIndex and DatetimeIndex with Arithmetic in pandas (Python)\nDESCRIPTION: Illustrates arithmetic operations between TimedeltaIndex and DatetimeIndex, demonstrating NaT-preserving element-wise addition and subtraction. Requires pandas. Constructs TimedeltaIndex (with a NaT) and DatetimeIndex, then shows to_list representations and combination operations. The operations preserve missing values (NaT) and return lists of timestamps.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ntdi = pd.TimedeltaIndex([\"1 days\", pd.NaT, \"2 days\"])\ntdi.to_list()\ndti = pd.date_range(\"20130101\", periods=3)\ndti.to_list()\n(dti + tdi).to_list()\n(dti - tdi).to_list()\n```\n\n----------------------------------------\n\nTITLE: Casting Timedelta Series to Days Precision using astype (Python)\nDESCRIPTION: This snippet converts a timedelta Series to days using the astype method with 'timedelta64[D]', truncating/rounding to nearest day. Requires pandas and NumPy. Input is a Series 'td'; output is a float64 Series showing complete days, with NaT as NaN. Useful for downcasting timedeltas for coarser time analysis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntd.astype('timedelta64[D]')\n```\n\n----------------------------------------\n\nTITLE: Multi-line Assignment in pandas eval - Python\nDESCRIPTION: Uses a multi-line string expression for multiple assignments in df.eval. Each assignment occurs sequentially, and only assignment operations are supported. All changes can be applied inplace when specified. Requires a DataFrame (df) and pandas. Demonstrates successive dependent column creation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf\ndf.eval(\"\"\"\ne = d + a\nf = e - 22\ng = f / 2.0\"\"\", inplace=True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Comparisons of Index and Numpy Arrays - Python\nDESCRIPTION: Demonstrates equality comparisons between pandas.Index objects of different lengths, highlighting old and new behaviors. Shows how exceptions are raised when lengths differ. Also illustrates corresponding numpy array comparison, broadcasting behavior, and differences in return type (exception vs. False or result array). Requires pandas and numpy installed and imported.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\npd.Index([1, 2, 3]) == pd.Index([1, 4, 5])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index([1, 2, 3]) == pd.Index([2])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index([1, 2, 3]) == pd.Index([1, 2])\n```\n\nLANGUAGE: python\nCODE:\n```\nnp.array([1, 2, 3]) == np.array([1])\n```\n\nLANGUAGE: python\nCODE:\n```\nnp.array([1, 2, 3]) == np.array([1, 2])\n```\n\n----------------------------------------\n\nTITLE: Converting Series to NumPy Arrays in Pandas Python\nDESCRIPTION: This snippet demonstrates conversion of a Series to a NumPy ndarray using both the .to_numpy() pandas method and numpy.asarray. This is crucial for downstream interoperability when NumPy operations are required, and behavior may differ if the Series uses extension types. Both code lines require pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ns.to_numpy()\nnp.asarray(s)\n```\n\n----------------------------------------\n\nTITLE: Using Backend Module Directly for Plotting in pandas Python\nDESCRIPTION: This code sample shows direct invocation of a backend's plot method with a pandas Series, bypassing pandas' plot method entirely. Dependencies: pandas and the selected backend module must be imported. Inputs: pandas Series; output: plot rendered by the third-party backend. Limitation: must match the backend's function signature and capabilities.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\n>>> import backend.module\n>>> backend.module.plot(pd.Series([1, 2, 3]))\n```\n\n----------------------------------------\n\nTITLE: Row and Column Indexing and Selection - Pandas - Python\nDESCRIPTION: Explores basic row and column access patterns including label and integer-based selection using loc and iloc. Returns either Series or DataFrame depending on selection type. Requires a DataFrame named 'df' and pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[\"b\"]\ndf.iloc[2]\n```\n\n----------------------------------------\n\nTITLE: Constructing MultiIndex from Categorical Arrays and DataFrames (ipython, Python)\nDESCRIPTION: Shows that MultiIndex construction and groupby/set_index now preserve categorical dtypes for index levels. Demonstrates construction with Categorical, level inspection, groupby and set_index operations, and resulting dtypes. Requires pandas; input is categorical arrays, levels, and DataFrame; outputs are MultiIndex, grouped DataFrame, and index level details.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ncat = pd.Categorical([\"a\", \"b\"], categories=list(\"bac\"))\nlvl1 = [\"foo\", \"bar\"]\nmidx = pd.MultiIndex.from_arrays([cat, lvl1])\nmidx\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: midx.levels[0]\nOut[4]: Index(['b', 'a', 'c'], dtype='object')\n\nIn [5]: midx.get_level_values[0]\nOut[5]: Index(['a', 'b'], dtype='object')\n```\n\nLANGUAGE: python\nCODE:\n```\nmidx.levels[0]\nmidx.get_level_values(0)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [0, 1], \"B\": [10, 11], \"C\": cat})\ndf_grouped = df.groupby(by=[\"A\", \"C\"], observed=False).first()\ndf_set_idx = df.set_index([\"A\", \"C\"])\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [11]: df_grouped.index.levels[1]\nOut[11]: Index(['b', 'a', 'c'], dtype='object', name='C')\nIn [12]: df_grouped.reset_index().dtypes\nOut[12]:\nA      int64\nC     object\nB    float64\ndtype: object\n\nIn [13]: df_set_idx.index.levels[1]\nOut[13]: Index(['b', 'a', 'c'], dtype='object', name='C')\nIn [14]: df_set_idx.reset_index().dtypes\nOut[14]:\nA      int64\nC     object\nB      int64\ndtype: object\n```\n\nLANGUAGE: python\nCODE:\n```\ndf_grouped.index.levels[1]\ndf_grouped.reset_index().dtypes\n\ndf_set_idx.index.levels[1]\ndf_set_idx.reset_index().dtypes\n```\n\n----------------------------------------\n\nTITLE: Updating DataFrame Construction from Multiple DataFrames in Python\nDESCRIPTION: API change in DataFrame constructor, no longer accepting a list of DataFrame objects due to changes in NumPy's handling of DataFrame objects as 2D.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n# DataFrame([df1, df2, df3])  # No longer valid\nDataFrame(np.array([df1, df2, df3]))  # Use numpy array instead\n```\n\n----------------------------------------\n\nTITLE: String Concatenation in pandas\nDESCRIPTION: Shows updates to str.cat() method with improved NaN handling and error messages.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.Series(['a', 'b', np.nan, 'c']).str.cat(sep=' ')\npd.Series(['a', 'b', np.nan, 'c']).str.cat(sep=' ', na_rep='?')\n```\n\n----------------------------------------\n\nTITLE: Cleanup: Removing Test Parquet File using os (Python)\nDESCRIPTION: Removes the file 'test.parquet' after Parquet index examples for cleanup. Utilizes os.remove from Python's standard library and will fail if the file does not exist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_214\n\nLANGUAGE: python\nCODE:\n```\nos.remove(\"test.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Operating on Nullable Integer Series - pandas (Python)\nDESCRIPTION: Shows standard arithmetic, comparison, indexing, and operations with other dtypes on a Series using the Int64 extension dtype with missing values. Requires pandas 0.24.0+ and NumPy. Examples include addition, type coercion, and subset selection using iloc. Inputs are Series with nullable integer values; outputs propagate missing values appropriately. Mixing with other extension types and coercing to float are illustrated. Be aware of type-based operation limitations and potential NA propagation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# arithmetic\ns + 1\n\n# comparison\ns == 1\n\n# indexing\ns.iloc[1:3]\n\n# operate with other dtypes\ns + s.iloc[1:3].astype('Int8')\n\n# coerce when needed\ns + 0.01\n```\n\n----------------------------------------\n\nTITLE: Using GroupBy.pipe for Chained Operations in Pandas\nDESCRIPTION: Demonstrates using the pipe method on GroupBy objects to compose functions in a clean, readable syntax. This example calculates prices (revenue/quantity) per store and product.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nn = 1000\ndf = pd.DataFrame({'Store': np.random.choice(['Store_1', 'Store_2'], n),\n                 'Product': np.random.choice(['Product_1',\n                                          'Product_2',\n                                          'Product_3'\n                                          ], n),\n                 'Revenue': (np.random.random(n) * 50 + 10).round(2),\n                 'Quantity': np.random.randint(1, 10, size=n)})\ndf.head(2)\n```\n\n----------------------------------------\n\nTITLE: Categorical.min Returns Value Instead of np.nan by Default (Python, pandas 1.0.0)\nDESCRIPTION: Demonstrates that calling Categorical.min on an ordered Categorical containing np.nan now returns the minimum non-missing value by default (skipna=True). Requires pandas as pd and numpy as np.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\npd.Categorical([1, 2, np.nan], ordered=True).min()\n```\n\n----------------------------------------\n\nTITLE: Boxplot by Stratified Quartiles Using Pandas and Matplotlib - Python\nDESCRIPTION: Creates a boxplot grouped by quartiles of a stratifying variable. Data is generated with a continuous stratifying variable and a numeric target ('price'); quartiles are assigned using pd.qcut, then a boxplot is drawn using DataFrame.boxplot, grouping by quartile bins. Dependencies: pandas, numpy, matplotlib (indirectly for plotting). Input: DataFrame with numeric columns; output: quartile boxplot. Useful for visualizing distribution differences across variable strata.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"stratifying_var\": np.random.uniform(0, 100, 20),\n        \"price\": np.random.normal(100, 5, 20),\n    }\n)\n\ndf[\"quartiles\"] = pd.qcut(\n    df[\"stratifying_var\"], 4, labels=[\"0-25%\", \"25-50%\", \"50-75%\", \"75-100%\"]\n)\n\n@savefig quartile_boxplot.png\ndf.boxplot(column=\"price\", by=\"quartiles\")\n```\n\n----------------------------------------\n\nTITLE: Commutative Index Union for Incompatible Types (Python)\nDESCRIPTION: Presents the new behavior when uniting Index objects of incompatible dtypes, which now produces a base Index of dtype object instead of raising an error or type coercion. Requires Pandas; demonstrates union of a PeriodIndex and Int64Index, as well as union with an empty Index of object dtype. Returns a generic Index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npd.period_range('19910905', periods=2).union(pd.Int64Index([1, 2, 3]))\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index([], dtype=object).union(pd.Index([1, 2, 3]))\n```\n\n----------------------------------------\n\nTITLE: GroupBy Sum with Observed=True and Dropna=False - pandas - IPython\nDESCRIPTION: This snippet demonstrates correct groupby behavior in pandas 1.5.0 when using observed=True and dropna=False: NA values are not dropped from results. Dependencies: pandas (pd) and the DataFrame 'df' defined previously. It groups by 'x', summing 'y'. Input: DataFrame 'df', groupby parameters; Output: grouped DataFrame including NA rows. Limitation: This behavior is labeled correct for 1.5.0 but may regress in 1.5.1.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.1.rst#_snippet_1\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: # Correct behavior, NA values are not dropped\n        df.groupby(\"x\", observed=True, dropna=False).sum()\nOut[3]:\n     y\nx\n1    3\nNaN  4\n```\n\n----------------------------------------\n\nTITLE: Using doc Decorator for Docstring Sharing in Python\nDESCRIPTION: Presents a pattern for sharing and customizing docstrings across class hierarchies using the @doc decorator. Assumes doc is imported or defined elsewhere. Shows a Parent method with a placeholder and two child classes illustrating substitution and inheritance in docstring documentation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nclass Parent:\n    @doc(klass=\"Parent\")\n    def my_function(self):\n        \"\"\"Apply my function to {klass}.\"\"\"\n        ...\n\n\nclass ChildA(Parent):\n```\n\n----------------------------------------\n\nTITLE: Finding the Longest String in a Series\nDESCRIPTION: Shows how to find the row with the longest string value in a column by combining str.len() with idxmax() and loc-based selection to identify and retrieve the passenger with the longest name.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/10_text_data.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Name\"].str.len()\n```\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Name\"].str.len().idxmax()\n```\n\nLANGUAGE: python\nCODE:\n```\ntitanic.loc[titanic[\"Name\"].str.len().idxmax(), \"Name\"]\n```\n\n----------------------------------------\n\nTITLE: Constructing DataFrame with string Dtype (Python)\nDESCRIPTION: Resolved an issue where constructing a DataFrame with dtype='string' would fail.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_62\n\nLANGUAGE: Python\nCODE:\n```\npd.DataFrame({'A': ['a', 'b']}, dtype='string')\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior of GroupBy on Categoricals with Missing Categories\nDESCRIPTION: This code shows the previous behavior where groupby would raise a ValueError when using sort=False with categorical data that had categories not present in the filtered data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: df[df.chromosomes != '1'].groupby('chromosomes', observed=False, sort=False).sum()\n---------------------------------------------------------------------------\nValueError: items in new_categories are not the same as in old categories\n```\n\n----------------------------------------\n\nTITLE: Discovering Type Inspection Functions via pandas.api.types - Python\nDESCRIPTION: Shows how to explore the available type introspection functions now exposed under pandas.api.types. The code uses dir() to list all public functions and pprint to print them nicely. Dependencies: pandas and pprint modules; this is useful for developers seeking to use the official public API for type checking.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nimport pprint\nfrom pandas.api import types\n\nfuncs = [f for f in dir(types) if not f.startswith(\"_\")]\npprint.pprint(funcs)\n```\n\n----------------------------------------\n\nTITLE: Replacing with Regex Patterns in String Series (Python)\nDESCRIPTION: Shows .str.replace() with a regex pattern (using case-insensitive matching and regex=True) to flexibly swap matching substrings or patterns in a string Series. Demonstrates replacement of entire words or patterns. Requires pandas and a Series with text/NA values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ns3 = pd.Series(\n    [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", \"\", np.nan, \"CABA\", \"dog\", \"cat\"],\n    dtype=\"string\",\n)\ns3\ns3.str.replace(\"^.a|dog\", \"XX-XX \", case=False, regex=True)\n```\n\n----------------------------------------\n\nTITLE: Pie Chart with Pandas Series\nDESCRIPTION: This example shows how to create a pie chart from a pandas Series using Series.plot.pie(). The figsize parameter sets the dimensions of the figure to ensure a proper circle shape.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nseries = pd.Series(3 * np.random.rand(4), index=[\"a\", \"b\", \"c\", \"d\"], name=\"series\")\n\nseries.plot.pie(figsize=(6, 6));\n```\n\n----------------------------------------\n\nTITLE: Concatenating Series and DataFrame Objects in Python\nDESCRIPTION: Demonstrates how to concatenate a Series with a DataFrame, where the Series is transformed into a DataFrame with its name as the column name.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([\"X0\", \"X1\", \"X2\", \"X3\"], name=\"X\")\nresult = pd.concat([df1, s1], axis=1)\nresult\n```\n\n----------------------------------------\n\nTITLE: Storing Mixed-Dtype DataFrames in HDF5 - pandas - Python\nDESCRIPTION: Creates a mixed-dtype DataFrame with floats, strings, ints, bools, and datetimes, sets NaNs, and persists it in HDFStore. Demonstrates min_itemsize for string columns to control storage, and shows querying the table structure and data types. Requires pandas and numpy; input is a dict of columns, output includes stored DataFrame and table metadata.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_175\n\nLANGUAGE: python\nCODE:\n```\ndf_mixed = pd.DataFrame(\n    {\n        \"A\": np.random.randn(8),\n        \"B\": np.random.randn(8),\n        \"C\": np.array(np.random.randn(8), dtype=\"float32\"),\n        \"string\": \"string\",\n        \"int\": 1,\n        \"bool\": True,\n        \"datetime64\": pd.Timestamp(\"20010102\"),\n    },\n    index=list(range(8)),\n)\ndf_mixed.loc[df_mixed.index[3:5], [\"A\", \"B\", \"string\", \"datetime64\"]] = np.nan\n\nstore.append(\"df_mixed\", df_mixed, min_itemsize={\"values\": 50})\ndf_mixed1 = store.select(\"df_mixed\")\ndf_mixed1\ndf_mixed1.dtypes.value_counts()\n\n# we have provided a minimum string column size\nstore.root.df_mixed.table\n```\n\n----------------------------------------\n\nTITLE: Partitioned Parquet Writing using pandas (Python)\nDESCRIPTION: Writes a pandas DataFrame partitioned by unique values in column 'a' into a directory ('test/') as Parquet files using the 'pyarrow' engine and no compression. The 'partition_cols' parameter specifies columns for partitioning, and files are grouped into directories named after values in 'a'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_215\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": [0, 0, 1, 1], \"b\": [0, 1, 0, 1]})\ndf.to_parquet(path=\"test\", engine=\"pyarrow\", partition_cols=[\"a\"], compression=None)\n```\n\n----------------------------------------\n\nTITLE: Enumerate Groups with ngroup - pandas Python\nDESCRIPTION: Applies ngroup to assign a unique sequential ID to each group. Demonstrates how groups are indexed in groupby context, with support for reversed/ascending order. Inputs: DataFrame with group key column. Outputs: Series of group numbers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\ndfg = pd.DataFrame(list(\"aaabba\"), columns=[\"A\"])\ndfg\n```\n\nLANGUAGE: python\nCODE:\n```\ndfg.groupby(\"A\").ngroup()\n```\n\nLANGUAGE: python\nCODE:\n```\ndfg.groupby(\"A\").ngroup(ascending=False)\n```\n\n----------------------------------------\n\nTITLE: Parsing Datetimes with Mixed Time Zones (New Behavior and Warning) in Pandas (ipython)\nDESCRIPTION: This snippet demonstrates pandas issuing a FutureWarning when parsing mixed time zone datetimes without setting utc=True. It informs users about the change to require UTC normalization for consistency. Incoming data is a list of time-zone-aware strings, and output is an Index with object dtype, shown alongside the warning.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\n  In [9]: pd.to_datetime(data, utc=False)\n  FutureWarning:\n    In a future version of pandas, parsing datetimes with mixed time zones will raise\n    a warning unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour\n    and silence this warning. To create a `Series` with mixed offsets and `object` dtype,\n    please use `apply` and `datetime.datetime.strptime`.\n  Index([2020-01-01 00:00:00+06:00, 2020-01-01 00:00:00+01:00], dtype='object')\n```\n\n----------------------------------------\n\nTITLE: Finding Rows Closest to a Value Using argsort in Python\nDESCRIPTION: Selects rows with values closest to a target using absolute difference and argsort. This technique sorts rows based on how close a column's values are to a specified value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n)\ndf\naValue = 43.0\ndf.loc[(df.CCC - aValue).abs().argsort()]\n```\n\n----------------------------------------\n\nTITLE: Creating and Summing Decimal Columns in Groups - pandas Python\nDESCRIPTION: Constructs a DataFrame with decimal.Decimal data in one column and computes the groupwise sum based on 'id', selecting only the 'dec_column'. Shows precision handling for decimal arithmetic in pandas. Dependencies: pandas, decimal. Inputs are lists of ids, integers, and Decimal values. Outputs: summed decimals per group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nfrom decimal import Decimal\n\ndf_dec = pd.DataFrame(\n    {\n        \"id\": [1, 2, 1, 2],\n        \"int_column\": [1, 2, 3, 4],\n        \"dec_column\": [\n            Decimal(\"0.50\"),\n            Decimal(\"0.15\"),\n            Decimal(\"0.25\"),\n            Decimal(\"0.40\"),\n        ],\n    }\n)\ndf_dec.groupby([\"id\"])[[\"dec_column\"]].sum()\n```\n\n----------------------------------------\n\nTITLE: Joining DataFrames on Key-Index Combination using pd.merge in pandas Python\nDESCRIPTION: Demonstrates merging DataFrames using a key column from the left DataFrame and the index from the right DataFrame using pd.merge with left_on, right_index, and how arguments. This allows flexible joins even when the join columns are not both in the columns. Requires pandas. Inputs: DataFrames, left_on column, right_index True; Output: merged DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.merge(\n    left, right, left_on=\"key\", right_index=True, how=\"left\", sort=False\n)\nresult\n```\n\n----------------------------------------\n\nTITLE: Random Number Plotting by Group - pandas Python\nDESCRIPTION: Initializes a reproducible random number generator and defines a DataFrame for further exploration with groupby plotting methods. Useful for demonstration and reproducibility in group-based plots. Requires numpy, pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(1234)\ndf = pd.DataFrame(np.random.randn(50, 2))\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame by Index Level and using Key Functions (pandas, Python)\nDESCRIPTION: Demonstrates DataFrame sorting by a single index level, with and without using a key function (e.g., lowercase transformation for string levels). The index is multi-level. Assumes s1 is a DataFrame with MultiIndex rows. Dependent on pandas. Input: DataFrame, Output: sorted DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.DataFrame({\"a\": [\"B\", \"a\", \"C\"], \"b\": [1, 2, 3], \"c\": [2, 3, 4]}).set_index(\n    list(\"ab\")\n)\ns1\n```\n\nLANGUAGE: python\nCODE:\n```\ns1.sort_index(level=\"a\")\ns1.sort_index(level=\"a\", key=lambda idx: idx.str.lower())\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for data manipulation functions\nDESCRIPTION: RestructuredText directive listing core Pandas data manipulation functions including melt, pivot, merge and related operations\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/general_functions.rst#_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n\n   melt\n   pivot\n   pivot_table\n   crosstab\n   cut\n   qcut\n   merge\n   merge_ordered\n   merge_asof\n   concat\n   get_dummies\n   from_dummies\n   factorize\n   unique\n   lreshape\n   wide_to_long\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading Compressed Pickle Files Using pandas - Python\nDESCRIPTION: Shows how pandas can read/write compressed pickled DataFrames and Series using various compression types, either inferred from the filename or specified. Demonstrates creation of a large DataFrame, writing and reading with gzip and bz2, and cleaning up files. Required dependencies: pandas, numpy, a writable filesystem. Outputs the first few rows from each loaded object; limitations depend on available compressors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': np.random.randn(1000),\n                   'B': 'foo',\n                   'C': pd.date_range('20130101', periods=1000, freq='s')})\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.to_pickle(\"data.pkl.compress\", compression=\"gzip\")\nrt = pd.read_pickle(\"data.pkl.compress\", compression=\"gzip\")\nrt.head()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.to_pickle(\"data.pkl.gz\")\nrt = pd.read_pickle(\"data.pkl.gz\")\nrt.head()\ndf[\"A\"].to_pickle(\"s1.pkl.bz2\")\nrt = pd.read_pickle(\"s1.pkl.bz2\")\nrt.head()\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.remove(\"data.pkl.compress\")\nos.remove(\"data.pkl.gz\")\nos.remove(\"s1.pkl.bz2\")\n```\n\n----------------------------------------\n\nTITLE: Converting a pandas Series with MultiIndex to SparseSeries and COO Format (Python)\nDESCRIPTION: Demonstrates transformation of a pandas Series with a MultiIndex into a SparseSeries and then conversion to a scipy.sparse coo_matrix using to_coo, specifying index levels for rows and columns. Dependencies: pandas, numpy, scipy.sparse. Inputs: A Series with MultiIndex, specifying row_levels and column_levels. Outputs: a tuple containing the COO matrix and arrays for row and column labels. Note: This highlights interoperability with the scipy sparse API and is useful for memory-efficient data storage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([3.0, np.nan, 1.0, 3.0, np.nan, np.nan])\ns.index = pd.MultiIndex.from_tuples([(1, 2, 'a', 0),\n                                     (1, 2, 'a', 1),\n                                     (1, 1, 'b', 0),\n                                     (1, 1, 'b', 1),\n                                     (2, 1, 'b', 0),\n                                     (2, 1, 'b', 1)],\n                                    names=['A', 'B', 'C', 'D'])\n\ns\n\n# SparseSeries\nss = s.to_sparse()\nss\n\nA, rows, columns = ss.to_coo(row_levels=['A', 'B'],\n                             column_levels=['C', 'D'],\n                             sort_labels=False)\n\nA\nA.todense()\nrows\ncolumns\n```\n\n----------------------------------------\n\nTITLE: Line Plotting with Custom Colormap using pandas DataFrame and Matplotlib in Python\nDESCRIPTION: This snippet illustrates creating a line plot from a DataFrame with multiple columns, using a named Matplotlib colormap ('cubehelix'). Required dependencies: numpy, pandas, matplotlib. The DataFrame is filled with cumulative sums of random numbers, and a specific colormap is applied via the colormap parameter. Input: DataFrame; output: multi-series line plot with distinctive colors. Limitation: colormap must be compatible with the amount of series/lines.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(123456)\ndf = pd.DataFrame(np.random.randn(1000, 10), index=ts.index)\ndf = df.cumsum()\n\nplt.figure();\ndf.plot(colormap=\"cubehelix\");\n```\n\n----------------------------------------\n\nTITLE: Renaming Both Index and Columns in DataFrame in pandas 1.0.0 (Python)\nDESCRIPTION: Demonstrates how to update both the row (index) and column labels of a DataFrame using keyword arguments in the rename method. This avoids ambiguity as enforced in pandas 1.0.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf.rename(index={0: 1}, columns={0: 2})\n```\n\n----------------------------------------\n\nTITLE: Improved Duplicate Column Handling in Pandas read_csv in Python\nDESCRIPTION: Shows the enhanced support for duplicate column names in the pandas read_csv function. Demonstrates creating a CSV data string with duplicate headers and parsing it using custom names. Highlights previous and new behaviors. Requires pandas and StringIO; input is CSV text and list of column names; output is a DataFrame with correct duplicate column assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = \"0,1,2\\n3,4,5\"\nnames = [\"a\", \"b\", \"a\"]\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(StringIO(data), names=names)\n```\n\n----------------------------------------\n\nTITLE: Formatting pandas Series Datetime Values to Strings (Python)\nDESCRIPTION: This snippet formats every datetime in a Series to a string (YYYY/MM/DD) using the .dt.strftime method, which follows the standard Python datetime formatting conventions. Assumes Series s contains datetimes. Dependencies: pandas, Series of datetime values. Input: Series, Output: Series of formatted strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(pd.date_range(\"20130101\", periods=4))\ns\ns.dt.strftime(\"%Y/%m/%d\")\n```\n\n----------------------------------------\n\nTITLE: Aligning String Concatenation in Series.str.cat with the join Argument - Python\nDESCRIPTION: Demonstrates the use of the join keyword in Series.str.cat to align Series on their index prior to concatenation. Requires pandas. Two examples show the new alignment feature—with and without join='left'—and handling NA values with the na_rep argument; output is a concatenated Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['a', 'b', 'c', 'd'])\nt = pd.Series(['b', 'd', 'e', 'c'], index=[1, 3, 4, 2])\ns.str.cat(t)\ns.str.cat(t, join='left', na_rep='-')\n```\n\n----------------------------------------\n\nTITLE: Interpolating Missing Data with limit_area Option in DataFrame.interpolate - Python\nDESCRIPTION: Illustrates advanced usage of DataFrame or Series interpolate with the limit_area argument to specify which NaN values to fill. Requires pandas and numpy. Various examples show filling inside-consecutive and outside-consecutive NaNs with different limit_direction and limit_area configurations; output is the interpolated Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([np.nan, np.nan, 5, np.nan, np.nan,\n                 np.nan, 13, np.nan, np.nan])\nser\n```\n\nLANGUAGE: python\nCODE:\n```\nser.interpolate(limit_direction='both', limit_area='inside', limit=1)\n```\n\nLANGUAGE: python\nCODE:\n```\nser.interpolate(limit_direction='backward', limit_area='outside')\n```\n\nLANGUAGE: python\nCODE:\n```\nser.interpolate(limit_direction='both', limit_area='outside')\n```\n\n----------------------------------------\n\nTITLE: Series Type Promotion on Assignment with Incompatible Types - pandas - Python\nDESCRIPTION: Shows type promotion within a Series upon assignment of a value incompatible with the current dtype. Requires pandas. Input: empty or typed Series; assignment of mismatched type triggers promotion. Output: Series holds all assigned values and reveals new promoted dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series()\n```\n\nLANGUAGE: python\nCODE:\n```\ns[\"a\"] = pd.Timestamp(\"2016-01-01\")\ns[\"b\"] = 3.0\ns\ns.dtype\n```\n\n----------------------------------------\n\nTITLE: Union and Concatenation of Categorical Data with Pandas in Python\nDESCRIPTION: Shows the usage of the new union_categoricals function in pandas to combine categorical objects, including those with different category sets. It also illustrates concatenation behavior of Series with category dtype. Requires pandas and access to pandas.api.types.union_categoricals; inputs are Categorical or Series objects, outputs concatenated or unified Categorical values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import union_categoricals\n\na = pd.Categorical([\"b\", \"c\"])\nb = pd.Categorical([\"a\", \"b\"])\nunion_categoricals([a, b])\n```\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([\"a\", \"b\"], dtype=\"category\")\ns2 = pd.Series([\"b\", \"c\"], dtype=\"category\")\n```\n\n----------------------------------------\n\nTITLE: Modulus and divmod with Timedeltas - pandas - Python\nDESCRIPTION: Shows use of the modulus (%) and divmod operations between Timedelta and timedelta-like or numeric arguments. Returns the remainder or tuple (count, remainder). Demonstrates both timedelta and numeric variants.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npd.Timedelta(hours=37) % datetime.timedelta(hours=2)\n\n# divmod against a timedelta-like returns a pair (int, Timedelta)\ndivmod(datetime.timedelta(hours=2), pd.Timedelta(minutes=11))\n\n# divmod against a numeric returns a pair (Timedelta, Timedelta)\ndivmod(pd.Timedelta(hours=25), 86400000000000)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Generated CSV File\nDESCRIPTION: Removes the temporary CSV file created in the previous example to maintain a clean environment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.remove(\"foo.csv\")\n```\n\n----------------------------------------\n\nTITLE: Comparing Missing Value Semantics: np.nan and pd.NA - pandas - Python\nDESCRIPTION: This code compares the behavior of numpy's np.nan and pandas' experimental pd.NA in comparison operations. It shows that np.nan > 1 yields False (numpy float rules) whereas pd.NA > 1 returns pd.NA, propagating the missing value in a logical sense. Requires numpy and pandas 1.0.0 or newer and illustrates the different propagation semantics for missing data between numpy and the new pandas indicator.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnp.nan > 1\\npd.NA > 1\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in Series.groupby for PeriodIndex\nDESCRIPTION: Addresses a regression in Series.groupby that would raise ValueError when grouping by PeriodIndex level.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nSeries.groupby()\n```\n\n----------------------------------------\n\nTITLE: Mutating pandas DataFrame Rows in apply UDFs - Python\nDESCRIPTION: Provides examples of unsafe and safe mutation inside a UDF used in DataFrame.apply. Shows why copying is needed to prevent mutation of rows being iterated over. Requires pandas. Outputs resulting DataFrame or possibly errors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/gotchas.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef f(s):\n    s.pop(\"a\")\n    return s\n\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\ndf.apply(f, axis=\"columns\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndef f(s):\n    s = s.copy()\n    s.pop(\"a\")\n    return s\n\ndf = pd.DataFrame({\"a\": [1, 2, 3], 'b': [4, 5, 6]})\ndf.apply(f, axis=\"columns\")\n```\n\n----------------------------------------\n\nTITLE: Tablewise Function Application using pipe()\nDESCRIPTION: Demonstrates method chaining using pipe() for applying functions that operate on entire DataFrames, including custom functions and external library integration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef extract_city_name(df):\n    \"\"\"\n    Chicago, IL -> Chicago for city_name column\n    \"\"\"\n    df[\"city_name\"] = df[\"city_and_code\"].str.split(\",\").str.get(0)\n    return df\n\ndef add_country_name(df, country_name=None):\n    \"\"\"\n    Chicago -> Chicago-US for city_name column\n    \"\"\"\n    col = \"city_name\"\n    df[\"city_and_country\"] = df[col] + country_name\n    return df\n\ndf_p = pd.DataFrame({\"city_and_code\": [\"Chicago, IL\"]})\n\nadd_country_name(extract_city_name(df_p), country_name=\"US\")\ndf_p.pipe(extract_city_name).pipe(add_country_name, country_name=\"US\")\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas Extensions Module\nDESCRIPTION: This snippet shows how to import the pandas extensions module. It's typically used at the beginning of a file to access the extensions API.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/extensions.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: pandas\n```\n\n----------------------------------------\n\nTITLE: Creating and Inspecting Pandas Categorical Series with .values (Python)\nDESCRIPTION: This snippet demonstrates how to create a pandas Categorical object and wrap it in a Series, then inspects both the Series and its .values attribute. It requires pandas and shows the input as a list and categories; outputs illustrate the internal Categorical representation and how .values stays a Categorical rather than a NumPy array. This snippet underscores differences between extension types and native NumPy dtypes in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/blog/extension-arrays.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> cat = pd.Categorical(['a', 'b', 'a'], categories=['a', 'b', 'c'])\n>>> ser = pd.Series(cat)\n>>> ser\n0    a\n1    b\n2    a\ndtype: category\nCategories (3, object): ['a', 'b', 'c']\n\n>>> ser.values\n[a, b, a]\nCategories (3, object): ['a', 'b', 'c']\n```\n\n----------------------------------------\n\nTITLE: Controlling Sorting Behavior in pandas.concat (Python)\nDESCRIPTION: Displays options for sorting columns in pandas.concat and related functions. Demonstrates passing sort=True to match previous automatic column sorting and sort=False to accept future behavior. Required dependencies are pandas. User must supply input DataFrames with different column orderings. Output DataFrame column order is affected by sort keyword.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame({\"a\": [1, 2], \"b\": [1, 2]}, columns=['b', 'a'])\ndf2 = pd.DataFrame({\"a\": [4, 5]})\n\npd.concat([df1, df2])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.concat([df1, df2], sort=True)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.concat([df1, df2], sort=False)\n```\n\n----------------------------------------\n\nTITLE: Applying a Binary Ufunc to Series using Array as Argument for Legacy Behavior - Pandas/Numpy - Python\nDESCRIPTION: Demonstrates how to force the old non-aligned behavior by passing the underlying array of a Series to a numpy function. 's1' is a pandas Series and 's2' provides the array. This usage will apply elementwise without index alignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nnp.power(s1, s2.array)\n```\n\n----------------------------------------\n\nTITLE: Inspecting and Modifying DataFrame Attributes in Pandas Python\nDESCRIPTION: This snippet showcases inspection of a DataFrame, manipulation of the columns attribute, and verification of changes. By slicing rows and using list comprehensions to lowercase column names, it highlights the mutability of axis labels. This operation is relevant in preparing or standardizing column headers before analysis. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf[:2]\ndf.columns = [x.lower() for x in df.columns]\ndf\n```\n\n----------------------------------------\n\nTITLE: Registering pandas Plotting Backend via Entry Points - Python\nDESCRIPTION: Illustrates how a pandas plotting backend can be registered using setuptools entry points in setup.py. The key 'pandas_plotting_backends' is used to let pandas discover custom backends. No code execution occurs here; this is setup/installation context. The dictionary maps friendly backend names (e.g., 'matplotlib') to the implementing module within the pandas (or other) package.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# in setup.py\nsetup(  # noqa: F821\n    ...,\n    entry_points={\n        \"pandas_plotting_backends\": [\n            \"matplotlib = pandas:plotting._matplotlib\",\n        ],\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: DataFrame.mean with Mixed Dtypes and numeric_only=None (ipython)\nDESCRIPTION: The snippet captures the former semantic, where DataFrame.mean would return an empty Series if invoked on a DataFrame with non-numeric columns, and would only produce means when explicitly given a numeric column subset. Outputs are a Series or empty Series depending on column selection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_10\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: df.mean()\nOut[3]: Series([], dtype: float64)\n\nIn [4]: df[[\"A\"]].mean()\nOut[4]:\nA    1.0\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Dropping a Column from pandas DataFrame using Python\nDESCRIPTION: This snippet shows how to drop the 'sex' column from the 'tips' DataFrame using the drop method with 'axis=1', which specifies column-wise operation. Requires the pandas library and a DataFrame named 'tips'. The method does not modify 'tips' in place unless 'inplace=True' is specified. Returns a new DataFrame excluding the dropped column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/column_selection.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntips.drop(\"sex\", axis=1)\n```\n\n----------------------------------------\n\nTITLE: MultiIndex Operations in Pandas\nDESCRIPTION: New MultiIndex functionality including getting level values, unstacking by name, and level-based sorting capabilities.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.4.x.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nMultiIndex.get_level_values()\nunstack()\nsortlevel()\n```\n\n----------------------------------------\n\nTITLE: Adding Table Caption with Custom Styling\nDESCRIPTION: Sets a caption for the table and applies custom CSS styling to control its appearance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ns.set_caption(\n    \"Confusion matrix for multiple cancer prediction models.\"\n).set_table_styles(\n    [{\"selector\": \"caption\", \"props\": \"caption-side: bottom; font-size:1.25em;\"}],\n    overwrite=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Sharing DataFrame Styles via Styler API in Pandas (Python)\nDESCRIPTION: This snippet demonstrates advanced style chaining on a DataFrame using pandas Styler, including mapping functions, opacity styling based on value conditions, custom table header styles, and hiding the index. Requires pandas and pre-defined functions (style_negative). Input is a DataFrame (df2), and the output is a styled DataFrame (style1) with enhanced readability and customized presentation. Limitations include DataFrame-only support and the need for unique identifiers for certain styling operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nstyle1 = (\n    df2.style.map(style_negative, props=\"color:red;\")\n    .map(lambda v: \"opacity: 20%;\" if (v < 0.3) and (v > -0.3) else None)\n    .set_table_styles([{\"selector\": \"th\", \"props\": \"color: blue;\"}])\n    .hide(axis=\"index\")\n)\nstyle1\n```\n\n----------------------------------------\n\nTITLE: Display Unicode Alignment in pandas DataFrame - Python\nDESCRIPTION: Shows creation of a DataFrame containing East Asian characters and demonstrates alignment issues that arise with such characters. Utilized when evaluating display output in Jupyter or a terminal. Requires pandas; input is a dictionary with Unicode columns; output is the DataFrame printed with alignment depending on display settings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({u\"\\u56fd\\u7c4d\": [\"UK\", u\"\\u65e5\\u672c\"], u\"\\u540d\\u524d\": [\"Alice\", u\"\\u3057\\u306e\\u3076\"]})\ndf\n```\n\n----------------------------------------\n\nTITLE: Reading SQL Table with Arrow Dtype Backend in pandas (Python)\nDESCRIPTION: Shows how to preserve SQL column types accurately by loading from the database using pandas read_sql with the dtype_backend set to 'pyarrow'. This ensures non-lossy round-tripping by avoiding unwanted type conversion to pandas/NumPy types. Requires adbc_driver_postgresql and pandas 2.2.0+. Input is SQL connection/table; output is DataFrame with Arrow data types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_3\n\nLANGUAGE: ipython\nCODE:\n```\n# for round-tripping\nwith pg_dbapi.connect(uri) as conn:\n    df2 = pd.read_sql(\"pandas_table\", conn, dtype_backend=\"pyarrow\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving pandas Display Options Using get_option - python\nDESCRIPTION: Demonstrates usage of pd.get_option() to access global display preferences ('display.max_rows') in pandas, part of the new options management API. This replaces previous set_printoptions, aligns with current best practice, and aids in fine-tuning output formatting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npd.get_option(\"display.max_rows\")\n```\n\n----------------------------------------\n\nTITLE: Using DataFrame.drop with columns Parameter in Python\nDESCRIPTION: Demonstrates the enhanced drop method that accepts index/columns keywords as an alternative to specifying the axis parameter. This makes the API more consistent with other DataFrame methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.arange(8).reshape(2, 4),\n                 columns=['A', 'B', 'C', 'D'])\ndf\ndf.drop(['B', 'C'], axis=1)\n# the following is now equivalent\ndf.drop(columns=['B', 'C'])\n```\n\n----------------------------------------\n\nTITLE: Instantiating Period('NaT') and NaT Arithmetic (ipython, Python)\nDESCRIPTION: Illustrates the transition where Period('NaT') now returns pd.NaT for consistency, and pd.NaT supports direct integer addition/subtraction. Requires pandas; demonstrates previous behavior where ValueError was raised and new behavior where arithmetic is possible. Inputs are 'NaT' values, outputs are either pd.NaT or Timedelta objects. No freq parameter is needed for new behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_34\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: pd.Period('NaT', freq='D')\nOut[5]: Period('NaT', 'D')\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Period(\"NaT\")\npd.Period(None)\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: pd.NaT + 1\n...\nValueError: Cannot add integral value to Timestamp without freq.\n```\n\nLANGUAGE: python\nCODE:\n```\npd.NaT + 1\npd.NaT - 1\n```\n\n----------------------------------------\n\nTITLE: Exporting Grouped Statistics to CSV and Excel with pandas in Python\nDESCRIPTION: This snippet illustrates how to export pandas DataFrame results to external files. The first part saves group mean results to a CSV; the second uses pd.ExcelWriter to write multiple DataFrames to different Excel sheets. Dependencies include pandas and that the DataFrame 'tips' is present. Parameters include the file paths ('summary.csv', 'results.xlsx') and specific statistics to export. Outputs are saved files containing the grouped and descriptive statistics. Constraints: proper write permissions and existing pandas installation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spss.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n   # Save summary statistics to CSV\n   tips.groupby('sex')[['total_bill', 'tip']].mean().to_csv('summary.csv')\n\n   # Save multiple results to Excel sheets\n   with pd.ExcelWriter('results.xlsx') as writer:\n       tips.describe().to_excel(writer, sheet_name='Descriptives')\n       tips.groupby('sex').mean().to_excel(writer, sheet_name='Means by Gender')\n```\n\n----------------------------------------\n\nTITLE: DatetimeIndex Timezone Conversion - Future and Previous Approaches - Python\nDESCRIPTION: These code snippets show the two supported ways to explicitly opt in to either the new or old behavior for interpreting integer data as datetime with timezone. Using pd.to_datetime with utc=True followed by tz_convert() opts in to the future pandas behavior; using tz_localize() retains the old behavior. Inputs are integer nanosecond values and the desired timezone; the output is a pandas Series or Index with datetime and correct tz semantics. No dependencies beyond pandas are required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime([946684800000000000], utc=True).tz_convert('US/Central')\n```\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime([946684800000000000]).tz_localize('US/Central')\n```\n\n----------------------------------------\n\nTITLE: Helper Function for Float to Integer Conversion\nDESCRIPTION: Example helper function to safely convert float values to integers within a specified tolerance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0006-ban-upcasting.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef maybe_convert_to_int(x: int | float, tolerance: float):\n    if np.abs(x - round(x)) < tolerance:\n        return round(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Creating Andrews Curves Plot for Multivariate Data\nDESCRIPTION: Generates Andrews curves visualization for the Iris dataset, coloring curves by the 'Name' column (class). Andrews curves transform multivariate data into curves to visualize clustering patterns, with similar classes typically forming closer groups.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.plotting import andrews_curves\n\ndata = pd.read_csv(\"data/iris.data\")\n\nplt.figure();\n\nandrews_curves(data, \"Name\");\n```\n\n----------------------------------------\n\nTITLE: Using the New IntervalIndex in Pandas\nDESCRIPTION: This code demonstrates the new IntervalIndex with its own interval dtype, showing how it is returned by cut() function and provides first-class support for interval notation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nc = pd.cut(range(4), bins=2)\nc\nc.categories\n```\n\n----------------------------------------\n\nTITLE: Constructing a Series with Optional Integer NA Support - pandas (Python)\nDESCRIPTION: Demonstrates creating a pandas Series with the Int64 extension dtype supporting missing values (NA). Requires pandas 0.24.0+ and NumPy. The Series is constructed using a list containing both integer values and np.nan, resulting in a nullable integer type with NA displayed as NaN. Input consists of mixed ints and np.nan; output is a Series of dtype Int64 extension. Limitations include experimental API status and use of capitalized dtype names such as 'Int64'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, np.nan], dtype='Int64')\ns\n```\n\n----------------------------------------\n\nTITLE: Evaluating Boolean Expressions with pandas.eval Pandas Engine - Python\nDESCRIPTION: Evaluates a boolean expression on multiple DataFrames using pandas.eval with the 'pandas' parser, relying on operator precedence without explicit parentheses. Requires pandas and numpy. The result is a boolean DataFrame, equivalent to the parenthesized version evaluated with the 'python' engine. Useful for performance or convenience, but be mindful of operator precedence.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nexpr_no_parens = \"df1 > 0 & df2 > 0 & df3 > 0 & df4 > 0\"\ny = pd.eval(expr_no_parens, parser=\"pandas\")\nnp.all(x == y)\n```\n\n----------------------------------------\n\nTITLE: Column-Wise Categorical Conversion using DataFrame.astype - Python\nDESCRIPTION: Shows conversion of DataFrame columns to Categorical dtype by supplying either 'category' as a string or an explicit CategoricalDtype. Requires pandas. Examples demonstrate the effects of both approaches, including consistency of category levels and ordering, and show checks of dtype for verification.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': list('abca'), 'B': list('bccd')})\ndf = df.astype('category')\ndf['A'].dtype\ndf['B'].dtype\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import CategoricalDtype\ndf = pd.DataFrame({'A': list('abca'), 'B': list('bccd')})\ncdt = CategoricalDtype(categories=list('abcd'), ordered=True)\ndf = df.astype(cdt)\ndf['A'].dtype\ndf['B'].dtype\n```\n\n----------------------------------------\n\nTITLE: Resampling Series with TimedeltaIndex in pandas (Python)\nDESCRIPTION: Shows how to use the resample method on a pandas Series indexed with TimedeltaIndex, performing daily ('D') aggregation with mean. Requires pandas and a suitable Series 's' constructed with TimedeltaIndex. The method splits data into daily bins and returns their means. Inputs must be compatible time-like indices, and 'resample' returns grouped/aggregated results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ns.resample(\"D\").mean()\n```\n\n----------------------------------------\n\nTITLE: Reindexing DataFrame with Nearest Method and Tolerance using pandas - Python\nDESCRIPTION: Illustrates DataFrame.reindex with the 'nearest' method and a numeric tolerance. This provides finer-grained control over which entries can be matched when filling missing values. Requires pandas; input is a DataFrame and a list of new labels. Output is a new DataFrame reindexed to the given values according to the tolerance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"x\": range(5), \"t\": pd.date_range(\"2000-01-01\", periods=5)})\ndf.reindex([0.1, 1.9, 3.5], method=\"nearest\", tolerance=0.2)\n```\n\n----------------------------------------\n\nTITLE: Creating Example DataFrame with Various dtypes using pandas (Python)\nDESCRIPTION: This snippet creates a pandas DataFrame with various types, including integer, unsigned integer, float, boolean, categorical, and datetime (with and without timezones), using Python. Requires pandas, pytz, numpy. The DataFrame columns are 'a' (str), 'b' (int), 'c' (uint8), 'd' (float64), 'e' (bool), 'f' (categorical), 'g' (datetime), 'h' (datetime with timezone), and 'i' (datetime with nanoseconds). The output is the DataFrame and its dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_202\n\nLANGUAGE: python\nCODE:\n```\nimport pytz\n\ndf = pd.DataFrame(\n    {\n        \"a\": list(\"abc\"),\n        \"b\": list(range(1, 4)),\n        \"c\": np.arange(3, 6).astype(\"u1\"),\n        \"d\": np.arange(4.0, 7.0, dtype=\"float64\"),\n        \"e\": [True, False, True],\n        \"f\": pd.Categorical(list(\"abc\")),\n        \"g\": pd.date_range(\"20130101\", periods=3),\n        \"h\": pd.date_range(\"20130101\", periods=3, tz=pytz.timezone(\"US/Eastern\")),\n        \"i\": pd.date_range(\"20130101\", periods=3, freq=\"ns\"),\n    }\n)\n\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Converting NoRowIndex DataFrame to Indexed DataFrame\nDESCRIPTION: Example showing how to add an index to a NoRowIndex DataFrame using set_axis with RangeIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf.set_axis(pd.RangeIndex(len(df)))\n```\n\n----------------------------------------\n\nTITLE: value_counts Returns Nullable Integer Dtype (Python, pandas 1.0.0)\nDESCRIPTION: Demonstrates that Series.value_counts with a nullable integer dtype now returns a result of nullable integer dtype instead of plain int64. Dependencies: pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npd.Series([2, 1, 1, None], dtype=\"Int64\").value_counts().dtype\n```\n\n----------------------------------------\n\nTITLE: Comparing And/Or with Keyword and pandas.eval - Python\nDESCRIPTION: Shows how to use the 'and' keyword in expressions evaluated by pandas.eval, and compares the output from the 'python' engine and the 'pandas' engine. Requires pandas and numpy. Inputs include DataFrames as before, expression strings with '&' vs 'and'. Verifies equivalence of two evaluation results with numpy.all.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nexpr = \"(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\"\nx = pd.eval(expr, parser=\"python\")\nexpr_with_ands = \"df1 > 0 and df2 > 0 and df3 > 0 and df4 > 0\"\ny = pd.eval(expr_with_ands, parser=\"pandas\")\nnp.all(x == y)\n```\n\n----------------------------------------\n\nTITLE: Index where() Method for Conditional Indexing - Pandas - Python\nDESCRIPTION: Introduces the .where() method for pandas Index objects, matching behavior in Series and DataFrame. The example applies a boolean mask, returning an Index where False positions are filled with NaN. Requires pandas library, with the mask parameter specifying which elements to keep.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nidx = pd.Index([\"a\", \"b\", \"c\"])\nidx.where([True, False, True])\n```\n\n----------------------------------------\n\nTITLE: Adding/Subtracting with Timestamps/TimedeltaIndex/DatetimeIndex - Pandas - IPython\nDESCRIPTION: These code snippets demonstrate the now-deprecated behavior of adding or subtracting raw integers or integer arrays to/from pandas Timestamp, TimedeltaIndex, and DatetimeIndex objects. This was previously allowed but can lead to ambiguous results; users are now encouraged to multiply the object's freq attribute by the integer value. No additional dependencies beyond pandas and numpy are required. Inputs include Timestamp, TimedeltaIndex, and DatetimeIndex objects, and integer or integer array arguments; outputs are shifted datetimelike objects. This pattern is deprecated and should be replaced as shown in the new behavior snippets.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_44\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: ts = pd.Timestamp('1994-05-06 12:15:16', freq=pd.offsets.Hour())\nIn [6]: ts + 2\nOut[6]: Timestamp('1994-05-06 14:15:16', freq='H')\n\nIn [7]: tdi = pd.timedelta_range('1D', periods=2)\nIn [8]: tdi - np.array([2, 1])\nOut[8]: TimedeltaIndex(['-1 days', '1 days'], dtype='timedelta64[ns]', freq=None)\n\nIn [9]: dti = pd.date_range('2001-01-01', periods=2, freq='7D')\nIn [10]: dti + pd.Index([1, 2])\nOut[10]: DatetimeIndex(['2001-01-08', '2001-01-22'], dtype='datetime64[ns]', freq=None)\n```\n\n----------------------------------------\n\nTITLE: Filtering Data in Pandas (Equivalent to SQL WHERE)\nDESCRIPTION: These snippets show how to filter data in pandas, which is equivalent to using WHERE clauses in SQL. It demonstrates single and multiple conditions using boolean indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntips[(tips[\"time\"] == \"Dinner\") & (tips[\"tip\"] > 5.00)]\n```\n\nLANGUAGE: python\nCODE:\n```\ntips[(tips[\"size\"] >= 5) | (tips[\"total_bill\"] > 45)]\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in DataFrame.groupby.rolling.cov and corr for unsorted groupings\nDESCRIPTION: Addresses a problem where DataFrame.groupby.rolling.cov and DataFrame.groupby.rolling.corr were computing incorrect results if the input groupings were not sorted.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.groupby(group).rolling(window).cov()\nDataFrame.groupby(group).rolling(window).corr()\n```\n\n----------------------------------------\n\nTITLE: Defining and Using Custom Correlation Function with DataFrame.corr - Python\nDESCRIPTION: Defines a custom distance correlation function and uses it as the method argument in DataFrame.corr to compute a custom correlation matrix. Dependencies: pandas, numpy. Inputs: DataFrame, custom function. Output: correlation matrix using the user-defined strategy. Useful for advanced statistical analyses.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ndef distcorr(x, y):\n    n = len(x)\n    a = np.zeros(shape=(n, n))\n    b = np.zeros(shape=(n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            a[i, j] = abs(x[i] - x[j])\n            b[i, j] = abs(y[i] - y[j])\n    a += a.T\n    b += b.T\n    a_bar = np.vstack([np.nanmean(a, axis=0)] * n)\n    b_bar = np.vstack([np.nanmean(b, axis=0)] * n)\n    A = a - a_bar - a_bar.T + np.full(shape=(n, n), fill_value=a_bar.mean())\n    B = b - b_bar - b_bar.T + np.full(shape=(n, n), fill_value=b_bar.mean())\n    cov_ab = np.sqrt(np.nansum(A * B)) / n\n    std_a = np.sqrt(np.sqrt(np.nansum(A ** 2)) / n)\n    std_b = np.sqrt(np.sqrt(np.nansum(B ** 2)) / n)\n    return cov_ab / std_a / std_b\n\n\ndf = pd.DataFrame(np.random.normal(size=(100, 3)))\ndf.corr(method=distcorr)\n```\n\n----------------------------------------\n\nTITLE: IO Operations Bug Fixes in Pandas 0.25.2\nDESCRIPTION: Fixes for DataFrame display in notebooks, to_csv functionality with IntervalIndex and ExtensionArray with list-like values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.2.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.to_csv()\nDataFrame.index\n```\n\n----------------------------------------\n\nTITLE: Importing All from pandas Namespace in Python\nDESCRIPTION: This snippet imports all objects from the pandas library into the current Python namespace, as used in documentation prepared for in-place code evaluation or demonstration. It suppresses output and disables linting warnings for unused and wildcard imports, which is necessary in documentation examples. No external dependencies are required apart from pandas, and it aids in writing concise examples without referencing the pandas namespace repeatedly. There are no input or output parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.3.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas import *  # noqa F401, F403\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in DataFrame.corr for Spearman method on 32-bit platforms\nDESCRIPTION: Resolves an issue where DataFrame.corr was raising ValueError with method=\"spearman\" on 32-bit platforms.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.corr(method=\"spearman\")\n```\n\n----------------------------------------\n\nTITLE: Casting Timedelta Series to Seconds Precision using astype (Python)\nDESCRIPTION: Converts a Series of timedeltas to seconds with astype('timedelta64[s]'). Requires pandas and NumPy. Input is a Series 'td'; output is a float64 Series representing each timedelta in whole seconds, rounding down fractions and producing NaN for NaT. Good for lowest common time resolutions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntd.astype('timedelta64[s]')\n```\n\n----------------------------------------\n\nTITLE: Correcting FY5253 Date Offset Arithmetic\nDESCRIPTION: Fixes an issue where FY5253 date offsets could incorrectly raise an AssertionError in arithmetic operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_58\n\nLANGUAGE: Python\nCODE:\n```\ndate + pd.offsets.FY5253()\n```\n\n----------------------------------------\n\nTITLE: Applying ufuncs to SparseArray with Custom Fill Value - pandas - Python\nDESCRIPTION: Applies a NumPy ufunc (e.g., np.abs) on a SparseArray configured with a specific fill value, illustrating that both stored data and the fill value are affected. Demonstrates extracting results as either sparse or dense array. Requires pandas, numpy. Outputs include the transformed SparseArray and its dense equivalent.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\narr = pd.arrays.SparseArray([1., -1, -1, -2., -1], fill_value=-1)\nnp.abs(arr)\nnp.abs(arr).to_dense()\n```\n\n----------------------------------------\n\nTITLE: Accessing Scalar NA Values in Nullable Integer Arrays (pandas, Python)\nDESCRIPTION: Shows that slicing and indexing into a pandas IntegerArray with missing values returns pandas.NA for missing elements, highlighting coherent NA propagation. Requires IntegerArray a of dtype 'Int64'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\na = pd.array([1, None], dtype=\"Int64\")\na[1]\n```\n\n----------------------------------------\n\nTITLE: DataFrame to_csv with na_rep in Python\nDESCRIPTION: Fixed regression in DataFrame.to_csv where specifying an na_rep might truncate the values written.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndf.to_csv('file.csv', na_rep='NA')\n```\n\n----------------------------------------\n\nTITLE: Downloading Release Wheels\nDESCRIPTION: Command for downloading wheels and source distribution for a pandas release using a script. Downloads artifacts to the dist directory.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/maintaining.rst#_snippet_5\n\nLANGUAGE: console\nCODE:\n```\nscripts/download_wheels.sh <VERSION>\n```\n\n----------------------------------------\n\nTITLE: Timestamp Constructor with Positional and Keyword Parameters - Pandas - Python\nDESCRIPTION: Demonstrates the flexibility in creating pd.Timestamp objects either with positional parameters (year, month, day) or with detailed keyword arguments for customization (including hour and minute). Requires only pandas. Inputs determine the resulting timestamp's value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\npd.Timestamp(2012, 1, 1)\n\npd.Timestamp(year=2012, month=1, day=1, hour=8, minute=30)\n```\n\n----------------------------------------\n\nTITLE: Disabling Cython Bounds Checking and Wraparound for Maximum ndarray Performance - Cython\nDESCRIPTION: Further optimizes Cython ndarray operations by disabling safety checks with '@cython.boundscheck(False)' and '@cython.wraparound(False)'. All arrays are precisely typed; index variables and math use np.float64_t and np.int64_t for maximum performance and compatibility. The core loop in 'apply_integrate_f_wrap' eliminates Python overhead, but out-of-bounds errors will result in segfaults. Requires Cython, numpy, and care with data alignment. Output is a float64 numpy array.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_9\n\nLANGUAGE: cython\nCODE:\n```\ncimport cython\ncimport numpy as np\nimport numpy as np\nnp.import_array()\ncdef np.float64_t f_typed(np.float64_t x) except? -2:\n    return x * (x - 1)\ncpdef np.float64_t integrate_f_typed(np.float64_t a, np.float64_t b, np.int64_t N):\n    cdef np.int64_t i\n    cdef np.float64_t s = 0.0, dx\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_typed(a + i * dx)\n    return s * dx\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncpdef np.ndarray[np.float64_t] apply_integrate_f_wrap(\n    np.ndarray[np.float64_t] col_a,\n    np.ndarray[np.float64_t] col_b,\n    np.ndarray[np.int64_t] col_N\n):\n    cdef np.int64_t i, n = len(col_N)\n    assert len(col_a) == len(col_b) == n\n    cdef np.ndarray[np.float64_t] res = np.empty(n, dtype=np.float64)\n    for i in range(n):\n        res[i] = integrate_f_typed(col_a[i], col_b[i], col_N[i])\n    return res\n```\n\n----------------------------------------\n\nTITLE: Preserving Dtype in SparseSeries Arithmetic and Converting Dtype - Python\nDESCRIPTION: These snippets show that after arithmetic operations, SparseSeries now preserves dtype, and demonstrate converting a SparseSeries to int64 using astype. The code also notes that astype will fail if fill_value cannot be coerced into the target dtype. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ns = pd.SparseSeries([0, 2, 0, 1], fill_value=0, dtype=np.int64)\ns.dtype\n\ns + 1\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.SparseSeries([1.0, 0.0, 2.0, 0.0], fill_value=0)\ns\ns.astype(np.int64)\n```\n\n----------------------------------------\n\nTITLE: Boolean Indexer for MultiIndex Slicing - pandas Python\nDESCRIPTION: Illustrates using a boolean indexer to filter rows based on column values, followed by advanced selection with IndexSlice. Creates a mask, then applies loc with a combination of boolean array and label lists, enabling value-dependent sub-selection in multi-indexed DataFrame contexts.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nmask = df[('a', 'foo')] > 200\ndf.loc[idx[mask, :, ['C1', 'C3']], idx[:, 'foo']]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Updated 'ewma' Function Behavior in Pandas (ipython)\nDESCRIPTION: These Code cells illustrate the impact of the new 'ignore_na' parameter in 'pd.ewma', showing differences in output when handling None values with 'ignore_na=True' (legacy behavior) and 'ignore_na=False' (default). Dependencies: pandas. Inputs include a Series with None and float values, the 'com' parameter, and the optional 'ignore_na' flag. Output is a Series of exponentially weighted moving averages. This change clarifies NaN propagation versus the legacy approach, suitable for users migrating from pre-0.15.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_15\n\nLANGUAGE: ipython\nCODE:\n```\nIn [7]: pd.ewma(pd.Series([None, 1., 8.]), com=2.)\nOut[7]:\n0    NaN\n1    1.0\n2    5.2\ndtype: float64\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [8]: pd.ewma(pd.Series([1., None, 8.]), com=2.,\n   ....:         ignore_na=True)  # pre-0.15.0 behavior\nOut[8]:\n0    1.0\n1    1.0\n2    5.2\ndtype: float64\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [9]: pd.ewma(pd.Series([1., None, 8.]), com=2.,\n   ....:         ignore_na=False)  # new default\nOut[9]:\n0    1.000000\n1    1.000000\n2    5.846154\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Visualizing Dataset with radviz Using pandas and matplotlib - Python\nDESCRIPTION: This code snippet demonstrates how to use pandas to load the Iris dataset and plot it using the radviz function from pandas.plotting, leveraging matplotlib for figure rendering. Dependencies include pandas, matplotlib, and the Iris dataset CSV file. 'data' is read from CSV, and radviz is used to visualize classification in a multi-dimensional space, labeling points by the 'Name' column; the output is a matplotlib plot.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.plotting import radviz\n\ndata = pd.read_csv(\"data/iris.data\")\n\nplt.figure();\n\n@savefig radviz.png\nradviz(data, \"Name\");\n```\n\n----------------------------------------\n\nTITLE: Applying Lambda with groupby (group_keys=False) in pandas - Python\nDESCRIPTION: Illustrates grouping and applying a lambda function with group_keys set to False, which excludes the group labels from appearing in the resulting MultiIndex. This can simplify resultant DataFrame structure when flattening groups. Requires a DataFrame 'df'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"A\", group_keys=False).apply(lambda x: x)\n```\n\n----------------------------------------\n\nTITLE: Iterating Over HDFStore Table Chunks using select with Chunksize - pandas HDFStore - Python\nDESCRIPTION: This snippet demonstrates the use of the iterator pattern when querying large HDFStore tables, using the chunksize parameter to fetch data in controlled-size pieces. The for loop yields DataFrames chunk by chunk, allowing efficient memory usage. Dependencies include pandas and an open HDFStore. Key parameter is chunksize, which determines the number of rows per chunk. The output is a series of DataFrames, each printed. Can be used with both store.select and pd.read_hdf context managers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_188\n\nLANGUAGE: python\nCODE:\n```\nfor df in store.select(\"df\", chunksize=3):\n    print(df)\n```\n\n----------------------------------------\n\nTITLE: Parsing Mixed Timezones in CSV - pandas - Python\nDESCRIPTION: Demonstrates reading a CSV from a string where a column has datetime strings with mixed timezones. Converts the column to pandas datetime with UTC normalization. Requires pandas and StringIO from the io module.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ncontent = \"\"\"\\\na\n2000-01-01T00:00:00+05:00\n2000-01-01T00:00:00+06:00\"\"\"\ndf = pd.read_csv(StringIO(content))\ndf[\"a\"] = pd.to_datetime(df[\"a\"], utc=True)\ndf[\"a\"]\n```\n\n----------------------------------------\n\nTITLE: Specifying Floating-Point Conversion Precision - pandas - Python\nDESCRIPTION: Compares how pandas.read_csv handles floating point values using different float_precision options: default, 'high', and 'round_trip', using the C engine. Shows the difference in floating point accuracy. Requires pandas and StringIO.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nval = \"0.3066101993807095471566981359501369297504425048828125\"\ndata = \"a,b,c\\n1,2,{0}\".format(val)\nabs(\n    pd.read_csv(\n        StringIO(data),\n        engine=\"c\",\n        float_precision=None,\n    )[\"c\"][0] - float(val)\n)\nabs(\n    pd.read_csv(\n        StringIO(data),\n        engine=\"c\",\n        float_precision=\"high\",\n    )[\"c\"][0] - float(val)\n)\nabs(\n    pd.read_csv(StringIO(data), engine=\"c\", float_precision=\"round_trip\")[\"c\"][0]\n    - float(val)\n)\n```\n\n----------------------------------------\n\nTITLE: Accessing and Localizing Timezone-Aware Series in pandas (Python)\nDESCRIPTION: This code snippet shows how to access a column containing timezone-aware datetimes from a DataFrame and how to remove the timezone localization using the dt.tz_localize(None) accessor method. It highlights pandas' improved support for efficient timezone manipulations on entire Series objects. Inputs are a DataFrame with timezone-aware columns; output is the Series before and after timezone removal. Requires pandas 0.17.0+.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf.B\ndf.B.dt.tz_localize(None)\n```\n\n----------------------------------------\n\nTITLE: Converting Dense DataFrame to Sparse using astype and SparseDtype - pandas - Python\nDESCRIPTION: Illustrates the conversion of a regular (dense) DataFrame into a sparse DataFrame using the astype method and a SparseDtype definition. Useful for reducing memory usage in data with repeated or default values. Dependencies: pandas. Key parameters are the dense DataFrame and a customized SparseDtype; output is a DataFrame with sparse storage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndense = pd.DataFrame({\"A\": [1, 0, 0, 1]})\ndtype = pd.SparseDtype(int, fill_value=0)\ndense.astype(dtype)\n```\n\n----------------------------------------\n\nTITLE: Incrementally Reading Stata Files in Chunks with StataReader - Python\nDESCRIPTION: This snippet shows how to iterate over a Stata .dta file in predefined chunk sizes using the chunksize parameter with pandas.read_stata. A StataReader object is returned and used as an iterator within a context manager to process file data in blocks, enabling memory-efficient reads. Input: path to .dta file, chunksize parameter. Output: individual DataFrames for each chunk.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_237\n\nLANGUAGE: python\nCODE:\n```\nwith pd.read_stata(\"stata.dta\", chunksize=3) as reader:\n    for df in reader:\n        print(df.shape)\n```\n\n----------------------------------------\n\nTITLE: Comparing Group Means and Standard Deviations Before and After Transformation with pandas in Python\nDESCRIPTION: This snippet calculates and compares the mean and standard deviation of original versus transformed time series data grouped by year. It shows how to validate the groupby transformation and requires the earlier defined ts and transformed variables. The code outputs group statistics for both the original and transformed data, highlighting the result of the standardization.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n# Original Data\ngrouped = ts.groupby(lambda x: x.year)\ngrouped.mean()\ngrouped.std()\n\n# Transformed Data\ngrouped_trans = transformed.groupby(lambda x: x.year)\ngrouped_trans.mean()\ngrouped_trans.std()\n```\n\n----------------------------------------\n\nTITLE: Chaining Style Maps to Demonstrate CSS Priority in pandas DataFrame (Python)\nDESCRIPTION: This snippet demonstrates how chaining multiple style maps in pandas' DataFrame Styler affects which CSS style is applied, specifically showing that the last-applied map takes precedence when the CSS selectors are of equal specificity. It requires pandas to be installed, and uses the style.map method with two different color lambdas. The input is a simple DataFrame, and the output is HTML with the corresponding style applied.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndf4 = pd.DataFrame([[\"text\"]])\ndf4.style.map(lambda x: \"color:green;\").map(lambda x: \"color:red;\")\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with String Categorical Columns in Pandas\nDESCRIPTION: Shows how read_csv handles categorical data when using dtype={'col': 'category'}, which by default creates categories with object (string) dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndata = 'A,B\\na,1\\nb,2\\nc,3'\npd.read_csv(StringIO(data), dtype={'B': 'category'}).B.cat.categories\n```\n\n----------------------------------------\n\nTITLE: Missing Dependency Error Example\nDESCRIPTION: Example showing the current behavior when attempting to use a connector with missing dependencies. This demonstrates how pandas handles optional dependency imports.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> pandas.read_gbq(query)\nTraceback (most recent call last):\n  ...\nImportError: Missing optional dependency 'pandas-gbq'.\npandas-gbq is required to load data from Google BigQuery.\nSee the docs: https://pandas-gbq.readthedocs.io.\nUse pip or conda to install pandas-gbq.\n```\n\n----------------------------------------\n\nTITLE: New Behavior: DataFrame.apply Evaluates First Row Once - python\nDESCRIPTION: Demonstrates that in updated pandas (1.1.0+), applying a function along axis=1 now evaluates each row exactly once, confirming improved internal logic for apply/applymap. Shows invocation, output, and correct print order. Relies only on prior definition of df and func.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndf.apply(func, axis=1)\n```\n\n----------------------------------------\n\nTITLE: Limiting Results in Pandas (Equivalent to SQL LIMIT)\nDESCRIPTION: This snippet demonstrates how to limit the number of rows returned in pandas using the head method, which is equivalent to SQL's LIMIT clause.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntips.head(10)\n```\n\n----------------------------------------\n\nTITLE: Mapping Series Values by Another Series - pandas - Python\nDESCRIPTION: Maps values of a pandas Series to new values defined in another Series, showing how to associate string values to numerical representations. Demonstrates .map() with a mapping Series (t). Input: mapping Series; Output: mapped values Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(\n    [\"six\", \"seven\", \"six\", \"seven\", \"six\"], index=[\"a\", \"b\", \"c\", \"d\", \"e\"]\n)\nt = pd.Series({\"six\": 6.0, \"seven\": 7.0})\ns\ns.map(t)\n```\n\n----------------------------------------\n\nTITLE: Modifying and Comparing DataFrames Using pandas in Python\nDESCRIPTION: This snippet demonstrates updating a specific value in a pandas DataFrame and comparing two DataFrames using the DataFrame.compare() method. It shows how by default, compare() will return a DataFrame highlighting differences, omitting rows or columns with all equal values, and using NaN to indicate where corresponding values have no differences. Dependencies include the Python pandas library and two DataFrame objects (df and df2). Input: DataFrames to be compared. Output: DataFrame of differences. Limitations: omits unchanged rows/columns, only shows differing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndf2.loc[2, \"col3\"] = 4.0\n   df2\n   df.compare(df2)\n```\n\n----------------------------------------\n\nTITLE: Indexing DateTime with UTC-Offsets in Pandas DataFrame (Python)\nDESCRIPTION: Demonstrates how indexing a DataFrame or Series with a date string including a UTC offset is now respected by Pandas, as opposed to previous behavior where the offset was ignored. Requires Pandas to be imported as pd and a DataFrame indexed by a tz-aware DatetimeIndex. The slice selects rows with a new respect for the offset. Returns a DataFrame or Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_3\n\nLANGUAGE: ipython\nCODE:\n```\ndf = pd.DataFrame([0], index=pd.DatetimeIndex(['2019-01-01'], tz='US/Pacific'))\ndf\n```\n\nLANGUAGE: ipython\nCODE:\n```\ndf['2019-01-01 12:00:00+04:00':'2019-01-01 13:00:00+04:00']\n```\n\n----------------------------------------\n\nTITLE: DataFrame Plotting with .plot Submethods in pandas (Python)\nDESCRIPTION: This pair of snippets illustrates pandas' new '.plot' submethod interface, which exposes different plot kinds as attributes for better discoverability and argument transparency. The first creates a DataFrame of random data, and the second plots a bar chart using df.plot.bar(). Requires pandas and matplotlib. Input: DataFrame; output: rendered plot. This makes plot methods match their signatures more closely and simplifies user API discovery.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_6\n\nLANGUAGE: ipython\nCODE:\n```\nIn [13]: df = pd.DataFrame(np.random.rand(10, 2), columns=['a', 'b'])\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [14]: df.plot.bar()\n```\n\n----------------------------------------\n\nTITLE: Warning for Series Treatment in rename_categories in Python\nDESCRIPTION: Shows a warning about how rename_categories currently treats Series as list-like, but will change to treat them as dict-like in a future version. Provides examples of how to write future-proof code.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nc.rename_categories(pd.Series([0, 1], index=['a', 'c']))\n```\n\n----------------------------------------\n\nTITLE: to_timedelta: Parsing List of Strings with NaN - pandas - Python\nDESCRIPTION: Converts a list of mixed duration strings (including nan) to TimedeltaIndex using pd.to_timedelta. Inputs can include valid time strings and missing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.to_timedelta([\"1 days 06:05:01.00003\", \"15.5us\", \"nan\"])\n```\n\n----------------------------------------\n\nTITLE: Copy-on-Write Warning Mode Configuration\nDESCRIPTION: Shows how to enable warning mode for Copy-on-Write operations in pandas 2.2.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/copy_on_write.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npd.options.mode.copy_on_write = \"warn\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Current vs Future String Dtype Inference in pandas\nDESCRIPTION: This snippet shows how pandas currently infers string data as object dtype, and how it will change to use PyArrow's string type in the future.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0010-required-pyarrow-dependency.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: import pandas as pd\n\nIn [2]: pd.Series([\"a\"]).dtype\n# Current behavior\nOut[2]: dtype('O')\n\n# Future behavior in 3.0\nOut[2]: string[pyarrow]\n```\n\n----------------------------------------\n\nTITLE: Defining Original Properties in pandas DataFrame Subclasses - Python\nDESCRIPTION: Illustrates adding both temporary (_internal_names) and persistent (_metadata) properties in a DataFrame subclass, as well as overriding the _constructor to return the custom subclass. Requires a pandas import as pd and prior definition of any properties that should be carried through manipulations. Custom properties enable extensions that are retained or reset according to design.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass SubclassedDataFrame2(pd.DataFrame):\n\n    # temporary properties\n    _internal_names = pd.DataFrame._internal_names + [\"internal_cache\"]\n    _internal_names_set = set(_internal_names)\n\n    # normal properties\n    _metadata = [\"added_property\"]\n\n    @property\n    def _constructor(self):\n        return SubclassedDataFrame2\n```\n\n----------------------------------------\n\nTITLE: Deprecated Series DateTime Week Access\nDESCRIPTION: Example showing deprecated and recommended ways to access week information from Series.dt objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n# Deprecated\nSeries.dt.week\nSeries.dt.weekofyear\n\n# Recommended\nSeries.dt.isocalendar().week\n```\n\n----------------------------------------\n\nTITLE: Fixing Recursive Loop in DataFrame and Series Aggregate Methods (Python)\nDESCRIPTION: Resolved an issue where DataFrame.aggregate and Series.aggregate could cause a recursive loop in certain cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\ndf.aggregate(custom_function)\n```\n\n----------------------------------------\n\nTITLE: Proposed Plugin-based PyArrow Converter Usage (Python)\nDESCRIPTION: Shows the intended plugin-based use for seamless PyArrow-to-pandas DataFrame interchange. After plugin registration, users can transform and filter data, then export it to PyArrow using the new interface. This relies on both pandas and the registered plugin. Inputs include the original table/data to load and query conditions; output is filtered and format-converted data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n(pandas.read_pyarrow(table)\n       .query('my_col > 0')\n       .to_pyarrow())\n```\n\n----------------------------------------\n\nTITLE: Disabling Unicode East Asian Width Display in pandas - Python\nDESCRIPTION: Shows how to reset the display option for Unicode East Asian width, restoring default alignment behavior for DataFrame and Series output. Input is the option string and boolean value; affects subsequent display behaviors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.unicode.east_asian_width\", False)\n```\n\n----------------------------------------\n\nTITLE: Hexbin Plot with Pandas\nDESCRIPTION: This example shows how to create a hexagonal bin plot using DataFrame.plot.hexbin(). Hexbin plots are useful for visualizing dense data where individual points would overlap.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(1000, 2), columns=[\"a\", \"b\"])\ndf[\"b\"] = df[\"b\"] + np.arange(1000)\n\ndf.plot.hexbin(x=\"a\", y=\"b\", gridsize=25);\n```\n\n----------------------------------------\n\nTITLE: Pandas DataFrame with DatetimeIndex Example\nDESCRIPTION: Demonstrates creating and indexing a DataFrame with datetime values\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': [1, 2, 3]}, pd.DatetimeIndex(['2011-12-31 23:59:59',\n                                                    '2012-01-01 00:00:00',\n                                                    '2012-01-01 00:00:01']))\n```\n\n----------------------------------------\n\nTITLE: Splitting a DataFrame Based on Cumulative and Rolling Logic - Pandas - Python\nDESCRIPTION: Splits a DataFrame into multiple subframes based on a cumulative sum and rolling median of a logical delineation column. Inputs require a DataFrame with a case identifier and a numeric data column. Outputs several DataFrames, each a split segment of the input. Requires familiarity with groupby, cumsum, and rolling methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    data={\n        \"Case\": [\"A\", \"A\", \"A\", \"B\", \"A\", \"A\", \"B\", \"A\", \"A\"],\n        \"Data\": np.random.randn(9),\n    }\n)\n\ndfs = list(\n    zip(\n        *df.groupby(\n            (1 * (df[\"Case\"] == \"B\"))\n            .cumsum()\n            .rolling(window=3, min_periods=1)\n            .median()\n        )\n    )\n)[-1]\n\ndfs[0]\ndfs[1]\ndfs[2]\n```\n\n----------------------------------------\n\nTITLE: Standard Integer-Based and Label-Based Slicing on Series - pandas - Python\nDESCRIPTION: Demonstrates differences between standard integer-based slicing (s[2:5]) and label-based slicing (s.loc[\"c\":\"e\"]) in a pandas Series. Shows how integer-based slicing works with position, while label-based slicing is inclusive. Useful for understanding how pandas' slicing diverges from standard Python lists. Requires a previously defined Series 's'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\ns[2:5]\n```\n\n----------------------------------------\n\nTITLE: Indexer Dtype Change for Index.get_indexer in pandas on 64-bit Windows - ipython\nDESCRIPTION: Compares the dtype of indexer arrays returned by Index.get_indexer before and after a pandas v0.19.0 change. Shows that the indexer's dtype moves from int32 (np.int_) to int64 (np.intp) on 64-bit Windows, ensuring compatibility with platform pointer sizes. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_46\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: i = pd.Index(['a', 'b', 'c'])\n\nIn [2]: i.get_indexer(['b', 'b', 'c']).dtype\nOut[2]: dtype('int32')\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: i = pd.Index(['a', 'b', 'c'])\n\nIn [2]: i.get_indexer(['b', 'b', 'c']).dtype\nOut[2]: dtype('int64')\n```\n\n----------------------------------------\n\nTITLE: Manual Join of MultiIndexed DataFrames (pre-0.24 workaround) - pandas (Python)\nDESCRIPTION: Shows how to join two DataFrames with MultiIndex by resetting indexes and merging on a common level ('key'), then rebuilding the MultiIndex after merge. Useful for compatibility with earlier pandas versions. Handles alignment and index reconstruction manually. Inputs are DataFrames after index reset; output is DataFrame with composite MultiIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\npd.merge(left.reset_index(), right.reset_index(),\n         on=['key'], how='inner').set_index(['key', 'X', 'Y'])\n```\n\n----------------------------------------\n\nTITLE: String Case Conversion - Stata\nDESCRIPTION: Illustrates Stata's functions for changing string case: 'strupper', 'strlower', and 'strproper' (and their Unicode variants). Also shows building a small data set. Input: String column named 'string'. Output: New columns with uppercase, lowercase, and title case transformations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_19\n\nLANGUAGE: stata\nCODE:\n```\nclear\\ninput str20 string\\n\"John Smith\"\\n\"Jane Cook\"\\nend\\n\\ngenerate upper = strupper(string)\\ngenerate lower = strlower(string)\\ngenerate title = strproper(string)\\nlist\n```\n\n----------------------------------------\n\nTITLE: Reading Tab-Delimited Data without Headers in Pandas\nDESCRIPTION: Shows how to read a tab-delimited CSV file without column names using pandas read_csv or read_table functions. This is similar to using Excel's Text Import Wizard for custom delimiters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntips = pd.read_csv(\"tips.csv\", sep=\"\\t\", header=None)\n\n# alternatively, read_table is an alias to read_csv with tab delimiter\ntips = pd.read_table(\"tips.csv\", header=None)\n```\n\n----------------------------------------\n\nTITLE: Deleting a Saved Plot File in Python (Suppressed Output)\nDESCRIPTION: This suppressed snippet uses the os module to delete the previously saved plot image file ('no2_concentrations.png'). import os loads the library, and os.remove deletes the file. Intended to clean up after a plot has been saved in the documentation workflow; has no effect outside this tutorial context.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.remove(\"no2_concentrations.png\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Copy-on-Modify Semantics in R Vectors - R\nDESCRIPTION: Compares pandas and R semantics by demonstrating R's copy-on-modify behavior: copying an object to a new variable, mutating the copy, and confirming the original is unchanged. Requires R. Input is a numeric vector (x), copied to y, and y[[1]] is mutated. Output: x remains unaffected, y changes. Highlights a major difference between R and Python objects (default mutability/copy behaviors).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_18\n\nLANGUAGE: r\nCODE:\n```\nx <- c(1, 2, 3)\ny <- x\ny[[1]] <- 10  # does not modify x\n```\n\n----------------------------------------\n\nTITLE: Filtering Series by Time Using between_time - pandas - IPython\nDESCRIPTION: Illustrates filtering of a Series indexed by DateTime using between_time, where only fixed set of time strings (e.g., HH:MMam/pm) are supported. Shows result output and highlights error handling for invalid date string parsing. Requires a Series (s) with a DatetimeIndex. Shows resulting filtered Series and error on invalid argument.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_20\n\nLANGUAGE: ipython\nCODE:\n```\nIn [107]: s = pd.Series(range(10), pd.date_range('2015-01-01', freq='H', periods=10))\n\nIn [108]: s.between_time(\"7:00am\", \"9:00am\")\nOut[108]:\n2015-01-01 07:00:00    7\n2015-01-01 08:00:00    8\n2015-01-01 09:00:00    9\nFreq: H, Length: 3, dtype: int64\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: s.between_time('20150101 07:00:00','20150101 09:00:00')\nValueError: Cannot convert arg ['20150101 07:00:00'] to a time.\n```\n\n----------------------------------------\n\nTITLE: Checking data types of Series elements in Python\nDESCRIPTION: Shows how to check the data type of elements in a pandas Series after iterating.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3])\ns\ntype(list(s)[0])\n```\n\n----------------------------------------\n\nTITLE: Parsing CSV with Custom Converters in Python\nDESCRIPTION: Demonstrates how file parsers no longer coerce to float or bool for columns that have custom converters specified in Pandas 0.9.1.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.9.1.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport io\n\ndata = ('A,B,C\\n'\n        '00001,001,5\\n'\n        '00002,002,6')\npd.read_csv(io.StringIO(data), converters={'A': lambda x: x.strip()})\n```\n\n----------------------------------------\n\nTITLE: Applying Function to Nested Lists with Pandas DataFrame - Python\nDESCRIPTION: Demonstrates how to convert nested lists within DataFrame columns into Series objects and concatenate them using pd.concat. It defines a helper function to convert each sublist into a Series and applies it row-wise using DataFrame.iterrows, before merging the results by index. Dependencies: pandas. Input is a DataFrame with columns of lists; output is a concatenated MultiIndex DataFrame where each nested list is expanded as a Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    data={\n        \"A\": [[2, 4, 8, 16], [100, 200], [10, 20, 30]],\n        \"B\": [[\"a\", \"b\", \"c\"], [\"jj\", \"kk\"], [\"ccc\"]],\n    },\n    index=[\"I\", \"II\", \"III\"],\n)\n\ndef SeriesFromSubList(aList):\n    return pd.Series(aList)\n\ndf_orgz = pd.concat(\n    {ind: row.apply(SeriesFromSubList) for ind, row in df.iterrows()}\n)\ndf_orgz\n```\n\n----------------------------------------\n\nTITLE: Aggregating with groupby and as_index=False Option with pandas in Python\nDESCRIPTION: Shows how setting as_index=False in pandas groupby allows group keys (here, 'kind') to appear as columns rather than index in the aggregation output. This is useful for flat, columnar outputs. Requires pandas and a suitable DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nanimals.groupby(\"kind\", as_index=False).sum()\n```\n\n----------------------------------------\n\nTITLE: Documenting a Function with Inline Code References in Python Docstrings\nDESCRIPTION: Illustrates the use of inline code (using double backticks or Sphinx cross-referencing syntax) for parameter, class, method, and function references in a docstring. The function 'add_values' returns the sum of an iterable, referencing standard Python and pandas objects in the docstring as per the project's reStructuredText conventions. This snippet requires only Python built-ins and aims to make documentation readable and linked in Sphinx. Input: array-like; Output: sum of the values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef add_values(arr):\n    \"\"\"\n    Add the values in ``arr``.\n\n    This is equivalent to Python ``sum`` of :meth:`pandas.Series.sum`.\n\n    Some sections are omitted here for simplicity.\n    \"\"\"\n    return sum(arr)\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: DataFrame.all with bool_only=True on Subsets and Full DataFrame (ipython)\nDESCRIPTION: This snippet depicts the previous outputs of DataFrame.all using bool_only=True. The first invocation on the full DataFrame returns only column 'C', omitting others. The second invocation on a selected subset returns 'B' and 'C' columns with their boolean reduction results. This highlights the prior inconsistency in behavior when applying reductions on the full vs selective column sets. Output is a Series (column-wise booleans).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_7\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: df.all(bool_only=True)\nOut[5]:\nC    True\ndtype: bool\n\nIn [6]: df[[\"B\", \"C\"]].all(bool_only=True)\nOut[6]:\nB    False\nC    True\ndtype: bool\n```\n\n----------------------------------------\n\nTITLE: Using Index.union with Changed Sort Parameter in Python\nDESCRIPTION: The default sort value for Index.union has changed from True to None. The behavior remains the same: the result is sorted unless specific conditions are met. This change allows sort=True to mean \"always sort\" in a future release.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nIndex.union(sort=None)  # New default, behaves like previous sort=True\n```\n\n----------------------------------------\n\nTITLE: Default Dtype Assignment in DataFrame Construction - Pandas - Python\nDESCRIPTION: Shows construction of DataFrames with int64 dtypes as default on all platforms when explicit dtype is not specified. Multiple patterns are demonstrated: single list, dictionary, and scalar with index. Notes that NumPy array-based construction can still default to int32 on 32-bit platforms. Demonstrates inspection of resulting dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_4\n\nLANGUAGE: ipython\nCODE:\n```\nIn [21]: pd.DataFrame([1, 2], columns=['a']).dtypes\nOut[21]:\na    int64\ndtype: object\n\nIn [22]: pd.DataFrame({'a': [1, 2]}).dtypes\nOut[22]:\na    int64\ndtype: object\n\nIn [23]: pd.DataFrame({'a': 1}, index=range(2)).dtypes\nOut[23]:\na    int64\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Detecting and Removing Duplicates in Index - Pandas Python\nDESCRIPTION: Demonstrates the use of duplicated and drop_duplicates methods on a pandas Index to identify and remove duplicate entries. Requires pandas and a one-dimensional Index. duplicated returns a boolean mask showing which elements are duplicates; drop_duplicates returns a new Index with only the first occurrence of each value. Input is an Index; outputs are a boolean array and deduplicated Index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([1, 2, 3, 4, 1, 2])\nidx\nidx.duplicated()\nidx.drop_duplicates()\n```\n\n----------------------------------------\n\nTITLE: DataFrame GroupBy Forward Fill Without Group Labels (Python)\nDESCRIPTION: Demonstrates that DataFrameGroupBy fill methods (ffill, bfill, pad, backfill) now return only the filled values, without including the group labels in the result. Requires a DataFrame grouped by a column. The new output aligns with other groupby transforms, returning a DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_10\n\nLANGUAGE: ipython\nCODE:\n```\ndf = pd.DataFrame({\"a\": [\"x\", \"y\"], \"b\": [1, 2]})\ndf\n```\n\nLANGUAGE: ipython\nCODE:\n```\ndf.groupby(\"a\").ffill()\n```\n\n----------------------------------------\n\nTITLE: Alternative Method for Group Minimums in Python\nDESCRIPTION: An alternative approach to find the row with the minimum value in each group using sort and first. This technique sorts the DataFrame and then takes the first row from each group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf.sort_values(by=\"BBB\").groupby(\"AAA\", as_index=False).first()\n```\n\n----------------------------------------\n\nTITLE: Casting Datetime to Object and Handling NaT/NaN - Pandas - Python\nDESCRIPTION: Casts a Series of datetime objects (with some NaNs) from datetime64[ns] to object dtype. Shows that NaT is converted back to np.nan upon casting to object. This is relevant for users needing to handle missing values when converting back from native datetime types for serialization or processing. Uses pandas, NumPy, and the datetime module.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\ns = pd.Series([datetime.datetime(2001, 1, 2, 0, 0) for i in range(3)])\ns.dtype\ns[1] = np.nan\ns\ns.dtype\ns = s.astype('O')\ns\ns.dtype\n```\n\n----------------------------------------\n\nTITLE: Converting a Panel to an xarray DataArray\nDESCRIPTION: Example showing how to convert a deprecated Panel object to an xarray DataArray using the to_xarray() method, which is another recommended alternative for working with 3D data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_20\n\nLANGUAGE: ipython\nCODE:\n```\np.to_xarray()\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame with MultiIndex Columns by a Specific Key in pandas (Python)\nDESCRIPTION: Shows explicitly how to sort DataFrames with MultiIndex columns using the sort_values method with by set to a tuple specifying all levels. Input: DataFrame with MultiIndex columns, parameter: column tuple. Output: sorted DataFrame. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\ndf1.columns = pd.MultiIndex.from_tuples(\n    [(\"a\", \"one\"), (\"a\", \"two\"), (\"b\", \"three\")]\n)\ndf1.sort_values(by=(\"a\", \"two\"))\n```\n\n----------------------------------------\n\nTITLE: Accessing Scalar Timedelta Attributes - pandas - Python\nDESCRIPTION: Accesses days and seconds properties of pandas Timedelta scalar values, including behavior of negative durations. Highlights direct access to base fields.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ntds = pd.Timedelta(\"31 days 5 min 3 sec\")\ntds.days\ntds.seconds\n(-tds).seconds\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Index Operations in Pandas\nDESCRIPTION: Shows operations on Index objects including slicing and type preservation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ni = pd.Index([1, 2, 3, 'a', 'b', 'c'])\ni[[0, 1, 2]]\ni.drop(['a', 'b', 'c'])\ni[[0, 1, 2]].astype(np.int_)\n```\n\n----------------------------------------\n\nTITLE: Writing Multiple DataFrames to Separate Excel Sheets\nDESCRIPTION: Demonstrates using ExcelWriter to write multiple DataFrames to different sheets within a single Excel file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_144\n\nLANGUAGE: python\nCODE:\n```\nwith pd.ExcelWriter(\"path_to_file.xlsx\") as writer:\n    df1.to_excel(writer, sheet_name=\"Sheet1\")\n    df2.to_excel(writer, sheet_name=\"Sheet2\")\n```\n\n----------------------------------------\n\nTITLE: Getting Number of Rows in Stored Table with get_storer - pandas HDFStore - Python\nDESCRIPTION: This snippet shows how to programmatically access the underlying storer object of a table in HDFStore to retrieve the number of stored rows. It uses store.get_storer('df_dc').nrows. Dependencies: pandas, HDFStore. Input: table name. Output: integer count of rows in the table.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_194\n\nLANGUAGE: python\nCODE:\n```\nstore.get_storer(\"df_dc\").nrows\n```\n\n----------------------------------------\n\nTITLE: Creating MultiIndex from Tuples in Python\nDESCRIPTION: Example showing how to create a MultiIndex object from tuples using pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npd.MultiIndex.from_tuples([('a',), ('b',)])\n```\n\n----------------------------------------\n\nTITLE: Deprecating Functions Using pandas.util._decorators in Python\nDESCRIPTION: This snippet demonstrates the use of the deprecate utility from pandas.util._decorators to mark a function as deprecated. It replaces the target function with a new implementation, specifying the name and the version in which the deprecation takes effect. The dependencies include pandas and access to the _decorators module. No parameters in this usage; the effect is to warn users at runtime if a deprecated function is called.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.util._decorators import deprecate\n\ndeprecate('old_func', 'new_func', '1.1.0')\n```\n\n----------------------------------------\n\nTITLE: Accessing Components of Timedelta Series - pandas - Python\nDESCRIPTION: Uses the .components attribute of TimedeltaIndex (.dt.components for Series) to retrieve DataFrame with split time components (days, hours, minutes, etc). Shows further access to components columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntd.dt.components\ntd.dt.components.seconds\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Current and Future DataFrame.prod() Behavior - pandas - Python\nDESCRIPTION: These code blocks show the output of df.prod() on a DataFrame containing both numeric and datetime columns, contrasting current silent dropping of unsupported columns with future TypeError-raising behavior. Examples include both all-column reductions and reductions on a selected column. Requires pandas >=1.3.0 for new behavior. Key parameters: the DataFrame instance (df). Output is a Series of products or TypeError. Constraints: future versions will raise on invalid columns instead of ignoring them.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_22\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: df.prod()\nOut[3]:\nA    24\ndtype: int64\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: df.prod()\n...\nTypeError: 'DatetimeArray' does not implement reduction 'prod'\n\nIn [5]: df[[\"A\"]].prod()\nOut[5]:\nA    24\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Running Pandas Tests via pd.test() Method (Python)\nDESCRIPTION: Invokes the built-in test suite for pandas by calling the test() method on an imported pandas instance, enabling users to run tests within a live Python environment or script. Requires an installed pandas with appropriate test dependencies. No arguments specified; runs standard pandas tests.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\npd.test()\n```\n\n----------------------------------------\n\nTITLE: GroupBy Operations Bug Fixes in Pandas 0.25.2\nDESCRIPTION: Fixes for DataFrameGroupBy.quantile with list of quantiles and timezone preservation in GroupBy shift, bfill, and ffill operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.2.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nDataFrameGroupBy.quantile()\nGroupBy.shift()\nGroupBy.bfill()\nGroupBy.ffill()\n```\n\n----------------------------------------\n\nTITLE: Creating HDFStore and Appending DataFrame with Data Columns in Pandas\nDESCRIPTION: Demonstrates how to create an HDFStore, initialize a DataFrame with random values and string columns, and append it to the store with specified data columns that can be queried.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstore = pd.HDFStore(\"store.h5\")\ndf = pd.DataFrame(\n    np.random.randn(8, 3),\n    index=pd.date_range(\"1/1/2000\", periods=8),\n    columns=[\"A\", \"B\", \"C\"],\n)\ndf[\"string\"] = \"foo\"\ndf.loc[df.index[4:6], \"string\"] = np.nan\ndf.loc[df.index[7:9], \"string\"] = \"bar\"\ndf[\"string2\"] = \"cool\"\ndf\n\n# on-disk operations\nstore.append(\"df\", df, data_columns=[\"B\", \"C\", \"string\", \"string2\"])\nstore.select(\"df\", \"B>0 and string=='foo'\")\n\n# this is in-memory version of this type of selection\ndf[(df.B > 0) & (df.string == \"foo\")]\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: Indexing Series with Incorrect Key Types - ipython\nDESCRIPTION: Shows how pandas previously raised TypeError or KeyError when accessing a Series with an incorrect key or type using both direct indexing and .loc. Accessing with an incompatible key type yielded TypeError, while non-existent keys caused KeyError. Demonstrates expectations and error messages prior to pandas 1.1.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_10\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: ser1[1.5]\\n...\\nTypeError: cannot do label indexing on Int64Index with these indexers [1.5] of type float\\n\\nIn [4] ser1[\"foo\"]\\n...\\nKeyError: 'foo'\\n\\nIn [5]: ser1.loc[1.5]\\n...\\nTypeError: cannot do label indexing on Int64Index with these indexers [1.5] of type float\\n\\nIn [6]: ser1.loc[\"foo\"]\\n...\\nKeyError: 'foo'\\n\\nIn [7]: ser2.loc[1]\\n...\\nTypeError: cannot do label indexing on DatetimeIndex with these indexers [1] of type int\\n\\nIn [8]: ser2.loc[pd.Timestamp(0)]\\n...\\nKeyError: Timestamp('1970-01-01 00:00:00')\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame with Multiple Columns in Python\nDESCRIPTION: Demonstrates how to sort a DataFrame by multiple columns with different ascending/descending orders using the new sort functionality in Pandas 0.9.1.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.9.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randint(0, 2, (6, 3)),\n                  columns=['A', 'B', 'C'])\n\ndf.sort(['A', 'B'], ascending=[1, 0])\n```\n\n----------------------------------------\n\nTITLE: Summing Empty and All-NA Series in pandas 0.21.x - Python\nDESCRIPTION: This code demonstrates the behavior of .sum() in pandas 0.21.x where summing an empty Series or a Series of all NaN values returns nan. This illustrates the legacy output before the change in pandas 0.22. Dependencies: pandas as pd, numpy as np. Input: empty Series or Series([np.nan]). Output: nan.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: pd.Series([]).sum()\nOut[1]: nan\n\nIn [2]: pd.Series([np.nan]).sum()\nOut[2]: nan\n```\n\n----------------------------------------\n\nTITLE: Renaming pd.ordered_merge() to pd.merge_ordered()\nDESCRIPTION: The top-level pandas function pd.ordered_merge() has been renamed to pd.merge_ordered(). The original name will be removed in a future version.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\npd.ordered_merge()\n```\n\nLANGUAGE: Python\nCODE:\n```\npd.merge_ordered()\n```\n\n----------------------------------------\n\nTITLE: Selecting a Single Column from a DataFrame with pandas (Python)\nDESCRIPTION: This snippet demonstrates how to select a single column ('Age') from a pandas DataFrame using bracket notation. The resulting object is a pandas Series containing the 'Age' values for each Titanic passenger. The code assumes the DataFrame titanic exists, and outputs the first few values of the 'Age' column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nages = titanic[\"Age\"]\nages.head()\n```\n\n----------------------------------------\n\nTITLE: Constructing TimedeltaIndex with Inferred Frequency - pandas - Python\nDESCRIPTION: Creates a TimedeltaIndex where the frequency is inferred from evenly spaced string inputs by passing freq='infer'. Results in a regular time delta step within the Index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\npd.TimedeltaIndex([\"0 days\", \"10 days\", \"20 days\"], freq=\"infer\")\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame and Evaluating Expressions with eval - pandas - Python\nDESCRIPTION: Creates a DataFrame using numpy linspace and range, then adds columns by evaluating expressions. Demonstrates both inplace and out-of-place column assignment with eval (including use of the inplace keyword). Requires pandas (pd), numpy (np), and an existing DataFrame (df). Illustrates the addition of new columns and highlights the future change in default behavior for eval.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': np.linspace(0, 10, 5), 'b': range(5)})\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.eval('c = a + b', inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Upcasting in Floating Point DataFrames During Boolean Indexing - Pandas - Python\nDESCRIPTION: Shows that DataFrames using float dtypes preserve their data types when filtered through boolean indexing, unlike integer-based DataFrames which may be upcast to float. Demonstrates assigning columns to specified float32 dtypes and verifying that type preservation occurs through selection operations. Highlights a safe path for avoiding dtype upcasting surprises.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_6\n\nLANGUAGE: ipython\nCODE:\n```\nIn [31]: df4 = df3.copy()\n\nIn [32]: df4['A'] = df4['A'].astype('float32')\n\nIn [33]: df4.dtypes\nOut[33]:\nA    float32\nB    float64\nC    float64\nD    float16\nE      int32\ndtype: object\n\nIn [34]: casted = df4[df4 > 0]\n\nIn [35]: casted\nOut[35]:\n          A         B    C    D  E\n0       NaN       NaN  NaN  1.0  1\n1       NaN  0.567020  1.0  1.0  1\n2       NaN  0.276232  2.0  1.0  1\n3       NaN       NaN  3.0  1.0  1\n4  1.933792       NaN  4.0  1.0  1\n5       NaN  0.113648  5.0  1.0  1\n6       NaN       NaN  6.0  1.0  1\n7       NaN  0.524988  7.0  1.0  1\n\nIn [36]: casted.dtypes\nOut[36]:\nA    float32\nB    float64\nC    float64\nD    float16\nE      int32\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Loading and Using I/O Plugins in Pandas (Python)\nDESCRIPTION: Demonstrates how an end user installs and loads I/O plugin connectors in pandas, enabling dynamic method injection for read and write operations using the plugin API. Dependencies include the pandas library at a version supporting `load_io_plugins()` and the respective connector package (e.g., for DuckDB or Hive). The example calls `load_io_plugins`, queries data via a custom `read_duckdb` method, and writes data to Hive using `to_hive`, illustrating the seamless augmentation of pandas' I/O functionality. Expected input includes standard DataFrame-compatible arguments for the corresponding methods, while output is a loaded/processed DataFrame or result according to plugin semantics.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas\n\npandas.load_io_plugins()\n\ndf = pandas.read_duckdb(\"SELECT * FROM 'dataset.parquet';\")\n\ndf.to_hive(hive_conn, \"hive_table\")\n```\n\n----------------------------------------\n\nTITLE: Using GroupBy Transform to Adjust Values by Group Mean in Pandas\nDESCRIPTION: This code demonstrates how to use the Pandas groupby.transform method to subtract the mean total bill amount for each smoking status group from the individual bill amounts. The result is stored in a new column 'adj_total_bill', showing the deviation of each bill from its group mean.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/transform.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngb = tips.groupby(\"smoker\")[\"total_bill\"]\ntips[\"adj_total_bill\"] = tips[\"total_bill\"] - gb.transform(\"mean\")\ntips\n```\n\n----------------------------------------\n\nTITLE: Multiplying Period Frequency in pandas Period and PeriodIndex (Python)\nDESCRIPTION: These snippets show the creation and manipulation of Period and PeriodIndex objects using 'multiplied' frequency notation (e.g., '3D' for three-day spans). It demonstrates period arithmetic (addition, subtraction) and conversion to timestamps, including both start and end. Inputs: period string, freq; outputs: period object, shifted periods, and corresponding timestamps. This offers more flexibility for representing spans of arbitrary length in period-based indexing. Requires pandas 0.17.0+.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\np = pd.Period(\"2015-08-01\", freq=\"3D\")\np\np + 1\np - 2\np.to_timestamp()\np.to_timestamp(how=\"E\")\n```\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.period_range(\"2015-08-01\", periods=4, freq=\"2D\")\nidx\nidx + 1\n```\n\n----------------------------------------\n\nTITLE: Complex MultiIndex Selection on Both Axes - pandas Python\nDESCRIPTION: Provides examples of advanced multi-level index slicing, demonstrating complex selections on both row and column MultiIndexes. It combines scalar and slicing selectors on the axes, using either direct values or IndexSlice objects. Supports selection by exact label or comprehensive subsetting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.loc['A1', (slice(None), 'foo')]\ndf.loc[idx[:, :, ['C1', 'C3']], idx[:, 'foo']]\n```\n\n----------------------------------------\n\nTITLE: Plotting Series with Logarithmic Y-Axis - pandas/matplotlib - Python\nDESCRIPTION: This example generates a pandas Series 'ts' with 1000 exponentially accumulated random values indexed by dates, then plots it using a logarithmic y-axis ('logy=True') to better display data spanning multiple orders of magnitude. Requires pandas, matplotlib, numpy, and no missing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nts = pd.Series(np.random.randn(1000), index=pd.date_range(\"1/1/2000\", periods=1000))\nts = np.exp(ts.cumsum())\n\n@savefig series_plot_logy.png\nts.plot(logy=True);\n```\n\n----------------------------------------\n\nTITLE: Swapping and Reordering MultiIndex Levels - pandas - Python\nDESCRIPTION: Illustrates changing the order of MultiIndex levels with swaplevel (pairwise) and reorder_levels (arbitrary permutations), enabling different data aggregation or view strategies. Requires DataFrame and MultiIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndf[:5]\ndf[:5].swaplevel(0, 1, axis=0)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf[:5].reorder_levels([1, 0], axis=0)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Function with GroupBy.apply Only Once (Python)\nDESCRIPTION: Illustrates that a function passed to DataFrameGroupBy.apply is now called only once for the first group, aligning with intuitive expectations and avoiding potential issues for functions with side effects. Requires a DataFrame grouped by one or more columns and a function taking the group as argument. The group names are printed for demonstration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_5\n\nLANGUAGE: ipython\nCODE:\n```\ndf = pd.DataFrame({\"a\": [\"x\", \"y\"], \"b\": [1, 2]})\ndf\n```\n\nLANGUAGE: ipython\nCODE:\n```\ndef func(group):\n    print(group.name)\n    return group\n```\n\n----------------------------------------\n\nTITLE: Extracting Substring from Pandas Series using Array Indexing\nDESCRIPTION: Demonstrates extracting a substring from a string column in a Pandas DataFrame using Python's array slice notation. Extracts the first character from the 'sex' column using [0:1] slice syntax.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/extract_substring.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntips[\"sex\"].str[0:1]\n```\n\n----------------------------------------\n\nTITLE: Cython Integration with ndarray and Function Application Over Arrays - Cython\nDESCRIPTION: Introduces use of Cython with NumPy ndarray objects for efficient computation. Functions check array types and iterate using typed variables for performance. 'apply_integrate_f' applies 'integrate_f_typed' to entire arrays, returning a NumPy array of results. Requires Cython, numpy (mported and cimported), and pre-converted float64 and int numpy arrays. Inputs must have matching lengths; assertion guards verify type and size consistency. Output is a numpy float64 array.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_7\n\nLANGUAGE: cython\nCODE:\n```\ncimport numpy as np\nimport numpy as np\nnp.import_array()\ncdef double f_typed(double x) except? -2:\n    return x * (x - 1)\ncpdef double integrate_f_typed(double a, double b, int N):\n    cdef int i\n    cdef double s, dx\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_typed(a + i * dx)\n    return s * dx\ncpdef np.ndarray[double] apply_integrate_f(np.ndarray col_a, np.ndarray col_b,\n                                           np.ndarray col_N):\n    assert (col_a.dtype == np.float64\n            and col_b.dtype == np.float64 and col_N.dtype == np.dtype(int))\n    cdef Py_ssize_t i, n = len(col_N)\n    assert (len(col_a) == len(col_b) == n)\n    cdef np.ndarray[double] res = np.empty(n)\n    for i in range(len(col_a)):\n        res[i] = integrate_f_typed(col_a[i], col_b[i], col_N[i])\n    return res\n```\n\n----------------------------------------\n\nTITLE: Fast Scalar Value Access in Pandas\nDESCRIPTION: Shows how to use at and iat methods for fast scalar value getting and setting in DataFrames and Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\ns.iat[5]\ndf.at[dates[5], 'A']\ndf.iat[3, 0]\n\ndf.at[dates[5], 'E'] = 7\ndf.iat[3, 0] = 7\n```\n\n----------------------------------------\n\nTITLE: Demonstrating GroupBy with Rolling Window DataFrame Summing Pandas Python\nDESCRIPTION: Showcases creating a DataFrame and viewing its contents, focusing on preparation for further groupby and rolling window operations to illustrate how grouped-by columns are handled in results. Requires pandas. Defines df with columns A and B, initializes data, and prints df.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1, 1, 2, 3], \"B\": [0, 1, 2, 3]})\ndf\n```\n\n----------------------------------------\n\nTITLE: Accessing Series Elements with Missing Keys - pandas - Python\nDESCRIPTION: Attempts to retrieve a value using a non-existent label, demonstrating that this results in an exception. Shows the error-handling behavior of pandas Series under label-based index access. 'okexcept' in the directive indicates error is allowed in the output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ns[\"f\"]\n```\n\n----------------------------------------\n\nTITLE: Rounded Division (Floor Division) with Timedelta Series - pandas - Python\nDESCRIPTION: Illustrates floor division (//) of Timedelta Series by a Timedelta scalar and vice versa, producing integer Series or Timedelta values. Useful for interval counting or bucketing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntd // pd.Timedelta(days=3, hours=4)\npd.Timedelta(days=3, hours=4) // td\n```\n\n----------------------------------------\n\nTITLE: Manually Aligning DataFrames Prior to NumPy Ufunc Operations - Python\nDESCRIPTION: Shows how to manually align two pandas DataFrames using the align method before performing an np.add operation. This produces results consistent with operator-based arithmetic (such as \"+\"), with matching indices and NaN values for non-overlapping rows. Dependencies: pandas and NumPy (imported as pd and np). Inputs: two unaligned DataFrames (df1 and df2); outputs: aligned DataFrames and a DataFrame showing elementwise addition aligned on indices and columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.1.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIn [8]: df1, df2 = df1.align(df2)\nIn [9]: np.add(df1, df2)\nOut[9]:\n     a    b\n0  NaN  NaN\n1  3.0  7.0\n2  NaN  NaN\n```\n\n----------------------------------------\n\nTITLE: Defining pandas.Series method: mean in Python\nDESCRIPTION: Shows how to define a custom mean method for a Series object. This snippet assumes pandas and numpy are imported. It constructs a Series and demonstrates calling mean, outputting the average value. Example highlights concise input and automatic output type inference.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass Series:\n\n    def mean(self):\n        \"\"\"\n        Compute the mean of the input.\n\n        Examples\n        --------\n        >>> ser = pd.Series([1, 2, 3])\n        >>> ser.mean()\n        2\n        \"\"\"\n        pass\n```\n\n----------------------------------------\n\nTITLE: Adding Columns with Missing Data in pandas - Python\nDESCRIPTION: This snippet adds two columns, value_x and value_y, from the outer_join DataFrame, demonstrating how pandas propagates NaN where data is missing. If either value_x or value_y is NaN for a row, the result will also be NaN in the output Series. Both columns must exist in the DataFrame, and pandas must be imported. The operation showcases automatic handling of missing data during arithmetic calculations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/missing_intro.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nouter_join[\"value_x\"] + outer_join[\"value_y\"]\n```\n\n----------------------------------------\n\nTITLE: Parsing Date Strings in Excel Read Operation\nDESCRIPTION: Demonstrates how to convert string columns that look like dates to proper datetime objects using the parse_dates parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_138\n\nLANGUAGE: python\nCODE:\n```\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", parse_dates=[\"date_strings\"])\n```\n\n----------------------------------------\n\nTITLE: Cloning and Configuring Fork for pandas Development - Shell\nDESCRIPTION: This snippet demonstrates how to clone your fork of the pandas repository, change into the cloned directory, add the canonical upstream remote, and fetch updates from upstream. It requires Git to be installed and a valid GitHub fork. Inputs are your personal GitHub username and optionally a custom directory name for the clone. Outputs are a local directory and remote configuration for upstream tracking.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/your-user-name/pandas.git pandas-yourname\\ncd pandas-yourname\\ngit remote add upstream https://github.com/pandas-dev/pandas.git\\ngit fetch upstream\n```\n\n----------------------------------------\n\nTITLE: Dividing Timedelta Series by Time Unit with Pandas (Python)\nDESCRIPTION: This snippet demonstrates dividing a Series of timedelta values by a time unit (days) using NumPy's timedelta64, resulting in a float Series representing the number of days. Requires pandas and NumPy. Input is a Series 'td'; output is a float64 Series. NaT values result in NaN. Applicable for timedelta arithmetic and normalization.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntd / np.timedelta64(1, 'D')\n```\n\n----------------------------------------\n\nTITLE: Creating a Value Counts Column and Reassigning to DataFrame with GroupBy Transform - Pandas - Python\nDESCRIPTION: Creates a DataFrame with 'Color' and 'Value' columns, computes the count of each color group, and assigns the resulting Series as a new column in the DataFrame. Uses groupby.transform(len) to return a Series aligned to the original data. Outputs the DataFrame with an added 'Counts' column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"Color\": \"Red Red Red Blue\".split(), \"Value\": [100, 150, 50, 50]}\n)\ndf\ndf[\"Counts\"] = df.groupby([\"Color\"]).transform(len)\ndf\n```\n\n----------------------------------------\n\nTITLE: Generating Dummies with Controlled Data Types using get_dummies - Python\nDESCRIPTION: Displays the creation of one-hot encoded columns using get_dummies, illustrating the new dtype argument to specify the dtype of resulting columns (e.g., bool). Requires pandas. The code shows default dtype (uint8) versus boolean dtype and extracts and prints the resulting column dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [5, 6]})\npd.get_dummies(df, columns=['c']).dtypes\npd.get_dummies(df, columns=['c'], dtype=bool).dtypes\n```\n\n----------------------------------------\n\nTITLE: Aligning Arithmetic Operators on Different Indexes with pandas Series and DataFrame - Python\nDESCRIPTION: Demonstrates that arithmetic operators between Series or DataFrames align on their index, and shows an example of how mismatched indexes operate. Requires pandas, with input Series/DataFrames having potentially different index labels. The result is computed pairwise alignment; missing indexes result in NaN.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([1, 2, 3], index=list(\"ABC\"))\ns2 = pd.Series([2, 2, 2], index=list(\"ABD\"))\ns1 + s2\n```\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame([1, 2, 3], index=list(\"ABC\"))\ndf2 = pd.DataFrame([2, 2, 2], index=list(\"ABD\"))\ndf1 + df2\n```\n\n----------------------------------------\n\nTITLE: Installing Airspeed Velocity (asv) for Pandas Benchmarks (Bash)\nDESCRIPTION: Explains how to install the latest asv (airspeed-velocity) benchmarking framework from GitHub using pip. Prerequisites include Python and pip. asv is essential for running, comparing, and reporting performance benchmarks in the pandas project.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/airspeed-velocity/asv\n```\n\n----------------------------------------\n\nTITLE: Handling Duplicate Labels in cut Function (Python)\nDESCRIPTION: The cut function now handles cases where the labels argument contains duplicates without raising an error.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\npd.cut(data, bins, labels=['A', 'A', 'B'])\n```\n\n----------------------------------------\n\nTITLE: Subclassing pandas ExtensionArray Test Base - Python\nDESCRIPTION: Demonstrates how to subclass a test base from pandas extension tests to reuse test infrastructure for custom extension array implementations. Requires import of the specific test base from pandas.tests.extension.base. No external dependencies beyond standard pandas testing utilities. The subclass should implement custom test logic or can use 'pass' to inherit all test cases as-is.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.tests.extension import base\n\n\nclass TestConstructors(base.BaseConstructorsTests):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Reindexing DateTime Indexed DataFrame with String Tolerance using pandas - Python\nDESCRIPTION: Shows how to use string representations (e.g., '1 day') for the tolerance argument when reindexing with nearest on a DateTime index. Demonstrates the practical application after setting a datetime index. Requires pandas; input is a DataFrame with a datetime index and new datetime labels; output is a DataFrame with nearest reindexing limited by time tolerance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf = df.set_index(\"t\")\ndf.reindex(pd.to_datetime([\"1999-12-31\"]), method=\"nearest\", tolerance=\"1 day\")\n```\n\n----------------------------------------\n\nTITLE: Find Substring Position - Stata\nDESCRIPTION: Uses 'strpos' to create a column indicating the position of the substring 'ale' in 'sex'. No dependencies. Input: Data with 'sex' column. Output: New column 'str_position' with integer results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_16\n\nLANGUAGE: stata\nCODE:\n```\ngenerate str_position = strpos(sex, \"ale\")\n```\n\n----------------------------------------\n\nTITLE: GroupBy nth Method Sorting Behavior Demonstrations in Pandas (Python)\nDESCRIPTION: Illustrates previous and new sorting behaviors of the nth method following groupby. Requires Pandas and NumPy. DataFrame is built with several numeric columns and is grouped by an integer column before applying nth for a fixed position under both sorted and unsorted conditions. Inputs are a DataFrame, column to group by, and sort flag; outputs are DataFrames reflecting row selection per group and sorting behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(1234)\ndf = pd.DataFrame(np.random.randn(100, 2), columns=[\"a\", \"b\"])\ndf[\"c\"] = np.random.randint(0, 4, 100)\ndf.groupby(\"c\", sort=True).nth(1)\ndf.groupby(\"c\", sort=False).nth(1)\n```\n\n----------------------------------------\n\nTITLE: Using DataFrame.where and DataFrame.mask Methods in Python\nDESCRIPTION: Demonstrates the new where and mask methods for selecting values in a DataFrame based on boolean conditions, including partial selection and replacement.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.9.1.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(5, 3), columns=['A', 'B', 'C'])\n\ndf\n\ndf[df['A'] > 0]\n\ndf[df > 0]\n\ndf.where(df > 0)\n\ndf.where(df > 0, -df)\n\ndf2 = df.copy()\ndf2[df2[1:4] > 0] = 3\ndf2\n\ndf.mask(df <= 0)\n```\n\n----------------------------------------\n\nTITLE: Preserving Categorical Dtype in GroupBy Aggregations (Python)\nDESCRIPTION: Demonstrates how categorical dtype columns are now preserved during groupby aggregations, rather than being converted to object dtype. Requires Pandas and creation of a Categorical column for grouping and aggregation. Shows the dtype before and after groupby operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_8\n\nLANGUAGE: ipython\nCODE:\n```\ncat = pd.Categorical([\"foo\", \"bar\", \"bar\", \"qux\"], ordered=True)\ndf = pd.DataFrame({'payload': [-1, -2, -1, -2], 'col': cat})\ndf\ndf.dtypes\n```\n\nLANGUAGE: ipython\nCODE:\n```\ndf.groupby('payload').first().col.dtype\n```\n\n----------------------------------------\n\nTITLE: Creating Pie Chart with Small Values in pandas\nDESCRIPTION: Creates a pandas Series with small values (sum less than 1.0) and plots it as a pie chart. The values are automatically rescaled to sum to 1 when plotting. Demonstrates automatic normalization of values in pie charts.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nseries = pd.Series([0.1] * 4, index=[\"a\", \"b\", \"c\", \"d\"], name=\"series2\")\n\nseries.plot.pie(figsize=(6, 6));\n```\n\n----------------------------------------\n\nTITLE: Rendering Maintainer Cards with Jinja2 Templating - HTML/Jinja2\nDESCRIPTION: This snippet uses Jinja2 templating within an HTML structure to dynamically generate cards displaying active maintainers for the pandas project. It iterates over the 'maintainers.active' list, fetching GitHub user information to display avatars, names (optionally linked to a blog), and GitHub profile links. Requires the maintainers data structure provided in the Jinja2 template context with associated GitHub user information. Inputs are lists of usernames and associated profile data. Output is a card group of maintainers, each in a Bootstrap card layout.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/team.md#_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n<div class=\\\"card-group maintainers\\\">\\n    {% for username in maintainers.active %}\\n        {% set person = maintainers.github_info.get(username) %}\\n        <div class=\\\"card\\\">\\n            <img class=\\\"card-img-top\\\" alt=\\\"\\\" src=\\\"{{ person.avatar_url }}\\\"/>\\n            <div class=\\\"card-body\\\">\\n                <h6 class=\\\"card-title\\\">\\n                    {% if person.blog %}\\n                        <a href=\\\"{{ person.blog }}\\\">\\n                            {{ person.name or person.login }}\\n                        </a>\\n                    {% else %}\\n                        {{ person.name or person.login }}\\n                    {% endif %}\\n                </h6>\\n                <p class=\\\"card-text small\\\"><a href=\\\"{{ person.html_url }}\\\">{{ person.login }}</a></p>\\n            </div>\\n        </div>\\n    {% endfor %}\\n</div>\n```\n\n----------------------------------------\n\nTITLE: Converting Existing Series to StringDtype with astype (Python)\nDESCRIPTION: Illustrates converting an existing pandas Series of default dtype to StringDtype by using .astype('string'). Useful for datasets loaded or created with heterogeneous dtypes, enabling downstream string operations and consistent text handling. Input is a generic Series, output is the same Series with StringDtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"b\", \"c\"])\ns\ns.astype(\"string\")\n```\n\n----------------------------------------\n\nTITLE: DataFrame Column Order (Previous Behavior) - Pandas - Python\nDESCRIPTION: Shows the previous output of DataFrame construction from list-of-dicts, where columns are sorted lexicographically, not by insertion order. Demonstrates input/output of pd.DataFrame before column ordering change.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: pd.DataFrame(data)\\nOut[1]:\\n   age finances      hobby  name state\\n0   18      NaN        NaN   Joe    NY\\n1   19      NaN  Minecraft  Jane    KY\\n2   20     good        NaN  Jean    OK\n```\n\n----------------------------------------\n\nTITLE: String Pattern Extraction Output Changes with str.extract (Python)\nDESCRIPTION: Demonstrates the updated return value of pandas str.extract when extracting groups from strings. In new behavior, always returns DataFrame unless expand=False. Illustrates both the older Series result (single group) and new DataFrame default, with code to revert using expand=False. Inputs are pandas Series and regex patterns; outputs are either Series or DataFrame. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['number 10', '12 eggs'])\nextracted = s.str.extract(r'.*(\\d\\d).*')\nextracted\ntype(extracted)\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['number 10', '12 eggs'])\nextracted = s.str.extract(r'.*(\\d\\d).*', expand=False)\nextracted\ntype(extracted)\n```\n\n----------------------------------------\n\nTITLE: Rolling Apply Returning Series on DataFrame - Pandas Python\nDESCRIPTION: Illustrates a rolling window computation across DataFrame slices, applying a custom function that generates and returns a Series. The function cumulates and transforms the sum of columns 'A' and 'B', applies a constant, and extracts the last value as the result. Dependencies: pandas, numpy. The code generates a DataFrame with random values, defines a calculation function, and builds a Series with the processed results by iterating over windowed slices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    data=np.random.randn(2000, 2) / 10000,\n    index=pd.date_range(\"2001-01-01\", periods=2000),\n    columns=[\"A\", \"B\"],\n)\ndf\n\ndef gm(df, const):\n    v = ((((df[\"A\"] + df[\"B\"]) + 1).cumprod()) - 1) * const\n    return v.iloc[-1]\n\ns = pd.Series(\n    {\n        df.index[i]: gm(df.iloc[i: min(i + 51, len(df) - 1)], 5)\n        for i in range(len(df) - 50)\n    }\n)\ns\n```\n\n----------------------------------------\n\nTITLE: DataFrame Operations and Methods - Python\nDESCRIPTION: Various DataFrame method fixes including replace(), to_csv(), loc[], fillna(), update(), nsmallest(), and other core pandas operations that address regressions in functionality.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.4.3.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.replace()\nDataFrame.to_csv()\nDataFrame.loc[]\nDataFrame.fillna()\nDataFrame.update()\nDataFrame.nsmallest()\n```\n\n----------------------------------------\n\nTITLE: Summing with min_count=1 to Get NaN for Empty Series - Python\nDESCRIPTION: Shows how to restore the old behavior by specifying min_count=1 so that sum returns NaN if there are no valid values. Inputs: empty Series or Series([np.nan]). Output: nan. Dependencies: pandas as pd, numpy as np. Demonstrates use of the new min_count parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.Series([]).sum(min_count=1)\n```\n\n----------------------------------------\n\nTITLE: Percentile Precision Update in pandas describe() Output - Python\nDESCRIPTION: Example setup for demonstrating changes in percentile identifier precision in describe() index output, as of pandas 0.19.0. Requires pandas. Input: numeric Series and DataFrame. Intended to observe output difference in index precision without changes to the describe() method call.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([0, 1, 2, 3, 4])\ndf = pd.DataFrame([0, 1, 2, 3, 4])\n```\n\n----------------------------------------\n\nTITLE: Declaring py.typed Status Using Python - none\nDESCRIPTION: Demonstrates a Shell command (via the python executable) to add a py.typed marker file into pandas' installation directory. This helps type checkers recognize pandas as a py.typed library for experimentation, even though pandas is not formally registered as one. Requires Python with access to the pandas and pathlib modules; the expected environment is an interactive shell or terminal.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_7\n\nLANGUAGE: none\nCODE:\n```\npython -c \"import pandas; import pathlib; (pathlib.Path(pandas.__path__[0]) / 'py.typed').touch()\"\n```\n\n----------------------------------------\n\nTITLE: Combining skiprows and header Parameters\nDESCRIPTION: Demonstrates how header is relative to the end of skiprows when both parameters are specified, allowing precise control over which rows to skip and which to use as headers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndata = (\n    \"# empty\\n\"\n    \"# second empty line\\n\"\n    \"# third emptyline\\n\"\n    \"X,Y,Z\\n\"\n    \"1,2,3\\n\"\n    \"A,B,C\\n\"\n    \"1,2.,4.\\n\"\n    \"5.,NaN,10.0\\n\"\n)\nprint(data)\npd.read_csv(StringIO(data), comment=\"#\", skiprows=4, header=1)\n```\n\n----------------------------------------\n\nTITLE: Highlighting Maximum Values by Column\nDESCRIPTION: Applies background highlighting to maximum values in each column using apply() method with axis=0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef highlight_max(s, props=\"\"):\n    return np.where(s == np.nanmax(s.values), props, \"\")\n\ns2.apply(highlight_max, props=\"color:white;background-color:darkblue\", axis=0)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating iloc Out-of-bounds Indexing in Pandas\nDESCRIPTION: Shows how iloc now handles out-of-bounds indexers for slices, returning empty results for valid slices beyond bounds while raising IndexError for invalid single indexers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndfl = pd.DataFrame(np.random.randn(5, 2), columns=list('AB'))\ndfl\ndfl.iloc[:, 2:3]\ndfl.iloc[:, 1:3]\ndfl.iloc[4:6]\n```\n\n----------------------------------------\n\nTITLE: Assigning Values to MultiIndex Subset - pandas Python\nDESCRIPTION: Shows how to perform bulk assignment of values to a MultiIndex-selected subset using axis-aware loc. Copies the DataFrame, applies the selection, and assigns -10 to that region. This is valuable for in-place data manipulation workflows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.copy()\ndf2.loc(axis=0)[:, :, ['C1', 'C3']] = -10\ndf2\n```\n\n----------------------------------------\n\nTITLE: DataFrame Operations in Pandas\nDESCRIPTION: Enhanced DataFrame capabilities including dtype inspection, appending with index ignoring, renaming, and improved indexing via ix attribute.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.4.x.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame.get_dtype_counts()\nDataFrame.dtypes\nDataFrame.append(ignore_index=True)\nDataFrame.rename(copy=False)\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in DataFrameGroupBy.agg and SeriesGroupBy.agg for mixed data types\nDESCRIPTION: Resolves an issue where DataFrameGroupBy.agg and SeriesGroupBy.agg were failing silently with mixed data types along axis=1 and MultiIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDataFrameGroupBy.agg()\nSeriesGroupBy.agg()\n```\n\n----------------------------------------\n\nTITLE: Template Substitution with Shared Parameters\nDESCRIPTION: Shows how to combine template substitution with shared documentation parameters using the @doc decorator.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n@doc(template, **_shared_doc_kwargs)\ndef my_function(self):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Fully-Formed pandas DataFrame Parquet Metadata Example - Text\nDESCRIPTION: Demonstrates a complete example of the metadata dictionary for a pandas DataFrame as stored in the Parquet file. This includes example index columns, column descriptors, versions, and creator/library information. The snippet can be used as a model or reference for metadata construction or validation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/developer.rst#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n   {'index_columns': ['__index_level_0__'],\n    'column_indexes': [\n        {'name': None,\n         'field_name': 'None',\n         'pandas_type': 'unicode',\n         'numpy_type': 'object',\n         'metadata': {'encoding': 'UTF-8'}}\n    ],\n    'columns': [\n        {'name': 'c0',\n         'field_name': 'c0',\n         'pandas_type': 'int8',\n         'numpy_type': 'int8',\n         'metadata': None},\n        {'name': 'c1',\n         'field_name': 'c1',\n         'pandas_type': 'bytes',\n         'numpy_type': 'object',\n         'metadata': None},\n        {'name': 'c2',\n         'field_name': 'c2',\n         'pandas_type': 'categorical',\n         'numpy_type': 'int16',\n         'metadata': {'num_categories': 1000, 'ordered': False}},\n        {'name': 'c3',\n         'field_name': 'c3',\n         'pandas_type': 'datetimetz',\n         'numpy_type': 'datetime64[ns]',\n         'metadata': {'timezone': 'America/Los_Angeles'}},\n        {'name': 'c4',\n         'field_name': 'c4',\n         'pandas_type': 'object',\n         'numpy_type': 'object',\n         'metadata': {'encoding': 'pickle'}},\n        {'name': None,\n         'field_name': '__index_level_0__',\n         'pandas_type': 'int64',\n         'numpy_type': 'int64',\n         'metadata': None}\n    ],\n    'pandas_version': '1.4.0',\n    'creator': {\n      'library': 'pyarrow',\n      'version': '0.13.0'\n    }}\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: Failing MultiIndex Lookup Returns Empty Series - ipython\nDESCRIPTION: Demonstrates that, prior to pandas 1.1.0, looking up an integer list key not present in the first MultiIndex level of a Series erroneously returned an empty Series rather than raising an error. Input is a list containing an out-of-bounds integer. Output is an empty Series with the expected dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_13\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: ser[[5]]\\nOut[5]: Series([], dtype: int64)\n```\n\n----------------------------------------\n\nTITLE: Requiring Direct Indexing for Parent DataFrame Mutation in pandas (Python)\nDESCRIPTION: In this snippet, the inability to propagate mutations from a subset back to the parent DataFrame under the new semantics is shown, and the only way to mutate the parent is through direct operation. Dependencies: pandas. Steps: create DataFrames, attempt to mutate the subset, then correctly mutate the parent. Input: DataFrames; Output: demonstrates how mutation only occurs directly. Used to clarify that chained assignment is no longer supported.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Case 1: user wants mutations of df2 to be reflected in df -> no longer possible\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4], \"C\": [5, 6]})\n>>> df2 = df[[\"A\", \"B\"]]\n>>> df2.loc[df2[\"A\"] > 1, \"A\"] = 1  # mutating df2 will not mutate df\n>>> df.loc[df[\"A\"] > 1, \"A\"] = 1  # need to directly mutate df instead\n```\n\n----------------------------------------\n\nTITLE: Deprecating Inplace Chained Methods for DataFrame Columns - pandas - Python\nDESCRIPTION: This snippet demonstrates the deprecation of applying inplace methods through chained assignment to DataFrame columns. The behavior will change in pandas 3.0, requiring users to modify their code to operate on the DataFrame directly. Input is a DataFrame with a column possibly containing NA values; output is a warning or an updated DataFrame. Instead of applying inplace methods to Series obtained via column selection, users must apply them to the DataFrame itself.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> df[\"foo\"].fillna(0, inplace=True)\nFutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for interval data function\nDESCRIPTION: RestructuredText directive listing Pandas interval range generation function\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/general_functions.rst#_snippet_4\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n\n   interval_range\n```\n\n----------------------------------------\n\nTITLE: Appending and Querying DataFrame with Data Columns in HDFStore - pandas HDFStore - Python\nDESCRIPTION: This snippet illustrates the use of data columns for querying in pandas HDFStore. It creates a DataFrame with additional string columns, makes selective updates, appends the DataFrame to a store with multiple columns marked as data_columns, and executes various queries using select. Also demonstrates in-memory boolean indexing for comparison. Requires pandas, numpy, a prepared DataFrame, and an open HDFStore. Inputs: DataFrame with columns to mark as data_columns. Outputs: Filtered DataFrames. Limitation: altering data_columns requires re-creating the table after initial append.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_187\n\nLANGUAGE: python\nCODE:\n```\ndf_dc = df.copy()\ndf_dc[\"string\"] = \"foo\"\ndf_dc.loc[df_dc.index[4:6], \"string\"] = np.nan\ndf_dc.loc[df_dc.index[7:9], \"string\"] = \"bar\"\ndf_dc[\"string2\"] = \"cool\"\ndf_dc.loc[df_dc.index[1:3], [\"B\", \"C\"]] = 1.0\ndf_dc\n\n# on-disk operations\nstore.append(\"df_dc\", df_dc, data_columns=[\"B\", \"C\", \"string\", \"string2\"])\nstore.select(\"df_dc\", where=\"B > 0\")\n\n# getting creative\nstore.select(\"df_dc\", \"B > 0 & C > 0 & string == foo\")\n\n# this is in-memory version of this type of selection\ndf_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == \"foo\")]\n\n# we have automagically created this index and the B/C/string/string2\n# columns are stored separately as ``PyTables`` columns\nstore.root.df_dc.table\n```\n\n----------------------------------------\n\nTITLE: Self Join of a DataFrame Using Merge - Pandas Python\nDESCRIPTION: Showcases a self join operation on a DataFrame, merging rows where combinations of columns match with offset values. After defining columns and introducing a shifted 'Test_1', the DataFrame is merged on three columns (including the shifted key), with suffixes to differentiate left/right fields. Dependencies: pandas, numpy. Input is a DataFrame with matching foreign keys; output is a DataFrame representing matched pairs via self join on custom columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    data={\n        \"Area\": [\"A\"] * 5 + [\"C\"] * 2,\n        \"Bins\": [110] * 2 + [160] * 3 + [40] * 2,\n        \"Test_0\": [0, 1, 0, 1, 2, 0, 1],\n        \"Data\": np.random.randn(7),\n    }\n)\ndf\n\ndf[\"Test_1\"] = df[\"Test_0\"] - 1\n\npd.merge(\n    df,\n    df,\n    left_on=[\"Bins\", \"Area\", \"Test_0\"],\n    right_on=[\"Bins\", \"Area\", \"Test_1\"],\n    suffixes=(\"_L\", \"_R\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Extension Array Classes\nDESCRIPTION: These classes are used to create custom array types that can be used with pandas objects. ExtensionArray is the base class, while NumpyExtensionArray is a specific implementation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/extensions.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\napi.extensions.ExtensionArray\narrays.NumpyExtensionArray\n```\n\n----------------------------------------\n\nTITLE: Updating Rows Conditionally in SQL - SQL\nDESCRIPTION: Doubles the values of the 'tip' column for all entries in the 'tips' table where 'tip' is less than 2. This sets new values directly in the database table based on a condition. Requires a writable SQL environment (e.g., Oracle, MySQL, PostgreSQL) and a 'tips' table with a 'tip' column. Output is the number of rows affected; no result set is returned.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_13\n\nLANGUAGE: sql\nCODE:\n```\nUPDATE tips\nSET tip = tip*2\nWHERE tip < 2;\n```\n\n----------------------------------------\n\nTITLE: Removing String Prefixes/Suffixes with Pandas - Python\nDESCRIPTION: Demonstrates the use of pandas Series.str.removeprefix and removeprefix for removing a string prefix or suffix from each element in a Series. Requires pandas 1.4.0+ and Python 3.9+ for these methods. Accepts a string prefix or suffix to remove. Returns a new Series with the specified part of each string removed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"str_foo\", \"str_bar\", \"no_prefix\"])\ns.str.removeprefix(\"str_\")\n\ns = pd.Series([\"foo_str\", \"bar_str\", \"no_suffix\"])\ns.str.removesuffix(\"_str\")\n```\n\n----------------------------------------\n\nTITLE: Handling NaN Values When Generating Dummies with get_dummies - Pandas - Python\nDESCRIPTION: Illustrates the effect of NaN in input when generating dummy variables using pandas.get_dummies, with and without the dummy_na argument. When dummy_na is True, a separate column is generated for NaN values; otherwise NaN rows are omitted from dummy generation. Requires pandas and numpy. Inputs: list with NaN values. Outputs: DataFrame with dummy columns, including or excluding NaN.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# previously, nan was erroneously counted as 2 here\\n# now it is not counted at all\\npd.get_dummies([1, 2, np.nan])\n```\n\nLANGUAGE: python\nCODE:\n```\n# unless requested\\npd.get_dummies([1, 2, np.nan], dummy_na=True)\n```\n\n----------------------------------------\n\nTITLE: DataFrame.describe for Empty Categorical/Object Columns (Python)\nDESCRIPTION: Shows that DataFrame.describe now always includes 'top' and 'freq' columns even for empty categorical or object columns, using NaN when there are no values present. Requires a DataFrame with an empty categorical column. Returns the describe summary including top/freq.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\ndf = pd.DataFrame({\"empty_col\": pd.Categorical([])})\ndf\n```\n\nLANGUAGE: ipython\nCODE:\n```\ndf.describe()\n```\n\n----------------------------------------\n\nTITLE: Using the fold argument in Timestamp constructor\nDESCRIPTION: Examples of using the new fold parameter in Timestamp constructor to support PEP 495. This is useful for handling ambiguous times during DST transitions when a time occurs twice.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nts = pd.Timestamp(\"2019-10-27 01:30:00+00:00\")\nts.fold\n```\n\nLANGUAGE: python\nCODE:\n```\nts = pd.Timestamp(year=2019, month=10, day=27, hour=1, minute=30,\n                  tz=\"dateutil/Europe/London\", fold=1)\nts\n```\n\n----------------------------------------\n\nTITLE: Reading line-delimited JSON with the pyarrow engine in pandas - Python\nDESCRIPTION: Illustrates how to use pandas' read_json function with the pyarrow engine when reading line-delimited JSON data from an in-memory bytes stream. Useful for optimizing performance with large datasets. Dependencies: pandas, pyarrow, io.BytesIO; input: byte-encoded line-delimited JSON string; output: DataFrame. pyarrow must be installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_84\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\ndf = pd.read_json(BytesIO(jsonl.encode()), lines=True, engine=\"pyarrow\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating a Deprecated SparseDataFrame (Python)\nDESCRIPTION: Example of the deprecated way to create a sparse dataframe using SparseDataFrame, which will be removed in a future version. This is shown for comparison with the new recommended approach.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.SparseDataFrame({\"A\": [0, 0, 1, 2]})\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame After Value Setting - Python\nDESCRIPTION: Shows the effect of previous setting operations on the DataFrame by outputting the DataFrame 'df'. This provides a current snapshot to verify all recent changes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf\n```\n\n----------------------------------------\n\nTITLE: Sphinx Documentation Template in reStructuredText\nDESCRIPTION: A template that uses Sphinx directives to generate automatic documentation for Pandas classes. It includes template variables for the full name, module name, and object name that get populated during documentation build.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/_templates/autosummary/class_without_autosummary.rst#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n{{ fullname }}\n{{ underline }}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n```\n\n----------------------------------------\n\nTITLE: Consistent Unique Handling with Extension Types (e.g., Datetime tz-aware) - Python\nDESCRIPTION: This snippet documents the prior inconsistent behaviors between Series.unique and pandas.unique, especially on Categorical and tz-aware datatypes, and shows that Pandas now ensures these return consistent types (typically an array of Timestamp with timezone-awareness). Dependency: Pandas. Inputs are Series or Index with timezone-aware timestamps. Outputs are arrays or Indexes of the same type and dtype. Limitation: Users may need to update logic that expects old behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\n# Series\npd.Series([pd.Timestamp('20160101', tz='US/Eastern'),\n           pd.Timestamp('20160101', tz='US/Eastern')]).unique()\n# Previous Output: array([Timestamp('2016-01-01 00:00:00-0500', tz='US/Eastern')], dtype=object)\n\npd.unique(pd.Series([pd.Timestamp('20160101', tz='US/Eastern'),\n                     pd.Timestamp('20160101', tz='US/Eastern')]))\n# Previous Output: array(['2016-01-01T05:00:00.000000000'], dtype='datetime64[ns]')\n\n# Index\npd.Index([pd.Timestamp('20160101', tz='US/Eastern'),\n          pd.Timestamp('20160101', tz='US/Eastern')]).unique()\n# Previous Output: DatetimeIndex(['2016-01-01 00:00:00-05:00'], dtype='datetime64[ns, US/Eastern]', freq=None)\n\npd.unique([pd.Timestamp('20160101', tz='US/Eastern'),\n           pd.Timestamp('20160101', tz='US/Eastern')])\n# Previous Output: array(['2016-01-01T05:00:00.000000000'], dtype='datetime64[ns]')\n\n# New behavior:\n# Series, returns an array of Timestamp tz-aware\npd.Series([pd.Timestamp(r'20160101', tz=r'US/Eastern'),\n           pd.Timestamp(r'20160101', tz=r'US/Eastern')]).unique()\n```\n\n----------------------------------------\n\nTITLE: HDFStore MultiIndex Serialization in Pandas\nDESCRIPTION: Example of storing and retrieving a DataFrame with a MultiIndex in HDFStore, including filtering by index levels which are automatically included as data columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.1.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nIn [19]: index = pd.MultiIndex(levels=[['foo', 'bar', 'baz', 'qux'],\n   ....:                               ['one', 'two', 'three']],\n   ....:                       labels=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3],\n   ....:                               [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]],\n   ....:                       names=['foo', 'bar'])\n   ....:\n\nIn [20]: df = pd.DataFrame(np.random.randn(10, 3), index=index,\n   ....:                   columns=['A', 'B', 'C'])\n   ....:\n\nIn [21]: df\nOut[21]:\n                  A         B         C\nfoo bar\nfoo one   -0.116619  0.295575 -1.047704\n    two    1.640556  1.905836  2.772115\n    three  0.088787 -1.144197 -0.633372\nbar one    0.925372 -0.006438 -0.820408\n    two   -0.600874 -1.039266  0.824758\nbaz two   -0.824095 -0.337730 -0.927764\n    three -0.840123  0.248505 -0.109250\nqux one    0.431977 -0.460710  0.336505\n    two   -3.207595 -1.535854  0.409769\n    three -0.673145 -0.741113 -0.110891\n\nIn [22]: store.append('mi', df)\n\nIn [23]: store.select('mi')\nOut[23]:\n                  A         B         C\nfoo bar\nfoo one   -0.116619  0.295575 -1.047704\n    two    1.640556  1.905836  2.772115\n    three  0.088787 -1.144197 -0.633372\nbar one    0.925372 -0.006438 -0.820408\n    two   -0.600874 -1.039266  0.824758\nbaz two   -0.824095 -0.337730 -0.927764\n    three -0.840123  0.248505 -0.109250\nqux one    0.431977 -0.460710  0.336505\n    two   -3.207595 -1.535854  0.409769\n    three -0.673145 -0.741113 -0.110891\n\n# the levels are automatically included as data columns\nIn [24]: store.select('mi', \"foo='bar'\")\nOut[24]:\n                A         B         C\nfoo bar\nbar one  0.925372 -0.006438 -0.820408\n    two -0.600874 -1.039266  0.824758\n```\n\n----------------------------------------\n\nTITLE: Chaining DataFrame Operations with Plugin Methods (Python)\nDESCRIPTION: Shows usage of the new plugin API for method chaining, enabling a fluid workflow by sequentially loading and exporting data in a single expression. This capability requires pandas and a registered plugin, and allows invoking plugin-injected read and to_* methods on DataFrame objects. Inputs consist of arguments accepted by the respective plugin, such as SQL queries and target connection/table details. The output is dependent on the final chained operation, typically a DataFrame or result of a data write.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n(pandas.read_duckdb(\"SELECT * FROM 'dataset.parquet';\")\n       .to_hive(hive_conn, \"hive_table\"))\n```\n\n----------------------------------------\n\nTITLE: Rolling Sum with min_periods=0 on All-NA Series in pandas 0.22.0 - Python\nDESCRIPTION: Shows that rolling sum with min_periods=0 on an all-NaN Series now returns 0s instead of NaNs in pandas 0.22.0. Dependencies: pandas as pd, numpy as np. Inputs: Series of NaNs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([np.nan, np.nan])\ns.rolling(2, min_periods=0).sum()\n```\n\n----------------------------------------\n\nTITLE: Dropping Created Column from DataFrame\nDESCRIPTION: Cleanup operation that removes the previously created 'bucket' column from the tips DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/if_then.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntips = tips.drop(\"bucket\", axis=1)\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for datetime functions\nDESCRIPTION: RestructuredText directive listing Pandas datetime and timedelta manipulation functions\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/general_functions.rst#_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n\n   to_datetime\n   to_timedelta\n   date_range\n   bdate_range\n   period_range\n   timedelta_range\n   infer_freq\n```\n\n----------------------------------------\n\nTITLE: Defining pandas.Series method: contains in Python\nDESCRIPTION: Illustrates a method to check if each Series value contains a pattern, with options for case sensitivity and NA handling. Depends on pandas and numpy. Covers multiple parameter usages: default case, case-insensitive matching, and replacement of missing output. Shows Series input, flexible parameterization, and expected boolean or NA output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef contains(self, pattern, case_sensitive=True, na=numpy.nan):\n    \"\"\"\n    Return whether each value contains ``pattern``.\n\n    In this case, we are illustrating how to use sections, even\n    if the example is simple enough and does not require them.\n\n    Examples\n    --------\n    >>> ser = pd.Series('Antelope', 'Lion', 'Zebra', np.nan)\n    >>> ser.contains(pattern='a')\n    0    False\n    1    False\n    2     True\n    3      NaN\n    dtype: bool\n\n    **Case sensitivity**\n\n    With ``case_sensitive`` set to ``False`` we can match ``a`` with both\n    ``a`` and ``A``:\n\n    >>> s.contains(pattern='a', case_sensitive=False)\n    0     True\n    1    False\n    2     True\n    3      NaN\n    dtype: bool\n\n    **Missing values**\n\n    We can fill missing values in the output using the ``na`` parameter:\n\n    >>> ser.contains(pattern='a', na=False)\n    0    False\n    1    False\n    2     True\n    3    False\n    dtype: bool\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Suppressing Output in IPython for Pandas Import (Python)\nDESCRIPTION: This code snippet suppresses output when importing all objects from pandas in an IPython environment. It's used to set up the environment for the documentation examples.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.2.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas import *  # noqa F401, F403\n```\n\n----------------------------------------\n\nTITLE: Creating and Stacking MultiIndex DataFrames in pandas (Python)\nDESCRIPTION: This snippet demonstrates creating a pandas DataFrame with MultiIndex columns and stacking its levels using both the previous and new versions of the stack method, highlighting the effect of the future_stack and dropna parameters. The code requires pandas (imported as pd), and numpy (imported as np for the second example). Inputs include nested lists for data, explicit indices, and MultiIndex columns. It illustrates how NA values are handled differently and how dtype coercion is affected. The output is the stacked DataFrame with NA values managed according to the provided parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncolumns = pd.MultiIndex.from_tuples([(\"B\", \"d\"), (\"A\", \"c\")])\\ndf = pd.DataFrame([[0, 2], [1, 3]], index=[\"z\", \"y\"], columns=columns)\\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.stack([0, 1], future_stack=False, dropna=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.stack([0, 1], future_stack=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[0, 2], [np.nan, np.nan]], columns=columns)\\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.stack([0, 1], future_stack=False, dropna=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.stack([0, 1], future_stack=False, dropna=False)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.stack([0, 1], future_stack=True)\n```\n\n----------------------------------------\n\nTITLE: Structuring Contribution Information Using HTML - HTML\nDESCRIPTION: This HTML snippet presents three pathways of supporting or contributing to pandas: Corporate support, Individual contributors, and Donations. It uses Bootstrap CSS classes for layout (container, row, col-md-4), Font Awesome icons for illustrative purposes, and links to relevant information pages. No programming logic is implemented and the main dependencies are Bootstrap and Font Awesome for proper display. Inputs are static; the output is a responsive, visually organized section. Requires HTML/CSS/JS runtime with the referenced asset libraries and assumes a templating environment for {{ base_url }} replacement.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/contribute.md#_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<section>\\n    <div class=\\\"container mt-5\\\">\\n      <div class=\\\"row text-center\\\">\\n        <div class=\\\"col-md-4\\\">\\n          <span class=\\\"fa-stack fa-4x\\\">\\n            <i class=\\\"fas fa-circle fa-stack-2x pink\\\"></i>\\n            <i class=\\\"fas fa-building fa-stack-1x fa-inverse\\\"></i>\\n          </span>\\n          <h4 class=\\\"service-heading mt-3 fw-bold blue\\\">Corporate support</h4>\\n          <p class=\\\"text-muted\\\">\\n            pandas depends on companies and institutions using the software to support its development. Hiring\\n            people to work on pandas, or letting existing employees to contribute to the\\n            software. Or sponsoring pandas with funds, so the project can hire people to\\n            progress on the <a href=\\\"{{ base_url }}about/roadmap.html\\\">pandas roadmap</a>.\\n          </p>\\n          <p>More information in the <a href=\\\"{{ base_url }}about/sponsors.html\\\">sponsors page</a></p>\\n        </div>\\n        <div class=\\\"col-md-4\\\">\\n          <span class=\\\"fa-stack fa-4x\\\">\\n            <i class=\\\"fas fa-circle fa-stack-2x pink\\\"></i>\\n            <i class=\\\"fas fa-users fa-stack-1x fa-inverse\\\"></i>\\n          </span>\\n          <h4 class=\\\"service-heading mt-3 fw-bold blue\\\">Individual contributors</h4>\\n          <p class=\\\"text-muted\\\">\\n            pandas is mostly developed by volunteers. All kind of contributions are welcome,\\n            such as contributions to the code, to the website (including graphical designers),\\n            to the documentation (including translators) and others. There are tasks for all\\n            levels, including beginners.\\n          </p>\\n          <p>More information in the <a href=\\\"{{ base_url }}docs/development/index.html\\\">contributing page</a></p>\\n        </div>\\n        <div class=\\\"col-md-4\\\">\\n          <span class=\\\"fa-stack fa-4x\\\">\\n            <i class=\\\"fas fa-circle fa-stack-2x pink\\\"></i>\\n            <i class=\\\"fas fa-dollar-sign fa-stack-1x fa-inverse\\\"></i>\\n          </span>\\n          <h4 class=\\\"service-heading mt-3 fw-bold blue\\\">Donations</h4>\\n          <p class=\\\"text-muted\\\">\\n            Individual donations are appreciated, and are used for things like the project\\n            infrastructure, travel expenses for our volunteer contributors to attend\\n            the in-person sprints, or to give small grants to develop features.\\n          </p>\\n          <p>Make your donation in the <a href=\\\"https://opencollective.com/pandas\\\">donate page</a></p>\\n        </div>\\n      </div>\\n    </div>\\n</section>\n```\n\n----------------------------------------\n\nTITLE: Selecting Row and Column Ranges with .iloc in pandas (Python)\nDESCRIPTION: This snippet demonstrates the .iloc accessor to select rows from index 9 to 24 and columns 2 to 4 (Python slicing is exclusive of the stop index). The result is a DataFrame containing the specified row and column range. Useful for position-based subsetting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntitanic.iloc[9:25, 2:5]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Arrow-backed String Inference in pandas (Python)\nDESCRIPTION: Illustrates the new default inference for string columns in pandas, comparing old behavior (object dtype) vs new Arrow string dtype. Also explains how to enable the feature for future dtype inference with pd.options. Arrow-backed string columns require pandas 3.0+ and pyarrow as a dependency. Inputs are lists or arrays of strings; output is Series with appropriate dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_1\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: ser = pd.Series([\"a\", \"b\"])\nOut[1]:\n0    a\n1    b\ndtype: object\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: ser = pd.Series([\"a\", \"b\"])\nOut[1]:\n0    a\n1    b\ndtype: string\n```\n\nLANGUAGE: ipython\nCODE:\n```\npd.options.future.infer_string = True\n```\n\n----------------------------------------\n\nTITLE: Deprecated Rolling and Covariance Functions in pandas - IPython\nDESCRIPTION: Demonstrates deprecated use of pd.rolling_mean and pd.rolling_cov functions on Series, triggering FutureWarnings and recommending preferred Series.rolling().mean() and Series.rolling().cov() usage instead. Shows before/after output. Requires a Series (s) and pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_21\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: s = pd.Series(range(3))\n\nIn [2]: pd.rolling_mean(s,window=2,min_periods=1)\n        FutureWarning: pd.rolling_mean is deprecated for Series and\n             will be removed in a future version, replace with\n             Series.rolling(min_periods=1,window=2,center=False).mean()\nOut[2]:\n        0    0.0\n        1    0.5\n        2    1.5\n        dtype: float64\n\nIn [3]: pd.rolling_cov(s, s, window=2)\n        FutureWarning: pd.rolling_cov is deprecated for Series and\n             will be removed in a future version, replace with\n             Series.rolling(window=2).cov(other=<Series>)\nOut[3]:\n        0    NaN\n        1    0.5\n        2    0.5\n        dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Selecting Data from Series with Interval Keys Previous vs. New Behavior - Pandas - Python\nDESCRIPTION: Contrasts old and new behavior for accessing Series rows using intervals as keys, via both square bracket syntax and .loc. Previously, all overlapping intervals were returned; now only exact matches return a result, otherwise a KeyError is raised. Assumes 's' is a Series indexed by IntervalIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nIn [8]: s[pd.Interval(1, 5)]\\nOut[8]:\\n(0, 4]    a\\n(1, 5]    b\\ndtype: object\\n\\nIn [9]: s.loc[pd.Interval(1, 5)]\\nOut[9]:\\n(0, 4]    a\\n(1, 5]    b\\ndtype: object\n```\n\nLANGUAGE: python\nCODE:\n```\ns[pd.Interval(1, 5)]\\ns.loc[pd.Interval(1, 5)]\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [9]: s[pd.Interval(2, 3)]\\nOut[9]:\\n(0, 4]    a\\n(1, 5]    b\\ndtype: object\\n\\nIn [10]: s.loc[pd.Interval(2, 3)]\\nOut[10]:\\n(0, 4]    a\\n(1, 5]    b\\ndtype: object\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [6]: s[pd.Interval(2, 3)]\\n---------------------------------------------------------------------------\\nKeyError: Interval(2, 3, closed='right')\\n\\nIn [7]: s.loc[pd.Interval(2, 3)]\\n---------------------------------------------------------------------------\\nKeyError: Interval(2, 3, closed='right')\n```\n\n----------------------------------------\n\nTITLE: Slicing and Reversing to Simulate Forward Rolling Aggregation - pandas - Python\nDESCRIPTION: This snippet achieves forward-looking rolling aggregation by reversing a DataFrame, applying a rolling window, then reversing the result. It uses a custom timestamp index and a time-based window ('2s'), computes the rolling sum, and restores the original order. Dependencies: pandas, timestamp-based index. Inputs: time-indexed DataFrame; Output: DataFrame with forward-window sum. This emulates forward rolling when a dedicated indexer is not used.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    data=[\n        [pd.Timestamp(\"2018-01-01 00:00:00\"), 100],\n        [pd.Timestamp(\"2018-01-01 00:00:01\"), 101],\n        [pd.Timestamp(\"2018-01-01 00:00:03\"), 103],\n        [pd.Timestamp(\"2018-01-01 00:00:04\"), 111],\n    ],\n    columns=[\"time\", \"value\"],\n).set_index(\"time\")\ndf\n\nreversed_df = df[::-1].rolling(\"2s\").sum()[::-1]\nreversed_df\n```\n\n----------------------------------------\n\nTITLE: Piping GroupBy Operations in Pandas\nDESCRIPTION: Demonstrates the use of the pipe method with GroupBy objects to create more readable code. The example calculates prices (revenue/quantity) per store and product.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\nn = 1000\ndf = pd.DataFrame(\n    {\n        \"Store\": np.random.choice([\"Store_1\", \"Store_2\"], n),\n        \"Product\": np.random.choice([\"Product_1\", \"Product_2\"], n),\n        \"Revenue\": (np.random.random(n) * 50 + 10).round(2),\n        \"Quantity\": np.random.randint(1, 10, size=n),\n    }\n)\ndf.head(2)\n\n(\n    df.groupby([\"Store\", \"Product\"])\n    .pipe(lambda grp: grp.Revenue.sum() / grp.Quantity.sum())\n    .unstack()\n    .round(2)\n)\n\ndef mean(groupby):\n    return groupby.mean()\n\n\ndf.groupby([\"Store\", \"Product\"]).pipe(mean)\n```\n\n----------------------------------------\n\nTITLE: Timing pandas.eval with Python Engine - Python\nDESCRIPTION: Measures the performance of the same DataFrame addition using pandas.eval with the 'python' engine. Useful for comparing with direct operations and other pandas.eval engines. Requires the same input DataFrames and dependencies as before. Output is measured execution time; note that this does not provide performance gain over native Python.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n%timeit pd.eval(\"df1 + df2 + df3 + df4\", engine=\"python\")\n```\n\n----------------------------------------\n\nTITLE: Calling Cython ndarray Integration from Python - Python\nDESCRIPTION: Demonstrates usage of the compiled 'apply_integrate_f' Cython function by passing in DataFrame columns converted to numpy arrays using to_numpy(). Requires prior Cython compilation and DataFrame columns of compatible types. Outputs timing information for the high-performance implementation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%timeit apply_integrate_f(df[\"a\"].to_numpy(), df[\"b\"].to_numpy(), df[\"N\"].to_numpy())\n```\n\n----------------------------------------\n\nTITLE: Hexbin Plot with Custom Aggregation\nDESCRIPTION: This example demonstrates a hexbin plot with custom aggregation. The 'C' parameter specifies values to aggregate within each hexbin, and reduce_C_function determines how to aggregate them.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(1000, 2), columns=[\"a\", \"b\"])\ndf[\"b\"] = df[\"b\"] + np.arange(1000)\ndf[\"z\"] = np.random.uniform(0, 3, 1000)\n\ndf.plot.hexbin(x=\"a\", y=\"b\", C=\"z\", reduce_C_function=np.max, gridsize=25);\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in GroupBy Rolling Apply Methods\nDESCRIPTION: Corrects a regression in DataFrameGroupBy.rolling.apply and SeriesGroupBy.rolling.apply ignoring args and kwargs parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nDataFrameGroupBy.rolling.apply()\n```\n\nLANGUAGE: Python\nCODE:\n```\nSeriesGroupBy.rolling.apply()\n```\n\n----------------------------------------\n\nTITLE: Subtraction between PeriodIndex and Period returns Int64Index (legacy pandas, Python)\nDESCRIPTION: Legacy code where subtracting a Period from a PeriodIndex returned an Int64Index; shows both creation and subtraction. Input: PeriodIndex, Period; output: Int64Index. This now returns Index of DateOffset in pandas 0.24+.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: pi = pd.period_range('June 2018', freq='M', periods=3)\n\nIn [3]: pi - pi[0]\nOut[3]: Int64Index([0, 1, 2], dtype='int64')\n```\n\n----------------------------------------\n\nTITLE: MultiIndex Reindexing Example in Python\nDESCRIPTION: Demonstrates the behavior of MultiIndex.get_indexer with backfill/pad methods using a DataFrame with multiple index levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\n    'a': [0, 0, 0, 0],\n    'b': [0, 2, 3, 4],\n    'c': ['A', 'B', 'C', 'D'],\n}).set_index(['a', 'b'])\nmi_2 = pd.MultiIndex.from_product([[0], [-1, 0, 1, 3, 4, 5]])\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.reindex(mi_2, method='backfill')\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.reindex(mi_2, method='pad')\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table with Text Matching in Python\nDESCRIPTION: Demonstrates how to search for specific tables in HTML content by matching text. This example searches for tables containing \"Metcalf Bank\".\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_92\n\nLANGUAGE: python\nCODE:\n```\nmatch = \"Metcalf Bank\"\ndf_list = pd.read_html(url, match=match)\n```\n\n----------------------------------------\n\nTITLE: Series Reindexing and dtype Change - pandas - Python\nDESCRIPTION: Illustrates how reindexing a pandas Series can silently insert NaN values, causing the dtype to change (e.g., from int to float). Shows creation of a Series, checking dtype before and after reindexing, and the resulting object. Highlights a subtlety that can lead to issues with numpy ufuncs. Requires pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nseries1 = pd.Series([1, 2, 3])\nseries1.dtype\nres = series1.reindex([0, 4])\nres.dtype\nres\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Subclass Retention with Custom Series/DataFrame - Python\nDESCRIPTION: Demonstrates the creation and type retention of custom Series/DataFrame subclasses after instantiation and manipulation. Requires prior definition of SubclassedSeries and SubclassedDataFrame with constructor properties. Example illustrates that slicing or converting will produce instances of the expected subclass type. No additional dependencies beyond pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> s = SubclassedSeries([1, 2, 3])\n>>> type(s)\n<class '__main__.SubclassedSeries'>\n\n>>> to_framed = s.to_frame()\n>>> type(to_framed)\n<class '__main__.SubclassedDataFrame'>\n\n>>> df = SubclassedDataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [7, 8, 9]})\n>>> df\n   A  B  C\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\n>>> type(df)\n<class '__main__.SubclassedDataFrame'>\n\n>>> sliced1 = df[[\"A\", \"B\"]]\n>>> sliced1\n   A  B\n0  1  4\n1  2  5\n2  3  6\n\n>>> type(sliced1)\n<class '__main__.SubclassedDataFrame'>\n\n>>> sliced2 = df[\"A\"]\n>>> sliced2\n0    1\n1    2\n2    3\nName: A, dtype: int64\n\n>>> type(sliced2)\n<class '__main__.SubclassedSeries'>\n```\n\n----------------------------------------\n\nTITLE: Creating Categorical DataFrame in Python\nDESCRIPTION: Shows how to create a DataFrame with all columns as categorical data types. This can be done during construction by specifying dtype=\"category\" in the DataFrame constructor.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\")}, dtype=\"category\")\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Summing Empty and All-NA Series in pandas 0.22.0 - Python\nDESCRIPTION: This snippet shows that in pandas 0.22.0, calling .sum() on an empty Series or a Series of only NaNs now returns 0 instead of nan by default. Dependencies: pandas as pd, numpy as np. Outputs: 0 and 0. Intended for illustrating the API-breaking change.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npd.Series([]).sum()\npd.Series([np.nan]).sum()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Expressions with inplace vs not inplace - pandas eval - Python\nDESCRIPTION: Demonstrates both inplace and non-inplace assignment of new columns to a DataFrame using eval. Shows that assignment is only applied to the result when inplace=False, while inplace=True modifies the original DataFrame. Requires a DataFrame (df) and pandas imported. Shows expected before-and-after states.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf\ndf.eval('d = c - b', inplace=False)\ndf\ndf.eval('d = c - b', inplace=True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Restoring and Sorting with Stack-Unstack - pandas - Python\nDESCRIPTION: This snippet tests if unstacking then stacking returns a sorted version of the original DataFrame. DataFrame is constructed with MultiIndex from a product, and values are generated from numpy. The operation all(df.unstack().stack() == df.sort_index()) checks if the transformation is lossless up to index sorting. Highlights automatic sorting behavior with stack/unstack.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nindex = pd.MultiIndex.from_product([[2, 1], [\"a\", \"b\"]])\ndf = pd.DataFrame(np.random.randn(4), index=index, columns=[\"A\"])\ndf\nall(df.unstack().stack() == df.sort_index())\n```\n\n----------------------------------------\n\nTITLE: Creating Series with 'boolean' Alias for BooleanDtype - pandas - Python\nDESCRIPTION: This code creates a Series of booleans using dtype=\"boolean\" (an alias for BooleanDtype), initializes it with True, False, and None, and prints the resulting Series. It highlights the use of missing data in boolean columns. Requires pandas 1.0.0 or newer.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([True, False, None], dtype=\"boolean\")\\ns\n```\n\n----------------------------------------\n\nTITLE: Cleanup: Removing Feather File using os (Python)\nDESCRIPTION: This snippet removes the 'example.feather' file to clean up after the example. Requires Python os module. Raises FileNotFoundError if the file does not exist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_205\n\nLANGUAGE: python\nCODE:\n```\nos.remove(\"example.feather\")\n```\n\n----------------------------------------\n\nTITLE: Reading JSON Files from Amazon S3 using fsspec via Pandas (Python)\nDESCRIPTION: Shows how to read a JSON file stored in Amazon S3 using pandas' read_json and s3:// URLs, which requires the 's3fs' package and fsspec. The DataFrame is returned after loading. Relevant for processing cloud data stores.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_json(\"s3://pandas-test/adatafile.json\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Inspecting pd.Timedelta Objects - Python\nDESCRIPTION: These snippets show how to construct scalar Timedelta objects in pandas using string-based times, including support for negative durations and NaT values. The inputs are various time strings; outputs are Timedelta objects with specific attributes accessible. The code relies on pandas >= 0.15.0, and is fully compatible with NumPy timedelta types. Note the somewhat different accessor behavior from Python's datetime.timedelta prior to version 0.16.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npd.Timedelta('1 days 06:05:01.00003')\npd.Timedelta('15.5us')\npd.Timedelta('1 hour 15.5us')\n\n# negative Timedeltas have this string repr\n# to be more consistent with datetime.timedelta conventions\npd.Timedelta('-1us')\n\n# a NaT\npd.Timedelta('nan')\n```\n\n----------------------------------------\n\nTITLE: Default Sentinel Value for Extension Methods\nDESCRIPTION: This sentinel value is used as the default in some extension methods. It should be compared using the 'is' operator to check if the user has provided a non-default value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/extensions.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\npandas.api.extensions.no_default\n```\n\n----------------------------------------\n\nTITLE: Upsampling with Resample and Sum in pandas 0.21.x - Python\nDESCRIPTION: Demonstrates upsampling with .resample().sum() in pandas 0.21.x. Empty (inserted) bins show NaN in the output. Input: Series with DateTimeIndex, resampling to finer intervals. Dependencies: pandas as pd, numpy as np.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nIn [14]: idx = pd.DatetimeIndex(['2017-01-01', '2017-01-02'])\n\nIn [15]: pd.Series([1, 2], index=idx).resample('12H').sum()\nOut[15]:\n2017-01-01 00:00:00    1.0\n2017-01-01 12:00:00    NaN\n2017-01-02 00:00:00    2.0\nFreq: 12H, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Deprecating flavor parameter in DataFrame.to_sql() method\nDESCRIPTION: The 'flavor' parameter in DataFrame.to_sql() method has been deprecated as it is no longer needed when SQLAlchemy is not installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame.to_sql(flavor=...)\n```\n\n----------------------------------------\n\nTITLE: Timing DataFrame Arithmetic with Unaligned Axes Using pandas.eval - Python\nDESCRIPTION: Times the performance of adding four DataFrames and a Series (with potentially unaligned axes) using pandas.eval. Helps establish if eval can handle and optimize addition of objects with different shapes or indices. Requires the same dependencies and inputs as above.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n%timeit pd.eval(\"df1 + df2 + df3 + df4 + s\")\n```\n\n----------------------------------------\n\nTITLE: Using rolling() method with corr() instead of rolling_corr_pairwise\nDESCRIPTION: Example of the new preferred way to compute pairwise rolling correlation, replacing the deprecated rolling_corr_pairwise function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n.rolling().corr(pairwise=True)\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for missing data functions\nDESCRIPTION: RestructuredText directive listing Pandas functions for handling missing/null data\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/general_functions.rst#_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n\n   isna\n   isnull\n   notna\n   notnull\n```\n\n----------------------------------------\n\nTITLE: Excluding pandas 0.21.x in conda YAML Requirements - YAML\nDESCRIPTION: This YAML snippet shows how to specify package dependencies in conda, explicitly excluding pandas 0.21.0 and 0.21.1, to prevent installation of problematic versions. Used for reproducible environments and CI setups.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nrequirements:\n  run:\n    - pandas !=0.21.0,!=0.21.1\n```\n\n----------------------------------------\n\nTITLE: Uploading Release to PyPI\nDESCRIPTION: Twine command for uploading wheels and source distribution to PyPI. Skips any existing files to avoid conflicts.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/maintaining.rst#_snippet_6\n\nLANGUAGE: console\nCODE:\n```\ntwine upload pandas/dist/pandas-<version>*.{whl,tar.gz} --skip-existing\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame and Simulating Cumulative Random Data - pandas/matplotlib - Python\nDESCRIPTION: This snippet creates a DataFrame 'df' from 1000x4 random data using NumPy, assigns it the same index as 'ts', names columns as 'A', 'B', 'C', 'D', and computes the cumulative sum. It prepares data for subsequent plotting examples. Dependencies: pandas, numpy, a defined ts.index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list(\"ABCD\"))\ndf = df.cumsum()\n```\n\n----------------------------------------\n\nTITLE: Displaying Wide DataFrames with Line Wrapping - python\nDESCRIPTION: Illustrates automatic splitting of wide DataFrames over multiple lines for improved readability, as well as toggling the option 'expand_frame_repr' for re-enabling previous summary display behavior. Useful for tailoring the DataFrame print output to presentation and inspection needs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwide_frame = pd.DataFrame(np.random.randn(5, 16))\n\nwide_frame\n```\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"expand_frame_repr\", False)\n\nwide_frame\n```\n\nLANGUAGE: python\nCODE:\n```\npd.reset_option(\"expand_frame_repr\")\n```\n\n----------------------------------------\n\nTITLE: Using PyArrow Backed String Data Type in Pandas\nDESCRIPTION: Demonstrates the new PyArrow backed string data type in pandas 1.3.0. This extension allows string arrays to be backed by PyArrow instead of NumPy arrays of Python objects, which can improve performance and memory usage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npd.Series(['abc', None, 'def'], dtype=pd.StringDtype(storage=\"pyarrow\"))\n```\n\n----------------------------------------\n\nTITLE: Installing pandas with Optional Excel Dependencies via pip in Shell\nDESCRIPTION: Installs pandas along with its extra dependencies required for reading Excel files using pip. The extras are specified as 'pandas[excel]', which pulls in all required packages for Excel support. Python and pip must be installed prior to execution. This command enables Excel file I/O features in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install \"pandas[excel]\"\n```\n\n----------------------------------------\n\nTITLE: Summarizing GroupBy API for pandas (reStructuredText)\nDESCRIPTION: This code snippet defines the structure and navigation for pandas' groupby API documentation using reStructuredText. It introduces the core GroupBy classes and summarizes key methods in categories like indexing, function application, computation, and plotting. It relies on pandas as the main dependency and expects user familiarity with Python and Sphinx for documentation; all references link to actual API objects within pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/groupby.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n=======\\nGroupBy\\n=======\\n.. currentmodule:: pandas.core.groupby\\n\\n:class:`pandas.api.typing.DataFrameGroupBy` and :class:`pandas.api.typing.SeriesGroupBy`\\ninstances are returned by groupby calls :func:`pandas.DataFrame.groupby` and\\n:func:`pandas.Series.groupby` respectively.\\n\\nIndexing, iteration\\n-------------------\\n.. autosummary::\\n   :toctree: api/\\n\\n   DataFrameGroupBy.__iter__\\n   SeriesGroupBy.__iter__\\n   DataFrameGroupBy.groups\\n   SeriesGroupBy.groups\\n   DataFrameGroupBy.indices\\n   SeriesGroupBy.indices\\n   DataFrameGroupBy.get_group\\n   SeriesGroupBy.get_group\\n\\n.. currentmodule:: pandas\\n\\n.. autosummary::\\n   :toctree: api/\\n   :template: autosummary/class_without_autosummary.rst\\n\\n   Grouper\\n\\nFunction application helper\\n---------------------------\\n.. autosummary::\\n   :toctree: api/\\n\\n   NamedAgg\\n\\n.. currentmodule:: pandas.core.groupby\\n\\nFunction application\\n--------------------\\n.. autosummary::\\n   :toctree: api/\\n\\n   SeriesGroupBy.apply\\n   DataFrameGroupBy.apply\\n   SeriesGroupBy.agg\\n   DataFrameGroupBy.agg\\n   SeriesGroupBy.aggregate\\n   DataFrameGroupBy.aggregate\\n   SeriesGroupBy.transform\\n   DataFrameGroupBy.transform\\n   SeriesGroupBy.pipe\\n   DataFrameGroupBy.pipe\\n   DataFrameGroupBy.filter\\n   SeriesGroupBy.filter\\n\\n``DataFrameGroupBy`` computations / descriptive stats\\n-----------------------------------------------------\\n.. autosummary::\\n   :toctree: api/\\n\\n   DataFrameGroupBy.all\\n   DataFrameGroupBy.any\\n   DataFrameGroupBy.bfill\\n   DataFrameGroupBy.corr\\n   DataFrameGroupBy.corrwith\\n   DataFrameGroupBy.count\\n   DataFrameGroupBy.cov\\n   DataFrameGroupBy.cumcount\\n   DataFrameGroupBy.cummax\\n   DataFrameGroupBy.cummin\\n   DataFrameGroupBy.cumprod\\n   DataFrameGroupBy.cumsum\\n   DataFrameGroupBy.describe\\n   DataFrameGroupBy.diff\\n   DataFrameGroupBy.ewm\\n   DataFrameGroupBy.expanding\\n   DataFrameGroupBy.ffill\\n   DataFrameGroupBy.first\\n   DataFrameGroupBy.head\\n   DataFrameGroupBy.idxmax\\n   DataFrameGroupBy.idxmin\\n   DataFrameGroupBy.last\\n   DataFrameGroupBy.max\\n   DataFrameGroupBy.mean\\n   DataFrameGroupBy.median\\n   DataFrameGroupBy.min\\n   DataFrameGroupBy.ngroup\\n   DataFrameGroupBy.nth\\n   DataFrameGroupBy.nunique\\n   DataFrameGroupBy.ohlc\\n   DataFrameGroupBy.pct_change\\n   DataFrameGroupBy.prod\\n   DataFrameGroupBy.quantile\\n   DataFrameGroupBy.rank\\n   DataFrameGroupBy.resample\\n   DataFrameGroupBy.rolling\\n   DataFrameGroupBy.sample\\n   DataFrameGroupBy.sem\\n   DataFrameGroupBy.shift\\n   DataFrameGroupBy.size\\n   DataFrameGroupBy.skew\\n   DataFrameGroupBy.kurt\\n   DataFrameGroupBy.std\\n   DataFrameGroupBy.sum\\n   DataFrameGroupBy.var\\n   DataFrameGroupBy.tail\\n   DataFrameGroupBy.take\\n   DataFrameGroupBy.value_counts\\n\\n``SeriesGroupBy`` computations / descriptive stats\\n--------------------------------------------------\\n.. autosummary::\\n   :toctree: api/\\n\\n   SeriesGroupBy.all\\n   SeriesGroupBy.any\\n   SeriesGroupBy.bfill\\n   SeriesGroupBy.corr\\n   SeriesGroupBy.count\\n   SeriesGroupBy.cov\\n   SeriesGroupBy.cumcount\\n   SeriesGroupBy.cummax\\n   SeriesGroupBy.cummin\\n   SeriesGroupBy.cumprod\\n   SeriesGroupBy.cumsum\\n   SeriesGroupBy.describe\\n   SeriesGroupBy.diff\\n   SeriesGroupBy.ewm\\n   SeriesGroupBy.expanding\\n   SeriesGroupBy.ffill\\n   SeriesGroupBy.first\\n   SeriesGroupBy.head\\n   SeriesGroupBy.last\\n   SeriesGroupBy.idxmax\\n   SeriesGroupBy.idxmin\\n   SeriesGroupBy.is_monotonic_increasing\\n   SeriesGroupBy.is_monotonic_decreasing\\n   SeriesGroupBy.max\\n   SeriesGroupBy.mean\\n   SeriesGroupBy.median\\n   SeriesGroupBy.min\\n   SeriesGroupBy.ngroup\\n   SeriesGroupBy.nlargest\\n   SeriesGroupBy.nsmallest\\n   SeriesGroupBy.nth\\n   SeriesGroupBy.nunique\\n   SeriesGroupBy.unique\\n   SeriesGroupBy.ohlc\\n   SeriesGroupBy.pct_change\\n   SeriesGroupBy.prod\\n   SeriesGroupBy.quantile\\n   SeriesGroupBy.rank\\n   SeriesGroupBy.resample\\n   SeriesGroupBy.rolling\\n   SeriesGroupBy.sample\\n   SeriesGroupBy.sem\\n   SeriesGroupBy.shift\\n   SeriesGroupBy.size\\n   SeriesGroupBy.skew\\n   SeriesGroupBy.kurt\\n   SeriesGroupBy.std\\n   SeriesGroupBy.sum\\n   SeriesGroupBy.var\\n   SeriesGroupBy.tail\\n   SeriesGroupBy.take\\n   SeriesGroupBy.value_counts\\n\\nPlotting and visualization\\n--------------------------\\n.. autosummary::\\n   :toctree: api/\\n\\n   DataFrameGroupBy.boxplot\\n   DataFrameGroupBy.hist\\n   SeriesGroupBy.hist\\n   DataFrameGroupBy.plot\\n   SeriesGroupBy.plot\\n\n```\n\n----------------------------------------\n\nTITLE: Series.map Return Type with datetime64 Values Changed - Python\nDESCRIPTION: This snippet demonstrates that Series.map applied to datetime64 series may now return int64 dtype instead of int32. Required: Pandas. Input is a datetime64[ns, tz] Series. Output: Series of extracted integer data, now with int64 dtype. Limitation: Code relying on dtype-specific features of int32 must be updated.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\ns = pd.Series(pd.date_range('2011-01-02T00:00', '2011-01-02T02:00', freq='H')\n                 .tz_localize('Asia/Tokyo'))\ns\n# Output:\n# 0   2011-01-02 00:00:00+09:00\n# 1   2011-01-02 01:00:00+09:00\n# 2   2011-01-02 02:00:00+09:00\n# Length: 3, dtype: datetime64[ns, Asia/Tokyo]\n\n# Previous behavior:\ns.map(lambda x: x.hour)\n# Output:\n# 0    0\n# 1    1\n# 2    2\n# dtype: int32\n\n# New behavior:\ns.map(lambda x: x.hour)\n# Output:\n# 0    0\n# 1    1\n# 2    2\n# Length: 3, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Using SemiMonthEnd and SemiMonthBegin Offsets in Pandas - Python\nDESCRIPTION: Showcases the importation of new frequency offsets, SemiMonthEnd and SemiMonthBegin, from pandas.tseries.offsets. These classes enable semi-monthly date calculations anchored by default on the 15th and the 1st/end of month. Usage requires only pandas installed, and these offsets can be supplied to date_range or added to Timestamps for advanced scheduling applications.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nfrom pandas.tseries.offsets import SemiMonthEnd, SemiMonthBegin\n```\n\n----------------------------------------\n\nTITLE: New Behavior: Failed MultiIndex Lookup Raises KeyError - ipython\nDESCRIPTION: Shows that with pandas 1.1.0, attempting to access a Series with a MultiIndex using a list containing a missing integer now raises a KeyError indicating the missing key(s). This brings behavior in line with single-level indexing. No further dependencies or parameters required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_14\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: ser[[5]]\\n...\\nKeyError: '[5] not in index'\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Rows of a DataFrame with iterrows (Python)\nDESCRIPTION: Demonstrates the use of iterrows to iterate through a DataFrame, yielding (row index, Series) pairs. Shows that dtypes are not preserved across Series rows. Includes example with dtype demonstration and notes the potential for silent dtype upcasting. Outputs each row index and associated row Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nfor row_index, row in df.iterrows():\n    print(row_index, row, sep=\"\\n\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf_orig = pd.DataFrame([[1, 1.5]], columns=[\"int\", \"float\"])\ndf_orig.dtypes\nrow = next(df_orig.iterrows())[1]\nrow\n```\n\nLANGUAGE: python\nCODE:\n```\nrow[\"int\"].dtype\ndf_orig[\"int\"].dtype\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2 = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\nprint(df2)\nprint(df2.T)\n\ndf2_t = pd.DataFrame({idx: values for idx, values in df2.iterrows()})\nprint(df2_t)\n```\n\n----------------------------------------\n\nTITLE: Testing for absence of warnings in Python using pandas\nDESCRIPTION: Example of how to test that no warnings are produced in a block of code using pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nwith tm.assert_produces_warning(False):\n    pd.no_warning_function()\n```\n\n----------------------------------------\n\nTITLE: Generating a Random DataFrame with Date Index for Time-Series Operations (Python)\nDESCRIPTION: This snippet initializes a reproducible random DataFrame with 5 dates as the index and two columns labeled 'A' and 'B'. The use of numpy's 'random.seed' ensures consistent random values across runs. This DataFrame is later used to demonstrate the change in broadcasting behavior. Dependencies: numpy, pandas. Input is implicit (none); output is a DataFrame with random floats and a date range index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(1234)\ndf = pd.DataFrame(\n    np.random.randn(5, 2),\n    columns=list(\"AB\"),\n    index=pd.date_range(\"2013-01-01\", periods=5),\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Timedelta ISO 8601 Conversion - pandas - Python\nDESCRIPTION: Uses the .isoformat() method of Timedelta to convert duration to ISO 8601 format string. Demonstrates construction from multiple units. Requires pandas 1.1+.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\npd.Timedelta(\n    days=6, minutes=50, seconds=3, milliseconds=10, microseconds=10, nanoseconds=12\n).isoformat()\n```\n\n----------------------------------------\n\nTITLE: HDF5 Table Storage and DataFrame Creation with HDFStore - ipython\nDESCRIPTION: Shows basic usage of pandas HDFStore for writing a DataFrame to disk in the HDF5 table format. Demonstrates DataFrame construction with a time series index and multiple columns. Highlights prerequisites (pandas, numpy, pytables), shows file-based persistent storage, and is the entry point to enhanced IO APIs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_11\n\nLANGUAGE: ipython\nCODE:\n```\nIn [41]: store = pd.HDFStore('store.h5')\n\nIn [42]: df = pd.DataFrame(np.random.randn(8, 3),\n   ....:                   index=pd.date_range('1/1/2000', periods=8),\n   ....:                   columns=['A', 'B', 'C'])\n\nIn [43]: df\nOut[43]:\n                   A         B         C\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to Parquet Without Index using pandas (Python)\nDESCRIPTION: Writes a DataFrame to a Parquet file while omitting the index by passing 'index=False'. Only columns 'a' and 'b' are stored. This is essential for compatibility with systems that do not expect an index column in Parquet files.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_213\n\nLANGUAGE: python\nCODE:\n```\ndf.to_parquet(\"test.parquet\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Displaying and Evaluating DataFrame State after eval Assignments - Python\nDESCRIPTION: Displays the current state of the DataFrame after a series of column assignments using DataFrame.eval, and creates a new temporary column 'e' without affecting the original DataFrame. Requires pandas. Demonstrates that new columns can be temporarily created and evaluated.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf\ndf.eval(\"e = a - c\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Fixing DataFrame Numeric Operations in Python\nDESCRIPTION: Bug fix for DataFrame floordiv method with axis=0, now treating division-by-zero consistently with Series.floordiv.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndf.floordiv(0, axis=0)  # Now behaves like Series.floordiv\n```\n\n----------------------------------------\n\nTITLE: Comparing Old and New 'ewmvar' Output and Debiasing Factors in Pandas (ipython)\nDESCRIPTION: These ipython cells compare pre- and post-0.15.0 results for 'pd.ewmvar' with and without the 'bias' argument, using a sample Series 's'. They reveal the prior use of a constant debiasing factor versus the new, entry-specific factors (including NaN for single points). Dependencies: pandas, Series 's' initialized with float data. Useful for users auditing variance calculations across version upgrades.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_16\n\nLANGUAGE: ipython\nCODE:\n```\nIn [89]: pd.ewmvar(s, com=2., bias=False)\nOut[89]:\n0   -2.775558e-16\n1    3.000000e-01\n2    9.556787e-01\n3    3.585799e+00\ndtype: float64\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [90]: pd.ewmvar(s, com=2., bias=False) / pd.ewmvar(s, com=2., bias=True)\nOut[90]:\n0    1.25\n1    1.25\n2    1.25\n3    1.25\ndtype: float64\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [14]: pd.ewmvar(s, com=2., bias=False)\nOut[14]:\n0         NaN\n1    0.500000\n2    1.210526\n3    4.089069\ndtype: float64\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [15]: pd.ewmvar(s, com=2., bias=False) / pd.ewmvar(s, com=2., bias=True)\nOut[15]:\n0         NaN\n1    2.083333\n2    1.583333\n3    1.425439\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Restricting eval Function to Named Functions (Python)\nDESCRIPTION: The eval function now ensures that only named functions can be used, improving security and predictability.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\ndf.eval('column_a + numpy.sin(column_b)')\n```\n\n----------------------------------------\n\nTITLE: Applying GroupBy with the Squeeze Keyword - Pandas - Python\nDESCRIPTION: Shows how to use the 'squeeze' keyword in DataFrame.groupby to control whether the grouped result reduces to a Series or remains a DataFrame, depending on the uniqueness of the group key. The example provides outputs for both 'squeeze=True' (unique group reduces to Series) and the default behavior. Demonstrates custom function application post-grouping. Requires pandas installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_1\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: df2 = pd.DataFrame([{\"val1\": 1, \"val2\": 20},\n   ...:                     {\"val1\": 1, \"val2\": 19},\n   ...:                     {\"val1\": 1, \"val2\": 27},\n   ...:                     {\"val1\": 1, \"val2\": 12}])\n\nIn [3]: def func(dataf):\n   ...:     return dataf[\"val2\"] - dataf[\"val2\"].mean()\n   ...:\n\nIn [4]: # squeezing the result frame to a series (because we have unique groups)\n   ...: df2.groupby(\"val1\", squeeze=True).apply(func)\nOut[4]:\n0    0.5\n1   -0.5\n2    7.5\n3   -7.5\nName: 1, dtype: float64\n\nIn [5]: # no squeezing (the default, and behavior in 0.10.1)\n   ...: df2.groupby(\"val1\").apply(func)\nOut[5]:\nval2    0    1    2    3\nval1\n1     0.5 -0.5  7.5 -7.5\n```\n\n----------------------------------------\n\nTITLE: Concatenating Categoricals Using Pandas concat - Python\nDESCRIPTION: Demonstrates the behavior of pd.concat when used on two pandas Series with incompatible categories. The code raises a ValueError indicating incompatibility, signaling that proper category alignment is required for concatenation. No external dependencies beyond pandas are needed; the error showcases a safety behavior when combining categorical Series with mismatched categories.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\npd.concat([s1, s2])\nValueError: incompatible categories in categorical concat\n```\n\n----------------------------------------\n\nTITLE: Iterating Over pandas DataFrame Columns (Python)\nDESCRIPTION: Illustrates basic iteration over DataFrame columns, where each iteration yields a column label (string). Requires pandas and a DataFrame. The for-loop iterates column names which can be printed or used for further processing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"col1\": np.random.randn(3), \"col2\": np.random.randn(3)}, index=[\"a\", \"b\", \"c\"]\n)\n\nfor col in df:\n    print(col)\n```\n\n----------------------------------------\n\nTITLE: Reading XML with Default Namespace via Temporary Prefix - pandas - Python\nDESCRIPTION: Demonstrates parsing XML with a default (unprefixed) namespace by assigning a temporary prefix for use in XPath expressions. Without the prefix mapping, XPath queries will not match any nodes. Requires an XML string with a default namespace and the 'namespaces' parameter to map a chosen prefix (e.g., 'pandas') to the URI.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_120\n\nLANGUAGE: python\nCODE:\n```\nxml = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n<data xmlns=\\\"https://example.com\\\">\n <row>\n   <shape>square</shape>\n   <degrees>360</degrees>\n   <sides>4.0</sides>\n </row>\n <row>\n   <shape>circle</shape>\n   <degrees>360</degrees>\n   <sides/>\n </row>\n <row>\n   <shape>triangle</shape>\n   <degrees>180</degrees>\n   <sides>3.0</sides>\n </row>\n</data>\"\"\"\n\ndf = pd.read_xml(StringIO(xml),\n                 xpath=\"//pandas:row\",\n                 namespaces={\"pandas\": \"https://example.com\"})\ndf\n```\n\n----------------------------------------\n\nTITLE: Date and Interval Functions in SAS DATA Step\nDESCRIPTION: Utilizes various built-in SAS functions for date creation (mdy), extraction (year, month), interval shifting (intnx), and difference calculation (intck). Demonstrates date manipulation features common in analytic tasks. Input: none, creates and manipulates dates in the DATA step; Output: dataset with date columns and calculations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_10\n\nLANGUAGE: SAS\nCODE:\n```\ndata tips;\\n    set tips;\\n    format date1 date2 date1_plusmonth mmddyy10.;\\n    date1 = mdy(1, 15, 2013);\\n    date2 = mdy(2, 15, 2015);\\n    date1_year = year(date1);\\n    date2_month = month(date2);\\n    * shift date to beginning of next interval;\\n    date1_next = intnx('MONTH', date1, 1);\\n    * count intervals between dates;\\n    months_between = intck('MONTH', date1, date2);\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Grouping by Categorical and Summing in pandas 0.22.0 - Python\nDESCRIPTION: This shows that groupby-sum on unobserved categories now returns 0 (was nan), aligning groupby behavior with overall sum logic changes. Input: numeric Series grouped by Categorical with categories not represented in the data. Dependencies: pandas as pd. Outputs: group sums, with 'b' now 0 instead of nan.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ngrouper = pd.Categorical([\"a\", \"a\"], categories=[\"a\", \"b\"])\npd.Series([1, 2]).groupby(grouper, observed=False).sum()\n```\n\n----------------------------------------\n\nTITLE: Reading Excel Files with Calamine Engine in Pandas\nDESCRIPTION: Uses pandas.read_excel with the calamine engine to read Excel and OpenDocument spreadsheet files. The calamine engine is based on the Rust library and offers better performance in most cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_151\n\nLANGUAGE: python\nCODE:\n```\n# Returns a DataFrame\npd.read_excel(\"path_to_file.xlsb\", engine=\"calamine\")\n```\n\n----------------------------------------\n\nTITLE: Managing Multiple HDFStore Handles and File Closing Semantics - Pandas - Python\nDESCRIPTION: Demonstrates opening multiple HDFStore handles to the same file, appending data, and displaying open vs closed states. Highlights the is_open property, and the difference between local instance closure and actual file closure (depending on handle reference counting). Inputs: DataFrames and HDFStore path, actions: append/close/print. Uses pandas, numpy, and filesystem for storage; demonstrates error handling and store information reporting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npath = 'test.h5'\\ndf = pd.DataFrame(np.random.randn(10, 2))\\nstore1 = pd.HDFStore(path)\\nstore2 = pd.HDFStore(path)\\nstore1.append('df', df)\\nstore2.append('df2', df)\\n\\nstore1\\nstore2\\nstore1.close()\\nstore2\\nstore2.close()\\nstore2\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\\nos.remove(path)\n```\n\n----------------------------------------\n\nTITLE: Casting to Higher Dimensional Array with acast - reshape2 - R\nDESCRIPTION: This R snippet uses melt and acast from reshape2 to aggregate a data.frame ('df') into a multidimensional array, producing means across combinations of 'week', 'month', and variable. It expects numeric columns, multiple id fields, and applies the mean aggregation function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_20\n\nLANGUAGE: R\nCODE:\n```\ndf <- data.frame(\n  x = runif(12, 1, 168),\n  y = runif(12, 7, 334),\n  z = runif(12, 1.7, 20.7),\n  month = rep(c(5,6,7),4),\n  week = rep(c(1,2), 6)\n)\n\nmdf <- melt(df, id=c(\"month\", \"week\"))\nacast(mdf, week ~ month ~ variable, mean)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up an Excel File using Python os.remove\nDESCRIPTION: Removes the file 'titanic.xlsx' from the filesystem using the Python os module. This snippet is often used in tutorials to prevent clutter from temporary files created during demonstrations and requires the 'os' library. Key parameter: path to the file to be deleted. No output if successful; raises an error if file not found.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/02_read_write.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.remove(\"titanic.xlsx\")\n```\n\n----------------------------------------\n\nTITLE: Transforming DataFrame and Series with Multiple Functions - pandas - Python\nDESCRIPTION: Applies a list of functions to a DataFrame or Series using .transform(), resulting in a DataFrame with MultiIndex columns (for DataFrame) or DataFrame output (for Series). Demonstrates flexible elementwise transformation and function composition. Output shape depends on input and function list.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ntsdf.transform([np.abs, lambda x: x + 1])\n```\n\nLANGUAGE: python\nCODE:\n```\ntsdf[\"A\"].transform([np.abs, lambda x: x + 1])\n```\n\n----------------------------------------\n\nTITLE: Reading XML from File Content into DataFrame - pandas - Python\nDESCRIPTION: Illustrates reading XML data written to a file and then read via StringIO into pandas 'read_xml'. The file is created and written with XML text, then reopened and its contents read into a DataFrame. Useful for scenarios needing intermediate file I/O. Requires write/read permissions and pandas (>=1.3.0); assumes variable 'xml' contains the XML data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_114\n\nLANGUAGE: python\nCODE:\n```\nfile_path = \"books.xml\"\nwith open(file_path, \"w\") as f:\n    f.write(xml)\n\nwith open(file_path, \"r\") as f:\n    df = pd.read_xml(StringIO(f.read()))\ndf\n```\n\n----------------------------------------\n\nTITLE: DataFrame Column Order Preservation (New Behavior) - Pandas - Python\nDESCRIPTION: Demonstrates new behavior where DataFrame columns preserve the order of keys in original dicts. This only applies on Python >= 3.6 and relevant pandas versions. Shows creation of the DataFrame using the same data as before.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame(data)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Querying Rolling Variance in Series Pandas Python\nDESCRIPTION: Shows how to compute rolling variance of a pandas Series, particularly in the context of updated precision/truncation logic for small values in pandas 1.3.0. Requires pandas. Defines a Series and applies .rolling(3).var() to obtain the rolling variance. Outputs a Series of variances, illustrating removal of artificial zero truncation for very small values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([7, 5, 5, 5])\ns.rolling(3).var()\n```\n\n----------------------------------------\n\nTITLE: Migrating TimeSeries to Series in Pandas - Python\nDESCRIPTION: This snippet demonstrates how to convert deprecated pd.TimeSeries objects to standard pd.Series in order to maintain compatibility with newer versions of Pandas. No external dependencies beyond Pandas are required. Inputs are time series data; outputs are either a TimeSeries or Series, with type-checking explicitly shown. Limitation: TimeSeries is fully removed in recent Pandas, so read/write support for HDF5 may break for old files.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\ns = pd.TimeSeries([1, 2, 3], index=pd.date_range('20130101', periods=3))\n\ns\n\n# Output:\n# 2013-01-01    1\n# 2013-01-02    2\n# 2013-01-03    3\n# Freq: D, dtype: int64\n\ntype(s)\n# Output: pandas.core.series.TimeSeries\n\ns = pd.Series(s)\n\ns\n\n# Output:\n# 2013-01-01    1\n# 2013-01-02    2\n# 2013-01-03    3\n# Freq: D, dtype: int64\n\ntype(s)\n# Output: pandas.core.series.Series\n```\n\n----------------------------------------\n\nTITLE: Listing First N Rows - Stata\nDESCRIPTION: This snippet displays the first five rows of a Stata dataset using the 'list in 1/5' command. There are no required dependencies. Input: Existing dataset in memory. Output: Printed display of rows 1 to 5.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_5\n\nLANGUAGE: stata\nCODE:\n```\nlist in 1/5\n```\n\n----------------------------------------\n\nTITLE: Deprecating Frequency Aliases with pd.date_range - pandas - IPython\nDESCRIPTION: This snippet compares previous and future usage patterns for defining custom frequencies using pd.date_range. It demonstrates the deprecation of short frequency aliases ('Q-NOV') in favor of expanded endings ('QE-NOV'). Requires the pandas library. Inputs are date start strings with period and frequency arguments; outputs are DatetimeIndex results. Ensures code remains functional in future pandas versions by adopting the new alias conventions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_15\n\nLANGUAGE: ipython\nCODE:\n```\nIn [8]: pd.date_range('2020-01-01', periods=3, freq='Q-NOV')\nOut[8]:\nDatetimeIndex(['2020-02-29', '2020-05-31', '2020-08-31'],\n              dtype='datetime64[ns]', freq='Q-NOV')\n```\n\nLANGUAGE: ipython\nCODE:\n```\npd.date_range('2020-01-01', periods=3, freq='QE-NOV')\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrames and Handling SettingWithCopyWarning - pandas - Python\nDESCRIPTION: Illustrates filtering a pandas DataFrame using a boolean mask, storing the result, and then assigning a new column, which triggers SettingWithCopyWarning due to modifying a copy of a slice. Requires pandas, and a DataFrame with columns 'A', 'B', 'C' as input. Demonstrates the assignment of a new column ('new_column') to the filtered DataFrame. Inputs involve a conditional row filter; output is the augmented DataFrame (if assignment is successful). Warning serves as a limitation, advising use of .copy() or .loc for direct mutation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4], \"C\": [5, 6]})\n>>> df_filtered = df[df[\"A\"] > 1]\n>>> df_filtered[\"new_column\"] = 1\nSettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n```\n\n----------------------------------------\n\nTITLE: Using the format Keyword and Inspecting HDFStore Contents - Pandas - Python\nDESCRIPTION: Provides examples of writing DataFrames to different HDFStore formats (table and fixed), including appending and inspecting multiple datasets. Shows file access in context manager, and use of to_hdf with various keywords. Inputs: DataFrames to write, path string. Outputs: HDFStore printout and file structure information. Uses pandas, numpy, and interaction with local filesystem. Cleans up test file after use.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npath = 'test.h5'\\ndf = pd.DataFrame(np.random.randn(10, 2))\\ndf.to_hdf(path, key='df_table', format='table')\\ndf.to_hdf(path, key='df_table2', append=True)\\ndf.to_hdf(path, key='df_fixed')\\nwith pd.HDFStore(path) as store:\\n    print(store)\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\\nos.remove(path)\n```\n\n----------------------------------------\n\nTITLE: Fixed Method References in Pandas 1.1.2\nDESCRIPTION: Code references to various Pandas methods and classes that were fixed in this release, including DatetimeIndex.intersection, DataFrame.append, RangeIndex.format, and others.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.2.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nDatetimeIndex.intersection\ndf['col'].fillna(.., inplace=True)\nDataFrame.append\nRangeIndex.format\nMultiIndex.get_loc\nDataFrame.replace\nIntervalIndex\nDataFrameGroupBy.agg\nSeries.groupby.rolling\nDataFrameGroupBy\n```\n\n----------------------------------------\n\nTITLE: DataFrame Dtype Preservation Example\nDESCRIPTION: Demonstrates how dtypes are now preserved during DataFrame enlargement operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[True, 1], [False, 2]],\n                   columns=[\"female\", \"fitness\"])\ndf\ndf.dtypes\ndf.loc[2] = df.loc[1]\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: float dtype Result for DataFrameGroupBy and SeriesGroupBy mean/median/var - Pandas Python\nDESCRIPTION: This snippet demonstrates that DataFrameGroupBy.mean, median, and var now always return float dtypes, regardless of input, fixing previous inconsistent output dtypes. It sets up a single-row DataFrame of bool, int, and float columns, groups by index, and computes mean. Requires pandas. Inputs: homogeneous DataFrame. Output: grouped mean result column, always as float dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': [True], 'b': [1], 'c': [1.0]})\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: df.groupby(df.index).mean()\nOut[5]:\n        a  b    c\n0    True  1  1.0\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(df.index).mean()\n```\n\n----------------------------------------\n\nTITLE: Writing Mixed Element and Attribute XML with pandas.to_xml (Python)\nDESCRIPTION: Combines attr_cols and elem_cols arguments to to_xml to write XML with shape as an attribute and degrees and sides as child elements. Demonstrates use of index=False to suppress row index in XML. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_127\n\nLANGUAGE: python\nCODE:\n```\nprint(\n    geom_df.to_xml(\n        index=False,\n        attr_cols=['shape'],\n        elem_cols=['degrees', 'sides'])\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Header Row Position in CSV Data\nDESCRIPTION: Demonstrates how to specify which row contains the column names using the header parameter, which will skip all preceding rows when reading the data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndata = \"skip this skip it\\na,b,c\\n1,2,3\\n4,5,6\\n7,8,9\"\npd.read_csv(StringIO(data), header=1)\n```\n\n----------------------------------------\n\nTITLE: Accessing MultiIndex Level Names in pandas (Python)\nDESCRIPTION: Demonstrates how to access level names in a pandas MultiIndex object. Despite refactoring, it is still possible to access names via the levels attribute, although updating them directly is no longer allowed. Requires pandas imported as pd. The variable 'mi' is a MultiIndex with named levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmi = pd.MultiIndex.from_product([[1, 2], ['a', 'b']], names=['x', 'y'])\nmi.levels[0].name\n```\n\n----------------------------------------\n\nTITLE: Constructing DatetimeIndex with Timezone and Inspecting Dtype (New pandas Behavior, Python)\nDESCRIPTION: Demonstrates updated dtype representation when creating timezone-aware DatetimeIndex objects in recent pandas. Shows a call to pd.date_range with tz and retrieval of the dtype attribute, reflecting pandas' internal changes to support improved timezone handling. Inputs and outputs are similar to previous versions but with subtly different dtype representation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\")\npd.date_range(\"20130101\", periods=3, tz=\"US/Eastern\").dtype\n```\n\n----------------------------------------\n\nTITLE: Assigning Values to Categorical Columns with Allowed and Disallowed Categories - Pandas Python\nDESCRIPTION: Demonstrates allowed assignment of values within categorical columns using .iloc, and error handling when assigned values not present in categories. Shows correct error propagation and category consistency requirements. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([\"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\"])\ncats = pd.Categorical([\"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\"], categories=[\"a\", \"b\"])\nvalues = [1, 1, 1, 1, 1, 1, 1]\ndf = pd.DataFrame({\"cats\": cats, \"values\": values}, index=idx)\n\ndf.iloc[2:4, :] = [[\"b\", 2], [\"b\", 2]]\ndf\ntry:\n    df.iloc[2:4, :] = [[\"c\", 3], [\"c\", 3]]\nexcept TypeError as e:\n    print(\"TypeError:\", str(e))\n```\n\n----------------------------------------\n\nTITLE: Creating Horizontally Oriented Box Plots in Pandas\nDESCRIPTION: This example demonstrates how to create a horizontal boxplot using pandas' plot.box() method. The vert=False parameter makes the boxplot horizontal, and positions allows custom positioning of the boxes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.plot.box(vert=False, positions=[1, 4, 5, 6, 8]);\n```\n\n----------------------------------------\n\nTITLE: Selecting Data with IntervalIndex by Specific Interval\nDESCRIPTION: This code shows how to select data from a DataFrame with an IntervalIndex by directly specifying an interval object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[pd.Interval(1.5, 3.0)]\n```\n\n----------------------------------------\n\nTITLE: Reading XML with XPath using pandas.read_xml (Python)\nDESCRIPTION: Demonstrates reading XML content with pandas.read_xml using an XPath expression to extract rows. Shows limitation where only child elements or attributes of the target XPath node are parsed, while attributes of child nodes (i.e., grandchildren) are not included. Requires pandas and StringIO from io.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_121\n\nLANGUAGE: python\nCODE:\n```\nxml = \"\"\"\n<data>\n  <row>\n    <shape sides=\\\"4\\\">square</shape>\n    <degrees>360</degrees>\n  </row>\n  <row>\n    <shape sides=\\\"0\\\">circle</shape>\n    <degrees>360</degrees>\n  </row>\n  <row>\n    <shape sides=\\\"3\\\">triangle</shape>\n    <degrees>180</degrees>\n  </row>\n</data>\"\"\"\n\ndf = pd.read_xml(StringIO(xml), xpath=\"./row\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Converting Categorical Data Back to Original Type in Python\nDESCRIPTION: Demonstrates how to convert categorical data back to its original data type using astype() or np.asarray(). This is useful when you need to revert categorical data to its original form.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"])\ns\ns2 = s.astype(\"category\")\ns2\ns2.astype(str)\nnp.asarray(s2)\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to Feather File using pandas (Python)\nDESCRIPTION: This snippet writes a pandas DataFrame to a Feather file called 'example.feather'. Requires pandas and a previously created DataFrame. Feather provides efficient binary serialization and preserves most pandas dtypes. Indexes are not supported for serialization; using non-default indexes will raise an error.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_203\n\nLANGUAGE: python\nCODE:\n```\ndf.to_feather(\"example.feather\")\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame for Demonstration in Python\nDESCRIPTION: Initializes a sample DataFrame with three numeric columns (AAA, BBB, CCC) for demonstration purposes. This DataFrame is used as the foundation for many of the examples in the cookbook.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]}\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Setting DataFrame Item with MultiIndex in Python\nDESCRIPTION: Fixed regression in DataFrame.__setitem__ that was raising an AttributeError when using a MultiIndex and a non-monotonic indexer.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndf.__setitem__(non_monotonic_indexer, value)\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: DataFrame.any with Category Dtype (ipython)\nDESCRIPTION: Shows that previously, DataFrame.any on a DataFrame with only a single categorical column would yield a boolean True for the column, potentially diverging from Series.any on a category. Output is a Series indicating True for the column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_13\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: df.any()\nOut[5]:\nA    True\ndtype: bool\n```\n\n----------------------------------------\n\nTITLE: Using IntervalIndex as Index in DataFrame\nDESCRIPTION: This code demonstrates how to create a DataFrame with an IntervalIndex and the different ways to select data using interval-based indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': range(4),\n                   'B': pd.cut([0, 3, 1, 1], bins=c.categories)\n                   }).set_index('B')\ndf\n```\n\n----------------------------------------\n\nTITLE: Loading and Filtering PM2.5 Air Quality Data with pandas in Python\nDESCRIPTION: This snippet loads particulate matter (PM2.5) measurements from a CSV file into a pandas DataFrame. It then selects only essential columns and displays the head of the DataFrame. Make sure the specified CSV file is available in the given path. Dependencies: pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nair_quality_pm25 = pd.read_csv(\"data/air_quality_pm25_long.csv\",\n                                   parse_dates=True)\nair_quality_pm25 = air_quality_pm25[[\"date.utc\", \"location\",\n                                     \"parameter\", \"value\"]]\nair_quality_pm25.head()\n```\n\n----------------------------------------\n\nTITLE: Type Conversion using Converters\nDESCRIPTION: Demonstrates using converters parameter to enforce string type conversion when reading CSV data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndata = \"col_1\\n1\\n2\\n'A'\\n4.22\"\ndf = pd.read_csv(StringIO(data), converters={\"col_1\": str})\ndf\ndf[\"col_1\"].apply(type).value_counts()\n```\n\n----------------------------------------\n\nTITLE: Pandas DataFrame and Series Methods - Bug Fixes\nDESCRIPTION: Details of bug fixes in core Pandas functionality including to_json, read_json, indexing operations, and data type handling. The fixes address issues with JSON serialization, duplicate indices, and type conversions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_43\n\nLANGUAGE: Python\nCODE:\n```\nto_json(orient='split')  # Fixed handling of duplicate columns\nread_json(orient='split')  # Fixed type conversion with duplicate columns\nDataFrame.set_values()  # Fixed name attribute preservation\nSeries.astype(str)  # Fixed string truncation\nDataFrame.apply()  # Fixed exception reraising\nDataFrame.iloc[]  # Fixed slice indexing\nDataFrame.loc[]  # Fixed duplicate index handling\nPanel.tshift()  # Added frequency support\n```\n\n----------------------------------------\n\nTITLE: Resampling and Aggregation with pandas DataFrame - Python\nDESCRIPTION: Demonstrates how to use pandas DataFrame resampling with different aggregation methods, such as min and mean, over a 2-second interval. This requires a DataFrame (df) with a DateTime index and pandas properly imported. Returns resampled DataFrame with the selected aggregation for each time bin.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf.resample('2s').min()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.resample('2s').mean().min()\n```\n\n----------------------------------------\n\nTITLE: GroupBy nth and Filtering Behavior - pandas - Python\nDESCRIPTION: Presents the use of the nth method on a groupby object, showing both reduction (returning the nth row from each group) and filtering. Includes use of dropna with 'any' to ignore NaN values and as_index=False to retain group labels as columns. Demonstrates equivalence with first() and last(), and the effect of groupby parameterization on the result structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[1, np.nan], [1, 4], [5, 6]], columns=['A', 'B'])\\ng = df.groupby('A')\\ng.nth(0)\n```\n\nLANGUAGE: python\nCODE:\n```\n# this is equivalent to g.first()\\ng.nth(0, dropna='any')\n```\n\nLANGUAGE: python\nCODE:\n```\n# this is equivalent to g.last()\\ng.nth(-1, dropna='any')\n```\n\nLANGUAGE: python\nCODE:\n```\ngf = df.groupby('A', as_index=False)\\ngf.nth(0)\\ngf.nth(0, dropna='any')\n```\n\n----------------------------------------\n\nTITLE: Constructing Series from Existing Series and Observing Mutation Propagation - pandas - Python\nDESCRIPTION: Explores creation of a new pandas Series from an existing Series and how mutations to the child propagate to the parent under current rules. Requires pandas, with an input Series s. The code creates another Series s2 from s, mutates s2 at position 0, and inspects s to observe mutation effect. The key parameters are the source Series and mutation site; output is a parent Series that reflects changes made in the derived Series. Under Copy-on-Write, such cross-linked mutation will be avoided.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n>>> s = pd.Series([1, 2, 3])\n>>> s2 = pd.Series(s)\n>>> s2.iloc[0] = 0   # will also modify the parent Series s\n>>> s\n0\t0  # <-- modified\n1\t2\n2\t3\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Handling Mixed Data Types in Pandas HDF5 Store\nDESCRIPTION: Demonstrates support for mixed data types when appending and selecting data from a Pandas HDF5 store. It shows how to add string and integer columns to a dataframe and store it.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf['string'] = 'string'\ndf['int'] = 1\nstore.append('df', df)\ndf1 = store.select('df')\ndf1.get_dtype_counts()\n```\n\n----------------------------------------\n\nTITLE: Copying Categorical Objects when Constructing Series - pandas - Python\nDESCRIPTION: Demonstrates safe copying of Categorical when creating a Series using copy=True to prevent side effects. Ensures independence between the new Series and the original Categorical. Dependencies: pandas. Key parameters: copy parameter. Input: Categorical object; Output: Unchanged original upon Series mutation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ncat = pd.Categorical([1, 2, 3, 10], categories=[1, 2, 3, 4, 10])\ns = pd.Series(cat, name=\"cat\", copy=True)\ncat\ns.iloc[0:2] = 10\ncat\n```\n\n----------------------------------------\n\nTITLE: True Division in pandas Series\nDESCRIPTION: Demonstrates true division behavior in pandas Series objects, showing how floating point division is performed by default.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_2\n\nLANGUAGE: ipython\nCODE:\n```\nIn [7]: pd.Series(arr) / pd.Series(arr2)  # no future import required\nOut[7]:\n0    0.200000\n1    0.666667\n2    1.500000\n3    4.000000\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Fare by Sex and Passenger Class Combination - pandas.groupby() - Python\nDESCRIPTION: This snippet groups Titanic passengers by both 'Sex' and 'Pclass' columns and computes the mean 'Fare' for each unique combination of sex and class. The groupby method receives a list of columns and .mean() is applied to the 'Fare' column. Requires the DataFrame to include 'Sex', 'Pclass', and 'Fare' columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/06_calculate_statistics.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntitanic.groupby([\"Sex\", \"Pclass\"])[\"Fare\"].mean()\n```\n\n----------------------------------------\n\nTITLE: Appending Data Without Index Creation and Creating Index Post-Append - pandas HDFStore - Python\nDESCRIPTION: This snippet illustrates appending multiple DataFrames to an HDFStore table without building indices on each append operation and then creating the index after appending is complete. It demonstrates disabling index creation using index=False for performance during bulk inserts, followed by explicit invocation of create_table_index. It requires pandas, numpy, and an HDFStore opened in write mode. Inputs are multiple DataFrames (df_1, df_2). The output shows the table state post-appending and after index creation. This approach is suitable for high-throughput data loading where index rebuilds are deferred.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_184\n\nLANGUAGE: python\nCODE:\n```\ndf_1 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\ndf_2 = pd.DataFrame(np.random.randn(10, 2), columns=list(\"AB\"))\n\nst = pd.HDFStore(\"appends.h5\", mode=\"w\")\nst.append(\"df\", df_1, data_columns=[\"B\"], index=False)\nst.append(\"df\", df_2, data_columns=[\"B\"], index=False)\nst.get_storer(\"df\").table\n```\n\n----------------------------------------\n\nTITLE: Using DataFrame.reindex with axis Parameter in Python\nDESCRIPTION: Demonstrates the enhanced reindex method that accepts the axis parameter to specify which axis to target. This makes reindexing rows or columns more explicit and readable.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.reindex(['A', 'B', 'C'], axis='columns')\ndf.reindex([0, 1, 3], axis='index')\n```\n\n----------------------------------------\n\nTITLE: Automating Regression Bisection with Git and Shell in Bash\nDESCRIPTION: These commands automate regression analysis in pandas using git bisect and bash scripting. After creating a test script (e.g., t.py), they run git bisect between known good and bad versions, reinstall the package in-place, and execute the regression test on each commit. The process isolates the first commit introducing a bug. Dependencies: git, bash, Python, pip, pandas and required build tools. Inputs are commit references and pandas source; the output is the commit hash introducing the regression. Long builds may occur due to C extension recompilation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/maintaining.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit bisect start\\ngit bisect good v1.4.0\\ngit bisect bad v1.5.0\\ngit bisect run bash -c \\\"python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true; python t.py\\\"\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit bisect reset\\npython -m pip install -ve . --no-build-isolation -Ceditable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Updating IntervalArray Value Setting in Python\nDESCRIPTION: Bug fix for IntervalArray, preventing changes to underlying data when setting values, maintaining data integrity.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nia = pd.IntervalArray.from_tuples([(0, 1), (1, 2)])\nia[0] = pd.Interval(2, 3)  # Now correctly updates without modifying underlying data\n```\n\n----------------------------------------\n\nTITLE: Importing pandas Library in Python\nDESCRIPTION: This snippet imports the pandas library, which is essential for loading, manipulating, and combining tabular data. All subsequent operations depend on pandas being available as 'pd'. Make sure pandas is installed in your Python environment before running the following tutorials.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: String Methods with PyArrow Backed Strings\nDESCRIPTION: Shows how string accessor methods work with PyArrow backed string arrays. The return types maintain string dtype where appropriate, while count operations return Int64Dtype values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ns.str.upper()\ns.str.split('b', expand=True).dtypes\ns.str.count(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Defining KeyValue Struct for Parquet Metadata - Shell\nDESCRIPTION: This snippet outlines the structure of the KeyValue object, which is used to hold individual metadata entries in Parquet files. Each KeyValue consists of a required key (string) and an optional value (string). It relies on Parquet's Thrift IDL for schema definition.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/developer.rst#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n   struct KeyValue {\n     1: required string key\n     2: optional string value\n   }\n```\n\n----------------------------------------\n\nTITLE: Storing datetime64 Data Columns in HDFStore\nDESCRIPTION: Demonstrates storing a DataFrame with datetime64 column types in HDFStore and retrieving it, along with examining the data types of the retrieved DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.1.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf_mixed = df.copy()\ndf_mixed[\"datetime64\"] = pd.Timestamp(\"20010102\")\ndf_mixed.loc[df_mixed.index[3:4], [\"A\", \"B\"]] = np.nan\n\nstore.append(\"df_mixed\", df_mixed)\ndf_mixed1 = store.select(\"df_mixed\")\ndf_mixed1\ndf_mixed1.dtypes.value_counts()\n```\n\n----------------------------------------\n\nTITLE: Parallel Local Test Execution with pytest-xdist in Pandas (Bash)\nDESCRIPTION: Illustrates use of pytest-xdist to run tests across multiple CPU cores for speed. The '-n' flag specifies the number of parallel workers or 'auto' for all cores. Requires pytest-xdist installed; improves turnaround for local test runs in multicore environments.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\npytest -n 4 pandas\n```\n\nLANGUAGE: bash\nCODE:\n```\npytest -n auto pandas\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data for HDF5 Storage in Pandas\nDESCRIPTION: Creates sample data objects including a Series with string indices and a DataFrame with a DatetimeIndex to be stored in the HDF5 file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_163\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.date_range(\"1/1/2000\", periods=8)\ns = pd.Series(np.random.randn(5), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\ndf = pd.DataFrame(np.random.randn(8, 3), index=index, columns=[\"A\", \"B\", \"C\"])\n\n# store.put('s', s) is an equivalent method\nstore[\"s\"] = s\n\nstore[\"df\"] = df\n\nstore\n```\n\n----------------------------------------\n\nTITLE: Scalar Float Indexing Allowed (Deprecation Warning) - Pandas - ipython\nDESCRIPTION: Depicts deprecated, but currently permitted, use of a scalar float as an indexer on Int64Index. Outputs the value, but this behavior is marked for deprecation. Shows backward compatibility and future-proofing notes. No extra dependencies. Output: scalar value from Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_9\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: pd.Series(range(5))[3.0]\\nOut[3]: 3\n```\n\n----------------------------------------\n\nTITLE: Adding a Row to a DataFrame in Pandas\nDESCRIPTION: Shows how to add a new row to a pandas DataFrame using concat, which is similar to adding a row in Excel. The example creates a new row and appends it to the existing DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf\nnew_row = pd.DataFrame([[\"E\", 51, True]],\n                       columns=[\"class\", \"student_count\", \"all_pass\"])\npd.concat([df, new_row])\n```\n\n----------------------------------------\n\nTITLE: Sorting a DataFrame by Key Before Merge - PROC SORT in SAS\nDESCRIPTION: Before performing a merge in SAS, datasets must be sorted by the join key. This snippet sorts 'df1' by 'key' using 'PROC SORT'. Necessary prerequisite for subsequent DATA step merge operations. Input: unsorted dataset; Output: sorted dataset by key.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_18\n\nLANGUAGE: SAS\nCODE:\n```\nproc sort data=df1;\\n    by key;\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Demonstrating New Concatenation Behavior with Categorical Data in Python\nDESCRIPTION: This snippet shows how concatenating a Categorical of ints with NA values now results in object dtype when combined with non-Categorical data in pandas 0.24.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([0, 1, np.nan])\nc = pd.Series([0, 1, np.nan], dtype=\"category\")\n\npd.concat([s, c])\n```\n\n----------------------------------------\n\nTITLE: Index Addition and Subtraction: Set Operations vs. Elementwise (ipython, Python)\nDESCRIPTION: Compares legacy set operation behavior using '+' and '-' on Index objects with new element-wise operations, affecting string, object, and datetime index types. Demonstrates both the deprecated and updated usage, highlighting output changes. Requires pandas; input is pairs of Index objects, output is either set union/difference or elementwise results. Now, explicit set operation methods should be used for set-like behaviors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_36\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pd.Index(['a', 'b']) + pd.Index(['a', 'c'])\nFutureWarning: using '+' to provide set union with Indexes is deprecated, use '|' or .union()\nOut[1]: Index(['a', 'b', 'c'], dtype='object')\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index([\"a\", \"b\"]) + pd.Index([\"a\", \"c\"])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index([1, 2, 3]) + pd.Index([2, 3, 4])\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: (pd.DatetimeIndex(['2016-01-01', '2016-01-02'])\n   ...:  - pd.DatetimeIndex(['2016-01-02', '2016-01-03']))\nFutureWarning: using '-' to provide set differences with datetimelike Indexes is deprecated, use .difference()\nOut[1]: DatetimeIndex(['2016-01-01'], dtype='datetime64[ns]', freq=None)\n```\n\nLANGUAGE: python\nCODE:\n```\n(\n    pd.DatetimeIndex([\"2016-01-01\", \"2016-01-02\"]) \n    - pd.DatetimeIndex([\"2016-01-02\", \"2016-01-03\"])\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating String Length in Excel Formulas\nDESCRIPTION: Shows how to calculate the length of a string in Excel using the LEN and TRIM functions. This formula counts the number of characters in a trimmed string from cell A2.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_spreadsheets.rst#_snippet_4\n\nLANGUAGE: excel\nCODE:\n```\n=LEN(TRIM(A2))\n```\n\n----------------------------------------\n\nTITLE: Updating MultiIndex Level Names Using set_names in pandas (Python)\nDESCRIPTION: Illustrates updating MultiIndex level names in pandas by using the set_names method. The operation returns a new MultiIndex object with the specified name changes. Requires pandas and a MultiIndex named mi.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmi2 = mi.set_names(\"new name\", level=0)\nmi2.names\n```\n\n----------------------------------------\n\nTITLE: Initializing DataFrame with Categorical Data - pandas - Python\nDESCRIPTION: This snippet creates a pandas DataFrame with a categorical column 'x' having categories [1, 2, 3] (including a missing value), and a numeric column 'y'. It serves as a setup for further groupby regression tests, where the handling of NA and unobserved categories is important. Required: pandas library (imported as pd). Input: lists for 'x' and 'y'; Output: DataFrame with defined categories, ready for groupby demonstrations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"x\": pd.Categorical([1, None], categories=[1, 2, 3]),\n        \"y\": [3, 4],\n    }\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Installing Pandas with PyArrow Using Pip\nDESCRIPTION: New pip installation command that allows installing pandas along with pyarrow dependency using a single command.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.1.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pandas[pyarrow]\n```\n\n----------------------------------------\n\nTITLE: Other Pandas Enhancements\nDESCRIPTION: Shows usage of new features including BusinessHour offset and enhanced DataFrame operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.1.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.Timestamp(\"2014-08-01 09:00\") + pd.tseries.offsets.BusinessHour()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(3, 3), columns=[\"A\", \"B\", \"C\"])\ndf.drop([\"A\", \"X\"], axis=1, errors=\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Creating Text Series with pandas (Python)\nDESCRIPTION: Demonstrates the default creation of a pandas Series from a list of strings, which results in the object dtype for backward compatibility. This snippet has no external dependencies aside from pandas and takes a simple list of strings as input, returning a Series object. The output Series shows its inferred dtype as object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npd.Series([\"a\", \"b\", \"c\"])\n```\n\n----------------------------------------\n\nTITLE: Creating Resampler Objects\nDESCRIPTION: Examples of how to create Resampler objects using the resample method on DataFrame and Series objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/resampling.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndf.resample(rule)\nseries.resample(rule)\n```\n\n----------------------------------------\n\nTITLE: Setting a Seed for Reproducibility on Unix Before Pytest (Bash)\nDESCRIPTION: Exports the PYTHONHASHSEED environment variable using Unix shell, then runs pytest with advanced options mirroring the Windows example. Ensures determinism in test runs where hash order can affect outcomes. Requires bash/zsh or compatible Unix shell.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nexport PYTHONHASHSEED=314159265\npytest pandas -n 4 -m \"not slow and not network and not db and not single_cpu\" -r sxX\n```\n\n----------------------------------------\n\nTITLE: Checking Weak and Strict Monotonicity of Index - pandas - Python\nDESCRIPTION: Demonstrates creating a pandas Index and assessing both weak and strict monotonicity using is_monotonic_increasing and is_unique attributes. This checks whether the index increases and whether all labels are unique. Useful for validating conditions required by certain DataFrame or Series slicing operations. Requires pandas to be imported as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nweakly_monotonic = pd.Index([\"a\", \"b\", \"c\", \"c\"])\nweakly_monotonic\nweakly_monotonic.is_monotonic_increasing\nweakly_monotonic.is_monotonic_increasing & weakly_monotonic.is_unique\n```\n\n----------------------------------------\n\nTITLE: Boolean Indexing on DataFrames using iloc and loc - Pandas - Python\nDESCRIPTION: Illustrates the correct usage of boolean indexing with pandas DataFrames using both 'loc' (label-based) and 'iloc' (position-based) accessors. Demonstrates why 'iloc' with a labeled boolean Series raises a ValueError, emphasizing that 'iloc' is strictly positional. Shows how to work with masks using both label and positional APIs. Requires pandas installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(range(5), index=list(\"ABCDE\"), columns=[\"a\"])\nmask = df.a % 2 == 0\nmask\n\n# this is what you should use\ndf.loc[mask]\n\n# this will work as well\ndf.iloc[mask.values]\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet File with dtype_backend using pandas (Python)\nDESCRIPTION: Reads a Parquet file with 'pyarrow' engine, specifying dtype_backend='pyarrow' to control the default dtypes of the resulting DataFrame. Only supported for the pyarrow engine. Outputs the DataFrame's dtypes, potentially using pyarrow-backed dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_209\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\", dtype_backend=\"pyarrow\")\n\nresult.dtypes\n```\n\n----------------------------------------\n\nTITLE: Reindexing with MultiIndex in Python\nDESCRIPTION: Shows how to use the reindex method with a MultiIndex or a list of tuples, allowing for flexible reordering and alignment of hierarchically indexed data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ns.reindex(index[:3])\ns.reindex([(\"foo\", \"two\"), (\"bar\", \"one\"), (\"qux\", \"one\"), (\"baz\", \"one\")])\n```\n\n----------------------------------------\n\nTITLE: Deleting Files with os.remove - Python\nDESCRIPTION: This snippet shows how to remove a file ('mi.csv') from the filesystem using the os module in Python. It is typically used for cleanup after performing file IO operations in scripts or tests. Prerequisite: the file must exist and you should have permission to delete it. Throws an error if the file does not exist or cannot be removed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.remove(\"mi.csv\")\n```\n\n----------------------------------------\n\nTITLE: Serializing DataFrame to JSON with Type Information using ntv_pandas in Python\nDESCRIPTION: This Python snippet shows how to convert a pandas DataFrame containing rich data types to a compact JSON representation using the ntv_pandas library. The resulting JSON includes explicit type annotations and named fields to preserve information so that the DataFrame can be reconstructed losslessly. The code depends on pandas and ntv_pandas. The core input is a typed DataFrame, and the output is a JSON object with type-specific keys and values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0012-compact-and-reversible-JSON-interface.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nIn [5]: df_to_json = npd.to_json(df)\n        pprint(df_to_json, width=120)\nOut[5]: {':tab': {'coord::point': [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0], [7.0, 8.0], [3.0, 4.0], [5.0, 6.0]],\n                  'dates::date': ['1964-01-01', '1985-02-05', '2022-01-21', '1964-01-01', '1985-02-05', '2022-01-21'],\n                  'index': [100, 200, 300, 400, 500, 600],\n                  'names::string': ['john', 'eric', 'judith', 'mila', 'hector', 'maria'],\n                  'res': [10, 20, 30, 10, 20, 30],\n                  'unique': [True, True, True, True, True, True],\n                  'value': [10, 10, 20, 20, 30, 30],\n                  'value32::int32': [12, 12, 22, 22, 32, 32]}}\n```\n\n----------------------------------------\n\nTITLE: GroupBy with Column Selection - pandas - IPython\nDESCRIPTION: Demonstrates selecting specific columns (here, 'B') from a groupby object and then applying head(1) to filter rows within each group. Highlights that column projection works naturally with groupby objects post pandas 0.14.0, and the output aligns shape and content accordingly. Useful when only a subset of grouped columns is of interest.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_8\n\nLANGUAGE: ipython\nCODE:\n```\nIn [19]: g[['B']].head(1)\\nOut[19]:\\n   B\\n0  2\\n2  6\\n\\n[2 rows x 1 columns]\n```\n\n----------------------------------------\n\nTITLE: DataFrame GroupBy with Period dtype in Python\nDESCRIPTION: Fixed regression in DataFrame.groupby whereby taking the minimum or maximum of a column with period dtype would raise a TypeError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndf.groupby('column')['period_column'].max()\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame Values Using Pandas - Python\nDESCRIPTION: Sorts a pandas DataFrame by a multiindex column (specifically, by the 'Labs' and 'II' columns) in descending order. Requires pandas and assumes the DataFrame exists in the current environment. Input is an existing DataFrame; output is a sorted DataFrame. The 'by' parameter expects a tuple for multi-level columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf.sort_values(by=(\"Labs\", \"II\"), ascending=False)\n```\n\n----------------------------------------\n\nTITLE: Disallowing Forward Propagation of Mutations to Subsets in pandas (Python)\nDESCRIPTION: This snippet demonstrates that after subsetting, changes to the parent DataFrame do not affect the previously created subset. Dependencies: pandas. Parameters: DataFrame creation, subsetting, and assignment. Expected input is a parent DataFrame and its view/copy; output shows the subset remains unchanged upon parent mutation. Reinforces the isolation of objects created via indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n>>> df2 = df[[\"A\"]]\n>>> df.iloc[0, 0] = 10\n>>> df2.iloc[0, 0]  # what is this value?\n```\n\n----------------------------------------\n\nTITLE: Series Operations in Pandas\nDESCRIPTION: New Series functionality including name attribute for improved representation, isnull/notnull methods for null checking, and align method for series alignment with flexible join options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.4.x.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nSeries.isnull()\nSeries.notnull()\nSeries.align()\n```\n\n----------------------------------------\n\nTITLE: Kernel Density Estimation Plot in Pandas\nDESCRIPTION: Shows how to create a histogram with kernel density estimation (KDE) plot using the new plotting capabilities. Combines histogram and KDE visualization for distribution analysis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.8.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(\n    np.concatenate((np.random.randn(1000), np.random.randn(1000) * 0.5 + 3))\n)\nplt.figure()\ns.hist(density=True, alpha=0.2)\ns.plot(kind=\"kde\")\n```\n\n----------------------------------------\n\nTITLE: Modifying Timestamps with Nanosecond Offsets (Python)\nDESCRIPTION: Shows adding a nanosecond offset to a Timestamp using pd.tseries.offsets.Nano. Requires pandas >= 0.13. Input: pd.Timestamp object; output: modified Timestamp. Ideal for fine-tuning event times and microstructure data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nt = pd.Timestamp('20130101 09:01:02')\nt + pd.tseries.offsets.Nano(123)\n```\n\n----------------------------------------\n\nTITLE: Restoring pandas Display Precision to Default after Tests (Python)\nDESCRIPTION: This suppressed snippet restores the 'display.precision' option in pandas back to its default value of 6, to ensure future displays utilize the standard precision after customizations for documentation or tests. This is particularly useful in notebook environments where global settings may affect unrelated code execution. Dependencies: pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.precision\", 6)\n```\n\n----------------------------------------\n\nTITLE: Loading Parameters Metadata Table with pandas in Python\nDESCRIPTION: This snippet loads a CSV file containing metadata for parameters measured at each station into a pandas DataFrame, and shows the head for inspection. Needed for later joining with the measurements table to add descriptive info for each parameter code. Dependencies: pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nair_quality_parameters = pd.read_csv(\"data/air_quality_parameters.csv\")\nair_quality_parameters.head()\n```\n\n----------------------------------------\n\nTITLE: Creating Bootstrap Plot for Statistical Uncertainty Analysis\nDESCRIPTION: Generates a bootstrap plot to assess the uncertainty of a statistic. The plot is created by resampling from random data, computing the statistic for each sample, and visualizing the distribution of results to understand statistical reliability.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.plotting import bootstrap_plot\n\ndata = pd.Series(np.random.rand(1000))\n\nbootstrap_plot(data, size=50, samples=500, color=\"grey\");\n```\n\n----------------------------------------\n\nTITLE: Using .iloc with get_indexer for Multiple Columns\nDESCRIPTION: Shows how to select multiple columns using .iloc with the get_indexer method to convert column labels to positional indices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndfd.iloc[[0, 2], dfd.columns.get_indexer(['A', 'B'])]\n```\n\n----------------------------------------\n\nTITLE: Type Inference Changes in pandas.array with Missing Values (Python, pandas 1.0.0)\nDESCRIPTION: Shows how pandas.array now infers extension types for string and integer data with missing values, returning StringArray and IntegerArray respectively. Assumes pandas is imported. Demonstrates updated dtype behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npd.array([\"a\", None])\npd.array([1, None])\n```\n\n----------------------------------------\n\nTITLE: Creating Categoricals with Missing Data - pandas - Python\nDESCRIPTION: Demonstrates creation of a pandas Series with categorical dtype containing missing values (np.nan). Shows how pandas treats missing values as distinct from categories and how to access their underlying codes. Dependencies: pandas, numpy. Key parameters: input values (categories and NaNs). Input: List of string or np.nan values; Output: Series with categorical dtype and corresponding codes. Limitation: NaN should not be included in categories; missing values get code -1.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"b\", np.nan, \"a\"], dtype=\"category\")\n# only two categories\ns\ns.cat.codes\n```\n\n----------------------------------------\n\nTITLE: Basic Boxplot with DataFrame.boxplot() Method\nDESCRIPTION: This example shows how to create a boxplot using the original DataFrame.boxplot() interface. It first creates a DataFrame with random values then plots the boxplot.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(10, 5))\nplt.figure();\n\nbp = df.boxplot()\n```\n\n----------------------------------------\n\nTITLE: Selecting Only Valid Keys with Index Intersection\nDESCRIPTION: Shows an efficient way to select only existing labels from a Series by intersecting the Series index with the desired labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nlabels = [1, 2, 3]\ns.loc[s.index.intersection(labels)]\n```\n\n----------------------------------------\n\nTITLE: Styling Excel Output with Pandas Styler\nDESCRIPTION: Shows how to apply CSS-like styling to Excel outputs using Pandas Styler, including borders and font weight modifications.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_147\n\nLANGUAGE: python\nCODE:\n```\ncss = \"border: 1px solid black; font-weight: bold;\"\ndf.style.map_index(lambda x: css).map_index(lambda x: css, axis=1).to_excel(\"myfile.xlsx\")\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing with Modin\nDESCRIPTION: This code demonstrates using Modin as a drop-in replacement for pandas to parallelize operations across all available CPU cores. Simply changing the import statement allows existing pandas code to leverage multiple cores for operations like reading CSV files.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# import pandas as pd\nimport modin.pandas as pd\n\ndf = pd.read_csv(\"big.csv\")  # use all your cores!\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame with Index to Parquet File using pandas (Python)\nDESCRIPTION: Serializes a basic pandas DataFrame containing columns 'a' and 'b' to a Parquet file ('test.parquet') using the 'pyarrow' engine. By default, the index is included as a column ('__index_level_0__'). Demonstrates the effect of index inclusion in serialization.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_212\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ndf.to_parquet(\"test.parquet\", engine=\"pyarrow\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying a DataFrame with Random Data - pandas - Python\nDESCRIPTION: This snippet constructs a pandas DataFrame with random values for columns 'a', 'b', and 'N', and the constant 'x'. It uses numpy for random data generation. The resulting DataFrame is then displayed. Requires pandas and numpy as dependencies, with the inputs generated procedurally and no external parameters required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000),\n        \"b\": np.random.randn(1000),\n        \"N\": np.random.randint(100, 1000, (1000)),\n        \"x\": \"x\",\n    }\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: DataFrame Indexing with Single datetime64 Column in Python\nDESCRIPTION: Fixed regression in DataFrame.loc and DataFrame.iloc when selecting a row containing a single datetime64 or timedelta64 column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndf.loc[0] # where df has a single datetime64 column\n```\n\n----------------------------------------\n\nTITLE: Selecting Subsets of Multiple Columns by Group with pandas GroupBy Filtration (Python)\nDESCRIPTION: This example filters specific columns ('order', 'max_speed') from each group using the nth method. It's useful for extracting multiple columns from selected group members. The input assumes a DataFrame named speeds with grouping on 'class'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\nspeeds.groupby(\"class\")[[\"order\", \"max_speed\"]].nth(1)\n```\n\n----------------------------------------\n\nTITLE: Selecting Data with IntervalIndex by Contained Value\nDESCRIPTION: This code demonstrates how to select data from a DataFrame with an IntervalIndex by providing a scalar value that is contained within one of the intervals.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[0]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Groups from a GroupBy Object (Python)\nDESCRIPTION: Shows how to retrieve grouped data from a DataFrameGroupBy object by key with get_group(). Demonstrates accessing groups 'A' and 'B' for column 'X'. This snippet is useful for extracting specific subsets after grouping. pandas (as pd) must be imported and DataFrame 'df3' is required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf3 = pd.DataFrame({\"X\": [\"A\", \"B\", \"A\", \"B\"], \"Y\": [1, 4, 3, 2]})\ndf3.groupby(\"X\").get_group(\"A\")\n\ndf3.groupby([\"X\"]).get_group((\"B\",))\n```\n\n----------------------------------------\n\nTITLE: Array Conversion and NumPy Compatibility Warning - Pandas, NumPy - Python\nDESCRIPTION: This snippet demonstrates the creation of a DatetimeIndex using pd.date_range, its conversion to a plain NumPy array (which yields an array of datetime64 values), and casting that array to dtype=object. It illustrates known bugs in NumPy 1.6 where converting to a string or to object dtype can result in invalid representations. Dependencies: pandas as pd, numpy as np. Inputs: date_range parameters defining the DatetimeIndex; Outputs: arrays of datetime64 or object dtype. This example reinforces the best practice of using the pandas API for datetime manipulations under NumPy 1.6.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.8.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nrng = pd.date_range(\"1/1/2000\", periods=10)\nrng\nnp.asarray(rng)\nconverted = np.asarray(rng, dtype=object)\nconverted[5]\n```\n\n----------------------------------------\n\nTITLE: Setting NaT with np.nan in Timedelta Series - pandas - Python\nDESCRIPTION: Demonstrates assigning np.nan to elements of a timedelta Series, setting them as NaT. Useful for representing missing values in time deltas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ny[1] = np.nan\ny\n```\n\n----------------------------------------\n\nTITLE: Selecting First Row per Group with groupby.first - pandas (Python)\nDESCRIPTION: Provides a pandas equivalent to Stata's bysort-list operation, using groupby on columns 'sex' and 'smoker' and calling .first() to retrieve the first row of each group. Requires an existing pandas DataFrame named tips. Inputs: DataFrame with columns sex, smoker; Outputs: DataFrame grouped and reduced to first entry per group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ntips.groupby([\"sex\", \"smoker\"]).first()\n```\n\n----------------------------------------\n\nTITLE: Profiling DataFrame Apply with Cython Function - Python\nDESCRIPTION: Benchmarks DataFrame.apply using the compiled Cython function 'integrate_f_plain' via the timeit magic command. Shows speedup compared to the pure Python function. Requires IPython and successful prior Cython compilation. Outputs execution timing for optimized usage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%timeit df.apply(lambda x: integrate_f_plain(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n```\n\n----------------------------------------\n\nTITLE: Error on Transposing DataFrame with NoRowIndex Columns (Python)\nDESCRIPTION: Illustrates that attempting to transpose a DataFrame with NoRowIndex as its index raises a ValueError, as such usage is prohibited by the proposal. The error message guides users to reset the index before transposing. Inputs: DataFrame with NoRowIndex; Outputs: Exception with informative message.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nIn [4]: df = pd.DataFrame({'a': [1,  2, 3], 'b': [4, 5, 6]}, index=NoRowIndex(3))\n\nIn [5]: df.transpose()\n---------------------------------------------------------------------------\nValueError: Columns cannot be NoRowIndex.\nIf you got here via `transpose` or an `axis=1` operation, then you should first set an index, e.g.: `df.pipe(lambda _df: _df.set_axis(pd.RangeIndex(len(_df))))`\n```\n\n----------------------------------------\n\nTITLE: Integer Division in pandas with NumPy Arrays\nDESCRIPTION: Shows how integer division works with numpy arrays and pandas Series objects, demonstrating the floor division operator.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_1\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: arr = np.array([1, 2, 3, 4])\n\nIn [4]: arr2 = np.array([5, 3, 2, 1])\n\nIn [5]: arr / arr2\nOut[5]: array([0, 0, 1, 4])\n\nIn [6]: pd.Series(arr) // pd.Series(arr2)\nOut[6]:\n0    0\n1    0\n2    1\n3    4\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Including Contributors List with Sphinx Extension - reStructuredText\nDESCRIPTION: This snippet uses the custom 'contributors' directive in reStructuredText/Sphinx to automatically list contributors between two versions, v1.2.2 and v1.2.3. It requires the corresponding Sphinx extension or plugin capable of interpreting 'contributors::' blocks. The expected output is a rendered list of GitHub contributors for the specified version range. Proper configuration of Sphinx and the contributors extension is required; it is not standard reStructuredText and will be ignored without the correct Sphinx build context.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.3.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. contributors:: v1.2.2..v1.2.3\n```\n\n----------------------------------------\n\nTITLE: Grouping with Ordered Categorical Variables - pandas Python\nDESCRIPTION: Creates a DataFrame with an ordered Categorical variable, demonstrating that grouping maintains category order. Shows grouped sum with both sorted and unsorted category results. Inputs: categorical values and integer data. Outputs: summed counts by category.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\ndays = pd.Categorical(\n    values=[\"Wed\", \"Mon\", \"Thu\", \"Mon\", \"Wed\", \"Sat\"],\n    categories=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"],\n)\ndata = pd.DataFrame(\n   {\n       \"day\": days,\n       \"workers\": [3, 4, 1, 4, 2, 2],\n   }\n)\ndata\n```\n\nLANGUAGE: python\nCODE:\n```\ndata.groupby(\"day\", observed=False, sort=True).sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndata.groupby(\"day\", observed=False, sort=False).sum()\n```\n\n----------------------------------------\n\nTITLE: Boolean Operations and Gotchas in pandas Series in Python\nDESCRIPTION: Highlights a common error when using Series in if-statements for boolean evaluation. Demonstrates the exception and advises reader to consult further documentation for the correct approach. Requires pandas. Input is a boolean Series. Output is a logical error or exception.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nif pd.Series([False, True, False]):\\n    print(\"I was true\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Current Pandas Behavior with Type Upcasting\nDESCRIPTION: Shows how pandas currently allows type upcasting when setting values of different types, changing a Series from int64 to object dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0006-ban-upcasting.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, 3], dtype='int64')\n\nser[2] = 'potage'\n\nser  # dtype changed to 'object'!\nOut[3]:\n0         1\n1         2\n2    potage\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Creating and Aggregating DataFrames Using pandas - Python\nDESCRIPTION: This snippet demonstrates creating a DataFrame with random values, assigning NaNs to a range of rows, and then performing aggregation using the new .agg() API. It initializes a DataFrame with columns 'A', 'B', 'C' and a DateTime index, then applies aggregation functions to columns. The snippet requires pandas and numpy, input parameters include the number of periods and column names, and outputs a DataFrame with aggregated results. Limitations include potentially having NaN rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],\n                     index=pd.date_range('1/1/2000', periods=10))\ndf.iloc[3:7] = np.nan\ndf\n```\n\n----------------------------------------\n\nTITLE: Error Raised on Conflicting Arguments in DataFrame.rename (Python, pandas 1.0.0)\nDESCRIPTION: Highlights the error when both a positional 'mapper' and a keyword argument (index/columns) are provided in DataFrame.rename under pandas 1.0.0. Examples provided to trigger TypeError exceptions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf.rename({0: 1}, index={0: 2})\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.rename(mapper={0: 1}, index={0: 2})\n```\n\n----------------------------------------\n\nTITLE: Using String Contains Method with NA Values in Pandas\nDESCRIPTION: Demonstrates how to use the str.contains() method on a pandas Series with string data, specifying how to handle NA values using the na parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ns4 = pd.Series(\n    [\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\"\n)\ns4.str.contains(\"A\", na=False)\n```\n\n----------------------------------------\n\nTITLE: Timedelta Arithmetic with pandas Series and datetime - Python\nDESCRIPTION: Performs arithmetic with pandas Series of dates and Python's datetime objects, producing timedelta results and offsetting datetimes. Dependencies: pandas, datetime. Input: Series of dates. Output: Series of timedeltas for differences and summing. Shows flexibility in working with time differences and adding fixed timedeltas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\ns = pd.Series(pd.date_range(\"2012-1-1\", periods=3, freq=\"D\"))\n\ns - s.max()\n\ns.max() - s\n\ns - datetime.datetime(2011, 1, 1, 3, 5)\n\ns + datetime.timedelta(minutes=5)\n\ndatetime.datetime(2011, 1, 1, 3, 5) - s\n```\n\n----------------------------------------\n\nTITLE: Automatic dtype Inference in pandas Series Construction - Python\nDESCRIPTION: Shows how pandas infers the dtype when constructing a Series from a mixed list. Demonstrates int-to-float upcasting and use of object dtype when string data is present. No external dependencies beyond pandas. Inputs are Python lists, outputs are printed Series and their inferred dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\n# these ints are coerced to floats\npd.Series([1, 2, 3, 4, 5, 6.0])\n\n# string data forces an ``object`` dtype\npd.Series([1, 2, 3, 6.0, \"foo\"])\n```\n\n----------------------------------------\n\nTITLE: Class-based Styling Implementation - Sub-optimal vs Optimized\nDESCRIPTION: Demonstrates efficient styling using CSS classes instead of direct style application for large DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf2.style.apply(\n    highlight_max, props=\"color:white;background-color:darkblue;\", axis=0\n).apply(highlight_max, props=\"color:white;background-color:pink;\", axis=1).apply(\n    highlight_max, props=\"color:white;background-color:purple\", axis=None\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nbuild = lambda x: pd.DataFrame(x, index=df2.index, columns=df2.columns)\ncls1 = build(df2.apply(highlight_max, props=\"cls-1 \", axis=0))\ncls2 = build(\n    df2.apply(highlight_max, props=\"cls-2 \", axis=1, result_type=\"expand\").values\n)\ncls3 = build(highlight_max(df2, props=\"cls-3 \"))\ndf2.style.set_table_styles(\n    [\n        {\"selector\": \".cls-1\", \"props\": \"color:white;background-color:darkblue;\"},\n        {\"selector\": \".cls-2\", \"props\": \"color:white;background-color:pink;\"},\n        {\"selector\": \".cls-3\", \"props\": \"color:white;background-color:purple;\"},\n    ]\n).set_td_classes(cls1 + cls2 + cls3)\n```\n\n----------------------------------------\n\nTITLE: Serializing pandas DataFrame to JSON Table Schema with 'orient=\"table\"' - Python\nDESCRIPTION: This snippet demonstrates how to serialize a DataFrame into a JSON string that adheres to the Table Schema specification using orient=\"table\". The output includes both the schema (describing fields and types) and the data (serialized records). Index information is included. Dependencies: pandas; input: DataFrame with different column types and index; output: JSON string with fields, types, index and data. Dates use ISO formatting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_85\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"A\": [1, 2, 3],\n        \"B\": [\"a\", \"b\", \"c\"],\n        \"C\": pd.date_range(\"2016-01-01\", freq=\"D\", periods=3),\n    },\n    index=pd.Index(range(3), name=\"idx\"),\n)\ndf\ndf.to_json(orient=\"table\", date_format=\"iso\")\n```\n\n----------------------------------------\n\nTITLE: Using Callable with Where and Mask Methods in Pandas\nDESCRIPTION: Demonstrates the new ability to use callable functions with the where() and mask() methods in Pandas for more flexible data manipulation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [7, 8, 9]})\ndf.where(lambda x: x > 4, lambda x: x + 10)\n```\n\n----------------------------------------\n\nTITLE: Using Copy-on-Write Locally with Context Manager in pandas\nDESCRIPTION: Shows how to enable the copy-on-write feature locally using a context manager, which restricts the feature to a specific code block.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith pd.option_context(\"mode.copy_on_write\", True):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Viewing DataFrame dtypes and Conversion to NumPy (Heterogeneous Data) - Python\nDESCRIPTION: This demonstrates that DataFrames like 'df2' with heterogeneous column types will have their values converted to a NumPy array of the broadest compatible dtype, often 'object', when calling to_numpy(). Also displays dtypes with df2.dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf2.dtypes\\ndf2.to_numpy()\n```\n\n----------------------------------------\n\nTITLE: Parsing Excel File with Column Names in Python\nDESCRIPTION: Shows how to reference Excel columns by their names when parsing an Excel file using Pandas 0.9.1.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.9.1.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nxl = pd.ExcelFile('data/test.xls')\n\nxl.parse('Sheet1', index_col=0, parse_dates=True,\n               parse_cols='A:D')\n```\n\n----------------------------------------\n\nTITLE: Applying String Methods via Series.str Accessor (Python)\nDESCRIPTION: Shows usage of the pandas Series .str accessor to apply string methods such as isalpha and find to each value in the Series. Requires pandas. Inputs: a Series of strings. Outputs: a Series of boolean values (for isalpha) or integer results (for find), matching element-wise application of the corresponding string methods. Extends available string processing options, enabling vectorized string transformations and testing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['abcd', '3456', 'EFGH'])\ns.str.isalpha()\ns.str.find('ab')\n```\n\n----------------------------------------\n\nTITLE: Docstring Short Summary Anti-patterns in Python\nDESCRIPTION: Shows various incorrect ways to write the short summary paragraph in a Python docstring, such as incorrect verb form, omission of the period, or being overly verbose. Each 'astype' example is syntactically correct, but demonstrates a particular stylistic mistake to avoid, especially relevant for contributors to pandas and similar projects. No external dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef astype(dtype):\n    \"\"\"\n    Casts Series type.\n\n    Verb in third-person of the present simple, should be infinitive.\n    \"\"\"\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\ndef astype(dtype):\n    \"\"\"\n    Method to cast Series type.\n\n    Does not start with verb.\n    \"\"\"\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\ndef astype(dtype):\n    \"\"\"\n    Cast Series type\n\n    Missing dot at the end.\n    \"\"\"\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\ndef astype(dtype):\n    \"\"\"\n    Cast Series type from its current type to the new type defined in\n    the parameter dtype.\n\n    Summary is too verbose and doesn't fit in a single line.\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Sorting Combined Air Quality Data Chronologically with pandas in Python\nDESCRIPTION: This snippet sorts the combined air_quality DataFrame by 'date.utc' to ensure chronological ordering of all records. The head of the sorted DataFrame is displayed. This is important for time series analysis or visualization. Input: unsorted DataFrame; Output: chronologically sorted DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nair_quality = air_quality.sort_values(\"date.utc\")\nair_quality.head()\n```\n\n----------------------------------------\n\nTITLE: Creating and Inspecting a MultiIndex in Python\nDESCRIPTION: This snippet demonstrates how to create a MultiIndex using pd.MultiIndex.from_product() and inspect its components including levels, codes, and names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/internals.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.MultiIndex.from_product(\n    [range(3), [\"one\", \"two\"]], names=[\"first\", \"second\"]\n)\nindex\nindex.levels\nindex.codes\nindex.names\n```\n\n----------------------------------------\n\nTITLE: Checking allows_duplicate_labels Flag on DataFrame - pandas - Python\nDESCRIPTION: Demonstrates setting and checking the allows_duplicate_labels attribute on a DataFrame. Constructs a DataFrame with unique index labels, sets allows_duplicate_labels to False, and queries the flag value. Input is DataFrame; output is boolean indicating label allowance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [0, 1, 2, 3]}, index=[\"x\", \"y\", \"X\", \"Y\"]).set_flags(\n    allows_duplicate_labels=False\n)\ndf\ndf.flags.allows_duplicate_labels\n```\n\n----------------------------------------\n\nTITLE: Describe Function with Type Filtering\nDESCRIPTION: Shows enhanced describe() functionality with type-based column filtering using include/exclude parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'catA': ['foo', 'foo', 'bar'] * 8,\n                   'catB': ['a', 'b', 'c', 'd'] * 6,\n                   'numC': np.arange(24),\n                   'numD': np.arange(24.) + .5})\ndf.describe(include=[\"object\"])\ndf.describe(include=[\"number\", \"object\"], exclude=[\"float\"])\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in Series.cat.categories setter for category updates\nDESCRIPTION: Resolves a problem where the Series.cat.categories setter was failing to update the categories on the Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nSeries.cat.categories = new_categories\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with PyArrow Engine in pandas (Python)\nDESCRIPTION: Illustrates reading a CSV file into a pandas DataFrame using the engine=\\\"pyarrow\\\" argument, which delegates the parsing and loading to the Arrow IO implementation. Dependencies are pandas, pyarrow, and Python's io module. The input is a text CSV; output is a DataFrame. This method facilitates accelerated data ingestion and missing value handling.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport io\ndata = io.StringIO(\"\"\"a,b,c\n   1,2.5,True\n   3,4.5,False\n\"\"\")\ndf = pd.read_csv(data, engine=\"pyarrow\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Specifying Minimum Item Size for String Columns in HDF5 Store\nDESCRIPTION: This example demonstrates how to specify the minimum item size for string columns when creating an HDF5 store. It shows different ways to set the minimum size for all columns or specific columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_200\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.DataFrame({\"A\": \"foo\", \"B\": \"bar\"}, index=list(range(5)))\ndfs\n\n# A and B have a size of 30\nstore.append(\"dfs\", dfs, min_itemsize=30)\nstore.get_storer(\"dfs\").table\n\n# A is created as a data_column with a size of 30\n# B is size is calculated\nstore.append(\"dfs2\", dfs, min_itemsize={\"A\": 30})\nstore.get_storer(\"dfs2\").table\n```\n\n----------------------------------------\n\nTITLE: Converting Timedelta Series to Seconds using Division (Python)\nDESCRIPTION: This example shows converting a timedelta Series to total seconds via division with np.timedelta64(1, 's'). Requires pandas and NumPy. The Series 'td' is divided, outputting float64 seconds. Handles NaT values as NaN. Helpful for fine-grained time intervals or aggregation to smaller units.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntd / np.timedelta64(1, 's')\n```\n\n----------------------------------------\n\nTITLE: Defining Classes with Shadowed Builtins and Type Annotations in Python\nDESCRIPTION: Demonstrates defining a Python class with a class variable named after a Python builtin (str), and provides the recommended annotation technique by creating an explicit alias for the builtin type. This addresses mypy issues due to shadowing builtins. There are no dependencies except Python type hinting facilities. Purpose: maintain type safety and clarity when builtins are shadowed. Inputs: none; Outputs: None.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass SomeClass1:\n    str = None\n```\n\nLANGUAGE: python\nCODE:\n```\nstr_type = str\n\nclass SomeClass2:\n    str: str_type = None\n```\n\n----------------------------------------\n\nTITLE: String Extraction Methods in pandas\nDESCRIPTION: Demonstrates the enhanced str.extract and new str.extractall methods for capturing regex groups from strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.Series(['a1', 'b2', 'c3']).str.extract(r'[ab](\\d)', expand=False)\npd.Series(['a1', 'b2', 'c3']).str.extract(r'[ab](\\d)', expand=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a1a2\", \"b1\", \"c1\"], [\"A\", \"B\", \"C\"])\ns\ns.str.extractall(r\"(?P<letter>[ab])(?P<digit>\\d)\")\n```\n\n----------------------------------------\n\nTITLE: Reading and Printing CSV Files with pandas - Python\nDESCRIPTION: This snippet demonstrates simple file IO and multi-level header/index CSV reading with pandas. It reads and prints the contents of a CSV file named \"mi.csv\" and then loads it as a DataFrame using custom header and index columns. Dependencies: pandas (imported as pd), and a CSV file named \"mi.csv\" must exist in the working directory. The main parameters shown are 'header' and 'index_col', used for handling complex CSV structures.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(open(\"mi.csv\").read())\npd.read_csv(\"mi.csv\", header=[0, 1, 2, 3], index_col=[0, 1])\n```\n\n----------------------------------------\n\nTITLE: Categorical.argsort Missing Value Placement - Pandas - Python\nDESCRIPTION: Provides an example of creating an ordered categorical with missing values, and demonstrates the output order for argsort and for indexing by the argsort result. Reflects the updated behavior of placing missing values at the end. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ncat = pd.Categorical(['b', None, 'a'], categories=['a', 'b'], ordered=True)\n```\n\n----------------------------------------\n\nTITLE: Installing pandas with pip (Shell)\nDESCRIPTION: This snippet shows how to install pandas using the Python package installer pip, directly from the Python Package Index (PyPI). The command downloads and installs pandas in the current Python environment, also installing required dependencies automatically. Input is a shell command; output is that pandas is installed and ready for use. Requires pip to be available in the environment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# or PyPI\\npip install pandas\n```\n\n----------------------------------------\n\nTITLE: Controlling Column Header Justification in DataFrame Display - Python\nDESCRIPTION: Demonstrates how to adjust alignment of column headers using the 'colheader_justify' display option in pandas. Shows both 'right' and 'left' alignment with a test DataFrame. Useful for aesthetic or print layout customization in table outputs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    np.array([np.random.randn(6), np.random.randint(1, 9, 6) * 0.1, np.zeros(6)]).T,\n    columns=[\"A\", \"B\", \"C\"],\n    dtype=\"float\",\n)\npd.set_option(\"colheader_justify\", \"right\")\ndf\npd.set_option(\"colheader_justify\", \"left\")\ndf\npd.reset_option(\"colheader_justify\")\n```\n\n----------------------------------------\n\nTITLE: Pandas Rolling Window Correlation Example\nDESCRIPTION: Demonstrates rolling window correlation calculations with MultiIndex output\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(1234)\ndf = pd.DataFrame(np.random.rand(100, 2),\n                columns=pd.Index(['A', 'B'], name='bar'),\n                index=pd.date_range('20160101',\n                                    periods=100, freq='D', name='foo'))\ndf.tail()\n\nres = df.rolling(12).corr()\nres.tail()\n\ndf.rolling(12).corr().loc['2016-04-07']\n```\n\n----------------------------------------\n\nTITLE: Running asv with a Specific Python Interpreter (Bash)\nDESCRIPTION: Further restricts asv to use a given Python interpreter version (e.g., python3.6) with '-E existing:python3.6' for benchmark runs, ensuring compatibility or reproducibility. Requires that the interpreter is on PATH and all necessary dependencies are available.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\nasv run -e -E existing:python3.6\n```\n\n----------------------------------------\n\nTITLE: Launching Interactive Data Visualization with PyGWalker in Python\nDESCRIPTION: This snippet demonstrates the minimal workflow to initiate the PyGWalker interface for interactive data visualization and EDA on a pandas DataFrame. It imports the pygwalker library and launches its browser-based UI with a given DataFrame as input. Requires pygwalker and pandas to be installed; input is any pandas DataFrame, output is a browser window with the interactive interface.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pygwalker as pyg\npyg.walk(df)\n```\n\n----------------------------------------\n\nTITLE: Prior Float Indexing Behavior in pandas Series - IPython\nDESCRIPTION: Shows previous behavior of Series scalar indexing by float (label and position), including FutureWarnings and value assignments via .ix. Illustrates deprecated/changed behaviors and their expected output representations. Useful for understanding legacy code behaviors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_23\n\nLANGUAGE: ipython\nCODE:\n```\n# this is label indexing\nIn [2]: s[5.0]\nFutureWarning: scalar indexers for index type Int64Index should be integers and not floating point\nOut[2]: 2\n\n# this is positional indexing\nIn [3]: s.iloc[1.0]\nFutureWarning: scalar indexers for index type Int64Index should be integers and not floating point\nOut[3]: 2\n\n# this is label indexing\nIn [4]: s.loc[5.0]\nFutureWarning: scalar indexers for index type Int64Index should be integers and not floating point\nOut[4]: 2\n\n# .ix would coerce 1.0 to the positional 1, and index\nIn [5]: s2.ix[1.0] = 10\nFutureWarning: scalar indexers for index type Index should be integers and not floating point\n\nIn [6]: s2\nOut[6]:\na     1\nb    10\nc     3\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Evaluating Expressions with FutureWarning - pandas eval - IPython\nDESCRIPTION: Shows how assigning new columns via eval without specifying inplace currently triggers a FutureWarning. The warning encourages explicit use of the inplace parameter as its default will change. Demonstrates the DataFrame after an in-place assignment of a new column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_16\n\nLANGUAGE: ipython\nCODE:\n```\nIn [12]: df.eval('c = a + b')\nFutureWarning: eval expressions containing an assignment currentlydefault to operating inplace.\nThis will change in a future version of pandas, use inplace=True to avoid this warning.\n\nIn [13]: df\nOut[13]:\n      a  b     c\n0   0.0  0   0.0\n1   2.5  1   3.5\n2   5.0  2   7.0\n3   7.5  3  10.5\n4  10.0  4  14.0\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas Resampler Class\nDESCRIPTION: Importing the Resampler class from pandas.core.resample module. This class is returned by resample calls on DataFrame and Series objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/resampling.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom pandas.core.resample import Resampler\n```\n\n----------------------------------------\n\nTITLE: Reading SQL Tables with pandas.read_sql_table (Python)\nDESCRIPTION: Demonstrates how to read database tables into pandas DataFrames using various arguments of pandas.read_sql_table(). Requires an active SQLAlchemy or ADBC engine connection. Examples show reading a table by name, using a specific index column, selecting a subset of columns, parsing date columns, and passing date parsing format strings or dictionaries for custom parsing; outputs a DataFrame from the queried SQL table.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_229\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_table(\"data\", engine)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_table(\"data\", engine, index_col=\"id\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_table(\"data\", engine, columns=[\"Col_1\", \"Col_2\"])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_table(\"data\", engine, parse_dates=[\"Date\"])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_table(\"data\", engine, parse_dates={\"Date\": \"%Y-%m-%d\"})\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_table(\n    \"data\",\n    engine,\n    parse_dates={\"Date\": {\"format\": \"%Y-%m-%d %H:%M:%S\"}},\n)\n```\n\n----------------------------------------\n\nTITLE: Restoring NaN in Upsampled Resample Sum with min_count - Python\nDESCRIPTION: Demonstrates using min_count=1 to restore the older behavior where empty resampled bins return NaN rather than 0. Dependencies: pandas as pd, numpy as np. Outputs: sum with NaN in empty bins.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nIn [16]: pd.Series([1, 2], index=idx).resample(\"12H\").sum(min_count=1)\nOut[16]:\n2017-01-01 00:00:00    1.0\n2017-01-01 12:00:00    NaN\n2017-01-02 00:00:00    2.0\nFreq: 12H, Length: 3, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Out-of-bounds iloc Selection\nDESCRIPTION: Demonstrates error cases when using iloc with invalid indexers that are completely out of bounds.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> dfl.iloc[[4, 5, 6]]\nIndexError: positional indexers are out-of-bounds\n\n>>> dfl.iloc[:, 4]\nIndexError: single positional indexer is out-of-bounds\n```\n\n----------------------------------------\n\nTITLE: Pandas Float Dtype Concatenation Example\nDESCRIPTION: Shows concatenation behavior with different float dtypes\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(np.array([1.0], dtype=np.float32, ndmin=2))\ndf1.dtypes\n\ndf2 = pd.DataFrame(np.array([np.nan], dtype=np.float32, ndmin=2))\ndf2.dtypes\n\npd.concat([df1, df2]).dtypes\n```\n\n----------------------------------------\n\nTITLE: Converting scipy.sparse COO Matrix to Sparse Series - pandas - Python\nDESCRIPTION: Creates a pandas Series in sparse format from a scipy.sparse.coo_matrix using Series.sparse.from_coo. The Series contains only non-null data and indexes correspond to non-zero matrix coordinates. Dependencies: pandas, scipy. Output is a compact Series suitable for further sparse processing in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nss = pd.Series.sparse.from_coo(A)\nss\n```\n\n----------------------------------------\n\nTITLE: Using Callable Indexing in Series\nDESCRIPTION: Shows how to use callable indexing with .loc on a Series to filter elements based on a condition.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf1['A'].loc[lambda s: s > 0]\n```\n\n----------------------------------------\n\nTITLE: High-Cardinality Categorical Memory Usage - pandas - Python\nDESCRIPTION: Shows how Categorical memory usage can be similar or greater than object dtype if the number of categories approaches the data length. Dependencies: pandas. Key parameters: number of unique values. Input: Series with many unique categories; Output: Byte sizes of object and categorical dtypes. Limitation: Efficiency gains disappear for near-unique data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"foo%04d\" % i for i in range(2000)])\n\n# object dtype\ns.nbytes\n\n# category dtype\ns.astype(\"category\").nbytes\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table with Index Column Specification in Python\nDESCRIPTION: Demonstrates how to specify which column should be used as the index when reading HTML tables with pandas.read_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_94\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, index_col=0)\n```\n\n----------------------------------------\n\nTITLE: Excluding Columns with usecols Parameter and a Callable\nDESCRIPTION: Shows how to use the usecols parameter with a callable function to exclude specific columns from the parsed result rather than including them.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(StringIO(data), usecols=lambda x: x not in [\"a\", \"c\"])\n```\n\n----------------------------------------\n\nTITLE: Extracting Named Regex Groups from Series Strings using str.extract (Python)\nDESCRIPTION: Uses named regex groups to extract and label components ('letter', 'digit') from matching strings in a Series. Dependencies: pandas. Input: Series of strings. Output: DataFrame with named columns. Simplifies mapping and further processing of complex string patterns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\npd.Series(['a1', 'b2', 'c3']).str.extract(\n    '(?P<letter>[ab])(?P<digit>\\d)')\n```\n\n----------------------------------------\n\nTITLE: Installing Compression Dependencies for Pandas\nDESCRIPTION: Command to install dependencies for compression functionality in pandas, enabling compressed file reading and writing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pandas[compression]\"\n```\n\n----------------------------------------\n\nTITLE: Applying Custom CSS in a Jupyter Notebook for Consistent Styling (Python, commented)\nDESCRIPTION: This commented-out snippet provides a hack for applying the same site-wide CSS style inside a notebook as is used on the main site, by reading a CSS file and injecting it into the notebook with an HTML style tag. It depends on IPython.display and that the CSS file exists at the given path. This is primarily for development and doc consistency rather than for library users.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# # Hack to get the same style in the notebook as the\n# # main site. This is hidden in the docs.\n# from IPython.display import HTML\n# with open(\"themes/nature_with_gtoc/static/nature.css_t\") as f:\n#     css = f.read()\n\n# HTML('<style>{}</style>'.format(css))\n```\n\n----------------------------------------\n\nTITLE: Inspecting and Modifying Table Indexes in HDFStore - Python\nDESCRIPTION: This snippet demonstrates inspecting and modifying table index optimization parameters directly on a pandas HDFStore object in Python. It accesses the underlying HDF5 table column index object to check and modify its optimization level (optlevel) and kind. Required dependencies include pandas and an open HDFStore object with a previously stored table (e.g., 'df'). Inputs are the table name and optional parameters such as optlevel and kind for optimization. The output shows the index settings before and after the modification. The snippet is meant for performance tuning of indexed queries.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_183\n\nLANGUAGE: python\nCODE:\n```\n# we have automagically already created an index (in the first section)\ni = store.root.df.table.cols.index.index\ni.optlevel, i.kind\n\n# change an index by passing new parameters\nstore.create_table_index(\"df\", optlevel=9, kind=\"full\")\ni = store.root.df.table.cols.index.index\ni.optlevel, i.kind\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Current and Future DataFrameGroupBy.prod() Behavior - pandas - Python\nDESCRIPTION: These code blocks show GroupBy.prod with mixed-type columns, illustrating the deprecation of silently dropping columns where 'prod' is not applicable, replaced by raising TypeError in the future. The final example shows explicitly selecting a numeric column to avoid the error. Requires pandas >=1.3.0 for new behavior. Key parameters: DataFrameGroupBy instance (gb), numeric_only parameter. Output is a DataFrame of products or a TypeError. Limitation: User must filter valid columns before calling prod in future versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_23\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: gb.prod(numeric_only=False)\nOut[4]:\nA\n1   2\n2  12\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: gb.prod(numeric_only=False)\n...\nTypeError: datetime64 type does not support prod operations\n\nIn [6]: gb[[\"A\"]].prod(numeric_only=False)\nOut[6]:\n    A\n1   2\n2  12\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to ORC File using pandas with pyarrow (Python)\nDESCRIPTION: Serializes a DataFrame to an ORC file ('example_pa.orc') using pandas and the 'pyarrow' engine. Requires pyarrow >= 7.0.0. ORC format has limited support for some dtypes, timezones are not preserved, and is not yet supported on Windows via pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_218\n\nLANGUAGE: python\nCODE:\n```\ndf.to_orc(\"example_pa.orc\", engine=\"pyarrow\")\n```\n\n----------------------------------------\n\nTITLE: Basic CSV Reading with Row Skipping in Python\nDESCRIPTION: Demonstrates reading CSV data from a StringIO object while skipping alternate rows using skiprows parameter with a lambda function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npd.read_csv(StringIO(data), skiprows=lambda x: x % 2 != 0)\n```\n\n----------------------------------------\n\nTITLE: Testing file operations in Python using pytest\nDESCRIPTION: Example of how to use the temp_file fixture for testing file operations in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef test_something(temp_file):\n    pd.DataFrame([1]).to_csv(str(temp_file))\n```\n\n----------------------------------------\n\nTITLE: Constructing MultiIndex with NaN or Invalid Codes (Python)\nDESCRIPTION: Shows the construction of a Pandas MultiIndex from levels and codes, including edge cases such as NaN, None, or NaT elements and negative code values. Illustrates that codes less than -1 are now disallowed and that NaN levels will have codes reassigned to -1. Requires Pandas and NumPy as np. Returns a MultiIndex; in newer Pandas versions, invalid code values now result in a ValueError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.MultiIndex(levels=[[np.nan, None, pd.NaT, 128, 2]],\n              codes=[[0, -1, 1, 2, 3, 4]])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.MultiIndex(levels=[[1, 2]], codes=[[0, -2]])\n```\n\n----------------------------------------\n\nTITLE: Resetting Indexes and Adding Series Without Index Alignment (Python)\nDESCRIPTION: Demonstrates resetting the indices of two pandas Series objects and adding them directly. By dropping the index and using default integer indexing, the addition proceeds positionally without NaN results. Expects two Series with non-matching indices as input and outputs a single Series with values added positionally. pandas is the only dependency.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nIn [3]: ser1.reset_index(drop=True) + ser2.reset_index(drop=True)\nOut[3]:\n0    20\n1    30\n2    40\n3    50\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Using loc and iloc for Different Slicing Methods in Python\nDESCRIPTION: Demonstrates different slicing approaches using loc (label-based) and iloc (position-based). This example shows the distinction between position-oriented and label-oriented slicing in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"AAA\": [4, 5, 6, 7], \"BBB\": [10, 20, 30, 40], \"CCC\": [100, 50, -30, -50]},\n    index=[\"foo\", \"bar\", \"boo\", \"kar\"],\n)\n\ndf.iloc[0:3]  # Positional\n\ndf.loc[\"bar\":\"kar\"]  # Label\n\n# Generic\ndf[0:3]\ndf[\"bar\":\"kar\"]\n```\n\n----------------------------------------\n\nTITLE: Changing and Resetting Display Max Rows in pandas - Python\nDESCRIPTION: Demonstrates querying, updating, resetting, and verifying the 'display.max_rows' setting for controlling DataFrame output. Uses a sequence of get_option, set_option, and reset_option calls, showing both the new value being set and restored. Input parameters are the option name and desired value. Output is the current value after each operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npd.get_option(\"display.max_rows\")\npd.set_option(\"display.max_rows\", 999)\npd.get_option(\"display.max_rows\")\npd.reset_option(\"display.max_rows\")\npd.get_option(\"display.max_rows\")\n```\n\n----------------------------------------\n\nTITLE: Creating Table Index on an Existing Table with Specific Columns and Parameters - pandas HDFStore - Python\nDESCRIPTION: This snippet shows how to build a table index on an existing table within an HDFStore, specifying which data columns to index and tuning optimization. It requires pandas, an open HDFStore, and a table without indices. Inputs include the table name and the list of columns to index ('columns'), as well as optlevel and kind. The output is the optimized indexed table ready for fast queried access. Limitations: columns to be indexed must have been stored as data_columns in previous appends.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_185\n\nLANGUAGE: python\nCODE:\n```\nst.create_table_index(\"df\", columns=[\"B\"], optlevel=9, kind=\"full\")\nst.get_storer(\"df\").table\n\nst.close()\n```\n\n----------------------------------------\n\nTITLE: Formatting Datetimes with Series.dt.strftime in pandas (Python)\nDESCRIPTION: Shows how to use the new Series.dt.strftime method to format Series of datetimes or periods into strings with a specified format. Examples include both DatetimeIndex and PeriodIndex. Input: Series of datetime-like or period data; output: Series of formatted strings. This method matches Python's standard strftime formatting. Dependency: pandas 0.17.0+.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# DatetimeIndex\ns = pd.Series(pd.date_range(\"20130101\", periods=4))\ns\ns.dt.strftime(\"%Y/%m/%d\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# PeriodIndex\ns = pd.Series(pd.period_range(\"20130101\", periods=4))\ns\ns.dt.strftime(\"%Y/%m/%d\")\n```\n\n----------------------------------------\n\nTITLE: Quantile Cut with Nullable Integer in Python\nDESCRIPTION: Fixed regression in qcut when passed a nullable integer.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\npd.qcut(pd.array([1, 2, 3, None], dtype='Int64'), q=4)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Extended NumPy Function Compatibility with Pandas (Python)\nDESCRIPTION: Shows enhanced compatibility of NumPy functions with Pandas SparseDataFrame, focusing on signature extensions. Requires Pandas and NumPy. Constructs a SparseDataFrame, then applies np.cumsum on axis 0 to demonstrate acceptance of NumPy-compatible arguments. Input is a SparseDataFrame; output is the cumulative sum performed along the specified axis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsp = pd.SparseDataFrame([1, 2, 3])\nsp\nnp.cumsum(sp, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in Series.isna and DataFrame.isna for Categorical Data\nDESCRIPTION: Addresses a regression where Series.isna and DataFrame.isna would raise an error for categorical dtype when pandas.options.mode.use_inf_as_na was set to True.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nSeries.isna()\n```\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame.isna()\n```\n\n----------------------------------------\n\nTITLE: Including Contributors Section in pandas Changelog - reStructuredText\nDESCRIPTION: This snippet denotes the contributors section in the pandas changelog using reStructuredText directives. It uses Sphinx-specific roles and references to automatically generate a list of contributors between the specified versions. Dependencies include Sphinx with appropriate extensions. Expected inputs are version identifiers, and output includes an auto-generated contributor list. Limitation: this code is static and only effective when processed through documentation generators like Sphinx; it is not executable Python code.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.1.rst#_snippet_4\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _whatsnew_0.14.1.contributors:\n\nContributors\n~~~~~~~~~~~~\n\n.. contributors:: v0.14.0..v0.14.1\n```\n\n----------------------------------------\n\nTITLE: Inspecting Shapes of Original and Combined DataFrames in Python\nDESCRIPTION: This snippet prints the shapes (row and column counts) for each measurement DataFrame and their concatenated result, enabling verification of the concatenation's effect. Dependencies: pandas, existing air_quality_pm25, air_quality_no2, and air_quality DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/08_combine_dataframes.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nprint('Shape of the ``air_quality_pm25`` table: ', air_quality_pm25.shape)\nprint('Shape of the ``air_quality_no2`` table: ', air_quality_no2.shape)\nprint('Shape of the resulting ``air_quality`` table: ', air_quality.shape)\n```\n\n----------------------------------------\n\nTITLE: Dropping and Filling Missing Data in DataFrames - Python\nDESCRIPTION: These snippets demonstrate two standard methods for handling missing data: df1.dropna(how=\"any\") drops rows with any nan values, while df1.fillna(value=5) fills missing entries with a specified value (5).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf1.dropna(how=\"any\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf1.fillna(value=5)\n```\n\n----------------------------------------\n\nTITLE: Using Integer Indices for Column Selection in Excel Read\nDESCRIPTION: Shows how to select columns by their numeric indices when reading from an Excel file. The order of indices doesn't matter in the selection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_135\n\nLANGUAGE: python\nCODE:\n```\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=[0, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Pushing Feature Branches and Inspecting Remote Configuration - Shell\nDESCRIPTION: This sequence demonstrates pushing your local feature branch commits to your GitHub fork and viewing the list of repository remotes. Requires authentication with GitHub and a configured remote. It also illustrates expected output for remote listings, showing both origin and upstream connections. Outputs include confirmation from git of the push and the list of remotes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing.rst#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit push origin shiny-new-feature\n```\n\nLANGUAGE: shell\nCODE:\n```\ngit remote -v\n```\n\nLANGUAGE: shell\nCODE:\n```\norigin  git@github.com:yourname/pandas.git (fetch)\\norigin  git@github.com:yourname/pandas.git (push)\\nupstream        git://github.com/pandas-dev/pandas.git (fetch)\\nupstream        git://github.com/pandas-dev/pandas.git (push)\n```\n\n----------------------------------------\n\nTITLE: Type Narrowing with isinstance (Preferred) in Python\nDESCRIPTION: Illustrates the recommended, cast-free method of performing type narrowing using isinstance checks within a Python function that accepts a union of types. This approach is more readable and compatible with type checkers. Dependencies: none besides basic Python. Parameters: obj (Union[str, int, float]). Outputs: uppercased string for str input, otherwise other types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef cannot_infer_good(obj: Union[str, int, float]):\n\n    if isinstance(obj, str):\n        return obj.upper()\n    else:\n        ...\n```\n\n----------------------------------------\n\nTITLE: Converting Timezone-Aware Series to NumPy Arrays - Controlled Dtype - Pandas - Python\nDESCRIPTION: This set of snippets shows how to control the dtype when converting timezone-aware pandas Series or DatetimeIndex to NumPy arrays. Specifying dtype='datetime64[ns]' produces the old (timezone-naive) behavior, while dtype=object preserves timezones as pandas.Timestamp. Inputs are a timezone-aware Series; outputs are arrays in either form. This method avoids warnings and gives full control over conversion behavior. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nnp.asarray(ser, dtype='datetime64[ns]')\n```\n\nLANGUAGE: python\nCODE:\n```\n# New behavior\nnp.asarray(ser, dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Parallel Processing with Bodo\nDESCRIPTION: This code demonstrates using Bodo, a high-performance compute engine for Python, to parallelize pandas workloads. The @bodo.jit decorator enables automatic parallelization of the function that processes a Parquet file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport bodo\n\n@bodo.jit\ndef process_data():\n    df = pd.read_parquet(\"my_data.pq\")\n    df2 = pd.DataFrame({\"A\": df.apply(lambda r: 0 if r.A == 0 else (r.B // r.A), axis=1)})\n    df2.to_parquet(\"out.pq\")\n\nprocess_data()\n```\n\n----------------------------------------\n\nTITLE: Comparing CategoricalDtype Instances in Python\nDESCRIPTION: Demonstrates the equality semantics of CategoricalDtype instances. Two instances are considered equal if they have the same categories and order, with special handling for unordered categoricals.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nc1 = CategoricalDtype([\"a\", \"b\", \"c\"], ordered=False)\n\n# Equal, since order is not considered when ordered=False\nc1 == CategoricalDtype([\"b\", \"c\", \"a\"], ordered=False)\n\n# Unequal, since the second CategoricalDtype is ordered\nc1 == CategoricalDtype([\"a\", \"b\", \"c\"], ordered=True)\n\nc1 == \"category\"\n```\n\n----------------------------------------\n\nTITLE: Comparing SAS XPORT Import and CSV Import Performance in IPython\nDESCRIPTION: Times loading a SAS XPORT file and a CSV file using pandas via IPython magic commands. Demonstrates IO performance difference for a 10M row file. Requires IPython and pandas. Inputs: 'big.xpt', 'big.csv' files. Output: elapsed wall time for each operation. Highlights that CSV import may be faster than XPORT format.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_28\n\nLANGUAGE: ipython\nCODE:\n```\n# version 0.17, 10M rows\n\nIn [8]: %time df = pd.read_sas('big.xpt')\nWall time: 14.6 s\n\nIn [9]: %time df = pd.read_csv('big.csv')\nWall time: 4.86 s\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading DataFrames/Series with MessagePack in Python\nDESCRIPTION: Demonstrates how to save Pandas DataFrames and Series to MessagePack format files and read them back. Also shows how to iterate over unpacked results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.rand(5, 2), columns=list('AB'))\ndf.to_msgpack('foo.msg')\npd.read_msgpack('foo.msg')\n\ns = pd.Series(np.random.rand(5), index=pd.date_range('20130101', periods=5))\npd.to_msgpack('foo.msg', df, s)\npd.read_msgpack('foo.msg')\n\nfor o in pd.read_msgpack('foo.msg', iterator=True):\n    print(o)\n```\n\n----------------------------------------\n\nTITLE: Setting Pandas Plotting Backend to hvPlot in Python\nDESCRIPTION: This snippet changes the pandas plotting backend to hvPlot, which provides a high-level, interactive plotting API directly from pandas. The 'hvplot' backend must be available and pandas must be imported. The only parameter is the backend string 'hvplot'. Output is interactive visualizations accessible via standard pandas plotting calls.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"plotting.backend\", \"hvplot\")\n```\n\n----------------------------------------\n\nTITLE: Updating Feature Branch with Main Branch Changes\nDESCRIPTION: Commands to sync a feature branch with the latest changes from pandas main branch. This helps keep the pull request up to date with the main repository.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing.rst#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout shiny-new-feature\ngit fetch upstream\ngit merge upstream/main\n```\n\n----------------------------------------\n\nTITLE: Multi-conditional Column Assignment with numpy.select - Python\nDESCRIPTION: Illustrates use of numpy.select for assigning values based on multiple conditions in a DataFrame. Requires pandas and numpy. Applies three color choices for mutually exclusive conditions, with 'black' as a default. Inputs: DataFrame and lists of conditions/choices; Output: DataFrame with a new conditionally-determined column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nconditions = [\n    (df['col2'] == 'Z') & (df['col1'] == 'A'),\n    (df['col2'] == 'Z') & (df['col1'] == 'B'),\n    (df['col1'] == 'B')\n]\nchoices = ['yellow', 'blue', 'purple']\ndf['color'] = np.select(conditions, choices, default='black')\ndf\n```\n\n----------------------------------------\n\nTITLE: SemiMonth Anchoring with Custom Days in date_range - Pandas - Python\nDESCRIPTION: Examples of specifying a custom anchor day for semi-monthly date ranges using the suffix in frequency string. The code demonstrates two usages: freq=\"SMS-16\" starts periods on the 1st and 16th, while freq=\"SM-14\" anchors on the 14th and end of month. Only pandas library is required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nIn [50]: pd.date_range(\"2015-01-01\", freq=\"SMS-16\", periods=4)\nOut[50]: DatetimeIndex(['2015-01-01', '2015-01-16', '2015-02-01', '2015-02-16'], dtype='datetime64[ns]', freq='SMS-16')\n\nIn [51]: pd.date_range(\"2015-01-01\", freq=\"SM-14\", periods=4)\nOut[51]: DatetimeIndex(['2015-01-14', '2015-01-31', '2015-02-14', '2015-02-28'], dtype='datetime64[ns]', freq='SM-14')\n```\n\n----------------------------------------\n\nTITLE: Replacing Slices in Strings using pandas Series.str.slice_replace (Python)\nDESCRIPTION: Displays usage of Series.str.slice_replace to replace specified slices in each string element of a Series. Dependencies: pandas. Inputs: Series of strings, optional start and stop indices, replacement string. Outputs: new Series with slices replaced accordingly. Limitations: Slicing bounds must be non-negative and within string length; replacing with an empty character results in character deletion.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['ABCD', 'EFGH', 'IJK'])\ns.str.slice_replace(1, 3, 'X')\n# replaced with empty char\ns.str.slice_replace(0, 1)\n```\n\n----------------------------------------\n\nTITLE: Equality Comparisons among Missing Values in pandas (Python)\nDESCRIPTION: These code samples illustrate the behavior of equality comparisons for None, np.nan, pd.NaT, and pd.NA in Python and pandas. They demonstrate that comparisons between missing values (unlike None) do not return True, and highlight the need to use pd.isna for reliable missing value detection. No external dependencies beyond pandas and numpy are required. Outputs may be unexpected for equality due to the special comparison semantics of these sentinels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nNone == None  # noqa: E711\n```\n\nLANGUAGE: python\nCODE:\n```\nnp.nan == np.nan\n```\n\nLANGUAGE: python\nCODE:\n```\npd.NaT == pd.NaT\n```\n\nLANGUAGE: python\nCODE:\n```\npd.NA == pd.NA\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame with Date Index and Columns to JSON File - pandas - Python\nDESCRIPTION: Illustrates copying an existing DataFrame, adding date and additional typed columns, modifying the index to be dates, and serializing to a file with to_json. The file is then read and printed. Key dependencies: pandas (with pd.date_range), file I/O permissions. Inputs: DataFrame. Outputs: JSON data written to disk and printed contents.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_71\n\nLANGUAGE: python\nCODE:\n```\ndfj2 = dfj.copy()\ndfj2[\"date\"] = pd.Timestamp(\"20130101\")\ndfj2[\"ints\"] = list(range(5))\ndfj2[\"bools\"] = True\ndfj2.index = pd.date_range(\"20130101\", periods=5)\ndfj2.to_json(\"test.json\", date_format=\"iso\")\n\nwith open(\"test.json\") as fh:\n    print(fh.read())\n```\n\n----------------------------------------\n\nTITLE: DatetimeIndex Integer Data with Timezone - Deprecation Handling - Python\nDESCRIPTION: This example shows the warning issued when constructing a DatetimeIndex from integer data with a timezone. In previous pandas versions, these integers were interpreted using the provided timezone; now users are warned about a future change whereby integers will be interpreted as UTC and then converted to the target timezone. Inputs include integer nanosecond values and a 'tz' parameter. Output is a DatetimeIndex object. Users should adapt their code using pd.to_datetime and tz_convert/tz_localize as detailed in the warning message.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: pd.DatetimeIndex([946684800000000000], tz=\"US/Central\")\n/bin/ipython:1: FutureWarning:\n    Passing integer-dtype data and a timezone to DatetimeIndex. Integer values\n    will be interpreted differently in a future version of pandas. Previously,\n    these were viewed as datetime64[ns] values representing the wall time\n    *in the specified timezone*. In the future, these will be viewed as\n    datetime64[ns] values representing the wall time *in UTC*. This is similar\n    to a nanosecond-precision UNIX epoch. To accept the future behavior, use\n\n        pd.to_datetime(integer_data, utc=True).tz_convert(tz)\n\n    To keep the previous behavior, use\n\n        pd.to_datetime(integer_data).tz_localize(tz)\n\n#!/bin/python3\nOut[3]: DatetimeIndex(['2000-01-01 00:00:00-06:00'], dtype='datetime64[ns, US/Central]', freq=None)\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table with Specific Parser in Python\nDESCRIPTION: Shows how to specify which parser to use when reading HTML tables with pandas.read_html(). You can use lxml, bs4, or a combination of parsers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_101\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=\"lxml\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, \"Metcalf Bank\", index_col=0, flavor=[\"lxml\", \"bs4\"])\n```\n\n----------------------------------------\n\nTITLE: Loading R Datasets into Pandas using pandas.rpy.common (Python)\nDESCRIPTION: Shows syntax for importing R datasets into pandas (legacy/deprecated interface as of v0.16.0). Dependencies: pandas, pandas.rpy package, R. Inputs: dataset name string. Outputs: loaded DataFrame. Used for cross-language interoperability and statistical analysis with R data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nimport pandas.rpy.common as com\ncom.load_data('Titanic')\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation for Pandas 2.2.3 Release\nDESCRIPTION: ReStructuredText formatted documentation detailing the changes and updates in Pandas 2.2.3, including Python 3.13 compatibility announcement, bug fixes for complex number evaluation and NumPy 2.1 compatibility, and license-related updates.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.3.rst#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _whatsnew_223:\\n\\nWhat's new in 2.2.3 (September 20, 2024)\\n----------------------------------------\\n\\nThese are the changes in pandas 2.2.3. See :ref:`release` for a full changelog\\nincluding other versions of pandas.\\n\\n{{ header }}\\n\\n.. ---------------------------------------------------------------------------\\n\\n.. _whatsnew_220.py13_compat:\\n\\nPandas 2.2.3 is now compatible with Python 3.13\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nPandas 2.2.3 is the first version of pandas that is generally compatible with the upcoming\\nPython 3.13, and both wheels for free-threaded and normal Python 3.13 will be uploaded for\\nthis release.\\n\\nAs usual please report any bugs discovered to our `issue tracker <https://github.com/pandas-dev/pandas/issues/new/choose>`_\\n\\n.. ---------------------------------------------------------------------------\\n.. _whatsnew_223.bug_fixes:\\n\\nBug fixes\\n~~~~~~~~~\\n- Bug in :func:`eval` on :class:`complex` including division ``/`` discards imaginary part. (:issue:`21374`)\\n- Minor fixes for numpy 2.1 compatibility. (:issue:`59444`)\\n\\n.. ---------------------------------------------------------------------------\\n.. _whatsnew_223.other:\\n\\nOther\\n~~~~~\\n- Missing licenses for 3rd party dependencies were added back into the wheels. (:issue:`58632`)\\n\\n.. ---------------------------------------------------------------------------\\n.. _whatsnew_223.contributors:\\n\\nContributors\\n~~~~~~~~~~~~\\n\\n.. contributors:: v2.2.2..v2.2.3|HEAD\n```\n\n----------------------------------------\n\nTITLE: Accessing DataFrame Index and Columns - pandas - Python\nDESCRIPTION: Shows how to retrieve the row (index) labels and column labels of a DataFrame using the '.index' and '.columns' attributes. Useful for inspecting or manipulating axis labels for further data operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf.index\ndf.columns\n```\n\n----------------------------------------\n\nTITLE: Panel Reconstruction Using Dictionary Comprehension in Pandas (Python)\nDESCRIPTION: Reconstructs a Panel by applying a normalization function f to each minor axis slice and aggregating the results with a dictionary comprehension. Each key in the dictionary is an element from panel.minor_axis, and the corresponding value is the normalized DataFrame for that slice. The constructed dictionary is then passed to pd.Panel to produce a new Panel object with a structure equivalent to that obtained via Panel.apply. Requires pandas and the function f from prior context.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.1.rst#_snippet_8\n\nLANGUAGE: ipython\nCODE:\n```\nIn [42]: result = pd.Panel({ax: f(panel.loc[:, :, ax]) for ax in panel.minor_axis})\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Business Hour in Python using Pandas\nDESCRIPTION: Creates a CustomBusinessHour object that respects US federal holidays and demonstrates its usage for date arithmetic.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.tseries.offsets import CustomBusinessHour\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\n\nbhour_us = CustomBusinessHour(calendar=USFederalHolidayCalendar())\n\nimport datetime\n\ndt = datetime.datetime(2014, 1, 17, 15)\n\ndt + bhour_us\n\ndt + bhour_us * 2\n```\n\n----------------------------------------\n\nTITLE: Reading Excel Files Using pd.read_excel - Pandas - Python\nDESCRIPTION: Compares the old and new ways of reading Excel files in pandas. The previous API used 'ExcelFile.parse', now replaced by 'pd.read_excel' for streamlined top-level access. Both approaches allow specifying sheet names, index columns, and NA values. Requires pandas, and for Excel support, the appropriate engine (e.g., xlrd for .xls) installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.io.parsers import ExcelFile\n\nxls = ExcelFile(\"path_to_file.xls\")\nxls.parse(\"Sheet1\", index_col=None, na_values=[\"NA\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", index_col=None, na_values=[\"NA\"])\n```\n\n----------------------------------------\n\nTITLE: Directly Modifying allows_duplicate_labels Flag - pandas - Python\nDESCRIPTION: Directly sets the allows_duplicate_labels flag to False on a DataFrame's flags attribute, toggling label policy in-place. Illustrates an alternative to set_flags. Requires DataFrame with flags property (from pandas 1.2.0+).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf2.flags.allows_duplicate_labels = False\ndf2.flags.allows_duplicate_labels\n```\n\n----------------------------------------\n\nTITLE: Converting String Column to Numeric Type with to_numeric in Python\nDESCRIPTION: Shows a complete example of using infer_objects followed by to_numeric to fully convert DataFrame columns. This demonstrates how to convert string columns that infer_objects doesn't automatically convert.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = df.infer_objects()\ndf['C'] = pd.to_numeric(df['C'], errors='coerce')\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Assigning Columns Using Callable Functions with DataFrame.assign (Python)\nDESCRIPTION: Shows passing a lambda function to DataFrame.assign where the function operates on the entire DataFrame and returns a column to insert. Requires pandas and the Iris dataset. This technique is useful for on-the-fly calculations using the existing DataFrame structure, returning a new DataFrame. Key parameter: a key-value pair where the value is a lambda that receives the DataFrame and returns the new column data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\niris.assign(sepal_ratio=lambda x: (x['SepalWidth']\n                                       / x['SepalLength'])).head()\n```\n\n----------------------------------------\n\nTITLE: Proposed Pandas Behavior without Type Upcasting\nDESCRIPTION: Demonstrates the proposed behavior where attempting to set a value of incompatible type raises a ValueError instead of upcasting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0006-ban-upcasting.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, 3])\n\nser[2] = 'potage'  # raises!\n---------------------------------------------------------------------------\nValueError: Invalid value 'potage' for dtype int64\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows with IF and WHERE Clauses - SAS\nDESCRIPTION: Shows two approaches to filtering a dataset in SAS: using an 'if' statement within a DATA step to keep rows meeting a condition, or using a 'where' clause, which can also be used in PROC statements. Both methods select rows where 'total_bill' is greater than 10. Input: 'tips' dataset; Output: filtered 'tips' dataset.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_8\n\nLANGUAGE: SAS\nCODE:\n```\ndata tips;\\n    set tips;\\n    if total_bill > 10;\\nrun;\\n\\ndata tips;\\n    set tips;\\n    where total_bill > 10;\\n    /* equivalent in this case - where happens before the\\n       DATA step begins and can also be used in PROC statements */\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Reading and Displaying a Custom Style Structure Template in Python\nDESCRIPTION: This snippet reads a custom HTML style template from file and stores its content, then displays the raw HTML using IPython's HTML function. It expects the 'html_style_structure.html' file to exist in the 'templates' directory. Used for checking the structure of the style template before use in rendering.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"templates/html_style_structure.html\") as f_sty:\n    style_structure = f_sty.read()\n```\n\nLANGUAGE: python\nCODE:\n```\nHTML(style_structure)\n```\n\n----------------------------------------\n\nTITLE: Creating Series from scipy.sparse COO Matrix using Series.sparse.from_coo - pandas - Python\nDESCRIPTION: Initializes a pandas Series with a sparse index from a SciPy coo_matrix, containing only non-null entries. Default behavior omits the full cartesian index, creating a compact sparse representation. Use when importing sparse matrix results into high-level pandas analytics. Dependencies: pandas, scipy. Output is a sparse Series indexed by (row, column) coordinates.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy import sparse\nA = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])), shape=(3, 4))\nA\nA.todense()\n```\n\n----------------------------------------\n\nTITLE: Stacking Columns to Index - pandas - Python\nDESCRIPTION: Uses DataFrame.stack to pivot columns into the innermost level of the row index, converting a DataFrame to either a Series or a DataFrame with MultiIndex. Enables hierarchical manipulation and data transformation for further analysis. Requires the existence of DataFrame 'df2' with labeled columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nstacked = df2.stack()\nstacked\n```\n\n----------------------------------------\n\nTITLE: Parsing Datetime Strings with pd.to_datetime - Python\nDESCRIPTION: Demonstrates the previous and updated behaviors of the pd.to_datetime function in pandas. Shows outputs when parsing a list containing a valid date string and an invalid string under default settings and with error parameters. Requires pandas installed and 'import pandas as pd'. Inputs are lists of strings, outputs may be arrays or exceptions depending on the errors parameter. Previous default was errors='ignore', now errors='raise'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime(['2009-07-31', 'asd'])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime(['2009-07-31', 'asd'])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime([\"2009-07-31\", \"asd\"], errors=\"coerce\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime([\"2009-07-31\", \"asd\"], errors=\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Reading ORC files with column selection in pandas\nDESCRIPTION: Demonstrates how to read an ORC file with pandas, selecting only specific columns ('a' and 'b'). The code also displays the data types of the resulting DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_220\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.read_orc(\n    \"example_pa.orc\",\n    columns=[\"a\", \"b\"],\n)\nresult.dtypes\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame Rows Using Query Strings with df.query (Python)\nDESCRIPTION: Applies DataFrame.query to select rows where the composite boolean condition 'a < b < c' is met. Requires pandas. Inputs: DataFrame with columns 'a', 'b', 'c'. Outputs: filtered DataFrame. Enables expressive, database-like filtering syntax for data science workflows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nn = 20\ndf = pd.DataFrame(np.random.randint(n, size=(n, 3)), columns=['a', 'b', 'c'])\ndf.query('a < b < c')\n```\n\n----------------------------------------\n\nTITLE: Reading CSV Data from URL - pandas - Python\nDESCRIPTION: This Python code illustrates reading CSV data directly from a remote URL using pandas' 'read_csv' function. The resulting DataFrame ('tips') is loaded with the contents of the specified CSV and is ready for further analysis. Dependencies: pandas is required. Input is a CSV URL; output is a pandas DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nurl = (\\n    \"https://raw.githubusercontent.com/pandas-dev/\"\\n    \"pandas/main/pandas/tests/io/data/csv/tips.csv\"\\n)\\ntips = pd.read_csv(url)\\ntips\n```\n\n----------------------------------------\n\nTITLE: Error Handling with pandas.eval for Undefined Context (ipython)\nDESCRIPTION: Demonstrates error handling in pandas.eval when attempting to use the '@' local variable prefix in contexts where it's not valid. Shows the resulting exception and proper fallback to standard Python variable references within pd.eval. Requires pandas. Inputs: scalar variables; Outputs: exception for invalid eval syntax, normal output for Python-style eval.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_19\n\nLANGUAGE: ipython\nCODE:\n```\na, b = 1, 2\npd.eval(\"@a + b\")\n\npd.eval(\"a + b\")\n```\n\n----------------------------------------\n\nTITLE: Creating Autocorrelation Plot for Time Series Analysis\nDESCRIPTION: Generates an autocorrelation plot to check randomness in time series data. The plot shows correlations between data points at different time lags, with horizontal lines indicating 95% and 99% confidence bands to help identify significant correlations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.plotting import autocorrelation_plot\n\nplt.figure();\n\nspacing = np.linspace(-9 * np.pi, 9 * np.pi, num=1000)\ndata = pd.Series(0.7 * np.random.rand(1000) + 0.3 * np.sin(spacing))\n\nautocorrelation_plot(data);\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Float64Index with Scalar and Slice Operations - Pandas - Python\nDESCRIPTION: Demonstrates creation of a pandas Index with float values, and the resulting label-based selection semantics using [], .loc, and .iloc. Includes scalar selection, slicing, and differentiation between label and positional indexing, as well as TypeError when applying float slicing to Int64Index. Prerequisites: pandas imported as pd, familiarity with indexers. Outputs include representations of Index, Series, scalar value selection, and exception traces.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.Index([1.5, 2, 3, 4.5, 5])\\nindex\\ns = pd.Series(range(5), index=index)\\ns\n```\n\nLANGUAGE: python\nCODE:\n```\ns[3]\\ns.loc[3]\n```\n\nLANGUAGE: python\nCODE:\n```\ns.iloc[3]\n```\n\nLANGUAGE: python\nCODE:\n```\ns.loc[2:4]\\ns.iloc[2:4]\n```\n\nLANGUAGE: python\nCODE:\n```\ns[2.1:4.6]\\ns.loc[2.1:4.6]\n```\n\n----------------------------------------\n\nTITLE: Applying a Binary Ufunc to Pandas Series with Aligned Indexes - Pandas/Numpy - Python\nDESCRIPTION: Illustrates the alignment behavior when applying numpy binary ufunc (like power) to two Series with different but overlapping indexes. Shows creation of two Series and the application of np.power, which aligns on index labels before calculation. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\\ns2 = pd.Series([3, 4, 5], index=['d', 'c', 'b'])\\ns1\\ns2\n```\n\nLANGUAGE: python\nCODE:\n```\nnp.power(s1, s2)\n```\n\n----------------------------------------\n\nTITLE: Indexing with NA-Filled Boolean Mask in pandas Series (Python)\nDESCRIPTION: This code demonstrates filling NA values in a nullable Boolean mask with True before using it for indexing a Series. This causes any NA values to be treated as True during masking. Inputs are the same Series and mask as previous; outputs are rows where mask or its fill value is True.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/boolean.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ns[mask.fillna(True)]\n```\n\n----------------------------------------\n\nTITLE: Aggregating with collapse - Stata\nDESCRIPTION: Shows how to use the 'collapse' command in Stata to aggregate columns (summing total_bill and tip) grouped by 'sex' and 'smoker' variables. Useful for reducing data by computing statistics across groups. Requires an existing dataset with the specified variables. Outputs a dataset with grouped aggregations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_22\n\nLANGUAGE: stata\nCODE:\n```\ncollapse (sum) total_bill tip, by(sex smoker)\n```\n\n----------------------------------------\n\nTITLE: Merging DataFrames on Columns and Index Levels - Pandas - Python\nDESCRIPTION: Demonstrates DataFrame.merge enhancements where column and index level names can be used together for joining. The example creates two DataFrames with named indexes and different columns and merges them using both index and column names. Requires pandas 0.23+. Inputs are indexed DataFrames; output is the merged DataFrame with matched rows based on specified criteria.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nleft_index = pd.Index(['K0', 'K0', 'K1', 'K2'], name='key1')\n\nleft = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n                     'B': ['B0', 'B1', 'B2', 'B3'],\n                     'key2': ['K0', 'K1', 'K0', 'K1']},\n                    index=left_index)\n\nright_index = pd.Index(['K0', 'K1', 'K2', 'K2'], name='key1')\n\nright = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3'],\n                      'key2': ['K0', 'K0', 'K0', 'K1']},\n                     index=right_index)\n\nleft.merge(right, on=['key1', 'key2'])\n\n```\n\n----------------------------------------\n\nTITLE: Multiplying Timedelta Series by Scalars and Series (Python)\nDESCRIPTION: Performs element-wise multiplication of a timedelta64[ns] Series with both an integer scalar and another Series of integers. Dependencies: pandas and NumPy. Input: a Series 'td'. Outputs: 'td * -1' produces negative timedeltas; 'td * pd.Series([1, 2, 3, 4])' multiplies each delta by a corresponding value. Handles broadcasting and missing values as per standard pandas behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntd * -1\ntd * pd.Series([1, 2, 3, 4])\n```\n\n----------------------------------------\n\nTITLE: Selecting Data from Timeseries in pandas DataFrame and Series (Python)\nDESCRIPTION: This series of snippets demonstrate how to create a datetime index using pd.date_range, create a Series and a DataFrame indexed by datetime, and perform partial string indexing to select all data from a given year. Relies on pandas and numpy; creates random float data for demonstration. The string-based selection retrieves all rows for '2001', with outputs shown for both Series and DataFrame. Expects the time index to be properly formatted pandas DateTimeIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.date_range(\"2001-10-1\", periods=5, freq='M')\n```\n\nLANGUAGE: python\nCODE:\n```\nts = pd.Series(np.random.rand(len(idx)), index=idx)\n```\n\nLANGUAGE: python\nCODE:\n```\nts['2001']\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': ts})\n```\n\nLANGUAGE: python\nCODE:\n```\ndf['2001']\n```\n\n----------------------------------------\n\nTITLE: Plotting Horizontal Stacked Bar Charts with DataFrame.plot - pandas (Python)\nDESCRIPTION: Illustrates how to produce a horizontal stacked bar chart using the plot method on a pandas DataFrame. Requires a pandas DataFrame named df. The kind argument set to \"barh\" generates a horizontal bar chart, and stacked=True stacks the bars. The output is a Matplotlib visualization. Ensure appropriate data columns are provided for meaningful charts.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.3.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf.plot(kind=\"barh\", stacked=True)  # noqa F821\n```\n\n----------------------------------------\n\nTITLE: Querying DataFrames with the inplace Parameter - pandas query - Python\nDESCRIPTION: Demonstrates usage of the DataFrame query method, highlighting differences between inplace and non-inplace behavior. Shows how queries filter DataFrames and affect the result/output, and that inplace defaults to False. Requires an initialized DataFrame (df) and pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf.query('a > 5')\ndf.query('a > 5', inplace=True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: DataFrame Right Merge Did Not Preserve Row Order - python\nDESCRIPTION: Illustrates that a right merge on two DataFrames did not preserve the row order of the right frame in previous pandas. The code merges left_df and right_df on 'animal' and 'max_speed', displaying the resulting DataFrame's rows. Inputs are both DataFrames; output is a DataFrame with the merged results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> left_df.merge(right_df, on=['animal', 'max_speed'], how=\"right\")\\n    animal  max_speed\\n0      pig         11\\n1  quetzal         80\n```\n\n----------------------------------------\n\nTITLE: Melting a Data Frame - reshape2::melt - R\nDESCRIPTION: This R example uses melt to reshape a structured data.frame ('cheese') from wide to long format, retaining 'first' and 'last' as identifier variables. Input is a data.frame with multiple columns for attributes. The output is a long-form data.frame; melt requires specification of id variables.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_18\n\nLANGUAGE: R\nCODE:\n```\ncheese <- data.frame(\n  first = c('John', 'Mary'),\n  last = c('Doe', 'Bo'),\n  height = c(5.5, 6.0),\n  weight = c(130, 150)\n)\nmelt(cheese, id=c(\"first\", \"last\"))\n```\n\n----------------------------------------\n\nTITLE: Bubble Chart using Scatter Plot\nDESCRIPTION: This example demonstrates how to create a bubble chart by varying the size of points based on values in a column. The 's' parameter controls point size and can be set to a column's values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndf.plot.scatter(x=\"a\", y=\"b\", s=df[\"c\"] * 200);\n```\n\n----------------------------------------\n\nTITLE: Manual Deprecation with Warnings in Python Functions\nDESCRIPTION: This code manually implements a deprecation warning for a Python function, using the warnings library and a pandas utility to identify the correct stack level. The old_func function is marked as deprecated and issues a FutureWarning, then calls the replacement new_func. This approach is used when deprecate cannot be used and requires updating documentation and tests. Required dependencies: warnings module and pandas.util._exceptions. Inputs: none; Outputs: warning and function behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nfrom pandas.util._exceptions import find_stack_level\n\n\ndef old_func():\n    \"\"\"Summary of the function.\n\n    .. deprecated:: 1.1.0\n       Use new_func instead.\n    \"\"\"\n    warnings.warn(\n        'Use new_func instead.',\n        FutureWarning,\n        stacklevel=find_stack_level(),\n    )\n    new_func()\n\n\ndef new_func():\n    pass\n```\n\n----------------------------------------\n\nTITLE: Appending to and Querying from Multiple Tables Simultaneously - append_to_multiple and select_as_multiple - pandas HDFStore - Python\nDESCRIPTION: This snippet demonstrates how to split a DataFrame across multiple HDFStore tables using append_to_multiple, then retrieve data using select and select_as_multiple. The selector mechanism allows efficient queries on an index-rich table and then retrieves columns from the associated tables. Dependencies: pandas, numpy, pd.date_range, open HDFStore. Inputs: DataFrame, dictionary mapping table names to column lists, selector designation. Outputs: created tables and query results as DataFrames. Limitation: synchronization relies on dropna argument; user must ensure tables align if dropna=False.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_195\n\nLANGUAGE: python\nCODE:\n```\ndf_mt = pd.DataFrame(\n    np.random.randn(8, 6),\n    index=pd.date_range(\"1/1/2000\", periods=8),\n    columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n)\ndf_mt[\"foo\"] = \"bar\"\ndf_mt.loc[df_mt.index[1], (\"A\", \"B\")] = np.nan\n\n# you can also create the tables individually\nstore.append_to_multiple(\n    {\"df1_mt\": [\"A\", \"B\"], \"df2_mt\": None}, df_mt, selector=\"df1_mt\"\n)\nstore\n\n# individual tables were created\nstore.select(\"df1_mt\")\nstore.select(\"df2_mt\")\n\n# as a multiple\nstore.select_as_multiple(\n    [\"df1_mt\", \"df2_mt\"],\n    where=[\"A>0\", \"B>0\"],\n    selector=\"df1_mt\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Series and Nullable Boolean Array - pandas (Python)\nDESCRIPTION: This snippet initializes a pandas Series of integers and a corresponding mask using a pandas nullable Boolean array with a missing value. It demonstrates the setup required for subsequent indexing operations and assumes pandas 1.0+ is imported as pd. No special parameters are required, and the Series and mask arrays are output for inspection. This context is necessary for understanding the behavior and regression demonstrated in subsequent snippets.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.2.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3, 4])\\nmask = pd.array([True, True, False, None], dtype=\\\"boolean\\\")\\ns\\nmask\n```\n\n----------------------------------------\n\nTITLE: Demonstrating pd.to_datetime Error Handling Changes - IPython - Python\nDESCRIPTION: This set of code snippets demonstrates the changes in pd.to_datetime behavior regarding error handling when using the unit parameter and different values for the errors argument. The examples show both the previous and new behaviors, outlining how Pandas transitions from returning NaT or raising an OverflowError to consistent exception handling and result outputs. Usage requires Pandas installed; main parameters shown are the integer values, unit (e.g., 's' for seconds, 'D' for days), and errors policy ('coerce', 'ignore', 'raise'). Outputs include NaT, original value, or exceptions like OutOfBoundsDatetime and OverflowError. Limitations involve handling only specific edge cases, not all possible input types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_15\n\nLANGUAGE: IPython\nCODE:\n```\nIn [27]: pd.to_datetime(1420043460, unit='s', errors='coerce')\\nOut[27]: NaT\n```\n\nLANGUAGE: IPython\nCODE:\n```\nIn [28]: pd.to_datetime(11111111, unit='D', errors='ignore')\\nOverflowError: Python int too large to convert to C long\n```\n\nLANGUAGE: IPython\nCODE:\n```\nIn [29]: pd.to_datetime(11111111, unit='D', errors='raise')\\nOverflowError: Python int too large to convert to C long\n```\n\nLANGUAGE: IPython\nCODE:\n```\nIn [2]: pd.to_datetime(1420043460, unit='s', errors='coerce')\\nOut[2]: Timestamp('2014-12-31 16:31:00')\n```\n\nLANGUAGE: IPython\nCODE:\n```\nIn [3]: pd.to_datetime(11111111, unit='D', errors='ignore')\\nOut[3]: 11111111\n```\n\nLANGUAGE: IPython\nCODE:\n```\nIn [4]: pd.to_datetime(11111111, unit='D', errors='raise')\\nOutOfBoundsDatetime: cannot convert input with unit 'D'\n```\n\n----------------------------------------\n\nTITLE: Creating Release Branch and Development Tag\nDESCRIPTION: Git commands for creating a new release branch and development tag when releasing a release candidate. Used to establish the maintenance branch and mark the start of next version development.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/maintaining.rst#_snippet_4\n\nLANGUAGE: console\nCODE:\n```\ngit checkout -b 1.4.x\ngit push upstream 1.4.x\ngit checkout main\ngit commit --allow-empty -m \"Start 1.5.0\"\ngit tag -a v1.5.0.dev0 -m \"DEV: Start 1.5.0\"\ngit push upstream main --follow-tags\n```\n\n----------------------------------------\n\nTITLE: Computing Index.difference and Index.symmetric_difference with NaNs (ipython, Python)\nDESCRIPTION: Shows difference and symmetric_difference methods between Indexes with NaN values, before and after improved handling of NaNs as regular values. Demonstrates consistent output for index operations. Requires pandas and numpy; inputs are Index objects with potential NaNs, and outputs are index objects containing expected differences.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nidx1 = pd.Index([1, 2, 3, np.nan])\nidx2 = pd.Index([0, 1, np.nan])\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: idx1.difference(idx2)\nOut[3]: Float64Index([nan, 2.0, 3.0], dtype='float64')\n\nIn [4]: idx1.symmetric_difference(idx2)\nOut[4]: Float64Index([0.0, nan, 2.0, 3.0], dtype='float64')\n```\n\nLANGUAGE: python\nCODE:\n```\nidx1.difference(idx2)\nidx1.symmetric_difference(idx2)\n```\n\n----------------------------------------\n\nTITLE: Handling Scalar Extension Type in DataFrame Column Assignment (Python)\nDESCRIPTION: Fixed a bug where a DataFrame column set to a scalar extension type was considered an object type rather than the extension type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_63\n\nLANGUAGE: Python\nCODE:\n```\ndf['col'] = pd.array([1, 2, 3], dtype='Int64')\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Dictionary of Lists or ndarrays - pandas - Python\nDESCRIPTION: Demonstrates DataFrame construction from a dictionary of equal-length Python lists or NumPy arrays. Also shows specifying custom row labels by passing an explicit 'index'. All value arrays must have the same length as the index (if provided).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nd = {\"one\": [1.0, 2.0, 3.0, 4.0], \"two\": [4.0, 3.0, 2.0, 1.0]}\npd.DataFrame(d)\npd.DataFrame(d, index=[\"a\", \"b\", \"c\", \"d\"])\n```\n\n----------------------------------------\n\nTITLE: GroupBy Transformation Examples\nDESCRIPTION: Demonstrates transformation operations like cumsum and diff that maintain the original index structure of the grouped data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ngrouped = speeds.groupby(\"class\")[\"max_speed\"]\ngrouped.cumsum()\ngrouped.diff()\n```\n\n----------------------------------------\n\nTITLE: Series.to_numpy Usage for Timezone-Aware Data - Pandas - Python\nDESCRIPTION: These snippets show the use of the pandas Series.to_numpy method, with and without specifying the dtype, to convert a timezone-aware Series to a NumPy array. Providing no dtype will yield the default behavior (future pandas will preserve tz info); specifying dtype as 'datetime64[ns]' ensures timezone info is stripped. Inputs are timezone-aware Series objects. Outputs are NumPy arrays with or without timezone preservation, depending on the dtype argument. pandas is a prerequisite.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nser.to_numpy()\n```\n\nLANGUAGE: python\nCODE:\n```\nser.to_numpy(dtype=\"datetime64[ns]\")\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in DataFrame.explode for non-string scalar columns\nDESCRIPTION: Resolves an issue where DataFrame.explode was raising AssertionError when the column parameter was any scalar which is not a string.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.explode(column)\n```\n\n----------------------------------------\n\nTITLE: Accessing Columns with .loc in MultiIndex DataFrames\nDESCRIPTION: Shows how to access specific columns when using .loc with a MultiIndex, by providing both row and column specifications in the .loc accessor.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[(\"bar\", \"two\"), \"A\"]\n```\n\n----------------------------------------\n\nTITLE: Empty DataFrame GroupBy with MultiIndex in Python\nDESCRIPTION: Fixed regression in DataFrame.groupby with an empty DataFrame grouping by a level of a MultiIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nempty_df.groupby(level=0)\n```\n\n----------------------------------------\n\nTITLE: Constructing Categorical with Levels: Migration Pattern in Pandas (python and ipython)\nDESCRIPTION: These snippets illustrate the change in the two-argument Categorical constructor in Pandas. The old Python example uses 'levels' which is now replaced by 'categories' in the new API. The ipython cell demonstrates the new 'Categorical.from_codes' usage. Dependencies: pandas. Input: integer codes and category labels. Output: a Categorical object with specified categories. Required for porting pre-v0.15.0 categorical code.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npd.Categorical([0,1,0,2,1], levels=['a', 'b', 'c'])\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom __pandas_priority__ for 3rd Party Arithmetic - Python\nDESCRIPTION: Defines a custom list class with a __pandas_priority__ attribute to override pandas arithmetic dispatch order. By assigning a higher value (e.g., 5000), pandas will dispatch arithmetic operations to this class instead of its own Series/DataFrame methods. The __radd__ method shows a minimal implementation returning self. Requires definition before arithmetic with pandas objects. Useful for controlling interoperability between custom types and pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass CustomList(list):\n    __pandas_priority__ = 5000\n\n    def __radd__(self, other):\n        # return `self` and not the addition for simplicity\n        return self\n\ncustom = CustomList()\nseries = pd.Series([1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Pairwise Covariance Calculation Example\nDESCRIPTION: Shows usage of the new pairwise keyword in statistical window functions for calculating moving window covariance matrices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(10, 4), columns=list('ABCD'))\n\ncovs = pd.rolling_cov(df[['A', 'B', 'C']],\n                      df[['B', 'C', 'D']],\n                      5,\n                      pairwise=True)\n\ncovs[df.index[-1]]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dictionary Insertion Order Preservation in Python 3.6+\nDESCRIPTION: Example showing how Series instantiated from dictionaries now preserve the insertion order in Python 3.6+, compared to the previous alphabetical ordering behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_17\n\nLANGUAGE: ipython\nCODE:\n```\npd.Series({'Income': 2000,\n           'Expenses': -1500,\n           'Taxes': -200,\n           'Net result': 300})\n```\n\n----------------------------------------\n\nTITLE: Calculations with Missing Data in pandas (Python)\nDESCRIPTION: These snippets show how arithmetic operations, reductions, and cumulative methods treat missing data in pandas. They demonstrate addition of Series with missing data, sum and product reductions, and cumulative sum calculations with and without skipping NA values. Dependencies include pandas and numpy. Outputs illustrate that missing values are skipped or handled according to method parameters, sometimes treated as zero (sum), one (product), or preserved (cumulative methods).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nser1 = pd.Series([np.nan, np.nan, 2, 3])\nser2 = pd.Series([np.nan, 1, np.nan, 4])\nser1\nser2\nser1 + ser2\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([np.nan]).sum()\npd.Series([], dtype=\"float64\").sum()\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([np.nan]).prod()\npd.Series([], dtype=\"float64\").prod()\n```\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, np.nan, 3, np.nan])\nser\nser.cumsum()\nser.cumsum(skipna=False)\n```\n\n----------------------------------------\n\nTITLE: Subclassing pandas Styler for Custom Jinja Templating (Python)\nDESCRIPTION: Defines MyStyler, a subclass of pandas' Styler class, overriding the templating environment to include both user and default template folders. It initializes a Jinja Environment with a ChoiceLoader, retrieves a custom table template, and is designed for rendering DataFrames with extended or modified HTML output. Dependencies include pandas, Jinja2, and access to the custom template file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nclass MyStyler(Styler):\n    env = Environment(\n        loader=ChoiceLoader(\n            [\n                FileSystemLoader(\"templates\"),  # contains ours\n                Styler.loader,  # the default\n            ]\n        )\n    )\n    template_html_table = env.get_template(\"myhtml.tpl\")\n```\n\n----------------------------------------\n\nTITLE: Boolean indexing of Series with boolean index in Python\nDESCRIPTION: Shows how boolean indexing of a pandas Series with a boolean Index now selects elements where True, consistent with numpy array indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3], index=[False, True, False])\ns\ns.loc[pd.Index([True, False, True])]\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Functions with Rolling/Expanding Windows in Pandas - Python\nDESCRIPTION: Shows usage of the Series.rolling and .expanding methods with the apply function, demonstrating the raw option to send either a Series or an ndarray to the applied function. Dependencies are pandas and numpy. It covers rolling calculations with customizable behavior and explains the impact of raw=True and raw=False via examples using a simple lambda to select the last item.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.arange(5), np.arange(5) + 1)\ns\n```\n\nLANGUAGE: python\nCODE:\n```\ns.rolling(2, min_periods=1).apply(lambda x: x.iloc[-1], raw=False)\n```\n\nLANGUAGE: python\nCODE:\n```\ns.rolling(2, min_periods=1).apply(lambda x: x[-1], raw=True)\n```\n\n----------------------------------------\n\nTITLE: Creating a SparseDataFrame from SciPy Sparse Matrix\nDESCRIPTION: This code demonstrates how to create a SparseDataFrame directly from a scipy.sparse.csr_matrix instance, showcasing the new integration between pandas and SciPy sparse matrices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy.sparse import csr_matrix\narr = np.random.random(size=(1000, 5))\narr[arr < .9] = 0\nsp_arr = csr_matrix(arr)\nsp_arr\nsdf = pd.SparseDataFrame(sp_arr)\nsdf\n```\n\n----------------------------------------\n\nTITLE: Writing to and Reading From SQL Tables with Schema Using Pandas (python)\nDESCRIPTION: These examples show usage of 'to_sql' and 'read_sql_table' with the new 'schema' argument, writing and reading a DataFrame to a specific schema in a SQL database. Dependencies: pandas, created DataFrame 'df', and SQLAlchemy engine. Inputs: DataFrame, table name, engine, and schema. Outputs: data written to or read from 'other_schema'. This feature supports schema names for better database integration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.to_sql('table', engine, schema='other_schema')  # noqa F821\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_sql_table('table', engine, schema='other_schema')  # noqa F821\n```\n\n----------------------------------------\n\nTITLE: Melting a 3D Array into Data Frame - reshape2::melt - R\nDESCRIPTION: This R snippet uses reshape2's melt to transform a 3D array into a long-form data frame. It expects a 3D array named 'a' and applies melt to flatten indices and values. Output is a data.frame with index columns and array values. The reshape2 package is required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_14\n\nLANGUAGE: R\nCODE:\n```\na <- array(c(1:23, NA), c(2,3,4))\ndata.frame(melt(a))\n```\n\n----------------------------------------\n\nTITLE: StringDtype Series: Boolean Output from str Methods (Python)\nDESCRIPTION: Shows use of .str.isdigit() and .str.match() on a Series with StringDtype, which produce pandas BooleanDtype results. Ensures proper handling of missing values in boolean context and demonstrates expected output types. Input is Series of text/missing values; output is boolean Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ns.str.isdigit()\ns.str.match(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Constructing DataFrame from Series - Pandas - Python\nDESCRIPTION: Builds a DataFrame from a pandas Series. The resulting DataFrame uses the Series index for rows and the original Series name as the column label. Requires pandas imported as pd. Useful for converting 1D data structures to 2D.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(range(3), index=list(\"abc\"), name=\"ser\")\npd.DataFrame(ser)\n```\n\n----------------------------------------\n\nTITLE: Using SemiMonthBegin for Date Arithmetic and Ranges - Pandas - Python\nDESCRIPTION: Demonstrates creating dates using the SemiMonthBegin offset. The first example adds SemiMonthBegin to a Timestamp to obtain the first semi-month anchor, while the second generates a DatetimeIndex with semi-monthly start frequency. Only pandas is required; parameters involve the anchor date and number of periods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\npd.Timestamp(\"2016-01-01\") + SemiMonthBegin()\n\npd.date_range(\"2015-01-01\", freq=\"SMS\", periods=4)\n```\n\n----------------------------------------\n\nTITLE: Initializing DataFrame and Arrays for Inplace Assignment - pandas, numpy (Python)\nDESCRIPTION: This snippet creates a DataFrame with a single float column 'price' and custom string index, and also initializes two arrays: original_prices (a pandas Series slice) and new_prices (a NumPy array of integers). It sets up internal state for testing inplace column assignment and dtype changes. Dependencies: pandas, numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'price': [11.1, 12.2]}, index=['book1', 'book2'])\noriginal_prices = df['price']\nnew_prices = np.array([98, 99])\n```\n\n----------------------------------------\n\nTITLE: Reading CSVs with Thousands Separators Interpreted as Numbers - pandas - Python\nDESCRIPTION: Reads the same CSV as above but sets the thousands parameter so numbers are correctly parsed as integers rather than strings. Shows the difference in output dtype. Requires pandas and a filesystem.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"tmp.csv\", sep=\"|\", thousands=\",\")\ndf\n\ndf.level.dtype\n```\n\n----------------------------------------\n\nTITLE: Comparing DataFrame Addition with NumPy Ufunc and Operator in Pandas - Python\nDESCRIPTION: Demonstrates how the behavior of adding two pandas DataFrames with mismatched indices differs when using np.add versus the \"+\" operator. For np.add, the indices are ignored and shape-based broadcasting occurs, resulting in merely position-wise addition, while the operator aligns indices and columns by default, producing NaNs for non-overlapping rows/columns. Requires pandas and NumPy to be imported as pd and np, respectively. Inputs are DataFrames df1 and df2 with differing indices; outputs show the impact of alignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: df1 = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]}, index=[0, 1])\nIn [2]: df2 = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]}, index=[1, 2])\nIn [3]: df1\nOut[3]:\n   a  b\n0  1  3\n1  2  4\nIn [4]: df2\nOut[4]:\n   a  b\n1  1  3\n2  2  4\n\nIn [5]: np.add(df1, df2)\nOut[5]:\n   a  b\n0  2  6\n1  4  8\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [6]: df1 + df2\nOut[6]:\n     a    b\n0  NaN  NaN\n1  3.0  7.0\n2  NaN  NaN\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Pandas Series Behavior Changes in Python\nDESCRIPTION: Illustrates changes in Pandas Series behavior after refactoring, showing differences between numpy and pandas operations on Series objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3, 4])\n\n# Numpy Usage\nnp.ones_like(s)\nnp.diff(s)\nnp.where(s > 1, s, np.nan)\n\n# Pandonic Usage\npd.Series(1, index=s.index)\ns.diff()\ns.where(s > 1)\n```\n\n----------------------------------------\n\nTITLE: Plotting DataFrame with Custom Axis Labels - pandas/matplotlib - Python\nDESCRIPTION: This snippet includes two parts: first, plotting a DataFrame with default axis labels, and second, customizing the 'xlabel' and 'ylabel' with user-specified strings. The parameters allow users to override default labels for clarity. Requires pandas, matplotlib, and a DataFrame 'df'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndf.plot();\n\n@savefig plot_xlabel_ylabel.png\ndf.plot(xlabel=\"new x\", ylabel=\"new y\");\n```\n\n----------------------------------------\n\nTITLE: GroupBy Named Aggregation in Python using Pandas\nDESCRIPTION: Demonstrates the new 'named aggregation' feature in pandas for naming output columns when applying multiple aggregation functions to specific columns in a groupby operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nanimals = pd.DataFrame({'kind': ['cat', 'dog', 'cat', 'dog'],\n                           'height': [9.1, 6.0, 9.5, 34.0],\n                           'weight': [7.9, 7.5, 9.9, 198.0]})\nanimals.groupby(\"kind\").agg(\n    min_height=pd.NamedAgg(column='height', aggfunc='min'),\n    max_height=pd.NamedAgg(column='height', aggfunc='max'),\n    average_weight=pd.NamedAgg(column='weight', aggfunc=\"mean\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Showing GroupBy Agg and Transform Preserve Result dtype for Callables - Pandas Python\nDESCRIPTION: This snippet illustrates that DataFrameGroupBy.aggregate and related methods now avoid changing result dtype when applying callables, which might have previously caused numeric columns to get cast based on np.allclose logic. The example constructs a boolean DataFrame, groups by a column, and aggregates with a lambda sum. Requires pandas and numpy. Input: DataFrame with logical columns, grouped aggregation. Output: Grouped results with dtypes preserved.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'key': [1, 1], 'a': [True, False], 'b': [True, True]})\ndf\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: df.groupby('key').agg(lambda x: x.sum())\nOut[5]:\n        a  b\nkey         \n1    True  2\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby('key').agg(lambda x: x.sum())\n```\n\n----------------------------------------\n\nTITLE: Recommended Indexing with reindex Method in Python\nDESCRIPTION: Demonstrates the recommended way to select potentially not-found elements using the reindex method, which is the idiomatic approach in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ns.reindex([1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Converting Timezone-Aware Series to NumPy Array (Current Behavior) - Pandas - Python\nDESCRIPTION: This example shows how NumPy conversion of a timezone-aware pandas Series to a NumPy array currently drops timezone information after converting to UTC and issues a FutureWarning. The output array has dtype 'datetime64[ns]' and timezone info is lost. If the dtype is not specified, users see a warning. Inputs are a timezone-aware Series; output is a timezone-naive NumPy array. Requires both pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nIn [8]: np.asarray(ser)\n/bin/ipython:1: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive\n      ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray\n      with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\n\n        To accept the future behavior, pass 'dtype=object'.\n        To keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\n #!/bin/python3\nOut[8]:\narray(['1999-12-31T23:00:00.000000000', '2000-01-01T23:00:00.000000000'],\n      dtype='datetime64[ns]')\n```\n\n----------------------------------------\n\nTITLE: Constructing DatetimeIndex with Timezone and Inspecting Dtype in pandas (Python)\nDESCRIPTION: This snippet illustrates previous behavior in pandas for constructing a timezone-aware DatetimeIndex and retrieving its dtype. It uses pd.date_range with the tz parameter to create a DatetimeIndex, and accesses its dtype attribute. It highlights a change in dtype string representation between prior and new pandas releases. No external dependencies beyond pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_3\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pd.date_range('20130101', periods=3, tz='US/Eastern')\nOut[1]: DatetimeIndex(['2013-01-01 00:00:00-05:00', '2013-01-02 00:00:00-05:00',\n                       '2013-01-03 00:00:00-05:00'],\n                      dtype='datetime64[ns]', freq='D', tz='US/Eastern')\n\nIn [2]: pd.date_range('20130101', periods=3, tz='US/Eastern').dtype\nOut[2]: dtype('<M8[ns]')\n```\n\n----------------------------------------\n\nTITLE: Applying Functions to Groups in R\nDESCRIPTION: Demonstrates how to use the tapply function in R to apply a function to subsets of a vector.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_6\n\nLANGUAGE: r\nCODE:\n```\nbaseball <-\n  data.frame(team = gl(5, 5,\n             labels = paste(\"Team\", LETTERS[1:5])),\n             player = sample(letters, 25),\n             batting.average = runif(25, .200, .400))\n\ntapply(baseball$batting.average, baseball.example$team,\n       max)\n```\n\n----------------------------------------\n\nTITLE: Retrieving pyarrow Arrays from pandas Series/Index (Python)\nDESCRIPTION: Shows how to convert a pandas Series or Index with Arrow-backed data into a pyarrow array using pyarrow.array construction. Useful for interop and further Arrow-specific processing. The inputs are pandas Series/Index objects; the output is a pyarrow.Array or pyarrow.ChunkedArray. Requires pandas and pyarrow.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, None], dtype=\"uint8[pyarrow]\")\npa.array(ser)\n```\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index(ser)\npa.array(idx)\n```\n\n----------------------------------------\n\nTITLE: Grouping by Multiple Index Levels with pandas GroupBy in Python\nDESCRIPTION: Shows how to create a pandas Series with a three-level MultiIndex and perform groupby aggregation over multiple specified levels. Requires numpy and pandas. The 'arrays' define index levels, which are combined with MultiIndex.from_arrays; np.random.randn(8) creates random values for the Series. The input is a MultiIndex Series, and the output is a grouped and summed Series based on 'first' and 'second' levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\narrays = [\n    [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n    [\"doo\", \"doo\", \"bee\", \"bee\", \"bop\", \"bop\", \"bop\", \"bop\"],\n    [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n]\nindex = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\", \"third\"])\ns = pd.Series(np.random.randn(8), index=index)\ns\ns.groupby(level=[\"first\", \"second\"]).sum()\n```\n\n----------------------------------------\n\nTITLE: Boolean Indexing in Pandas\nDESCRIPTION: Examples of using boolean vectors and conditions to filter data in Series and DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(range(-3, 4))\ns[s > 0]\ns[(s < -1) | (s > 0.5)]\ns[~(s < 0)]\n\ndf[df['A'] > 0]\n```\n\n----------------------------------------\n\nTITLE: Advanced Indexing with .loc and MultiIndex in Python\nDESCRIPTION: Demonstrates advanced indexing with .loc accessor for hierarchically indexed DataFrames, showing how to access data by specifying complete or partial tuple keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf = df.T\ndf\ndf.loc[(\"bar\", \"two\")]\n```\n\n----------------------------------------\n\nTITLE: Running pytest to test pandas build in Python (Shell command)\nDESCRIPTION: This shell command runs pytest (a popular Python testing framework) to execute all tests in the pandas package. It requires Python and the pytest package to be installed. The command assumes you have a development version of pandas in the environment. Input: no explicit arguments. Output: test output in the terminal; may take a while due to the large pandas test suite. Cancel the run with Ctrl-C if desired.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_gitpod.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ python -m pytest pandas\n```\n\n----------------------------------------\n\nTITLE: Aggregating DataFrames with Mixed dtypes Using pandas - Python\nDESCRIPTION: Shows that when aggregating over DataFrames with mixed data types, only valid aggregations are performed per column. The snippet first creates a DataFrame with columns of int, float, object, and datetime types, then applies 'min' and 'sum' aggregations and prints their output types. Requires pandas, demonstrates expected dtypes for mixed columns, and outputs an aggregated DataFrame with selective aggregation results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': [1, 2, 3],\n                   'B': [1., 2., 3.],\n                   'C': ['foo', 'bar', 'baz'],\n                   'D': pd.date_range('20130101', periods=3)})\ndf.dtypes\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [10]: df.agg(['min', 'sum'])\nOut[10]:\n     A    B          C          D\nmin  1  1.0        bar 2013-01-01\nsum  6  6.0  foobarbaz        NaT\n```\n\n----------------------------------------\n\nTITLE: Implementing Docstring Inheritance in Child Classes\nDESCRIPTION: Shows how child classes inherit and customize docstrings from parent class using @doc decorator with class-specific parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n@doc(Parent.my_function, klass=\"ChildA\")\ndef my_function(self):\n    ...\n```\n\nLANGUAGE: python\nCODE:\n```\nclass ChildB(Parent):\n    @doc(Parent.my_function, klass=\"ChildB\")\n    def my_function(self):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Registering a Pandas Connector via pyproject.toml (TOML)\nDESCRIPTION: Illustrates how a third-party package declares an entrypoint for a pandas DataFrame connector using `pyproject.toml`. The registration under the `dataframe.io` entry points group is required for pandas to dynamically discover and inject the corresponding read/to methods. Dependencies are a standard pyproject-based project and compliance with the entrypoints API. The snippet registers a function that can become available as `pandas.read_duckdb` after plugin loading. The connector must implement the function referenced by the entry point.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_4\n\nLANGUAGE: toml\nCODE:\n```\n[project.entry-points.\"dataframe.io\"]\nreader_duckdb = \"pandas_duckdb:read_duckdb\"\n```\n\n----------------------------------------\n\nTITLE: to_timedelta: Numeric Array Conversion with Units - pandas - Python\nDESCRIPTION: Uses pd.to_timedelta to convert a numeric NumPy array to TimedeltaIndex, specifying the unit (seconds or days). Requires pandas and numpy. Illustrates the effect of the 'unit' parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.to_timedelta(np.arange(5), unit=\"s\")\npd.to_timedelta(np.arange(5), unit=\"D\")\n```\n\n----------------------------------------\n\nTITLE: Instantiating the Custom Styler Subclass with a DataFrame (Python)\nDESCRIPTION: This snippet creates an instance of the previously defined MyStyler class, passing a DataFrame (df3) as an argument. This prepares the MyStyler object to apply custom rendering logic and templates when calling further methods such as to_html. The input requirement is a DataFrame variable df3.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nMyStyler(df3)\n```\n\n----------------------------------------\n\nTITLE: Creating timezone-aware pandas Series and unique extraction (Python)\nDESCRIPTION: Illustrates constructing a pandas Series with timezone-aware Timestamp objects and outlines the prior unique extraction behavior that returned a numpy object array. This setup is prerequisite for testing updated behavior (currently unique returns a DatetimeArray). Inputs are two identical Timestamps with UTC timezone.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([pd.Timestamp('2000', tz='UTC'),\n                 pd.Timestamp('2000', tz='UTC')])\n```\n\n----------------------------------------\n\nTITLE: Renaming DataFrame Columns by Function in pandas (Python)\nDESCRIPTION: This snippet shows how to programmatically rename all columns in a DataFrame by applying a function, specifically converting all column names to lowercase via str.lower in the rename() method. The result is a DataFrame where all column labels are in lower case. The input is an existing DataFrame ('air_quality_renamed'), and the primary dependency is pandas. The output is a DataFrame with updated column names. The approach can be generalized with other functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/05_add_columns.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nair_quality_renamed = air_quality_renamed.rename(columns=str.lower)\nair_quality_renamed.head()\n```\n\n----------------------------------------\n\nTITLE: Alignable Assignment via MultiIndex Selection - pandas Python\nDESCRIPTION: Demonstrates assignment to a MultiIndex DataFrame subset using an alignable right-hand-side (RHS) object. Copies the DataFrame, selects a region using loc and IndexSlice, and assigns a scaled version of the entire DataFrame. Ensures alignment of data shapes for safe broadcasting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.copy()\ndf2.loc[idx[:, :, ['C1', 'C3']], :] = df2 * 1000\ndf2\n```\n\n----------------------------------------\n\nTITLE: Enabling Compression for HDF5 Store in Python\nDESCRIPTION: This snippet demonstrates how to enable compression for all objects within an HDF5 file using pandas. It specifies the compression level and library to use.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_196\n\nLANGUAGE: python\nCODE:\n```\nstore_compressed = pd.HDFStore(\n    \"store_compressed.h5\", complevel=9, complib=\"blosc:blosclz\"\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Displaying a DataFrame - Pandas - Python\nDESCRIPTION: This snippet demonstrates how to construct a Pandas DataFrame with named columns and string indices, which is used in subsequent indexing examples. Requires Pandas to be imported as pd. Returns the DataFrame object, showing the structure for further indexing operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': [1, 2, 3],\n                   'B': [4, 5, 6]},\n                  index=list('abc'))\n\ndf\n```\n\n----------------------------------------\n\nTITLE: Incrementally Reading an XPORT SAS File in Chunks - Python\nDESCRIPTION: This snippet demonstrates how to process an SAS XPORT (.xpt) file in chunks, reading 100,000 lines at a time by setting the chunk parameter to pandas.read_sas. The reader object allows you to iterate over file segments for efficient memory usage. You can process each chunk (e.g., with a custom do_something function). Inputs: file path, chunk size. Output: processed chunks via a user-defined function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_240\n\nLANGUAGE: python\nCODE:\n```\ndef do_something(chunk):\n    pass\n\n\nwith pd.read_sas(\"sas_xport.xpt\", chunk=100000) as rdr:\n    for chunk in rdr:\n        do_something(chunk)\n```\n\n----------------------------------------\n\nTITLE: Testing warnings in Python using pandas\nDESCRIPTION: Example of how to test for warnings in pandas using the tm.assert_produces_warning context manager.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nwith tm.assert_produces_warning(DeprecationWarning, match=\"the warning message\"):\n    pd.deprecated_function()\n```\n\n----------------------------------------\n\nTITLE: Copying Mask in IntegerArray.astype Method (Python)\nDESCRIPTION: Fixed a bug in IntegerArray.astype to correctly copy the mask as well as the data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_64\n\nLANGUAGE: Python\nCODE:\n```\nIntegerArray([1, 2, pd.NA]).astype('float')\n```\n\n----------------------------------------\n\nTITLE: Timing DataFrame Arithmetic with Unaligned Axes - Python\nDESCRIPTION: Demonstrates timing for DataFrame arithmetic when adding a Series with unaligned axes. Tests how pandas handles broadcasting and addition performance for mixed objects. Useful for real-world scenarios with dimension mismatches. Requires pandas, numpy, and IPython.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(np.random.randn(50))\n%timeit df1 + df2 + df3 + df4 + s\n```\n\n----------------------------------------\n\nTITLE: Highlighting Row and DataFrame Maximums\nDESCRIPTION: Applies different background colors to highlight maximum values across rows (pink) and the entire DataFrame (purple).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ns2.apply(highlight_max, props=\"color:white;background-color:pink;\", axis=1).apply(\n    highlight_max, props=\"color:white;background-color:purple\", axis=None\n)\n```\n\n----------------------------------------\n\nTITLE: Defining pandas.Series method: fillna in Python\nDESCRIPTION: Demonstrates the creation of a fillna method for a Series that replaces missing values with a specified scalar. Requires numpy and pandas. Shows a single missing value (NaN), replacement input, and expected list output. Highlights default and custom replacement capability.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef fillna(self, value):\n    \"\"\"\n    Replace missing values by ``value``.\n\n    Examples\n    --------\n    >>> ser = pd.Series([1, np.nan, 3])\n    >>> ser.fillna(0)\n    [1, 0, 3]\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in DataFrameGroupBy.agg with ExtensionArray Dtypes\nDESCRIPTION: Corrects a regression in DataFrameGroupBy.agg with dictionary input losing ExtensionArray dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nDataFrameGroupBy.agg()\n```\n\n----------------------------------------\n\nTITLE: Concatenating Categoricals After Behavior Change - Pandas - Python\nDESCRIPTION: Shows the updated syntax or behavior for concatenating two categorical Series after an internal pandas enhancement. The code demonstrates the new outcome of pd.concat([s1, s2]) post-update; it presumes the same context as the previous example, but the focus is to highlight any improvement or change. Only pandas is needed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\npd.concat([s1, s2])\n```\n\n----------------------------------------\n\nTITLE: Checking Instance Types for Interval and Period in Python\nDESCRIPTION: Shows how to check if an object is an instance of Interval or Period using isinstance() instead of the removed is_interval and is_period functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v3.0.0.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nisinstance(obj, pd.Interval)\nisinstance(obj, pd.Period)\n```\n\n----------------------------------------\n\nTITLE: Parsing Non-nanosecond Resolution Datetimes in Python\nDESCRIPTION: Fixed regression in to_datetime when parsing non-nanosecond resolution datetimes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\npd.to_datetime('2020-01-01 12:00:00.123')\n```\n\n----------------------------------------\n\nTITLE: Restoring NaN for NA bins in Resample Sum with min_count - Python\nDESCRIPTION: Shows how to revert to older behavior where resampled bins with all NA values return NaN, by passing min_count=1 to the sum. Dependencies: pandas as pd, numpy as np. Inputs/outputs: see code.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nIn [13]: s.resample(\"2d\").sum(min_count=1)\nOut[13]:\n2017-01-01    2.0\n2017-01-03    NaN\nFreq: 2D, Length: 2, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Using DataFrame.where for Conditional Assignment in Python\nDESCRIPTION: Demonstrates using the where method with a boolean mask to conditionally replace values. This approach provides an alternative to the loc-based conditional assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf_mask = pd.DataFrame(\n    {\"AAA\": [True] * 4, \"BBB\": [False] * 4, \"CCC\": [True, False] * 2}\n)\ndf.where(df_mask, -1000)\n```\n\n----------------------------------------\n\nTITLE: Removing an HDF5 Store File with os.remove - Python\nDESCRIPTION: Deletes the file 'store.h5' from the filesystem, effectively removing any existing HDFStore data for a fresh start. This is a dependency for subsequent code snippets to ensure a clean environment. Requires the os standard library; input is a filename string, and output is the removal of the file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_170\n\nLANGUAGE: python\nCODE:\n```\nos.remove(\"store.h5\")\n```\n\n----------------------------------------\n\nTITLE: Automatic Index Column Detection in CSV Data\nDESCRIPTION: Shows how pandas automatically uses the first column as row names when a file has one more column of data than the number of column names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,c\\n4,apple,bat,5.7\\n8,orange,cow,10\"\npd.read_csv(StringIO(data))\n```\n\n----------------------------------------\n\nTITLE: Exporting to LaTeX with Short Caption and Table Position using pandas.to_latex (Python)\nDESCRIPTION: Shows specifying table position and short caption when using DataFrame.to_latex. The parameters include 'position' (for LaTeX table placement) and 'caption' (which now accepts a tuple for full/short captions). Dependencies: pandas. Inputs: DataFrame; Output: LaTeX table string.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\ntable = data.to_latex(position='ht')\nprint(table)\n```\n\nLANGUAGE: python\nCODE:\n```\ndata = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\ntable = data.to_latex(caption=('the full long caption', 'short caption'))\nprint(table)\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Values in Series Using fillna with Value or Method - ipython\nDESCRIPTION: Shows correct usage of the fillna() method on a pandas Series, enforcing the requirement of either a fill value or a method (e.g., 'pad'). First, it attempts value filling, then demonstrates the 'pad' method for forward filling. Ensures compatibility with updated function signatures.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_5\n\nLANGUAGE: ipython\nCODE:\n```\nIn [6]: s = pd.Series([np.nan, 1.0, 2.0, np.nan, 4])\n\nIn [7]: s\nOut[7]:\n0      NaN\n1      1.0\n2      2.0\n3      NaN\n4      4.0\ndtype: float64\n\nIn [8]: s.fillna(0)\nOut[8]:\n0      0.0\n1      1.0\n2      2.0\n3      0.0\n4      4.0\ndtype: float64\n\nIn [9]: s.fillna(method=\"pad\")\nOut[9]:\n0      NaN\n1      1.0\n2      2.0\n3      2.0\n4      4.0\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Creating a MultiIndex from Tuples in Python\nDESCRIPTION: Demonstrates how to create a pandas MultiIndex object from a list of arrays, convert them to tuples, and use it to index a Series with random values. The example shows naming levels in the index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\narrays = [\n    [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n    [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n]\ntuples = list(zip(*arrays))\ntuples\n\nindex = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"])\nindex\n\ns = pd.Series(np.random.randn(8), index=index)\ns\n```\n\n----------------------------------------\n\nTITLE: Defining pandas.Series plot Method Example with ..plot:: in Python\nDESCRIPTION: Demonstrates the proper way to document plotting methods so example plots are executed when building documentation. Relies on pandas imported as pd. Shows a small Series and the call to its plot method within a ..plot:: directive for Sphinx rendering, ensuring visual output documentation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nclass Series:\n    def plot(self):\n        \"\"\"\n        Generate a plot with the ``Series`` data.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            >>> ser = pd.Series([1, 2, 3])\n            >>> ser.plot()\n        \"\"\"\n        pass\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Table Styles for Magnification in Pandas (Python)\nDESCRIPTION: This function defines a list of CSS rules for magnifying table elements on hover, intended for use with the pandas Styler set_table_styles method. It uses selectors for headers and data cells to adjust font size and padding dynamically. No external dependencies are required, but it should be paired with data displayed via pandas Styler table rendering. The function output is a list of dictionaries specifying the selectors and their corresponding CSS properties.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef magnify():\n    return [\n        {\"selector\": \"th\", \"props\": [(\"font-size\", \"4pt\")]},\n        {\"selector\": \"td\", \"props\": [(\"padding\", \"0em 0em\")]},\n        {\"selector\": \"th:hover\", \"props\": [(\"font-size\", \"12pt\")]},\n        {\n            \"selector\": \"tr:hover td:hover\",\n            \"props\": [(\"max-width\", \"200px\"), (\"font-size\", \"12pt\")],\n        },\n    ]\n```\n\n----------------------------------------\n\nTITLE: Extracting Underlying Data from Pandas Series and Index in Python\nDESCRIPTION: These snippets show how to use the .array property on Series and Index objects to access the underlying ExtensionArray or backing data. It's important when lower-level data manipulation is necessary or for interoperability with other libraries. Only available in recent versions of pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ns.array\ns.index.array\n```\n\n----------------------------------------\n\nTITLE: Column Assignment by Name (Direct setitem) - pandas, numpy (Python, deprecated behavior)\nDESCRIPTION: Shows how to retain the old assignment behavior by directly setting a DataFrame column by name (df[df.columns[0]] = new_prices). This approach replaces the column without affecting previously held Series references and maintains integer dtypes. Useful in cases where iloc's new inplace behavior is not desired. Dependencies: pandas, numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: df[df.columns[0]] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]:\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: price, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Checking for Constant Values in Series - Basic Approach\nDESCRIPTION: A performant approach to check if a Series contains constant values, without using nunique(). This method is more efficient as it doesn't compute all unique values first.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\nv = s.to_numpy()\nis_constant = v.shape[0] == 0 or (s[0] == s).all()\n```\n\n----------------------------------------\n\nTITLE: Converting SparseDataFrame to DataFrame with SparseArray (Python)\nDESCRIPTION: Example showing how to replace the deprecated SparseDataFrame with a standard DataFrame using SparseArray. This demonstrates the recommended way to handle sparse data in pandas after SparseDataFrame deprecation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": pd.arrays.SparseArray([0, 0, 1, 2])})\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Scatter Plot with Categorical Colors\nDESCRIPTION: This example shows how to create a scatter plot with colors based on a categorical column. When a categorical column is passed to the 'c' parameter, a discrete colorbar is produced.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndf.plot.scatter(x=\"a\", y=\"b\", c=\"species\", cmap=\"viridis\", s=50);\n```\n\n----------------------------------------\n\nTITLE: Appending and Selecting Data from HDF5 Store in Pandas\nDESCRIPTION: Demonstrates how to append dataframes to an HDF5 store and select data using Pandas. It shows appending partial dataframes, selecting the entire store, and querying specific data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf1 = df[0:4]\ndf2 = df[4:]\nstore.append('df', df1)\nstore.append('df', df2)\nstore.select('df')\n```\n\n----------------------------------------\n\nTITLE: Profiling DataFrame Apply Performance - Python\nDESCRIPTION: Uses IPython magic commands to benchmark execution time and profile function calls for integrating 'f' over DataFrame rows. Useful for identifying bottlenecks in pure Python approaches. Requires IPython environment. Outputs timing and profiling information for performance investigation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%timeit df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)\n```\n\nLANGUAGE: python\nCODE:\n```\n# most time consuming 4 calls\n%prun -l 4 df.apply(lambda x: integrate_f(x[\"a\"], x[\"b\"], x[\"N\"]), axis=1)  # noqa E999\n```\n\n----------------------------------------\n\nTITLE: Writing Excel Files to Memory Buffer\nDESCRIPTION: Shows how to write Excel data to a BytesIO object in memory instead of a physical file, useful for web applications or avoiding disk I/O.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_145\n\nLANGUAGE: python\nCODE:\n```\nfrom io import BytesIO\n\nbio = BytesIO()\n\n# By setting the 'engine' in the ExcelWriter constructor.\nwriter = pd.ExcelWriter(bio, engine=\"xlsxwriter\")\ndf.to_excel(writer, sheet_name=\"Sheet1\")\n\n# Save the workbook\nwriter.save()\n\n# Seek to the beginning and read to copy the workbook to a variable in memory\nbio.seek(0)\nworkbook = bio.read()\n```\n\n----------------------------------------\n\nTITLE: Computing Mean and Quantile Reductions on Timedelta Series (Python)\nDESCRIPTION: Illustrates calculation of the mean and a specific quantile (10th percentile) on a timedelta Series. Dependencies: pandas, NumPy. Input: Series 'td'. Outputs: single timedelta or float value, respectively. Useful for summarizing distributions of durations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ntd.mean()\ntd.quantile(.1)\n```\n\n----------------------------------------\n\nTITLE: Label and Positional Indexing Behavior for Float Indexers - pandas Series - Python and IPython\nDESCRIPTION: Explores Series indexing with floating point scalars for both label and positional indexers, highlighting error and warning behavior before and after pandas 0.18. Shows both code and standard outputs, including raising TypeError or FutureWarning. Requires Series (s, s2), pandas, and numpy if Float64Index is used.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3], index=[4, 5, 6])\ns\ns2 = pd.Series([1, 2, 3], index=list('abc'))\ns2\n```\n\nLANGUAGE: python\nCODE:\n```\ns[5.0]\ns.loc[5.0]\n```\n\nLANGUAGE: python\nCODE:\n```\ns_copy = s.copy()\ns_copy[5.0] = 10\ns_copy\ns_copy = s.copy()\ns_copy.loc[5.0] = 10\ns_copy\n```\n\nLANGUAGE: python\nCODE:\n```\ns.loc[5.0:6]\n```\n\nLANGUAGE: python\nCODE:\n```\ns.loc[5.1:6]\n```\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3], index=np.arange(3.))\ns[1.0]\ns[1.0:2.5]\n```\n\n----------------------------------------\n\nTITLE: Concatenating Series with Name Preservation in pandas - Python\nDESCRIPTION: Shows the effect of concatenating named and unnamed Series objects along axis=1, contrasting previous and new behaviors. Under the current version, named Series retain their column names in the result. Requires pandas; input are a list of Series, output is a concatenated DataFrame with name handling.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfoo = pd.Series([1, 2], name=\"foo\")\nbar = pd.Series([1, 2])\nbaz = pd.Series([4, 5])\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: DataFrameGroupBy.size Ignores as_index=False - ipython\nDESCRIPTION: Shows that the .size() method on DataFrameGroupBy objects previously ignored the as_index=False parameter and returned a Series with the group label as index. Demonstrates outputs that lack columns for grouping keys. Input: DataFrame, groupby with as_index, and size method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_26\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: df.groupby(\"a\", as_index=False).size()\\nOut[4]:\\na\\nx    2\\ny    2\\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Reindexing Pandas Integer Series and Observing Dtype Changes (Python)\nDESCRIPTION: This snippet demonstrates how reindexing a pandas Series of integers causes a dtype change from int64 to float64 when missing values (NaN) are introduced. Dependencies include pandas. The input is an integer-indexed Series, reindexed to add a missing label, resulting in floating-point representation due to NaN. The output is shown as a printed Series with float dtype; this illustrates pandas' default handling of integer data with missing values before nullable integer dtypes were introduced.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/blog/extension-arrays.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> int_ser = pd.Series([1, 2], index=[0, 2])\n>>> int_ser\n0    1\n2    2\ndtype: int64\n\n>>> int_ser.reindex([0, 1, 2])\n0    1.0\n1    NaN\n2    2.0\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Column Selection after DataFrame GroupBy with pandas in Python\nDESCRIPTION: Provides an example of selecting specific columns from a pandas GroupBy object. After grouping a DataFrame by column 'A', it shows how to access the grouped columns 'C' and 'D'. Requires pandas and a DataFrame containing columns 'A', 'B', 'C', and 'D'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n        \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n        \"C\": np.random.randn(8),\n        \"D\": np.random.randn(8),\n    }\n)\n\ndf\n\ngrouped = df.groupby([\"A\"])\ngrouped_C = grouped[\"C\"]\ngrouped_D = grouped[\"D\"]\n```\n\n----------------------------------------\n\nTITLE: Constructing DataFrame from Dict or Dict of Arrays with from_dict() - Pandas - Python\nDESCRIPTION: Utilizes DataFrame.from_dict to build a DataFrame from a dictionary of dicts or array-like objects. The 'orient' parameter controls whether keys are columns (default) or index. Column names can be specified. Requires pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame.from_dict(dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])]))\n```\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame.from_dict(\n    dict([(\"A\", [1, 2, 3]), (\"B\", [4, 5, 6])]),\n    orient=\"index\",\n    columns=[\"one\", \"two\", \"three\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Importing and Using scatter_matrix for DataFrame Visualization - pandas (Python)\nDESCRIPTION: Demonstrates how to import and use the scatter_matrix function from pandas.tools.plotting for visualizing pairwise relationships in a DataFrame. Requires Pandas installed (version 0.7.3 or compatible), and an existing DataFrame named df. The alpha parameter controls the transparency of the points. Outputs a scatter plot matrix, typically displayed in a Matplotlib window. Limitations include deprecation or location changes of scatter_matrix in newer versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.3.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.tools.plotting import scatter_matrix\n\nscatter_matrix(df, alpha=0.2)  # noqa F821\n```\n\n----------------------------------------\n\nTITLE: Series and DataFrame Map with NA Action - pandas - IPython\nDESCRIPTION: Shows the previous behavior of the map and applymap methods with 'na_action=\"ignore\"' on Series, DataFrames, and Index objects of categorical type. Requires pandas and NumPy. The snippet demonstrates cases where 'na_action' previously raised errors or only worked for DataFrames, highlighting limitations with categorical and extension types. Inputs are a categorical Series and related DataFrame/Index; outputs show error messages or DataFrame results, illustrating inconsistencies corrected in newer pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_3\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: ser = pd.Series([\"a\", \"b\", np.nan], dtype=\"category\")\nIn [2]: ser.map(str.upper, na_action=\"ignore\")\nNotImplementedError\nIn [3]: df = pd.DataFrame(ser)\nIn [4]: df.applymap(str.upper, na_action=\"ignore\")  # worked for DataFrame\n     0\n0    A\n1    B\n2  NaN\nIn [5]: idx = pd.Index(ser)\nIn [6]: idx.map(str.upper, na_action=\"ignore\")\nTypeError: CategoricalIndex.map() got an unexpected keyword argument 'na_action'\n```\n\n----------------------------------------\n\nTITLE: Indexing with Deprecated .ix Indexer - Pandas - Python\nDESCRIPTION: Illustrates the use of the deprecated .ix indexer to access rows by position and columns by label. The example, which is now discouraged, shows the old way to select the 0th and 2nd elements for column 'A'. Input is indices and column label; output is a Series of the selected values. Requires a DataFrame named df.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: df.ix[[0, 2], 'A']\nOut[3]:\na    1\nc    3\nName: A, dtype: int64\n```\n\n----------------------------------------\n\nTITLE: Weighted Rolling Window Normalization and Mean Calculation - pandas - Python\nDESCRIPTION: Shows use of rolling_window with weights (win_type='triang') and compare normalization change in 0.15.0. Input is Series with float values; output is weighted/normalized rolling means. Demonstrates impact of pandas version and window specification.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([10.5, 8.8, 11.4, 9.7, 9.3])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.rolling_window(s, window=3, win_type='triang', center=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Cloud Storage Access Dependencies for Pandas\nDESCRIPTION: Command to install dependencies for accessing data in cloud storage services like AWS S3 and Google Cloud Storage from pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pandas[fss, aws, gcp]\"\n```\n\n----------------------------------------\n\nTITLE: RST Directive for Regression Reference\nDESCRIPTION: ReStructuredText directive defining a reference label for regression fixes section\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.3.rst#_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. _whatsnew_103.regressions:\n```\n\n----------------------------------------\n\nTITLE: Creating and Storing a Pandas Panel in HDF5\nDESCRIPTION: Shows how to create a Pandas Panel object with random data and store it in an HDF5 file. It also demonstrates selecting data from the panel using query terms.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nwp = pd.Panel(np.random.randn(2, 5, 4), items=['Item1', 'Item2'],\n              major_axis=pd.date_range('1/1/2000', periods=5),\n              minor_axis=['A', 'B', 'C', 'D'])\nstore.append('wp', wp)\nstore.select('wp', [pd.Term('major_axis>20000102'),\n                    pd.Term('minor_axis', '=', ['A', 'B'])])\n```\n\n----------------------------------------\n\nTITLE: Timedelta Minimum and Maximum Limits - pandas - Python\nDESCRIPTION: Shows how to access the minimum and maximum representable Timedelta values in pandas. Useful for understanding data limits. Returns pd.Timedelta.min and pd.Timedelta.max as scalars.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npd.Timedelta.min\npd.Timedelta.max\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame for Join Examples in Python\nDESCRIPTION: Creation of an additional DataFrame (df4) with overlapping indexes but different columns for demonstrating join operations in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf4 = pd.DataFrame(\n    {\n        \"B\": [\"B2\", \"B3\", \"B6\", \"B7\"],\n        \"D\": [\"D2\", \"D3\", \"D6\", \"D7\"],\n        \"F\": [\"F2\", \"F3\", \"F6\", \"F7\"],\n    },\n    index=[2, 3, 6, 7],\n)\nresult = pd.concat([df1, df4], axis=1)\nresult\n```\n\n----------------------------------------\n\nTITLE: Initializing ArcticDB Storage for DataFrames (Python)\nDESCRIPTION: This example covers the initial setup for ArcticDB, including importing dependencies and creating an Arctic instance backed by an LMDB file store. Required dependencies are arcticdb, pandas, and numpy. The arctic instance provides access to collections of DataFrame symbols for scalable storage and retrieval; the connection string specifies the storage backend. Only local LMDB backend setup is shown, but ArcticDB supports S3/Blob storage via different URIs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport arcticdb as adb\nimport numpy as np\nimport pandas as pd\n# this will set up the storage using the local file system\narctic = adb.Arctic(\"lmdb://arcticdb_test\")\n```\n\n----------------------------------------\n\nTITLE: Filling Missing Values in pandas Index (Python)\nDESCRIPTION: This code snippet shows how to use the fillna method on a pandas Index object to replace NaN (missing) values with a specified value. Requires pandas and numpy, and is useful for cleaning up index objects before further processing. Parameter '2' is provided as the fill value, and the method returns a new Index with missing values filled.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.1.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npd.Index([1, np.nan, 3]).fillna(2)\n```\n\n----------------------------------------\n\nTITLE: Plotting Stacked Bar Charts with DataFrame.plot - pandas (Python)\nDESCRIPTION: Shows how to generate a vertical stacked bar chart using the plot method on a pandas DataFrame. Requires a pandas DataFrame named df containing appropriate numerical data. The kind argument specifies bar type and stacked=True creates a stacked bar chart. Outputs a bar plot, typically via Matplotlib. Only works with DataFrame or Series objects; use proper column types for best results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.3.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf.plot(kind=\"bar\", stacked=True)  # noqa F821\n```\n\n----------------------------------------\n\nTITLE: Enabling Future String Inference - pandas - Python\nDESCRIPTION: Demonstrates how to enable the new string inference behavior in pandas 2.1 by setting the option 'future.infer_string'. Requires pandas 2.1+, PyArrow to be installed, and allows storing string columns as PyArrow-backed dtypes for better performance and memory efficiency. The snippet sets the global pandas option, and affects all subsequent reads and object creation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npd.options.future.infer_string = True\n```\n\n----------------------------------------\n\nTITLE: Running Static Type Analysis with pre-commit Hooks and mypy/pyright - Shell\nDESCRIPTION: This shell code block shows how to invoke pre-commit to run static type checking tools (mypy, pyright, pyright_reportGeneralTypeIssues, stubtest) for the entire codebase. Ensures that local type hints are valid and compatible with these tools. Dependencies: pre-commit, mypy, pyright. Run in an activated Python environment. These commands validate type correctness across the repository.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npre-commit run --hook-stage manual --all-files mypy\npre-commit run --hook-stage manual --all-files pyright\npre-commit run --hook-stage manual --all-files pyright_reportGeneralTypeIssues\n# the following might fail if the installed pandas version does not correspond to your local git version\npre-commit run --hook-stage manual --all-files stubtest\n```\n\n----------------------------------------\n\nTITLE: Using Offset Objects with date_range for Nanosecond Steps (Python)\nDESCRIPTION: Demonstrates specifying frequency via offset objects when creating a date range with nanosecond granularity. Dependencies: pandas >= 0.13, numpy >= 1.7. Inputs: start date, periods, pd.offsets.Nano object. Outputs: high-precision DatetimeIndex. Useful where offset programmatic frequency specification is needed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\npd.date_range('2013-01-01', periods=5, freq=pd.offsets.Nano(5))\n```\n\n----------------------------------------\n\nTITLE: Comparing Tuple vs List Selection in MultiIndex\nDESCRIPTION: Demonstrates the difference between using tuples and lists for indexing with MultiIndex - tuples traverse levels horizontally while lists scan levels vertically.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(\n    [1, 2, 3, 4, 5, 6],\n    index=pd.MultiIndex.from_product([[\"A\", \"B\"], [\"c\", \"d\", \"e\"]]),\n)\ns.loc[[(\"A\", \"c\"), (\"B\", \"d\")]]  # list of tuples\ns.loc[([\"A\", \"B\"], [\"c\", \"d\"])]  # tuple of lists\n```\n\n----------------------------------------\n\nTITLE: Running a Specific Benchmark Group/Class in asv (Bash)\nDESCRIPTION: Invokes asv to benchmark only a particular group or class (e.g., 'GroupByMethods') inside a benchmark file, using '.' as a path separator in '-b'. Reduces run time when interested in a narrow performance aspect and requires that the group/class is defined in the benchmark file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\nasv continuous -f 1.1 upstream/main HEAD -b groupby.GroupByMethods\n```\n\n----------------------------------------\n\nTITLE: Using asof with DatetimeIndex in Pandas\nDESCRIPTION: Demonstrates the fixed behavior of DatetimeIndex.asof when looking up a partial string label, which now includes values that match the string even if they are after the start of the partial string label.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime(['2000-01-31', '2000-02-28']).asof('2000-02')\n```\n\n----------------------------------------\n\nTITLE: Timing Chained DataFrame Comparisons with pandas.eval - Python\nDESCRIPTION: Measures the performance of chained boolean comparison using pandas.eval, potentially leveraging the numexpr engine for speedup. Requires the same inputs as previous snippets. Results can be directly compared to standard operations for efficiency analysis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n%timeit pd.eval(\"(df1 > 0) & (df2 > 0) & (df3 > 0) & (df4 > 0)\")\n```\n\n----------------------------------------\n\nTITLE: Horizontal Cumulative Histogram for Series (Python)\nDESCRIPTION: Plots a single column from a DataFrame as a horizontal, cumulative histogram using Series.plot.hist(orientation=\"horizontal\", cumulative=True). Only column \"a\" is visualized and the cumulative distribution is displayed. Dependencies: pandas, numpy, matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\\n\\n@savefig hist_new_kwargs.png\\ndf4[\"a\"].plot.hist(orientation=\"horizontal\", cumulative=True);\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas and Creating DataFrame in Python\nDESCRIPTION: This snippet demonstrates how to import the pandas library and create a simple DataFrame. It shows the standard Python input format used in the documentation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/index.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\npd.DataFrame({'A': [1, 2, 3]})\n```\n\n----------------------------------------\n\nTITLE: UTC Datetime Series Conversion - Previous Behavior\nDESCRIPTION: Example showing the previous behavior of converting a Series to UTC datetime.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['20130101 00:00:00'] * 3)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime(s, utc=True)\nOut[12]:\n0   2013-01-01\n1   2013-01-01\n2   2013-01-01\ndtype: datetime64[ns]\n```\n\n----------------------------------------\n\nTITLE: Preserving Index Name When Concatenating Series Using pandas.concat (Python)\nDESCRIPTION: Shows that when using concat with Series having a named index, the index name is preserved in the resulting DataFrame. Dependencies: pandas. Inputs: Multiple Series objects with named indices; Output: DataFrame with correctly maintained index/column names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index(range(5), name='abc')\nser = pd.Series(range(5, 10), index=idx)\npd.concat({'x': ser[1:], 'y': ser[:-1]}, axis=1)\n```\n\n----------------------------------------\n\nTITLE: Casting Empty DataFrame to SparseDtype (Python)\nDESCRIPTION: Resolved a bug where an empty DataFrame could not be cast to SparseDtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_56\n\nLANGUAGE: Python\nCODE:\n```\npd.DataFrame().astype('Sparse')\n```\n\n----------------------------------------\n\nTITLE: Enabling Future String Dtype Inference in Pandas (Python)\nDESCRIPTION: This code snippet demonstrates how to enable the 'future.infer_string' option in pandas to opt-in to the upcoming default behavior for string columns in pandas 3.0. When this option is enabled, creating a Series with string data results in a dtype of 'string', and missing values are represented as NaN for compatibility with existing defaults. This requires pandas 2.2 or later, and optionally PyArrow for optimized string storage; otherwise, a NumPy object-dtype fallback is used. The snippet showcases the effect of this setting during construction of a Series with string and missing (None) values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0014-string-dtype.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> pd.options.future.infer_string = True\n>>> pd.Series([\"a\", \"b\", None])\n0      a\n1      b\n2    NaN\ndtype: string\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Info with and without Deep Memory Usage in pandas (Python)\nDESCRIPTION: This snippet demonstrates how to use pandas DataFrame.info() to display information about the DataFrame, including memory usage metrics. It shows both default and deep memory introspection, which is more accurate but computationally expensive. Dependencies include pandas; it uses a categorical column to illustrate memory reporting. Key parameters: 'memory_usage=\"deep\"' provides a detailed breakdown.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [\"foo\"] * 1000})  # noqa: F821\ndf[\"B\"] = df[\"A\"].astype(\"category\")\n\n# shows the '+' as we have object dtypes\ndf.info()\n\n# we have an accurate memory assessment (but can be expensive to compute this)\ndf.info(memory_usage=\"deep\")\n```\n\n----------------------------------------\n\nTITLE: Controlling Categorical Behavior with CategoricalDtype in Python\nDESCRIPTION: Illustrates how to use CategoricalDtype to control category inference and ordering. This allows for explicit specification of categories and whether they should be ordered.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import CategoricalDtype\n\ns = pd.Series([\"a\", \"b\", \"c\", \"a\"])\ncat_type = CategoricalDtype(categories=[\"b\", \"c\", \"d\"], ordered=True)\ns_cat = s.astype(cat_type)\ns_cat\n```\n\n----------------------------------------\n\nTITLE: min, max, idxmin, idxmax on Timedelta Series - pandas - Python\nDESCRIPTION: Performs reductions (min/max) and corresponding idx operations on Timedelta Series; resulting in scalar Timedelta or positional index. Illustrates chaining reductions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf.min().max()\ndf.min(axis=1).min()\n\ndf.min().idxmax()\ndf.min(axis=1).idxmin()\n```\n\n----------------------------------------\n\nTITLE: Formatting Integers in FloatIndex and CSV Output - pandas - Python\nDESCRIPTION: Shows transition from previous to new formatting for FloatIndex in pandas Series, where floats like 1.0 show decimal representation both in display and in I/O methods like to_csv. Requires pandas (and numpy for np.arange()), illustrates Series creation, index examination, and CSV output. Inputs involve Series with float index, output is properly formatted display and CSV string.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3], index=np.arange(3.))\ns\ns.index\nprint(s.to_csv(path_or_buf=None, header=False))\n```\n\n----------------------------------------\n\nTITLE: Using Nullable Integer Dtypes with Pandas Series (Python)\nDESCRIPTION: This snippet shows how pandas' new nullable integer dtypes (Int64Dtype) allow integer Series to represent missing values, preserving integer type while handling NaN. Requires pandas 0.24.0 or newer. The Series is constructed with Int64Dtype, and reindexing to include a missing label introduces NaN while retaining Int64 dtype. The output demonstrates zero-copy, type preservation, and improved handling of missing integer data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/blog/extension-arrays.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> int_ser = pd.Series([1, 2], index=[0, 2], dtype=pd.Int64Dtype())\n>>> int_ser\n0    1\n2    2\ndtype: Int64\n\n>>> int_ser.reindex([0, 1, 2])\n0      1\n1    NaN\n2      2\ndtype: Int64\n```\n\n----------------------------------------\n\nTITLE: Generating Accessor Attribute Documentation Template in Jinja\nDESCRIPTION: A Sphinx documentation template that creates documentation for accessor attributes. It uses Jinja templating to dynamically insert the full name, underline, module name, and object name into the documentation structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/_templates/autosummary/accessor_attribute.rst#_snippet_0\n\nLANGUAGE: jinja\nCODE:\n```\n{{ fullname }}\n{{ underline }}\n\n.. currentmodule:: {{ module.split('.')[0] }}\n\n.. autoaccessorattribute:: {{ (module.split('.')[1:] + [objname]) | join('.') }}\n```\n\n----------------------------------------\n\nTITLE: Defining a Cross-Section Normalization Function Using NumPy in Python\nDESCRIPTION: Defines a function f(x) that normalizes each row of a DataFrame-like object by subtracting the mean and dividing by the standard deviation, both measured across columns. The function uses NumPy broadcasting and is intended for use with Panel.apply or similar rowwise operations. Input is assumed to be a DataFrame or a 2D array, and the output is the normalized version with the same shape. No external dependencies beyond NumPy and pandas are required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.1.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    return ((x.T - x.mean(1)) / x.std(1)).T\n```\n\n----------------------------------------\n\nTITLE: Demonstrating DataFrame.apply with descriptive statistics in Python\nDESCRIPTION: Example of using DataFrame.apply method to calculate descriptive statistics on each column of a DataFrame containing random values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: df = pd.DataFrame(np.random.randn(10, 4))\nIn [2]: df.apply(lambda x: x.describe())\nOut[2]:\n           0          1          2          3\ncount  10.000000  10.000000  10.000000  10.000000\nmean    0.190912  -0.395125  -0.731920  -0.403130\nstd     0.730951   0.813266   1.112016   0.961912\nmin    -0.861849  -2.104569  -1.776904  -1.469388\n25%    -0.411391  -0.698728  -1.501401  -1.076610\n50%     0.380863  -0.228039  -1.191943  -1.004091\n75%     0.658444   0.057974  -0.034326   0.461706\nmax     1.212112   0.577046   1.643563   1.071804\n\n[8 rows x 4 columns]\n```\n\n----------------------------------------\n\nTITLE: Setting pandas Display Width Option and Showing Wide DataFrame (Python)\nDESCRIPTION: Changes the pandas option display.width to control how much of a DataFrame is printed on one row. After setting, a 3x12 DataFrame is displayed to show effect. Requires pandas, NumPy, and optionally previous DataFrame code.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.width\", 40)  # default is 80\n\npd.DataFrame(np.random.randn(3, 12))\n```\n\n----------------------------------------\n\nTITLE: DataFrame Column Product Operation with numeric_only None - pandas (Python, deprecated usage)\nDESCRIPTION: This code demonstrates product aggregation across mixed-type DataFrame columns. It highlights that calling .prod() on a slice with both integer and non-numeric (string) columns led to implicit behavior when numeric_only=None (now deprecated). Future pandas versions will require explicit specification of numeric_only or column selection. pandas is required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: df = pd.DataFrame({\"a\": [1, 2], \"b\": [\"x\", \"y\"]})\n\nIn [2]: # Reading the next line without knowing the contents of df, one would\n        # expect the result to contain the products for both columns a and b.\n        df[[\"a\", \"b\"]].prod()\nOut[2]:\na    2\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Converting Categorical String Values to Numeric Types\nDESCRIPTION: Shows how to convert string categories to numeric types after reading data by using to_numeric function and rename_categories method on categorical data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(StringIO(data), dtype=\"category\")\ndf.dtypes\ndf[\"col3\"]\nnew_categories = pd.to_numeric(df[\"col3\"].cat.categories)\ndf[\"col3\"] = df[\"col3\"].cat.rename_categories(new_categories)\ndf[\"col3\"]\n```\n\n----------------------------------------\n\nTITLE: Aligning and Applying NumPy Binary Universal Functions on Multiple Pandas Series (Python)\nDESCRIPTION: Illustrates how pandas aligns Series by index labels before applying a NumPy binary universal function (e.g., remainder). Dependencies: pandas, NumPy. ser1 and ser2 are created with different index orders; np.remainder(ser1, ser2) returns a Series indexed by the union of indices, filling missing values where needed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nser1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\nser2 = pd.Series([1, 3, 5], index=[\"b\", \"a\", \"c\"])\nser1\nser2\nnp.remainder(ser1, ser2)\n```\n\n----------------------------------------\n\nTITLE: Timing DataFrame Arithmetic with Standard Python Operators - Python\nDESCRIPTION: Measures execution time for adding four large DataFrames via standard Python DataFrame addition to establish a baseline for performance. Requires pandas, numpy, and IPython. Useful as a control for subsequent eval benchmarking.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n%timeit df1 + df2 + df3 + df4\n```\n\n----------------------------------------\n\nTITLE: Extracting Underlying IntervalArray and PeriodArray from Series - pandas (Python)\nDESCRIPTION: Illustrates extracting the underlying extension arrays (IntervalArray, PeriodArray) from Series storing interval or period data via the .array attribute. Inputs are Series of intervals or periods; outputs are the low-level extension arrays, suitable for custom extension logic, validation, or sharing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nser.array\npser.array\n```\n\n----------------------------------------\n\nTITLE: Setting Pandas Plotting Backend to Pandas-Bokeh in Python\nDESCRIPTION: This snippet configures pandas to use the Pandas-Bokeh plotting backend, enabling interactive web-based charts and maps with the familiar pandas plotting interface. No external imports are needed beyond pandas and the Pandas-Bokeh package must be installed. The main parameter is the backend string, which specifies 'pandas_bokeh'. Output is interactive Bokeh visualizations created with pandas' plot functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"plotting.backend\", \"pandas_bokeh\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Window Functions in pandas\nDESCRIPTION: Shows the new window function API where rolling operations are now methods on Series/DataFrame objects rather than top-level functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(1234)\ndf = pd.DataFrame({'A': range(10), 'B': np.random.randn(10)})\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\nr = df.rolling(window=3)\nr\nr.mean()\n```\n\n----------------------------------------\n\nTITLE: Pandas 1.2.5 Release Notes Structure\nDESCRIPTION: ReStructuredText markup defining the structure and sections of the pandas 1.2.5 release notes document.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.5.rst#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _whatsnew_125:\n\nWhat's new in 1.2.5 (June 22, 2021)\n-----------------------------------\n\nThese are the changes in pandas 1.2.5. See :ref:`release` for a full changelog\nincluding other versions of pandas.\n\n{{ header }}\n\n.. ---------------------------------------------------------------------------\n\n.. _whatsnew_125.regressions:\n\nFixed regressions\n~~~~~~~~~~~~~~~~~\n- Fixed regression in :func:`concat` between two :class:`DataFrame` where one has an :class:`Index` that is all-None and the other is :class:`DatetimeIndex` incorrectly raising (:issue:`40841`)\n- Fixed regression in :meth:`DataFrame.sum` and :meth:`DataFrame.prod` when ``min_count`` and ``numeric_only`` are both given (:issue:`41074`)\n- Fixed regression in :func:`read_csv` when using ``memory_map=True`` with an non-UTF8 encoding (:issue:`40986`)\n- Fixed regression in :meth:`DataFrame.replace` and :meth:`Series.replace` when the values to replace is a NumPy float array (:issue:`40371`)\n- Fixed regression in :func:`ExcelFile` when a corrupt file is opened but not closed (:issue:`41778`)\n- Fixed regression in :meth:`DataFrame.astype` with ``dtype=str`` failing to convert ``NaN`` in categorical columns (:issue:`41797`)\n\n.. ---------------------------------------------------------------------------\n\n.. _whatsnew_125.contributors:\n\nContributors\n~~~~~~~~~~~~\n\n.. contributors:: v1.2.4..v1.2.5|HEAD\n```\n\n----------------------------------------\n\nTITLE: Declaring C Types in Integration Functions with Cython - Cython\nDESCRIPTION: Optimizes Cython functions by annotating argument and local variable types, using 'cdef' and 'cpdef' for improved calling conventions and C-level performance. 'f_typed' and 'integrate_f_typed' are typed for double and int precision. Requires Cython and a compatible IPython environment. Inputs and outputs remain numeric, with much reduced overhead and significant speed gains compared to the untyped version.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_5\n\nLANGUAGE: cython\nCODE:\n```\ncdef double f_typed(double x) except? -2:\n    return x * (x - 1)\ncpdef double integrate_f_typed(double a, double b, int N):\n    cdef int i\n    cdef double s, dx\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_typed(a + i * dx)\n    return s * dx\n```\n\n----------------------------------------\n\nTITLE: Creating Series with Numeric and Datetime Indexes using pandas - Python\nDESCRIPTION: Demonstrates creating two pandas Series: one with an integer index and another with a DatetimeIndex. The Series objects are initialized using pd.Series and pd.date_range, setting up for subsequent indexing behavior demonstrations. No special dependencies beyond pandas are required; 'range' determines the data and 'index' sets the Series' axis labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nser1 = pd.Series(range(3), index=[0, 1, 2])\\nser2 = pd.Series(range(3), index=pd.date_range(\"2020-02-01\", periods=3))\n```\n\n----------------------------------------\n\nTITLE: Interpolating NaN Values in Series with Limit and Direction using pandas - Python\nDESCRIPTION: Shows how to use Series.interpolate with the limit and limit_direction arguments. Fills each consecutive NaN with at most one value both before and after valid data. Requires numpy and pandas; input is a Series with NaNs. Output is a new Series with select NaNs filled according to options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([np.nan, np.nan, 5, np.nan, np.nan, np.nan, 13])\nser.interpolate(limit=1, limit_direction=\"both\")\n```\n\n----------------------------------------\n\nTITLE: Upcasting Integer Data During DataFrame Operations - Pandas - Python\nDESCRIPTION: Demonstrates how integer-typed DataFrames can be partially upcast to larger integer types or floats during column assignment and boolean indexing. Shows integer-to-integer64 column casting and the effects of filtering and assignment with booleans, resulting in mixed int32, int64, and float64 columns. Highlights potential pitfalls with upcasting (preservation or change of dtypes).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_5\n\nLANGUAGE: ipython\nCODE:\n```\nIn [24]: dfi = df3.astype('int32')\n\nIn [25]: dfi['D'] = dfi['D'].astype('int64')\n\nIn [26]: dfi\nOut[26]:\n  A  B  C  D  E\n0  0  0  0  1  1\n1 -2  0  1  1  1\n2 -2  0  2  1  1\n3  0 -1  3  1  1\n4  1  0  4  1  1\n5  0  0  5  1  1\n6  0 -1  6  1  1\n7  0  0  7  1  1\n\nIn [27]: dfi.dtypes\nOut[27]:\nA    int32\nB    int32\nC    int32\nD    int64\nE    int32\ndtype: object\n\nIn [28]: casted = dfi[dfi > 0]\n\nIn [29]: casted\nOut[29]:\n    A   B    C  D  E\n0  NaN NaN  NaN  1  1\n1  NaN NaN  1.0  1  1\n2  NaN NaN  2.0  1  1\n3  NaN NaN  3.0  1  1\n4  1.0 NaN  4.0  1  1\n5  NaN NaN  5.0  1  1\n6  NaN NaN  6.0  1  1\n7  NaN NaN  7.0  1  1\n\nIn [30]: casted.dtypes\nOut[30]:\nA    float64\nB    float64\nC    float64\nD      int64\nE      int32\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Reindexing a DataFrame using method='nearest' in pandas (Python)\nDESCRIPTION: Demonstrates usage of DataFrame.reindex with method='nearest' to align a DataFrame with non-exact floating-point indices, where the nearest available index is used. Dependencies: pandas. Inputs: DataFrame, list of new indices, and reindexing method. Outputs: DataFrame with new index, populated from rows with nearest original index. This feature is only applicable if the DataFrame's index is monotonic and is valuable for time series and signal alignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'x': range(5)})\ndf.reindex([0.2, 1.8, 3.5], method='nearest')\n```\n\n----------------------------------------\n\nTITLE: Reading DataFrame from Parquet File using pandas with pyarrow/fastparquet (Python)\nDESCRIPTION: Reads the DataFrame back from Parquet files written with either 'fastparquet' or 'pyarrow' using pandas. Requires the appropriate engine library and the file previously written. Outputs the reloaded DataFrame and its dtypes, confirming dtype preservation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_208\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.read_parquet(\"example_fp.parquet\", engine=\"fastparquet\")\nresult = pd.read_parquet(\"example_pa.parquet\", engine=\"pyarrow\")\n\nresult.dtypes\n```\n\n----------------------------------------\n\nTITLE: Reading JSON from File to DataFrame - pandas - Python\nDESCRIPTION: Loads a DataFrame from a file containing JSON with pd.read_json. This assumes test.json exists and is well-formed. Inputs: file path as string. Outputs: DataFrame parsed from disk-based JSON.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_74\n\nLANGUAGE: python\nCODE:\n```\npd.read_json(\"test.json\")\n```\n\n----------------------------------------\n\nTITLE: Storing Period Data in Series - pandas (Python)\nDESCRIPTION: Shows analogous support for period data in pandas Series with appropriate extension dtype. Requires pandas >=0.24.0. Creates a period range and stores it in Series. Outputs confirm native period support in extension arrays, enhancing efficiency for temporal data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npser = pd.Series(pd.period_range(\"2000\", freq=\"D\", periods=5))\npser\npser.dtype\n```\n\n----------------------------------------\n\nTITLE: Converting a Panel to MultiIndex DataFrame - Pandas - Python\nDESCRIPTION: Shows how to convert a deprecated 3D Panel object to a MultiIndex DataFrame with the .to_frame() method. Input is a Panel object. The output is a DataFrame where major and minor axes form a MultiIndex, making data easier to manipulate as 2D. Requires an instantiated Panel p.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_46\n\nLANGUAGE: python\nCODE:\n```\np.to_frame()\n```\n\n----------------------------------------\n\nTITLE: Rounding DataFrame Values with pandas.DataFrame.round - Python\nDESCRIPTION: Illustrates the DataFrame.round method to round numeric values to a single number of decimals or use a dictionary for per-column rounding. Demonstrates both methods. Requires pandas and numpy. Input is a DataFrame; output is a DataFrame with rounded values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    np.random.random([3, 3]),\n    columns=[\"A\", \"B\", \"C\"],\n    index=[\"first\", \"second\", \"third\"],\n)\ndf\ndf.round(2)\ndf.round({\"A\": 0, \"C\": 2})\n```\n\n----------------------------------------\n\nTITLE: Exporting Data to SAS XPORT Format in SAS\nDESCRIPTION: Saves a copy of the 'tips' table into an XPORT-format transport file, renaming the 'total_bill' column to 'tbill' (to comply with 6-character limit). Requires access to the file system and permissions to write 'transport-file.xpt'. Outputs: XPORT-formatted SAS data, ready for interop. Limitation: SAS XPORT format restricts variable name length.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_25\n\nLANGUAGE: sas\nCODE:\n```\nlibname xportout xport 'transport-file.xpt';\ndata xportout.tips;\n    set tips(rename=(total_bill=tbill));\n    * xport variable names limited to 6 characters;\nrun;\n```\n\n----------------------------------------\n\nTITLE: Previous IntervalIndex Membership Test - Pandas - Python\nDESCRIPTION: Demonstrates the old behavior when using the 'in' operator with IntervalIndex, whereby any overlapping interval would return True. This uses the pandas Interval class and assumes 'ii' has been previously constructed. No additional imports are required beyond pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nIn [4]: pd.Interval(1, 2, closed='neither') in ii\\nOut[4]: True\\n\\nIn [5]: pd.Interval(-10, 10, closed='both') in ii\\nOut[5]: True\n```\n\n----------------------------------------\n\nTITLE: Label-Based Access and Membership in Series - pandas - Python\nDESCRIPTION: Shows accessing and modifying Series values by label, as well as checking label membership in the index. Demonstrates assignment, retrieval, and containment logic. The snippet will raise an error if the label does not exist (as shown below), unless handled.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ns[\"a\"]\ns[\"e\"] = 12.0\ns\n\"e\" in s\n\"f\" in s\n```\n\n----------------------------------------\n\nTITLE: Using pandas._typing for Common Type Aliases in Python Functions\nDESCRIPTION: Shows the import and use of a common type alias Dtype from the private pandas._typing module, intended for annotating function parameters that can accept various dtype representations. This aids code consistency and type safety for functions that interface with multiple dtype formats. Dependencies: pandas._typing, Python type hints. Parameters: dtype (Dtype). No function body is shown. Output: unspecified.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas._typing import Dtype\n\ndef as_type(dtype: Dtype) -> ...:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Fixing String to Nullable Integer Conversion in Python\nDESCRIPTION: Bug fix for Series.astype method when converting 'string' dtype data to nullable integer dtype, ensuring correct conversion.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['1', '2', '3'], dtype='string')\ns.astype('Int64')  # Now correctly converts to nullable integer\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Offset Normalization in Pandas - Python\nDESCRIPTION: This snippet illustrates the behavior of Pandas date offsets, showing the change introduced in version 0.14.1 where time is preserved by default during operations with offsets such as MonthEnd. It is designed to compare the previous and current behaviors for adding offsets to Timestamps, and demonstrates how to use the normalize=True parameter to achieve the former result. The code assumes that pandas is imported as pd and is run in an IPython or Jupyter environment; pandas.tseries.offsets must be available.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.tseries import offsets\n\nd = pd.Timestamp('2014-01-01 09:00')\n\n# old behaviour < 0.14.1\nd + offsets.MonthEnd()  # Out[8]: pd.Timestamp('2014-01-31 00:00:00')\n```\n\n----------------------------------------\n\nTITLE: Extending Pandas with Custom Extension Arrays - Pandas and Cyberpandas - Python\nDESCRIPTION: Illustrates storing custom non-NumPy array data types in pandas containers via the ExtensionArray interface. Uses the third-party library 'cyberpandas' to create an IPArray, inserts it into a pandas Series, and demonstrates type preservation and missing value semantics. Requires cyberpandas, pandas 0.23+, and Python. Key parameters include initializing the extension array and wrapping it as a pandas object; output is proper integration and dtype recognition within pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom cyberpandas import IPArray\n\n```\n\nLANGUAGE: python\nCODE:\n```\nvalues = IPArray([\n    0,\n    3232235777,\n    42540766452641154071740215577757643572\n])\n\n```\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(values)\n\n```\n\nLANGUAGE: python\nCODE:\n```\nser\n\n```\n\nLANGUAGE: python\nCODE:\n```\nser.isna()\n\n```\n\n----------------------------------------\n\nTITLE: Product of Empty and All-NA Series in pandas 0.22.0 - Python\nDESCRIPTION: Shows updated product computation in pandas 0.22.0: .prod() on empty or all-NaN Series returns 1, with min_count=1 restoring NaN. Demonstrates new API logic for products. Inputs: empty Series, Series([np.nan]), outputs: 1 or nan. Dependencies: pandas as pd, numpy as np.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.Series([]).prod()\npd.Series([np.nan]).prod()\npd.Series([]).prod(min_count=1)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Arithmetic Expressions Directly on DataFrames (Python)\nDESCRIPTION: Compares performance between pandas.eval and native Python operators for DataFrame arithmetic on large data. Dependencies: pandas, numpy. Inputs: df1, df2, df3, df4. Output: summed DataFrame and time measurements. Shows pros/cons of vectorized versus NumExpr backends.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n%timeit df1 + df2 + df3 + df4\n```\n\n----------------------------------------\n\nTITLE: Creating Shallow Copies of DataFrames - pandas - Python\nDESCRIPTION: Demonstrates creating a 'shallow' copy of a DataFrame using copy(deep=False), where the resulting DataFrame shares data/memory with the original, so modifying one mutates the other. Needs pandas, and a DataFrame. The code creates an initial DataFrame, clones it shallowly, and updates one element; both DataFrames reflect the change. Inputs are the original DataFrame and deep=False flag; output is a pair of linked DataFrames. Limitation: under new Copy-on-Write rules, this memory-sharing would be eliminated in favor of delayed or separate copy behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4], \"C\": [5, 6]})\n>>> df2 = df.copy(deep=False)\n>>> df2.iloc[0, 0] = 0   # will also modify df (but no longer with this proposal)\n```\n\n----------------------------------------\n\nTITLE: Positional Slicing of Series - pandas (Python, deprecated behavior)\nDESCRIPTION: This example shows positional slicing (old behavior) on a pandas Series with a custom integer index using ser[2:4]. The result returns rows based on positional order rather than index labels, demonstrating behavior that will change to label-based slicing in future pandas versions. The output reflects the selected rows and their values, and is formatted as would be shown in an IPython session.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: ser[2:4]\nOut[3]:\n5    3\n7    4\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Updating and Creating Columns via Expressions in DATA Step - SAS\nDESCRIPTION: Illustrates basic column arithmetic and assignment in the SAS DATA step. Creates two new/modified columns 'total_bill' and 'new_bill' based on existing values. The 'set tips;' statement loads input, and new variables are created with direct math expressions. Input: 'tips' dataset; output: modified 'tips' dataset with new columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_7\n\nLANGUAGE: SAS\nCODE:\n```\ndata tips;\\n    set tips;\\n    total_bill = total_bill - 2;\\n    new_bill = total_bill / 2;\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Embedding YouTube Tutorial Video in HTML\nDESCRIPTION: HTML iframe code for embedding a Pandas tutorial video from YouTube with responsive sizing and security parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/getting_started.md#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<iframe\n  src=\"https://www.youtube.com/embed/_T8LGqJtuGc\"\n  style=\"width: 100%; max-width: 560px; height: 315px;\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n  allowfullscreen\n></iframe>\n```\n\n----------------------------------------\n\nTITLE: Implementing PostgreSQL COPY Clause Insertion with Pandas and SQLAlchemy (Python)\nDESCRIPTION: Defines a callable that implements fast insertion using PostgreSQL's COPY FROM command via a custom method argument to pandas DataFrame.to_sql(). Dependencies include pandas, the csv module, SQLAlchemy, and a compatible PostgreSQL database driver. The function receives the table object, SQLAlchemy connection, list of column names (keys), and an iterable of data to insert; it formats and writes all rows efficiently using a single COPY command. The input data must be an iterable compatible with the csv.writer API; designed specifically for backends supporting the COPY statement.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_228\n\nLANGUAGE: python\nCODE:\n```\n# Alternative to_sql() *method* for DBs that support COPY FROM\nimport csv\nfrom io import StringIO\n\ndef psql_insert_copy(table, conn, keys, data_iter):\n    \"\"\"\n    Execute SQL statement inserting data\n\n    Parameters\n    ----------\n    table : pandas.io.sql.SQLTable\n    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n    keys : list of str\n        Column names\n    data_iter : Iterable that iterates the values to be inserted\n    \"\"\"\n    # gets a DBAPI connection that can provide a cursor\n    dbapi_conn = conn.connection\n    with dbapi_conn.cursor() as cur:\n        s_buf = StringIO()\n        writer = csv.writer(s_buf)\n        writer.writerows(data_iter)\n        s_buf.seek(0)\n\n        columns = ', '.join(['\"{}\"'.format(k) for k in keys])\n        if table.schema:\n            table_name = '{}.{}'.format(table.schema, table.name)\n        else:\n            table_name = table.name\n\n        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n            table_name, columns)\n        cur.copy_expert(sql=sql, file=s_buf)\n\n```\n\n----------------------------------------\n\nTITLE: Reading HTML with Colspan/Rowspan Handling - pandas (Python)\nDESCRIPTION: Demonstrates improved parsing of HTML tables with colspan and rowspan attributes using read_html from pandas >=0.24.0. Shows actual DataFrame output obtained after parsing a representative HTML table, preserving multi-cell data as expected. Requires pandas, and StringIO for input. Input is an HTML table string; output is a list of DataFrames with proper parsing of cell merges.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\nresult = pd.read_html(StringIO(\"\"\"\n  <table>\n    <thead>\n      <tr>\n        <th>A</th><th>B</th><th>C</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td colspan=\\\"2\\\">1</td><td>2</td>\n      </tr>\n    </tbody>\n  </table>\"\"\"))\n```\n\n----------------------------------------\n\nTITLE: Pickling DataFrame Objects in Pandas\nDESCRIPTION: Shows how to serialize a DataFrame to disk using the pickle format with the to_pickle method. This saves the entire data structure which can be loaded later.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_155\n\nLANGUAGE: python\nCODE:\n```\ndf\ndf.to_pickle(\"foo.pkl\")\n```\n\n----------------------------------------\n\nTITLE: Launching D-Tale Web Client for a Pandas DataFrame in Python\nDESCRIPTION: This snippet demonstrates how to open D-Tale's web interface for interactive exploration and manipulation of a pandas DataFrame. By calling dtale.show with a DataFrame, users can visualize, filter, and analyze their data in a browser-based environment. Requires dtale and pandas to be installed; the key parameter is the DataFrame, and output is an interactive local server session.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport dtale\n\ndtale.show(df)\n```\n\n----------------------------------------\n\nTITLE: Selecting, Dropping, and Renaming Columns in SAS DATA Step\nDESCRIPTION: Presents three SAS code examples: selecting columns ('keep'), removing ('drop'), and renaming ('rename') in the DATA step. Inputs are prior datasets; outputs are datasets with specified columns kept, dropped, or renamed respectively. Useful for data cleaning and preparation workflows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_11\n\nLANGUAGE: SAS\nCODE:\n```\ndata tips;\\n    set tips;\\n    keep sex total_bill tip;\\nrun;\\n\\ndata tips;\\n    set tips;\\n    drop sex;\\nrun;\\n\\ndata tips;\\n    set tips;\\n    rename total_bill=total_bill_2;\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Timezone Preservation in DataFrame Construction\nDESCRIPTION: Demonstrates how timezone information is now preserved when creating a DataFrame from a DatetimeIndex, stored as object dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ni = pd.date_range('1/1/2011', periods=3, freq='10s', tz='US/Eastern')\ni\ndf = pd.DataFrame({'a': i})\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: RST Documentation - Pandas 1.3.1 Release Notes\nDESCRIPTION: ReStructuredText documentation detailing the changes, fixed regressions, and bug fixes in Pandas 1.3.1 release from July 25, 2021. Includes sections on regressions, bug fixes, and contributors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.1.rst#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _whatsnew_131:\n\nWhat's new in 1.3.1 (July 25, 2021)\n-----------------------------------\n\nThese are the changes in pandas 1.3.1. See :ref:`release` for a full changelog\nincluding other versions of pandas.\n\n{{ header }}\n\n.. ---------------------------------------------------------------------------\n\n.. _whatsnew_131.regressions:\n\nFixed regressions\n~~~~~~~~~~~~~~~~~\n- pandas could not be built on PyPy (:issue:`42355`)\n- :class:`DataFrame` constructed with an older version of pandas could not be unpickled (:issue:`42345`)\n- Performance regression in constructing a :class:`DataFrame` from a dictionary of dictionaries (:issue:`42248`)\n- Fixed regression in :meth:`DataFrame.agg` dropping values when the DataFrame had an Extension Array dtype, a duplicate index, and ``axis=1`` (:issue:`42380`)\n- Fixed regression in :meth:`DataFrame.astype` changing the order of noncontiguous data (:issue:`42396`)\n- Performance regression in :class:`DataFrame` in reduction operations requiring casting such as :meth:`DataFrame.mean` on integer data (:issue:`38592`)\n- Performance regression in :meth:`DataFrame.to_dict` and :meth:`Series.to_dict` when ``orient`` argument one of \"records\", \"dict\", or \"split\" (:issue:`42352`)\n- Fixed regression in indexing with a ``list`` subclass incorrectly raising ``TypeError`` (:issue:`42433`, :issue:`42461`)\n- Fixed regression in :meth:`DataFrame.isin` and :meth:`Series.isin` raising ``TypeError`` with nullable data containing at least one missing value (:issue:`42405`)\n- Regression in :func:`concat` between objects with bool dtype and integer dtype casting to object instead of to integer (:issue:`42092`)\n- Bug in :class:`Series` constructor not accepting a ``dask.Array`` (:issue:`38645`)\n- Fixed regression for ``SettingWithCopyWarning`` displaying incorrect stacklevel (:issue:`42570`)\n- Fixed regression for :func:`merge_asof` raising ``KeyError`` when one of the ``by`` columns is in the index (:issue:`34488`)\n- Fixed regression in :func:`to_datetime` returning pd.NaT for inputs that produce duplicated values, when ``cache=True`` (:issue:`42259`)\n- Fixed regression in :meth:`SeriesGroupBy.value_counts` that resulted in an ``IndexError`` when called on a Series with one row (:issue:`42618`)\n\n.. ---------------------------------------------------------------------------\n\n.. _whatsnew_131.bug_fixes:\n\nBug fixes\n~~~~~~~~~\n- Fixed bug in :meth:`DataFrame.transpose` dropping values when the DataFrame had an Extension Array dtype and a duplicate index (:issue:`42380`)\n- Fixed bug in :meth:`DataFrame.to_xml` raising ``KeyError`` when called with ``index=False`` and an offset index (:issue:`42458`)\n- Fixed bug in :meth:`.Styler.set_sticky` not handling index names correctly for single index columns case (:issue:`42537`)\n- Fixed bug in :meth:`DataFrame.copy` failing to consolidate blocks in the result (:issue:`42579`)\n\n.. ---------------------------------------------------------------------------\n\n.. _whatsnew_131.contributors:\n\nContributors\n~~~~~~~~~~~~\n\n.. contributors:: v1.3.0..v1.3.1\n```\n\n----------------------------------------\n\nTITLE: Running a Subset of asv Benchmarks by Filename (Bash)\nDESCRIPTION: Filters asv benchmarks to run only matching files or patterns using '-b' with a regex. For example, only benchmarks from 'groupby.py' will be executed. Useful for targeting specific operation benchmarks and saving time.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nasv continuous -f 1.1 upstream/main HEAD -b ^groupby\n```\n\n----------------------------------------\n\nTITLE: Boolean/Bitwise Operations with Scalars - Python\nDESCRIPTION: Presents examples and limitations for mixing scalar and boolean/bitwise operators in pandas.eval contexts. This code illustrates cases that will raise exceptions if attempted in eval expressions, and clarifies the proper expected Python behavior. No actual evaluation or output; this is meant as a warning and a reference about operator precedence.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n1 and 2  # would parse to 1 & 2, but should evaluate to 2\n3 or 4  # would parse to 3 | 4, but should evaluate to 3\n~1  # this is okay, but slower when using eval\n```\n\n----------------------------------------\n\nTITLE: Preserving Extension Dtype in DataFrame Concatenation (Python)\nDESCRIPTION: Fixed a bug in concat when concatenating DataFrame objects with non-overlapping columns, now preserving the extension dtype instead of resulting in object-dtype columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_58\n\nLANGUAGE: Python\nCODE:\n```\npd.concat([df1, df2], axis=1) # where df1 and df2 have non-overlapping columns with extension dtypes\n```\n\n----------------------------------------\n\nTITLE: Deprecated TimeSeries Broadcasting with DataFrames in pandas (Python)\nDESCRIPTION: This snippet shows the now-deprecated behavior of broadcasting a Series (df.A) across a DataFrame by implicit alignment along the index, triggering a FutureWarning. This approach allowed operations like addition between a DataFrame and a Series on their shared index. In future pandas versions, explicit broadcasting is required. Dependencies: pandas. Input: DataFrame and Series; Output: FutureWarning and computed DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndf + df.A\nFutureWarning: TimeSeries broadcasting along DataFrame index by default is deprecated.\nPlease use DataFrame.<op> to explicitly broadcast arithmetic operations along the index\n```\n\n----------------------------------------\n\nTITLE: Inspecting the dtype of a pandas Series - Python\nDESCRIPTION: Retrieves the dtype of a specific column ('A') from a DataFrame using the .dtype attribute of a Series. Requires pandas, and assumes 'dft' DataFrame exists. The output is the dtype of the Series, usually used for diagnostics or type checks.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_73\n\nLANGUAGE: python\nCODE:\n```\ndft[\"A\"].dtype\n```\n\n----------------------------------------\n\nTITLE: Using Centered Datetime-like Rolling Windows in Pandas\nDESCRIPTION: Demonstrates how to perform rolling calculations on DataFrame objects with a datetime-like index using a centered window. This feature allows for more flexible time-based rolling operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"A\": [0, 1, 2, 3, 4]}, index=pd.date_range(\"2020\", periods=5, freq=\"1D\")\n)\ndf\ndf.rolling(\"2D\", center=True).mean()\n```\n\n----------------------------------------\n\nTITLE: Advanced pipe Method Usage with Statistical Modeling\nDESCRIPTION: Demonstrates advanced usage of the pipe method with statsmodels for statistical analysis. Shows how to handle functions that don't take DataFrame as the first argument by using a tuple of (function, keyword) to indicate where the DataFrame should flow.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.2.rst#_snippet_2\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: import statsmodels.formula.api as sm\n\nIn [2]: bb = pd.read_csv(\"data/baseball.csv\", index_col=\"id\")\n\n# sm.ols takes (formula, data)\nIn [3]: (\n...:     bb.query(\"h > 0\")\n...:     .assign(ln_h=lambda df: np.log(df.h))\n...:     .pipe((sm.ols, \"data\"), \"hr ~ ln_h + year + g + C(lg)\")\n...:     .fit()\n...:     .summary()\n...: )\n...: \nOut[3]:\n<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                                OLS Regression Results\n==============================================================================\nDep. Variable:                     hr   R-squared:                       0.685\nModel:                            OLS   Adj. R-squared:                  0.665\nMethod:                 Least Squares   F-statistic:                     34.28\nDate:                Tue, 22 Nov 2022   Prob (F-statistic):           3.48e-15\nTime:                        05:35:23   Log-Likelihood:                -205.92\nNo. Observations:                  68   AIC:                             421.8\nDf Residuals:                      63   BIC:                             432.9\nDf Model:                           4\nCovariance Type:            nonrobust\n===============================================================================\n                    coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -8484.7720   4664.146     -1.819      0.074   -1.78e+04     835.780\nC(lg)[T.NL]    -2.2736      1.325     -1.716      0.091      -4.922       0.375\nln_h           -1.3542      0.875     -1.547      0.127      -3.103       0.395\nyear            4.2277      2.324      1.819      0.074      -0.417       8.872\ng               0.1841      0.029      6.258      0.000       0.125       0.243\n==============================================================================\nOmnibus:                       10.875   Durbin-Watson:                   1.999\nProb(Omnibus):                  0.004   Jarque-Bera (JB):               17.298\nSkew:                           0.537   Prob(JB):                     0.000175\nKurtosis:                       5.225   Cond. No.                     1.49e+07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Setting Display Chop Threshold for Zero Rounding - Python\nDESCRIPTION: Controls at what numeric magnitude pandas rounds displayed DataFrame and Series values to zero via 'chop_threshold'. Demonstrates changing the threshold, displaying changes, and resetting. Only affects visual output, not internal storage. Useful for hiding negligible values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(6, 6))\npd.set_option(\"chop_threshold\", 0)\ndf\npd.set_option(\"chop_threshold\", 0.5)\ndf\npd.reset_option(\"chop_threshold\")\n```\n\n----------------------------------------\n\nTITLE: Constructing Nullable IntegerArray with pandas (Python)\nDESCRIPTION: Shows how to construct a pandas IntegerArray by passing a Python list containing integers and None values to pd.array along with an explicit Int64Dtype. This creates a nullable integer array, with missing values handled as pandas.NA. Requires pandas to be imported as pd; the result is an IntegerArray suitable for Series or DataFrame storage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\narr = pd.array([1, 2, None], dtype=pd.Int64Dtype())\narr\n```\n\n----------------------------------------\n\nTITLE: Series of Timedeltas with NaT Support - pandas - Python\nDESCRIPTION: Constructs a Series of time differences containing NaT (Not a Time) values via shifting. Useful for working with incomplete time series data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ny = s - s.shift()\ny\n```\n\n----------------------------------------\n\nTITLE: Series indexing with integer indexes in Python\nDESCRIPTION: Example showing that with integer indexes, Series indexing with sequences and slices behaves like ndarray indexing (positional) rather than label-based.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nIn [13]: s = pd.Series(np.random.randn(6), index=range(0, 12, 2))\n\nIn [14]: s[[4, 0, 2]]\nOut[14]:\n4    0.132003\n0    0.410835\n2    0.813850\nLength: 3, dtype: float64\n\nIn [15]: s[1:5]\nOut[15]:\n2    0.813850\n4    0.132003\n6   -0.827317\n8   -0.076467\nLength: 4, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Pandas GroupBy Describe Example\nDESCRIPTION: Shows the formatting of groupby describe() output\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': [1, 1, 2, 2], 'B': [1, 2, 3, 4]})\n\ndf.groupby('A').describe()\n\ndf.groupby('A').agg([\"mean\", \"std\", \"min\", \"max\"])\n```\n\n----------------------------------------\n\nTITLE: Default Integer and Float dtypes in pandas DataFrame Construction - Python\nDESCRIPTION: These examples illustrate pandas' default behavior of assigning int64 or float64 dtypes to integers and floats in DataFrames, independent of platform. They accept various forms of data and columns. Outputs are dtype Series showing default type selection. No extra dependencies other than pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_77\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame([1, 2], columns=[\"a\"]).dtypes\npd.DataFrame({\"a\": [1, 2]}).dtypes\npd.DataFrame({\"a\": 1}, index=list(range(2))).dtypes\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in DataFrame.groupby.rolling iteration for unsorted groupings\nDESCRIPTION: Resolves an issue where iterating over a DataFrame.groupby.rolling object caused the resulting DataFrames to have an incorrect index if the input groupings were not sorted.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor df in DataFrame.groupby(group).rolling(window):\n    # Iteration code\n```\n\n----------------------------------------\n\nTITLE: Serving the Built Pandas Website Locally - Bash\nDESCRIPTION: This bash command launches a simple Python-based HTTP server from the output directory containing the built pandas website. It allows users to access the site in a browser via localhost, which is necessary due to the use of absolute links and assets that are not compatible with directly opening HTML files. The command requires Python 3 and is run from the build output directory ('web/build/'). It serves all files in the current directory at http://localhost:8000 by default.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m http.server\n```\n\n----------------------------------------\n\nTITLE: Importing NumPy and pandas in Python\nDESCRIPTION: This snippet shows how to import the NumPy and pandas libraries to initialize your data science environment. Both 'np' and 'pd' are standard aliases and are prerequisites for all following operations and code examples in this document. No inputs or outputs; these imports are necessary for any pandas or NumPy code.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Rolling Sum with min_periods=0 on All-NA Series in pandas 0.21.1 - Python\nDESCRIPTION: This snippet demonstrates legacy behavior (pandas 0.21.1) where rolling sum with min_periods=0 over an all-NaN Series produced NaN output. Input: Series of NaNs, rolling window. Dependencies: pandas as pd, numpy as np.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nIn [17]: s = pd.Series([np.nan, np.nan])\n\nIn [18]: s.rolling(2, min_periods=0).sum()\nOut[18]:\n0   NaN\n1   NaN\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Applying NumPy any to a DataFrame with pandas 0.23.1 (Python)\nDESCRIPTION: Shows the behavior of applying np.any to a pandas DataFrame under NumPy 1.15 with pandas 0.23.1. Instead of reducing over all axes to a single value, np.any preserves column-wise evaluation, resulting in a Series of booleans indexed by column name. Requires both NumPy and pandas (versions specified). The input is a DataFrame with all-False entries; the output is a Series where each column is False. Illustrates behavior before pandas 0.23.2.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.2.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> # NumPy 1.15, pandas 0.23.1\n>>> np.any(pd.DataFrame({\"A\": [False], \"B\": [False]}))\nA    False\nB    False\ndtype: bool\n\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container for pandas Debugging\nDESCRIPTION: Docker command to run a container with a pre-installed debug environment for pandas, mounting the current directory.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/debugging_extensions.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it -w /data -v ${PWD}:/data pandas/pandas-debug\n```\n\n----------------------------------------\n\nTITLE: Handling NA Values in StringArray.isna with inf_as_na Option (Python)\nDESCRIPTION: Fixed a bug where StringArray.isna would return False for NA values when pandas.options.mode.use_inf_as_na was set to True.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_59\n\nLANGUAGE: Python\nCODE:\n```\nStringArray(['a', pd.NA, 'b']).isna()\n```\n\n----------------------------------------\n\nTITLE: Installing Data Format Dependencies for Pandas\nDESCRIPTION: Command to install dependencies for various data formats including HDF5, Parquet, Feather, SPSS, and Excel file handling in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pandas[hdf5, parquet, feather, spss, excel]\"\n```\n\n----------------------------------------\n\nTITLE: Evaluating pandas Series in Boolean Context - Python\nDESCRIPTION: Illustrates common mistakes when using pandas Series in Python boolean contexts (such as if statements), which raises ValueError due to ambiguity. Shows recommended workarounds like using .any(), .all(), .empty, or explicit checks against None. Requires pandas. Outputs boolean evaluation outcomes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/gotchas.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif pd.Series([False, True, False]):\n    print(\"I was true\")\n```\n\nLANGUAGE: python\nCODE:\n```\nif pd.Series([False, True, False]) is not None:\n    print(\"I was not None\")\n```\n\nLANGUAGE: python\nCODE:\n```\nif pd.Series([False, True, False]).any():\n    print(\"I am any\")\n```\n\n----------------------------------------\n\nTITLE: Reading SQL DataFrames (Old API) Using read_frame - Pandas - Python\nDESCRIPTION: Shows the previous method of reading SQL data into pandas, utilizing 'read_frame' from pandas.io.sql. This API is replaced by 'read_sql' in newer versions. Useful for loading query results directly as a DataFrame. Requires a pandas-compatible SQL database connection and necessary adapters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.io.sql import read_frame\n\nread_frame(...)\n```\n\n----------------------------------------\n\nTITLE: Staging and Listing Changes in Git Working Directory - Shell\nDESCRIPTION: This snippet shows how to inspect the status of your working tree, add changed or new files to the staging area, and view the status again to confirm. Inputs are file paths relative to the repository root. Outputs include the current branch and staged file list as displayed by 'git status'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing.rst#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ngit status\n```\n\nLANGUAGE: shell\nCODE:\n```\ngit add path/to/file-to-be-added-or-changed.py\n```\n\nLANGUAGE: shell\nCODE:\n```\nOn branch shiny-new-feature\\n\\n     modified:   /relative/path/to/file-to-be-added-or-changed.py\n```\n\n----------------------------------------\n\nTITLE: Handling min_count in DataFrameGroupBy for Nullable Boolean Dtypes (Python)\nDESCRIPTION: Fixed an issue where DataFrameGroupBy would ignore the min_count argument for aggregations on nullable Boolean dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_61\n\nLANGUAGE: Python\nCODE:\n```\ndf.groupby('A')['B'].sum(min_count=1) # where B is nullable Boolean dtype\n```\n\n----------------------------------------\n\nTITLE: Setting a Seed for Reproducibility on Windows Before Pytest (Bash)\nDESCRIPTION: Sets the PYTHONHASHSEED environment variable to a fixed value on Windows and then runs the test suite with advanced filtering and parallelization. Ensures reproducible hash-based operations and test results. Requires Windows shell and pytest/pytest-xdist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nset PYTHONHASHSEED=314159265\npytest pandas -n 4 -m \"not slow and not network and not db and not single_cpu\" -r sxX\n```\n\n----------------------------------------\n\nTITLE: Performing Arithmetic Operations (Modulo, Division) on DataFrames - Pandas - Python\nDESCRIPTION: Demonstrates how modulo and division operations behave on pandas DataFrame objects when encountering zeros or self-comparisons. This snippet illustrates pandas' correction for consistent NaN/Inf results across dtypes, matching float dtype behavior and fixing a prior inconsistency with NumPy integer operations. Requires pandas and numpy installed; shows expected outputs for edge mathematical cases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\np = pd.DataFrame({\"first\": [4, 5, 8], \"second\": [0, 0, 3]})\np % 0\np % p\np / p\np / 0\n```\n\n----------------------------------------\n\nTITLE: Propagating allows_duplicate_labels with set_flags and rename - pandas - Python\nDESCRIPTION: Creates a Series with allows_duplicate_labels set to False, then attempts to rename an index value to produce a duplicate, illustrating the propagation of strict label policy and error-raising behavior. Requires pandas 1.2.0+. Highlights limitations of feature propagation in some methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series(0, index=[\"a\", \"b\"]).set_flags(allows_duplicate_labels=False)\ns1\ns1.head().rename({\"a\": \"b\"})\n```\n\n----------------------------------------\n\nTITLE: Importing pandas and numpy in Python\nDESCRIPTION: This snippet imports the pandas and numpy libraries, necessary for all following examples. 'pd' and 'np' serve as commonly used aliases. Ensure both packages are installed before running any code examples using them.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/boolean.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Rolling Function min Behavior When min_periods > len(arg) - pandas - Python\nDESCRIPTION: Shows use and edge case where rolling_min returns NaN series if argument length is less than min_periods (for pandas <0.15, used to raise ValueError). Input is a pandas Series; outputs are rolling minimums or NaN, depending on parameters. Demonstrates backward and forward compatibility.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([10, 11, 12, 13])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.rolling_min(s, window=10, min_periods=5)\n```\n\n----------------------------------------\n\nTITLE: Creating New DataFrames from Existing DataFrames Using Methods - pandas - Python\nDESCRIPTION: Showcases creating new DataFrames by chaining DataFrame methods (like rename and set_index), each returning a copy by default in current pandas behavior. Needs pandas, and a DataFrame (df). Input: DataFrame, chained method calls; output: new DataFrames (df2, df3) each having their own separate data copies. In the Copy-on-Write approach, actual copies may be delayed until mutation, but logical behavior remains: methods return independently mutable DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n>>> df2 = df.rename(columns=str.lower)\n>>> df3 = df2.set_index(\"a\")\n```\n\n----------------------------------------\n\nTITLE: Using Sample Method in Pandas\nDESCRIPTION: Shows how to use the new sample() method to randomly sample from Series and DataFrames with various options including weighted sampling.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nexample_series = pd.Series([0, 1, 2, 3, 4, 5])\n\n# When no arguments are passed, returns 1\nexample_series.sample()\n\n# One may specify either a number of rows:\nexample_series.sample(n=3)\n\n# Or a fraction of the rows:\nexample_series.sample(frac=0.5)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"col1\": [9, 8, 7, 6], \"weight_column\": [0.5, 0.4, 0.1, 0]})\ndf.sample(n=3, weights=\"weight_column\")\n```\n\n----------------------------------------\n\nTITLE: DataFrame Creation with MultiIndex and Resample Preparation - Pandas - Python\nDESCRIPTION: Shows how to create a pandas DataFrame and a MultiIndex from arrays, setting up data for resampling demonstrations. The DataFrame includes a date column, a data column, and a MultiIndex with values and dates. Requires pandas (and numpy for np.arange).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"date\": pd.date_range(\"2015-01-01\", freq=\"W\", periods=5), \"a\": np.arange(5)},\n    index=pd.MultiIndex.from_arrays(\n        [[1, 2, 3, 4, 5], pd.date_range(\"2015-01-01\", freq=\"W\", periods=5)],\n        names=[\"v\", \"d\"],\n    ),\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in Ordered Categorical Min/Max\nDESCRIPTION: Addresses a regression where an ordered Categorical containing only NaN values would raise rather than returning NaN when taking the minimum or maximum.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nCategorical.min()\n```\n\nLANGUAGE: Python\nCODE:\n```\nCategorical.max()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Data from HDFStore in Pandas\nDESCRIPTION: Demonstrates different ways to retrieve stored objects from an HDFStore, including dictionary-style access and attribute access. Both methods return the original pandas object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_164\n\nLANGUAGE: python\nCODE:\n```\n# store.get('df') is an equivalent method\nstore[\"df\"]\n\n# dotted (attribute) access provides get as well\nstore.df\n```\n\n----------------------------------------\n\nTITLE: Reading CSV from RawIOBase with Encoding in Python\nDESCRIPTION: Fixed regression in read_csv where the encoding option was not recognized when used with a file-like object of type RawIOBase.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\npd.read_csv(file_like_object, encoding='utf-8')\n```\n\n----------------------------------------\n\nTITLE: Generating an Infinite Sequence with Python Generator and NumPy\nDESCRIPTION: This code defines a sample_values Python generator function yielding random floats between 0 and 1 indefinitely, using NumPy's random.random(). The docstring specifies the use of a 'Yields' section describing the type and meaning of each yielded value. Dependencies: numpy as np. Inputs: none. Outputs: infinite sequence of floats produced via generator protocol. Useful for demonstrating proper documentation of Python generator methods and yield annotation in docstrings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef sample_values():\n    \"\"\"\n    Generate an infinite sequence of random numbers.\n\n    The values are sampled from a continuous uniform distribution between\n    0 and 1.\n\n    Yields\n    ------\n    float\n        Random number generated.\n    \"\"\"\n    while True:\n        yield np.random.random()\n\n```\n\n----------------------------------------\n\nTITLE: Suppressing Tick Resolution Adjustment in Timeseries Plots - pandas/matplotlib - Python\nDESCRIPTION: These snippets show how to suppress automatic tick resolution adjustment on the x-axis in timeseries plots using the 'x_compat' parameter, and globally via a context manager. Requires pandas, matplotlib, and a timeseries DataFrame 'df'. Outputs control over tick labeling for improved axis alignment across plots.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\n\n@savefig ser_plot_suppress.png\ndf[\"A\"].plot();\n```\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\n\n@savefig ser_plot_suppress_parm.png\ndf[\"A\"].plot(x_compat=True);\n```\n\nLANGUAGE: python\nCODE:\n```\nplt.figure();\n\n@savefig ser_plot_suppress_context.png\nwith pd.plotting.plot_params.use(\"x_compat\", True):\n    df[\"A\"].plot(color=\"r\")\n    df[\"B\"].plot(color=\"g\")\n    df[\"C\"].plot(color=\"b\")\n```\n\n----------------------------------------\n\nTITLE: Consistent Parsing of Datetime Strings with Timestamp and DatetimeIndex - Python\nDESCRIPTION: Shows previous and updated parsing behaviors with pd.Timestamp and pd.DatetimeIndex for ambiguous or incomplete string dates (e.g., quarterly or year-only). Requires pandas installed and imported. Input strings like '2012Q2' and '2014' highlight differences in supported formats and default properties. New behavior allows consistent and successful parsing where previously ValueError was raised.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\npd.Timestamp('2012Q2')\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Timestamp('2014')\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Timestamp(\"2012Q2\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Timestamp(\"2014\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.DatetimeIndex([\"2012Q2\", \"2014\"])\n```\n\n----------------------------------------\n\nTITLE: get_dummies with sparse=True always returning DataFrame (pandas >=0.24, Python)\nDESCRIPTION: Demonstrates the new deterministic behavior in pandas 0.24 where pd.get_dummies with sparse=True always returns a DataFrame object, never a SparseDataFrame, regardless of input columns. Inputs are DataFrame instances, outputs are class objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ntype(pd.get_dummies(df, sparse=True))\ntype(pd.get_dummies(df[['B', 'C']], sparse=True))\n```\n\n----------------------------------------\n\nTITLE: Creating Example DataFrame for ORC Serialization using pandas (Python)\nDESCRIPTION: Constructs a pandas DataFrame with columns 'a', 'b', 'c', 'd' (bool), and 'e' (datetime) for use with ORC format examples. Requires pandas and numpy. Outputs the DataFrame and its dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_217\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"a\": list(\"abc\"),\n        \"b\": list(range(1, 4)),\n        \"c\": np.arange(4.0, 7.0, dtype=\"float64\"),\n        \"d\": [True, False, True],\n        \"e\": pd.date_range(\"20130101\", periods=3),\n    }\n)\n\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Potential Upcasting and Pitfalls with DataFrame.loc and astype - Python\nDESCRIPTION: Explains the effects of using .loc and .astype in combination, demonstrating that assignment via .loc tries to keep existing dtypes, potentially leading to unintentional upcasting, while direct assignment via slicing overwrites dtypes. Shows astype calls, assignment, and resulting dtypes. Requires pandas and NumPy; emphasizes behavioral subtleties when manipulating DataFrames in-place.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_84\n\nLANGUAGE: python\nCODE:\n```\ndft = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]})\ndft.loc[:, [\"a\", \"b\"]].astype(np.uint8).dtypes\ndft.loc[:, [\"a\", \"b\"]] = dft.loc[:, [\"a\", \"b\"]].astype(np.uint8)\ndft.dtypes\n```\n\n----------------------------------------\n\nTITLE: Sorting and Merging DataFrames in SAS\nDESCRIPTION: Sorts the df2 dataset by 'key', then performs a merge (join) between df1 and df2, outputting datasets for inner, left, right, and outer joins based on input flags. Dependencies: SAS Base module. Key parameters: df1, df2 (tables); 'key' (join key). Inputs: two datasets (df1, df2); outputs: four datasets, each representing a join type. Assumes both input datasets exist and have a 'key' field.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_19\n\nLANGUAGE: sas\nCODE:\n```\nproc sort data=df2;\n    by key;\nrun;\n\ndata left_join inner_join right_join outer_join;\n    merge df1(in=a) df2(in=b);\n\n    if a and b then output inner_join;\n    if a then output left_join;\n    if b then output right_join;\n    if a or b then output outer_join;\nrun;\n```\n\n----------------------------------------\n\nTITLE: Mixing ID and Class in set_table_styles and set_td_classes in pandas Styling (Python)\nDESCRIPTION: This snippet expands the CSS specificity example by adding a class-based style, and associates a class to a table cell with set_td_classes. The example requires pandas, and expects a DataFrame whose single cell is assigned the class 'cls-1'. The precedence is demonstrated where the ID plus class selector out-prioritizes others and styles the cell blue. Inputs include the DataFrame and the class assignment DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndf4.style.set_uuid(\"b_\").set_table_styles(\n    [\n        {\"selector\": \"td\", \"props\": \"color:red;\"},\n        {\"selector\": \".cls-1\", \"props\": \"color:blue;\"},\n    ]\n).map(lambda x: \"color:green;\").set_td_classes(pd.DataFrame([[\"cls-1\"]]))\n```\n\n----------------------------------------\n\nTITLE: Initializing a Library for Storing DataFrames in ArcticDB (Python)\nDESCRIPTION: This snippet shows how to get (or create) a library within an ArcticDB instance for storing multiple DataFrames (symbols). The get_library method returns a library object, and the create_if_missing parameter ensures the library is created if it does not exist. The required dependency is arcticdb, and this is a prerequisite step before performing further DataFrame operations on ArcticDB.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlib = arctic.get_library('sample', create_if_missing=True)\n```\n\n----------------------------------------\n\nTITLE: Enabling Unicode East Asian Width Display in pandas - Python\nDESCRIPTION: Demonstrates enabling pandas' option to handle Unicode East Asian width characters during DataFrame display. Requires pandas; input is the option string and boolean value. Changing this affects the alignment of DataFrame or Series outputs in the console.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.unicode.east_asian_width\", True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Counting Columns by dtype in a DataFrame - Python\nDESCRIPTION: Utilizes the value_counts() method of the .dtypes Series attribute to count columns by their dtype in a DataFrame. Requires a pandas DataFrame (e.g., 'dft') and returns a Series mapping dtype names to counts. This offers a high-level overview of DataFrame schema composition with no additional dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_75\n\nLANGUAGE: python\nCODE:\n```\ndft.dtypes.value_counts()\n```\n\n----------------------------------------\n\nTITLE: Using Nested Function Calls in Pandas (Before pipe Method)\nDESCRIPTION: Demonstrates the traditional way of chaining function calls in Pandas, with nested functions that can be difficult to read as the logic flows from inside out and function names are separated from their arguments.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.2.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# df is a DataFrame\n# f, g, and h are functions that take and return DataFrames\nf(g(h(df), arg1=1), arg2=2, arg3=3)  # noqa F821\n```\n\n----------------------------------------\n\nTITLE: Multi-column Sorting with Categorical Data\nDESCRIPTION: Shows how categorical columns participate in multi-column sorting operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.DataFrame({\n   \"A\": pd.Categorical(\n       list(\"bbeebbaa\"),\n       categories=[\"e\", \"a\", \"b\"],\n       ordered=True,\n   ),\n   \"B\": [1, 2, 1, 2, 2, 1, 2, 1],\n})\ndfs.sort_values(by=[\"A\", \"B\"])\n```\n\n----------------------------------------\n\nTITLE: Arithmetic Operations with NaT and Timedelta Types - pandas - Python\nDESCRIPTION: Showcases expanded support for arithmetic operations involving NaT (Not-a-Time) and Timedelta types, including scalar arithmetic, Null propagation, and operations within Series of datetime64[ns] and timedelta64[ns] dtypes. Requires pandas and numpy, input values include NaT, Timedelta strings, and Series of those types. Outputs illustrate result types, propagation rules, and error behavior for unsupported combinations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npd.NaT * 1\npd.NaT * 1.5\npd.NaT / 2\npd.NaT * np.nan\n```\n\nLANGUAGE: python\nCODE:\n```\npd.NaT / pd.NaT\npd.Timedelta('1s') / pd.NaT\n```\n\nLANGUAGE: python\nCODE:\n```\npd.NaT + pd.NaT\n\n# same as\npd.Timedelta('1s') + pd.Timedelta('1s')\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Timestamp('19900315') + pd.Timestamp('19900315')\n# TypeError: unsupported operand type(s) for +: 'Timestamp' and 'Timestamp'\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Series([pd.NaT], dtype='<M8[ns]') + pd.Series([pd.NaT], dtype='<M8[ns]')\n```\n\n----------------------------------------\n\nTITLE: Combining Positional and Label-Based Indexing\nDESCRIPTION: Shows how to get elements from specific positions in the index using .loc with index attribute methods for precise selection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndfd = pd.DataFrame({'A': [1, 2, 3],\n                  'B': [4, 5, 6]},\n                 index=list('abc'))\ndfd\ndfd.loc[dfd.index[[0, 2]], 'A']\n```\n\n----------------------------------------\n\nTITLE: New Behavior of GroupBy on Categoricals with Missing Categories\nDESCRIPTION: This code demonstrates the fixed behavior in pandas 0.20.0 where groupby operations properly handle categorical data with missing categories when sort=False is specified.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf[df.chromosomes != '1'].groupby('chromosomes', observed=False, sort=False).sum()\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to ArcticDB\nDESCRIPTION: This code demonstrates how to write a pandas DataFrame to an ArcticDB storage library. ArcticDB is a high-performance datastore for numeric data designed for financial timeseries.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwrite_record = lib.write(\"test\", df)\n```\n\n----------------------------------------\n\nTITLE: GroupBy Aggregations with Categorical dtype in Python\nDESCRIPTION: Fixed regression in .groupby() aggregations with categorical dtype using Cythonized reduction functions (e.g. first).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndf.groupby('categorical_column').first()\n```\n\n----------------------------------------\n\nTITLE: Updating Pip Development Environment\nDESCRIPTION: Commands to update local main branch and refresh pip-based development environment with latest dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing.rst#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\n# activate the virtual environment based on your platform\npython -m pip install --upgrade -r requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: Summarizing Plotting Functions with Sphinx Autosummary (reStructuredText)\nDESCRIPTION: This reStructuredText snippet defines a documentation section for plotting utilities in the pandas.plotting module, using Sphinx's autosummary extension to auto-generate an API index. No non-standard dependencies are needed beyond Sphinx configuration. The listed identifiers (e.g., 'andrews_curves', 'autocorrelation_plot') represent public-facing API functions to be included as documented entries; no function implementations are present. The snippet is intended for auto-inclusion in the documentation build process, and expects the containing environment to be set up for Sphinx and pandas source documentation. The output is not executable code but RST that controls rendered API documentation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/plotting.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n========\nPlotting\n========\n.. currentmodule:: pandas.plotting\n\nThe following functions are contained in the ``pandas.plotting`` module.\n\n.. autosummary::\n   :toctree: api/\n\n   andrews_curves\n   autocorrelation_plot\n   bootstrap_plot\n   boxplot\n   deregister_matplotlib_converters\n   lag_plot\n   parallel_coordinates\n   plot_params\n   radviz\n   register_matplotlib_converters\n   scatter_matrix\n   table\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Retention of Custom Properties After Manipulation - Python\nDESCRIPTION: Shows how temporary (_internal_names) and persistent (_metadata) properties behave during manipulations on SubclassedDataFrame2. 'internal_cache' is lost after slicing, while 'added_property' is retained. Demonstrates assignment, retrieval, and error raising on property access. Requires pandas and appropriate class definition.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> df = SubclassedDataFrame2({\"A\": [1, 2, 3], \"B\": [4, 5, 6], \"C\": [7, 8, 9]})\n>>> df\n   A  B  C\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\n>>> df.internal_cache = \"cached\"\n>>> df.added_property = \"property\"\n\n>>> df.internal_cache\ncached\n>>> df.added_property\nproperty\n\n# properties defined in _internal_names is reset after manipulation\n>>> df[[\"A\", \"B\"]].internal_cache\nAttributeError: 'SubclassedDataFrame2' object has no attribute 'internal_cache'\n\n# properties defined in _metadata are retained\n>>> df[[\"A\", \"B\"]].added_property\nproperty\n```\n\n----------------------------------------\n\nTITLE: Autosummary for DatetimeIndex and TimedeltaIndex - reStructuredText\nDESCRIPTION: These code blocks provide a Sphinx autosummary registry for methods and properties of the pandas DatetimeIndex and TimedeltaIndex classes, including core class registration, time/date-related attributes, selection methods, conversion routines, and statistical calculations. Sections are organized to group similar methods for easier reference. Requires Sphinx, pandas, and valid member objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/indexing.rst#_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n   :template: autosummary/class_without_autosummary.rst\\n\\n   DatetimeIndex\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   DatetimeIndex.year\\n   DatetimeIndex.month\\n   DatetimeIndex.day\\n   DatetimeIndex.hour\\n   DatetimeIndex.minute\\n   DatetimeIndex.second\\n   DatetimeIndex.microsecond\\n   DatetimeIndex.nanosecond\\n   DatetimeIndex.date\\n   DatetimeIndex.time\\n   DatetimeIndex.timetz\\n   DatetimeIndex.dayofyear\\n   DatetimeIndex.day_of_year\\n   DatetimeIndex.dayofweek\\n   DatetimeIndex.day_of_week\\n   DatetimeIndex.weekday\\n   DatetimeIndex.quarter\\n   DatetimeIndex.tz\\n   DatetimeIndex.freq\\n   DatetimeIndex.freqstr\\n   DatetimeIndex.is_month_start\\n   DatetimeIndex.is_month_end\\n   DatetimeIndex.is_quarter_start\\n   DatetimeIndex.is_quarter_end\\n   DatetimeIndex.is_year_start\\n   DatetimeIndex.is_year_end\\n   DatetimeIndex.is_leap_year\\n   DatetimeIndex.inferred_freq\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   DatetimeIndex.indexer_at_time\\n   DatetimeIndex.indexer_between_time\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   DatetimeIndex.normalize\\n   DatetimeIndex.strftime\\n   DatetimeIndex.snap\\n   DatetimeIndex.tz_convert\\n   DatetimeIndex.tz_localize\\n   DatetimeIndex.round\\n   DatetimeIndex.floor\\n   DatetimeIndex.ceil\\n   DatetimeIndex.month_name\\n   DatetimeIndex.day_name\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   DatetimeIndex.as_unit\\n   DatetimeIndex.to_period\\n   DatetimeIndex.to_pydatetime\\n   DatetimeIndex.to_series\\n   DatetimeIndex.to_frame\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n    :toctree: api/\\n\\n    DatetimeIndex.mean\\n    DatetimeIndex.std\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n   :template: autosummary/class_without_autosummary.rst\\n\\n   TimedeltaIndex\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   TimedeltaIndex.days\\n   TimedeltaIndex.seconds\\n   TimedeltaIndex.microseconds\\n   TimedeltaIndex.nanoseconds\\n   TimedeltaIndex.components\\n   TimedeltaIndex.inferred_freq\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   TimedeltaIndex.as_unit\\n   TimedeltaIndex.to_pytimedelta\\n   TimedeltaIndex.to_series\\n   TimedeltaIndex.round\\n   TimedeltaIndex.floor\\n   TimedeltaIndex.ceil\\n   TimedeltaIndex.to_frame\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n    :toctree: api/\\n\\n    TimedeltaIndex.mean\\n\n```\n\n----------------------------------------\n\nTITLE: Defining Integration Functions in Plain Cython - Cython\nDESCRIPTION: Copies the pure Python integration logic to Cython using the IPython Cython cell magic. Defines 'f_plain' and 'integrate_f_plain' without type annotations. Requires the %load_ext Cython IPython extension. Accepts and returns standard Python numeric types. Provides moderate speedup via Cython compilation but lacks further C-typed optimization.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_3\n\nLANGUAGE: cython\nCODE:\n```\ndef f_plain(x):\n    return x * (x - 1)\ndef integrate_f_plain(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f_plain(a + i * dx)\n    return s * dx\n```\n\n----------------------------------------\n\nTITLE: Running asv Benchmarks in the Existing Python Environment (Bash)\nDESCRIPTION: Runs the full asv benchmark suite in-place using the locally available pandas and environment, bypassing conda/virtualenv creation. Useful for developers working with in-place builds or nonstandard environments. '-e' runs in-place, '-E existing' tells asv to use the environment as-is.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\nasv run -e -E existing\n```\n\n----------------------------------------\n\nTITLE: Citing pandas Software via BibTeX Entry - BibTeX\nDESCRIPTION: This BibTeX snippet provides the correct format for citing the pandas software package in academic publications. It references the pandas software as archived on Zenodo and specifies the required fields such as author, title, version, DOI, and URL. To use, include this entry in your bibliography file; replace 'latest' with the used version if needed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/citing.md#_snippet_0\n\nLANGUAGE: BibTeX\nCODE:\n```\n@software{reback2020pandas,\n    author       = {The pandas development team},\n    title        = {pandas-dev/pandas: Pandas},\n    month        = feb,\n    year         = 2020,\n    publisher    = {Zenodo},\n    version      = {latest},\n    doi          = {10.5281/zenodo.3509134},\n    url          = {https://doi.org/10.5281/zenodo.3509134}\n}\n```\n\n----------------------------------------\n\nTITLE: Subtraction between PeriodIndex and Period returns Index of DateOffset (pandas >=0.24, Python)\nDESCRIPTION: Shows that in pandas 0.24+, subtracting a Period from a PeriodIndex now returns an Index of DateOffset objects for greater flexibility and correctness. Inputs: PeriodIndex, Period. Output: Index of DateOffset.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\npi = pd.period_range('June 2018', freq='M', periods=3)\npi - pi[0]\n```\n\n----------------------------------------\n\nTITLE: Table Method Weighted Mean Calculation\nDESCRIPTION: Demonstrates using table method with numba engine to calculate weighted means across multiple columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef weighted_mean(x):\n    arr = np.ones((1, x.shape[1]))\n    arr[:, :2] = (x[:, :2] * x[:, 2]).sum(axis=0) / x[:, 2].sum()\n    return arr\n\ndf = pd.DataFrame([[1, 2, 0.6], [2, 3, 0.4], [3, 4, 0.2], [4, 5, 0.7]])\ndf.rolling(2, method=\"table\", min_periods=0).apply(weighted_mean, raw=True, engine=\"numba\")\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Index.intersection order preservation in Pandas\nDESCRIPTION: This snippet shows how Index.intersection now preserves the order of the calling Index (left) rather than the second Index (right), which is a breaking change in Pandas 0.20.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_38\n\nLANGUAGE: ipython\nCODE:\n```\nleft = pd.Index([2, 1, 0])\nleft\nright = pd.Index([1, 2, 3])\nright\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: left.intersection(right)\nOut[4]: Int64Index([1, 2], dtype='int64')\n```\n\nLANGUAGE: ipython\nCODE:\n```\nleft.intersection(right)\n```\n\n----------------------------------------\n\nTITLE: Transforming Series with Single Function - pandas - Python\nDESCRIPTION: Applies a single function (NumPy abs) to a Series via .transform(), producing a transformed Series of identical shape and index. Inputs: Series and function; Output: transformed Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ntsdf[\"A\"].transform(np.abs)\n```\n\n----------------------------------------\n\nTITLE: Concatenating and Merging Categoricals with pd.concat and union_categoricals - Pandas Python\nDESCRIPTION: Covers various methods for combining Series and categoricals, highlighting behavior where categories are aligned or differ. Shows results of pd.concat (and resulting dtype), as well as using pandas.api.types.union_categoricals to forcibly combine categories into a union. Outlines consequences for type and memory usage. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import union_categoricals\n\n# same categories\ns1 = pd.Series([\"a\", \"b\"], dtype=\"category\")\ns2 = pd.Series([\"a\", \"b\", \"a\"], dtype=\"category\")\npd.concat([s1, s2])\n\n# different categories\ns3 = pd.Series([\"b\", \"c\"], dtype=\"category\")\npd.concat([s1, s3])\n\n# Output dtype is inferred based on categories values\nint_cats = pd.Series([1, 2], dtype=\"category\")\nfloat_cats = pd.Series([3.0, 4.0], dtype=\"category\")\npd.concat([int_cats, float_cats])\n\npd.concat([s1, s3]).astype(\"category\")\nunion_categoricals([s1.array, s3.array])\n```\n\n----------------------------------------\n\nTITLE: Serializing and Roundtripping tz-naive Datetimes with to_json in pandas - Python\nDESCRIPTION: These snippets show how pandas.Series with tz-naive DatetimeIndex are serialized to JSON with the to_json method (using date_format='iso') and deserialized back with read_json. The roundtripping behavior is tested, showing that earlier pandas versions would not roundtrip tz-naive indexes correctly, whereas newer versions preserve the original index. Requires: pandas, io.StringIO. Inputs are Series with DatetimeIndex; outputs are JSON strings and Series objects; behavior with UTC handling is version-dependent.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_10\n\nLANGUAGE: ipython\nCODE:\n```\nIn [32]: index = pd.date_range(\n   ....:     start='2020-12-28 00:00:00',\n   ....:     end='2020-12-28 02:00:00',\n   ....:     freq='1H',\n   ....: )\n   ....:\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [33]: a = pd.Series(\n   ....:     data=range(3),\n   ....:     index=index,\n   ....: )\n   ....:\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: from io import StringIO\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [5]: a.to_json(date_format='iso')\nOut[5]: '{\"2020-12-28T00:00:00.000Z\":0,\"2020-12-28T01:00:00.000Z\":1,\"2020-12-28T02:00:00.000Z\":2}'\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [6]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ=\"series\").index == a.index\nOut[6]: array([False, False, False])\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [34]: from io import StringIO\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [35]: a.to_json(date_format='iso')\nOut[35]: '{\"2020-12-28T00:00:00.000Z\":0,\"2020-12-28T01:00:00.000Z\":1,\"2020-12-28T02:00:00.000Z\":2}'\n```\n\nLANGUAGE: ipython\nCODE:\n```\n# Roundtripping now works\nIn [36]: pd.read_json(StringIO(a.to_json(date_format='iso')), typ=\"series\").index == a.index\nOut[36]: array([ True,  True,  True])\n```\n\n----------------------------------------\n\nTITLE: Handling Invalid bool as header in pandas read_csv (Python)\nDESCRIPTION: This snippet illustrates the new error-raising behavior when passing a boolean to the 'header' parameter in 'pd.read_csv'. Previously, a boolean input would be coerced; now, a TypeError is raised with an explanatory message. Dependencies: pandas. The intended input is a CSV filename and 'header' argument. Improper input of a boolean (True/False) results in an exception to inform correct usage of None, int, or list of ints.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv('data.csv', header=False)\nTypeError: Passing a bool to header is invalid. Use header=None for no header or\nheader=int or list-like of ints to specify the row(s) making up the column names\n```\n\n----------------------------------------\n\nTITLE: Evaluating Expressions in pandas\nDESCRIPTION: Shows how to evaluate expressions using the eval method in pandas, similar to R's with function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": np.random.randn(10), \"b\": np.random.randn(10)})\ndf.eval(\"a + b\")\ndf[\"a\"] + df[\"b\"]  # same as the previous expression\n```\n\n----------------------------------------\n\nTITLE: Float Slice Assignment Alters Dtype - pandas - Python\nDESCRIPTION: Shows change in dtype behavior when a float is assigned into an integer-typed DataFrame slice; the dtype switches to float rather than remaining integer, even if downcasting would not lose information. Requires pandas and numpy, inputs are 2D numpy array for DataFrame, multi-index, and float array for assignment. Outputs are DataFrame display and value changes after assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.array(range(1,10)).reshape(3,3),\n                  columns=list('abc'),\n                  index=[[4,4,8], [8,10,12]])\ndf\ndf.loc[4, 'c'] = np.array([0., 1.])\ndf\n```\n\n----------------------------------------\n\nTITLE: Using !important CSS to Override Specificity in pandas DataFrame Styling (Python)\nDESCRIPTION: This snippet showcases the use of the '!important' flag in a CSS property string returned by a Styler map, to forcibly override any other rules regardless of specificity. Requires pandas, and uses the same DataFrame, styles, and class assignment as previous examples. The output will have green text for the cell due to the '!important' override.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndf4.style.set_uuid(\"d_\").set_table_styles(\n    [\n        {\"selector\": \"td\", \"props\": \"color:red;\"},\n        {\"selector\": \".cls-1\", \"props\": \"color:blue;\"},\n        {\"selector\": \"td.data\", \"props\": \"color:yellow;\"},\n    ]\n).map(lambda x: \"color:green !important;\").set_td_classes(pd.DataFrame([[\"cls-1\"]]))\n```\n\n----------------------------------------\n\nTITLE: DataFrame.to_dict with non-unique index raises ValueError (pandas >=0.24, Python)\nDESCRIPTION: Demonstrates how DataFrame.to_dict(orient='index') now raises ValueError on non-unique indexes instead of silently losing data. Assumes pandas is imported as pd, shows both construction and effect. Inputs: DataFrame with duplicate indices; output: ValueError exception.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': [1, 2], 'b': [0.5, 0.75]}, index=['A', 'A'])\ndf\n\ndf.to_dict(orient='index')\n```\n\n----------------------------------------\n\nTITLE: Division Behavior with Pandas Series\nDESCRIPTION: Shows the new behavior of division operations involving a Series, where 0/0 and 0//0 now give np.nan instead of np.inf.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\np = pd.Series([0, 1])\np / 0\np // 0\n```\n\n----------------------------------------\n\nTITLE: Specifying dtypes in read_fwf with pandas - Python\nDESCRIPTION: Shows how to specify data types for columns when reading fixed-width text files using read_fwf and dtype keyword argument. Demonstrates default dtype inference versus explicit dtype assignment per column. Input is a StringIO object; output is the dtype result for each column. Requires pandas and StringIO.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a  b\\n1  2\\n3  4\"\npd.read_fwf(StringIO(data)).dtypes\npd.read_fwf(StringIO(data), dtype={'a': 'float64', 'b': 'object'}).dtypes\n```\n\n----------------------------------------\n\nTITLE: Safe Mutation of Python Lists During Iteration - Python\nDESCRIPTION: Shows two patterns: unsafe mutation of a list during iteration (which may result in unexpected behavior), and safe mutation using a copied list. Intended to illustrate why you should not mutate a container during iteration, especially relevant for UDFs in pandas. Requires only built-in Python. Outputs the resulting mutated list.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/gotchas.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvalues = [0, 1, 2, 3, 4, 5]\nn_removed = 0\nfor k, value in enumerate(values):\n    idx = k - n_removed\n    if value % 2 == 1:\n        del values[idx]\n        n_removed += 1\n    else:\n        values[idx] = value + 1\nvalues\n```\n\nLANGUAGE: python\nCODE:\n```\nvalues = [0, 1, 2, 3, 4, 5]\nn_removed = 0\nfor k, value in enumerate(values.copy()):\n    idx = k - n_removed\n    if value % 2 == 1:\n        del values[idx]\n        n_removed += 1\n    else:\n        values[idx] = value + 1\nvalues\n```\n\n----------------------------------------\n\nTITLE: Using Where Queries with Fixed Format HDF5 (TypeError Example)\nDESCRIPTION: Demonstrates that fixed format HDF5 files do not support queries with the 'where' parameter. This example shows the TypeError that occurs when attempting to use 'where' with a fixed format store.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_169\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame(np.random.randn(10, 2)).to_hdf(\"test_fixed.h5\", key=\"df\")\npd.read_hdf(\"test_fixed.h5\", \"df\", where=\"index>5\")\n```\n\n----------------------------------------\n\nTITLE: Legacy behavior: Creating Tick DateOffset with normalize=True (pandas, Python)\nDESCRIPTION: Shows prior ability to create pandas Tick (such as Hour) with normalize=True and resultant inconsistent time addition and non-associativity. Demonstrates Timestamp and offset creation, then non-associativity upon chained additions. Inputs include Timestamp and Tick; outputs are Timestamp and boolean result.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: ts = pd.Timestamp('2018-06-11 18:01:14')\n\nIn [3]: ts\nOut[3]: Timestamp('2018-06-11 18:01:14')\n\nIn [4]: tic = pd.offsets.Hour(n=2, normalize=True)\n   ...:\n\nIn [5]: tic\nOut[5]: <2 * Hours>\n\nIn [6]: ts + tic\nOut[6]: Timestamp('2018-06-11 00:00:00')\n\nIn [7]: ts + tic + tic + tic == ts + (tic + tic + tic)\nOut[7]: False\n```\n\n----------------------------------------\n\nTITLE: Aggregating by Group with Pivot Table and groupby - pandas - Python\nDESCRIPTION: This Python code shows two methods for summarizing data like R's dcast: first, using pandas' pivot_table to sum 'Amount' for each 'Animal' and 'FeedType'; second, grouping by both columns and summing. Inputs should be a DataFrame with categorical and numerical columns; output is a summary table with group sums. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"Animal\": [\n            \"Animal1\",\n            \"Animal2\",\n            \"Animal3\",\n            \"Animal2\",\n            \"Animal1\",\n            \"Animal2\",\n            \"Animal3\",\n        ],\n        \"FeedType\": [\"A\", \"B\", \"A\", \"A\", \"B\", \"B\", \"A\"],\n        \"Amount\": [10, 7, 4, 2, 5, 6, 2],\n    }\n)\n\ndf.pivot_table(values=\"Amount\", index=\"Animal\", columns=\"FeedType\", aggfunc=\"sum\")\n```\n\nLANGUAGE: Python\nCODE:\n```\ndf.groupby([\"Animal\", \"FeedType\"])[\"Amount\"].sum()\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in read_csv for UnicodeDecodeError with memory mapping\nDESCRIPTION: Addresses an issue where read_csv was raising UnicodeDecodeError exception when memory_map=True.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nread_csv(file, memory_map=True)\n```\n\n----------------------------------------\n\nTITLE: Adding Calculated Column in Pandas (Equivalent to SQL SELECT with Calculation)\nDESCRIPTION: This snippet demonstrates how to add a calculated column to a pandas DataFrame using the assign method. It's equivalent to adding a calculated column in a SQL SELECT statement.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntips.assign(tip_rate=tips[\"tip\"] / tips[\"total_bill\"])\n```\n\n----------------------------------------\n\nTITLE: Autosummary for PeriodIndex and Properties - reStructuredText\nDESCRIPTION: This set of autosummary blocks declares the PeriodIndex core class and its comprehensive list of properties and methods for Sphinx-driven pandas documentation. Properties cover calendar and frequency fields, while methods include conversion and frequency adjustments. Sphinx, pandas, and correct RTT paths are prerequisites.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/indexing.rst#_snippet_4\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n   :template: autosummary/class_without_autosummary.rst\\n\\n   PeriodIndex\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n    :toctree: api/\\n\\n    PeriodIndex.day\\n    PeriodIndex.dayofweek\\n    PeriodIndex.day_of_week\\n    PeriodIndex.dayofyear\\n    PeriodIndex.day_of_year\\n    PeriodIndex.days_in_month\\n    PeriodIndex.daysinmonth\\n    PeriodIndex.end_time\\n    PeriodIndex.freq\\n    PeriodIndex.freqstr\\n    PeriodIndex.hour\\n    PeriodIndex.is_leap_year\\n    PeriodIndex.minute\\n    PeriodIndex.month\\n    PeriodIndex.quarter\\n    PeriodIndex.qyear\\n    PeriodIndex.second\\n    PeriodIndex.start_time\\n    PeriodIndex.week\\n    PeriodIndex.weekday\\n    PeriodIndex.weekofyear\\n    PeriodIndex.year\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n    :toctree: api/\\n\\n    PeriodIndex.asfreq\\n    PeriodIndex.strftime\\n    PeriodIndex.to_timestamp\\n    PeriodIndex.from_fields\\n    PeriodIndex.from_ordinals\\n\n```\n\n----------------------------------------\n\nTITLE: IntervalIndex.get_loc Previous vs. New Behavior - Pandas - Python\nDESCRIPTION: Compares previous and updated results for IntervalIndex.get_loc method. Old behavior allowed overlapping intervals to match multiple locations; the new behavior only allows exact matches, returning an integer location or raising KeyError. Requires creation of 'ii' as an IntervalIndex and pandas installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nIn [6]: ii.get_loc(pd.Interval(1, 5))\\nOut[6]: array([0, 1])\\n\\nIn [7]: ii.get_loc(pd.Interval(2, 6))\\nOut[7]: array([0, 1, 2])\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [6]: ii.get_loc(pd.Interval(1, 5))\\nOut[6]: 1\\n\\nIn [7]: ii.get_loc(pd.Interval(2, 6))\\n---------------------------------------------------------------------------\\nKeyError: Interval(2, 6, closed='right')\n```\n\n----------------------------------------\n\nTITLE: Boolean Filtering with NA Handling in Series - pandas (IPython)\nDESCRIPTION: Shows how to apply a boolean mask with explicit NA filtering when selecting data from a Series object. Assumes a pandas Series named series. Uses the == operator to create a mask, combines it with notnull() to ensure NA values are excluded, then uses the mask to index the Series. The result is a filtered Series containing only matching values. Proper NA handling prevents unexpected False/True values in boolean operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.3.rst#_snippet_4\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: mask = series == \"Steve\"\n\nIn [5]: series[mask & series.notnull()]\nOut[5]:\n0    Steve\nLength: 1, dtype: object\n```\n\n----------------------------------------\n\nTITLE: Creating N-Dimensional Panels in Pandas\nDESCRIPTION: Shows how to create a 4-dimensional panel (Panel4D) in Pandas using random data. This demonstrates experimental support for n-dimensional named panels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\np4d = Panel4D(np.random.randn(2, 2, 5, 4),\n      labels=['Label1','Label2'],\n      items=['Item1', 'Item2'],\n      major_axis=date_range('1/1/2000', periods=5),\n      minor_axis=['A', 'B', 'C', 'D'])\n```\n\n----------------------------------------\n\nTITLE: Validating Docstrings Script Usage\nDESCRIPTION: Shows how to use the validation script to check docstring formatting and examples for a specific pandas method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_documentation.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython scripts/validate_docstrings.py pandas.DataFrame.mean\n```\n\n----------------------------------------\n\nTITLE: Series indexing with sequences and slices in Python\nDESCRIPTION: Examples demonstrating Series indexing with sequences of labels and label slices, which behaves similarly to the ix method except with integer indexes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nIn [8]: s = pd.Series(np.random.randn(6), index=list('acegkm'))\n\nIn [9]: s\nOut[9]:\na   -1.206412\nc    2.565646\ne    1.431256\ng    1.340309\nk   -1.170299\nm   -0.226169\nLength: 6, dtype: float64\n\nIn [10]: s[['m', 'a', 'c', 'e']]\nOut[10]:\nm   -0.226169\na   -1.206412\nc    2.565646\ne    1.431256\nLength: 4, dtype: float64\n\nIn [11]: s['b':'l']\nOut[11]:\nc    2.565646\ne    1.431256\ng    1.340309\nk   -1.170299\nLength: 4, dtype: float64\n\nIn [12]: s['c':'k']\nOut[12]:\nc    2.565646\ne    1.431256\ng    1.340309\nk   -1.170299\nLength: 4, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Error Handling with Out of Bounds Indexers\nDESCRIPTION: Demonstrates how .iloc raises IndexError when a single out-of-bounds index or a list containing any out-of-bounds index is used.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndfl.iloc[[4, 5, 6]]\n```\n\n----------------------------------------\n\nTITLE: Using String and Datetime Accessors with pandas Series of Categorical Type (Python)\nDESCRIPTION: These snippets illustrate how to apply string (str) and datetime (dt) accessors on pandas Series objects with type 'category', provided the underlying data supports the operation. The first demo uses 'str.contains' to operate on a categorical string series, while the second uses 'dt.day' to extract day components from a categorical datetime series. Requires pandas and numpy/date_range, and demonstrates interoperability improvements for Series of categorical dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.1.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(list(\"aabb\")).astype(\"category\")\ns\ns.str.contains(\"a\")\n\ndate = pd.Series(pd.date_range(\"1/1/2015\", periods=5)).astype(\"category\")\ndate\ndate.dt.day\n```\n\n----------------------------------------\n\nTITLE: Presenting String dtype Specification Mapping Table - Markdown\nDESCRIPTION: This markdown snippet provides a structured table mapping user-provided dtype specifications to the underlying pandas \"StringDtype\" instantiations, their string aliases, and explanations or version notes. The table is useful for documentation or references within the pandas codebase and related developer communications. Inputs are assumed to be markdown-rendered content; outputs are a formatted table—no computational function, but an essential guide for feature development and support for dtype options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0014-string-dtype.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| User specification                          | Concrete dtype                                                | String alias                          | Note     |\n|---------------------------------------------|---------------------------------------------------------------|---------------------------------------|----------|\n| Unspecified (inference)                     | `StringDtype(storage=\"pyarrow\"\\|\"python\", na_value=np.nan)`   | \"str\"                                 | (1)      |\n| \"str\" or `StringDtype(na_value=np.nan)`   | `StringDtype(storage=\"pyarrow\"\\|\"python\", na_value=np.nan)`   | \"str\"                                 | (1)      |\n| `StringDtype(\"pyarrow\", na_value=np.nan)`   | `StringDtype(storage=\"pyarrow\", na_value=np.nan)`             | \"str\"                                 |          |\n| `StringDtype(\"python\", na_value=np.nan)`    | `StringDtype(storage=\"python\", na_value=np.nan)`              | \"str\"                                 |          |\n| `StringDtype(\"pyarrow\")`                    | `StringDtype(storage=\"pyarrow\", na_value=pd.NA)`              | \"string[pyarrow]\"                     |          |\n| `StringDtype(\"python\")`                     | `StringDtype(storage=\"python\", na_value=pd.NA)`               | \"string[python]\"                      |          |\n| \"string\" or `StringDtype()`               | `StringDtype(storage=\"pyarrow\"\\|\"python\", na_value=pd.NA)`    | \"string[pyarrow]\" or \"string[python]\" | (1)      |\n| `StringDtype(\"pyarrow_numpy\")`              | `StringDtype(storage=\"pyarrow\", na_value=np.nan)`             | \"string[pyarrow_numpy]\"               | (2)      |\n```\n\n----------------------------------------\n\nTITLE: Uninstalling pandas via pip in Shell\nDESCRIPTION: Removes the pandas package from the current Python environment using pip. The '-y' flag automatically confirms uninstallation. pip must be installed and accessible. Useful before installing the development version or troubleshooting a broken installation. Produces output confirming the package removal.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip uninstall pandas -y\n```\n\n----------------------------------------\n\nTITLE: Docstring Output Examples\nDESCRIPTION: Demonstrates the resulting customized docstrings for Parent and Child classes after inheritance and substitution.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n>>> print(Parent.my_function.__doc__)\nApply my function to Parent.\n>>> print(ChildA.my_function.__doc__)\nApply my function to ChildA.\n>>> print(ChildB.my_function.__doc__)\nApply my function to ChildB.\n```\n\n----------------------------------------\n\nTITLE: Another Example of Out of Bounds Column Indexing\nDESCRIPTION: Shows that attempting to access a column index that is out of bounds with .iloc raises an IndexError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndfl.iloc[:, 4]\n```\n\n----------------------------------------\n\nTITLE: Getting the Type and Shape of a Multi-Column Selection in pandas (Python)\nDESCRIPTION: These snippets display how to use type() and .shape to identify the output of selecting multiple columns from a DataFrame. Selecting multiple columns returns a DataFrame, and .shape reveals the number of rows and columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntype(titanic[[\"Age\", \"Sex\"]])\n```\n\nLANGUAGE: python\nCODE:\n```\ntitanic[[\"Age\", \"Sex\"]].shape\n```\n\n----------------------------------------\n\nTITLE: Installing Clipboard Dependencies for Pandas\nDESCRIPTION: Command to install dependencies for clipboard operations in pandas, allowing data to be copied to and from the system clipboard.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pandas[clipboard]\"\n```\n\n----------------------------------------\n\nTITLE: Comparing Series with datetime.date Objects in Pandas\nDESCRIPTION: Demonstrates the behavior changes in comparing Series containing datetimes with datetime.date objects across Pandas versions 0.22.0, 0.23.0, and 0.23.1. Shows how coercion behavior and warnings have evolved.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# 0.22.0... Silently coerce the datetime.date\n>>> import datetime\n>>> pd.Series(pd.date_range('2017', periods=2)) == datetime.date(2017, 1, 1)\n0     True\n1    False\ndtype: bool\n\n# 0.23.0... Do not coerce the datetime.date\n>>> pd.Series(pd.date_range('2017', periods=2)) == datetime.date(2017, 1, 1)\n0    False\n1    False\ndtype: bool\n\n# 0.23.1... Coerce the datetime.date with a warning\n>>> pd.Series(pd.date_range('2017', periods=2)) == datetime.date(2017, 1, 1)\n/bin/python:1: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n'datetime.date' is coerced to a datetime. In the future pandas will\nnot coerce, and the values not compare equal to the 'datetime.date'.\nTo retain the current behavior, convert the 'datetime.date' to a\ndatetime with 'pd.Timestamp'.\n  #!/bin/python3\n0     True\n1    False\ndtype: bool\n```\n\n----------------------------------------\n\nTITLE: Assigning Array to DataFrame via iloc (Old Behavior) - pandas, numpy (Python)\nDESCRIPTION: Demonstrates deprecated behavior of assigning a NumPy array to a DataFrame column using iloc. This operation used to replace the selected column, but left the original Series reference (original_prices) unchanged. The code highlights the difference in dtypes and values before and after the assignment. Dependencies: pandas (pd), numpy (np).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: df.iloc[:, 0] = new_prices\nIn [4]: df.iloc[:, 0]\nOut[4]:\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: price, float: 64\n```\n\n----------------------------------------\n\nTITLE: Converting a DataFrame with Multiple Data Types to Reversible JSON using ntv_pandas in Python\nDESCRIPTION: This Python code demonstrates the creation of a pandas DataFrame with various data types and the use of ntv_pandas to serialize it to a compact, type-aware JSON format. It highlights ntv_pandas' ability to track extended types (like int32, date, point, string) to enhance reversibility. Dependencies include pandas, ntv_pandas, shapely, and Python's datetime module. The input is a typed pandas DataFrame, and the output is a JSON object that can be round-tripped without data loss.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0012-compact-and-reversible-JSON-interface.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: from shapely.geometry import Point\n        from datetime import date\n        import pandas as pd\n        import ntv_pandas as npd\n\nIn [2]: data = {'index':           [100, 200, 300, 400, 500, 600],\n                'dates::date':     [date(1964,1,1), date(1985,2,5), date(2022,1,21), date(1964,1,1), date(1985,2,5), date(2022,1,21)],\n                'value':           [10, 10, 20, 20, 30, 30],\n                'value32':         pd.Series([12, 12, 22, 22, 32, 32], dtype='int32'),\n                'res':             [10, 20, 30, 10, 20, 30],\n                'coord::point':    [Point(1,2), Point(3,4), Point(5,6), Point(7,8), Point(3,4), Point(5,6)],\n                'names':           pd.Series(['john', 'eric', 'judith', 'mila', 'hector', 'maria'], dtype='string'),\n                'unique':          True }\n\nIn [3]: df = pd.DataFrame(data).set_index('index')\n\nIn [4]: df\nOut[4]:       dates::date  value  value32  res coord::point   names  unique\n        index\n        100    1964-01-01     10       12   10  POINT (1 2)    john    True\n        200    1985-02-05     10       12   20  POINT (3 4)    eric    True\n        300    2022-01-21     20       22   30  POINT (5 6)  judith    True\n        400    1964-01-01     20       22   10  POINT (7 8)    mila    True\n        500    1985-02-05     30       32   20  POINT (3 4)  hector    True\n        600    2022-01-21     30       32   30  POINT (5 6)   maria    True\n```\n\n----------------------------------------\n\nTITLE: Creating a Panel and Converting to MultiIndex DataFrame\nDESCRIPTION: Example demonstrating how to create a Panel (which is now deprecated) and convert it to a MultiIndex DataFrame using the to_frame() method, which is the recommended approach for handling 3D data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_19\n\nLANGUAGE: ipython\nCODE:\n```\nimport pandas._testing as tm\n\np = tm.makePanel()\n\np\n```\n\nLANGUAGE: ipython\nCODE:\n```\np.to_frame()\n```\n\n----------------------------------------\n\nTITLE: Resample DataFrame with group_keys Control\nDESCRIPTION: Shows how to use the new group_keys parameter in DataFrame.resample() to control index behavior when applying functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {'a': range(6)},\n    index=pd.date_range(\"2021-01-01\", periods=6, freq=\"8H\")\n)\n\ndf.resample(\"D\", group_keys=True).apply(lambda x: x)\n\ndf.resample(\"D\", group_keys=False).apply(lambda x: x)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame and Series for Orient Examples - pandas - Python\nDESCRIPTION: Defines a DataFrame and Series with designated structure for demonstrating various JSON orientation options in pandas. The DataFrame uses specific index and column labels, and the Series is named. No output or serialization is performed in this code, but it sets up context for later examples. Requires pandas. Inputs: column values and Series dictionary.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_67\n\nLANGUAGE: python\nCODE:\n```\ndfjo = pd.DataFrame(\n    dict(A=range(1, 4), B=range(4, 7), C=range(7, 10)),\n    columns=list(\"ABC\"),\n    index=list(\"xyz\"),\n)\ndfjo\nsjo = pd.Series(dict(x=15, y=16, z=17), name=\"D\")\nsjo\n```\n\n----------------------------------------\n\nTITLE: Limiting Number of Splits using n Parameter (Python)\nDESCRIPTION: Illustrates limiting split operations to a single division using n=1 parameter in .str.split() while expanding, resulting in two columns. Useful for scenarios where only the first occurrence should be split. Input is string Series; output is two-column DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ns2.str.split(\"_\", expand=True, n=1)\n```\n\n----------------------------------------\n\nTITLE: Nullable Integer Dtype Inference in pd.array and pd.Series (pandas, Python)\nDESCRIPTION: Examines dtype inference differences between pd.array and pd.Series, highlighting that pd.array infers nullable integer dtype when encountering None, while Series may revert to standard integer or float for backwards compatibility. Explicit dtype specification is recommended for consistency. Requires only pandas imported as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.array([1, None])\npd.array([1, 2])\n```\n\n----------------------------------------\n\nTITLE: Storing IntegerArray in a pandas Series (Python)\nDESCRIPTION: Demonstrates how to construct a pandas Series directly from a previously created IntegerArray. This preserves the nullable integer dtype for downstream operations. It requires an IntegerArray (arr) and pandas (pd); the output is a Series with nullable integers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.Series(arr)\n```\n\n----------------------------------------\n\nTITLE: Installing pandas with conda (Shell)\nDESCRIPTION: This snippet demonstrates how to install the latest version of pandas in a Python environment using the conda package manager with the conda-forge channel. The command pulls binary distributions and resolves all dependencies automatically. Input is a shell command; output is that pandas and its dependencies are installed in the active conda environment. Requires conda (e.g., Miniconda or Anaconda) to be installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# conda\\nconda install -c conda-forge pandas\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with Numeric Categorical Columns Using CategoricalDtype in Pandas\nDESCRIPTION: Demonstrates using CategoricalDtype with read_csv to automatically convert categorical values to the correct type (integers in this case) rather than strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndtype = {'B': CategoricalDtype([1, 2, 3])}\npd.read_csv(StringIO(data), dtype=dtype).B.cat.categories\n```\n\n----------------------------------------\n\nTITLE: Preserving Blank Lines in CSV Parsing with skip_blank_lines\nDESCRIPTION: Shows how setting skip_blank_lines=False preserves blank lines in the parsed result instead of ignoring them, which is the default behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndata = \"a,b,c\\n\\n1,2,3\\n\\n\\n4,5,6\"\npd.read_csv(StringIO(data), skip_blank_lines=False)\n```\n\n----------------------------------------\n\nTITLE: Converting a DataFrame with Table Schema Data Types to JSON Table Schema with ntv_pandas in Python\nDESCRIPTION: This Python code illustrates the conversion of a pandas DataFrame that uses Table Schema-annotated columns to a JSON Table Schema using the ntv_pandas library. Each column is typed (e.g., date, geopoint, email), and the resulting JSON includes a schema with field definitions and data records. It requires pandas and ntv_pandas (and optionally shapely). The inputs are a typed DataFrame; the JSON output follows the Table Schema spec and can be used for interoperability.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0012-compact-and-reversible-JSON-interface.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: from shapely.geometry import Point\n        from datetime import date\n\nIn [2]: df = pd.DataFrame({\n            'end february::date': ['date(2023,2,28)', 'date(2024,2,29)', 'date(2025,2,28)'],\n            'coordinates::point': ['Point([2.3, 48.9])', 'Point([5.4, 43.3])', 'Point([4.9, 45.8])'],\n            'contact::email':     ['john.doe@table.com', 'lisa.minelli@schema.com', 'walter.white@breaking.com']\n            })\n\nIn [3]: df\nOut[3]: end february::date coordinates::point             contact::email\n        0         2023-02-28   POINT (2.3 48.9)         john.doe@table.com\n        1         2024-02-29   POINT (5.4 43.3)    lisa.minelli@schema.com\n        2         2025-02-28   POINT (4.9 45.8)  walter.white@breaking.com\n```\n\n----------------------------------------\n\nTITLE: Ranking with tie-handling using pandas rank(method='min') - Python\nDESCRIPTION: Calculates group-wise ranks in the 'tips' DataFrame for entries with 'tip' less than 2, using the 'min' ranking method, and annotates them with a new column. This mimics the ranking behavior of SQL's RANK(), where identical values are given the lowest rank in their tie group. Requires pandas and a 'tips' DataFrame with 'sex' and 'tip'. Returns filtered and sorted data, showing that identical tips receive the same minimum rank.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n(\n    tips[tips[\"tip\"] < 2]\n    .assign(rnk_min=tips.groupby([\"sex\"])[\"tip\"].rank(method=\"min\"))\n    .query(\"rnk_min < 3\")\n    .sort_values([\"sex\", \"rnk_min\"])\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Sphinx Autosummary for pandas Index Objects - reStructuredText\nDESCRIPTION: This snippet uses the Sphinx '.. autosummary::' directive to enumerate available methods and properties for the pandas Index class. It organizes functionality by topic such as properties, computation, conversion, and set operations, facilitating automated API reference generation for the pandas documentation. This format requires Sphinx and the pandas Python package, and expects the documented objects (e.g., 'Index', 'Index.values', etc.) to be importable members from pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/indexing.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index.values\\n   Index.is_monotonic_increasing\\n   Index.is_monotonic_decreasing\\n   Index.is_unique\\n   Index.has_duplicates\\n   Index.hasnans\\n   Index.dtype\\n   Index.inferred_type\\n   Index.shape\\n   Index.name\\n   Index.names\\n   Index.nbytes\\n   Index.ndim\\n   Index.size\\n   Index.empty\\n   Index.T\\n   Index.memory_usage\\n   Index.array\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index.all\\n   Index.any\\n   Index.argmin\\n   Index.argmax\\n   Index.copy\\n   Index.delete\\n   Index.drop\\n   Index.drop_duplicates\\n   Index.duplicated\\n   Index.equals\\n   Index.factorize\\n   Index.identical\\n   Index.insert\\n   Index.is_\\n   Index.min\\n   Index.max\\n   Index.reindex\\n   Index.rename\\n   Index.repeat\\n   Index.where\\n   Index.take\\n   Index.putmask\\n   Index.unique\\n   Index.nunique\\n   Index.value_counts\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index.set_names\\n   Index.droplevel\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index.fillna\\n   Index.dropna\\n   Index.isna\\n   Index.notna\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index.astype\\n   Index.item\\n   Index.map\\n   Index.ravel\\n   Index.to_list\\n   Index.to_series\\n   Index.to_frame\\n   Index.to_numpy\\n   Index.view\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index.argsort\\n   Index.searchsorted\\n   Index.sort_values\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index.shift\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index.append\\n   Index.join\\n   Index.intersection\\n   Index.union\\n   Index.difference\\n   Index.symmetric_difference\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n   Index.asof\\n   Index.asof_locs\\n   Index.get_indexer\\n   Index.get_indexer_for\\n   Index.get_indexer_non_unique\\n   Index.get_level_values\\n   Index.get_loc\\n   Index.get_slice_bound\\n   Index.isin\\n   Index.slice_indexer\\n   Index.slice_locs\\n\n```\n\n----------------------------------------\n\nTITLE: Creating a SparseSeries from a scipy COO Matrix (Python)\nDESCRIPTION: Explains how to create a pandas SparseSeries from a scipy.sparse coo_matrix instance using the from_coo convenience method. Dependencies: pandas, scipy.sparse. Inputs: a coo_matrix (constructed from data, row, and column arrays with specified shape). Output: a SparseSeries representing the non-zero elements of the matrix. This is efficient for working with large, sparse data. Note: The SparseSeries API may differ compared to later pandas versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy import sparse\nA = sparse.coo_matrix(([3.0, 1.0, 2.0], ([1, 0, 0], [0, 2, 3])),\n                      shape=(3, 4))\nA\nA.todense()\n\nss = pd.SparseSeries.from_coo(A)\nss\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame with MultiIndex Rows and Columns and Saving to CSV - Pandas - Python\nDESCRIPTION: Creates a DataFrame with MultiIndex for both rows and columns using 'pd.MultiIndex.from_arrays', assigns row and column names, and writes the DataFrame to a CSV file. Demonstrates compatibility and behavior for complex DataFrame structures with hierarchical indexing. Requires pandas, numpy, and appropriate CSV file permissions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmi_idx = pd.MultiIndex.from_arrays([[1, 2, 3, 4], list(\"abcd\")], names=list(\"ab\"))\nmi_col = pd.MultiIndex.from_arrays([[1, 2], list(\"ab\")], names=list(\"cd\"))\ndf = pd.DataFrame(np.ones((4, 2)), index=mi_idx, columns=mi_col)\ndf.to_csv(\"mi.csv\")\n```\n\n----------------------------------------\n\nTITLE: Preserving dtypes in DataFrame.combine_first - Pandas Python\nDESCRIPTION: This code snippet illustrates how DataFrame.combine_first now preserves column dtypes when combining DataFrames. It initializes two DataFrames with integer columns, invokes combine_first, and displays the types before and after the operation. Requires pandas; inputs are two DataFrames. Outputs: a combined DataFrame and its dtypes before and after the change. Demonstrates that dtypes no longer get upcast to float64 in the updated version.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [1, 2, 3]}, index=[0, 1, 2])\ndf1\ndf2 = pd.DataFrame({\"B\": [4, 5, 6], \"C\": [1, 2, 3]}, index=[2, 3, 4])\ndf2\ncombined = df1.combine_first(df2)\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: combined.dtypes\nOut[2]:\nA    float64\nB    float64\nC    float64\ndtype: object\n```\n\nLANGUAGE: python\nCODE:\n```\ncombined.dtypes\n```\n\n----------------------------------------\n\nTITLE: NumPy Ufuncs with NA using pandas (Python)\nDESCRIPTION: These samples show how pandas' NA interacts with NumPy universal functions (ufuncs), such as np.log, np.add, and np.greater. Most ufuncs return NA when presented with NA input, but interactions with ndarray broadcast to object-dtype arrays of NA. These snippets depend on pandas and NumPy. Key parameters are the ufuncs and input arrays/scalars. Future changes may affect return types. Outputs reflect NA propagation in ufunc results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/missing_data.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnp.log(pd.NA)\nnp.add(pd.NA, 1)\n```\n\nLANGUAGE: python\nCODE:\n```\na = np.array([1, 2, 3])\nnp.greater(a, pd.NA)\n```\n\n----------------------------------------\n\nTITLE: Subtraction from timedelta64[ns] DataFrame column with NaN raises TypeError (pandas >=0.24, Python)\nDESCRIPTION: Exhibits updated behavior: subtracting np.nan from a DataFrame column of timedelta64[ns] now raises a TypeError to prevent silent errors and match Index/Series. Inputs: DataFrame with pd.Timedelta; output: TypeError exception.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: df - np.nan\n...\nTypeError: unsupported operand type(s) for -: 'TimedeltaIndex' and 'float'\n```\n\n----------------------------------------\n\nTITLE: Creating PandasArray for Standard Numeric Data with pandas.array - pandas (Python)\nDESCRIPTION: Shows that using pandas.array without specifying an extension type creates a PandasArray, a thin wrapper around a standard NumPy ndarray, suitable for use where extension array API compatibility is needed. Inputs are numeric lists; output is a PandasArray object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npd.array([1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Using Series.sparse accessor and density in pandas (Python)\nDESCRIPTION: Demonstrates the use of pandas Series with SparseDtype and accessing the sparse accessor's density method, which returns the proportion of non-fill values. Assumes pandas is imported as pd. Input: Series with Sparse[int] dtype. Output: float density value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([0, 0, 1, 1, 1], dtype='Sparse[int]')\ns.sparse.density\n```\n\n----------------------------------------\n\nTITLE: Querying Google BigQuery Data with Pandas in Python\nDESCRIPTION: Shows how to query data from Google BigQuery using pandas.io.gbq, process the results with Pandas, and reshape the data. Requires a Google BigQuery account and project ID.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.io import gbq\n\n# A query to select the average monthly temperatures in the\n# in the year 2000 across the USA. The dataset,\n# publicata:samples.gsod, is available on all BigQuery accounts,\n# and is based on NOAA gsod data.\n\nquery = \"\"\"SELECT station_number as STATION,\nmonth as MONTH, AVG(mean_temp) as MEAN_TEMP\nFROM publicdata:samples.gsod\nWHERE YEAR = 2000\nGROUP BY STATION, MONTH\nORDER BY STATION, MONTH ASC\"\"\"\n\n# Fetch the result set for this query\n\n# Your Google BigQuery Project ID\n# To find this, see your dashboard:\n# https://console.developers.google.com/iam-admin/projects?authuser=0\nprojectid = 'xxxxxxxxx'\ndf = gbq.read_gbq(query, project_id=projectid)\n\n# Use pandas to process and reshape the dataset\n\ndf2 = df.pivot(index='STATION', columns='MONTH', values='MEAN_TEMP')\ndf3 = pd.concat([df2.min(), df2.mean(), df2.max()],\n                axis=1, keys=[\"Min Tem\", \"Mean Temp\", \"Max Temp\"])\n```\n\n----------------------------------------\n\nTITLE: Other Functionality Fixes in Pandas 0.25.2\nDESCRIPTION: Compatibility fixes for Python 3.8 in DataFrame.query and improvements to IPython console tab-completion for deprecated attributes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.2.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.query()\n```\n\n----------------------------------------\n\nTITLE: Handling Empty Input in Series.value_counts for Int64 Dtype (Python)\nDESCRIPTION: Fixed an issue where Series.value_counts would raise an error on empty input of Int64 dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\npd.Series([], dtype='Int64').value_counts()\n```\n\n----------------------------------------\n\nTITLE: Installing XML Dependencies for Pandas\nDESCRIPTION: Command to install dependencies for XML parsing and serialization in pandas. This is required for read_xml and to_xml functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pandas[xml]\"\n```\n\n----------------------------------------\n\nTITLE: Plotting DataFrame Without Legend - pandas/matplotlib - Python\nDESCRIPTION: This snippet demonstrates plotting a pandas DataFrame without displaying the legend using the 'legend=False' argument. The output is a matplotlib line plot for all columns in 'df' with no legend box. Requires pandas, matplotlib, and a defined DataFrame 'df'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n@savefig frame_plot_basic_noleg.png\ndf.plot(legend=False);\n```\n\n----------------------------------------\n\nTITLE: Creating a Multi-Axis Panel with Named Axes in Pandas (python)\nDESCRIPTION: This Python snippet constructs a Panel (now deprecated in Pandas), explicitly naming axes for multi-dimensional indexing. Dependencies: pandas, numpy. Inputs: reshaped array, item names, major and minor axis names. Output: Panel object. Demonstrates complex data storage and axes labeling, useful for updating legacy Panel code.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n>>> p = pd.Panel(np.arange(2 * 3 * 4).reshape(2, 3, 4),\n...              items=['ItemA', 'ItemB'],\n...              major_axis=[1, 2, 3],\n...              minor_axis=['A', 'B', 'C', 'D'])\n>>> p\n<class 'pandas.core.panel.Panel'>\nDimensions: 2 (items) x 3 (major_axis) x 4 (minor_axis)\nItems axis: ItemA to ItemB\nMajor_axis axis: 1 to 3\nMinor_axis axis: A to D\n```\n\n----------------------------------------\n\nTITLE: Slicing a DataFrame with NoRowIndex (Python)\nDESCRIPTION: Provides an example of slicing a DataFrame that uses the proposed NoRowIndex, showing that the sliced result also uses NoRowIndex of the appropriate length. Demonstrates both valid and invalid ways to slice/index with NoRowIndex. Input: DataFrame with NoRowIndex; Output: Series or DataFrame with preserved or appropriate NoRowIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nIn [12]: df = pd.DataFrame({'a': [1,  2, 3], 'b': [4, 5, 6]}, index=NoRowIndex(3))\n\nIn [13]: df.loc[df['a']>1, 'b']\nOut[13]:\n5\n6\nName: b, dtype: int64\n\nIn [14]: df.loc[df['a']>1, 'b'].index\nOut[14]: NoRowIndex(len=2)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Current Flexible Parsing with Pandas to_datetime - ipython\nDESCRIPTION: This snippet showcases how pandas' to_datetime function interprets a list of ambiguous date strings. It highlights that pandas can silently infer different formats for each entry, potentially resulting in unexpected interpretations for the user (e.g., switching day and month). No explicit dependencies except for importing pandas as pd are required. Input is a list of date strings; output is a DatetimeIndex, potentially parsed inconsistently due to internal heuristics.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0004-consistent-to-datetime-parsing.md#_snippet_0\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: pd.to_datetime(['12-01-2000 00:00:00', '13-01-2000 00:00:00'])\nOut[1]: DatetimeIndex(['2000-12-01', '2000-01-13'], dtype='datetime64[ns]', freq=None)\n```\n\n----------------------------------------\n\nTITLE: Adding Tooltips to Cells\nDESCRIPTION: Implements tooltips for specific cells with custom content and styling using set_tooltips() method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntt = pd.DataFrame(\n    [\n        [\n            \"This model has a very strong true positive rate\",\n            \"This model's total number of false negatives is too high\",\n        ]\n    ],\n    index=[\"Tumour (Positive)\"],\n    columns=df.columns[[0, 3]],\n)\ns.set_tooltips(\n    tt,\n    props=\"visibility: hidden; position: absolute; z-index: 1; \"\n    \"border: 1px solid #000066;\"\n    \"background-color: white; color: #000066; font-size: 0.8em;\"\n    \"transform: translate(0px, -24px); padding: 0.6em; \"\n    \"border-radius: 0.5em;\",\n)\n```\n\n----------------------------------------\n\nTITLE: Conditional Column Assignment - Stata\nDESCRIPTION: Shows how to create a new column based on conditions with 'generate ... if' and then update it with 'replace ... if'. No dependencies. Input: Data with 'total_bill'. Output: New 'bucket' column assigned 'low' or 'high' according to condition.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_11\n\nLANGUAGE: stata\nCODE:\n```\ngenerate bucket = \"low\" if total_bill < 10\\nreplace bucket = \"high\" if total_bill >= 10\n```\n\n----------------------------------------\n\nTITLE: Parsing ISO8601 Strings with Specified Format in Pandas - ipython\nDESCRIPTION: This code snippet demonstrates parsing strings explicitly known to be in ISO8601 format by passing format='ISO8601' to pandas' to_datetime. This ensures strict and predictable conversion. Requires pandas as pd. Inputs are ISO8601-compliant string list and the format argument. Output is a DatetimeIndex with accurately parsed timestamps according to the ISO8601 standard.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0004-consistent-to-datetime-parsing.md#_snippet_3\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: pd.to_datetime(['2020-01-01', '2020-01-01 03:00'], format='ISO8601')\nOut[4]: DatetimeIndex(['2020-01-01 00:00:00', '2020-01-01 03:00:00'], dtype='datetime64[ns]', freq=None)\n```\n\n----------------------------------------\n\nTITLE: Melting a List into Data Frame - reshape2::melt - R\nDESCRIPTION: This R snippet converts a list containing numbers and NA into a melted (long-form) data.frame using melt. The function expands list indices and values into columns. Requires reshape2 for melt; input is a simple numeric list possibly containing NA.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_16\n\nLANGUAGE: R\nCODE:\n```\na <- as.list(c(1:4, NA))\ndata.frame(melt(a))\n```\n\n----------------------------------------\n\nTITLE: Generating and Plotting Group Means with Error Bars using pandas and Matplotlib in Python\nDESCRIPTION: This snippet demonstrates generating raw data with a pandas MultiIndex, grouping it to compute group means and standard deviations, and plotting these with error bars using Matplotlib. Required dependencies are pandas, matplotlib, and numpy; variables include a grouped DataFrame and computed mean and standard deviation DataFrames. Input is a DataFrame with hierarchical index; output is a bar plot with vertical error bars representing standard deviation. The error bars are symmetrical, corresponding to the group statistics.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\n# Generate the data\nix3 = pd.MultiIndex.from_arrays(\n    [\n        [\"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\"],\n        [\"foo\", \"foo\", \"foo\", \"bar\", \"bar\", \"foo\", \"foo\", \"bar\", \"bar\", \"bar\"],\n    ],\n    names=[\"letter\", \"word\"],\n)\n\ndf3 = pd.DataFrame(\n    {\n        \"data1\": [9, 3, 2, 4, 3, 2, 4, 6, 3, 2],\n        \"data2\": [9, 6, 5, 7, 5, 4, 5, 6, 5, 1],\n    },\n    index=ix3,\n)\n\n# Group by index labels and take the means and standard deviations\n# for each group\ngp3 = df3.groupby(level=(\"letter\", \"word\"))\nmeans = gp3.mean()\nerrors = gp3.std()\nmeans\nerrors\n\n# Plot\nfig, ax = plt.subplots()\nmeans.plot.bar(yerr=errors, ax=ax, capsize=4, rot=0);\n```\n\n----------------------------------------\n\nTITLE: GroupBy Filtering and Application - pandas - IPython\nDESCRIPTION: Shows usage of DataFrame groupby operations, specifically demonstrating the difference between head/filter-style and aggregation methods. The code defines a DataFrame, groups by column 'A', applies head(1) directly and via apply with a lambda. Illustrates how head(1) now filters while apply preserves group keys in the result. Output is provided to clarify data structure transformations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_7\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=['A', 'B'])\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: g = df.groupby('A')\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: g.head(1)  # filters DataFrame\\nOut[3]:\\n   A  B\\n0  1  2\\n2  5  6\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: g.apply(lambda x: x.head(1))  # used to simply fall-through\\nOut[4]:\\n     A  B\\nA        \\n1 0  1  2\\n5 2  5  6\n```\n\n----------------------------------------\n\nTITLE: Converting SparseDataFrame to SciPy COO Matrix\nDESCRIPTION: This code shows how to convert a pandas SparseDataFrame back to a scipy.sparse matrix in COO format using the new to_coo() method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nsdf.to_coo()\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: Assignment to Non-Existent DataFrame Columns - ipython\nDESCRIPTION: Demonstrates that previously, assigning a value to multiple DataFrame columns, where some columns did not exist, did not add new columns as expected. Instead, the value was incorrectly assigned only to existing columns or to the last column, potentially resulting in missing columns. Output is a DataFrame showing unintended results.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_19\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: df[['a', 'c']] = 1\\nIn [4]: df\\nOut[4]:\\n   a  b\\n0  1  1\\n1  1  1\\n2  1  1\n```\n\n----------------------------------------\n\nTITLE: Importing Jinja2, IPython, and pandas Styler for Template Customization (Python)\nDESCRIPTION: Prepares the environment by importing the necessary classes and modules for template customization and HTML rendering, including Jinja2 for templating, IPython.display for HTML output, and pandas Styler for data formatting. These imports are prerequisites for later examples involving Styler subclassing and custom template rendering.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nfrom jinja2 import Environment, ChoiceLoader, FileSystemLoader\nfrom IPython.display import HTML\nfrom pandas.io.formats.style import Styler\n```\n\n----------------------------------------\n\nTITLE: New Behavior: Assignment Creates New DataFrame Columns - python\nDESCRIPTION: Shows that after the pandas update, assigning a value to multiple DataFrame columns, including new ones, properly creates the new columns and assigns values. Demonstrates that DataFrame assignment now constructs missing columns as needed and sets their values. Inputs: DataFrame, columns, and assignment value.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndf[['a', 'c']] = 1\\ndf\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in DataFrame.replace\nDESCRIPTION: Corrects a regression in DataFrame.replace that casts columns to object dtype if items in to_replace are not in values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame.replace()\n```\n\n----------------------------------------\n\nTITLE: New Interval Range Implementation\nDESCRIPTION: Demonstrates the new interval_range behavior including the end point in the final interval.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\npd.interval_range(start=0, end=4)\n```\n\n----------------------------------------\n\nTITLE: Handling Unsupported Datatypes in DataFrame JSON Serialization - pandas - Python\nDESCRIPTION: Demonstrates pandas' fallback behavior during JSON serialization for unsupported dtypes (e.g., NumPy complex). Provides an example where attempting to serialize such a DataFrame raises an error. Suggests resolving this by passing default_handler=str to to_json, allowing unsupported values to be stringified instead of erroring. Inputs: DataFrame with complex numbers. Outputs: raises RuntimeError by default, or produces stringified JSON with default handler.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_72\n\nLANGUAGE: python\nCODE:\n```\n>>> DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json()  # raises\nRuntimeError: Unhandled numpy dtype 15\n```\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame([1.0, 2.0, complex(1.0, 2.0)]).to_json(default_handler=str)\n```\n\n----------------------------------------\n\nTITLE: Summarizing Offset Class APIs with Sphinx Autosummary - reStructuredText\nDESCRIPTION: This snippet demonstrates how to document classes, their properties, and methods in the pandas library using Sphinx's '.. autosummary::' directive in reStructuredText format. It lists the available properties and methods of each time-based offset class for automated API documentation generation. The '.. autosummary::' directives require Sphinx as a dependency and assume that the target Python classes and members are available and correctly referenced in the package API. Inputs are class and member names; output is an automated summary table with links for API docs. Limitations: Does not provide explanation or code logic, only documentation structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/offset_frequency.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    Week\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    Week.freqstr\\n    Week.kwds\\n    Week.name\\n    Week.nanos\\n    Week.normalize\\n    Week.rule_code\\n    Week.n\\n    Week.weekday\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    Week.copy\\n    Week.is_on_offset\\n    Week.is_month_start\\n    Week.is_month_end\\n    Week.is_quarter_start\\n    Week.is_quarter_end\\n    Week.is_year_start\\n    Week.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    WeekOfMonth\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    WeekOfMonth.freqstr\\n    WeekOfMonth.kwds\\n    WeekOfMonth.name\\n    WeekOfMonth.nanos\\n    WeekOfMonth.normalize\\n    WeekOfMonth.rule_code\\n    WeekOfMonth.n\\n    WeekOfMonth.week\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    WeekOfMonth.copy\\n    WeekOfMonth.is_on_offset\\n    WeekOfMonth.weekday\\n    WeekOfMonth.is_month_start\\n    WeekOfMonth.is_month_end\\n    WeekOfMonth.is_quarter_start\\n    WeekOfMonth.is_quarter_end\\n    WeekOfMonth.is_year_start\\n    WeekOfMonth.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    LastWeekOfMonth\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    LastWeekOfMonth.freqstr\\n    LastWeekOfMonth.kwds\\n    LastWeekOfMonth.name\\n    LastWeekOfMonth.nanos\\n    LastWeekOfMonth.normalize\\n    LastWeekOfMonth.rule_code\\n    LastWeekOfMonth.n\\n    LastWeekOfMonth.weekday\\n    LastWeekOfMonth.week\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    LastWeekOfMonth.copy\\n    LastWeekOfMonth.is_on_offset\\n    LastWeekOfMonth.is_month_start\\n    LastWeekOfMonth.is_month_end\\n    LastWeekOfMonth.is_quarter_start\\n    LastWeekOfMonth.is_quarter_end\\n    LastWeekOfMonth.is_year_start\\n    LastWeekOfMonth.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    BQuarterEnd\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BQuarterEnd.freqstr\\n    BQuarterEnd.kwds\\n    BQuarterEnd.name\\n    BQuarterEnd.nanos\\n    BQuarterEnd.normalize\\n    BQuarterEnd.rule_code\\n    BQuarterEnd.n\\n    BQuarterEnd.startingMonth\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BQuarterEnd.copy\\n    BQuarterEnd.is_on_offset\\n    BQuarterEnd.is_month_start\\n    BQuarterEnd.is_month_end\\n    BQuarterEnd.is_quarter_start\\n    BQuarterEnd.is_quarter_end\\n    BQuarterEnd.is_year_start\\n    BQuarterEnd.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    BQuarterBegin\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BQuarterBegin.freqstr\\n    BQuarterBegin.kwds\\n    BQuarterBegin.name\\n    BQuarterBegin.nanos\\n    BQuarterBegin.normalize\\n    BQuarterBegin.rule_code\\n    BQuarterBegin.n\\n    BQuarterBegin.startingMonth\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BQuarterBegin.copy\\n    BQuarterBegin.is_on_offset\\n    BQuarterBegin.is_month_start\\n    BQuarterBegin.is_month_end\\n    BQuarterBegin.is_quarter_start\\n    BQuarterBegin.is_quarter_end\\n    BQuarterBegin.is_year_start\\n    BQuarterBegin.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    QuarterEnd\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    QuarterEnd.freqstr\\n    QuarterEnd.kwds\\n    QuarterEnd.name\\n    QuarterEnd.nanos\\n    QuarterEnd.normalize\\n    QuarterEnd.rule_code\\n    QuarterEnd.n\\n    QuarterEnd.startingMonth\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    QuarterEnd.copy\\n    QuarterEnd.is_on_offset\\n    QuarterEnd.is_month_start\\n    QuarterEnd.is_month_end\\n    QuarterEnd.is_quarter_start\\n    QuarterEnd.is_quarter_end\\n    QuarterEnd.is_year_start\\n    QuarterEnd.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    QuarterBegin\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    QuarterBegin.freqstr\\n    QuarterBegin.kwds\\n    QuarterBegin.name\\n    QuarterBegin.nanos\\n    QuarterBegin.normalize\\n    QuarterBegin.rule_code\\n    QuarterBegin.n\\n    QuarterBegin.startingMonth\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    QuarterBegin.copy\\n    QuarterBegin.is_on_offset\\n    QuarterBegin.is_month_start\\n    QuarterBegin.is_month_end\\n    QuarterBegin.is_quarter_start\\n    QuarterBegin.is_quarter_end\\n    QuarterBegin.is_year_start\\n    QuarterBegin.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    BHalfYearEnd\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BHalfYearEnd.freqstr\\n    BHalfYearEnd.kwds\\n    BHalfYearEnd.name\\n    BHalfYearEnd.nanos\\n    BHalfYearEnd.normalize\\n    BHalfYearEnd.rule_code\\n    BHalfYearEnd.n\\n    BHalfYearEnd.startingMonth\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BHalfYearEnd.copy\\n    BHalfYearEnd.is_on_offset\\n    BHalfYearEnd.is_month_start\\n    BHalfYearEnd.is_month_end\\n    BHalfYearEnd.is_quarter_start\\n    BHalfYearEnd.is_quarter_end\\n    BHalfYearEnd.is_year_start\\n    BHalfYearEnd.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    BHalfYearBegin\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BHalfYearBegin.freqstr\\n    BHalfYearBegin.kwds\\n    BHalfYearBegin.name\\n    BHalfYearBegin.nanos\\n    BHalfYearBegin.normalize\\n    BHalfYearBegin.rule_code\\n    BHalfYearBegin.n\\n    BHalfYearBegin.startingMonth\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BHalfYearBegin.copy\\n    BHalfYearBegin.is_on_offset\\n    BHalfYearBegin.is_month_start\\n    BHalfYearBegin.is_month_end\\n    BHalfYearBegin.is_quarter_start\\n    BHalfYearBegin.is_quarter_end\\n    BHalfYearBegin.is_year_start\\n    BHalfYearBegin.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    HalfYearEnd\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    HalfYearEnd.freqstr\\n    HalfYearEnd.kwds\\n    HalfYearEnd.name\\n    HalfYearEnd.nanos\\n    HalfYearEnd.normalize\\n    HalfYearEnd.rule_code\\n    HalfYearEnd.n\\n    HalfYearEnd.startingMonth\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    HalfYearEnd.copy\\n    HalfYearEnd.is_on_offset\\n    HalfYearEnd.is_month_start\\n    HalfYearEnd.is_month_end\\n    HalfYearEnd.is_quarter_start\\n    HalfYearEnd.is_quarter_end\\n    HalfYearEnd.is_year_start\\n    HalfYearEnd.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    HalfYearBegin\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    HalfYearBegin.freqstr\\n    HalfYearBegin.kwds\\n    HalfYearBegin.name\\n    HalfYearBegin.nanos\\n    HalfYearBegin.normalize\\n    HalfYearBegin.rule_code\\n    HalfYearBegin.n\\n    HalfYearBegin.startingMonth\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    HalfYearBegin.copy\\n    HalfYearBegin.is_on_offset\\n    HalfYearBegin.is_month_start\\n    HalfYearBegin.is_month_end\\n    HalfYearBegin.is_quarter_start\\n    HalfYearBegin.is_quarter_end\\n    HalfYearBegin.is_year_start\\n    HalfYearBegin.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    BYearEnd\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BYearEnd.freqstr\\n    BYearEnd.kwds\\n    BYearEnd.name\\n    BYearEnd.nanos\\n    BYearEnd.normalize\\n    BYearEnd.rule_code\\n    BYearEnd.n\\n    BYearEnd.month\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BYearEnd.copy\\n    BYearEnd.is_on_offset\\n    BYearEnd.is_month_start\\n    BYearEnd.is_month_end\\n    BYearEnd.is_quarter_start\\n    BYearEnd.is_quarter_end\\n    BYearEnd.is_year_start\\n    BYearEnd.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    BYearBegin\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BYearBegin.freqstr\\n    BYearBegin.kwds\\n    BYearBegin.name\\n    BYearBegin.nanos\\n    BYearBegin.normalize\\n    BYearBegin.rule_code\\n    BYearBegin.n\\n    BYearBegin.month\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    BYearBegin.copy\\n    BYearBegin.is_on_offset\\n    BYearBegin.is_month_start\\n    BYearBegin.is_month_end\\n    BYearBegin.is_quarter_start\\n    BYearBegin.is_quarter_end\\n    BYearBegin.is_year_start\\n    BYearBegin.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    YearEnd\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    YearEnd.freqstr\\n    YearEnd.kwds\\n    YearEnd.name\\n    YearEnd.nanos\\n    YearEnd.normalize\\n    YearEnd.rule_code\\n    YearEnd.n\\n    YearEnd.month\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    YearEnd.copy\\n    YearEnd.is_on_offset\\n    YearEnd.is_month_start\\n    YearEnd.is_month_end\\n    YearEnd.is_quarter_start\\n    YearEnd.is_quarter_end\\n    YearEnd.is_year_start\\n    YearEnd.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    YearBegin\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    YearBegin.freqstr\\n    YearBegin.kwds\\n    YearBegin.name\\n    YearBegin.nanos\\n    YearBegin.normalize\\n    YearBegin.rule_code\\n    YearBegin.n\\n    YearBegin.month\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    YearBegin.copy\\n    YearBegin.is_on_offset\\n    YearBegin.is_month_start\\n    YearBegin.is_month_end\\n    YearBegin.is_quarter_start\\n    YearBegin.is_quarter_end\\n    YearBegin.is_year_start\\n    YearBegin.is_year_end\\n\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\\n   :toctree: api/\\n\\n    FY5253\\n\\nProperties\\n~~~~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    FY5253.freqstr\\n    FY5253.kwds\\n    FY5253.name\\n    FY5253.nanos\\n    FY5253.normalize\\n    FY5253.rule_code\\n    FY5253.n\\n    FY5253.startingMonth\\n    FY5253.variation\\n    FY5253.weekday\\n\\nMethods\\n~~~~~~~\\n.. autosummary::\\n   :toctree: api/\\n\\n    FY5253.copy\\n    FY5253.get_rule_code_suffix\\n    FY5253.get_year_end\\n    FY5253.is_on_offset\\n    FY5253.is_month_start\\n    FY5253.is_month_end\\n    FY5253.is_quarter_start\\n    FY5253.is_quarter_end\\n    FY5253.is_year_start\\n    FY5253.is_year_end\\n\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Datetime Columns with pandas - Python\nDESCRIPTION: This snippet removes the previously created datetime-related columns from the DataFrame to restore it to its original structure. It uses the pandas 'drop' method with a list of column names and specifies axis=1 to target columns. The dependency is pandas and an up-to-date 'tips' DataFrame. Inputs are the column names to drop, and the main output is an updated DataFrame with these columns removed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/time_date.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntips = tips.drop(\\n    [\"date1\", \"date2\", \"date1_year\", \"date2_month\", \"date1_next\", \"months_between\"],\\n    axis=1,\\n)\\n\n```\n\n----------------------------------------\n\nTITLE: Custom HTML Formatting with Escaping in Pandas Styler (Python)\nDESCRIPTION: This code snippet formats DataFrame values as HTML links, ensuring that the content is safely escaped before rendering. Requires pandas, an existing DataFrame (df4), and optional safe use of the format string. It accepts a format string and escape argument. The result displays each cell value inside an anchor tag, with special characters safely escaped for HTML.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndf4.style.format(\n    '<a href=\"https://pandas.pydata.org\" target=\"_blank\">{}</a>', escape=\"html\"\n)\n```\n\n----------------------------------------\n\nTITLE: MultiIndex Slicing with Slices and Lists - pandas Python\nDESCRIPTION: Showcases label-based MultiIndex slicing using loc, with a slice for the first index level, colon slice for the second, a label list for the third, and all columns selected. Utilizes the internal logic of label-based selection supported by MultiIndex, including lists and slices for powerful subsetting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[(slice('A1', 'A3'), slice(None), ['C1', 'C3']), :]\n```\n\n----------------------------------------\n\nTITLE: Filtering Data with String Pattern Matching\nDESCRIPTION: Demonstrates how to filter DataFrame rows based on string pattern matching. The example finds passengers with 'Countess' in their names using the str.contains method combined with boolean indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/10_text_data.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntitanic[\"Name\"].str.contains(\"Countess\")\n```\n\nLANGUAGE: python\nCODE:\n```\ntitanic[titanic[\"Name\"].str.contains(\"Countess\")]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Datetime Resolution Inference in Pandas 3.0\nDESCRIPTION: This code snippet shows how Pandas 3.0 infers the appropriate resolution (unit) for datetime objects when converting to datetime64 dtype. It demonstrates the new behavior for pydatetime objects, which now infers microsecond resolution.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v3.0.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: dt = pd.Timestamp(\"2024-03-22 11:36\").to_pydatetime()\nIn [2]: pd.to_datetime([dt]).dtype\nIn [3]: pd.Index([dt]).dtype\nIn [4]: pd.DatetimeIndex([dt]).dtype\nIn [5]: pd.Series([dt]).dtype\n```\n\n----------------------------------------\n\nTITLE: Indexing Bug Fixes in Pandas 0.25.2\nDESCRIPTION: Fixes for DataFrame.reindex limit argument functionality and RangeIndex.get_indexer behavior for decreasing RangeIndex values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.2.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.reindex()\nRangeIndex.get_indexer()\n```\n\n----------------------------------------\n\nTITLE: Complex Boolean Logic in pandas.DataFrame.query vs Pure Python - Python\nDESCRIPTION: Demonstrates advanced boolean combinations in DataFrame.query and their equivalent in Python with array operations, using and/or/not operators. Requires pandas and numpy. Returns two DataFrames filtered by a complex expression and verifies equivalence. Inputs: DataFrame; Output: Filtered DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_66\n\nLANGUAGE: python\nCODE:\n```\n# short query syntax\nshorter = df.query('a < b < c and (not bools) or bools > 2')\n\n# equivalent in pure Python\nlonger = df[(df['a'] < df['b'])\n            & (df['b'] < df['c'])\n            & (~df['bools'])\n            | (df['bools'] > 2)]\n\nshorter\nlonger\n\nshorter == longer\n```\n\n----------------------------------------\n\nTITLE: Roundtripping pandas DataFrames with Table Schema metadata using read_json and to_json - Python\nDESCRIPTION: Demonstrates writing and then reading a DataFrame as JSON with orient=\"table\", showing preservation of metadata, dtypes, and categorical columns. Also highlights that certain index or column names are not fully round-trippable. Dependencies: pandas; inputs: DataFrame with various dtypes, including categorical and datetime; outputs: JSON file and DataFrame post-roundtrip. Note about index names that start with 'level_' or literal 'index'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_87\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"foo\": [1, 2, 3, 4],\n        \"bar\": [\"a\", \"b\", \"c\", \"d\"],\n        \"baz\": pd.date_range(\"2018-01-01\", freq=\"D\", periods=4),\n        \"qux\": pd.Categorical([\"a\", \"b\", \"c\", \"c\"]),\n    },\n    index=pd.Index(range(4), name=\"idx\"),\n)\ndf\ndf.dtypes\n\ndf.to_json(\"test.json\", orient=\"table\")\nnew_df = pd.read_json(\"test.json\", orient=\"table\")\nnew_df\nnew_df.dtypes\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.index.name = \"index\"\ndf.to_json(\"test.json\", orient=\"table\")\nnew_df = pd.read_json(\"test.json\", orient=\"table\")\nprint(new_df.index.name)\n```\n\nLANGUAGE: python\nCODE:\n```\nos.remove(\"test.json\")\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame Columns with infer_objects Method in Python\nDESCRIPTION: Demonstrates the infer_objects method which performs dtype inference on object columns, converting Python objects to native types when possible. This example shows conversion of object-dtype numeric columns to their appropriate numeric types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': [1, 2, 3],\n                 'B': np.array([1, 2, 3], dtype='object'),\n                 'C': ['1', '2', '3']})\ndf.dtypes\ndf.infer_objects().dtypes\n```\n\n----------------------------------------\n\nTITLE: Plotting with Custom Table Data using pandas DataFrame and Matplotlib in Python\nDESCRIPTION: This code enables plotting a DataFrame while visualizing a custom table (rounded and transposed data) beneath the chart using table argument. Dependencies include pandas, numpy, matplotlib. Key parameters: table is set to a manually transposed and rounded DataFrame. Input is the original DataFrame; output is a line plot with a user-controlled table layout. Users must handle transposition and formatting if not relying on the default behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(1, 1, figsize=(7, 6.75))\nax.xaxis.tick_top()  # Display x-axis ticks on top.\ndf.plot(table=np.round(df.T, 2), ax=ax);\n```\n\n----------------------------------------\n\nTITLE: Displaying IntervalArray Representation in pandas 1.0.0 (Python)\nDESCRIPTION: Shows the representation (__repr__) of an IntervalArray created using from_tuples in pandas 1.0.0, highlighting formatting changes. Dependencies: import pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\npd.arrays.IntervalArray.from_tuples([(0, 1), (2, 3)])\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in GroupBy First and Last Methods\nDESCRIPTION: Corrects a regression in DataFrameGroupBy.first, SeriesGroupBy.first, DataFrameGroupBy.last, and SeriesGroupBy.last where None was not preserved in object dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nDataFrameGroupBy.first()\n```\n\nLANGUAGE: Python\nCODE:\n```\nSeriesGroupBy.first()\n```\n\nLANGUAGE: Python\nCODE:\n```\nDataFrameGroupBy.last()\n```\n\nLANGUAGE: Python\nCODE:\n```\nSeriesGroupBy.last()\n```\n\n----------------------------------------\n\nTITLE: Concatenating Styled DataFrames Using Styler.concat - pandas - Python\nDESCRIPTION: Demonstrates concatenation of two styled DataFrames (Stylers) provided they share the same columns, useful for presenting summaries (like sum or mean) alongside raw data. Dependencies: pandas, numpy (for previous df). Uses .agg, .style.format, .relabel_index, and .concat on Stylers. Inputs: numeric DataFrame; Outputs: concatenated Styler preserving independent styles, ready for formatted display.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsummary_styler = (\n    df.agg([\"sum\", \"mean\"]).style.format(precision=3).relabel_index([\"Sum\", \"Average\"])\n)\ndf.style.format(precision=1).concat(summary_styler)\n```\n\n----------------------------------------\n\nTITLE: Using ptrepack Utility for HDF5 Compression\nDESCRIPTION: This command demonstrates how to use the ptrepack utility to compress an existing HDF5 file. It specifies compression level, library, and other options for optimizing the file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_198\n\nLANGUAGE: shell\nCODE:\n```\nptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5\n```\n\n----------------------------------------\n\nTITLE: Correcting Timestamp Timezone Offset Validation\nDESCRIPTION: Fixes a bug where passing an invalid timezone offset designator (Z) to the Timestamp constructor did not raise a ValueError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_56\n\nLANGUAGE: Python\nCODE:\n```\npd.Timestamp('2021-01-01Z')\n```\n\n----------------------------------------\n\nTITLE: Testing Pandas Series Addition with Custom Objects in Python\nDESCRIPTION: This code snippet tests the addition behavior of Pandas Series with custom objects. It shows that Series.__add__() returns NotImplemented for unknown types, causing Python to use the custom object's __radd__ method instead.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n# Series refuses to add custom, since it's an unknown type with higher priority\nassert series.__add__(custom) is NotImplemented\n\n# This will cause the custom class `__radd__` being used instead\nassert series + custom is custom\n```\n\n----------------------------------------\n\nTITLE: Fixing Regression in Categorical.replace\nDESCRIPTION: Corrects a regression where Categorical.replace would replace with NaN whenever the new value and replacement value were equal.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nCategorical.replace()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Expressions in R\nDESCRIPTION: Demonstrates how to evaluate expressions using the with function in R.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_10\n\nLANGUAGE: r\nCODE:\n```\ndf <- data.frame(a=rnorm(10), b=rnorm(10))\nwith(df, a + b)\ndf$a + df$b  # same as the previous expression\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading DataFrames with HDF5 in pandas (Python)\nDESCRIPTION: This snippet demonstrates how to save a DataFrame to an HDF5 file using pandas' to_hdf() method with append mode, and then read it back using read_hdf() with a conditional filter on the index. Requires the pandas and PyTables libraries, and an existing DataFrame named df. The 'where' argument allows subsetting rows during the file read; ensure the HDF5 file exists before running read_hdf(), and that the DataFrame is indexed correctly to match the query.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf.to_hdf('store.h5', key='table', append=True)\npd.read_hdf('store.h5', 'table', where=['index > 2'])\n```\n\n----------------------------------------\n\nTITLE: Styler HTML Table Representation with Categorical Labels - pandas - Python\nDESCRIPTION: Constructs a DataFrame with complex multi-level index and columns for the purpose of demonstrating Styler's HTML/CSS class capabilities in representing category relationships. Dependencies: pandas, numpy. Inputs: 2x6 matrix with hierarchical indices and MultiIndex columns; output is a Styler object ready for rendering as styled HTML or for further custom styling. Intended for demonstration of HTML representation, not modification of data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index([\"Tumour (Positive)\", \"Non-Tumour (Negative)\"], name=\"Actual Label:\")\ncols = pd.MultiIndex.from_product(\n    [[\"Decision Tree\", \"Regression\", \"Random\"], [\"Tumour\", \"Non-Tumour\"]],\n    names=[\"Model:\", \"Predicted:\"],\n)\ndf = pd.DataFrame(\n    [[38.0, 2.0, 18.0, 22.0, 21, np.nan], [19, 439, 6, 452, 226, 232]],\n    index=idx,\n    columns=cols,\n)\ndf.style\n```\n\n----------------------------------------\n\nTITLE: Using DataFrame.rename with axis Parameter in Python\nDESCRIPTION: Shows how the rename method now accepts the axis parameter to specify which axis to target with the operation. Both index and columns can be renamed using function transformations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\ndf.rename(str.lower, axis='columns')\ndf.rename(id, axis='index')\n```\n\n----------------------------------------\n\nTITLE: Disallowed Direct Update of MultiIndex Level Names in pandas (Python)\nDESCRIPTION: Shows that assigning a new name directly to a MultiIndex level no longer updates the index. Demonstrates that manual mutation via 'levels[0].name' does not propagate. Compatible with pandas 1.0.0 and higher.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nmi.levels[0].name = \"new name\"\nmi.names\n```\n\n----------------------------------------\n\nTITLE: Reading Compressed CSV Files from URLs with pandas - Python\nDESCRIPTION: Shows how to read compressed CSV files from a URL using read_csv with compression inference and explicit compression options. Demonstrates using file extension inference and manual specification (bz2). Outputs the head of the loaded DataFrame. pandas is required, input is a URL to a compressed CSV file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nurl = ('https://github.com/{repo}/raw/{branch}/{path}'\n       .format(repo='pandas-dev/pandas',\n               branch='main',\n               path='pandas/tests/io/parser/data/salaries.csv.bz2'))\n# default, infer compression\ndf = pd.read_csv(url, sep='\\t', compression='infer')\n# explicitly specify compression\ndf = pd.read_csv(url, sep='\\t', compression='bz2')\ndf.head(2)\n```\n\n----------------------------------------\n\nTITLE: Writing XML with Namespace Prefix using pandas.to_xml (Python)\nDESCRIPTION: Exports DataFrame XML with a prefixed namespace in the root element using namespaces and prefix arguments, enabling XML document scoping. Only the root will have the prefix. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_130\n\nLANGUAGE: python\nCODE:\n```\nprint(\n    geom_df.to_xml(namespaces={\"doc\": \"https://example.com\"},\n                   prefix=\"doc\")\n)\n```\n\n----------------------------------------\n\nTITLE: Overwriting an Existing DataFrame with Sorted Output using pandas (Python)\nDESCRIPTION: Shows how to persist sorting in a pandas DataFrame by reassigning the result of sort_values back to the original variable. Requires pandas. The operation sorts the DataFrame based on the specified column (here, \\\"col1\\\") and overwrites the original DataFrame. No intermediate copy is kept; after this operation, df references the sorted DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/includes/copies.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = df.sort_values(\"col1\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Custom Numba-Accelerated DataFrame Operations (ipython)\nDESCRIPTION: Benchmarks the speed of a custom DataFrame operation where columns are processed through Numba-accelerated functions defined in Python. Requires a DataFrame and compute_numba function (e.g. as above). Shows timing of such operations for comparison with other approaches. Inputs: a DataFrame; Outputs: performance timing for applying custom JIT-compiled function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_14\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: %timeit compute_numba(df)\n1000 loops, best of 3: 798 us per loop\n```\n\n----------------------------------------\n\nTITLE: Series Interpolation with Limit Parameter (Python)\nDESCRIPTION: Applies the interpolate method to a Series with a limit argument restricting maximum consecutive NaNs to fill. Dependencies: pandas. Input: Series 'ser' with missing values. Output: partially interpolated Series with at most 'limit' consecutive fills. Useful for controlled imputation in data cleaning.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 3, np.nan, np.nan, np.nan, 11])\nser.interpolate(limit=2)\n```\n\n----------------------------------------\n\nTITLE: Basic Arithmetic, Comparison, and Slicing on Nullable Integer Series (pandas, Python)\nDESCRIPTION: Showcases arithmetic, comparison, and slicing operations on a pandas Series with nullable integer dtype, including addition, equality, slicing, operations with other dtypes, and type upcasting during operations. Requires a Series s with dtype 'Int64'; demonstrates compatibility with standard numpy-like operations while propagating missing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/integer_na.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, None], dtype=\"Int64\")\n\n# arithmetic\ns + 1\n\n# comparison\ns == 1\n\n# slicing operation\ns.iloc[1:3]\n\n# operate with other dtypes\ns + s.iloc[1:3].astype(\"Int8\")\n\n# coerce when needed\ns + 0.01\n```\n\n----------------------------------------\n\nTITLE: Concatenating with Ignored Indexes and Names in Python\nDESCRIPTION: Example of concatenating a Series and DataFrame with ignore_index=True, which drops all name references and creates new numeric column names.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresult = pd.concat([df1, s1], axis=1, ignore_index=True)\nresult\n```\n\n----------------------------------------\n\nTITLE: Asserting Regression Behavior in pandas with Python\nDESCRIPTION: This code snippet demonstrates creating a regression test for pandas by asserting expected behavior in a standalone Python file. It imports pandas, creates a Series object, and checks that the sum function returns a specific value. This script is used alongside git bisect to locate the commit that introduced a bug. Dependencies: pandas. The input is hardcoded (Series([1,1])) and the expected output is asserted (sum == 2), causing the script to exit with zero if regression is absent, or nonzero if present.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/maintaining.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\\nassert pd.Series([1, 1]).sum() == 2\n```\n\n----------------------------------------\n\nTITLE: MultiIndex Operations with set_index\nDESCRIPTION: Demonstrates the new behavior of set_index with MultiIndexes, showing how it preserves MultiIndex structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntuple_ind\ndf_multi.set_index(tuple_ind)\nmi\ndf_multi.set_index(mi)\ndf_multi.set_index([df_multi.index, df_multi.index])\n```\n\n----------------------------------------\n\nTITLE: Histogram Plotting for Multiple Columns (Python)\nDESCRIPTION: Creates a DataFrame of three columns with different means, and generates a semi-transparent histogram overlay using DataFrame.plot.hist(alpha=0.5). Each column is plotted as overlapping colored histograms. Requirements: pandas, numpy, matplotlib.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf4 = pd.DataFrame(\\n    {\\n        \"a\": np.random.randn(1000) + 1,\\n        \"b\": np.random.randn(1000),\\n        \"c\": np.random.randn(1000) - 1,\\n    },\\n    columns=[\"a\", \"b\", \"c\"],\\n)\\n\\nplt.figure();\\n\\n@savefig hist_new.png\\ndf4.plot.hist(alpha=0.5);\n```\n\n----------------------------------------\n\nTITLE: Preventing Parent DataFrame Mutation with Copy-on-Write Semantics in pandas (Python)\nDESCRIPTION: This code demonstrates that under Copy-on-Write semantics, modifying a subset DataFrame will not affect the original parent DataFrame. After selecting a subset and modifying it, accessing the original DataFrame confirms it remains unchanged. Dependencies: pandas. Parameters: DataFrame selection and assignment. Input is a DataFrame and its subset; output verifies non-propagation of mutation. Highlights the key behavioral guarantee of the proposal.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Case 2: The user does not want mutating df2 to mutate the parent df, via CoW\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4], \"C\": [5, 6]})\n>>> df2 = df[[\"A\", \"B\"]]\n>>> df2.loc[df2[\"A\"] > 1, \"A\"] = 1\n>>> df.iloc[1, 0]  # df was not mutated\n2\n```\n\n----------------------------------------\n\nTITLE: Groupby with Categorical Grouper (observed=False) - pandas Python\nDESCRIPTION: Counts entries grouped by a 'Categorical' with specified categories and returns observed categories only if observed is False. Demonstrates the effect of observed argument. Dependencies: pandas, numpy. Inputs: a Series and a Categorical object. Outputs: Series with counts for all defined categorical levels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\npd.Series([1, 1, 1]).groupby(\n    pd.Categorical([\"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]), observed=False\n).count()\n```\n\n----------------------------------------\n\nTITLE: Initial DataFrame Setup for Examples\nDESCRIPTION: Sets up example DataFrame and Series objects to demonstrate affected operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0006-ban-upcasting.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = DataFrame({\"a\": [1, 2, np.nan], \"b\": [4, 5, 6]})\nser = df[\"a\"].copy()\n```\n\n----------------------------------------\n\nTITLE: Concatenating Series with ignore_index in pandas (Python)\nDESCRIPTION: Shows the use of pd.concat to concatenate two Series along axis=1 with ignore_index=True. Despite ignore_index=True, the operation still aligns by index, leading to a DataFrame where rows are keyed by the union of both indices. Demonstrates an important behavioral quirk of pandas when concatenating misaligned Series. Depends on pandas and two input Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nIn [42]: pd.concat([ser1, ser2], axis=1, ignore_index=True)\nOut[42]:\n      0     1\n1  10.0  10.0\n2  15.0  15.0\n3  20.0  20.0\n5  25.0   NaN\n4   NaN  25.0\n```\n\n----------------------------------------\n\nTITLE: Deleting Objects from HDFStore in Pandas\nDESCRIPTION: Shows how to remove a stored object from an HDFStore using dictionary-style deletion. The store.remove() method provides an equivalent functionality.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_165\n\nLANGUAGE: python\nCODE:\n```\n# store.remove('df') is an equivalent method\ndel store[\"df\"]\n\nstore\n```\n\n----------------------------------------\n\nTITLE: Rendering In-Kind Sponsors List Using Jinja2 For-Loop in HTML\nDESCRIPTION: This snippet generates an HTML list of in-kind sponsors by iterating through 'sponsors.inkind' using a Jinja2 for-loop. For each company, it displays a linked name and a description. The backend must supply 'sponsors.inkind' as a list of dictionary-like objects with 'url', 'name', and 'description'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/sponsors.md#_snippet_2\n\nLANGUAGE: Jinja2\nCODE:\n```\n<ul>\\n    {% for company in sponsors.inkind %}\\n        <li><a href=\"{{ company.url }}\">{{ company.name }}</a>: {{ company.description }}</li>\\n    {% endfor %}\\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Clearing Matplotlib Figure before Plotting in Python (Suppressed Output)\nDESCRIPTION: This suppressed snippet ensures the current Matplotlib figure is cleared, preventing plot accumulation when regenerating documentation or running multiple plot commands. This step is generally unnecessary in interactive workflows like notebooks. It uses plt.clf(), which has no inputs and no direct output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nplt.clf()\n```\n\n----------------------------------------\n\nTITLE: Online EWM Calculation\nDESCRIPTION: Shows how to perform online exponentially weighted mean calculations with updates.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[1, 2, 0.6], [2, 3, 0.4], [3, 4, 0.2], [4, 5, 0.7]])\ndf.ewm(0.5).mean()\n\nonline_ewm = df.head(2).ewm(0.5).online()\nonline_ewm.mean()\nonline_ewm.mean(update=df.tail(1))\n```\n\n----------------------------------------\n\nTITLE: Applying NumPy any to a DataFrame with pandas 0.23.2 (Python)\nDESCRIPTION: Illustrates the NumPy 1.15 and pandas 0.23.2 interoperability where np.any(pd.DataFrame(...)) correctly performs reduction across the entire DataFrame, returning a single boolean scalar. Requires NumPy and pandas >= 0.23.2. Input is a DataFrame with all-False values; output is the scalar value False, reflecting correct logical reduction as per updated API. This snippet demonstrates enhanced compatibility and correctness.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.2.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnp.any(pd.DataFrame({\"A\": [False], \"B\": [False]}))\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment for Pandas with pyenv\nDESCRIPTION: Commands to create and activate a Python virtual environment using pyenv for Pandas development, and install required dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Create a virtual environment\n# Use an ENV_DIR of your choice. We'll use ~/Users/<yourname>/.pyenv/versions/pandas-dev\npyenv virtualenv <version> <name-to-give-it>\n\n# For instance:\npyenv virtualenv 3.10 pandas-dev\n\n# Activate the virtualenv\npyenv activate pandas-dev\n\n# Now install the build dependencies in the cloned pandas repo\npython -m pip install -r requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: Inverting Visibility with Selective Display Using Styler.hide - pandas - Python\nDESCRIPTION: Shows how to invert hiding logic by constructing a 'show' list and hiding all non-'show' indices/columns. Uses comprehensions over DataFrame indices/columns, demonstrating a pattern to show specific rows/columns via Styler. Dependencies: pandas, numpy. Inputs: DataFrame, list of indices to show. Output: display of DataFrame with only specified indices visible and the rest hidden.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nshow = [0, 2, 4]\ndf.style.hide([row for row in df.index if row not in show], axis=0).hide(\n    [col for col in df.columns if col not in show], axis=1\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using RangeIndex\nDESCRIPTION: Demonstrates RangeIndex creation and shows that it's the default index type for pandas Series and DataFrame objects, providing an optimized representation of monotonic integer ranges.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.RangeIndex(5)\nidx\n\nser = pd.Series([1, 2, 3])\nser.index\ndf = pd.DataFrame([[1, 2], [3, 4]])\ndf.index\ndf.columns\n\nidx[[0, 2]]\n```\n\n----------------------------------------\n\nTITLE: Initializing MultiIndex DataFrame and Querying GroupBy Rolling Sum Pandas Python\nDESCRIPTION: Demonstrates creation of a pandas DataFrame with MultiIndex, grouped by an index level, and applies rolling sum after grouping. Requires pandas. Shows that groupby with rolling on a MultiIndex now retains all levels in the output, addressing previous level dropping. Includes construction of the MultiIndex, assignment to DataFrame, and invocation of groupby/rolling/sum.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.MultiIndex.from_tuples([('idx1', 'idx2')], names=['label1', 'label2'])\ndf = pd.DataFrame({'a': [1], 'b': [2]}, index=index)\ndf\ndf.groupby('label1').rolling(1).sum()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Default pandas Behavior Without Copy-on-Write\nDESCRIPTION: Illustrates the default pandas behavior where modifying a view (derived DataFrame) affects the parent DataFrame when copy-on-write is not enabled.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_7\n\nLANGUAGE: ipython\nCODE:\n```\ndf = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": 1})\nview = df[\"foo\"]\nview.iloc[0]\ndf\n```\n\n----------------------------------------\n\nTITLE: Reindexing a Series from Another Series - Python\nDESCRIPTION: This example illustrates the updated behavior when creating a pandas Series from another Series and passing an explicit index. It shows that the new Series will reindex based on the provided index labels instead of treating the Series as a NumPy array, which prevents unintentional 'by accident' behaviors. The first snippet creates a standard Series; the second creates a reindexed Series with custom labels. Dependencies: pandas. Key parameters: a sequence for data and a list for index. Outputs: two Series, the second with possibly all NA values if index labels do not match.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.9.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ns1 = pd.Series([1, 2, 3])\ns1\n```\n\nLANGUAGE: python\nCODE:\n```\ns2 = pd.Series(s1, index=[\"foo\", \"bar\", \"baz\"])\ns2\n```\n\n----------------------------------------\n\nTITLE: Setting pandas Maximum Column Width for Displayed Data (Python)\nDESCRIPTION: Adjusts pandas display.max_colwidth to control truncation of string columns, showing effect on a DataFrame containing filenames and paths. Demonstrates setting to 30 and 100, showing resulting output with pd.DataFrame. Requires pandas and a dictionary datafile with long string fields.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndatafile = {\n    \"filename\": [\"filename_01\", \"filename_02\"],\n    \"path\": [\n        \"media/user_name/storage/folder_01/filename_01\",\n        \"media/user_name/storage/folder_02/filename_02\",\n    ],\n}\n\npd.set_option(\"display.max_colwidth\", 30)\npd.DataFrame(datafile)\n\npd.set_option(\"display.max_colwidth\", 100)\npd.DataFrame(datafile)\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with Comments Included\nDESCRIPTION: Shows the default behavior of pandas.read_csv which includes comments in the output data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"tmp.csv\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Showing KeyError on non-existent integer key in Series\nDESCRIPTION: Example demonstrating how accessing a non-existent integer key in a Series now raises a KeyError, a change from prior versions which would fall back to position-based lookup.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: s[1]\nKeyError: 1\n```\n\n----------------------------------------\n\nTITLE: Bar Plotting with Colormap using pandas DataFrame and Matplotlib in Python\nDESCRIPTION: This snippet demonstrates building a bar plot from a DataFrame of cumulative sums of absolute random values and assigning a Matplotlib colormap ('Greens'). Dependencies: numpy, pandas, matplotlib. Inputs include random data transformed to absolute values and cumulatively summed; output is a bar plot with varying color intensities based on the colormap. Limitations: Color distinction relies on the provided colormap and the number of bars/columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(123456)\ndd = pd.DataFrame(np.random.randn(10, 10)).map(abs)\ndd = dd.cumsum()\n\nplt.figure();\ndd.plot.bar(colormap=\"Greens\");\n```\n\n----------------------------------------\n\nTITLE: Accessing Datetime Fields Now Returns Index, Not ndarray - Python\nDESCRIPTION: This snippet shows the change in DatetimeIndex, PeriodIndex, and TimedeltaIndex field accessors: they now return Index objects instead of numpy arrays (except boolean fields, which still return boolean ndarrays). Dependency: Pandas. Input: DatetimeIndex object. Output: Index object containing the requested field values. Caveat: Index is immutable, unlike numpy arrays; use np.asarray to convert if mutability is required.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\nidx = pd.date_range(\"2015-01-01\", periods=5, freq='10H')\nidx.hour\n# Previous Output: array([ 0, 10, 20,  6, 16], dtype=int32)\n\n# New Output: Index([0, 10, 20, 6, 16], dtype='int32')\n```\n\n----------------------------------------\n\nTITLE: Querying by pandas.MultiIndex Levels in DataFrame.query - Python\nDESCRIPTION: Explains querying columns and MultiIndex levels as if they were DataFrame columns, with or without names. Requires pandas and numpy. Illustrates filtering by the 'color' level of a MultiIndex and by index level number when no names are present. Inputs: MultiIndex DataFrame; Output: Filtered DataFrame by index level.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nn = 10\ncolors = np.random.choice(['red', 'green'], size=n)\nfoods = np.random.choice(['eggs', 'ham'], size=n)\ncolors\nfoods\n\nindex = pd.MultiIndex.from_arrays([colors, foods], names=['color', 'food'])\ndf = pd.DataFrame(np.random.randn(n, 2), index=index)\ndf\ndf.query('color == \"red\"')\n\ndf.index.names = [None, None]\ndf\ndf.query('ilevel_0 == \"red\"')\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows with Not-Null Values Using notna in pandas (Python)\nDESCRIPTION: This snippet filters out rows with missing values in the 'Age' column using the notna() method. Only rows with non-null 'Age' values are selected, and head() shows sample entries. Useful for excluding incomplete data from analysis.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nage_no_na = titanic[titanic[\"Age\"].notna()]\nage_no_na.head()\n```\n\n----------------------------------------\n\nTITLE: Updating pandas.tseries.frequencies.to_offset() usage\nDESCRIPTION: The 'freqstr' keyword in pandas.tseries.frequencies.to_offset() is deprecated. Use 'freq' instead. Also, get_standard_freq is replaced with a new method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\npandas.tseries.frequencies.to_offset(freqstr=...)\n```\n\nLANGUAGE: Python\nCODE:\n```\npandas.tseries.frequencies.to_offset(freq=...)\n```\n\nLANGUAGE: Python\nCODE:\n```\npandas.tseries.frequencies.to_offset(freq).rule_code\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Timestamp comparison error in HDF queries in Pandas\nDESCRIPTION: This code demonstrates the new behavior when trying to use a timestamp variable in an HDF query, which now raises a TypeError when comparing a Timestamp object to a string column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_37\n\nLANGUAGE: ipython\nCODE:\n```\nIn [18]: ts = pd.Timestamp('2014-01-01')\n\nIn [19]: pd.read_hdf('store.h5', 'key', where='unparsed_date > ts')\nTypeError: Cannot compare 2014-01-01 00:00:00 of\ntype <class 'pandas.tslib.Timestamp'> to string column\n```\n\n----------------------------------------\n\nTITLE: Reading MultiIndex CSV Files\nDESCRIPTION: Shows how to read CSV files with multiple index levels and multiple column header levels using pandas read_csv\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\ndata = 'year,indiv,zit,xit\\n1977,\"A\",1.2,.6\\n1977,\"B\",1.5,.5'\nprint(data)\nwith open(\"mindex_ex.csv\", mode=\"w\") as f:\n    f.write(data)\n\ndf = pd.read_csv(\"mindex_ex.csv\", index_col=[0, 1])\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with PyArrow Backend Dtypes in pandas (Python)\nDESCRIPTION: Shows reading a CSV into a pandas DataFrame with all columns backed by PyArrow storage by specifying dtype_backend=\\\"pyarrow\\\". This can be used with or without specifying engine=\\\"pyarrow\\\". Dependencies are pandas, pyarrow, and io; input is a CSV string. Output is a pandas DataFrame where dtypes reflect Arrow dtypes and .dtypes shows extension types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport io\ndata = io.StringIO(\"\"\"a,b,c,d,e,f,g,h,i\n    1,2.5,True,a,,,,,\n    3,4.5,False,b,6,7.5,True,a,\n\"\"\")\ndf_pyarrow = pd.read_csv(data, dtype_backend=\"pyarrow\")\ndf_pyarrow.dtypes\n```\n\n----------------------------------------\n\nTITLE: Explicitly Providing a Mixed Format Option to to_datetime - ipython\nDESCRIPTION: This snippet shows how users may explicitly ask for flexible parsing by passing format='mixed' to to_datetime, accommodating inputs of various formats at their own risk. Requires pandas as pd. Inputs are a list of date strings and format parameter set to 'mixed'. Output is a DatetimeIndex parsed using multiple potential date formats.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0004-consistent-to-datetime-parsing.md#_snippet_2\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: pd.to_datetime(['12-01-2000 00:00:00', '13-01-2000 00:00:00'], format='mixed')\nOut[3]: DatetimeIndex(['2000-12-01', '2000-01-13'], dtype='datetime64[ns]', freq=None)\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Pandas with Meson\nDESCRIPTION: Command to build and install Pandas from source using the Meson build system via pip, with verbose output for editable installs.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install -ve . --no-build-isolation -Ceditable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Assembling Datetimes from DataFrame in Pandas\nDESCRIPTION: Shows how to use pd.to_datetime() to assemble datetime objects from columns in a DataFrame, demonstrating flexibility in datetime creation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\"year\": [2015, 2016], \"month\": [2, 3], \"day\": [4, 5], \"hour\": [2, 3]}\n)\ndf\n\npd.to_datetime(df)\n\npd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n```\n\n----------------------------------------\n\nTITLE: Creating a pandas DataFrame with Ambiguous Width Unicode Characters - Python\nDESCRIPTION: This code sample constructs a pandas DataFrame wherein the string values include ambiguous-width Unicode symbols, such as inverted exclamation marks. By default, these are rendered with a width of 1, which can cause possible alignment issues depending on the terminal and codepage. The snippet is useful to demonstrate behaviors before adjusting ambiguous width handling options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": [\"xxx\", \"¡¡\"], \"b\": [\"yyy\", \"¡¡\"]})\ndf\n```\n\n----------------------------------------\n\nTITLE: Index str.extractall() for Regular Expressions - Pandas - Python\nDESCRIPTION: Shows the use of Index.str.extractall() to extract regular expression groups from Index values and return a DataFrame result. The example uses a named group to find digits after specified letters. Only pandas is needed; input includes a regex pattern and an Index of strings. Returns a DataFrame with matches.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nidx = pd.Index([\"a1a2\", \"b1\", \"c1\"])\nidx.str.extractall(r\"[ab](?P<digit>\\d)\")\n```\n\n----------------------------------------\n\nTITLE: Parsing multiple datetime strings with same UTC offset\nDESCRIPTION: Demonstrates how parsing multiple datetime strings with the same UTC offset preserves the timezone information in the resulting DatetimeIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime([\"2015-11-18 15:30:00+05:30\"] * 2)\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: Groupby with as_index=True Includes Group Column - ipython\nDESCRIPTION: Shows that, prior to changes, performing groupby with as_index=True and an aggregation like nunique would incorrectly keep the grouping key as a column in the result, rather than index only. This code groups by column 'a' and applies nunique, displaying unintended columns in the output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_22\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: df.groupby(\"a\", as_index=True).nunique()\\nOut[4]:\\n   a  b\\na\\nx  1  1\\ny  1  2\n```\n\n----------------------------------------\n\nTITLE: Reindexing with CategoricalIndex\nDESCRIPTION: Illustrates reindexing operations with CategoricalIndex and shows how the resulting index type depends on the indexer type used for reindexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndf3 = pd.DataFrame(\n    {\"A\": np.arange(3), \"B\": pd.Series(list(\"abc\")).astype(\"category\")}\n)\ndf3 = df3.set_index(\"B\")\ndf3\n\ndf3.reindex([\"a\", \"e\"])\ndf3.reindex([\"a\", \"e\"]).index\ndf3.reindex(pd.Categorical([\"a\", \"e\"], categories=list(\"abe\")))\ndf3.reindex(pd.Categorical([\"a\", \"e\"], categories=list(\"abe\"))).index\n```\n\n----------------------------------------\n\nTITLE: Grouping, Describing, and Applying Custom Functions to DataFrame Columns - pandas (IPython)\nDESCRIPTION: Demonstrates creating a sample DataFrame, grouping data by column A, and applying summary/statistical and custom operations on column C. Requires pandas as pd, numpy as np, and a DataFrame named df. Shows grouping by one column and describing statistics, as well as using groupby.apply with a lambda function to sort and select top values. The output is a Series or DataFrame, depending on the operation. Data and function are customizable, and outputs depend on group structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.3.rst#_snippet_5\n\nLANGUAGE: ipython\nCODE:\n```\nIn [6]: df = pd.DataFrame(\n   ...:     {\n   ...:         \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n   ...:         \"B\": [\"one\", \"one\", \"two\", \"three\", \"two\", \"two\", \"one\", \"three\"],\n   ...:         \"C\": np.random.randn(8),\n   ...:         \"D\": np.random.randn(8),\n   ...:     }\n   ...: )\n   ...:\n\nIn [7]: df\nOut[7]:\n   A      B         C         D\n0  foo    one  0.469112 -0.861849\n1  bar    one -0.282863 -2.104569\n2  foo    two -1.509059 -0.494929\n3  bar  three -1.135632  1.071804\n4  foo    two  1.212112  0.721555\n5  bar    two -0.173215 -0.706771\n6  foo    one  0.119209 -1.039575\n7  foo  three -1.044236  0.271860\n\n[8 rows x 4 columns]\n\nIn [8]: grouped = df.groupby(\"A\")[\"C\"]\n\nIn [9]: grouped.describe()\nOut[9]:\n   count      mean       std       min       25%       50%       75%       max\nA\nbar    3.0 -0.530570  0.526860 -1.135632 -0.709248 -0.282863 -0.228039 -0.173215\nfoo    5.0 -0.150572  1.113308 -1.509059 -1.044236  0.119209  0.469112  1.212112\n\n[2 rows x 8 columns]\n\nIn [10]: grouped.apply(lambda x: x.sort_values()[-2:])  # top 2 values\nOut[10]:\nA\nbar  1   -0.282863\n     5   -0.173215\nfoo  0    0.469112\n     4    1.212112\nName: C, Length: 4, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Toggling allows_duplicate_labels Flag via set_flags - pandas - Python\nDESCRIPTION: Creates a new DataFrame view with allows_duplicate_labels flag set to True by calling set_flags on an existing DataFrame. Shows how attribute changes return a view sharing data with the original. Input is DataFrame; output is new DataFrame with modified label allowance flag.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.set_flags(allows_duplicate_labels=True)\ndf2.flags.allows_duplicate_labels\n```\n\n----------------------------------------\n\nTITLE: Displaying HTML Strings in DataFrames Using Pandas Styler (Python)\nDESCRIPTION: This simple snippet demonstrates the creation of a DataFrame containing raw HTML strings and obtaining a Styler object for further formatting. Requires pandas. The output is a styled DataFrame (df4.style) where each cell contains potentially unsafe HTML content. No formatting is applied yet; further steps are needed to control HTML rendering or escaping.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndf4 = pd.DataFrame([[\"<div></div>\", '\"&other\"', \"<span></span>\"]])\ndf4.style\n```\n\n----------------------------------------\n\nTITLE: Groupwise Transformation with egen and bysort - Stata\nDESCRIPTION: Utilizes Stata's 'bysort' and 'egen' to compute the mean of total_bill within each 'sex' and 'smoker' group, assigning results to group_bill. The subsequent 'generate' statement subtracts this group mean from total_bill to produce a centered value. Requires columns total_bill, sex, and smoker in the dataset. Outputs an adjusted total_bill per group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_23\n\nLANGUAGE: stata\nCODE:\n```\nbysort sex smoker: egen group_bill = mean(total_bill)\\ngenerate adj_total_bill = total_bill - group_bill\n```\n\n----------------------------------------\n\nTITLE: Reading DataFrame from ArcticDB\nDESCRIPTION: This code shows how to read back a previously stored DataFrame from ArcticDB. The data is accessed through the 'data' attribute of the read_record object, maintaining the original data types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nread_record = lib.read(\"test\")\nread_record.data\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Creating MultiIndex Series for Integer Lookup Bug Demo - Python\nDESCRIPTION: Demonstrates constructing a pandas Series with a MultiIndex composed of an integer Index and a DatetimeIndex. Prepares for showing the bug fix where looking up an integer not in the first index level raises KeyError. Relies on pd.Index, pd.date_range, pd.MultiIndex.from_product, and pd.Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.Index(range(4))\\ndti = pd.date_range(\"2000-01-03\", periods=3)\\nmi = pd.MultiIndex.from_product([idx, dti])\\nser = pd.Series(range(len(mi)), index=mi)\n```\n\n----------------------------------------\n\nTITLE: Deprecated DatetimeIndex Week Access\nDESCRIPTION: Example showing deprecated and recommended ways to access week information from DatetimeIndex objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n# Deprecated\nDatetimeIndex.week\nDatetimeIndex.weekofyear\n\n# Recommended\nDatetimeIndex.isocalendar().week\n```\n\n----------------------------------------\n\nTITLE: Comparing Index.map Old and New Return Types - Python\nDESCRIPTION: These snippets illustrate the behavioral change in Index.map and MultiIndex.map methods: the methods now return Index objects instead of numpy arrays. No additional dependencies are needed beyond Pandas. Inputs are regular Index or MultiIndex; outputs are previously numpy arrays, now Index. For interoperability with libraries expecting numpy arrays, np.asarray(Index) should be used.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nidx = pd.Index([1, 2])\nidx\nmi = pd.MultiIndex.from_tuples([(1, 2), (2, 4)])\nmi\n```\n\nLANGUAGE: Python\nCODE:\n```\nidx.map(lambda x: x * 2)\n# Previous Output: array([2, 4])\n```\n\nLANGUAGE: Python\nCODE:\n```\nidx.map(lambda x: (x, x * 2))\n# Previous Output: array([(1, 2), (2, 4)], dtype=object)\n```\n\nLANGUAGE: Python\nCODE:\n```\nmi.map(lambda x: x)\n# Previous Output: array([(1, 2), (2, 4)], dtype=object)\n```\n\nLANGUAGE: Python\nCODE:\n```\nmi.map(lambda x: x[0])\n# Previous Output: array([1, 2])\n```\n\n----------------------------------------\n\nTITLE: Checking Index Dtype of a Grouped Series - pandas Python\nDESCRIPTION: After grouping and counting by a categorical, this snippet retrieves the dtype of the resulting index to confirm that all categories are present in the grouped result. Inputs: a pandas Series and categorical grouper. Returns: dtype of the index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ns = (\n    pd.Series([1, 1, 1])\n    .groupby(pd.Categorical([\"a\", \"a\", \"a\"], categories=[\"a\", \"b\"]), observed=True)\n    .count()\n)\ns.index.dtype\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrames with All Missing Rows to HDF5 Using to_hdf - Python\nDESCRIPTION: Shows how to write a DataFrame with missing data to an HDF5 file using the to_hdf method with format='table'. The snippet includes a DataFrame with NaNs and demonstrates saving and reading it back. In pandas 0.17.0, rows with all missing data are now retained unless dropna=True is set. Requires pandas, numpy, and HDF5 support.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndf_with_missing = pd.DataFrame(\n    {\"col1\": [0, np.nan, 2], \"col2\": [1, np.nan, np.nan]}\n)\n\ndf_with_missing\n```\n\nLANGUAGE: python\nCODE:\n```\ndf_with_missing.to_hdf('file.h5',\n                       key='df_with_missing',\n                       format='table',\n                       mode='w')\n```\n\nLANGUAGE: python\nCODE:\n```\npd.read_hdf('file.h5', 'df_with_missing')\n```\n\n----------------------------------------\n\nTITLE: Squeezing Single-Dimension Panels in pandas (Python)\nDESCRIPTION: This snippet illustrates how to reduce the dimensionality of a pandas.Panel by using the squeeze() method after reindexing to a single item or minor axis. The Panel is created using three items and multiple axes, then subsetting reduces the Panel to a DataFrame or Series. Requires pandas and numpy, and assumes Panel usage (now deprecated in recent pandas versions). The main limitation is that Panel is obsolete and should only be used for compatibility with legacy code.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\np = pd.Panel(np.random.randn(3, 4, 4), items=['ItemA', 'ItemB', 'ItemC'],\n             major_axis=pd.date_range('20010102', periods=4),\n             minor_axis=['A', 'B', 'C', 'D'])\np\n# Output: <class 'pandas.core.panel.Panel'>\n# Dimensions: 3 (items) x 4 (major_axis) x 4 (minor_axis)\n# Items axis: ItemA to ItemC\n# Major_axis axis: 2001-01-02 00:00:00 to 2001-01-05 00:00:00\n# Minor_axis axis: A to D\n```\n\nLANGUAGE: python\nCODE:\n```\np.reindex(items=['ItemA']).squeeze()\n```\n\nLANGUAGE: python\nCODE:\n```\np.reindex(items=['ItemA'], minor=['B']).squeeze()\n```\n\n----------------------------------------\n\nTITLE: Axis-aware MultiIndex Slicing with loc(axis=0) - pandas Python\nDESCRIPTION: Demonstrates specifying the axis explicitly in loc for advanced MultiIndex slicing, allowing for clearer intent when slicing across complex axes. This usage is helpful to disambiguate which axis the slicers are applied to, especially when both indices are MultiIndexed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf.loc(axis=0)[:, :, ['C1', 'C3']]\n```\n\n----------------------------------------\n\nTITLE: Listing Bug Fixes in Pandas Release Notes - Markdown - English\nDESCRIPTION: This snippet enumerates bug fixes by referencing issue numbers and describing the core problem fixed. The dependencies are the project issue tracker and Sphinx for issue linking. Inputs are issue numbers and concise descriptions; outputs are human-readable release notes. The limitations are that it is static and must be manually maintained.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.2.rst#_snippet_3\n\nLANGUAGE: Markdown\nCODE:\n```\n- Bug where MySQL interface could not handle numeric table/column names (:issue:`10255`)\n- Bug in ``read_csv`` with a ``date_parser`` that returned a ``datetime64`` array of other time resolution than ``[ns]`` (:issue:`10245`)\n- Bug in ``Panel.apply`` when the result has ndim=0 (:issue:`10332`)\n- Bug in ``read_hdf`` where ``auto_close`` could not be passed (:issue:`9327`).\n- Bug in ``read_hdf`` where open stores could not be used (:issue:`10330`).\n- Bug in adding empty ``DataFrames``, now results in a ``DataFrame`` that ``.equals`` an empty ``DataFrame`` (:issue:`10181`).\n- Bug in ``to_hdf`` and ``HDFStore`` which did not check that complib choices were valid (:issue:`4582`, :issue:`8874`).\n```\n\n----------------------------------------\n\nTITLE: Demonstrating intelligent datetime precision in Pandas 0.13.1\nDESCRIPTION: Illustrates how Pandas 0.13.1 intelligently limits precision for datetime and timedelta64 values based on the values in the array.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.1.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    [pd.Timestamp(\"20010101\"), pd.Timestamp(\"20040601\")], columns=[\"age\"]\n)\ndf[\"today\"] = pd.Timestamp(\"20130419\")\ndf[\"diff\"] = df[\"today\"] - df[\"age\"]\ndf\n```\n\n----------------------------------------\n\nTITLE: Converting Sparse DataFrame to scipy.sparse COO Matrix - pandas - Python\nDESCRIPTION: Converts a sparse pandas DataFrame back to a SciPy sparse COO matrix using the .sparse.to_coo() accessor, ensuring round-trip conversion and compatibility with SciPy's sparse tools. Only available on DataFrames with sparse data types. Dependencies: pandas, scipy.sparse. Output is a scipy.sparse.coo_matrix containing the data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nsdf.sparse.to_coo()\n```\n\n----------------------------------------\n\nTITLE: Constructing Timedelta from DateOffsets - pandas - Python\nDESCRIPTION: Shows how pandas DateOffset objects (like Second, Day) can be used to create Timedelta instances. Prerequisites are pandas offset objects. Demonstrates use of pd.Timedelta with a DateOffset argument.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/timedeltas.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npd.Timedelta(pd.offsets.Second(2))\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Chained Assignment Limitation in pandas - Python\nDESCRIPTION: This snippet demonstrates that chained assignment (`df[df['B'] > 3]['B'] = 10`) will not modify the parent DataFrame (`df`) in Copy-on-Write mode, as indexed results always behave as copies. This behavior eliminates ambiguity and renders the SettingWithCopyWarning unnecessary. Dependencies: a pandas DataFrame named df containing a column \"B\". The mask df['B'] > 3 creates a subset, and the assignment affects only the derived object. No output; the operation does not modify df.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> df[df['B'] > 3]['B'] = 10\n```\n\n----------------------------------------\n\nTITLE: Converting Strings and Arrays to Timedelta Using to_timedelta - Pandas - ipython\nDESCRIPTION: Demonstrates use of the top-level pandas.to_timedelta to parse strings or arrays into Timedelta or TimedeltaIndex objects. Accepts single timedelta-like strings, lists including NaN, and numerical arrays with explicit unit. Shows outputs for various units (seconds, days) and highlights nanosecond precision. Requires pandas, numpy, and (for some features) numpy \\u2265 1.7. Inputs: string, list, or np.arange. Outputs: Timedelta, TimedeltaIndex, NaT.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_14\n\nLANGUAGE: ipython\nCODE:\n```\nIn [53]: pd.to_timedelta('1 days 06:05:01.00003')\\nOut[53]: Timedelta('1 days 06:05:01.000030')\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [54]: pd.to_timedelta('15.5us')\\nOut[54]: Timedelta('0 days 00:00:00.000015500')\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [55]: pd.to_timedelta(['1 days 06:05:01.00003', '15.5us', 'nan'])\\nOut[55]: TimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015500', NaT], dtype='timedelta64[ns]', freq=None)\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [56]: pd.to_timedelta(np.arange(5), unit='s')\\nOut[56]:\\nTimedeltaIndex(['0 days 00:00:00', '0 days 00:00:01', '0 days 00:00:02',\\n                '0 days 00:00:03', '0 days 00:00:04'],\\n               dtype='timedelta64[ns]', freq=None)\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [57]: pd.to_timedelta(np.arange(5), unit='d')\\nOut[57]: TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'], dtype='timedelta64[ns]', freq=None)\n```\n\n----------------------------------------\n\nTITLE: Using expanding() method with corr() instead of expanding_corr_pairwise\nDESCRIPTION: Example of the new preferred way to compute pairwise expanding correlation, replacing the deprecated expanding_corr_pairwise function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n.expanding().corr(pairwise=True)\n```\n\n----------------------------------------\n\nTITLE: Previous Behavior: DataFrame.apply Evaluates First Row Twice - ipython\nDESCRIPTION: Shows that in previous pandas versions, applying a function along axis=1 would evaluate the first row twice, printing and returning it two times, before processing the rest correctly. Useful for library authors and advanced users to debug apply/applymap semantics and side effects in earlier pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_32\n\nLANGUAGE: ipython\nCODE:\n```\nIn [4]: df.apply(func, axis=1)\\na    1\\nb    3\\nName: 0, dtype: int64\\na    1\\nb    3\\nName: 0, dtype: int64\\na    2\\nb    6\\nName: 1, dtype: int64\\nOut[4]:\\n   a  b\\n0  1  3\\n1  2  6\n```\n\n----------------------------------------\n\nTITLE: Using .iloc with get_loc for Column Selection\nDESCRIPTION: Demonstrates using .iloc with the get_loc method to achieve the same selection as .loc but with explicit positional indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndfd.iloc[[0, 2], dfd.columns.get_loc('A')]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Series label-based slicing requirements in Python\nDESCRIPTION: Example showing how label-based slicing with ix requires the index to be sorted (monotonic) unless both the start and endpoint are contained in the index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nIn [1]: s = pd.Series(np.random.randn(6), index=list('gmkaec'))\n\nIn [2]: s\nOut[2]:\ng   -1.182230\nm   -0.276183\nk   -0.243550\na    1.628992\ne    0.073308\nc   -0.539890\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Demonstrating GroupBy on Categoricals in Pandas\nDESCRIPTION: This code creates a DataFrame with a categorical column 'chromosomes' and demonstrates how pandas 0.20.0 allows groupby operations on categorical data with missing categories when sort=False is specified.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nchromosomes = np.r_[np.arange(1, 23).astype(str), ['X', 'Y']]\ndf = pd.DataFrame({\n    'A': np.random.randint(100),\n    'B': np.random.randint(100),\n    'C': np.random.randint(100),\n    'chromosomes': pd.Categorical(np.random.choice(chromosomes, 100),\n                                  categories=chromosomes,\n                                  ordered=True)})\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating String Arrays with PyArrow using String Alias\nDESCRIPTION: Shows how to create a PyArrow backed string array using the string alias syntax. This provides a more concise way to specify the PyArrow storage backend for string data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['abc', None, 'def'], dtype=\"string[pyarrow]\")\ns\n```\n\n----------------------------------------\n\nTITLE: Accessing Dtype in DataFrame Apply Functions - pandas - Python\nDESCRIPTION: Illustrates applying functions to DataFrame rows or columns where columns have categorical dtype. Shows that .apply returns object dtype Series for rows and dtype objects for columns; highlights pandas' current limitation in dtype preservation. Dependencies: pandas. Key parameters: DataFrame with categorical column. Input: DataFrame; Output: Series of types/dtypes for each row/column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"a\": [1, 2, 3, 4],\n        \"b\": [\"a\", \"b\", \"c\", \"d\"],\n        \"cats\": pd.Categorical([1, 2, 3, 2]),\n    }\n)\ndf.apply(lambda row: type(row[\"cats\"]), axis=1)\ndf.apply(lambda col: col.dtype, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Accessing a Single DataFrame Value with .loc\nDESCRIPTION: Demonstrates accessing a single value in a DataFrame using .loc with row and column labels, equivalent to using .at accessor.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# this is also equivalent to ``df1.at['a','A']``\ndf1.loc['a', 'A']\n```\n\n----------------------------------------\n\nTITLE: Updating to_datetime Timezone Parsing in Python\nDESCRIPTION: Bug fix for to_datetime function with infer_datetime_format=True, improving parsing of timezone names like UTC.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime('2020-01-01 12:00 UTC', infer_datetime_format=True)  # Now correctly parses UTC\n```\n\n----------------------------------------\n\nTITLE: Documenting NA for Representing Missing Values in Pandas (RST)\nDESCRIPTION: RST documentation snippet that references the pandas NA object, which is used to represent missing values for nullable dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/missing_value.rst#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/class_without_autosummary.rst\n\n   NA\n```\n\n----------------------------------------\n\nTITLE: Applying Background Gradient Styling with pandas Styler (Python)\nDESCRIPTION: This code creates a DataFrame with random numbers and applies a conditional HTML gradient background using pandas Styler. It demonstrates how to visually format data frames in HTML via the 'background_gradient' method with a specified color map and lower bound. Requires pandas and numpy, and is intended to be run in an environment supporting HTML rendering, such as Jupyter Notebook.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(123)\ndf = pd.DataFrame(np.random.randn(10, 5), columns=list(\"abcde\"))\nhtml = df.style.background_gradient(cmap=\"viridis\", low=0.5)\n```\n\n----------------------------------------\n\nTITLE: GroupBy Count and Describe - pandas - Python\nDESCRIPTION: Illustrates counting and describing groups with groupby, including both cases where the grouped column becomes the index and where as_index=False leaves it as a column. Shows effects of these settings on output structure and how they interact with groupby in pandas 0.14.0+. Input DataFrame covers cases with NaN and non-NaN values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[1, np.nan], [1, 4], [5, 6], [5, 8]], columns=['A', 'B'])\\ng = df.groupby('A')\\ng.count()\\ng.describe()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[1, np.nan], [1, 4], [5, 6], [5, 8]], columns=['A', 'B'])\\ng = df.groupby('A', as_index=False)\\ng.count()\\ng.describe()\n```\n\n----------------------------------------\n\nTITLE: Successful label-based slicing with existing endpoints in Python\nDESCRIPTION: Example showing that label-based slicing with ix works when both slice endpoints exist in the index, even if the index is not sorted.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: s.ix['k':'e']\nOut[3]:\nk   -0.243550\na    1.628992\ne    0.073308\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for datetime format function\nDESCRIPTION: RestructuredText directive listing Pandas datetime format detection function\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/general_functions.rst#_snippet_6\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n\n   tseries.api.guess_datetime_format\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregating Data in SAS with PROC SUMMARY\nDESCRIPTION: Aggregates 'total_bill' and 'tip' columns by the grouping variables 'sex' and 'smoker' using PROC SUMMARY. Produces an output table 'tips_summed' with summed values. Dependencies: SAS Base module. Inputs: 'tips' table; outputs: 'tips_summed'. Required columns: 'sex', 'smoker', 'total_bill', 'tip'. Intended for summary statistics generation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_21\n\nLANGUAGE: sas\nCODE:\n```\nproc summary data=tips nway;\n    class sex smoker;\n    var total_bill tip;\n    output out=tips_summed sum=;\nrun;\n```\n\n----------------------------------------\n\nTITLE: Using get_dummies() for string columns in Pandas 0.13.1\nDESCRIPTION: Demonstrates the new Series.str.get_dummies() method for extracting dummy/indicator variables from separated string columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.1.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([\"a\", \"a|b\", np.nan, \"a|c\"])\ns.str.get_dummies(sep=\"|\")\n```\n\n----------------------------------------\n\nTITLE: Appending Data to a DataFrame Using .loc - Pandas - Python\nDESCRIPTION: Demonstrates how to append a new row to a pandas DataFrame using the .loc accessor. Shows assignment of a scalar value to all columns at a new index position, followed by evaluation of the modified DataFrame. Prerequisites: pandas library installed and a DataFrame variable dfi defined. Inputs: scalar assigned at position 3. Output: expanded DataFrame with new row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndfi.loc[3] = 5\\ndfi\n```\n\n----------------------------------------\n\nTITLE: Pandas MultiIndex DataFrame Sorting Example\nDESCRIPTION: Demonstrates sorting behavior with MultiIndexed DataFrames\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'value': [1, 2, 3, 4]},\n                index=pd.MultiIndex([['a', 'b'], ['bb', 'aa']],\n                                    [[0, 0, 1, 1], [0, 1, 0, 1]]))\ndf\n```\n\n----------------------------------------\n\nTITLE: Demonstrating min_periods Handling in Exponential Moving Average (ewma) - pandas - Python\nDESCRIPTION: Compares prior and new behavior of ewma with min_periods. Shows how the resulting Series output indices and NaN behavior changes depending on the min_periods value and input's non-NaN values. Requires pandas, input is Series with None/NaN, and output is exponentially weighted mean. Used for illustrating BC-breaking change.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ns  = pd.Series([1, None, None, None, 2, 3])\n```\n\nLANGUAGE: python\nCODE:\n```\npd.ewma(s, com=3., min_periods=2)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Sparse Attributes with Series.sparse - pandas - Python\nDESCRIPTION: Illustrates the use of the .sparse accessor on a pandas Series with SparseDtype to check its density (fraction of non-fill values) and the current fill_value. This accessor provides methods and properties unique to sparse data handling. Requires pandas. Output includes both density (a float) and fill_value (the placeholder for omitted data).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([0, 0, 1, 2], dtype=\"Sparse[int]\")\ns.sparse.density\ns.sparse.fill_value\n```\n\n----------------------------------------\n\nTITLE: Explicitly Casting Series to Object Before Assignment in Pandas (python)\nDESCRIPTION: This snippet shows the recommended approach of casting a pandas Series to 'object' dtype prior to assigning a value of incompatible type. It utilizes the astype method to enable heterogeneous types, then updates an element. Input is a numerical list; output is a Series with 'object' dtype supporting mixed types. This prevents future warnings or errors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n  ser = pd.Series([1, 2, 3])\n  ser = ser.astype('object')\n  ser[0] = 'not an int64'\n  ser\n```\n\n----------------------------------------\n\nTITLE: Checking pandas version after local build in Python\nDESCRIPTION: This code snippet demonstrates how to import pandas and check its version after a local build. The exact output may differ depending on the build.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> import pandas\n>>> print(pandas.__version__)  # note: the exact output may differ\n2.0.0.dev0+880.g2b9e661fbb.dirty\n```\n\n----------------------------------------\n\nTITLE: Managing HDF5 Store Data in Pandas\nDESCRIPTION: Illustrates how to remove data from HDF5 tables, delete entire stores, and work with hierarchical keys in Pandas HDF5 storage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nstore.remove('wp', pd.Term('major_axis>20000103'))\ndel store['df']\nstore.put('foo/bar/bah', df)\nstore.append('food/orange', df)\nstore.append('food/apple', df)\nstore.remove('food')\n```\n\n----------------------------------------\n\nTITLE: Applying On-the-Fly Compression for HDF5 Tables in Python\nDESCRIPTION: This code shows how to apply compression on-the-fly when appending data to an HDF5 store. This method only works for tables in stores where compression is not already enabled.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_197\n\nLANGUAGE: python\nCODE:\n```\nstore.append(\"df\", df, complib=\"zlib\", complevel=5)\n```\n\n----------------------------------------\n\nTITLE: GroupBy Sum Behavior in pandas 1.5.1 - pandas - Python\nDESCRIPTION: Two snippets here show 1.5.1 groupby regressions: observed=True with dropna=False (incorrectly dropping NA values), and observed=False with dropna=False (correctly listing unobserved categories, but NA values are still dropped). Dependencies: pandas, DataFrame 'df'. These operations clarify the trade-off reinstated in 1.5.1. Inputs: DataFrame 'df', groupby parameters. Outputs: groupby results exposing new and regressed behaviors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.1.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Incorrect behavior, NA values are dropped\ndf.groupby(\"x\", observed=True, dropna=False).sum()\n```\n\nLANGUAGE: python\nCODE:\n```\n# Correct behavior, unobserved categories present (NA values still dropped)\ndf.groupby(\"x\", observed=False, dropna=False).sum()\n```\n\n----------------------------------------\n\nTITLE: Updating Period Frequency Input in Python\nDESCRIPTION: API change for Period class, no longer accepting tuples for the freq argument. Users should now pass frequency as a string.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nPeriod(freq='D')  # Correct usage\n# Period(freq=('D',))  # No longer valid\n```\n\n----------------------------------------\n\nTITLE: Demonstrating New Index Display Representations with Truncation in Pandas (Python)\nDESCRIPTION: This code illustrates changes to pandas Index and CategoricalIndex string representations under the new formatting rules (in effect as of pandas 0.16.1). After setting the display width, multiple Index and CategoricalIndex objects are constructed with varying data sizes and options. The output changes dynamically: small objects display on one line, larger ones wrap, and very large objects are truncated with head/tail display. This uses pandas, requires Python, and is intended to highlight the flexible, dynamic display behavior in output. Inputs are the constructors; outputs are the string representations affected by display settings and index type.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.1.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.width\", 80)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index(range(4), name=\"foo\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index(range(30), name=\"foo\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.Index(range(104), name=\"foo\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.CategoricalIndex([\"a\", \"bb\", \"ccc\", \"dddd\"], ordered=True, name=\"foobar\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.CategoricalIndex([\"a\", \"bb\", \"ccc\", \"dddd\"] * 10, ordered=True, name=\"foobar\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.CategoricalIndex([\"a\", \"bb\", \"ccc\", \"dddd\"] * 100, ordered=True, name=\"foobar\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.date_range(\"20130101\", periods=4, name=\"foo\", tz=\"US/Eastern\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.date_range(\"20130101\", periods=25, freq=\"D\")\n```\n\nLANGUAGE: python\nCODE:\n```\npd.date_range(\"20130101\", periods=104, name=\"foo\", tz=\"US/Eastern\")\n```\n\n----------------------------------------\n\nTITLE: Using non-monotonic PeriodIndex with partial string slicing in pandas\nDESCRIPTION: Demonstration of PeriodIndex supporting partial string slicing for non-monotonic indexes, similar to the DatetimeIndex behavior. Example shows creating period indexes and slicing by year and month.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndti = pd.date_range(\"2014-01-01\", periods=30, freq=\"30D\")\npi = dti.to_period(\"D\")\nser_monotonic = pd.Series(np.arange(30), index=pi)\nshuffler = list(range(0, 30, 2)) + list(range(1, 31, 2))\nser = ser_monotonic.iloc[shuffler]\nser\n```\n\nLANGUAGE: python\nCODE:\n```\nser[\"2014\"]\nser.loc[\"May 2015\"]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Copy Trigger on Shared Data in pandas (Python)\nDESCRIPTION: This snippet demonstrates how modifying a pandas DataFrame (df2) that shares memory with another (df) triggers a copy-on-write operation, due to an inplace mutation. The code constructs two DataFrames, with df2 created as a reset_index view of df (without copying), and then updates a single value. Dependencies: pandas must be installed (imported as pd). Key parameters include the reset_index method with drop=True and the iloc operation for element assignment. Outcome: After assignment, pandas must create a copy to prevent df from also being mutated; this illustrates CoW behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/copy_on_write.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\ndf2 = df.reset_index(drop=True)\ndf2.iloc[0, 0] = 100\n```\n\n----------------------------------------\n\nTITLE: Fixing Timedelta Construction with High Precision in Python\nDESCRIPTION: Bug fix for Timedelta construction with high precision integers, preventing rounding of Timedelta components.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nTimedelta(nanoseconds=1234567890123456789)  # Now preserves precision\n```\n\n----------------------------------------\n\nTITLE: Describing Series/DataFrame with Custom Percentiles (ipython, Python)\nDESCRIPTION: Demonstrates the use of pandas Series and DataFrame describe() method with a detailed set of percentiles, both in prior and updated versions. Shows outputs and error messages that occur when percentiles are duplicated, highlighting changes in error handling and output structure. Requires pandas library; key parameters are the 'percentiles' list, and expected output is a statistical summary or ValueError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_32\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: s.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[3]:\ncount     5.000000\nmean      2.000000\nstd       1.581139\nmin       0.000000\n0.0%      0.000400\n0.1%      0.002000\n0.1%      0.004000\n50%       2.000000\n99.9%     3.996000\n100.0%    3.998000\n100.0%    3.999600\nmax       4.000000\ndtype: float64\n\nIn [4]: df.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\nOut[4]:\n...\nValueError: cannot reindex from a duplicate axis\n```\n\nLANGUAGE: python\nCODE:\n```\ns.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\ndf.describe(percentiles=[0.0001, 0.0005, 0.001, 0.999, 0.9995, 0.9999])\n```\n\n----------------------------------------\n\nTITLE: Pandas Categorical Unique Values Example\nDESCRIPTION: Shows the old and new behavior of getting unique values from categorical data types in Pandas\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# returns a Categorical\npd.Series(list('baabc'), dtype='category').unique()\npd.unique(pd.Series(list('baabc'), dtype='category'))\n```\n\n----------------------------------------\n\nTITLE: Starting Cython Debugger in Docker\nDESCRIPTION: Commands to change to the debug build directory and start the Cython debugger (cygdb) for debugging Cython extensions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/debugging_extensions.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd debug\ncygdb\n```\n\n----------------------------------------\n\nTITLE: Printing a Tabular String Representation of a DataFrame Subset (Python)\nDESCRIPTION: Uses DataFrame.to_string to create a string representation of a DataFrame slice (last 20 rows, first 12 columns). Useful for wide DataFrames, but may not fit console width. Requires pandas, a populated DataFrame baseball.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nprint(baseball.iloc[-20:, :12].to_string())\n```\n\n----------------------------------------\n\nTITLE: Summarizing and Referencing pandas.Series API Methods in reStructuredText\nDESCRIPTION: This snippet demonstrates the use of Sphinx autosummary directives in reStructuredText for generating API documentation. It lists the full range of available methods, attributes, and accessors for pandas.Series, grouped by functionality (e.g., conversion, indexing, binary operators, statistics, missing data handling, accessors). The autosummary pattern requires Sphinx and pandas doc build dependencies. Inputs are method/class names, which reference pandas' Python internals, and outputs are standalone reStructuredText files or HTML for documentation sites. Limitations include the dependency on accurate underlying docstrings and the Sphinx build process.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/series.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n======\nSeries\n======\n.. currentmodule:: pandas\n\nConstructor\n-----------\n.. autosummary::\n   :toctree: api/\n\n   Series\n\nAttributes\n----------\n**Axes**\n\n.. autosummary::\n   :toctree: api/\n\n   Series.index\n   Series.array\n   Series.values\n   Series.dtype\n   Series.info\n   Series.shape\n   Series.nbytes\n   Series.ndim\n   Series.size\n   Series.T\n   Series.memory_usage\n   Series.hasnans\n   Series.empty\n   Series.dtypes\n   Series.name\n   Series.flags\n   Series.set_flags\n\nConversion\n----------\n.. autosummary::\n   :toctree: api/\n\n   Series.astype\n   Series.convert_dtypes\n   Series.infer_objects\n   Series.copy\n   Series.to_numpy\n   Series.to_period\n   Series.to_timestamp\n   Series.to_list\n   Series.__array__\n\nIndexing, iteration\n-------------------\n.. autosummary::\n   :toctree: api/\n\n   Series.get\n   Series.at\n   Series.iat\n   Series.loc\n   Series.iloc\n   Series.__iter__\n   Series.items\n   Series.keys\n   Series.pop\n   Series.item\n   Series.xs\n\nFor more information on ``.at``, ``.iat``, ``.loc``, and\n``.iloc``,  see the :ref:`indexing documentation <indexing>`.\n\nBinary operator functions\n-------------------------\n.. autosummary::\n   :toctree: api/\n\n   Series.add\n   Series.sub\n   Series.mul\n   Series.div\n   Series.truediv\n   Series.floordiv\n   Series.mod\n   Series.pow\n   Series.radd\n   Series.rsub\n   Series.rmul\n   Series.rdiv\n   Series.rtruediv\n   Series.rfloordiv\n   Series.rmod\n   Series.rpow\n   Series.combine\n   Series.combine_first\n   Series.round\n   Series.lt\n   Series.gt\n   Series.le\n   Series.ge\n   Series.ne\n   Series.eq\n   Series.product\n   Series.dot\n\nFunction application, GroupBy & window\n--------------------------------------\n.. autosummary::\n   :toctree: api/\n\n   Series.apply\n   Series.agg\n   Series.aggregate\n   Series.transform\n   Series.map\n   Series.groupby\n   Series.rolling\n   Series.expanding\n   Series.ewm\n   Series.pipe\n\n.. _api.series.stats:\n\nComputations / descriptive stats\n--------------------------------\n.. autosummary::\n   :toctree: api/\n\n   Series.abs\n   Series.all\n   Series.any\n   Series.autocorr\n   Series.between\n   Series.clip\n   Series.corr\n   Series.count\n   Series.cov\n   Series.cummax\n   Series.cummin\n   Series.cumprod\n   Series.cumsum\n   Series.describe\n   Series.diff\n   Series.factorize\n   Series.kurt\n   Series.max\n   Series.mean\n   Series.median\n   Series.min\n   Series.mode\n   Series.nlargest\n   Series.nsmallest\n   Series.pct_change\n   Series.prod\n   Series.quantile\n   Series.rank\n   Series.sem\n   Series.skew\n   Series.std\n   Series.sum\n   Series.var\n   Series.kurtosis\n   Series.unique\n   Series.nunique\n   Series.is_unique\n   Series.is_monotonic_increasing\n   Series.is_monotonic_decreasing\n   Series.value_counts\n\nReindexing / selection / label manipulation\n-------------------------------------------\n.. autosummary::\n   :toctree: api/\n\n   Series.align\n   Series.case_when\n   Series.drop\n   Series.droplevel\n   Series.drop_duplicates\n   Series.duplicated\n   Series.equals\n   Series.head\n   Series.idxmax\n   Series.idxmin\n   Series.isin\n   Series.reindex\n   Series.reindex_like\n   Series.rename\n   Series.rename_axis\n   Series.reset_index\n   Series.sample\n   Series.set_axis\n   Series.take\n   Series.tail\n   Series.truncate\n   Series.where\n   Series.mask\n   Series.add_prefix\n   Series.add_suffix\n   Series.filter\n\nMissing data handling\n---------------------\n.. autosummary::\n   :toctree: api/\n\n   Series.bfill\n   Series.dropna\n   Series.ffill\n   Series.fillna\n   Series.interpolate\n   Series.isna\n   Series.isnull\n   Series.notna\n   Series.notnull\n   Series.replace\n\nReshaping, sorting\n------------------\n.. autosummary::\n   :toctree: api/\n\n   Series.argsort\n   Series.argmin\n   Series.argmax\n   Series.reorder_levels\n   Series.sort_values\n   Series.sort_index\n   Series.swaplevel\n   Series.unstack\n   Series.explode\n   Series.searchsorted\n   Series.repeat\n   Series.squeeze\n\nCombining / comparing / joining / merging\n-----------------------------------------\n.. autosummary::\n   :toctree: api/\n\n   Series.compare\n   Series.update\n\nTime Series-related\n-------------------\n.. autosummary::\n   :toctree: api/\n\n   Series.asfreq\n   Series.asof\n   Series.shift\n   Series.first_valid_index\n   Series.last_valid_index\n   Series.resample\n   Series.tz_convert\n   Series.tz_localize\n   Series.at_time\n   Series.between_time\n\nAccessors\n---------\n\npandas provides dtype-specific methods under various accessors.\nThese are separate namespaces within :class:`Series` that only apply\nto specific data types.\n\n.. autosummary::\n    :toctree: api/\n    :nosignatures:\n    :template: autosummary/accessor.rst\n\n    Series.str\n    Series.cat\n    Series.dt\n    Series.sparse\n    DataFrame.sparse\n    Index.str\n\n\n=========================== =================================\nData Type                   Accessor\n=========================== =================================\nDatetime, Timedelta, Period :ref:`dt <api.series.dt>`\nString                      :ref:`str <api.series.str>`\nCategorical                 :ref:`cat <api.series.cat>`\nSparse                      :ref:`sparse <api.series.sparse>`\n=========================== =================================\n\n.. _api.series.dt:\n\nDatetimelike properties\n~~~~~~~~~~~~~~~~~~~~~~~\n\n``Series.dt`` can be used to access the values of the series as\ndatetimelike and return several properties.\nThese can be accessed like ``Series.dt.<property>``.\n\nDatetime properties\n^^^^^^^^^^^^^^^^^^^\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_attribute.rst\n\n   Series.dt.date\n   Series.dt.time\n   Series.dt.timetz\n   Series.dt.year\n   Series.dt.month\n   Series.dt.day\n   Series.dt.hour\n   Series.dt.minute\n   Series.dt.second\n   Series.dt.microsecond\n   Series.dt.nanosecond\n   Series.dt.dayofweek\n   Series.dt.day_of_week\n   Series.dt.weekday\n   Series.dt.dayofyear\n   Series.dt.day_of_year\n   Series.dt.days_in_month\n   Series.dt.quarter\n   Series.dt.is_month_start\n   Series.dt.is_month_end\n   Series.dt.is_quarter_start\n   Series.dt.is_quarter_end\n   Series.dt.is_year_start\n   Series.dt.is_year_end\n   Series.dt.is_leap_year\n   Series.dt.daysinmonth\n   Series.dt.days_in_month\n   Series.dt.tz\n   Series.dt.freq\n   Series.dt.unit\n\nDatetime methods\n^^^^^^^^^^^^^^^^\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_method.rst\n\n   Series.dt.isocalendar\n   Series.dt.to_period\n   Series.dt.to_pydatetime\n   Series.dt.tz_localize\n   Series.dt.tz_convert\n   Series.dt.normalize\n   Series.dt.strftime\n   Series.dt.round\n   Series.dt.floor\n   Series.dt.ceil\n   Series.dt.month_name\n   Series.dt.day_name\n   Series.dt.as_unit\n\nPeriod properties\n^^^^^^^^^^^^^^^^^\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_attribute.rst\n\n   Series.dt.qyear\n   Series.dt.start_time\n   Series.dt.end_time\n\nTimedelta properties\n^^^^^^^^^^^^^^^^^^^^\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_attribute.rst\n\n   Series.dt.days\n   Series.dt.seconds\n   Series.dt.microseconds\n   Series.dt.nanoseconds\n   Series.dt.components\n   Series.dt.unit\n\nTimedelta methods\n^^^^^^^^^^^^^^^^^\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_method.rst\n\n   Series.dt.to_pytimedelta\n   Series.dt.total_seconds\n   Series.dt.as_unit\n\n\n.. _api.series.str:\n\nString handling\n~~~~~~~~~~~~~~~\n\n``Series.str`` can be used to access the values of the series as\nstrings and apply several methods to it. These can be accessed like\n``Series.str.<function/property>``.\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_method.rst\n\n   Series.str.capitalize\n   Series.str.casefold\n   Series.str.cat\n   Series.str.center\n   Series.str.contains\n   Series.str.count\n   Series.str.decode\n   Series.str.encode\n   Series.str.endswith\n   Series.str.extract\n   Series.str.extractall\n   Series.str.find\n   Series.str.findall\n   Series.str.fullmatch\n   Series.str.get\n   Series.str.index\n   Series.str.join\n   Series.str.len\n   Series.str.ljust\n   Series.str.lower\n   Series.str.lstrip\n   Series.str.match\n   Series.str.normalize\n   Series.str.pad\n   Series.str.partition\n   Series.str.removeprefix\n   Series.str.removesuffix\n   Series.str.repeat\n   Series.str.replace\n   Series.str.rfind\n   Series.str.rindex\n   Series.str.rjust\n   Series.str.rpartition\n   Series.str.rstrip\n   Series.str.slice\n   Series.str.slice_replace\n   Series.str.split\n   Series.str.rsplit\n   Series.str.startswith\n   Series.str.strip\n   Series.str.swapcase\n   Series.str.title\n   Series.str.translate\n   Series.str.upper\n   Series.str.wrap\n   Series.str.zfill\n   Series.str.isalnum\n   Series.str.isalpha\n   Series.str.isdigit\n   Series.str.isspace\n   Series.str.islower\n   Series.str.isupper\n   Series.str.istitle\n   Series.str.isnumeric\n   Series.str.isdecimal\n   Series.str.get_dummies\n\n.. _api.series.cat:\n\nCategorical accessor\n~~~~~~~~~~~~~~~~~~~~\n\nCategorical-dtype specific methods and attributes are available under\nthe ``Series.cat`` accessor.\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_attribute.rst\n\n   Series.cat.categories\n   Series.cat.ordered\n   Series.cat.codes\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_method.rst\n\n   Series.cat.rename_categories\n   Series.cat.reorder_categories\n   Series.cat.add_categories\n   Series.cat.remove_categories\n   Series.cat.remove_unused_categories\n   Series.cat.set_categories\n   Series.cat.as_ordered\n   Series.cat.as_unordered\n\n\n.. _api.series.sparse:\n\nSparse accessor\n~~~~~~~~~~~~~~~\n\nSparse-dtype specific methods and attributes are provided under the\n``Series.sparse`` accessor.\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_attribute.rst\n\n   Series.sparse.npoints\n   Series.sparse.density\n   Series.sparse.fill_value\n   Series.sparse.sp_values\n\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/accessor_method.rst\n\n   Series.sparse.from_coo\n   Series.sparse.to_coo\n\n\n.. _api.series.list:\n\nList accessor\n~~~~~~~~~~~~~\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas in IPython\nDESCRIPTION: Suppressed import of all Pandas functions and classes in an IPython environment. This is used to set up the environment for the documentation examples.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas import *  # noqa F401, F403\n```\n\n----------------------------------------\n\nTITLE: Demonstrating RangeIndex Behavior in Pandas\nDESCRIPTION: Shows how RangeIndex is not preserved under filtering operations, resulting in conversion to Int64Index.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, 3])\n\nser[ser != 2].index\n```\n\n----------------------------------------\n\nTITLE: Column Assignment with Non-Unique Columns via isetitem - pandas, numpy (Python)\nDESCRIPTION: Demonstrates the use of DataFrame.isetitem to update a specific column by index in the presence of duplicate column names. This approach ensures only the intended column is modified, leaving other columns and series unaffected. pandas 1.5 or later is required for isetitem. Dependencies: pandas, numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: df_with_duplicated_cols = pd.concat([df, df], axis='columns')\nIn [3]: df_with_duplicated_cols.isetitem(0, new_prices)\nIn [4]: df_with_duplicated_cols.iloc[:, 0]\nOut[4]:\nbook1    98\nbook2    99\nName: price, dtype: int64\nIn [5]: original_prices\nOut[5]:\nbook1    11.1\nbook2    12.2\nName: 0, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Reduction Operations with Nullable IntegerArray Returning pandas.NA (Python, pandas 1.0.0)\nDESCRIPTION: Shows that performing reduction operations (e.g., sum) on Series with IntegerArray and skipna=False now returns pandas.NA instead of numpy.nan. Requires pandas and a defined array 'a'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npd.Series(a).sum(skipna=False)\n```\n\n----------------------------------------\n\nTITLE: Documenting NaT for Representing Missing Values in Datetime Types (RST)\nDESCRIPTION: RST documentation snippet that references the pandas NaT object, which is used to represent missing values for timedelta and datetime data types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/missing_value.rst#_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n   :template: autosummary/class_without_autosummary.rst\n\n   NaT\n```\n\n----------------------------------------\n\nTITLE: Basic Styler Initialization - Sub-optimal vs Optimized\nDESCRIPTION: Demonstrates the difference between default Styler initialization and optimized version without UUID and cell IDs for better performance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf4 = pd.DataFrame([[1, 2], [3, 4]])\ns4 = df4.style\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.io.formats.style import Styler\n\ns4 = Styler(df4, uuid_len=0, cell_ids=False)\n```\n\n----------------------------------------\n\nTITLE: Running Selected Pytest Unit Tests for Pandas (Bash)\nDESCRIPTION: Demonstrates how to invoke pytest to run a specific test file and filter tests matching a regular expression. Requires pytest and the pandas source tree; -k takes a substring or regex, and 'pandas/path/to/test.py' is the path to the test module. Output displays results for matching tests only.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\npytest pandas/path/to/test.py -k regex_matching_test_name\n```\n\n----------------------------------------\n\nTITLE: Documenting Method Returns and Related Methods in a Pandas Series Class (Python)\nDESCRIPTION: This snippet provides documentation for the head method on a Series class, returning the first 5 elements. The docstring demonstrates the 'Returns' section for specifying the output type and purpose, and the 'See Also' section for cross-referencing related methods (tail and iloc). The code sample highlights effective method documentation practices in a Pandas-like class context. Inputs: none. Outputs: first 5 Series elements. Useful as a baseline for class method documentation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass Series:\n    def head(self):\n        \"\"\"\n        Return the first 5 elements of the Series.\n\n        This function is mainly useful to preview the values of the\n        Series without displaying the whole of it.\n\n        Returns\n        -------\n        Series\n            Subset of the original series with the 5 first values.\n\n        See Also\n        --------\n        Series.tail : Return the last 5 elements of the Series.\n        Series.iloc : Return a slice of the elements in the Series,\n            which can also be used to return the first or last n.\n        \"\"\"\n        return self.iloc[:5]\n\n```\n\n----------------------------------------\n\nTITLE: Preserving Column Order when Creating DataFrame from List of Dicts - Pandas - Python\nDESCRIPTION: Demonstrates the effect of insertion-order preservation for columns in DataFrame construction from a list of dicts. Requires a list of dictionaries with overlapping and unique keys, and pandas version >= 0.25 running on Python >= 3.6.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ndata = [\\n    {'name': 'Joe', 'state': 'NY', 'age': 18},\\n    {'name': 'Jane', 'state': 'KY', 'age': 19, 'hobby': 'Minecraft'},\\n    {'name': 'Jean', 'state': 'OK', 'age': 20, 'finances': 'good'}\\n]\n```\n\n----------------------------------------\n\nTITLE: Indexing Series with Nullable Boolean Mask (Error in pandas 1.0.0-1.0.1) - pandas (Python)\nDESCRIPTION: This snippet demonstrates that, in pandas versions 1.0.0 and 1.0.1, indexing a Series using a nullable Boolean mask that contains NA raises a ValueError. This highlights an old limitation where NA values in boolean masks were not supported, with the expected output shown as a traceback. Appropriate for developers needing to understand error handling and regression fixes in later versions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.2.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> s[mask]\\nTraceback (most recent call last):\\n...\\nValueError: cannot mask with array containing NA / NaN values\n```\n\n----------------------------------------\n\nTITLE: Defining Pandas Version Format in Markdown\nDESCRIPTION: This snippet explains the format of pandas version numbers using semantic versioning.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0017-backwards-compatibility-and-deprecation-policy.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nA pandas release number is written in the format of ``MAJOR.MINOR.PATCH``.\n```\n\n----------------------------------------\n\nTITLE: Raising TypeError in DataFrame Append Operation (Python)\nDESCRIPTION: When appending a dictionary to a DataFrame without passing ignore_index=True, a TypeError is raised with a specific error message.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame().append({}, ignore_index=False)\n```\n\n----------------------------------------\n\nTITLE: Checking for Constant Values in Series - Handling NA Values\nDESCRIPTION: A variation of checking for constant values that first drops NA values. This approach is useful when missing values should be excluded from the constant value check.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_68\n\nLANGUAGE: python\nCODE:\n```\nv = s.dropna().to_numpy()\nis_constant = v.shape[0] == 0 or (s[0] == s).all()\n```\n\n----------------------------------------\n\nTITLE: Assigning to Series with Nullable Integer dtype in Python\nDESCRIPTION: Fixed regression in assigning to a Series using a nullable integer dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ns = pd.Series([1, 2, 3], dtype='Int64')\ns[0] = 4\n```\n\n----------------------------------------\n\nTITLE: Manual Incremental Reading from Stata Files with Iterator - Python\nDESCRIPTION: This code demonstrates fine-grained control over reading Stata .dta files by enabling iterator mode in pandas.read_stata and specifying the number of rows to read with each call to reader.read(). This approach lets you pull custom-sized chunks. Inputs: Stata .dta filename, iterator=True, chunk sizes per read(). Output: DataFrames for each chunk. Must be used as a context manager.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_238\n\nLANGUAGE: python\nCODE:\n```\nwith pd.read_stata(\"stata.dta\", iterator=True) as reader:\n    chunk1 = reader.read(5)\n    chunk2 = reader.read(5)\n```\n\n----------------------------------------\n\nTITLE: Unioning Categoricals with union_categoricals for Category Alignment - Pandas Python\nDESCRIPTION: Uses union_categoricals to combine categoricals with possibly non-identical categories, showing resulting category alignment and optional sorting of categories. Covers ordered categoricals, ignoring order flag, and effect on result dtype. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import union_categoricals\n\na = pd.Categorical([\"b\", \"c\"])\nb = pd.Categorical([\"a\", \"b\"])\nunion_categoricals([a, b])\n```\n\nLANGUAGE: python\nCODE:\n```\nunion_categoricals([a, b], sort_categories=True)\n```\n\nLANGUAGE: python\nCODE:\n```\na = pd.Categorical([\"a\", \"b\"], ordered=True)\nb = pd.Categorical([\"a\", \"b\", \"a\"], ordered=True)\nunion_categoricals([a, b])\n```\n\nLANGUAGE: python\nCODE:\n```\na = pd.Categorical([\"a\", \"b\"], ordered=True)\nb = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True)\nunion_categoricals([a, b])\n```\n\nLANGUAGE: python\nCODE:\n```\na = pd.Categorical([\"a\", \"b\", \"c\"], ordered=True)\nb = pd.Categorical([\"c\", \"b\", \"a\"], ordered=True)\nunion_categoricals([a, b], ignore_order=True)\n```\n\n----------------------------------------\n\nTITLE: Take First and Last Rows of Each Group - pandas Python\nDESCRIPTION: Creates a DataFrame, groups by column 'A', and demonstrates selecting the first (head) or last (tail) n rows from each group. Shows behavior of head(n) and tail(n) on a groupby object. Useful for inspection or filtering per group. Input: small dataset, Output: DataFrame with selected rows per group.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=[\"A\", \"B\"])\ndf\n```\n\nLANGUAGE: python\nCODE:\n```\ng = df.groupby(\"A\")\ng.head(1)\n```\n\nLANGUAGE: python\nCODE:\n```\ng.tail(1)\n```\n\n----------------------------------------\n\nTITLE: Grouping by Index Level Names in DataFrame.groupby - pandas - Python\nDESCRIPTION: Shows the creation of a MultiIndex DataFrame and using groupby to reference both index level names and columns as grouping criteria. Demonstrates summing grouped data. Requires pandas and numpy, creates a MultiIndex from arrays, then groups by index level and column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\narrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n          ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\n\nindex = pd.MultiIndex.from_arrays(arrays, names=['first', 'second'])\n\ndf = pd.DataFrame({'A': [1, 1, 1, 1, 2, 2, 3, 3],\n                   'B': np.arange(8)},\n                  index=index)\ndf\n\ndf.groupby(['second', 'A']).sum()\n```\n\n----------------------------------------\n\nTITLE: Preserving Dtype on DataFrame Assignment - pandas - Python\nDESCRIPTION: Illustrates new dtype preservation behavior when assigning slices to a DataFrame. Demonstrates creation of DataFrame with specified column dtypes, selection via boolean indexing, slice assignment, and dtype inspection before and after assignment. Requires pandas and numpy, inputs are DataFrame columns and indices, output is the DataFrame's dtypes and values, ensuring type is preserved unless value changes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'a': [0, 1, 1],\n                   'b': pd.Series([100, 200, 300], dtype='uint32')})\ndf.dtypes\nix = df['a'] == 1\ndf.loc[ix, 'b'] = df.loc[ix, 'b']\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Calculating String Length with LENGTHN and LENGTHC - SAS\nDESCRIPTION: Shows how to compute the length of a character string and its padded version using SAS built-in functions 'LENGTHN' (ignores trailing blanks) and 'LENGTHC' (includes them). This code uses 'put' to output lengths of 'time' column in the SAS log. Inputs: 'tips' dataset; outputs: string lengths in log.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_13\n\nLANGUAGE: SAS\nCODE:\n```\ndata _null_;\\nset tips;\\nput(LENGTHN(time));\\nput(LENGTHC(time));\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Styling Negative Values and Low Magnitude Values\nDESCRIPTION: Applies red coloring to negative values and reduces opacity for values between -0.3 and 0.3 using map() method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef style_negative(v, props=\"\"):\n    return props if v < 0 else None\n\ns2 = df2.style.map(style_negative, props=\"color:red;\").map(\n    lambda v: \"opacity: 20%;\" if (v < 0.3) and (v > -0.3) else None\n)\ns2\n```\n\n----------------------------------------\n\nTITLE: Removing Files Using Python's os Module\nDESCRIPTION: This snippet shows how to remove a file ('store.h5') from the filesystem using Python's built-in os module. The code first imports os and then deletes the specified file. Requires the os module (included in the Python standard library) and the target file must exist; otherwise, a FileNotFoundError may be raised.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nos.remove('store.h5')\n```\n\n----------------------------------------\n\nTITLE: Installing JupyterLite Core and Pyodide Kernel - Bash\nDESCRIPTION: This bash snippet installs the core JupyterLite package and its Pyodide kernel via pip. Dependencies include Python and pip, and it must be run in a shell environment with internet access. The commands ensure that the components required to build and run JupyterLite-based interactive environments are present. The primary expected output is the installation of both packages; errors may result if Python or pip are missing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/interactive_terminal/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install jupyterlite-core\npython -m pip install jupyterlite-pyodide-kernel\n```\n\n----------------------------------------\n\nTITLE: Importing pandas as pd in Python\nDESCRIPTION: This snippet imports the pandas library and assigns it the conventional alias 'pd'. This is required for all subsequent pandas operations, matching the standard convention in the pandas documentation. No external dependencies are needed beyond installing the pandas package.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/01_table_oriented.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: DataFrame Reset Index with CoW\nDESCRIPTION: Shows how reset_index operation works with Copy-on-Write and demonstrates data sharing behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/copy_on_write.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})\ndf2 = df.reset_index(drop=True)\ndf2.iloc[0, 0] = 100\n\ndf\ndf2\n```\n\n----------------------------------------\n\nTITLE: Single Character Regex Pattern Replacement (Python)\nDESCRIPTION: Demonstrates that single character patterns with regex=True in .str.replace() are treated as regular expressions rather than literals. Shows transformation where every character matching the pattern is replaced. Input is a string Series; output replaces all dots with 'a'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/text.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ns4 = pd.Series([\"a.b\", \".\", \"b\", np.nan, \"\"], dtype=\"string\")\ns4\ns4.str.replace(\".\", \"a\", regex=True)\n```\n\n----------------------------------------\n\nTITLE: Using groupby and transform Operations in pandas - Python\nDESCRIPTION: These ipython code snippets illustrate the old vs. new behavior when applying groupby and transform methods to pandas DataFrames with NaN values using the dropna parameter. They demonstrate how output changes when using built-in functions ('sum', 'ffill') versus custom lambda functions with transform, highlighting subtle differences in output shapes and NaN handling. Dependencies: pandas (imported as pd), numpy (imported as np). Main variables: df (DataFrame with possible NaN). Inputs are DataFrames; outputs are transformed DataFrames. Limitations are inherent to pandas version changes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_9\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: # Value in the last row should be np.nan\n        df.groupby('a', dropna=True).transform('sum')\nOut[3]:\n   b\n0  5\n1  5\n2  5\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: # Should have one additional row with the value np.nan\n        df.groupby('a', dropna=True).transform(lambda x: x.sum())\nOut[3]:\n   b\n0  5\n1  5\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: # The value in the last row is np.nan interpreted as an integer\n        df.groupby('a', dropna=True).transform('ffill')\nOut[3]:\n                     b\n0                    2\n1                    3\n2 -9223372036854775808\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: # Should have one additional row with the value np.nan\n        df.groupby('a', dropna=True).transform(lambda x: x)\nOut[3]:\n   b\n0  2\n1  3\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby('a', dropna=True).transform('sum')\ndf.groupby('a', dropna=True).transform(lambda x: x.sum())\ndf.groupby('a', dropna=True).transform('ffill')\ndf.groupby('a', dropna=True).transform(lambda x: x)\n```\n\n----------------------------------------\n\nTITLE: Getting End Time of a Period in Python\nDESCRIPTION: Shows how Period.end_time now returns the last nanosecond in the time interval in Pandas 0.9.1.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.9.1.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\np = pd.Period('2012')\n\np.end_time\n```\n\n----------------------------------------\n\nTITLE: Series Alignment Example\nDESCRIPTION: Demonstrates counter-intuitive Series index alignment behavior when adding two series with different indices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nser1 = pd.Series([1, 1, 1], index=[1, 2, 3])\nser2 = pd.Series([1, 1, 1], index=[3, 4, 5])\nprint(ser1 + ser2)\n```\n\n----------------------------------------\n\nTITLE: Comparing Period end_time and to_timestamp in pandas (Python)\nDESCRIPTION: Demonstrates the legacy behavior of pandas Period and PeriodIndex end_time attributes and to_timestamp method, showing how time values defaulted to '00:00:00' for Series.dt.end_time and end_time was handled differently for Period. Assumes import of pandas as pd. Inputs are Period and PeriodIndex objects; outputs are Timestamps. This behavior has been updated in newer pandas releases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: p = pd.Period('2017-01-01', 'D')\nIn [3]: pi = pd.PeriodIndex([p])\n\nIn [4]: pd.Series(pi).dt.end_time[0]\nOut[4]: Timestamp(2017-01-01 00:00:00)\n\nIn [5]: p.end_time\nOut[5]: Timestamp(2017-01-01 23:59:59.999999999)\n```\n\n----------------------------------------\n\nTITLE: Indexing DataFrame with MultiIndex using Label List in Python\nDESCRIPTION: Fixed performance regression when indexing a DataFrame or Series with a MultiIndex for the index using a list of labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndf.loc[['label1', 'label2']]\n```\n\n----------------------------------------\n\nTITLE: Using origin Parameter in to_datetime with pandas - Python\nDESCRIPTION: Demonstrates using the 'origin' parameter in pd.to_datetime() to set the reference date for integer date parsing. Shows both custom (1960-01-01) and default (unix epoch) origins. Inputs are list of integers, unit specifier, and optional origin. Outputs datetime Series. Requires pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime([1, 2, 3], unit='D', origin=pd.Timestamp('1960-01-01'))\n```\n\nLANGUAGE: python\nCODE:\n```\npd.to_datetime([1, 2, 3], unit='D')\n```\n\n----------------------------------------\n\nTITLE: Applying Binary Ufunc to Series with Different Indexes (Previous Behavior) - Pandas/Numpy - Python\nDESCRIPTION: Shows the pre-0.25 behavior for applying a numpy ufunc to two Series objects: the operation applies element-wise by position, ignoring alignment by index. The result is provided as a Series in input index order. Requires pandas, numpy, and that s1 and s2 exist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nIn [5]: np.power(s1, s2)\\nOut[5]:\\na      1\\nb     16\\nc    243\\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Series Multiplication with Timedelta Scalar in Python\nDESCRIPTION: Fixed regression in Series multiplication when multiplying a numeric Series with >10000 elements with a timedelta-like scalar.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nlarge_series * timedelta_scalar\n```\n\n----------------------------------------\n\nTITLE: Preserving Join Key Dtypes in pandas merge() Output - Python\nDESCRIPTION: Shows that merge() now preserves the data type of the join keys, unlike previous versions which might have upcast to float when introducing missing values. Requires pandas. Input: two DataFrames with integer join keys. Output: Merged DataFrame with proper key dtype, and dtypes inspection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame({\"key\": [1], \"v1\": [10]})\ndf1\ndf2 = pd.DataFrame({\"key\": [1, 2], \"v1\": [20, 30]})\ndf2\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge(df1, df2, how=\"outer\")\npd.merge(df1, df2, how=\"outer\").dtypes\n```\n\nLANGUAGE: python\nCODE:\n```\npd.merge(df1, df2, how=\"outer\", on=\"key\")\npd.merge(df1, df2, how=\"outer\", on=\"key\").dtypes\n```\n\n----------------------------------------\n\nTITLE: Uninstalling pandas before recompiling in Python\nDESCRIPTION: This command uninstalls the existing pandas installation before recompiling. This is necessary to avoid conflicts with the meson-python loader script.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip uninstall pandas\n```\n\n----------------------------------------\n\nTITLE: Creating and Manipulating Categoricals in pandas DataFrames - Python\nDESCRIPTION: This snippet demonstrates how to incorporate the new Categorical type in pandas Series and DataFrames, including conversion from object columns, renaming and reordering categories, and using category-based grouping. Dependencies include pandas >= 0.15.0. The key parameters involve the Series creation, category names, and ordering. Inputs are a DataFrame with raw grade data; outputs include a DataFrame with categoricals, sorted results, and group counts. Note: Previous two-argument construction is unsupported; use new API methods instead.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"id\": [1, 2, 3, 4, 5, 6],\n                   \"raw_grade\": ['a', 'b', 'b', 'a', 'a', 'e']})\n\ndf[\"grade\"] = df[\"raw_grade\"].astype(\"category\")\ndf[\"grade\"]\n\n# Rename the categories\ndf[\"grade\"] = df[\"grade\"].cat.rename_categories([\"very good\", \"good\", \"very bad\"])\n\n# Reorder the categories and simultaneously add the missing categories\ndf[\"grade\"] = df[\"grade\"].cat.set_categories([\"very bad\", \"bad\",\n                                                  \"medium\", \"good\", \"very good\"])\ndf[\"grade\"]\ndf.sort_values(\"grade\")\ndf.groupby(\"grade\", observed=False).size()\n```\n\n----------------------------------------\n\nTITLE: Wrapping pyarrow.Array/ChunkedArray into ArrowExtensionArray (pandas Python API)\nDESCRIPTION: Demonstrates using ArrowExtensionArray to wrap an existing pyarrow.Array or pyarrow.ChunkedArray and create corresponding pandas Series. This enables working with Arrow arrays directly within pandas while retaining backing by the Arrow memory layout. Requires pandas and pyarrow; inputs are pyarrow arrays with possibly complex item types. The output is a pandas Series with ArrowExtensionArray dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/pyarrow.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npa_array = pa.array(\n    [{\"1\": \"2\"}, {\"10\": \"20\"}, None],\n    type=pa.map_(pa.string(), pa.string()),\n)\nser = pd.Series(pd.arrays.ArrowExtensionArray(pa_array))\nser\n```\n\n----------------------------------------\n\nTITLE: Slicing DataFrame Columns in R\nDESCRIPTION: Demonstrates how to access data.frame columns by name and integer location in R.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_0\n\nLANGUAGE: r\nCODE:\n```\ndf <- data.frame(a=rnorm(5), b=rnorm(5), c=rnorm(5), d=rnorm(5), e=rnorm(5))\ndf[, c(\"a\", \"c\", \"e\")]\n```\n\nLANGUAGE: r\nCODE:\n```\ndf <- data.frame(matrix(rnorm(1000), ncol=100))\ndf[, c(1:10, 25:30, 40, 50:100)]\n```\n\n----------------------------------------\n\nTITLE: Multiple Scatter Plots on Same Axes\nDESCRIPTION: This example demonstrates how to plot multiple scatter plots on the same axes by specifying the ax parameter. Color and label parameters help distinguish between the groups.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nax = df.plot.scatter(x=\"a\", y=\"b\", color=\"DarkBlue\", label=\"Group 1\")\ndf.plot.scatter(x=\"c\", y=\"d\", color=\"DarkGreen\", label=\"Group 2\", ax=ax);\n```\n\n----------------------------------------\n\nTITLE: Serializing Table Schema DataFrame to JSON Table Schema with ntv_pandas in Python\nDESCRIPTION: This code snippet shows how to convert a Table Schema-annotated DataFrame into a JSON Table Schema using ntv_pandas, with a schema section detailing field types and formats, and a data section for records. It demonstrates ntv_pandas' capacity for extended typing and compatibility with Table Schema. Dependencies include pandas and ntv_pandas. Inputs are a Table Schema DataFrame, and the output is a JSON dictionary structured according to the standard.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0012-compact-and-reversible-JSON-interface.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nIn [4]: df_to_table = npd.to_json(df, table=True)\n        pprint(df_to_table, width=140, sort_dicts=False)\nOut[4]: {'schema': {'fields': [{'name': 'index', 'type': 'integer'},\n                               {'name': 'end february', 'type': 'date'},\n                               {'name': 'coordinates', 'type': 'geopoint', 'format': 'array'},\n                               {'name': 'contact', 'type': 'string', 'format': 'email'}],\n                    'primaryKey': ['index'],\n                    'pandas_version': '1.4.0'},\n         'data': [{'index': 0, 'end february': '2023-02-28', 'coordinates': [2.3, 48.9], 'contact': 'john.doe@table.com'},\n                  {'index': 1, 'end february': '2024-02-29', 'coordinates': [5.4, 43.3], 'contact': 'lisa.minelli@schema.com'},\n                  {'index': 2, 'end february': '2025-02-28', 'coordinates': [4.9, 45.8], 'contact': 'walter.white@breaking.com'}]}\n```\n\n----------------------------------------\n\nTITLE: Utility Method for Array Indexing\nDESCRIPTION: This utility method is used to ensure correct behavior when indexing arrays in custom pandas extensions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/extensions.rst#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\napi.indexers.check_array_indexer\n```\n\n----------------------------------------\n\nTITLE: Converting a Panel to Xarray DataArray - Pandas - Python\nDESCRIPTION: Illustrates converting a Panel object to an xarray DataArray using the .to_xarray() method. Depends on xarray being installed and available. Facilitates migration to xarray for n-dimensional analytics. Input: Panel p; Output: DataArray with labeled axes. Limitations: Panel is deprecated and this method will be removed in the future.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_47\n\nLANGUAGE: python\nCODE:\n```\np.to_xarray()\n```\n\n----------------------------------------\n\nTITLE: Drawing Area Plots as Subplots with pandas and Matplotlib in Python\nDESCRIPTION: This snippet creates an area plot for each column of the DataFrame in separate subplots. It calls air_quality.plot.area with figsize set for width/height and subplots=True to generate axes per column. plt.show() renders the plots. The function returns a sequence of matplotlib Axes objects. Requires numeric columns in the DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/04_plotting.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\naxs = air_quality.plot.area(figsize=(12, 4), subplots=True)\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining pandas.Series method: groupby_mean in Python\nDESCRIPTION: Defines a method for grouping a Series by index and calculating the mean for each group. Relies on pandas and numpy. Illustrates grouping by repeated string indices and aggregating numeric values, then outputs a Series indexed by group names with computed means.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef groupby_mean(self):\n    \"\"\"\n    Group by index and return mean.\n\n    Examples\n    --------\n    >>> ser = pd.Series([380., 370., 24., 26],\n    ...               name='max_speed',\n    ...               index=['falcon', 'falcon', 'parrot', 'parrot'])\n    >>> ser.groupby_mean()\n    index\n    falcon    375.0\n    parrot     25.0\n    Name: max_speed, dtype: float64\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Reading XML with Custom Data Types and Converters\nDESCRIPTION: Shows how to use the new dtype, converters, and parse_dates parameters in read_xml() for type control.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom io import StringIO\nxml_dates = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n<data>\n  <row>\n    <shape>square</shape>\n    <degrees>00360</degrees>\n    <sides>4.0</sides>\n    <date>2020-01-01</date>\n   </row>\n  <row>\n    <shape>circle</shape>\n    <degrees>00360</degrees>\n    <sides/>\n    <date>2021-01-01</date>\n  </row>\n  <row>\n    <shape>triangle</shape>\n    <degrees>00180</degrees>\n    <sides>3.0</sides>\n    <date>2022-01-01</date>\n  </row>\n</data>\"\"\"\n\ndf = pd.read_xml(\n    StringIO(xml_dates),\n    dtype={'sides': 'Int64'},\n    converters={'degrees': str},\n    parse_dates=['date']\n)\ndf\ndf.dtypes\n```\n\n----------------------------------------\n\nTITLE: Indexing Series with iloc and Floating Point (Deprecation Example) - pandas ipython\nDESCRIPTION: Provides an example of a future warning when using iloc with a floating-point index value for a Series with Int64Index. Emphasizes that iloc and related indexers will soon require integer indices for non-float-indexed Series, reflecting stricter future behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_23\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: pd.Series(1, np.arange(5)).iloc[3.0]\n        pandas/core/index.py:469: FutureWarning: scalar indexers for index type Int64Index should be integers and not floating point\n```\n\n----------------------------------------\n\nTITLE: GroupBy Apply with Non-pandas Return in Python\nDESCRIPTION: Fixed regression in .DataFrameGroupBy.apply and .SeriesGroupBy.apply when called with a function that returned a non-pandas non-scalar object (e.g. a list or numpy array).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.1.rst#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndf.groupby('column').apply(lambda x: [1, 2, 3])\n```\n\n----------------------------------------\n\nTITLE: Using SemiMonthEnd for Date Arithmetic and Ranges - Pandas - Python\nDESCRIPTION: Illustrates how to use SemiMonthEnd offset for adding to a pandas Timestamp and generating a date range with semi-monthly frequency. The examples output Timestamp or DatetimeIndex reflecting the SemiMonthEnd logic, anchored by default to the 15th. Only pandas is required; parameters include the start date, frequency string, and periods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nIn [46]: pd.Timestamp(\"2016-01-01\") + SemiMonthEnd()\nOut[46]: Timestamp('2016-01-15 00:00:00')\n\nIn [47]: pd.date_range(\"2015-01-01\", freq=\"SM\", periods=4)\nOut[47]: DatetimeIndex(['2015-01-15', '2015-01-31', '2015-02-15', '2015-02-28'], dtype='datetime64[ns]', freq='SM-15')\n```\n\n----------------------------------------\n\nTITLE: Restoring DataFrame from Table Schema JSON and Checking Equality in Python\nDESCRIPTION: This snippet recovers a pandas DataFrame from a Table Schema-compliant JSON object produced by ntv_pandas and checks for equality with the original DataFrame. The process uses npd.read_json and df.equals, showcasing reversibility and type preservation. Dependencies include pandas and ntv_pandas. Input is the Table Schema JSON, and the output is the reconstructed DataFrame and a check for identity.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0012-compact-and-reversible-JSON-interface.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nIn [5]: df_from_table = npd.read_json(df_to_table)\n        print('df created from JSON is equal to initial df ? ', df_from_table.equals(df))\nOut[5]: df created from JSON is equal to initial df ?  True\n```\n\n----------------------------------------\n\nTITLE: Sticky MultiIndex Levels in DataFrames with Pandas Styler (Python)\nDESCRIPTION: This snippet shows how to set sticky headers for specific levels in a MultiIndex DataFrame using pandas Styler's set_sticky method. It first assigns a MultiIndex to bigdf, then makes index levels 1 and 2 sticky with a given pixel size. Requires pandas with MultiIndex support. Inputs are new index values and sticky configuration, output is a DataFrame display with persistent multi-level column headers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/style.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nbigdf.index = pd.MultiIndex.from_product([[\"A\", \"B\"], [0, 1], [0, 1, 2, 3]])\nbigdf.style.set_sticky(axis=\"index\", pixel_size=18, levels=[1, 2])\n```\n\n----------------------------------------\n\nTITLE: Demonstrating MultiIndex Indexing Beyond Lexsort Depth in Python\nDESCRIPTION: Shows how indexing in MultiIndex now supports beyond lex-sort depth, with a performance warning. Includes example of creating a DataFrame with MultiIndex and accessing data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.2.rst#_snippet_0\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: df = pd.DataFrame({'jim':[0, 0, 1, 1],\n   ...:                    'joe':['x', 'x', 'z', 'y'],\n   ...:                    'jolie':np.random.rand(4)}).set_index(['jim', 'joe'])\n   ...: \n\nIn [2]: df\nOut[2]: \n            jolie\njim joe          \n0   x    0.126970\n    x    0.966718\n1   z    0.260476\n    y    0.897237\n\n[4 rows x 1 columns]\n\nIn [3]: df.index.lexsort_depth\nOut[3]: 1\n\n# in prior versions this would raise a KeyError\n# will now show a PerformanceWarning\nIn [4]: df.loc[(1, 'z')]\nOut[4]: \n            jolie\njim joe          \n1   z    0.260476\n\n[1 rows x 1 columns]\n\n# lexically sorting\nIn [5]: df2 = df.sort_index()\n\nIn [6]: df2\nOut[6]: \n            jolie\njim joe          \n0   x    0.126970\n    x    0.966718\n1   y    0.897237\n    z    0.260476\n\n[4 rows x 1 columns]\n\nIn [7]: df2.index.lexsort_depth\nOut[7]: 2\n\nIn [8]: df2.loc[(1,'z')]\nOut[8]: \n            jolie\njim joe          \n1   z    0.260476\n\n[1 rows x 1 columns]\n```\n\n----------------------------------------\n\nTITLE: Adding and Subtracting Timedeltas from PeriodIndex - Pandas Python\nDESCRIPTION: Illustrates supported addition/subtraction operations between pandas PeriodIndex objects and timedelta-like objects (e.g., pd.offsets.Hour, pd.Timedelta, pd.offsets.MonthEnd). Requires PeriodIndex and suitable offsets/timedeltas. Key parameters include the PeriodIndex frequency and type of offset. The inputs are a PeriodIndex and an offset/timedelta; the output is a new PeriodIndex with the adjusted values. Some combinations are only permitted when resulting frequencies are compatible.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nidx = pd.period_range('2014-07-01 09:00', periods=5, freq='H')\n\nidx\n# Out:\n# PeriodIndex(['2014-07-01 09:00', '2014-07-01 10:00', '2014-07-01 11:00',\n#              '2014-07-01 12:00', '2014-07-01 13:00'],\n#             dtype='period[H]')\n\nidx + pd.offsets.Hour(2)\n# Out:\n# PeriodIndex(['2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00',\n#              '2014-07-01 14:00', '2014-07-01 15:00'],\n#             dtype='period[H]')\n\nidx + pd.Timedelta('120m')\n# Out:\n# PeriodIndex(['2014-07-01 11:00', '2014-07-01 12:00', '2014-07-01 13:00',\n#              '2014-07-01 14:00', '2014-07-01 15:00'],\n#             dtype='period[H]')\n\nidx = pd.period_range('2014-07', periods=5, freq='M')\n\nidx\n# Out: PeriodIndex(['2014-07', '2014-08', '2014-09', '2014-10', '2014-11'], dtype='period[M]')\n\nidx + pd.offsets.MonthEnd(3)\n# Out: PeriodIndex(['2014-10', '2014-11', '2014-12', '2015-01', '2015-02'], dtype='period[M]')\n```\n\n----------------------------------------\n\nTITLE: Boolean indexing of Series with non-numeric index in Python\nDESCRIPTION: Demonstrates that boolean indexing of a pandas Series with a non-numeric index is now treated as a boolean indexer rather than raising a KeyError.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\ns\ns.loc[pd.Index([True, False, True])]\n```\n\n----------------------------------------\n\nTITLE: Reading OpenDocument Spreadsheets with Pandas\nDESCRIPTION: Demonstrates how to read data from OpenDocument spreadsheet (.ods) files using pandas with the odf engine.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_148\n\nLANGUAGE: python\nCODE:\n```\n# Returns a DataFrame\npd.read_excel(\"path_to_file.ods\", engine=\"odf\")\n```\n\n----------------------------------------\n\nTITLE: Positional Float Indexing with .ix in pandas Series - IPython\nDESCRIPTION: Demonstrates that assigning to a Series with .ix and a float indexer adds the float as a new index, rather than positional assignment. Presents the resulting Series with mixed-type indices. .ix is deprecated; recommends using explicit label or position indexers.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.0.rst#_snippet_25\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: s2.ix[1.0] = 10\nIn [4]: s2\nOut[4]:\na       1\nb       2\nc       3\n1.0    10\ndtype: int64\n```\n\n----------------------------------------\n\nTITLE: Using Deprecated and Updated Plotting APIs - Pandas - Python\nDESCRIPTION: Compares deprecated plotting function calls (pandas.tools.plotting.scatter_matrix and pandas.scatter_matrix) to the recommended usage via pandas.plotting. Requires pandas as pd; DataFrame df as input; outputs scatter matrix plots. Users should migrate to the top-level pandas.plotting module.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\npd.tools.plotting.scatter_matrix(df)\npd.scatter_matrix(df)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.plotting.scatter_matrix(df)\n```\n\n----------------------------------------\n\nTITLE: Extracting a Substring by Position using SUBSTR in SAS\nDESCRIPTION: Demonstrates use of SAS's 'substr' function to extract part of a string (the first character of 'sex') and output via 'put' statement. Inputs: 'tips' dataset, target string; Outputs: substring printed to log. Useful for parsing or splitting fields.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_15\n\nLANGUAGE: SAS\nCODE:\n```\ndata _null_;\\nset tips;\\nput(substr(sex,1,1));\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Label-based slicing with sorted index in Python\nDESCRIPTION: Example showing that label-based slicing with endpoints not in the index works when the index is sorted (monotonic), as it can determine the appropriate range.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nIn [4]: s2 = s.sort_index()\n\nIn [5]: s2\nOut[5]:\na    1.628992\nc   -0.539890\ne    0.073308\ng   -1.182230\nk   -0.243550\nm   -0.276183\ndtype: float64\n\nIn [6]: s2.ix['b':'h']\nOut[6]:\nc   -0.539890\ne    0.073308\ng   -1.182230\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Installing pandas with Debug Symbols in Python\nDESCRIPTION: Command to install pandas from source with debug symbols. This generates a development build suitable for debugging.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/debugging_extensions.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install -ve . --no-build-isolation -Cbuilddir=\"debug\" -Csetup-args=\"-Dbuildtype=debug\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Reference Tracking for Copy on Write in Pandas\nDESCRIPTION: This snippet explains the reference tracking mechanism used in Pandas to implement Copy on Write. It describes the use of BlockValuesRefs object to track shared memory between blocks using weak references.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/copy_on_write.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BlockValuesRefs:\n    # Implementation details not provided in the snippet\n    pass\n\nclass Block:\n    def copy(self, deep=False):\n        # Implementation of shallow copy\n        pass\n\nclass BlockManager:\n    # Implementation details not provided in the snippet\n    pass\n\nclass DataFrame:\n    # Implementation details not provided in the snippet\n    pass\n\nclass Series:\n    # Implementation details not provided in the snippet\n    pass\n```\n\n----------------------------------------\n\nTITLE: Importing All Pandas Namespace for Documentation - Python\nDESCRIPTION: This snippet imports all public objects from the pandas namespace using a wildcard import. It is used within a Sphinx documentation context to suppress output and set up the environment for code examples and doctests. The 'noqa F401, F403' comment suppresses linter warnings related to unused imports and wildcard imports. No inputs or outputs are expected; it is intended for internal documentation setup only.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.2.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas import *  # noqa F401, F403\n```\n\n----------------------------------------\n\nTITLE: Listing First Observation per Group with bysort - Stata\nDESCRIPTION: Illustrates how to use 'bysort' in Stata to list the first observation in each (sex, smoker) group according to sort order. The special variable _n refers to the position within each group. Appropriate for analyzing or extracting group representatives. Requires sex and smoker columns in the dataset.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_24\n\nLANGUAGE: stata\nCODE:\n```\nbysort sex smoker: list if _n == 1\n```\n\n----------------------------------------\n\nTITLE: Current Polars-to-Pandas Conversion Workflow (Python)\nDESCRIPTION: Displays the direct conversion from pandas DataFrame to Polars DataFrame, applying a query in-between. This snippet depends on the polars and pandas libraries, and demonstrates current cross-library DataFrame conversion. Inputs are a DataFrame and filter condition; output is a filtered Polars DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npolars.DataFrame(df.to_pandas()\n                   .query('my_col > 0'))\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrames with Explicit Data Types - Pandas - Python\nDESCRIPTION: Demonstrates constructing DataFrames and Series with specific numeric dtypes (float64, float32, uint8) and examines their preservation during operations such as reindexing, filling, and arithmetic addition. Requires pandas and NumPy installations; df1, df2, and df3 all illustrate how dtype information is maintained or upcast during operations. Shows how to inspect column dtypes of each DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf1 = pd.DataFrame(np.random.randn(8, 1), columns=['A'], dtype='float64')\ndf1\ndf1.dtypes\ndf2 = pd.DataFrame({'A': pd.Series(np.random.randn(8), dtype='float32'),\n                   'B': pd.Series(np.random.randn(8)),\n                   'C': pd.Series(range(8), dtype='uint8')})\ndf2\ndf2.dtypes\n\n# here you get some upcasting\ndf3 = df1.reindex_like(df2).fillna(value=0.0) + df2\ndf3\ndf3.dtypes\n```\n\n----------------------------------------\n\nTITLE: Forcing Date Coercion in Series - Pandas - Python\nDESCRIPTION: Illustrates converting mixed-type object Series entries to datetime, with the non-datelike entries becoming NaT (Not a Time) when convert_dates='coerce' is used. Shows how to include a mix of datetime, strings, and numbers in a Series and force datetime inference. Dependencies: pandas, NumPy, datetime.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_3\n\nLANGUAGE: ipython\nCODE:\n```\nIn [18]: import datetime\n\nIn [19]: s = pd.Series([datetime.datetime(2001, 1, 1, 0, 0), 'foo', 1.0, 1,\n   ....:                pd.Timestamp('20010104'), '20010105'], dtype='O')\n   ....:\n\nIn [20]: s.convert_objects(convert_dates='coerce')\nOut[20]:\n0   2001-01-01\n1          NaT\n2          NaT\n3          NaT\n4   2001-01-04\n5   2001-01-05\ndtype: datetime64[ns]\n```\n\n----------------------------------------\n\nTITLE: Converting DatetimeIndex to datetime.datetime Array Using to_pydatetime - Pandas - Python\nDESCRIPTION: This snippet converts a Pandas DatetimeIndex (e.g., rng) into a numpy array of built-in Python datetime.datetime objects using the to_pydatetime() method. This is useful for maximum compatibility with libraries such as matplotlib, as it expects datetime.datetime objects rather than pandas.Timestamp. Required dependencies: pandas. Inputs: a DatetimeIndex; Outputs: a numpy array of datetime.datetime objects. No parameters are required for to_pydatetime(). This approach is preferred if third-party libraries have limited support for pandas's own types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.8.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndt_array = rng.to_pydatetime()\ndt_array\ndt_array[5]\n```\n\n----------------------------------------\n\nTITLE: Stricter Type Checking in Series.str Accessor (Python)\nDESCRIPTION: Shows the new, stricter dtype inference in the Pandas str accessor for Series. Series containing only 'bytes' data will now raise exceptions for some string methods (except specific ones), which was not enforced previously. Requires Pandas and NumPy. Demonstrates creation and attempted string matching over a Series with byte strings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_7\n\nLANGUAGE: ipython\nCODE:\n```\ns = pd.Series(np.array(['a', 'ba', 'cba'], 'S'), dtype=object)\ns\n```\n\nLANGUAGE: ipython\nCODE:\n```\ns.str.startswith(b'a')\n```\n\n----------------------------------------\n\nTITLE: Using Callable with Indexing Methods in Pandas\nDESCRIPTION: Shows how to use callable functions with loc[], iloc[], and ix[] indexing methods in Pandas for more flexible data selection.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.18.1.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.loc[lambda x: x.A >= 2, lambda x: x.sum() > 10]\n\ndf.loc[lambda x: [1, 2], lambda x: [\"A\", \"B\"]]\n\ndf[lambda x: \"A\"]\n```\n\n----------------------------------------\n\nTITLE: Creating and Accessing a DataFrame with DatetimeIndex Using .loc\nDESCRIPTION: Example showing how .loc is strict with incompatible slicers, raising TypeError when using integers with DatetimeIndex. It demonstrates creating a DataFrame with a DatetimeIndex and attempting to slice with integer indices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndfl = pd.DataFrame(np.random.randn(5, 4),\n                 columns=list('ABCD'),\n                 index=pd.date_range('20130101', periods=5))\ndfl\ndfl.loc[2:3]\n```\n\n----------------------------------------\n\nTITLE: Creating SparseDtype with DateTime64 Type - pandas - Python\nDESCRIPTION: Constructs a pandas SparseDtype for datetime64[ns], specifying only the data type. An appropriate default fill value is used for missing datetime data. Dependency: pandas. Outputs a SparseDtype usable for Series, DataFrame conversions or type annotations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/sparse.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npd.SparseDtype(np.dtype('datetime64[ns]'))\n```\n\n----------------------------------------\n\nTITLE: Creating Categorical Data in Pandas\nDESCRIPTION: This snippet illustrates the use of pd.Categorical to create a categorical representation of a series. It converts the same mixed-type series used in the factorize example into a categorical object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\npd.Categorical(x)\n```\n\n----------------------------------------\n\nTITLE: Explicit Object Inference and Downcasting Options - pandas - IPython\nDESCRIPTION: These snippets illustrate how to explicitly control object inference and opt into the new non-silent downcasting behavior. They use the infer_objects method for explicit conversion and the set_option function to toggle future downcasting handling. Requires pandas library. Inputs are Series or DataFrames possibly containing object dtype columns; outputs are objects with updated or unchanged dtypes as specified. Users are encouraged to adopt explicit data type conversions for forward compatibility.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.2.0.rst#_snippet_16\n\nLANGUAGE: ipython\nCODE:\n```\nresult = result.infer_objects(copy=False)\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [9]: pd.set_option(\"future.no_silent_downcasting\", True)\n```\n\n----------------------------------------\n\nTITLE: Clipboard Data Format Example in Console\nDESCRIPTION: Shows the text format that can be copied to clipboard and imported as a DataFrame using read_clipboard. The data is tab-separated with row and column labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_152\n\nLANGUAGE: console\nCODE:\n```\n  A B C\nx 1 4 p\ny 2 5 q\nz 3 6 r\n```\n\n----------------------------------------\n\nTITLE: Setting Up MultiIndex Series and Grouping by Level (Python)\nDESCRIPTION: Creates a pandas Series with a two-level MultiIndex and shows how to group by the top-level index with groupby(level=0). Sums are then computed for each group. This is essential for multi-dimensional hierarchical data in pandas. Requires pandas (as pd) and numpy (as np) to be imported.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\narrays = [\n    [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n    [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n]\nindex = pd.MultiIndex.from_arrays(arrays, names=[\"first\", \"second\"])\ns = pd.Series(np.random.randn(8), index=index)\ns\n```\n\nLANGUAGE: python\nCODE:\n```\ngrouped = s.groupby(level=0)\ngrouped.sum()\n```\n\n----------------------------------------\n\nTITLE: Creating and Using CategoricalIndex in Pandas\nDESCRIPTION: Demonstrates how to create and work with CategoricalIndex, including indexing, sorting and groupby operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': np.arange(6),\n                   'B': pd.Series(list('aabbca'))\n                          .astype('category', categories=list('cab'))\n                   })\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2 = df.set_index('B')\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2.loc['a']\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2.sort_index()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2.groupby(level=0).sum()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf2.reindex(['a', 'e'])\n```\n\n----------------------------------------\n\nTITLE: Ignoring warnings in Python tests using pytest\nDESCRIPTION: Example of how to ignore specific warnings in tests using pytest.mark.filterwarnings decorator.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.filterwarnings(\"ignore:msg:category\")\ndef test_thing(self):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Building the JupyterLite Application - Bash\nDESCRIPTION: This bash command builds the JupyterLite application assets in the 'web/interactive_terminal' directory. It requires that 'jupyterlite' is already installed, and should be executed within the project subdirectory where the JupyterLite configuration files are present. The output is a set of static assets ready for web deployment; misconfiguration or missing dependencies may result in build errors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/interactive_terminal/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\njupyter lite build\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table with Row Skipping in Python\nDESCRIPTION: Shows how to skip rows when reading HTML tables with pandas.read_html(). You can skip a specified number of rows or use a range to skip multiple rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_95\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, skiprows=0)\n```\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, skiprows=range(2))\n```\n\n----------------------------------------\n\nTITLE: Fixing DataFrame Matrix Multiplication\nDESCRIPTION: Addresses a bug where Series __rmatmul__ didn't support matrix vector multiplication.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_59\n\nLANGUAGE: Python\nCODE:\n```\nmatrix @ series\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table with NaN Value Handling in Python\nDESCRIPTION: Shows how to specify values that should be converted to NaN when reading HTML tables with pandas.read_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_97\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, na_values=[\"No Acquirer\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ndfs = pd.read_html(url, keep_default_na=False)\n```\n\n----------------------------------------\n\nTITLE: Reading HTML Table with Attribute Selection in Python\nDESCRIPTION: Demonstrates how to select HTML tables based on HTML attributes like id or class using pandas.read_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_96\n\nLANGUAGE: python\nCODE:\n```\ndfs1 = pd.read_html(url, attrs={\"id\": \"table\"})\ndfs2 = pd.read_html(url, attrs={\"class\": \"sortable\"})\nprint(np.array_equal(dfs1[0], dfs2[0]))  # Should be True\n```\n\n----------------------------------------\n\nTITLE: Inspecting DataFrame Array Dtype - Pandas - Python\nDESCRIPTION: Retrieves the underlying Numpy dtype of the values array in a DataFrame (df3.values.dtype), illustrating how upcasting/conversion during DataFrame operations can affect array representation. Requires pandas and NumPy. Returns the dtype of the internal Numpy array resulting from previous DataFrame calculations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf3.values.dtype\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame and Using HDFStore Read/Write API - Pandas - Python\nDESCRIPTION: Illustrates the creation of a simple DataFrame and notes (in context) the enhanced HDFStore API, including support for read_hdf/to_hdf methods similar to read_csv/to_csv. Prepares for subsequent persistence usage with HDF5 formats; dependencies: pandas, h5py, tables. DataFrame contains columns A and B with integer sequences.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.11.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': range(5), 'B': range(5)})\n```\n\n----------------------------------------\n\nTITLE: Ranking DataFrame with NA Handling Options in Python\nDESCRIPTION: Shows how to use the new na_option parameter in DataFrame.rank() to assign either the largest or smallest rank to missing values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.9.1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(6, 3), columns=['A', 'B', 'C'])\n\ndf.loc[2:4] = np.nan\n\ndf.rank()\n\ndf.rank(na_option='top')\n\ndf.rank(na_option='bottom')\n```\n\n----------------------------------------\n\nTITLE: Reducing across all axes with DataFrame.all in pandas (Python)\nDESCRIPTION: Demonstrates the use of DataFrame.all with axis=None, a feature added in pandas 0.23.2 to allow logical reduction over all axes, returning a single boolean scalar value. Depends on pandas (imported as pd), and requires that the DataFrame contains logical (boolean or numerically truthy) values. The axis=None parameter indicates reduction across the entire DataFrame, producing a single truth value. The input is a DataFrame and the output is a boolean scalar. Requires pandas >= 0.23.2 for axis=None support.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.2.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": [1, 2], \"B\": [True, False]})\ndf.all(axis=None)\n\n```\n\n----------------------------------------\n\nTITLE: Offset Normalization and Usage of the 'normalize' Keyword in Pandas - Python\nDESCRIPTION: This snippet demonstrates the use of the normalize keyword with Pandas offset objects as of version 0.14.1. It shows how to apply standard Day offsets to a Timestamp and how normalization affects the result, resetting the time to midnight if set to True. The code is intended to illustrate the effect of normalization on date arithmetic; it requires the pandas and pandas.tseries.offsets modules, and expects pd to be in scope.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas.tseries.offsets as offsets\n\nday = offsets.Day()\nday.apply(pd.Timestamp(\"2014-01-01 09:00\"))\n\nday = offsets.Day(normalize=True)\nday.apply(pd.Timestamp(\"2014-01-01 09:00\"))\n```\n\n----------------------------------------\n\nTITLE: Applying User-Defined Functions with agg and transform - Python\nDESCRIPTION: Demonstrates applying custom functions to all DataFrame columns using df.agg(lambda x: np.mean(x) * 5.6) and broadcasting via df.transform(lambda x: x * 101.2). These methods allow complex aggregation and transformation in one step.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/10min.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndf.agg(lambda x: np.mean(x) * 5.6)\\ndf.transform(lambda x: x * 101.2)\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame Columns with Boolean Conditions\nDESCRIPTION: Demonstrates using boolean conditions with .loc to filter DataFrame columns based on values in a specific row.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf1.loc['a'] > 0\ndf1.loc[:, df1.loc['a'] > 0]\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with Random Data for Compression Example\nDESCRIPTION: Creates a sample DataFrame with random numeric data, string column, and datetime index to be used in compression examples. This serves as test data for pickle compression.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_157\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(\n    {\n        \"A\": np.random.randn(1000),\n        \"B\": \"foo\",\n        \"C\": pd.date_range(\"20130101\", periods=1000, freq=\"s\"),\n    }\n)\ndf\n```\n\n----------------------------------------\n\nTITLE: Converting DataFrame to HTML with Float Formatting in Python\nDESCRIPTION: Shows how to control the precision of floating point values when converting a DataFrame to HTML using the float_format parameter of to_html().\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_105\n\nLANGUAGE: python\nCODE:\n```\nhtml = df.to_html(float_format=\"{0:.10f}\".format)\nprint(html)\ndisplay(HTML(html))\n```\n\n----------------------------------------\n\nTITLE: Parsing Datetimes with Mixed Time Zones (Previous Behavior) in Pandas (ipython)\nDESCRIPTION: This code example illustrates the deprecated behavior when parsing a list of datetimes with mixed time zones using pd.to_datetime with utc=False. The output is an Index containing strings with time zone offsets and object dtype. Requires the pandas library; demonstrates older handling without warnings or conversion to UTC.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_10\n\nLANGUAGE: ipython\nCODE:\n```\n  In [7]: data = [\"2020-01-01 00:00:00+06:00\", \"2020-01-01 00:00:00+01:00\"]\n\n  In [8]:  pd.to_datetime(data, utc=False)\n  Out[8]:\n  Index([2020-01-01 00:00:00+06:00, 2020-01-01 00:00:00+01:00], dtype='object')\n```\n\n----------------------------------------\n\nTITLE: Series Reindexing with Boolean dtype Alignment - pandas - Python\nDESCRIPTION: Shows how reindex_like can adjust a Series to another's index, inserting NaN and changing the dtype as necessary. Demonstrates this with a Series of dtype bool reindexed to a Series of dtype int's shape. This impacts downstream operations, such as with numpy ufuncs, and is relevant for correct type handling. Requires 'series1' and pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nseries2 = pd.Series([True])\nseries2.dtype\nres = series2.reindex_like(series1)\nres.dtype\nres\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in merge function for integer and NaN keys\nDESCRIPTION: Addresses a problem where the merge function was failing with outer merge when using integer and NaN keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmerge()\n```\n\n----------------------------------------\n\nTITLE: Selecting First Row by Group in SAS\nDESCRIPTION: Sorts the 'tips' table by 'sex' and 'smoker', then creates 'tips_first', outputting only the first record of each 'sex' or 'smoker' group using SAS's FIRST. variables. Useful for deduplication or extracting representatives. Dependencies: SAS Base. Inputs: 'tips' table; outputs: 'tips_first'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_23\n\nLANGUAGE: sas\nCODE:\n```\nproc sort data=tips;\n   by sex smoker;\nrun;\n\ndata tips_first;\n    set tips;\n    by sex smoker;\n    if FIRST.sex or FIRST.smoker then output;\nrun;\n```\n\n----------------------------------------\n\nTITLE: Converting DatetimeIndex to Object Array Using astype - Pandas - Python\nDESCRIPTION: This snippet demonstrates how to convert a Pandas DatetimeIndex (here, 'rng') into an array of Timestamp objects using the astype(object) method. The result is that each element in the array is a pandas.Timestamp, preserving full datetime and nanosecond resolution. Key parameters include the original DatetimeIndex and specifying 'object' as the desired type. Input: a DatetimeIndex (e.g., rng); Output: a numpy array of Timestamp objects (stamp_array). Requires pandas. Limitations: matplotlib does not natively plot arrays of Timestamps, so further conversion may be necessary when creating plots.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.8.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstamp_array = rng.astype(object)\nstamp_array\nstamp_array[5]\n```\n\n----------------------------------------\n\nTITLE: Setting environment variable for verbose meson output in Shell\nDESCRIPTION: These commands set the MESONPY_EDITABLE_VERBOSE environment variable to enable verbose output during pandas import, which triggers a rebuild when using meson.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n# On Linux/macOS\nMESONPY_EDITABLE_VERBOSE=1 python\n\n# Windows\nset MESONPY_EDITABLE_VERBOSE=1 # Only need to set this once per session\npython\n```\n\n----------------------------------------\n\nTITLE: Vectorizing Functions with Numba for Efficient pandas Column Operations (python)\nDESCRIPTION: Demonstrates traditional vs Numba-vectorized functions for pandas/numpy vectors. Shows how to write a normal Python function and then a Numba-decorated function using @numba.vectorize to achieve better performance through vectorization. Inputs: 1D numpy arrays (pandas column). Outputs: transformed arrays. Demonstrates minimal change to code for significant speedup. Requires Numba.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/enhancingperf.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport numba\n\n\ndef double_every_value_nonumba(x):\n    return x * 2\n\n\n@numba.vectorize\ndef double_every_value_withnumba(x):  # noqa E501\n    return x * 2\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame for GroupBy Aggregation - Pandas - Python\nDESCRIPTION: Creates a DataFrame with three columns for use in subsequent groupby aggregation examples. Inputs are dict definitions of columns. Essential for context in groupby and aggregation demonstration. No special dependencies other than importing pandas as pd.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({'A': [1, 1, 1, 2, 2],\n                   'B': range(5),\n                   'C': range(5)})\ndf\n```\n\n----------------------------------------\n\nTITLE: Defining Key-Value Metadata Structure in Apache Parquet - Shell\nDESCRIPTION: This code snippet demonstrates the Parquet file footer metadata structure used to store key-value entries. These entries provide information about each column and file-level properties. The snippet is typically interpreted within the context of the Parquet file format specification and requires familiarity with Thrift schemas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/developer.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n  5: optional list<KeyValue> key_value_metadata\n```\n\n----------------------------------------\n\nTITLE: Maintaining Alphabetical Order with sort_index() in Python 3.6+\nDESCRIPTION: Example showing how to retain alphabetical ordering when creating a Series from a dictionary in Python 3.6+ by using the sort_index() method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\npd.Series({'Income': 2000,\n           'Expenses': -1500,\n           'Taxes': -200,\n           'Net result': 300}).sort_index()\n```\n\n----------------------------------------\n\nTITLE: Configuring DataFrame info() display in Pandas 0.13.1\nDESCRIPTION: Demonstrates how to use the max_info_rows option to control the display of null counts in DataFrame.info() output for large DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmax_info_rows = pd.get_option(\"max_info_rows\")\n\ndf = pd.DataFrame(\n    {\n        \"A\": np.random.randn(10),\n        \"B\": np.random.randn(10),\n        \"C\": pd.date_range(\"20130101\", periods=10),\n    }\n)\ndf.iloc[3:6, [0, 2]] = np.nan\n\n# set to not display the null counts\npd.set_option(\"max_info_rows\", 0)\ndf.info()\n\n# this is the default (same as in 0.13.0)\npd.set_option(\"max_info_rows\", max_info_rows)\ndf.info()\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Series integer indexing and KeyError in Python\nDESCRIPTION: Example showing how integer indexing with Series now raises KeyError when asking for a key not contained in the Series, changing from previous behavior of falling back to location-based lookup.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: s = pd.Series(np.random.randn(10), index=range(0, 20, 2))\nIn [4]: s\nOut[4]:\n0    -1.294524\n2     0.413738\n4     0.276662\n6    -0.472035\n8    -0.013960\n10   -0.362543\n12   -0.006154\n14   -0.923061\n16    0.895717\n18    0.805244\nLength: 10, dtype: float64\n\nIn [5]: s[0]\nOut[5]: -1.2945235902555294\n\nIn [6]: s[2]\nOut[6]: 0.41373810535784006\n\nIn [7]: s[4]\nOut[7]: 0.2766617129497566\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Crawler Access Rules in robots.txt\nDESCRIPTION: Defines access rules for web crawlers, allowing access to all content except the /preview/ directory. Uses standard robots.txt syntax to specify user-agent and disallow directives.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/robots.txt#_snippet_0\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: *\nDisallow: /preview/\n```\n\n----------------------------------------\n\nTITLE: Concatenating Unnamed Series with DataFrames in Python\nDESCRIPTION: Example of concatenating unnamed Series with a DataFrame, showing how pandas automatically assigns numeric names to the Series columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/merging.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns2 = pd.Series([\"_0\", \"_1\", \"_2\", \"_3\"])\nresult = pd.concat([df1, s2, s2, s2], axis=1)\nresult\n```\n\n----------------------------------------\n\nTITLE: Demonstrating DataFrame and Binary Operations in pandas - Python\nDESCRIPTION: This snippet demonstrates the usage and upcoming deprecation of special-case behavior for binary operations (like subtraction) between DataFrames and Series with time series indices. Users should migrate from the implicit broadcasting approach (e.g., df - df[0]) to explicit axis alignment (e.g., df.sub(df[0], axis=0)) for clarity and future compatibility. Requires pandas and numpy as dependencies, and illustrates how to create a DataFrame with a date range index, perform the deprecated operation, and use the preferred method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.randn(6, 4), index=pd.date_range(\"1/1/2000\", periods=6))\ndf\n# deprecated now\ndf - df[0]\n# Change your code to\ndf.sub(df[0], axis=0)  # align on axis 0 (rows)\n```\n\n----------------------------------------\n\nTITLE: Grouping DataFrame by Index Level and Column Key Names with pandas GroupBy in Python\nDESCRIPTION: Demonstrates how to group a pandas DataFrame by index level name 'second' and column 'A' using groupby, passing them directly as keys. Requires a MultiIndex DataFrame with levels appropriately named. Input is 'df', output is the summed DataFrame grouped by 'second' and 'A'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/groupby.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby([\"second\", \"A\"]).sum()\n```\n\n----------------------------------------\n\nTITLE: Parsing with infer_datetime_format Enabled - ipython\nDESCRIPTION: This example enables the infer_datetime_format argument while calling to_datetime, showing that, despite the inference intent, pandas may still unpredictably interpret formats per entry without enforcing consistency. Requires pandas as pd. The key parameters are the date string list and infer_datetime_format=True. Output remains a DatetimeIndex, but confusion from silent format changes may persist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0004-consistent-to-datetime-parsing.md#_snippet_1\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: pd.to_datetime(['12-01-2000 00:00:00', '13-01-2000 00:00:00'], infer_datetime_format=True)\nOut[2]: DatetimeIndex(['2000-12-01', '2000-01-13'], dtype='datetime64[ns]', freq=None)\n```\n\n----------------------------------------\n\nTITLE: Creating a Panel for 3D Data Structures - Pandas Testing - Python\nDESCRIPTION: This snippet demonstrates the creation of a deprecated Panel object with test data using pandas._testing.makePanel(). Intended for illustrative use in migration guides. Requires import of pandas._testing as tm. Returns a Panel with shape and axes description.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nimport pandas._testing as tm\n\np = tm.makePanel()\n\np\n```\n\n----------------------------------------\n\nTITLE: Reading Stata Data - pandas - Python\nDESCRIPTION: Illustrates reading a Stata '.dta' file into pandas with 'read_stata'. Requires the data file ('data.dta') and the pandas library. Input: Stata binary file. Output: DataFrame ('df') containing the Stata data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_stata(\"data.dta\")\n```\n\n----------------------------------------\n\nTITLE: Type Casting and Inference Patterns in pandas (Discouraged Example) - Python\nDESCRIPTION: Shows a discouraged approach to type narrowing using typing.cast inside a function for cases where static analysis can't infer the type after a custom inference function. Relies on is_number from pandas.core.dtypes.common. Dependencies: typing module, pandas dtype utilities. Parameters: obj (Union[str, int, float]), Output: uppercased string or None. Limitation: use of cast should be avoided where possible.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import cast\n\nfrom pandas.core.dtypes.common import is_number\n\ndef cannot_infer_bad(obj: Union[str, int, float]):\n\n    if is_number(obj):\n        ...\n    else:  # Reasonably only str objects would reach this but...\n        obj = cast(str, obj)  # Mypy complains without this!\n        return obj.upper()\n```\n\n----------------------------------------\n\nTITLE: Generating Class Documentation Page Structure in Jinja2 for Pandas\nDESCRIPTION: A Jinja2 template that structures documentation for Pandas classes. It creates sections for class attributes and methods, displaying them in an autosummary format while filtering out private members (those starting with '_') with an exception for the '__call__' method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/_templates/autosummary/class.rst#_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ fullname | escape | underline}}\n\n.. currentmodule:: {{ module }}\n\n.. autoclass:: {{ objname }}\n\n   {% block methods %}\n\n   {% block attributes %}\n   {% if attributes %}\n   .. rubric:: {{ _('Attributes') }}\n\n   .. autosummary::\n   {% for item in attributes %}\n      {% if item in members and not item.startswith('_') %}\n        ~{{ name }}.{{ item }}\n      {% endif %}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n\n   {% if methods %}\n   .. rubric:: {{ _('Methods') }}\n\n   .. autosummary::\n   {% for item in methods %}\n      {% if item in members and (not item.startswith('_') or item in ['__call__']) %}\n        ~{{ name }}.{{ item }}\n      {% endif %}\n   {%- endfor %}\n   {% endif %}\n   {% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Applying a Function Over Two Panel Axes Using Panel.apply in Pandas (Python)\nDESCRIPTION: Demonstrates applying a normalization function across the 'items' and 'major_axis' axes of a pandas Panel using the Panel.apply method. The code takes a Panel object named 'panel' and applies the function f defined above, resulting in a transformed Panel assigned to the variable 'result'. Inputs are a pandas.Panel and a function; output is a new normalized Panel. Requires pandas v0.13.1 and above (Panel is deprecated in later versions).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.1.rst#_snippet_7\n\nLANGUAGE: ipython\nCODE:\n```\nIn [39]: result = panel.apply(f, axis=['items', 'major_axis'])\n```\n\n----------------------------------------\n\nTITLE: Specifying Column Subset in Excel Read Operation\nDESCRIPTION: Demonstrates how to read only specific columns from an Excel file using the usecols parameter with a string specifying Excel column names or ranges.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_134\n\nLANGUAGE: python\nCODE:\n```\npd.read_excel(\"path_to_file.xls\", \"Sheet1\", usecols=\"A,C:E\")\n```\n\n----------------------------------------\n\nTITLE: Correcting to_datetime Behavior with DataFrame Input\nDESCRIPTION: Addresses a bug where box and utc arguments were ignored when passing a DataFrame or dict of unit mappings to to_datetime function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_54\n\nLANGUAGE: Python\nCODE:\n```\npd.to_datetime(df, box=True, utc=True)\n```\n\n----------------------------------------\n\nTITLE: Column-wise Math Operations - Stata\nDESCRIPTION: Demonstrates updating and creating columns in Stata using 'replace' and 'generate', and removing a column with 'drop'. No dependencies. Input: Data set in memory with at least 'total_bill'. Output: Modified data set with column values changed or dropped.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_9\n\nLANGUAGE: stata\nCODE:\n```\nreplace total_bill = total_bill - 2\\ngenerate new_bill = total_bill / 2\\ndrop new_bill\n```\n\n----------------------------------------\n\nTITLE: Applying NumPy Universal Functions on a Pandas Series (Python)\nDESCRIPTION: Shows how a NumPy universal function operates on a pandas Series by leveraging Series.__array_ufunc__. Requires pandas and NumPy. The input is a Series ser, and np.exp(ser) applies the exponential function element-wise. The output is a Series with the same index and transformed values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/dsintro.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series([1, 2, 3, 4])\nnp.exp(ser)\n```\n\n----------------------------------------\n\nTITLE: Creating and Comparing Series with NA Values - pandas (IPython)\nDESCRIPTION: Demonstrates creation of a pandas Series containing strings and NA values, and performs element-wise comparisons to show how NA/NaN is treated. Relies on pandas and numpy (aliased as pd and np). The == and != operators are used to compare Series elements to a string, illustrating that NA comparisons yield False or True accordingly. Output is a Series of boolean values; NA handling may differ for other operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.3.rst#_snippet_3\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]: series = pd.Series([\"Steve\", np.nan, \"Joe\"])\n\nIn [2]: series == \"Steve\"\nOut[2]:\n0     True\n1    False\n2    False\nLength: 3, dtype: bool\n\nIn [3]: series != \"Steve\"\nOut[3]:\n0    False\n1     True\n2     True\nLength: 3, dtype: bool\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment for Pandas on Windows\nDESCRIPTION: PowerShell commands to create and activate a Python virtual environment for Pandas development on Windows, and install required dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\n# Create a virtual environment\npython -m venv $env:USERPROFILE\\virtualenvs\\pandas-dev\n\n# Activate the virtualenv. Use activate.bat for cmd.exe\n~\\virtualenvs\\pandas-dev\\Scripts\\Activate.ps1\n\n# Install the build dependencies\npython -m pip install -r requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: Label-Based Indexing with NoRowIndex Raises Exception (Python)\nDESCRIPTION: Shows that attempting to use label-based indexing with .loc on a DataFrame using NoRowIndex raises an IndexError. Demonstrates the designed strictness of NoRowIndex for disallowing label-based operations. Input: DataFrame with NoRowIndex; Output: Exception raised for label-based indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nIn [15]: df.loc[0, 'b']\n---------------------------------------------------------------------------\nIndexError: Cannot use label-based indexing on NoRowIndex!\n```\n\n----------------------------------------\n\nTITLE: Restoring DataFrame from ntv_pandas JSON and Checking Equality in Python\nDESCRIPTION: This snippet demonstrates how to deserialize a compact ntv_pandas-generated JSON object back into a pandas DataFrame and verify that it matches the original DataFrame. It uses npd.read_json to reconstruct the DataFrame and df.equals to confirm reversibility, ensuring that all data types are preserved. Dependencies remain ntv_pandas and pandas. The input is the JSON object; the output is a DataFrame (and a boolean check for identity).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0012-compact-and-reversible-JSON-interface.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIn [5]: df_from_json = npd.read_json(df_to_json)\n        print('df created from JSON is equal to initial df ? ', df_from_json.equals(df))\nOut[5]: df created from JSON is equal to initial df ?  True\n```\n\n----------------------------------------\n\nTITLE: Forward-Filling Missing Values Using ffill Convenience Method - python\nDESCRIPTION: Presents the use of the ffill() convenience method on a pandas Series to propagate last valid observation forward for missing values. Offers an alternative to fillna(method='pad'), yielding equivalent results, and is effective in preprocessing time series with gaps.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([np.nan, 1.0, 2.0, np.nan, 4])\ns.ffill()\n```\n\n----------------------------------------\n\nTITLE: NumPy Functions on Categorical Series Error Handling - pandas - Python\nDESCRIPTION: Attempts to use NumPy aggregation functions (e.g., np.sum) on a Series with categorical dtype, which raises a TypeError. Demonstrates that categoricals are not intended for numeric operations even when categories are numeric. Dependencies: pandas, numpy. Key parameters: numeric categories. Input: Categorical Series; Output: TypeError message. Limitation: NumPy numeric functions are not supported for categoricals.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(pd.Categorical([1, 2, 3, 4]))\ntry:\n    np.sum(s)\n    # same with np.log(s),...\nexcept TypeError as e:\n    print(\"TypeError:\", str(e))\n```\n\n----------------------------------------\n\nTITLE: Creating Timezone-Aware Series with DatetimeIndex - Pandas - Python\nDESCRIPTION: This snippet demonstrates constructing a timezone-aware Series using pandas's date_range with the 'tz' argument. This forms the basis for subsequent conversion experiments using NumPy. Inputs are a start date, number of periods, and a timezone string. The output is a pandas Series of Timestamps with the specified timezone. pandas must be installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(pd.date_range('2000', periods=2, tz=\"CET\"))\nser\n```\n\n----------------------------------------\n\nTITLE: Creating and Saving Large Time Series Dataset in Python\nDESCRIPTION: Demonstrates how to create a large time series dataset with multiple columns and save it as a Parquet file. This snippet shows the process of generating synthetic data and writing it to disk.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/scale.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\ndef make_timeseries(start=\"2000-01-01\", end=\"2000-12-31\", freq=\"1D\", seed=None):\n    index = pd.date_range(start=start, end=end, freq=freq, name=\"timestamp\")\n    n = len(index)\n    state = np.random.RandomState(seed)\n    columns = {\n        \"name\": state.choice([\"Alice\", \"Bob\", \"Charlie\"], size=n),\n        \"id\": state.poisson(1000, size=n),\n        \"x\": state.rand(n) * 2 - 1,\n        \"y\": state.rand(n) * 2 - 1,\n    }\n    df = pd.DataFrame(columns, index=index, columns=sorted(columns))\n    if df.index[-1] == end:\n        df = df.iloc[:-1]\n    return df\n\ntimeseries = [\n    make_timeseries(freq=\"1min\", seed=i).rename(columns=lambda x: f\"{x}_{i}\")\n    for i in range(10)\n]\nts_wide = pd.concat(timeseries, axis=1)\nts_wide.head()\nts_wide.to_parquet(\"timeseries_wide.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Building HTML documentation with Sphinx using make.py in Python (Shell command)\nDESCRIPTION: This shell command navigates to the documentation directory and runs the make.py script to build the full HTML documentation for pandas using Sphinx. It assumes Python and Sphinx are installed and that the command is executed in the '/doc' directory. Inputs: no explicit arguments for a full build. Outputs: HTML documentation files in the specified build directory. Requires all documentation dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_gitpod.rst#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n$ cd doc\n$ python make.py html\n```\n\n----------------------------------------\n\nTITLE: Setting Option mode.no_row_index for DataFrame Index Control (Python)\nDESCRIPTION: Demonstrates the hypothetical usage of pandas' set_option to enable a mode where new DataFrames and Series use NoRowIndex by default. NoRowIndex mode affects methods like reset_index, concat, merge, and DataFrame construction. Relies on pandas' set_option API and assumes a future option 'mode.no_row_index'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\npd.set_option(\"mode.no_row_index\", True)\n```\n\n----------------------------------------\n\nTITLE: Plotting Time Series with Secondary Y-Axis in Pandas\nDESCRIPTION: Demonstrates plotting time series data with a secondary y-axis using the new plotting features in Pandas 0.8.0. Shows how to create multiple line plots with different styles and axes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.8.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfx = pd.read_pickle(\"data/fx_prices\")\nimport matplotlib.pyplot as plt\n\nplt.figure()\n\nfx[\"FR\"].plot(style=\"g\")\n\nfx[\"IT\"].plot(style=\"k--\", secondary_y=True)\n```\n\n----------------------------------------\n\nTITLE: Committing Staged Changes with Descriptive Messages - Shell\nDESCRIPTION: This snippet shows the command to commit staged changes to your local git repository with an explanatory message. It assumes files are already staged via 'git add'. The commit message should briefly and specifically describe the change. The input is a message string; the output is a new commit on the current local branch.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing.rst#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit commit -m \\\"your commit message goes here\\\"\n```\n\n----------------------------------------\n\nTITLE: Running asv Continuous Using Virtualenv (Bash)\nDESCRIPTION: Runs asv continuous but forces use of virtualenv (-E virtualenv) instead of conda for creating environments. Required when user prefers virtualenv or does not use conda. Otherwise functionally identical to standard asv benchmark comparison.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\nasv continuous -f 1.1 -E virtualenv upstream/main HEAD\n```\n\n----------------------------------------\n\nTITLE: Reading CSV with and without Header and Prefix in pandas - ipython\nDESCRIPTION: This snippet illustrates how pd.read_csv() assigns default column names when the header is not specified, switching to integer-based columns by default. It also shows how to reintroduce the old behavior using the prefix parameter. Helps users understand column name assignment in file parsing upgrades.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_3\n\nLANGUAGE: ipython\nCODE:\n```\nIn [6]: import io\n\nIn [7]: data = \"\"\"\n  ...: a,b,c\n  ...: 1,Yes,2\n  ...: 3,No,4\n  ...: \"\"\"\n  ...:\n\nIn [8]: print(data)\n\n    a,b,c\n    1,Yes,2\n    3,No,4\n\nIn [9]: pd.read_csv(io.StringIO(data), header=None)\nOut[9]:\n       0    1  2\n0      a    b  c\n1      1  Yes  2\n2      3   No  4\n\nIn [10]: pd.read_csv(io.StringIO(data), header=None, prefix=\"X\")\nOut[10]:\n        X0   X1 X2\n0       a    b  c\n1       1  Yes  2\n2       3   No  4\n```\n\n----------------------------------------\n\nTITLE: Fixing DatetimeIndex Construction from Categorical\nDESCRIPTION: Corrects a bug where constructing a DatetimeIndex from a Categorical or CategoricalIndex incorrectly dropped timezone information.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_52\n\nLANGUAGE: Python\nCODE:\n```\nDatetimeIndex(categorical_data)\n```\n\n----------------------------------------\n\nTITLE: Date Column Creation and Manipulation - Stata\nDESCRIPTION: Covers date feature examples in Stata, including using 'mdy', 'date', 'year', 'month', and more to create and derive new columns such as 'date1_year', 'date2_month', or 'months_between'. No external dependencies, but expects columns named as referenced. Inputs: None or data columns as specified. Outputs: Additional date-related columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_12\n\nLANGUAGE: stata\nCODE:\n```\ngenerate date1 = mdy(1, 15, 2013)\\ngenerate date2 = date(\"Feb152015\", \"MDY\")\\n\\ngenerate date1_year = year(date1)\\ngenerate date2_month = month(date2)\\n\\n* shift date to beginning of next month\\ngenerate date1_next = mdy(month(date1) + 1, 1, year(date1)) if month(date1) != 12\\nreplace date1_next = mdy(1, 1, year(date1) + 1) if month(date1) == 12\\ngenerate months_between = mofd(date2) - mofd(date1)\\n\\nlist date1 date2 date1_year date2_month date1_next months_between\n```\n\n----------------------------------------\n\nTITLE: Documenting Version Addition with Sphinx .rst Directive (reStructuredText)\nDESCRIPTION: Shows how to annotate documentation in reStructuredText to indicate when a feature, method, or option was added using '.. versionadded::'. This is inserted in docstrings or docs to help users understand feature history. Requires usage in files processed by Sphinx.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_34\n\nLANGUAGE: rst\nCODE:\n```\n.. versionadded:: 2.1.0\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Previous pandas Behavior\nDESCRIPTION: Shows how mutations could affect multiple objects before Copy-on-Write implementation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/copy_on_write.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})\nsubset = df[\"foo\"]\nsubset.iloc[0] = 100\ndf\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Indexing and Assignment in pandas (Python)\nDESCRIPTION: This snippet illustrates a typical workflow using DataFrame indexing to subset columns and conditionally modify values. The code creates a DataFrame, selects a subset of columns to form a new DataFrame, and updates one of its values based on a condition. Dependencies: pandas. Key variables include the parent DataFrame `df` and subset `df2`; mutation is performed using loc. Expected input: DataFrame objects; output: modified `df2`. This is used to exemplify confusion regarding whether such assignment mutates the original DataFrame or not.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4], \"C\": [5, 6]})\n>>> df2 = df[[\"A\", \"B\"]]\n>>> df2.loc[df2[\"A\"] > 1, \"A\"] = 1\n```\n\n----------------------------------------\n\nTITLE: Fixed regression in Series.cat.reorder_categories for category updates\nDESCRIPTION: Addresses an issue where Series.cat.reorder_categories was failing to update the categories on the Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nSeries.cat.reorder_categories()\n```\n\n----------------------------------------\n\nTITLE: Saving Dataset to Hugging Face Dataset Hub\nDESCRIPTION: This code shows how to save a pandas DataFrame to your Hugging Face account. This requires creating a dataset on Hugging Face and being logged in to your account.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Save the dataset to my Hugging Face account\ndf.to_parquet(\"hf://datasets/username/dataset_name/train.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Using In-Memory HDFStore with driver=H5FD_CORE - Python\nDESCRIPTION: Opens an HDFStore using the in-memory driver, so changes are only written to disk on store.close(). Dependencies: pandas with HDF5 (PyTables). Input: file path, mode 'w', driver option. Output: in-memory operations; data becomes persistent only after closing. Useful for high-speed intermediate storage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nstore = pd.HDFStore(\"test.h5\", \"w\", driver=\"H5FD_CORE\")\n\ndf = pd.DataFrame(np.random.randn(8, 3))\nstore[\"test\"] = df\n\n# only after closing the store, data is written to disk:\nstore.close()\n```\n\n----------------------------------------\n\nTITLE: Adding and Removing Categories\nDESCRIPTION: Examples of adding new categories and removing existing ones from categorical data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ns = s.cat.add_categories([4])\ns.cat.categories\ns\n\ns = s.cat.remove_categories([4])\ns\n```\n\n----------------------------------------\n\nTITLE: Reading CSV In Chunks with Progressive Indexing (ipython, Python)\nDESCRIPTION: Illustrates the change in behavior for read_csv() with chunksize where chunked outputs now have progressive, non-overlapping indexes across chunks. Requires pandas; example input is CSV text, and output is concatenated DataFrame with a continuous index instead of repeated 0..n-1 per chunk; improves seamless concatenation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndata = \"A,B\\n0,1\\n2,3\\n4,5\\n6,7\"\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [2]: pd.concat(pd.read_csv(StringIO(data), chunksize=2))\nOut[2]:\n   A  B\n0  0  1\n1  2  3\n0  4  5\n```\n\n----------------------------------------\n\nTITLE: Creating Series with Mixed Time Zone Offsets Using apply and datetime.strptime in Pandas (python)\nDESCRIPTION: This code creates a Series with mixed time zone offsets and object dtype using pandas and Python's built-in datetime module. It parses each string separately via Series.apply and datetime.datetime.strptime. Dependencies include pandas and the datetime module; input is a list of ISO-8601 datetime strings with time zone offsets.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n    import datetime as dt\n\n    data = [\"2020-01-01 00:00:00+06:00\", \"2020-01-01 00:00:00+01:00\"]\n    pd.Series(data).apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S%z'))\n```\n\n----------------------------------------\n\nTITLE: ExtensionArray Methods and Attributes\nDESCRIPTION: This list includes various methods and attributes of the ExtensionArray class, such as data manipulation, comparison, and property access methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/extensions.rst#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\napi.extensions.ExtensionArray._accumulate\napi.extensions.ExtensionArray._concat_same_type\napi.extensions.ExtensionArray._explode\napi.extensions.ExtensionArray._formatter\napi.extensions.ExtensionArray._from_factorized\napi.extensions.ExtensionArray._from_sequence\napi.extensions.ExtensionArray._from_sequence_of_strings\napi.extensions.ExtensionArray._hash_pandas_object\napi.extensions.ExtensionArray._pad_or_backfill\napi.extensions.ExtensionArray._reduce\napi.extensions.ExtensionArray._values_for_argsort\napi.extensions.ExtensionArray._values_for_factorize\napi.extensions.ExtensionArray.argsort\napi.extensions.ExtensionArray.astype\napi.extensions.ExtensionArray.copy\napi.extensions.ExtensionArray.view\napi.extensions.ExtensionArray.dropna\napi.extensions.ExtensionArray.duplicated\napi.extensions.ExtensionArray.equals\napi.extensions.ExtensionArray.factorize\napi.extensions.ExtensionArray.fillna\napi.extensions.ExtensionArray.insert\napi.extensions.ExtensionArray.interpolate\napi.extensions.ExtensionArray.isin\napi.extensions.ExtensionArray.isna\napi.extensions.ExtensionArray.ravel\napi.extensions.ExtensionArray.repeat\napi.extensions.ExtensionArray.searchsorted\napi.extensions.ExtensionArray.shift\napi.extensions.ExtensionArray.take\napi.extensions.ExtensionArray.unique\napi.extensions.ExtensionArray.dtype\napi.extensions.ExtensionArray.nbytes\napi.extensions.ExtensionArray.ndim\napi.extensions.ExtensionArray.shape\napi.extensions.ExtensionArray.tolist\n```\n\n----------------------------------------\n\nTITLE: Building Documentation Commands\nDESCRIPTION: Various commands for building pandas documentation, including full builds, clean builds, and selective section compilation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_documentation.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython make.py html\n```\n\nLANGUAGE: bash\nCODE:\n```\npython make.py clean\npython make.py html\n```\n\nLANGUAGE: bash\nCODE:\n```\npython make.py clean\npython make.py --no-api\n```\n\nLANGUAGE: bash\nCODE:\n```\npython make.py clean\npython make.py --single development/contributing.rst\n```\n\nLANGUAGE: bash\nCODE:\n```\npython make.py clean\npython make.py --single pandas.DataFrame.join\n```\n\nLANGUAGE: bash\nCODE:\n```\npython make.py clean\npython make.py --whatsnew\n```\n\nLANGUAGE: bash\nCODE:\n```\npython make.py html --num-jobs 4\n```\n\n----------------------------------------\n\nTITLE: Obfuscating and Rewriting Mailto Links for Workgroups - JavaScript\nDESCRIPTION: This JavaScript snippet is embedded in the workgroups section to post-process the rendered HTML, replacing obfuscated email addresses by removing a prefix and reconstructing the correct mailto hyperlinks. It runs client-side, expects an anchor tag with an id matching the workgroup name, adjusts both the displayed text and the 'href' attribute. The email address is originally obfuscated by prefixing 'asp.' to discourage scraping; the script removes this on page load. Requires correct element id and inner HTML template. Outputs are updated mailto links in the DOM.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/team.md#_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nvar mail_tag_id = '{{ workgroup.name|replace(' ', '-') }}';\\nvar mail_tag_element = document.getElementById( mail_tag_id );\\nmail_tag_element.innerHTML = mail_tag_element.innerHTML.replace(/^asp./, \\\"\\\");\\nmail_tag_element.setAttribute('href', \\\"mailto:\\\"+mail_tag_element.innerHTML);\n```\n\n----------------------------------------\n\nTITLE: Getting and Setting Debug Mode Option in pandas - Python\nDESCRIPTION: Demonstrates retrieving and updating the 'mode.sim_interactive' option in pandas, typically used for internal debugging and simulation of interactive mode. Utilizes get_option and set_option. The parameter is usually not needed in production code, and it should be used with caution. Inputs are the option name and a boolean value; outputs are the current setting.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.get_option(\"mode.sim_interactive\")\npd.set_option(\"mode.sim_interactive\", True)\npd.get_option(\"mode.sim_interactive\")\n```\n\n----------------------------------------\n\nTITLE: Using Index.intersection with Changed Sort Parameter in Python\nDESCRIPTION: The sort option for Index.intersection has changed. The default is now False, restoring pre-0.24 behavior. sort=None provides the previous sort=True behavior, and sort=True is no longer allowed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nIndex.intersection(sort=False)  # New default, unsorted\nIndex.intersection(sort=None)   # Sorted if values not identical\n```\n\n----------------------------------------\n\nTITLE: Defining a test class in Python\nDESCRIPTION: Example of the existing class-based test structure in pandas, which is no longer the preferred method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass TestReallyCoolFeature:\n    def test_cool_feature_aspect(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Filtering Missing Data in SAS\nDESCRIPTION: Creates two datasets by filtering 'outer_join' on missingness of the 'value_x' column: one containing only missing values and one containing non-missing values. Dependencies: SAS Base module. Assumes input dataset 'outer_join' has the variable 'value_x'. Outputs: 'outer_join_nulls' (only nulls) and 'outer_join_no_nulls' (no nulls).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_20\n\nLANGUAGE: sas\nCODE:\n```\ndata outer_join_nulls;\n    set outer_join;\n    if value_x = .;\nrun;\n\ndata outer_join_no_nulls;\n    set outer_join;\n    if value_x ^= .;\nrun;\n```\n\n----------------------------------------\n\nTITLE: Resetting All Display Options Using Regex - Python\nDESCRIPTION: Shows a one-liner for resetting all pandas options related to DataFrame display using a regex pattern. This restores a consistent output configuration, which is helpful after experiments or tests have altered multiple output settings. Requires only the pandas library.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/options.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npd.reset_option(\"^display\")\n```\n\n----------------------------------------\n\nTITLE: Acknowledging Contributors with Sphinx Directive - reStructuredText - English\nDESCRIPTION: This snippet acknowledges contributors by using the Sphinx 'contributors' directive, accompanied by a header. Dependencies include Sphinx documentation tools and the presence of contributor metadata. The input is a version range; output is a rendered contributor summary. It requires a Sphinx build system to interpret the directive.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.2.rst#_snippet_4\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _whatsnew_0.16.2.contributors:\n\nContributors\n~~~~~~~~~~~~\n\n.. contributors:: v0.16.1..v0.16.2\n```\n\n----------------------------------------\n\nTITLE: Creating Series with Custom Indices in pandas (Python)\nDESCRIPTION: Illustrates how to create pandas Series objects with explicit index lists. Demonstrates the assignment of specific indices to two Series, which sets up context for later arithmetic and concatenation operations. No external dependencies are required beyond pandas itself. Inputs are integer value lists along with custom indices; outputs are pandas Series objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nIn[37]: ser1 = pd.Series([10, 15, 20, 25], index=[1, 2, 3, 5])\n\nIn[38]: ser2 = pd.Series([10, 15, 20, 25], index=[1, 2, 3, 4])\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for evaluation function\nDESCRIPTION: RestructuredText directive listing Pandas evaluation function\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/general_functions.rst#_snippet_5\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n\n   eval\n```\n\n----------------------------------------\n\nTITLE: Embedding JupyterLite REPL with Example pandas Code - HTML\nDESCRIPTION: This HTML snippet embeds a JupyterLite REPL instance within the page, preloading example Python pandas code for immediate user interaction. The iframe source specifies a JupyterLite environment with the kernel set to Python and includes a code parameter to run a pandas DataFrame example. Users need modern browsers and a stable internet connection due to the resource requirements. The frame provides an interactive area to try out pandas code directly in the browser, but performance and compatibility constraints are noted.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/try.md#_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<iframe\\n  src=\\\"./lite/repl/index.html?toolbar=1&kernel=python&execute=0&code=import%20pandas%20as%20pd%0Adf%20%3D%20pd.DataFrame%28%7B%22num_legs%22%3A%20%5B2%2C%204%5D%2C%20%22num_wings%22%3A%20%5B2%2C%200%5D%7D%2C%20index%3D%5B%22falcon%22%2C%20%22dog%22%5D%29%0Adf\\\"\\n  style=\\\"width: 100%; max-width: 650px; height: 600px; border: 1px solid #130753;\\\"\\n></iframe>\n```\n\n----------------------------------------\n\nTITLE: Converting SparseSeries with Incompatible Fill Value in pandas - ipython\nDESCRIPTION: This snippet attempts to convert a SparseSeries with nan fill_value to an int64 dtype using astype, which fails because nan cannot be coerced to int64. The resulting ValueError shows pandas' improved error checking in v0.19.0. Requires pandas and numpy. Input sequence must include float and nan values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_45\n\nLANGUAGE: ipython\nCODE:\n```\nIn [7]: pd.SparseSeries([1., np.nan, 2., np.nan], fill_value=np.nan).astype(np.int64)\nOut[7]:\nValueError: unable to coerce current fill_value nan to int64 dtype\n```\n\n----------------------------------------\n\nTITLE: Using get() Method in Pandas Series\nDESCRIPTION: This snippet demonstrates the use of the get() method in Pandas Series, which allows retrieving values by index with an option to specify a default value for missing keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_69\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\ns.get('a')  # equivalent to s['a']\ns.get('x', default=-1)\n```\n\n----------------------------------------\n\nTITLE: Inefficient or Unsupported Indexing on Unsorted MultiIndexes - pandas - Python\nDESCRIPTION: Demonstrates that label-based selection on unsorted MultiIndex objects triggers a PerformanceWarning and may return a copy. Once sorted, selection proceeds efficiently. Also includes checks for ascending monotonicity of MultiIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/advanced.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndfm = pd.DataFrame(\n    {\"jim\": [0, 0, 1, 1], \"joe\": [\"x\", \"x\", \"z\", \"y\"], \"jolie\": np.random.rand(4)}\n)\ndfm = dfm.set_index([\"jim\", \"joe\"])\ndfm\ndfm.loc[(1, 'z')]\n```\n\nLANGUAGE: python\nCODE:\n```\ndfm.loc[(0, 'y'):(1, 'z')]\n```\n\nLANGUAGE: python\nCODE:\n```\ndfm.index.is_monotonic_increasing\n```\n\nLANGUAGE: python\nCODE:\n```\ndfm = dfm.sort_index()\ndfm\ndfm.index.is_monotonic_increasing\n```\n\nLANGUAGE: python\nCODE:\n```\ndfm.loc[(0, \"y\"):(1, \"z\")]\n```\n\n----------------------------------------\n\nTITLE: Extracting Regular Expressions from Series Strings using str.extract (Python)\nDESCRIPTION: Uses the str.extract method to apply a regex and extract digit matches from strings starting with 'a' or 'b'. Dependencies: pandas. Input: a Series of strings. Output: a Series of matched digits (or NaN for no match). Useful for cleaning or parsing structured string patterns efficiently.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\npd.Series(['a1', 'b2', 'c3']).str.extract('[ab](\\d)')\n```\n\n----------------------------------------\n\nTITLE: Registering Extension Types and Accessors\nDESCRIPTION: These functions are used to register custom extension data types and accessors for DataFrame, Series, and Index objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/extensions.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\napi.extensions.register_extension_dtype\napi.extensions.register_dataframe_accessor\napi.extensions.register_series_accessor\napi.extensions.register_index_accessor\napi.extensions.ExtensionDtype\n```\n\n----------------------------------------\n\nTITLE: Creating Release Tag and Commit\nDESCRIPTION: Git commands for creating an empty commit and version tag for a pandas release. This includes cleaning the working directory, creating a commit, tagging the version, and pushing to upstream.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/maintaining.rst#_snippet_3\n\nLANGUAGE: console\nCODE:\n```\ngit checkout <branch>\ngit pull --ff-only upstream <branch>\ngit clean -xdf\ngit commit --allow-empty --author=\"pandas Development Team <pandas-dev@python.org>\" -m \"RLS: <version>\"\ngit tag -a v<version> -m \"Version <version>\"\ngit push upstream <branch> --follow-tags\n```\n\n----------------------------------------\n\nTITLE: Assigning to Series Extracted from DataFrame Columns - pandas - Python\nDESCRIPTION: Examines extracting a column from a DataFrame as a Series and mutating it; in current pandas versions, such changes propagate back to the parent DataFrame due to it being a view. Requires pandas and a DataFrame as input. The code creates a DataFrame, extracts Column 'A' as a Series, and sets one of its elements via .loc; the result is that both the Series and original DataFrame are mutated at that element. Intended input is a DataFrame, output is a modified DataFrame and Series. This illustrates view-based assignment risks.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4], \"C\": [5, 6]})\n>>> s = df[\"A\"]\n>>> s.loc[0] = 0   # will also modify df (but no longer with this proposal)\n```\n\n----------------------------------------\n\nTITLE: Installing Cython with pip for pandas source installation (Shell)\nDESCRIPTION: This snippet details how to install the Cython build dependency needed when building pandas from source using pip. Cython is required to compile certain C extensions in the pandas codebase. The input is a simple pip install command; output is Cython being available in the current Python environment. Requires pip to be installed; should be run before building pandas from source.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install cython\n```\n\n----------------------------------------\n\nTITLE: Resample Operation Result Values - Pandas 0.25.x\nDESCRIPTION: Example demonstrating how resample operation handles non-category values in Pandas 0.25.x.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_28\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1] df.resample(\"2D\").agg(lambda x: 'c')\nOut[1]:\n\n     A\n0  NaN\n```\n\n----------------------------------------\n\nTITLE: Deprecating Timestamp.offset property in favor of freq\nDESCRIPTION: The 'offset' property of Timestamp objects (and the corresponding constructor argument) has been deprecated. Use 'freq' instead.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\nTimestamp(..., offset=...)\n```\n\nLANGUAGE: Python\nCODE:\n```\nTimestamp(..., freq=...)\n```\n\n----------------------------------------\n\nTITLE: Matching and Filtering Data in R\nDESCRIPTION: Demonstrates how to use the %in% operator and match function in R for data selection and matching.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_4\n\nLANGUAGE: r\nCODE:\n```\ns <- 0:4\ns %in% c(2,4)\n```\n\nLANGUAGE: r\nCODE:\n```\ns <- 0:4\nmatch(s, c(2,4))\n```\n\n----------------------------------------\n\nTITLE: File Cleanup with os.remove in Loop - Python\nDESCRIPTION: This snippet removes a calculated set of files in a loop using os.remove, typically after performing IO operations in pandas. Dependencies: os library. Input: removes files named file_0.csv, file_1.csv, file_2.csv. Output: files deleted from filesystem. Must have permission to delete files and files must exist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nfor i in range(3):\n    os.remove(\"file_{}.csv\".format(i))\n```\n\n----------------------------------------\n\nTITLE: Extracting Underlying Data from Pandas Series using .array and .to_numpy() (Python)\nDESCRIPTION: This snippet displays two ways to extract data from a pandas Series based on extension arrays: .array (zero-copy, returns ExtensionArray) and .to_numpy() (returns a NumPy array, possibly with conversion). pandas is required. Input is a categorical Series, outputs show both the extension array view and conversion to a NumPy object array, highlighting performance and type differences.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/blog/extension-arrays.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> ser.array\n[a, b, a]\nCategories (3, object): ['a', 'b', 'c']\n\n>>> ser.to_numpy()\narray(['a', 'b', 'a'], dtype=object)\n```\n\n----------------------------------------\n\nTITLE: Version Number Format Example in RST\nDESCRIPTION: Demonstrates the semantic versioning format used by pandas, showing the MAJOR.MINOR.PATCH structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/policies.rst#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n``MAJOR.MINOR.PATCH``\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas and Creating DataFrame in IPython\nDESCRIPTION: This snippet shows the same pandas import and DataFrame creation, but in the IPython notebook format used in the documentation. It illustrates how input is displayed in Jupyter Notebooks.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/index.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\npd.DataFrame({'A': [1, 2, 3]})\n```\n\n----------------------------------------\n\nTITLE: Installing HTML Parsing Dependencies for Pandas\nDESCRIPTION: Command to install dependencies for HTML parsing functionality in pandas, which is required for the read_html function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pandas[html]\"\n```\n\n----------------------------------------\n\nTITLE: Batch Converting DataFrame Columns to Categorical in Python\nDESCRIPTION: Demonstrates how to convert all columns in an existing DataFrame to categorical data types using the astype() method. This allows for post-creation conversion of multiple columns simultaneously.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\")})\ndf_cat = df.astype(\"category\")\ndf_cat.dtypes\n```\n\n----------------------------------------\n\nTITLE: Handling Ellipsis in DatetimeIndex and TimedeltaIndex\nDESCRIPTION: Fixes an issue where indexing with Ellipsis incorrectly lost the index's freq attribute in DatetimeIndex and TimedeltaIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\ndatetime_index[...]\n```\n\n----------------------------------------\n\nTITLE: Initializing DataFrames for Pivot Operations - pandas - Python\nDESCRIPTION: This snippet demonstrates how to create a DataFrame in Python using pandas, with columns for 'value', 'variable', and 'date', where 'date' is processed with pd.to_datetime. The context sets up sample data for subsequent reshaping and pivoting operations. Inputs include dictionary-based data, and output is a pandas DataFrame with structured columns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndata = {\n   \"value\": range(12),\n   \"variable\": [\"A\"] * 3 + [\"B\"] * 3 + [\"C\"] * 3 + [\"D\"] * 3,\n   \"date\": pd.to_datetime([\"2020-01-03\", \"2020-01-04\", \"2020-01-05\"] * 4)\n}\ndf = pd.DataFrame(data)\n```\n\n----------------------------------------\n\nTITLE: SettingWithCopy Warning Example\nDESCRIPTION: Shows how chained assignment on mixed-dtype DataFrames now raises SettingWithCopy warning.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.0.rst#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.arange(0, 9), columns=['count'])\ndf['group'] = 'b'\ndf.iloc[0:5]['group'] = 'a'\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Negative Step Support for Label-Based Slices in Python\nDESCRIPTION: Shows the new behavior of negative step support for label-based slices in Pandas Series objects.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.15.2.rst#_snippet_3\n\nLANGUAGE: ipython\nCODE:\n```\ns = pd.Series(np.arange(3), ['a', 'b', 'c'])\ns.loc['c':'a':-1]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Unique Values from HDFStore in Pandas\nDESCRIPTION: Shows how to retrieve unique values from indexable or data columns in an HDFStore table, though this method is deprecated as of Pandas 0.14.0.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.1.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# note that this is deprecated as of 0.14.0\n# can be replicated by: store.select_column('df','index').unique()\nstore.unique(\"df\", \"index\")\nstore.unique(\"df\", \"string\")\n```\n\n----------------------------------------\n\nTITLE: get_dummies Previous Behavior Data Types - Pandas - Python\nDESCRIPTION: Illustrates the dtypes returned by pd.get_dummies before pandas 0.19.0, where dummy-coded columns were of dtype float64. Only pandas is required; input is a list of strings, output is a Series of dtypes.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nIn [1]: pd.get_dummies(['a', 'b', 'a', 'c']).dtypes\n\nOut[1]:\na    float64\nb    float64\nc    float64\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Aligning and Expanding Panel Axes with .loc Assignment - Pandas - Python\nDESCRIPTION: Shows construction of a 3D pandas Panel, alignment of its axes, and dynamic addition of a new minor axis by assigning a Series via .loc. The code demonstrates axis growth, auto-alignment, and selection of new axis values. Requires pandas and numpy, and Panel (now deprecated in recent pandas versions). Inputs: 3D numpy array. Output: Panel with expanded minor_axis, selection by .loc, and resulting DataFrame display.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_6\n\nLANGUAGE: ipython\nCODE:\n```\nIn [20]: p = pd.Panel(np.arange(16).reshape(2, 4, 2),\\n   ....:              items=['Item1', 'Item2'],\\n   ....:              major_axis=pd.date_range('2001/1/12', periods=4),\\n   ....:              minor_axis=['A', 'B'], dtype='float64')\\n   ....:\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [21]: p\\nOut[21]:\\n<class 'pandas.core.panel.Panel'>\\nDimensions: 2 (items) x 4 (major_axis) x 2 (minor_axis)\\nItems axis: Item1 to Item2\\nMajor_axis axis: 2001-01-12 00:00:00 to 2001-01-15 00:00:00\\nMinor_axis axis: A to B\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [22]: p.loc[:, :, 'C'] = pd.Series([30, 32], index=p.items)\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [23]: p\\nOut[23]:\\n<class 'pandas.core.panel.Panel'>\\nDimensions: 2 (items) x 4 (major_axis) x 3 (minor_axis)\\nItems axis: Item1 to Item2\\nMajor_axis axis: 2001-01-12 00:00:00 to 2001-01-15 00:00:00\\nMinor_axis axis: A to C\n```\n\nLANGUAGE: ipython\nCODE:\n```\nIn [24]: p.loc[:, :, 'C']\\nOut[24]:\\n           Item1  Item2\\n2001-01-12   30.0   32.0\\n2001-01-13   30.0   32.0\\n2001-01-14   30.0   32.0\\n2001-01-15   30.0   32.0\n```\n\n----------------------------------------\n\nTITLE: Resample Operation Dtype Changes - Pandas 0.25.x\nDESCRIPTION: Example showing how resample operation handles categorical dtypes in Pandas 0.25.x version.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_27\n\nLANGUAGE: ipython\nCODE:\n```\nIn [1]> df.resample(\"2D\").agg(lambda x: 'a').A.dtype\nOut[1]:\nCategoricalDtype(categories=['a', 'b'], ordered=False)\n```\n\n----------------------------------------\n\nTITLE: Interactive Try Button HTML\nDESCRIPTION: HTML markup for a primary button that links to an interactive Pandas shell.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/getting_started.md#_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<p>\n    <a class=\"btn btn-primary\" href=\"{{ base_url }}try.html\">Try it in your browser</a>\n</p>\n```\n\n----------------------------------------\n\nTITLE: Defining pip Requirements for pandas Development Environment - plaintext\nDESCRIPTION: This requirements listing details all Python package dependencies needed to build, test, and develop the pandas project. The entries may include version constraints and extra requirement markers, and are meant to be processed by pip. It relies on a separate environment.yml file for comments and manual edits, as this file is auto-generated. Outputs are used to provision local or CI development environments with all the necessary tools and libraries for pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/requirements-dev.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# This file is auto-generated from environment.yml, do not modify.\\n# See that file for comments about the need/usage of each dependency.\\n\\npip\\nversioneer[toml]\\ncython~=3.0.5\\nmeson[ninja]==1.2.1\\nmeson-python==0.13.1\\npytest>=7.3.2\\npytest-cov\\npytest-xdist>=3.4.0\\npytest-qt>=4.4.0\\npytest-localserver\\nPyQt5>=5.15.9\\ncoverage\\npython-dateutil\\nnumpy<3\\nbeautifulsoup4>=4.12.3\\nblosc\\nbottleneck>=1.3.6\\nfastparquet>=2024.2.0\\nfsspec>=2024.2.0\\nhtml5lib>=1.1\\nhypothesis>=6.84.0\\ngcsfs>=2024.2.0\\nipython\\npickleshare\\njinja2>=3.1.3\\nlxml>=4.9.2\\nmatplotlib>=3.8.3\\nnumba>=0.59.0\\nnumexpr>=2.9.0\\nopenpyxl>=3.1.2\\nodfpy>=1.4.1\\npsycopg2-binary>=2.9.6\\npyarrow>=10.0.1\\npymysql>=1.1.0\\npyreadstat>=1.2.6\\ntables>=3.8.0\\npython-calamine>=0.1.7\\npytz>=2023.4\\npyxlsb>=1.0.10\\ns3fs>=2024.2.0\\nscipy>=1.12.0\\nSQLAlchemy>=2.0.0\\ntabulate>=0.9.0\\nxarray>=2024.1.1, <=2024.9.0\\nxlrd>=2.0.1\\nxlsxwriter>=3.2.0\\nzstandard>=0.22.0\\ndask\\nseaborn\\nmoto\\nflask\\nasv>=0.6.1\\nflake8==7.1.0\\nmypy==1.13.0\\ntokenize-rt\\npre-commit>=4.2.0\\ngitpython\\ngitdb\\ngoogle-auth\\nnatsort\\numpydoc\\npydata-sphinx-theme==0.16\\npytest-cython\\nsphinx\\nsphinx-design\\nsphinx-copybutton\\ntypes-python-dateutil\\ntypes-PyMySQL\\ntypes-pytz\\ntypes-PyYAML\\ntypes-setuptools\\nnbconvert>=7.11.0\\nnbsphinx\\npandoc\\nipywidgets\\nnbformat\\nnotebook>=7.0.6\\nipykernel\\nmarkdown\\nfeedparser\\npyyaml\\nrequests\\npygments\\njupyterlite-core\\njupyterlite-pyodide-kernel\\nadbc-driver-postgresql>=0.10.0\\nadbc-driver-sqlite>=0.8.0\\ntyping_extensions; python_version<\\\"3.11\\\"\\ntzdata>=2022.7\n```\n\n----------------------------------------\n\nTITLE: Accessing Series with Deprecated List Slice\nDESCRIPTION: Example of deprecated syntax for accessing Series with a single-item list containing a slice. The recommended approach is to either convert the list to a tuple or pass the slice directly.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nser[[slice(0, 4)]]  # deprecated\nser[slice(0, 4)]   # recommended\n```\n\n----------------------------------------\n\nTITLE: Variable Assignment and Explicit Print in Python\nDESCRIPTION: This snippet shows the equivalent Python code for variable assignment and explicit printing. It illustrates the difference between standard Python and IPython notebook output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/index.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\na = 1\nprint(a)\n```\n\n----------------------------------------\n\nTITLE: Installing pandas in Development Mode with pip (Shell)\nDESCRIPTION: This snippet shows how to install pandas in development (editable) mode using pip. The command enables developers to modify the source code and have changes immediately reflected without reinstalling the package. The '--no-build-isolation' flag ensures that the build uses the environment's installed dependencies; '-ve' specifies editable install. Additional build options are supplied as well. Requires pip and all build dependencies to be pre-installed, and should be run inside the pandas source directory.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython -m pip install -ve . --no-build-isolation -Ceditable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Rendering Past Institutional Partners Section Using Jinja2 For-Loop in HTML\nDESCRIPTION: Using a Jinja2 for-loop, this snippet iterates over 'sponsors.past' and filters for partners by checking 'kind == \"partner\"'. Each past partner is displayed as a linked name in an HTML unordered list. Assumes 'sponsors.past' is populated by the backend and each entry has a 'url' and 'name' property.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/sponsors.md#_snippet_3\n\nLANGUAGE: Jinja2\nCODE:\n```\n<ul>\\n    {% for company in sponsors.past if company.kind == \"partner\" %}\\n        <li><a href=\"{{ company.url }}\">{{ company.name }}</a></li>\\n    {% endfor %}\\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Python Assignment Semantics for pandas DataFrames (Python)\nDESCRIPTION: This code block explains that assigning a DataFrame to a new variable does not create a copy, but creates an alias pointing to the exact same object in memory. Modifications to either variable are reflected in both, as shown by identical object ids and value mutation results. Dependencies: pandas. Inputs: DataFrame assignment and mutation. Outputs: verification using `id()` and value assignment. Key limitation: true copies are only created by explicit copying mechanisms, not simple assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> df1 = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n>>> df2 = df1\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> id(df1) == id(df2)  # or: df1 is df2\nTrue\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> df1.iloc[0, 0]\n1\n>>> df2.iloc[0, 0] = 10\n>>> df1.iloc[0, 0]\n10\n```\n\n----------------------------------------\n\nTITLE: Handling All-sparse SparseArray Filled with NaN in DataFrame (Python)\nDESCRIPTION: Fixed an issue where a DataFrame containing an all-sparse SparseArray filled with NaN was not properly handled when indexed by a list-like object.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_55\n\nLANGUAGE: Python\nCODE:\n```\ndf[df.columns] # where df contains all-sparse SparseArray filled with NaN\n```\n\n----------------------------------------\n\nTITLE: Accessing PyTables Nodes and Key-Based Access - pandas - Python\nDESCRIPTION: Contrasts two approaches for accessing tables in HDFStore: (1) dotted (attribute) access through the PyTables root node, useful for direct node manipulation, and (2) string key-based access, the recommended way for nested keys. Demonstrates syntax/limitations for both. The first snippet may raise exceptions if the node does not exist or is not at the root.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_174\n\nLANGUAGE: python\nCODE:\n```\nstore.foo.bar.bah\n```\n\nLANGUAGE: python\nCODE:\n```\nstore.root.foo.bar.bah\n```\n\nLANGUAGE: python\nCODE:\n```\nstore[\"foo/bar/bah\"]\n```\n\n----------------------------------------\n\nTITLE: Chained Assignment Warning Demo in pandas\nDESCRIPTION: Demonstrates the new warning system for chained assignments in pandas, showing both incorrect and correct methods of assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndfc = pd.DataFrame({'A': ['aaa', 'bbb', 'ccc'], 'B': [1, 2, 3]})\npd.set_option('chained_assignment', 'warn')\n\ndfc.loc[0]['A'] = 1111\n\ndfc.loc[0, 'A'] = 11\ndfc\n```\n\n----------------------------------------\n\nTITLE: Installing Excel Dependencies for Pandas\nDESCRIPTION: Command to install dependencies for Excel file support in pandas. This includes packages for reading and writing various Excel file formats.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install \"pandas[excel]\"\n```\n\n----------------------------------------\n\nTITLE: Converting Panel4D to xarray using to_xarray() method\nDESCRIPTION: Panel4D objects can now be converted to xarray objects using the to_xarray() method, as Panel4D constructors are being deprecated.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.19.0.rst#_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\npanel4d.to_xarray()\n```\n\n----------------------------------------\n\nTITLE: Writing and Reading DataFrames using HDF5 with pandas (Python)\nDESCRIPTION: This snippet demonstrates how to serialize a pandas DataFrame with missing values to an HDF5 file using the 'to_hdf' method, and subsequently load it back with 'read_hdf'. The 'format=\"table\"' argument enables queryable storage, and the 'mode=\"w\"' argument ensures the file is overwritten. Required dependencies: pandas, pytables (for HDF5 support). Input is a DataFrame (possibly with NaN values); output is the same DataFrame persisted and read from disk. The file is named 'file.h5', and should exist in the working directory.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndf_with_missing.to_hdf(\"file.h5\", key=\"df_with_missing\", format=\"table\", mode=\"w\")\n\npd.read_hdf(\"file.h5\", \"df_with_missing\")\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns from HDFStore\nDESCRIPTION: Shows how to filter columns when retrieving data from HDFStore using the columns keyword, equivalent to using a Term('columns', list_of_columns_to_filter).\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.1.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstore.select(\"df\", columns=[\"A\", \"B\"])\n```\n\n----------------------------------------\n\nTITLE: Chained Assignment with Switched Index Order in pandas - Python\nDESCRIPTION: This snippet demonstrates a chained assignment in pandas with the selection order reversed (`df['B'][df['B'] > 3] = 10`). In Copy-on-Write mode, neither the parent DataFrame nor the resulting subset is changed by this assignment due to the enforced copy semantics for all indexing. Required dependency: pandas with Copy-on-Write enabled, and a DataFrame df containing a column \"B\". No output is produced; df is not mutated.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> df['B'][df['B'] > 3] = 10\n```\n\n----------------------------------------\n\nTITLE: Documenting Changelog Entries using reStructuredText - reStructuredText\nDESCRIPTION: This snippet uses reStructuredText markup to clearly define the sections of a changelog for the pandas project, version 0.7.1. It includes headings, bullet lists of new features and improvements, and a directive for listing contributors. The content requires Sphinx or compatible documentation tools to render directives (such as '.. contributors::') and issue references. The inputs are plain text features, while outputs appear as structured documentation in the rendered documentation. Constraints include reliance on correct reST syntax and Sphinx extensions for specialized directives.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.1.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _whatsnew_0701:\n\nVersion 0.7.1 (February 29, 2012)\n---------------------------------\n\n{{ header }}\n\n\nThis release includes a few new features and addresses over a dozen bugs in\n0.7.0.\n\nNew features\n~~~~~~~~~~~~\n\n  - Add ``to_clipboard`` function to pandas namespace for writing objects to\n    the system clipboard (:issue:`774`)\n  - Add ``itertuples`` method to DataFrame for iterating through the rows of a\n    dataframe as tuples (:issue:`818`)\n  - Add ability to pass fill_value and method to DataFrame and Series align\n    method (:issue:`806`, :issue:`807`)\n  - Add fill_value option to reindex, align methods (:issue:`784`)\n  - Enable concat to produce DataFrame from Series (:issue:`787`)\n  - Add ``between`` method to Series (:issue:`802`)\n  - Add HTML representation hook to DataFrame for the IPython HTML notebook\n    (:issue:`773`)\n  - Support for reading Excel 2007 XML documents using openpyxl\n\nPerformance improvements\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n  - Improve performance and memory usage of fillna on DataFrame\n  - Can concatenate a list of Series along axis=1 to obtain a DataFrame (:issue:`787`)\n\n\n\n.. _whatsnew_0.7.1.contributors:\n\nContributors\n~~~~~~~~~~~~\n\n.. contributors:: v0.7.0..v0.7.1\n```\n\n----------------------------------------\n\nTITLE: Creating Symlink for Compilation Database\nDESCRIPTION: Command to create a symlink to the compilation database, which can be used by IDEs and language servers for code completion and error checking.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/debugging_extensions.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nln -s debug/compile_commands.json .\n```\n\n----------------------------------------\n\nTITLE: Documentation Note Block in RST\nDESCRIPTION: ReStructuredText note block explaining pandas' policy on behavior-changing bug fixes in minor or patch releases.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/policies.rst#_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. note::\n\n   pandas will sometimes make *behavior changing* bug fixes, as part of\n   minor or patch releases. Whether or not a change is a bug fix or an\n   API-breaking change is a judgement call. We'll do our best, and we\n   invite you to participate in development discussion on the issue\n   tracker or mailing list.\n```\n\n----------------------------------------\n\nTITLE: Aggregating Data in R\nDESCRIPTION: Demonstrates how to split data into subsets and compute the mean for each group in R using the aggregate function.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_2\n\nLANGUAGE: r\nCODE:\n```\ndf <- data.frame(\n  v1 = c(1,3,5,7,8,3,5,NA,4,5,7,9),\n  v2 = c(11,33,55,77,88,33,55,NA,44,55,77,99),\n  by1 = c(\"red\", \"blue\", 1, 2, NA, \"big\", 1, 2, \"red\", 1, NA, 12),\n  by2 = c(\"wet\", \"dry\", 99, 95, NA, \"damp\", 95, 99, \"red\", 99, NA, NA))\naggregate(x=df[, c(\"v1\", \"v2\")], by=list(mydf2$by1, mydf2$by2), FUN = mean)\n```\n\n----------------------------------------\n\nTITLE: Extract Nth Word from String - Stata\nDESCRIPTION: Shows how to construct a small string data set, then extract the first and last words from a 'name' column using 'word(name, 1)' and 'word(name, -1)'. Prerequisites: None. Output: Columns 'first_name' and 'last_name' with parsed values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_18\n\nLANGUAGE: stata\nCODE:\n```\nclear\\ninput str20 string\\n\"John Smith\"\\n\"Jane Cook\"\\nend\\n\\ngenerate first_name = word(name, 1)\\ngenerate last_name = word(name, -1)\n```\n\n----------------------------------------\n\nTITLE: Rendering Sponsor Organizations with Jinja2 For-Loop in HTML\nDESCRIPTION: This snippet renders a list of sponsor organizations using a Jinja2 for-loop in an HTML unordered list. It iterates through 'sponsors.active' and displays those with 'regular' kind, outputting each sponsor's name as a link and their description. Requires a backend data structure 'sponsors.active' and expects it to provide 'url', 'name', and 'description' attributes per sponsor.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/sponsors.md#_snippet_1\n\nLANGUAGE: Jinja2\nCODE:\n```\n<ul>\\n    {% for company in sponsors.active if company.kind == \"regular\" %}\\n        <li><a href=\"{{ company.url }}\">{{ company.name }}</a>: {{ company.description }}</li>\\n    {% endfor %}\\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Handling Empty Series in where Method (Python)\nDESCRIPTION: Resolved an issue with Series.where when dealing with an empty Series and empty condition having non-bool dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.0.rst#_snippet_53\n\nLANGUAGE: Python\nCODE:\n```\nempty_series.where(pd.Series(dtype=int))\n```\n\n----------------------------------------\n\nTITLE: Filtering with isin Conditional Function in pandas (Python)\nDESCRIPTION: This snippet uses the .isin() method to filter rows where the 'Pclass' column is 2 or 3. By applying titanic[\"Pclass\"].isin([2, 3]) inside the selection brackets, it returns a DataFrame with only those passengers in classes 2 or 3. head() displays sample rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass_23 = titanic[titanic[\"Pclass\"].isin([2, 3])]\nclass_23.head()\n```\n\n----------------------------------------\n\nTITLE: Slicing Second Level of MultiIndex in Python\nDESCRIPTION: Selects data from the second level of a MultiIndex. This demonstrates how to access specific elements within a hierarchical index structure.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndf.xs(\"six\", level=1, axis=0)\n```\n\n----------------------------------------\n\nTITLE: Referencing Bug Fix in DataFrameGroupBy.quantile Method (RST)\nDESCRIPTION: Documents a bug fix in the DataFrameGroupBy.quantile method where NA values in grouping could cause segfaults or incorrect results. The fix is referenced by an issue number.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.3.rst#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n- Bug in :meth:`DataFrameGroupBy.quantile` where NA values in the grouping could cause segfaults or incorrect results (:issue:`28882`)\n```\n\n----------------------------------------\n\nTITLE: Assigning to Slices in pandas DataFrames - Python\nDESCRIPTION: Demonstrates directly assigning values to a slice of a DataFrame column (df['B'][0:5]) which can inadvertently produce a chained assignment and may raise a ChainedAssignmentError or SettingWithCopyWarning. Requires pandas as a dependency, with a DataFrame (df) populated. The assignment modifies rows 0 through 4 in column 'B', but due to pandas' reference/storage rules, such assignment is discouraged; intended inputs are Series-slice assignment, output is an updated (or not, if misapplied) DataFrame. Limitations arise from unintended copy/view ambiguity in pandas.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n>>> df['B'][0:5] = 10\n```\n\n----------------------------------------\n\nTITLE: Warning on Incompatible Assignment in Series for Future Pandas (ipython)\nDESCRIPTION: This code demonstrates the new pandas behavior where assigning a value with incompatible dtype to a Series element issues a FutureWarning. The warning explains that such assignments will raise errors in the future, and users are prompted to cast Series types explicitly. Requires the pandas library; parameters and outputs are as above but includes shown warning text.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v2.1.0.rst#_snippet_7\n\nLANGUAGE: ipython\nCODE:\n```\n  In [1]: ser = pd.Series([1, 2, 3])\n\n  In [2]: ser\n  Out[2]:\n  0    1\n  1    2\n  2    3\n  dtype: int64\n\n  In [3]: ser[0] = 'not an int64'\n  FutureWarning:\n    Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas.\n    Value 'not an int64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\n  In [4]: ser\n  Out[4]:\n  0    not an int64\n  1               2\n  2               3\n  dtype: object\n```\n\n----------------------------------------\n\nTITLE: Multi-table Creation and Selection in HDFStore\nDESCRIPTION: Demonstrates creating multiple tables with append_to_multiple and selecting from them with select_as_multiple, allowing operations across multiple tables using a selector table.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.1.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf_mt = pd.DataFrame(\n    np.random.randn(8, 6),\n    index=pd.date_range(\"1/1/2000\", periods=8),\n    columns=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n)\ndf_mt[\"foo\"] = \"bar\"\n\n# you can also create the tables individually\nstore.append_to_multiple(\n    {\"df1_mt\": [\"A\", \"B\"], \"df2_mt\": None}, df_mt, selector=\"df1_mt\"\n)\nstore\n\n# individual tables were created\nstore.select(\"df1_mt\")\nstore.select(\"df2_mt\")\n\n# as a multiple\nstore.select_as_multiple(\n    [\"df1_mt\", \"df2_mt\"], where=[\"A>0\", \"B>0\"], selector=\"df1_mt\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining pandas.DataFrame method: method Example in Python\nDESCRIPTION: Provides an anti-example emphasizing what not to do when drafting docstrings and method usage. Uses a DataFrame with random numbers, demonstrates both positional and keyword arguments, and outputs sample results. Explicitly imports numpy and pandas to mirror bad patterns.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef method(foo=None, bar=None):\n    \"\"\"\n    A sample DataFrame method.\n\n    Do not import NumPy and pandas.\n\n    Try to use meaningful data, when it makes the example easier\n    to understand.\n\n    Try to avoid positional arguments like in ``df.method(1)``. They\n    can be all right if previously defined with a meaningful name,\n    like in ``present_value(interest_rate)``, but avoid them otherwise.\n\n    When presenting the behavior with different parameters, do not place\n    all the calls one next to the other. Instead, add a short sentence\n    explaining what the example shows.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import pandas as pd\n    >>> df = pd.DataFrame(np.random.randn(3, 3),\n    ...                   columns=('a', 'b', 'c'))\n    >>> df.method(1)\n    21\n    >>> df.method(bar=14)\n    123\n    \"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Suppressing Comments in CSV Parsing\nDESCRIPTION: Demonstrates how to use the comment parameter to remove inline comments from each line when reading CSV data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\"tmp.csv\", comment=\"#\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Generating API Reference Documentation with Sphinx/Jinja - reStructuredText\nDESCRIPTION: This snippet is an example of a Sphinx documentation template that uses reStructuredText combined with Jinja templating. It displays the object's full name, underlines it, sets the Sphinx documentation context to the target module, and automatically includes documentation for a method or property accessor using the autoaccessormethod directive. Dependencies include Sphinx (with the autodoc extension), Jinja, and appropriately configured variables such as 'fullname', 'underline', 'module', 'objname' in the template context. Input data must provide the module path and name for correct substitution, and the template is primarily for use in building Python API documentation. Output is generated HTML or docset documentation for Python accessor methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/_templates/autosummary/accessor_method.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n{{ fullname }}\n{{ underline }}\n\n.. currentmodule:: {{ module.split('.')[0] }}\n\n.. autoaccessormethod:: {{ (module.split('.')[1:] + [objname]) | join('.') }}\n```\n\n----------------------------------------\n\nTITLE: Filtering Missing Values - Stata\nDESCRIPTION: Demonstrates filtering operations to select missing and non-missing values in Stata using conditional expressions on a column (e.g., value_x). Relies on Stata's handling of missing data (represented by .) and uses 'list' to display filtered results. Expects a loaded Stata dataset with a variable named value_x. Returns matching rows based on missing status.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_21\n\nLANGUAGE: stata\nCODE:\n```\n* Keep missing values\\nlist if value_x == .\\n* Keep non-missing values\\nlist if value_x != .\n```\n\n----------------------------------------\n\nTITLE: Merging Data Sets with Outer, Left, and Right Join - Stata\nDESCRIPTION: This sequence of Stata commands demonstrates preparing two data sets (df1 and df2), saving df2 to disk, and performing various joins (outer by default, left, right, inner) using 'merge 1:n key using ...' and filtering with '_merge'. It uses 'preserve' and 'restore' to manage data set state. Inputs: Simulated string key/value columns. Outputs: Merged data sets according to join type. No extra dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_20\n\nLANGUAGE: stata\nCODE:\n```\n* First create df2 and save to disk\\nclear\\ninput str1 key\\nB\\nD\\nD\\nE\\nend\\ngenerate value = rnormal()\\nsave df2.dta\\n\\n* Now create df1 in memory\\nclear\\ninput str1 key\\nA\\nB\\nC\\nD\\nend\\ngenerate value = rnormal()\\n\\npreserve\\n\\n* Left join\\nmerge 1:n key using df2.dta\\nkeep if _merge == 1\\n\\n* Right join\\nrestore, preserve\\nmerge 1:n key using df2.dta\\nkeep if _merge == 2\\n\\n* Inner join\\nrestore, preserve\\nmerge 1:n key using df2.dta\n```\n\n----------------------------------------\n\nTITLE: Bug Fixed Method References in Pandas 1.1.2\nDESCRIPTION: Code references to Pandas methods where bugs were fixed, including DataFrame.eval, Series constructor, DataFrame.apply, and various other methods.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.1.2.rst#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame.eval\nSeries()\nDataFrame.apply\nSeries.astype\nDataFrame.astype\nDateTimeIndex.format\nPeriodIndex.format\nFloat64Index.__contains__\nSeries.dt.isocalendar\nDatetimeIndex.isocalendar\nDataFrame.corr\nimport_optional_dependency\n```\n\n----------------------------------------\n\nTITLE: Upcasting Data Types during DataFrame Operations - Python\nDESCRIPTION: Shows how combining DataFrames (e.g., reindexing and arithmetic between differing types) may lead to upcasting according to NumPy rules. Requires pandas and NumPy; expects 'df1' and 'df2' to be previously defined. Outputs include the resulting DataFrame ('df3') and its dtypes, illustrating promotion of smaller types to more general types upon operation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/basics.rst#_snippet_79\n\nLANGUAGE: python\nCODE:\n```\ndf3 = df1.reindex_like(df2).fillna(value=0.0) + df2\ndf3\ndf3.dtypes\n```\n\n----------------------------------------\n\nTITLE: Comparing Performance of String Operations: Length Calculation\nDESCRIPTION: This snippet compares the performance of the str.len() operation between object dtype and PyArrow string dtype Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0010-required-pyarrow-dependency.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nIn[1]: %timeit ser_object.str.len()\n118 ms ± 260 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nIn[2]: %timeit ser_string.str.len()\n24.2 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n```\n\n----------------------------------------\n\nTITLE: Executing MeeseeksDev Backport Command\nDESCRIPTION: Command to trigger the MeeseeksDev bot to backport changes to a specific version branch. Used for backporting merged pull requests to maintenance branches.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/maintaining.rst#_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n@Meeseeksdev backport <branch>\n```\n\n----------------------------------------\n\nTITLE: DataFrame Method Usage in Pandas 1.2.4\nDESCRIPTION: Sample methods and operations that were fixed in Pandas 1.2.4, including DataFrame.sum, DataFrame.to_json, DataFrame.where, DataFrame.replace, and NumPy ufunc operations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.2.4.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nDataFrame.sum(min_count=n) # Fixed ValueError when min_count > DataFrame shape\nDataFrame.to_json() # Fixed AttributeError on PyPy\npd.NaT == np.array # Fixed comparison returning scalar instead of array\nDataFrame.where(condition) # Fixed copy return behavior\nDataFrame.replace(regex_dict) # Fixed IndexError with multi-key dictionary\nDataFrame.to_string() # Fixed float_format respect in object columns\nnp.add(DataFrame, value) # Fixed argument passing for NumPy ufuncs\n```\n\n----------------------------------------\n\nTITLE: Expanded Equivalence of Chained Assignment via Explicit Copy - Python\nDESCRIPTION: This snippet breaks down chained assignment into two explicit steps: first, creating a masked subset (df2 = df[df['B'] > 3]), then assigning a new value to its 'B' column (df2['B'] = 10). As with Copy-on-Write, this leaves the original DataFrame (df) unmodified. Dependencies: pandas with Copy-on-Write enabled, an existing df. df2 is a new object and deletion of df2 does not affect df.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> df2 = df[df['B'] > 3]  # Copy under NumPy's rules\n>>> df2['B'] = 10  # Update (the copy) df2, df not changed\n>>> del df2  # All references to df2 are lost, goes out of scope\n```\n\n----------------------------------------\n\nTITLE: Displaying Python Version Usage in pandas\nDESCRIPTION: This code snippet shows the distribution of Python versions used by pandas users, indicating a strong preference for Python 3.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/blog/2019-user-survey.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n3        92.39%\n2 & 3     6.80%\n2         0.81%\nName: Python 2 or 3?, dtype: object\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Unique Value Extraction for Categoricals in pandas (Python)\nDESCRIPTION: This snippet shows the behavioral change in pandas 0.17.0 where Categorical.unique returns a new Categorical rather than a NumPy array. It demonstrates creation of both ordered and unordered Categoricals, their content, and unique value extraction. Key parameters: categories (optional ordering), input values. Output is a Categorical instance with unique values. Required: pandas. The new method sorts unordered by appearance, keeps category order for ordered.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.17.0.rst#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ncat = pd.Categorical([\"C\", \"A\", \"B\", \"C\"], categories=[\"A\", \"B\", \"C\"], ordered=True)\ncat\ncat.unique()\n\ncat = pd.Categorical([\"C\", \"A\", \"B\", \"C\"], categories=[\"A\", \"B\", \"C\"])\ncat\ncat.unique()\n```\n\n----------------------------------------\n\nTITLE: Displaying Workgroup Information with Jinja2 Templating - HTML/Jinja2\nDESCRIPTION: This snippet loops through all workgroups using Jinja2 templating to display their name, contact link, responsibilities, and members. Each workgroup is rendered as an HTML block with a mailto link generated for contact, a description of responsibilities, and a list of members, marking the lead. Dependencies include the workgroups data structure provided in the Jinja2 context. Inputs are dictionaries of workgroup information; outputs are structured HTML sections for each workgroup.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/team.md#_snippet_1\n\nLANGUAGE: jinja\nCODE:\n```\n{% for k, workgroup in workgroups.items() %}\\n\\n### {{ workgroup.name }}\\n\\n<ul>\\n    <li><b>Contact:</b>\\n        <a id=\\\"{{ workgroup.name|replace(' ', '-') }}\\\" href=\\\"mailto:asp.{{ workgroup.contact }}\\\">asp.{{ workgroup.contact }}</a>\\n        <script TYPE=\\\"text/javascript\\\">\\n            var mail_tag_id = '{{ workgroup.name|replace(' ', '-') }}';\\n            var mail_tag_element = document.getElementById( mail_tag_id );\\n            mail_tag_element.innerHTML = mail_tag_element.innerHTML.replace(/^asp./, \\\"\\\");\\n            mail_tag_element.setAttribute('href', \\\"mailto:\\\"+mail_tag_element.innerHTML);\\n        </script>\\n    </li>\\n    <li><b>Responsibilities:</b> {{ workgroup.responsibilities }}</li>\\n    <li><b>Members:</b>\\n        <ul>\\n            {% for person in workgroup.members %}\\n                <li>{{ person }}{% if loop.first %} (lead){% endif %}</li>\\n            {% endfor %}\\n        </ul>\\n    </li>\\n</ul>\\n\\n{% endfor %}\n```\n\n----------------------------------------\n\nTITLE: Serializing and Deserializing DataFrames with NTV-pandas (Python)\nDESCRIPTION: This snippet demonstrates how to convert a pandas DataFrame to a JSON string using ntv-pandas and reload it as a DataFrame, ensuring round-trip fidelity. It shows the usage of the npd.to_json method (with optional Table Schema mode) and npd.read_json for restoring the DataFrame from JSON. Dependencies include ntv_pandas and pandas. Inputs must be pandas DataFrames and outputs are JSON strings and DataFrames, with correctness verified using DataFrame.equals.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport ntv_pandas as npd\n\njsn = df.npd.to_json(table=False)  # save df as a JSON-value (format Table Schema if table is True else format NTV )\ndf  = npd.read_json(jsn)  # load a JSON-value as a `DataFrame`\n\ndf.equals(npd.read_json(df.npd.to_json(df)))  # `True` in any case, whether `table=True` or not\n```\n\n----------------------------------------\n\nTITLE: Summarizing by Groups with ddply - plyr - R\nDESCRIPTION: This R snippet demonstrates how to use plyr's ddply to group a data.frame by the 'month' and 'week' columns and compute summary statistics (mean and standard deviation) for the 'x' column. It requires the plyr library, and expects a data.frame named 'df' with numeric columns. The output is a summarized data.frame; key inputs are the column names used for grouping and the summarization functions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_12\n\nLANGUAGE: R\nCODE:\n```\nrequire(plyr)\ndf <- data.frame(\n  x = runif(120, 1, 168),\n  y = runif(120, 7, 334),\n  z = runif(120, 1.7, 20.7),\n  month = rep(c(5,6,7,8),30),\n  week = sample(1:4, 120, TRUE)\n)\n\nddply(df, .(month, week), summarize,\n      mean = round(mean(x), 2),\n      sd = round(sd(x), 2))\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows by Condition - Stata\nDESCRIPTION: Filters and displays rows where 'total_bill' is greater than 10, using 'list if total_bill > 10'. Input: Data in memory. Output: Rows matching the condition. No dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_10\n\nLANGUAGE: stata\nCODE:\n```\nlist if total_bill > 10\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Temporary CSV File\nDESCRIPTION: Removes the temporary CSV file created for the comment handling example.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nos.remove(\"tmp.csv\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Collapsible Data Section with Link - HTML\nDESCRIPTION: This HTML snippet creates a collapsible content section presenting contextual information about the air quality dataset used in the Pandas tutorial. It uses Bootstrap classes and collapse functionality to toggle the display of data source details and provides a direct download link for the raw CSV file. The snippet assumes Bootstrap dependencies are loaded and requires proper container setup for UI integration.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/includes/air_quality_no2.rst#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div data-bs-toggle=\\\"collapse\\\" href=\\\"#collapsedata\\\" role=\\\"button\\\" aria-expanded=\\\"false\\\" aria-controls=\\\"collapsedata\\\">\\n    <span class=\\\"badge bg-secondary\\\">Air quality data</span>\\n</div>\\n<div class=\\\"collapse\\\" id=\\\"collapsedata\\\">\\n    <div class=\\\"card-body\\\">\\n        <p class=\\\"card-text\\\">\\n\\nFor this tutorial, air quality data about :math:`NO_2` is used, made\\navailable by `OpenAQ <https://openaq.org>`__ and using the\\n`py-openaq <http://dhhagan.github.io/py-openaq/index.html>`__ package.\\nThe ``air_quality_no2.csv`` data set provides :math:`NO_2` values for\\nthe measurement stations *FR04014*, *BETR801* and *London Westminster*\\nin respectively Paris, Antwerp and London.\\n\\n</p>\\n        <a href=\\\"https://github.com/pandas-dev/pandas/tree/main/doc/data/air_quality_no2.csv\\\" class=\\\"btn btn-dark btn-sm\\\">To raw data</a>\\n    </div>\\n</div>\n```\n\n----------------------------------------\n\nTITLE: Pushing Updated Feature Branch to GitHub\nDESCRIPTION: Command to push local feature branch changes to GitHub, which automatically updates the pull request and triggers CI checks.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing.rst#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ngit push origin shiny-new-feature\n```\n\n----------------------------------------\n\nTITLE: Sphinx autosummary directive for DataFrame conversion function\nDESCRIPTION: RestructuredText directive listing Pandas function for importing from other DataFrame libraries\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/reference/general_functions.rst#_snippet_8\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n   :toctree: api/\n\n   api.interchange.from_dataframe\n```\n\n----------------------------------------\n\nTITLE: Fixed performance regression in GroupBy first and last methods with StringDtype\nDESCRIPTION: Resolves performance issues in DataFrameGroupBy.first, SeriesGroupBy.first, DataFrameGroupBy.last, and SeriesGroupBy.last methods when used with StringDtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nDataFrameGroupBy.first()\nSeriesGroupBy.first()\nDataFrameGroupBy.last()\nSeriesGroupBy.last()\n```\n\n----------------------------------------\n\nTITLE: Sorting a Dataset by Multiple Columns using PROC SORT - SAS\nDESCRIPTION: Uses 'PROC SORT' in SAS to order the dataset 'tips' by the variables 'sex' and 'total_bill'. No new columns are created, and the output is the sorted 'tips' dataset. Input: unsorted dataset; Output: sorted dataset. Applies to scenarios with single or multiple sort keys.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_12\n\nLANGUAGE: SAS\nCODE:\n```\nproc sort data=tips;\\n    by sex total_bill;\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Writing and Iteratively Reading HDF5 with pandas.read_hdf - Python\nDESCRIPTION: This snippet demonstrates chunked writing and iteration support using pandas with HDFStore via PyTables, showing how to write a DataFrame to an HDF5 file and then iterate over it with read_hdf's chunksize parameter. Requires: pandas, PyTables (with support for the table format), numpy, and the ability to write to disk. Inputs are a DataFrame and chunked read size; outputs are DataFrame chunks. Only tables can be read in iterator mode.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.12.0.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npath = 'store_iterator.h5'\npd.DataFrame(np.random.randn(10, 2)).to_hdf(path, 'df', table=True)\nfor df in pd.read_hdf(path, 'df', chunksize=3):\n    print(df)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrames with NoRowIndex and Appending (Python)\nDESCRIPTION: Shows the creation of DataFrames using the hypothetical NoRowIndex as their index. Demonstrates appending one NoRowIndex DataFrame to another using pd.concat, resulting in a DataFrame that still uses NoRowIndex, per the proposal. Shows expected input/output structure, but NoRowIndex is not present in modern pandas. Input: DataFrames with NoRowIndex; Output: Concatenated DataFrame with NoRowIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nIn [6]: df1 = pd.DataFrame({'a': [1,  2], 'b': [4, 5]}, index=NoRowIndex(2))\n\nIn [7]: df2 = pd.DataFrame({'a': [4], 'b': [0]}, index=NoRowIndex(1))\n\nIn [8]: df1\nOut[8]:\n a  b\n 1  4\n 2  5\n\nIn [9]: df2\nOut[9]:\n a  b\n 4  0\n\nIn [10]: pd.concat([df1, df2])\nOut[10]:\n a  b\n 1  4\n 2  5\n 4  0\n\nIn [11]: pd.concat([df1, df2]).index\nOut[11]: NoRowIndex(len=3)\n```\n\n----------------------------------------\n\nTITLE: String Case Conversion Functions: UPCASE, LOWCASE, PROPCASE - SAS\nDESCRIPTION: Assigns new columns with the uppercase, lowercase, and proper case transformations of a string using SAS's built-in functions. Inputs are strings entered via 'datalines2'; outputs are additional columns showing each case-modified version of the string. Useful for cleaning and standardizing text data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_17\n\nLANGUAGE: SAS\nCODE:\n```\ndata firstlast;\\ninput String $60.;\\nstring_up = UPCASE(string);\\nstring_low = LOWCASE(string);\\nstring_prop = PROPCASE(string);\\ndatalines2;\\nJohn Smith;\\nJane Cook;\\n;;;\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Updating Symlinks for Pandas Documentation (Bash)\nDESCRIPTION: These commands update the symlinks for the stable documentation on the Pandas web server. They are used to point to the latest version for major/minor releases or to update patch release links.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/maintaining.rst#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd /var/www/html/pandas-docs/\nln -sfn version/2.1 stable\nln -sfn version/2.0.3 version/2.0\n```\n\n----------------------------------------\n\nTITLE: Cleanup: Removing Partitioned Parquet Directory using shutil.rmtree (Python)\nDESCRIPTION: Deletes the directory 'test' and all its contents to clean up after partitioned Parquet file creation. Utilizes shutil.rmtree and handles OSError if the directory does not exist.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_216\n\nLANGUAGE: python\nCODE:\n```\nfrom shutil import rmtree\n\ntry:\n    rmtree(\"test\")\nexcept OSError:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Demonstrating pandas Behavior With Copy-on-Write Enabled\nDESCRIPTION: Shows how enabling the copy-on-write feature prevents modifications to a derived DataFrame from affecting the parent DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.5.0.rst#_snippet_8\n\nLANGUAGE: ipython\nCODE:\n```\nwith pd.option_context(\"mode.copy_on_write\", True):\n    df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": 1})\n    view = df[\"foo\"]\n    view.iloc[0]\n    df\n```\n\n----------------------------------------\n\nTITLE: Evaluating Expressions on DataFrames using pd.eval with NumExpr (Python)\nDESCRIPTION: Benchmarks the use of pandas' eval function (with the NumExpr backend) for fast computation of arithmetic expressions over large DataFrames. Dependencies: pandas, numpy, optional numexpr. Inputs: large DataFrames df1, df2, df3, df4. Output: summed DataFrame. Useful for accelerating compute-bound workloads on large datasets.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.0.rst#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nnrows, ncols = 20000, 100\ndf1, df2, df3, df4 = [pd.DataFrame(np.random.randn(nrows, ncols))\n                      for _ in range(4)]\n```\n\nLANGUAGE: python\nCODE:\n```\n%timeit pd.eval('df1 + df2 + df3 + df4')\n```\n\n----------------------------------------\n\nTITLE: Ambiguous MultiIndex Selection Without Specifying Columns - pandas Python\nDESCRIPTION: Illustrates an ambiguous use of loc where only row selectors are given, omitting column selectors. This form can be misinterpreted in cases where both index and columns are MultiIndex, and thus is generally discouraged for clarity and correctness when manipulating MultiIndexed DataFrames.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n>>> df.loc[(slice('A1', 'A3'), ...)]  # noqa: E901\n```\n\n----------------------------------------\n\nTITLE: Creating Test DataFrame in Python\nDESCRIPTION: This snippet demonstrates how to create a large DataFrame with random data for performance testing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/io.rst#_snippet_243\n\nLANGUAGE: ipython\nCODE:\n```\nsz = 1000000\ndf = pd.DataFrame({'A': np.random.randn(sz), 'B': [1] * sz})\n\ndf.info()\n```\n\n----------------------------------------\n\nTITLE: Rendering Institutional Partners Section Using Jinja2 Loops in HTML\nDESCRIPTION: This snippet uses a Jinja2 for-loop embedded in HTML to iterate through the 'sponsors.active' collection and select those with 'partner' kind. It outputs a list item for each partner including a linked name and description. This requires a backend that supplies 'sponsors.active' as a data source and is typically run within a web framework that supports Jinja2 rendering (e.g., Flask or Django). Expects each company to have 'url', 'name', and 'description' fields.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/sponsors.md#_snippet_0\n\nLANGUAGE: Jinja2\nCODE:\n```\n<ul>\\n    {% for company in sponsors.active if company.kind == \"partner\" %}\\n        <li><a href=\"{{ company.url }}\">{{ company.name }}</a>: {{ company.description }}</li>\\n    {% endfor %}\\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Generating the Pandas Website using a Python Script - Bash\nDESCRIPTION: This bash command generates the website for the pandas documentation by running a Python script with the source directory as an argument. It assumes that the script (pandas_web.py) and its dependencies are present, as well as a suitable Python environment. The command './pandas_web.py pandas' processes configuration files and builds the web output in the specified directory. Inputs include the website sources and configuration YAML; output is a full website build, ready to serve locally.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./pandas_web.py pandas\n```\n\n----------------------------------------\n\nTITLE: Pandas Timestamp Unique Values Example\nDESCRIPTION: Demonstrates getting unique timestamp values from a Pandas Series and Index with timezone information\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.20.0.rst#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\npd.unique(pd.Series([pd.Timestamp('20160101', tz='US/Eastern'),\n               pd.Timestamp('20160101', tz='US/Eastern')]))\n\n# Index, returns a DatetimeIndex\npd.Index([pd.Timestamp('20160101', tz='US/Eastern'),\n          pd.Timestamp('20160101', tz='US/Eastern')]).unique()\npd.unique(pd.Index([pd.Timestamp('20160101', tz='US/Eastern'),\n                    pd.Timestamp('20160101', tz='US/Eastern')]))\n```\n\n----------------------------------------\n\nTITLE: Documenting Copyright and License Banner Usage - Python\nDESCRIPTION: This snippet provides the standardized copyright and license banner to be included at the top of pandas source code files. It indicates the copyright holder (PyData Development Team), summarizes the rights retained, and associates the file with the BSD Simplified License. To use it, place this banner as the first lines in each Python source file. There are no runtime dependencies, as this is a comment block. The banner must be preserved exactly for clarity and legal compliance.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/AUTHORS.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#-----------------------------------------------------------------------------\n# Copyright (c) 2012, PyData Development Team\n# All rights reserved.\n#\n# Distributed under the terms of the BSD Simplified License.\n#\n# The full license is in the LICENSE file, distributed with this software.\n#-----------------------------------------------------------------------------\n\n```\n\n----------------------------------------\n\nTITLE: Using Lux for Automated Visualization Recommendations in Python\nDESCRIPTION: This snippet shows the integration of Lux with pandas to enable automatic visualization recommendation by simply displaying a DataFrame. After importing Lux and pandas and reading data from a CSV file, viewing the DataFrame in Jupyter or compatible interfaces triggers Lux to suggest relevant charts. Prerequisites include lux and pandas installed; key parameters are the data source (CSV file path) and DataFrame variable. Output is an interactive dashboard within Jupyter or similar environments.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lux\nimport pandas as pd\n\ndf = pd.read_csv(\"data.csv\")\ndf  # discover interesting insights!\n```\n\n----------------------------------------\n\nTITLE: C Struct Definition and Writing to Binary File - C\nDESCRIPTION: Defines a simple C struct with int, double, and float fields, populates it in a loop, and writes an array of such structs to a binary file named 'binary.dat'. Dependencies: stdio.h, stdint.h. Input: none. Output: binary file suitable for interchange with NumPy. Padding/alignment may vary by architecture.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/cookbook.rst#_snippet_57\n\nLANGUAGE: c\nCODE:\n```\n#include <stdio.h>\n#include <stdint.h>\n\ntypedef struct _Data\n{\n    int32_t count;\n    double avg;\n    float scale;\n} Data;\n\nint main(int argc, const char *argv[])\n{\n    size_t n = 10;\n    Data d[n];\n\n    for (int i = 0; i < n; ++i)\n    {\n        d[i].count = i;\n        d[i].avg = i + 1.0;\n        d[i].scale = (float) i + 2.0f;\n    }\n\n    FILE *file = fopen(\"binary.dat\", \"wb\");\n    fwrite(&d, sizeof(Data), n, file);\n    fclose(file);\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Previous Index Display Representations in Pandas (Python)\nDESCRIPTION: This group of snippets demonstrates the string representation format previously used by pandas for Index and DatetimeIndex objects. Dependencies: Python 3.x, pandas. The code initializes various Index types (by range and date ranges) with specific names and, for DatetimeIndex, with time zones. Outputs show full, unwrapped listings regardless of size. Inputs are the index constructors; outputs are their string representations.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.1.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: pd.Index(range(4), name='foo')\nOut[2]: Int64Index([0, 1, 2, 3], dtype='int64')\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: pd.Index(range(104), name='foo')\nOut[3]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...], dtype='int64')\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [4]: pd.date_range('20130101', periods=4, name='foo', tz='US/Eastern')\nOut[4]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-01-01 00:00:00-05:00, ..., 2013-01-04 00:00:00-05:00]\nLength: 4, Freq: D, Timezone: US/Eastern\n```\n\nLANGUAGE: python\nCODE:\n```\nIn [5]: pd.date_range('20130101', periods=104, name='foo', tz='US/Eastern')\nOut[5]:\n<class 'pandas.tseries.index.DatetimeIndex'>\n[2013-01-01 00:00:00-05:00, ..., 2013-04-14 00:00:00-04:00]\nLength: 104, Freq: D, Timezone: US/Eastern\n```\n\n----------------------------------------\n\nTITLE: Resample and Sum of Series with NA bins in pandas 0.21.x - Python\nDESCRIPTION: Illustrates that, prior to pandas 0.22.0, summing all-NA bins when resampling a time-indexed Series yields NaN for those bins. Inputs: Series with some NaN entries and a DateTimeIndex, resampled with sum. Dependencies: pandas as pd, numpy as np.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nIn [11]: s = pd.Series([1, 1, np.nan, np.nan],\n   ....:               index=pd.date_range('2017', periods=4))\n   ....: s\nOut[11]:\n2017-01-01    1.0\n2017-01-02    1.0\n2017-01-03    NaN\n2017-01-04    NaN\nFreq: D, dtype: float64\n\nIn [12]: s.resample('2d').sum()\nOut[12]:\n2017-01-01    2.0\n2017-01-03    NaN\nFreq: 2D, dtype: float64\n```\n\n----------------------------------------\n\nTITLE: Aligning NoRowIndex Series Objects (Python)\nDESCRIPTION: Demonstrates arithmetic operations (element-wise addition) between two Series objects with NoRowIndex of the same length (works) and of differing lengths (raises TypeError). Enforces rule that NoRowIndex alignment is only allowed for same-length objects. pandas and NoRowIndex (proposed) are required. Input: Series with NoRowIndex; Output: combined Series or exception.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nIn [1]: ser1 = pd.Series([1, 2, 3], index=NoRowIndex(3))\n\nIn [2]: ser2 = pd.Series([4, 5, 6], index=NoRowIndex(3))\n\nIn [3]: ser1 + ser2  # works!\nOut[3]:\n5\n7\n9\ndtype: int64\n\nIn [4]: ser1 + ser2.iloc[1:]  # errors!\n---------------------------------------------------------------------------\nTypeError: Cannot join NoRowIndex of different lengths\n```\n\n----------------------------------------\n\nTITLE: Selecting Rows by Condition Using select (Deprecated) - pandas - IPython\nDESCRIPTION: This snippet demonstrates selecting DataFrame rows using the deprecated .select method with a lambda function, filtering for index labels 'bar' and 'baz'. A FutureWarning is raised to indicate deprecation; users should prefer .loc with boolean indexing. Dependencies are pandas and a pre-created DataFrame 'df'; input is a lambda filtering the index; output is the filtered DataFrame.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_26\n\nLANGUAGE: ipython\nCODE:\n```\nIn [3]: df.select(lambda x: x in ['bar', 'baz'])\nFutureWarning: select is deprecated and will be removed in a future release. You can use .loc[crit] as a replacement\nOut[3]:\n     A\nbar  2\nbaz  3\n```\n\n----------------------------------------\n\nTITLE: Listing Top-Level pandas DataFrame Parquet Metadata Specification - Text\nDESCRIPTION: This code illustrates the main structure of the pandas metadata dictionary embedded in the Parquet file's FileMetaData. It includes index_columns, column_indexes, columns, pandas_version, and creator fields, each storing important details for DataFrame reconstruction. No runtime dependencies are assumed but JSON serialization is required for storage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/developer.rst#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n   {'index_columns': [<descr0>, <descr1>, ...],\n    'column_indexes': [<ci0>, <ci1>, ..., <ciN>],\n    'columns': [<c0>, <c1>, ...],\n    'pandas_version': $VERSION,\n    'creator': {\n      'library': $LIBRARY,\n      'version': $LIBRARY_VERSION\n    }}\n```\n\n----------------------------------------\n\nTITLE: Consistent Index Division by Zero Handling (Python)\nDESCRIPTION: Contrasts previous and current behavior when dividing pandas Index objects by zero, matching the scalar behavior found in Series. Demonstrates that positive_over_zero yields np.inf, negative_over_zero yields -np.inf, and zero_over_zero yields np.nan. Input is numeric Index; output is an Index of floats or raises ZeroDivisionError with earlier versions. Requires pandas and numpy.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.23.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.Int64Index([-1, 0, 1])\nindex / 0\n```\n\nLANGUAGE: python\nCODE:\n```\nindex / 0.0\n```\n\nLANGUAGE: python\nCODE:\n```\nindex = pd.UInt64Index([0, 1])\nindex / np.array([0, 0], dtype=np.uint64)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.RangeIndex(1, 5) / 0\n```\n\n----------------------------------------\n\nTITLE: Building pandas with Debug Symbols in Docker\nDESCRIPTION: Command to install pandas with debug symbols within the Docker container, placing build artifacts in a 'debug' folder.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/debugging_extensions.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install -ve . --no-build-isolation -Cbuilddir=\"debug\" -Csetup-args=\"-Dbuildtype=debug\"\n```\n\n----------------------------------------\n\nTITLE: Listing Contributors with Sphinx Extension - reStructuredText\nDESCRIPTION: This snippet uses the Sphinx directive 'contributors:: v0.8.0..v0.8.1' to automatically generate a list of people who contributed to Pandas between versions 0.8.0 and 0.8.1. Requires the Sphinx documentation generator and the appropriate extension to interpret the 'contributors' directive. No parameters required; it outputs a formatted contributors list in the rendered documentation. The result depends on the underlying project configuration and extension support.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.8.1.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. contributors:: v0.8.0..v0.8.1\n```\n\n----------------------------------------\n\nTITLE: Defining Developer Documentation Table of Contents (Sphinx, reStructuredText)\nDESCRIPTION: This snippet uses the Sphinx 'toctree' directive in reStructuredText to define a hierarchical navigation structure for developer documentation. It references multiple sub-pages on contributing, maintenance, and internal guides, and sets the reference label for cross-linking ('development'). Dependencies include Sphinx and reStructuredText support. No parameters or outputs are defined as it strictly serves documentation navigation purposes and relies on the existence of the referenced source files.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/index.rst#_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n===========\nDevelopment\n===========\n\n.. If you update this toctree, also update the manual toctree in the\n   main index.rst.template\n\n.. toctree::\n    :maxdepth: 2\n\n    contributing\n    contributing_environment\n    contributing_documentation\n    contributing_codebase\n    maintaining\n    internals\n    copy_on_write\n    debugging_extensions\n    extending\n    developer\n    policies\n    community\n```\n\n----------------------------------------\n\nTITLE: Direct Element Assignment Using .loc in pandas DataFrames - Python\nDESCRIPTION: Shows best-practice direct element mutation in a DataFrame using .loc to avoid ambiguity and warnings, ensuring the underlying DataFrame itself is modified. Requires pandas and a DataFrame with column 'A'. Input parameters are explicit row and column selectors. Output is the DataFrame with the given cell updated. This is the idiomatic/expected way to mutate DataFrames in pandas, bypassing chained assignment issues.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0007-copy-on-write.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n>>> df.loc[0, \"A\"] = 0\n```\n\n----------------------------------------\n\nTITLE: Summing Series with Different Indices in pandas (Python)\nDESCRIPTION: Demonstrates the result of adding two pandas Series with non-matching indices, highlighting that missing index pairs in one Series lead to NaN values in the result. Shows integer addition aligned by index, where unpaired indices are filled with NaN. Expects pandas as the only dependency, and Series of equal length but different indices as input.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nIn [41]: ser1 + ser2\nOut[41]:\n1    20.0\n2    30.0\n3    40.0\n4     NaN\n5     NaN\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Incorrect Parameter List in Docstring for Python Method (Anti-pattern Example)\nDESCRIPTION: Illustrates a docstring that fails to document all required parameters, notably omitting 'color' in the parameter list, which is against pandas and NumPy conventions. This negative example is intended as a cautionary showcase for proper docstring completeness and structure, not for code functionality. There are no dependencies; the example is not functional but pedagogical.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_docstring.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Series:\n    def plot(self, kind, **kwargs):\n        \"\"\"\n        Generate a plot.\n\n```\n\n----------------------------------------\n\nTITLE: Padding Strings in a pandas Series with str.pad and fillchar (Python)\nDESCRIPTION: Illustrates padding string values in a Series using the pad method with a specified fill character. Dependencies: pandas. Parameters: width (int) for target string length, fillchar (str) for filling. Returns a Series of padded strings. This allows for consistent string formatting, useful when aligning columns or preparing data for export.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(['12', '300', '25'])\ns.str.pad(5, fillchar='_')\n```\n\n----------------------------------------\n\nTITLE: Installing pandas using conda - Bash\nDESCRIPTION: Provides the bash command for installing the pandas library via the conda package manager from the conda-forge channel. This command requires having Anaconda or Miniconda installed beforehand. Executing this command will fetch and install the latest compatible version of pandas and its dependencies from the conda-forge repository. The output will be the successful installation of pandas, ready for use in Python environments managed by conda.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/index.rst#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge pandas\n```\n\n----------------------------------------\n\nTITLE: Using IF/THEN Logic for Conditionals in DATA Step - SAS\nDESCRIPTION: Demonstrates conditional logic in SAS for assigning values to new columns using 'if/then/else'. Here, rows with 'total_bill' < 10 are labeled as 'low' in 'bucket', others as 'high'. The 'format' statement sets the type and length. Input: 'tips' dataset; output: 'tips' dataset with new 'bucket' column.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_9\n\nLANGUAGE: SAS\nCODE:\n```\ndata tips;\\n    set tips;\\n    format bucket $4.;\\n\\n    if total_bill < 10 then bucket = 'low';\\n    else bucket = 'high';\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Proposed Plugin-based Polars Converter Usage (Python)\nDESCRIPTION: Demonstrates the new plugin style for Polars interoperation, where reading and exporting between pandas and Polars can be performed via registered methods. Requires the appropriate plugin in addition to pandas. The user supplies the source data and query; returned is a filtered DataFrame, likely in Polars format after `to_polars`.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n(pandas.read_polars(df)\n       .query('my_col > 0')\n       .to_polars())\n```\n\n----------------------------------------\n\nTITLE: Initializing Seaborn Visualization Theme in Python\nDESCRIPTION: This snippet sets a global visualization theme using Seaborn, which ensures uniform styling for subsequent plots. Requires the seaborn package and optionally pandas/matplotlib for working with data. The 'set_theme' method affects all following visualizations by applying optimized styles and color palettes; there are no parameters in this minimal usage.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport seaborn as sns\nsns.set_theme()\n```\n\n----------------------------------------\n\nTITLE: Subtraction from timedelta64[ns] DataFrame column with NaN (legacy pandas, Python)\nDESCRIPTION: Shows legacy subtraction with np.nan on DataFrame column containing timedelta64[ns] returning all-NaT, which is now an error. Inputs: DataFrame with Timedelta; output: DataFrame with NaT. Current pandas raises TypeError instead.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.24.0.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nIn [4]: df = pd.DataFrame([pd.Timedelta(days=1)])\n\nIn [5]: df - np.nan\nOut[5]:\n    0\n0 NaT\n```\n\n----------------------------------------\n\nTITLE: Setting pandas Plotting Backend via Function Argument in Python\nDESCRIPTION: This code demonstrates using a custom plotting backend by passing its module name as the 'backend' keyword to the Series.plot function. Required dependency is any compatible backend module. Input: pandas Series; output: plot rendered by the selected backend. Limitation: the backend string must be a valid Python module and the backend must implement the plotting interface.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_65\n\nLANGUAGE: python\nCODE:\n```\n>>> Series([1, 2, 3]).plot(backend=\"backend.module\")\n```\n\n----------------------------------------\n\nTITLE: Installing pandas using pip - Bash\nDESCRIPTION: Shows the bash command for installing the pandas library using the pip package manager from the Python Package Index (PyPI). Prerequisites include having Python and pip installed. Running this command will download and install the latest version of pandas and its required dependencies into the active Python environment. The output is pandas being available for import in Python scripts and interactive sessions.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/index.rst#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pandas\n```\n\n----------------------------------------\n\nTITLE: Current PyArrow-to-Pandas Conversion Workflow (Python)\nDESCRIPTION: Displays the existing workflow for converting a pandas DataFrame to a PyArrow Table and performing a pandas query on it. This snippet assumes PyArrow and pandas are installed, with dependencies on both libraries. It chains DataFrame conversion and query using built-in methods. The primary input is a DataFrame, and the output is a PyArrow Table with the filtered contents.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npyarrow.Table.from_pandas(table.to_pandas()\n                               .query('my_col > 0'))\n```\n\n----------------------------------------\n\nTITLE: Assigning New Values to DataFrame Slices with .iloc in pandas (Python)\nDESCRIPTION: This code assigns the string 'anonymous' to the first three rows (indices 0, 1, 2) of the fourth column (index 3) of the titanic DataFrame, and then outputs the first five rows of that column. It leverages .iloc for selection and assignment. The DataFrame must be mutable and unsliced to permit assignment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/intro_tutorials/03_subset_data.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntitanic.iloc[0:3, 3] = \"anonymous\"\ntitanic.iloc[:5, 3]\n```\n\n----------------------------------------\n\nTITLE: Installing pandas via Conda in Shell\nDESCRIPTION: Installs the pandas library using the conda package manager from the conda-forge channel. Requires Conda to be installed on the system; Miniforge is recommended for a lightweight Conda setup. Ensure you have an active internet connection and appropriate permissions. Input is run on the command line, and output is the installation of pandas in the current environment.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/install.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda install -c conda-forge pandas\n```\n\n----------------------------------------\n\nTITLE: Building and Installing pandas from Source with pip (Shell)\nDESCRIPTION: This code demonstrates how to install pandas directly from a local source directory using pip. The command should be run inside the pandas source directory and will build and install pandas into the current Python environment using the dependencies specified in the project. Prerequisites include satisfying all required dependencies and having pip and Cython installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install .\n```\n\n----------------------------------------\n\nTITLE: Importing pandas and numpy - Python\nDESCRIPTION: This snippet imports the pandas and numpy libraries, which are required dependencies for all subsequent code. pandas (imported as pd) is used for data manipulation with Series and DataFrame, while numpy (imported as np) supplies auxiliary numerical functionalities. No inputs or outputs; importing must occur before using any pandas API calls.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Implementing __arrow_array__ for ExtensionArray Arrow Compatibility - Python\nDESCRIPTION: Defines the __arrow_array__ method in a custom ExtensionArray subclass to support conversion to pyarrow.Array. This is necessary for Arrow/Parquet interoperability in pandas. Requires the pyarrow library, and optionally accepts a 'type' parameter for the Arrow type. Typical usage is within a custom ExtensionArray implementation to support roundtripping to Arrow-based formats.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/extending.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyExtensionArray(ExtensionArray):\n    ...\n\n    def __arrow_array__(self, type=None):\n        # convert the underlying array values to a pyarrow Array\n        import pyarrow\n\n        return pyarrow.array(..., type=type)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating KeyError on DataFrame integer indexing in Python\nDESCRIPTION: Example showing that DataFrame also now raises KeyError when attempting to access a key not contained in the index, which is a change from previous behavior.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nIn [3]: df = pd.DataFrame(np.random.randn(8, 4), index=range(0, 16, 2))\n\nIn [4]: df\n    0        1       2       3\n0   0.88427  0.3363 -0.1787  0.03162\n2   0.14451 -0.1415  0.2504  0.58374\n4  -1.44779 -0.9186 -1.4996  0.27163\n6  -0.26598 -2.4184 -0.2658  0.11503\n8  -0.58776  0.3144 -0.8566  0.61941\n10  0.10940 -0.7175 -1.0108  0.47990\n12 -1.16919 -0.3087 -0.6049 -0.43544\n14 -0.07337  0.3410  0.0424 -0.16037\n\nIn [5]: df.ix[3]\nKeyError: 3\n```\n\n----------------------------------------\n\nTITLE: Printing DataFrame with NoRowIndex (Python)\nDESCRIPTION: Shows the output of a DataFrame with NoRowIndex when printed: row labels are omitted from display. Demonstrates the intended display formatting behavior if NoRowIndex is implemented in pandas. Inputs: DataFrame with NoRowIndex; Output: Table without visible row labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nIn [15]: df = pd.DataFrame({'a': [1,  2, 3], 'b': [4, 5, 6]}, index=NoRowIndex(3))\n\nIn [16]: df\nOut[16]:\n a  b\n 1  4\n 2  5\n 3  6\n```\n\n----------------------------------------\n\nTITLE: Summing All-NA Series with min_count=1 - Python\nDESCRIPTION: This example demonstrates that sum on a Series of all NaN values with min_count=1 yields NaN, due to insufficient non-NA values. Dependencies: pandas as pd, numpy as np. Input: Series([np.nan]), Output: nan. Highlights interplay of min_count and skipna.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npd.Series([np.nan]).sum(min_count=1)  # skipna=True by default\n```\n\n----------------------------------------\n\nTITLE: Configuring DataFrame Output Width Using line_width Option - python\nDESCRIPTION: Shows use of pd.set_option() to adjust the printed output width of DataFrames via the 'line_width' parameter, controlling line breaks for better fit in narrow displays. Includes a demo printout with a wide DataFrame to reflect applied settings.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.10.0.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"line_width\", 40)\n\nwide_frame\n```\n\n----------------------------------------\n\nTITLE: Loading Data via DuckDB with the Plugin API (Python)\nDESCRIPTION: Shows use of a third-party plugin to load data into pandas from DuckDB by providing a SQL SELECT statement. It assumes the appropriate DuckDB connector plugin is installed and registered. Input is the SQL query for DuckDB, and the output is a pandas DataFrame containing the result of the query.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npandas.read_duckdb(\"SELECT *\n                    FROM 'dataset.parquet'\n                    WHERE my_col > 0\")\n```\n\n----------------------------------------\n\nTITLE: Creating Lag Plot for Time Series Analysis\nDESCRIPTION: Generates a lag plot to check randomness in time series data. The plot is created using sinusoidal data with random noise. Lag plots help identify patterns in sequential data by plotting data points against their lagged values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/visualization.rst#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.plotting import lag_plot\n\nplt.figure();\n\nspacing = np.linspace(-99 * np.pi, 99 * np.pi, num=1000)\ndata = pd.Series(0.1 * np.random.rand(1000) + 0.9 * np.sin(spacing))\n\nlag_plot(data);\n```\n\n----------------------------------------\n\nTITLE: Aggregating Duplicate Rows with Groupby Mean - pandas - Python\nDESCRIPTION: Resolves duplicate index labels by aggregating with groupby(level=0) and taking the mean of grouped rows. Used for deduplicating while retaining information from all rows. Requires pandas; input is DataFrame with duplicate index labels; output is a deduplicated DataFrame where duplicates are averaged.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/duplicates.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf2.groupby(level=0).mean()\n```\n\n----------------------------------------\n\nTITLE: Running asv Benchmark Comparison on Local Changes (Bash)\nDESCRIPTION: Demonstrates changing to the asv_bench directory and running a benchmark comparison with 'asv continuous', measuring performance changes relative to upstream/main. The '-f 1.1' flag sets a reporting threshold, and 'HEAD' can be replaced by a branch name. Requires asv installed and the benchmarks directory present.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_codebase.rst#_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\nasv continuous -f 1.1 upstream/main HEAD\n```\n\n----------------------------------------\n\nTITLE: Unioning Categorical Series and CategoricalIndex - Pandas Python\nDESCRIPTION: Demonstrates union_categoricals with Series of categorical dtype, showing that the result is always a plain Categorical even if the input types differ. Highlights the conversion from Series/CategoricalIndex to plain Categorical. Useful for code requiring category merging across different pandas types.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/categorical.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\na = pd.Series([\"b\", \"c\"], dtype=\"category\")\nb = pd.Series([\"a\", \"b\"], dtype=\"category\")\nunion_categoricals([a, b])\n```\n\n----------------------------------------\n\nTITLE: Limiting Output Rows - Printing Top N in SAS\nDESCRIPTION: Shows how to print only the first 5 observations (rows) from the 'df' dataset in SAS using the 'obs=' option within 'proc print'. No inputs are required beyond the dataset; output is the first five rows printed to the SAS log/output window.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_4\n\nLANGUAGE: SAS\nCODE:\n```\nproc print data=df(obs=5);\\nrun;\n```\n\n----------------------------------------\n\nTITLE: Citing pandas Original Publication via BibTeX Entry - BibTeX\nDESCRIPTION: This BibTeX snippet should be used to cite the original pandas research paper: 'Data Structures for Statistical Computing in Python' by Wes McKinney. The entry covers required fields like author, title, booktitle, pages, year, editor, and DOI. Researchers should include this citation in LaTeX projects when referencing the foundational pandas paper.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/about/citing.md#_snippet_1\n\nLANGUAGE: BibTeX\nCODE:\n```\n@InProceedings{ mckinney-proc-scipy-2010,\n  author    = { {W}es {M}c{K}inney },\n  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },\n  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },\n  pages     = { 56 - 61 },\n  year      = { 2010 },\n  editor    = { {S}t\\'efan van der {W}alt and {J}arrod {M}illman },\n  doi       = { 10.25080/Majora-92bf1922-00a }\n}\n```\n\n----------------------------------------\n\nTITLE: Referencing Public API in Markdown\nDESCRIPTION: This snippet provides a link to the pandas public API documentation using Markdown syntax.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0017-backwards-compatibility-and-deprecation-policy.md#_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n[1]: https://pandas.pydata.org/docs/reference/index.html\n```\n\n----------------------------------------\n\nTITLE: Slicing with .loc in Pandas DataFrame\nDESCRIPTION: Shows the new behavior of slicing a DataFrame with .loc where the start and/or stop bounds are not found in the index. This is now allowed and matches the behavior of .ix.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame(np.random.randn(5, 4),\n                   columns=list('ABCD'),\n                   index=pd.date_range('20130101', periods=5))\ndf\ns = pd.Series(range(5), [-2, -1, 1, 2, 3])\ns\n\ndf.loc['2013-01-02':'2013-01-10']\ns.loc[-10:3]\n```\n\n----------------------------------------\n\nTITLE: Deleting Rows Conditionally in SQL - SQL\nDESCRIPTION: Removes rows from the 'tips' table where the value of 'tip' exceeds 9. Useful for cleaning data by removing outliers or unwanted records. Requires a SQL table named 'tips' with a 'tip' column. The operation permanently deletes qualifying rows; the output is the number of affected rows.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sql.rst#_snippet_15\n\nLANGUAGE: sql\nCODE:\n```\nDELETE FROM tips\nWHERE tip > 9;\n```\n\n----------------------------------------\n\nTITLE: Creating Nullable Boolean Series with BooleanDtype - pandas - Python\nDESCRIPTION: This snippet demonstrates creation of a Series containing booleans with the experimental BooleanDtype, which supports missing values (using None). This allows tracking and propagating missing boolean entries. Requires pandas 1.0.0 or later.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.0.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npd.Series([True, False, None], dtype=pd.BooleanDtype())\n```\n\n----------------------------------------\n\nTITLE: Using Third-Party Pandas Connectors Example\nDESCRIPTION: Example showing how third-party connectors like DuckDB and Delta Lake could be used with pandas after loading I/O plugins. This demonstrates the proposed simplified interface for external connectors.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0009-io-extensions.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas\n\npandas.load_io_plugins()\n\ndf = pandas.DataFrame.read_duckdb(\"SELECT * FROM 'my_dataset.parquet';\")\n\ndf.to_deltalake('/delta/my_dataset')\n```\n\n----------------------------------------\n\nTITLE: Parallelizing pandas Operations with Pandarallel\nDESCRIPTION: This code shows how to use Pandarallel to parallelize pandas operations with minimal code changes. It initializes Pandarallel with a progress bar and demonstrates how to replace a standard apply method with its parallel equivalent.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom pandarallel import pandarallel\n\npandarallel.initialize(progress_bar=True)\n\n# df.apply(func)\ndf.parallel_apply(func)\n```\n\n----------------------------------------\n\nTITLE: Creating Categorical Data with CategoricalDtype in Python\nDESCRIPTION: Demonstrates using the CategoricalDtype class to specify the set of categories and orderedness of an array, independent of the data. This converts string data to a Categorical with specified categories.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas.api.types import CategoricalDtype\n\ns = pd.Series(['a', 'b', 'c', 'a'])  # strings\ndtype = CategoricalDtype(categories=['a', 'b', 'c', 'd'], ordered=True)\ns.astype(dtype)\n```\n\n----------------------------------------\n\nTITLE: Loading Dataset from Hugging Face Dataset Hub\nDESCRIPTION: This code demonstrates how to load a dataset directly from Hugging Face's Dataset Hub using the 'hf://' path format. It loads the IMDB dataset from the stanfordnlp account.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/ecosystem.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n# Load the IMDB dataset\ndf = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/plain_text/train-00000-of-00001.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Series using IntervalIndex as the Index - Pandas - Python\nDESCRIPTION: Shows the creation of a pandas Series object with an IntervalIndex as its index and simple string data. This is used for demonstrating key-based Series access with intervals. Requires pandas and construction of 'ii' as the IntervalIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.25.0.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(list('abc'), index=ii)\\ns\n```\n\n----------------------------------------\n\nTITLE: Comparing Performance of String Operations: Startswith\nDESCRIPTION: This snippet compares the performance of the str.startswith() operation between object dtype and PyArrow string dtype Series.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0010-required-pyarrow-dependency.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nIn[3]: %timeit ser_object.str.startswith(\"a\")\n136 ms ± 300 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nIn[4]: %timeit ser_string.str.startswith(\"a\")\n11 ms ± 19.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Potential Interoperability with Polars\nDESCRIPTION: This snippet shows how making PyArrow a required dependency could improve interoperability with other Arrow-based libraries like Polars.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0010-required-pyarrow-dependency.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport polars as pl\n\ndf = pd.DataFrame(\n  {\n    'a': ['one', 'two'],\n    'b': [{'name': 'Billy', 'age': 3}, {'name': 'Bob', 'age': 4}],\n  }\n)\npl.from_pandas(df)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Current Behavior with Nested Datatypes\nDESCRIPTION: This code shows how pandas currently stores dictionaries in a Series using object dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0010-required-pyarrow-dependency.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nIn [6]: pd.Series([{'a': 1, 'b': 2}, {'a': 2, 'b': 99}])\nOut[6]:\n0     {'a': 1, 'b': 2}\n1    {'a': 2, 'b': 99}\ndtype: object\n```\n\n----------------------------------------\n\nTITLE: Example of Hidden Bug in Current Behavior\nDESCRIPTION: Shows how the current behavior can hide bugs by silently upcasting types when invalid data is provided.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0006-ban-upcasting.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nser = pd.Series(pd.date_range(\"2000\", periods=3))\n\nser[2] = \"2000-01-04\"  # works, is converted to datetime64\n\nser[2] = \"2000-01-04x\"  # typo - but pandas does not error, it upcasts to object\n```\n\n----------------------------------------\n\nTITLE: Substring Extraction by Position - Stata\nDESCRIPTION: Uses 'substr' to extract the first (position 1) character of 'sex' and place it in 'short_sex'. Assumes 'sex' is a string column. Output: New column with extracted characters. No dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_17\n\nLANGUAGE: stata\nCODE:\n```\ngenerate short_sex = substr(sex, 1, 1)\n```\n\n----------------------------------------\n\nTITLE: Simple Tabular Data Structure\nDESCRIPTION: A basic 2x3 table structure with alphabetic column headers (A, B, C) and numeric data values arranged in two rows. This format is commonly used to demonstrate Pandas DataFrame creation and manipulation.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/pandas/tests/io/data/fixed_width/fixed_width_format.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nA   B   C\n1   2   3\n4   5   6\n```\n\n----------------------------------------\n\nTITLE: DataFrame Sampling with Column Weights\nDESCRIPTION: Examples of sampling DataFrame rows using a weight column and sampling columns using the axis parameter.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_44\n\nLANGUAGE: python\nCODE:\n```\ndf2 = pd.DataFrame({'col1': [9, 8, 7, 6],\n                    'weight_column': [0.5, 0.4, 0.1, 0]})\ndf2.sample(n=3, weights='weight_column')\n\ndf3 = pd.DataFrame({'col1': [1, 2, 3], 'col2': [2, 3, 4]})\ndf3.sample(n=1, axis=1)\n```\n\n----------------------------------------\n\nTITLE: Exporting DataFrame to CSV - pandas - Python\nDESCRIPTION: This code uses the pandas 'to_csv' method to export a DataFrame ('tips') to a CSV file named 'tips2.csv'. Requires pandas and a DataFrame called 'tips'. Input: DataFrame object. Output: CSV file on disk.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_stata.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntips.to_csv(\"tips2.csv\")\n```\n\n----------------------------------------\n\nTITLE: Datetime64 Type Demonstration in Pandas\nDESCRIPTION: Illustrates the behavior of the new datetime64 data type and Timestamp objects in Pandas 0.8.0, showing type conversion and handling of datetime values.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.8.0.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nrng = pd.date_range(\"1/1/2000\", periods=10)\nrng[5]\nisinstance(rng[5], datetime.datetime)\nrng_asarray = np.asarray(rng)\nscalar_val = rng_asarray[5]\ntype(scalar_val)\n```\n\n----------------------------------------\n\nTITLE: Converting Dummy Variables Back to Categories in Python\nDESCRIPTION: Demonstrates how to use pandas.from_dummies() to convert indicator variables back to categorical data.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/reshaping.rst#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame({\"prefix_a\": [0, 1, 0], \"prefix_b\": [1, 0, 1]})\ndf\n\npd.from_dummies(df, sep=\"_\")\n\ndf = pd.DataFrame({\"prefix_a\": [0, 1, 0]})\ndf\n\npd.from_dummies(df, sep=\"_\", default_category=\"b\")\n```\n\n----------------------------------------\n\nTITLE: Updating Conda Development Environment\nDESCRIPTION: Commands to update local main branch and refresh the conda development environment with latest dependencies.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing.rst#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout main\ngit fetch upstream\ngit merge upstream/main\nconda activate pandas-dev\nconda env update -f environment.yml --prune\n```\n\n----------------------------------------\n\nTITLE: Concatenating DataFrames with Empty/NA Columns - Python\nDESCRIPTION: Documentation of a behavior reversion in pandas.concat() function where it reverts back to the pre-1.4.0 behavior of handling empty or all-NA columns with float or object dtype.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.4.3.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nconcat()\n```\n\n----------------------------------------\n\nTITLE: Interval Range Previous Implementation\nDESCRIPTION: Shows the previous behavior of interval_range function when specifying start, end, and periods parameters.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npd.interval_range(start=0, end=4, periods=6)\nOut[2]:\nIntervalIndex([(0, 1], (1, 2], (2, 3]]\n             closed='right',\n             dtype='interval[int64]')\n```\n\n----------------------------------------\n\nTITLE: Failed label-based slicing with non-existent endpoint in Python\nDESCRIPTION: Example showing that label-based slicing raises KeyError when the slice start point is not in the index and the index is not sorted.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.7.0.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nIn [12]: s.ix['b':'h']\nKeyError 'b'\n```\n\n----------------------------------------\n\nTITLE: Compiling pandas with setup.py in Python\nDESCRIPTION: This command uses setup.py to compile pandas in development mode. Note that this method is deprecated and will be removed soon.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Handling NaN Values in Series Sum Operation in Python\nDESCRIPTION: Demonstrates the new consistent behavior of sum operation on a Series containing only NaN values, which now returns NaN regardless of whether bottleneck is installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.0.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series([np.nan])\ns.sum()\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas Namespace\nDESCRIPTION: Code to import all pandas functionality into the current namespace, used for documentation examples. This is suppressed in the actual output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.21.1.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas import *  # noqa F401, F403\n```\n\n----------------------------------------\n\nTITLE: Improving Error Message for np.min/max on Unordered Categorical\nDESCRIPTION: Fixes a regression in the error message with np.min or np.max on unordered Categorical.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nnp.min(unordered_categorical)\n```\n\nLANGUAGE: Python\nCODE:\n```\nnp.max(unordered_categorical)\n```\n\n----------------------------------------\n\nTITLE: Listing Bug Fixes in pandas Changelog - reStructuredText\nDESCRIPTION: This snippet enumerates bug fixes as a bulleted list, using reStructuredText markup. Each item succinctly describes a bug and its fix, often referencing related pandas methods, classes, or GitHub issues using double backticks or Sphinx roles. No special dependencies beyond the reStructuredText format are required. The entries inform users about resolved issues and can serve as a basis for documenting API changes. Inputs/outputs are strictly textual—intended as documentation and not programmatic code. Limitations: content is static and non-executable, relaying information for human readers and documentation generators.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.14.1.rst#_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n- Bug in ``StringMethods.extract()`` where a single match group Series\n  would use the matcher\\'s name instead of the group name (:issue:`7313`).\n- Bug in ``isnull()`` when ``mode.use_inf_as_null == True`` where isnull\n  wouldn\\'t test ``True`` when it encountered an ``inf``/``-inf``\n  (:issue:`7315`).\n- Bug in inferred_freq results in None for eastern hemisphere timezones (:issue:`7310`)\n- Bug in ``Easter`` returns incorrect date when offset is negative (:issue:`7195`)\n- Bug in broadcasting with ``.div``, integer dtypes and divide-by-zero (:issue:`7325`)\n- Bug in ``CustomBusinessDay.apply`` raises ``NameError`` when ``np.datetime64`` object is passed (:issue:`7196`)\n- Bug in ``MultiIndex.append``, ``concat`` and ``pivot_table`` don\\'t preserve timezone (:issue:`6606`)\n- Bug in ``.loc`` with a list of indexers on a single-multi index level (that is not nested) (:issue:`7349`)\n- Bug in ``Series.map`` when mapping a dict with tuple keys of different lengths (:issue:`7333`)\n- Bug all ``StringMethods`` now work on empty Series (:issue:`7242`)\n- Fix delegation of ``read_sql`` to ``read_sql_query`` when query does not contain 'select' (:issue:`7324`).\n- Bug where a string column name assignment to a ``DataFrame`` with a\n  ``Float64Index`` raised a ``TypeError`` during a call to ``np.isnan``\n  (:issue:`7366`).\n- Bug where ``NDFrame.replace()`` didn\\'t correctly replace objects with\n  ``Period`` values (:issue:`7379`).\n- Bug in ``.ix`` getitem should always return a Series (:issue:`7150`)\n- Bug in MultiIndex slicing with incomplete indexers (:issue:`7399`)\n- Bug in MultiIndex slicing with a step in a sliced level (:issue:`7400`)\n- Bug where negative indexers in ``DatetimeIndex`` were not correctly sliced\n  (:issue:`7408`)\n- Bug where ``NaT`` wasn\\'t repr'd correctly in a ``MultiIndex`` (:issue:`7406`,\n  :issue:`7409`).\n- Bug where bool objects were converted to ``nan`` in ``convert_objects``\n  (:issue:`7416`).\n- Bug in ``quantile`` ignoring the axis keyword argument (:issue:`7306`)\n- Bug where ``nanops._maybe_null_out`` doesn\\'t work with complex numbers\n  (:issue:`7353`)\n- Bug in several ``nanops`` functions when ``axis==0`` for\n  1-dimensional ``nan`` arrays (:issue:`7354`)\n- Bug where ``nanops.nanmedian`` doesn\\'t work when ``axis==None``\n  (:issue:`7352`)\n- Bug where ``nanops._has_infs`` doesn\\'t work with many dtypes\n  (:issue:`7357`)\n- Bug in ``StataReader.data`` where reading a 0-observation dta failed (:issue:`7369`)\n- Bug in ``StataReader`` when reading Stata 13 (117) files containing fixed width strings (:issue:`7360`)\n- Bug in ``StataWriter`` where encoding was ignored (:issue:`7286`)\n- Bug in ``DatetimeIndex`` comparison doesn\\'t handle ``NaT`` properly (:issue:`7529`)\n- Bug in passing input with ``tzinfo`` to some offsets ``apply``, ``rollforward`` or ``rollback`` resets ``tzinfo`` or raises ``ValueError`` (:issue:`7465`)\n- Bug in ``DatetimeIndex.to_period``, ``PeriodIndex.asobject``, ``PeriodIndex.to_timestamp`` doesn\\'t preserve ``name`` (:issue:`7485`)\n- Bug in ``DatetimeIndex.to_period`` and ``PeriodIndex.to_timestamp`` handle ``NaT`` incorrectly (:issue:`7228`)\n- Bug in ``offsets.apply``, ``rollforward`` and ``rollback`` may return normal ``datetime`` (:issue:`7502`)\n- Bug in ``resample`` raises ``ValueError`` when target contains ``NaT`` (:issue:`7227`)\n- Bug in ``Timestamp.tz_localize`` resets ``nanosecond`` info (:issue:`7534`)\n- Bug in ``DatetimeIndex.asobject`` raises ``ValueError`` when it contains ``NaT`` (:issue:`7539`)\n- Bug in ``Timestamp.__new__`` doesn\\'t preserve nanosecond properly (:issue:`7610`)\n- Bug in ``Index.astype(float)`` where it would return an ``object`` dtype\n  ``Index`` (:issue:`7464`).\n- Bug in ``DataFrame.reset_index`` loses ``tz`` (:issue:`3950`)\n- Bug in ``DatetimeIndex.freqstr`` raises ``AttributeError`` when ``freq`` is ``None`` (:issue:`7606`)\n- Bug in ``GroupBy.size`` created by ``TimeGrouper`` raises ``AttributeError`` (:issue:`7453`)\n- Bug in single column bar plot is misaligned (:issue:`7498`).\n- Bug in area plot with tz-aware time series raises ``ValueError`` (:issue:`7471`)\n- Bug in non-monotonic ``Index.union`` may preserve ``name`` incorrectly (:issue:`7458`)\n- Bug in ``DatetimeIndex.intersection`` doesn\\'t preserve timezone (:issue:`4690`)\n- Bug in ``rolling_var`` where a window larger than the array would raise an error(:issue:`7297`)\n- Bug with last plotted timeseries dictating ``xlim`` (:issue:`2960`)\n- Bug with ``secondary_y`` axis not being considered for timeseries ``xlim`` (:issue:`3490`)\n- Bug in ``Float64Index`` assignment with a non scalar indexer (:issue:`7586`)\n- Bug in ``pandas.core.strings.str_contains`` does not properly match in a case insensitive fashion when ``regex=False`` and ``case=False`` (:issue:`7505`)\n- Bug in ``expanding_cov``, ``expanding_corr``, ``rolling_cov``, and ``rolling_corr`` for two arguments with mismatched index  (:issue:`7512`)\n- Bug in ``to_sql`` taking the boolean column as text column (:issue:`7678`)\n- Bug in grouped ``hist`` doesn\\'t handle ``rot`` kw and ``sharex`` kw properly (:issue:`7234`)\n- Bug in ``.loc`` performing fallback integer indexing with ``object`` dtype indices (:issue:`7496`)\n- Bug (regression) in ``PeriodIndex`` constructor when passed ``Series`` objects (:issue:`7701`).\n```\n\n----------------------------------------\n\nTITLE: Variable Assignment and Print in IPython\nDESCRIPTION: This snippet demonstrates variable assignment and implicit printing in an IPython environment. It shows how the last line of a cell is automatically printed in Jupyter Notebooks.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/index.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\na = 1\na\n```\n\n----------------------------------------\n\nTITLE: Controlling DataFrame dimension display in Pandas 0.13.1\nDESCRIPTION: Shows how to use the show_dimensions display option to control whether DataFrame dimensions are printed in the output.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.13.1.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame([[1, 2], [3, 4]])\npd.set_option(\"show_dimensions\", False)\ndf\n\npd.set_option(\"show_dimensions\", True)\ndf\n```\n\n----------------------------------------\n\nTITLE: Combining Index Intersection and Reindexing\nDESCRIPTION: Shows how to intersect desired labels with the current index and then reindex to handle non-existent labels.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/indexing.rst#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ns.loc[s.index.intersection(labels)].reindex(labels)\n```\n\n----------------------------------------\n\nTITLE: Preserving 'nearest' Indexing for CFTimeIndex\nDESCRIPTION: Fixes a regression to preserve the ability to index with the 'nearest' method with xarray's CFTimeIndex, an Index subclass.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.4.rst#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nIndex.get_indexer(method='nearest')\n```\n\n----------------------------------------\n\nTITLE: Timezone Localization Error Example\nDESCRIPTION: Demonstrates how tz_localize raises an error when used on non-datetime indices.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/pdeps/0005-no-default-index-mode.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nser.tz_localize('UTC')\n```\n\n----------------------------------------\n\nTITLE: Running IPython Code Examples in Documentation\nDESCRIPTION: Demonstrates how to include executable code examples in documentation using IPython directive that will be run during doc build.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_documentation.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nx = 2\nx**3\n```\n\n----------------------------------------\n\nTITLE: Displaying pandas Series Formatting in Python\nDESCRIPTION: This pair of code snippets demonstrates the change in numeric formatting of pandas Series when truncated display is involved, as controlled by the max_rows option. They illustrate previous and new output behaviors for consistent formatting of floating-point numbers in a Series, requiring pandas and Python. Inputs involve constructing a large Series, and outputs differ in the precision and alignment of displayed numbers. The code requires prior import of pandas as pd and is affected by the configuration of display options.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.16.0.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nIn [2]: pd.options.display.max_rows = 10\nIn [3]: s = pd.Series([1,1,1,1,1,1,1,1,1,1,0.9999,1,1]*10)\nIn [4]: s\nOut[4]:\n0    1\n1    1\n2    1\n...\n127    0.9999\n128    1.0000\n129    1.0000\nLength: 130, dtype: float64\n```\n\nLANGUAGE: python\nCODE:\n```\n0      1.0000\n1      1.0000\n2      1.0000\n3      1.0000\n4      1.0000\n...\n125    1.0000\n126    1.0000\n127    0.9999\n128    1.0000\n129    1.0000\ndtype: float64\n```\n\n----------------------------------------\n\nTITLE: Subsetting Data in R\nDESCRIPTION: Demonstrates how to subset data in R using the subset function and boolean indexing.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_r.rst#_snippet_8\n\nLANGUAGE: r\nCODE:\n```\ndf <- data.frame(a=rnorm(10), b=rnorm(10))\nsubset(df, a <= b)\ndf[df$a <= df$b,]  # note the comma\n```\n\n----------------------------------------\n\nTITLE: Time-based Rolling Window\nDESCRIPTION: Demonstrates rolling window operations using time-based windows with DateTimeIndex.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/user_guide/window.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ns = pd.Series(range(5), index=pd.date_range('2020-01-01', periods=5, freq='1D'))\ns.rolling(window='2D').sum()\n```\n\n----------------------------------------\n\nTITLE: Importing All pandas Symbols - Python\nDESCRIPTION: This snippet imports all public objects from the pandas library into the current namespace, which is used to simplify example code by making all pandas functions and objects directly accessible. No parameters or outputs are involved. Dependencies: pandas must be installed.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v0.22.0.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pandas import *  # noqa F401, F403\n```\n\n----------------------------------------\n\nTITLE: RST Directive for Bug Fixes Reference\nDESCRIPTION: ReStructuredText directive defining a reference label for bug fixes section\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.3.rst#_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. _whatsnew_103.bug_fixes:\n```\n\n----------------------------------------\n\nTITLE: RST Directive for Version Reference\nDESCRIPTION: ReStructuredText directive defining a reference label for Pandas 1.0.3 documentation\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.3.rst#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _whatsnew_103:\n```\n\n----------------------------------------\n\nTITLE: Fixed performance regression in MultiIndex.equals\nDESCRIPTION: Addresses a performance regression in the MultiIndex.equals method.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.3.4.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nMultiIndex.equals()\n```\n\n----------------------------------------\n\nTITLE: RST Contributors Directive\nDESCRIPTION: ReStructuredText directive to generate list of contributors between versions v1.0.2 and v1.0.3\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/whatsnew/v1.0.3.rst#_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. contributors:: v1.0.2..v1.0.3\n```\n\n----------------------------------------\n\nTITLE: Installing pandas with verbose editable mode in Python\nDESCRIPTION: This command installs pandas in editable mode with verbose output enabled for every build. It sets the 'editable-verbose' config setting to 'true'.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_environment.rst#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\npython -m pip install -ve . -Ceditable-verbose=true\n```\n\n----------------------------------------\n\nTITLE: Building a single page of HTML documentation with Sphinx using make.py in Python (Shell command)\nDESCRIPTION: This shell command runs make.py with the --single option to build only one documentation page with Sphinx. It requires the path to the RST (reStructuredText) file to build. Prerequisites: Sphinx and required extensions must be installed; the command should be run in the 'doc' directory. Inputs: --single option and page path. Outputs: Single HTML page for the specified documentation file.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/development/contributing_gitpod.rst#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython make.py --single development/contributing_gitpod.rst\n```\n\n----------------------------------------\n\nTITLE: Displaying User Satisfaction with pandas Stability\nDESCRIPTION: This code snippet presents the percentage of users who find pandas stable enough for their needs, showing a high level of satisfaction.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/web/pandas/community/blog/2019-user-survey.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nYes    94.89%\nNo      5.11%\nName: Is Pandas stable enough for you?, dtype: object\n```\n\n----------------------------------------\n\nTITLE: Importing CSV Data with PROC IMPORT - SAS\nDESCRIPTION: This SAS snippet uses 'PROC IMPORT' to read data from a CSV file ('tips.csv') into a SAS data set ('tips'). The 'dbms=csv' option specifies the file type, and 'getnames=yes' indicates that the first row contains column names. Input is a CSV file; output is a new SAS dataset.\nSOURCE: https://github.com/pandas-dev/pandas/blob/main/doc/source/getting_started/comparison/comparison_with_sas.rst#_snippet_1\n\nLANGUAGE: SAS\nCODE:\n```\nproc import datafile='tips.csv' dbms=csv out=tips replace;\\n    getnames=yes;\\nrun;\n```"
  }
]
[
  {
    "owner": "funaudiollm",
    "repo": "sensevoice",
    "content": "TITLE: Inferencing with SenseVoice Model using AutoModel\nDESCRIPTION: Complete example of using the SenseVoice model for speech recognition with VAD (Voice Activity Detection). This code loads the model, processes an audio file, and outputs transcribed text with support for various languages and post-processing options.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",    \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,  #\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Inferencing with SenseVoice Model in Python\nDESCRIPTION: This snippet demonstrates how to load the SenseVoice model, perform inference on an audio file, and post-process the results. It supports input of audio in any format with a duration limit of 30 seconds or less.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res[0][0][\"text\"])\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Inferencing with SenseVoice using FunASR\nDESCRIPTION: Example code showing how to use SenseVoice with the FunASR framework for inference. This implementation supports arbitrary audio formats and durations, with voice activity detection (VAD) for processing longer audio files.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom funasr import AutoModel\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\n\nmodel = AutoModel(\n    model=model_dir,\n    trust_remote_code=True,\n    remote_code=\"./model.py\",  \n    vad_model=\"fsmn-vad\",\n    vad_kwargs={\"max_single_segment_time\": 30000},\n    device=\"cuda:0\",\n)\n\n# en\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\",  # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size_s=60,\n    merge_vad=True,\n    merge_length_s=15,\n)\ntext = rich_transcription_postprocess(res[0][\"text\"])\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Batch Inference with SenseVoice for Short Audio Files\nDESCRIPTION: Simplified code for batch processing of short audio files (<30s) without using the VAD model. This approach improves inference efficiency by setting an appropriate batch size for parallel processing.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"zh\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    batch_size=64, \n)\n```\n\n----------------------------------------\n\nTITLE: Batch Inference with SenseVoice for Short Audio Files\nDESCRIPTION: Optimized code for batch processing of short audio files (less than 30s) with SenseVoice. This approach removes the VAD model and configures batch size to increase inference efficiency.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoModel(model=model_dir, trust_remote_code=True, device=\"cuda:0\")\n\nres = model.generate(\n    input=f\"{model.model_path}/example/en.mp3\",\n    cache={},\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=True,\n    batch_size=64, \n)\n```\n\n----------------------------------------\n\nTITLE: Direct Inference with SenseVoice\nDESCRIPTION: Code for direct inference with SenseVoice for audio files under 30 seconds. This approach uses the model class directly rather than through the AutoModel interface, providing more direct control.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom model import SenseVoiceSmall\nfrom funasr.utils.postprocess_utils import rich_transcription_postprocess\n\nmodel_dir = \"iic/SenseVoiceSmall\"\nm, kwargs = SenseVoiceSmall.from_pretrained(model=model_dir, device=\"cuda:0\")\nm.eval()\n\nres = m.inference(\n    data_in=f\"{kwargs ['model_path']}/example/en.mp3\",\n    language=\"auto\", # \"zh\", \"en\", \"yue\", \"ja\", \"ko\", \"nospeech\"\n    use_itn=False,\n    ban_emo_unk=False,\n    **kwargs,\n)\n\ntext = rich_transcription_postprocess(res [0][0][\"text\"])\nprint(text)\n```\n\n----------------------------------------\n\nTITLE: Exporting and Testing SenseVoice Model with ONNX\nDESCRIPTION: This code exports the SenseVoice model to ONNX format and demonstrates how to use it for inference. It requires the funasr and funasr-onnx packages to be installed.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\n\n----------------------------------------\n\nTITLE: ONNX Model Export and Inference with SenseVoice\nDESCRIPTION: Example for exporting SenseVoice to ONNX format and performing inference. This uses the funasr-onnx package to handle the quantized model and provides optimized performance.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# pip3 install -U funasr funasr-onnx\nfrom pathlib import Path\nfrom funasr_onnx import SenseVoiceSmall\nfrom funasr_onnx.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, quantize=True)\n\n# inference\nwav_or_scp = [\"{}/. cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\n\n----------------------------------------\n\nTITLE: Exporting and Testing SenseVoice Model with Libtorch\nDESCRIPTION: This snippet demonstrates how to export the SenseVoice model to Libtorch format and use it for inference. It uses the funasr_torch package.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/.cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess(i) for i in res])\n```\n\n----------------------------------------\n\nTITLE: LibTorch Model Export and Inference with SenseVoice\nDESCRIPTION: Example for exporting SenseVoice to LibTorch format and performing inference. This uses the funasr-torch package for deployment in C++ environments with PyTorch backend.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nfrom funasr_torch import SenseVoiceSmall\nfrom funasr_torch.utils.postprocess_utils import rich_transcription_postprocess\n\n\nmodel_dir = \"iic/SenseVoiceSmall\"\n\nmodel = SenseVoiceSmall(model_dir, batch_size=10, device=\"cuda:0\")\n\nwav_or_scp = [\"{}/. cache/modelscope/hub/{}/example/en.mp3\".format(Path.home(), model_dir)]\n\nres = model(wav_or_scp, language=\"auto\", use_itn=True)\nprint([rich_transcription_postprocess (i) for i in res])\n```\n\n----------------------------------------\n\nTITLE: Deploying SenseVoice Model with FastAPI\nDESCRIPTION: This shell command sets up the SenseVoice model for deployment using FastAPI. It specifies the CUDA device to use and runs the FastAPI server on port 50000.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n----------------------------------------\n\nTITLE: Deploying SenseVoice with FastAPI\nDESCRIPTION: Commands to deploy SenseVoice as a web service using FastAPI. This setup configures the GPU device and specifies the port for handling inference requests.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nexport SENSEVOICE_DEVICE=cuda:0\nfastapi run --port 50000\n```\n\n----------------------------------------\n\nTITLE: Installing SenseVoice Dependencies with pip\nDESCRIPTION: Command to install the required dependencies for the SenseVoice model using pip and the requirements.txt file.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing SenseVoice Dependencies with pip\nDESCRIPTION: Command to install the required dependencies for SenseVoice using pip package manager. This is the first step before using the model for inference or other tasks.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for SenseVoice\nDESCRIPTION: This requirements file lists all necessary Python packages with their version constraints. It includes PyTorch (≤2.3), torchaudio, modelscope, huggingface libraries, FunASR (≥1.1.3), NumPy (≤1.26.4), Gradio for UI, and FastAPI (≥0.111.1) for backend services.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/requirements.txt#2025-04-17_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorch<=2.3\ntorchaudio\nmodelscope\nhuggingface\nhuggingface_hub\nfunasr>=1.1.3\nnumpy<=1.26.4\ngradio\nfastapi>=0.111.1\n```\n\n----------------------------------------\n\nTITLE: Running SenseVoice Web UI\nDESCRIPTION: This Python command launches a web-based user interface for interacting with the SenseVoice model. It allows users to input audio and view the model's output through a graphical interface.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npython webui.py\n```\n\n----------------------------------------\n\nTITLE: Launching SenseVoice WebUI in Python\nDESCRIPTION: Provides the command to start the SenseVoice WebUI for interactive use of the model.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\npython webui.py\n```\n\n----------------------------------------\n\nTITLE: Installing FunASR for SenseVoice Fine-tuning\nDESCRIPTION: This shell command clones the FunASR repository and installs it in editable mode, which is required for fine-tuning the SenseVoice model.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n----------------------------------------\n\nTITLE: Installing SenseVoice Training Environment\nDESCRIPTION: Commands to set up the training environment for SenseVoice by cloning the FunASR repository and installing it in development mode. This is needed for fine-tuning the model.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/alibaba/FunASR.git && cd FunASR\npip3 install -e ./\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning SenseVoice Model\nDESCRIPTION: This shell command initiates the fine-tuning process for the SenseVoice model. It runs a script named finetune.sh, which should be configured with the correct paths and parameters.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nbash finetune.sh\n```\n\n----------------------------------------\n\nTITLE: Starting SenseVoice Training in Shell\nDESCRIPTION: Shows the command to start the SenseVoice model training process using a shell script.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nbash finetune.sh\n```\n\n----------------------------------------\n\nTITLE: Generating Training Data for SenseVoice Fine-tuning\nDESCRIPTION: This command uses the sensevoice2jsonl tool to generate training data in JSONL format from various input files. It processes audio files, transcriptions, language IDs, emotion labels, and event labels.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README.md#2025-04-17_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Generating Training Data with sensevoice2jsonl in Shell\nDESCRIPTION: Shows how to use the sensevoice2jsonl command to generate training data from various input files.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\", \"../../../data/list/train_text_language.txt\", \"../../../data/list/train_emo.txt\", \"../../../data/list/train_event.txt\"]' \\\n++data_type_list='[\"source\", \"target\", \"text_language\", \"emo_target\", \"event_target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\"\n```\n\n----------------------------------------\n\nTITLE: Generating Training Data with Automatic Labeling in Shell\nDESCRIPTION: Demonstrates how to generate training data using sensevoice2jsonl with automatic labeling for language, emotion, and event types.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nsensevoice2jsonl \\\n++scp_file_list='[\"../../../data/list/train_wav.scp\", \"../../../data/list/train_text.txt\"]' \\\n++data_type_list='[\"source\", \"target\"]' \\\n++jsonl_file_out=\"../../../data/list/train.jsonl\" \\\n++model_dir='iic/SenseVoiceSmall'\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Data Format for Audio Processing in Text\nDESCRIPTION: Specifies the required JSON format for audio data, including fields for unique key, language, emotion, event type, text content, and audio file path.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n{\"key\": \"YOU0000008470_S0000238_punc_itn\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|withitn|>\", \"target\": \"Including legal due diligence, subscription agreement, negotiation.\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/YOU0000008470_S0000238.wav\", \"target_len\": 7, \"source_len\": 140}\n{\"key\": \"AUD0000001556_S0007580\", \"text_language\": \"<|en|>\", \"emo_target\": \"<|NEUTRAL|>\", \"event_target\": \"<|Speech|>\", \"with_or_wo_itn\": \"<|woitn|>\", \"target\": \"there is a tendency to identify the self or take interest in what one has got used to\", \"source\": \"/cpfs01/shared/Group-speech/beinian.lzr/data/industrial_data/english_all/audio/AUD0000001556_S0007580.wav\", \"target_len\": 18, \"source_len\": 360}\n```\n\n----------------------------------------\n\nTITLE: Formatting Audio Transcription Data in Bash\nDESCRIPTION: Shows the format for train_text.txt file, which contains audio file IDs and their corresponding transcriptions.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 甚至出现交易几乎停滞的情况\nBAC009S0916W0489 湖北一公司以员工名义贷款数十员工负债千万\nasr_example_cn_en 所有只要处理 data 不管你是做 machine learning 做 deep learning 做 data analytics 做 data science 也好 scientist 也好通通都要都做的基本功啊那 again 先先对有一些 > 也许对\nID0012W0014 he tried to think how it could be\n```\n\n----------------------------------------\n\nTITLE: Defining Audio File Paths in Bash\nDESCRIPTION: Demonstrates the format for train_wav.scp file, which maps audio file IDs to their corresponding file paths.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0764W0121.wav\nBAC009S0916W0489 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/BAC009S0916W0489.wav\nasr_example_cn_en https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_cn_en.wav\nID0012W0014 https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav\n```\n\n----------------------------------------\n\nTITLE: Specifying Audio Language Labels in Bash\nDESCRIPTION: Shows the format for train_text_language.txt file, which assigns language labels to audio file IDs.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 <|zh|>\nBAC009S0916W0489 <|zh|>\nasr_example_cn_en <|zh|>\nID0012W0014 <|en|>\n```\n\n----------------------------------------\n\nTITLE: Defining Emotion Labels for Audio Files in Bash\nDESCRIPTION: Illustrates the format for train_emo.txt file, which assigns emotion labels to audio file IDs.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 <|NEUTRAL|>\nBAC009S0916W0489 <|NEUTRAL|>\nasr_example_cn_en <|NEUTRAL|>\nID0012W0014 <|NEUTRAL|>\n```\n\n----------------------------------------\n\nTITLE: Assigning Event Labels to Audio Files in Bash\nDESCRIPTION: Demonstrates the format for train_event.txt file, which assigns event type labels to audio file IDs.\nSOURCE: https://github.com/FunAudioLLM/SenseVoice/blob/main/README_zh.md#2025-04-17_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nBAC009S0764W0121 <|Speech|>\nBAC009S0916W0489 <|Speech|>\nasr_example_cn_en <|Speech|>\nID0012W0014 <|Speech|>\n```"
  }
]
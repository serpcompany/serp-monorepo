[
  {
    "owner": "pytorch",
    "repo": "pytorch",
    "content": "TITLE: Exporting a PyTorch Model Using torch.onnx.export (Python)\nDESCRIPTION: This snippet demonstrates exporting a custom PyTorch nn.Module to the ONNX format using torch.onnx.export. It defines a simple convolutional model, generates a random input tensor, and invokes the export function with model, input, and output filename specified. Key parameters include input_names for naming ONNX inputs and dynamo for selecting the exporter backend. The code requires torch to be installed and outputs an ONNX file named 'my_model.onnx'; model inputs and outputs should be tensor-like and match the model signature. The example is self-contained, but the exported model may depend on ONNX specification version and runtime compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.conv1 = torch.nn.Conv2d(1, 128, 5)\n\n    def forward(self, x):\n        return torch.relu(self.conv1(x))\n\ninput_tensor = torch.rand((1, 1, 128, 128), dtype=torch.float32)\n\nmodel = MyModel()\n\ntorch.onnx.export(\n    model,                  # model to export\n    (input_tensor,),        # inputs of the model,\n    \"my_model.onnx\",        # filename of the ONNX model\n    input_names=[\"input\"],  # Rename inputs for the ONNX model\n    dynamo=True             # True or False to select the exporter to use\n)\n\n```\n\n----------------------------------------\n\nTITLE: Using Learning Rate Schedulers in PyTorch\nDESCRIPTION: Example showing how to use a learning rate scheduler with an optimizer. The scheduler adjusts the learning rate after each epoch based on a predefined strategy.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = ExponentialLR(optimizer, gamma=0.9)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Using ATen (aten.mm.default) in PyTorch (Python)\nDESCRIPTION: Each snippet shows usage of aten.mm (dense matrix multiplication), applied between pairs of 2D float16 tensors of various shapes. All operands require matched inner channel dimensions (e.g., [32,1000] x [1000,1280]), returned result is a matrix of leading batch size. This is used in fully connected neural network or classification layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), T([1000, 1280], f16)), {})\ncnt: 1, ((T([1000, 32], f16, stride=(0, 0)), T([32, 1280], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple MLP Model with PyTorch - Python\nDESCRIPTION: Defines a simple multilayer perceptron (MLP) as a subclass of nn.Module in PyTorch with three fully connected layers and ReLU activations. The network expects input of shape [*, 784] (flattened 28x28), and outputs logits for 10 classes. No external dependencies are needed beyond PyTorch, and the architecture is designed for tasks like MNIST digit classification.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Here's a simple MLP\nclass SimpleMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.flatten(1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = self.fc3(x)\n        return x\n\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Custom PyTorch Module (MyLinear)\nDESCRIPTION: Defines a custom linear layer module named `MyLinear` that inherits from `torch.nn.Module`. It initializes learnable weight and bias parameters using `nn.Parameter` and defines the forward computation (affine transformation) in the `forward` method. This demonstrates the fundamental structure of a PyTorch module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import nn\n\nclass MyLinear(nn.Module):\n  def __init__(self, in_features, out_features):\n    super().__init__()\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n    self.bias = nn.Parameter(torch.randn(out_features))\n\n  def forward(self, input):\n    return (input @ self.weight) + self.bias\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom PyTorch Module with Different Training and Evaluation Behavior\nDESCRIPTION: Demonstrates how to create a custom PyTorch module that behaves differently in training and evaluation modes using the 'training' attribute.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass ModalModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n  def forward(self, x):\n    if self.training:\n      # Add a constant only in training mode.\n      return x + 1.\n    else:\n      return x\n\n\nm = ModalModule()\nx = torch.randn(4)\n\nprint('training mode output: {}'.format(m(x)))\n: tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481])\n\nm.eval()\nprint('evaluation mode output: {}'.format(m(x)))\n: tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])\n```\n\n----------------------------------------\n\nTITLE: Performing Batch Matrix Multiplication with ATen BMM Operator\nDESCRIPTION: Covers `aten.bmm`, an essential operator for batch matrix multiplication in sequence models. Supports tensors of dimensions like [256, 1024, 32]. The operation is highly parallelizable and supports efficient computation on GPUs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 2, ((T([256, 1024, 32], f16, stride=(32768, 1, 1024)), T([256, 32, 1024], f16)), {})\ncnt: 2, ((T([256, 1024, 1024], f16), T([256, 1024, 32], f16, stride=(32768, 1, 1024))), {})\n```\n\n----------------------------------------\n\nTITLE: Activation Functions: GELU and LayerNorm (Forward and Backward) in PyTorch (Python)\nDESCRIPTION: Demonstrates use of the Gaussian Error Linear Unit (gelu) and native_layer_norm (plus their backwards), common in transformer and modern deep models. Forward ops take tensors with compatible feature size; backward ops require additional gradients and scale tensors. All inputs are in fp16, and intermediate results are also fp16 or fp32.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.gelu.default\ncnt: 12, ((T([4, 128, 3072], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.gelu.default\ncnt: 1, ((T([4, 128, 768], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.gelu_backward.default\ncnt: 1, ((T([4, 128, 768], f16), T([4, 128, 768], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([4, 128, 3072], f16), T([4, 128, 3072], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_layer_norm.default\ncnt: 26, ((T([4, 128, 768], f16), [768], T([768], f16), T([768], f16), 1e-12), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_layer_norm_backward.default\ncnt: 26, ((T([4, 128, 768], f16), T([4, 128, 768], f16), [768], T([4, 128, 1], f32), T([4, 128, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Subgraph Rewriting in PyTorch FX\nDESCRIPTION: This example demonstrates how to use the subgraph rewriter in PyTorch FX to replace a pattern of operations with a single operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.fx import subgraph_rewriter\n\ndef replace_patterns(graph_module):\n    def pattern(x, y):\n        x = torch.ops.aten.add.Tensor(x, y)\n        x = torch.ops.aten.mul.Tensor(x, y)\n        return x\n\n    def replacement(x, y):\n        return torch.ops.aten.sub.Tensor(x, y)\n\nreplaced_patterns = subgraph_rewriter.replace_pattern_with_filters(\n    traced_module, pattern, replacement\n)\n```\n\n----------------------------------------\n\nTITLE: Optional Type Annotation for Local Variables\nDESCRIPTION: Demonstrates annotating local variables with Optional type to handle variables that may be None or a specific type. This is necessary when a variable might have different concrete types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef f(a, setVal: bool):\n    value: Optional[torch.Tensor] = None\n    if setVal:\n        value = a\n    return value\n\nones = torch.ones([6])\nm = torch.jit.script(f)\nprint(\"TorchScript:\", m(ones, True), m(ones, False))\n```\n\n----------------------------------------\n\nTITLE: Convolution Backward Operations in PyTorch\nDESCRIPTION: Statistics for the aten.convolution_backward.default operator showing backward pass for convolution operations. These operations compute gradients for various convolutional layers with different configurations, used during the backpropagation phase of training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 3072, 6, 6], f16), T([128, 1536, 6, 6], f16), T([3072, 1536, 1, 1], f16), [3072], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 9, ((T([128, 1536, 1, 1], f16), T([128, 768, 1, 1], f16), T([1536, 768, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 9, ((T([128, 768, 1, 1], f16), T([128, 1536, 1, 1], f16), T([768, 1536, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 3, ((T([128, 1536, 6, 6], f16), T([128, 768, 6, 6], f16), T([1536, 768, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 5, ((T([128, 768, 6, 6], f16), T([128, 768, 6, 6], f16), T([768, 128, 3, 3], f16), [768], [1, 1], [1, 1], [1, 1], False, [0, 0], 6, [True, True, True]), {})\ncnt: 2, ((T([128, 768, 6, 6], f16), T([128, 1536, 6, 6], f16), T([768, 1536, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 768, 6, 6], f16), T([128, 768, 13, 13], f16), T([768, 128, 3, 3], f16), [768], [2, 2], [0, 0], [1, 1], False, [0, 0], 6, [True, True, True]), {})\ncnt: 6, ((T([128, 768, 12, 12], f16), T([128, 1536, 12, 12], f16), T([768, 1536, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 1536, 6, 6], f16), T([128, 1536, 6, 6], f16), T([1536, 1536, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 6, ((T([128, 1536, 12, 12], f16), T([128, 768, 12, 12], f16), T([1536, 768, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 11, ((T([128, 768, 12, 12], f16), T([128, 768, 12, 12], f16), T([768, 128, 3, 3], f16), [768], [1, 1], [1, 1], [1, 1], False, [0, 0], 6, [True, True, True]), {})\ncnt: 1, ((T([128, 768, 12, 12], f16), T([128, 768, 25, 25], f16), T([768, 128, 3, 3], f16), [768], [2, 2], [0, 0], [1, 1], False, [0, 0], 6, [True, True, True]), {})\ncnt: 1, ((T([128, 768, 24, 24], f16), T([128, 512, 24, 24], f16), T([768, 512, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 1536, 12, 12], f16), T([128, 512, 12, 12], f16), T([1536, 512, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 512, 1, 1], f16), T([128, 256, 1, 1], f16), T([512, 256, 1, 1], f16), [512], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 256, 1, 1], f16), T([128, 512, 1, 1], f16), T([256, 512, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 3, ((T([128, 512, 24, 24], f16), T([128, 256, 24, 24], f16), T([512, 256, 1, 1], f16), [512], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 3, ((T([128, 256, 24, 24], f16), T([128, 256, 24, 24], f16), T([256, 128, 3, 3], f16), [256], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 24, 24], f16), T([128, 512, 24, 24], f16), T([256, 512, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 24, 24], f16), T([128, 256, 49, 49], f16), T([256, 128, 3, 3], f16), [256], [2, 2], [0, 0], [1, 1], False, [0, 0], 2, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 48, 48], f16), T([128, 256, 48, 48], f16), T([256, 256, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 1, 1], f16), T([128, 128, 1, 1], f16), T([256, 128, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 128, 1, 1], f16), T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), [128], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 256, 48, 48], f16), T([128, 128, 48, 48], f16), T([256, 128, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 128, 48, 48], f16), T([128, 128, 48, 48], f16), T([128, 128, 3, 3], f16), [128], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Computing Per-Sample Gradients using torch.func.vmap and torch.func.grad in Python\nDESCRIPTION: This snippet demonstrates the composition of `vmap` and `grad` to compute per-sample gradients, a task challenging in standard PyTorch. A `compute_loss` function calculates the MSE loss for a single example. `grad(compute_loss)` creates a function to compute the gradient of the loss with respect to the first argument (weights). `vmap` is then applied, specifying that the `weights` argument should not be mapped (`None`) while the `example` and `target` arguments should be mapped along their 0th dimension. This results in computing the gradient of the loss with respect to the weights for each example in the batch independently.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.func import vmap\nbatch_size, feature_size = 3, 5\n\ndef model(weights,feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\ndef compute_loss(weights, example, target):\n    y = model(weights, example)\n    return ((y - target) ** 2).mean()  # MSELoss\n\nweights = torch.randn(feature_size, requires_grad=True)\nexamples = torch.randn(batch_size, feature_size)\ntargets = torch.randn(batch_size)\ninputs = (weights,examples, targets)\ngrad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n```\n\n----------------------------------------\n\nTITLE: Compiling PyTorch Model with AOTInductor\nDESCRIPTION: Demonstrates how to export a PyTorch model using torch.export and compile it using AOTInductor. The example includes a simple neural network model with dynamic batch dimension support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 16)\n        self.relu = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(16, 1)\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\nwith torch.no_grad():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = Model().to(device=device)\n    example_inputs=(torch.randn(8, 10, device=device),)\n    batch_dim = torch.export.Dim(\"batch\", min=1, max=1024)\n    exported = torch.export.export(model, example_inputs, dynamic_shapes={\"x\": {0: batch_dim}})\n    output_path = torch._inductor.aoti_compile_and_package(\n        exported,\n        package_path=os.path.join(os.getcwd(), \"model.pt2\"),\n    )\n```\n\n----------------------------------------\n\nTITLE: Custom Module with State Dict Serialization - PyTorch - Python\nDESCRIPTION: Presents the process of defining a custom torch.nn.Module with submodules, extracting its state_dict, saving this to disk, and restoring it into an identically structured module. Depends on torch and torch.nn.functional. Inputs are the module definition and instantiation; outputs are saved and restored state_dicts. Key parameters include layer dimensions. This pattern is extendable to complex, nested modules.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# A module with two linear layers\n>>> class MyModule(torch.nn.Module):\n      def __init__(self):\n        super().__init__()\n        self.l0 = torch.nn.Linear(4, 2)\n        self.l1 = torch.nn.Linear(2, 1)\n\n      def forward(self, input):\n        out0 = self.l0(input)\n        out0_relu = torch.nn.functional.relu(out0)\n        return self.l1(out0_relu)\n\n>>> m = MyModule()\n>>> m.state_dict()\nOrderedDict([('l0.weight', tensor([[ 0.1400, 0.4563, -0.0271, -0.4406],\n                                   [-0.3289, 0.2827, 0.4588, 0.2031]])),\n             ('l0.bias', tensor([ 0.0300, -0.1316])),\n             ('l1.weight', tensor([[0.6533, 0.3413]])),\n             ('l1.bias', tensor([-0.1112]))])\n\n>>> torch.save(m.state_dict(), 'mymodule.pt')\n>>> m_state_dict = torch.load('mymodule.pt')\n>>> new_m = MyModule()\n>>> new_m.load_state_dict(m_state_dict)\n<All keys matched successfully>\n```\n\n----------------------------------------\n\nTITLE: Initializing Distributed Process Group with Shared File - PyTorch (Python)\nDESCRIPTION: Initializes a PyTorch distributed process group using a shared file location as the rendezvous point. Relies on 'torch.distributed' and requires specifying 'backend', 'init_method' (with a valid file:// path), 'world_size', and 'rank' arguments. This setup assumes all participating nodes can access the same shared file; the user must ensure the file does not pre-exist and is cleaned up afterward to avoid deadlocks or errors. The rank must be specified for every process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic TorchDynamo Optimization\nDESCRIPTION: Demonstrates how to use TorchDynamo's optimize decorator to compile and optimize a PyTorch function. Shows a basic example with tensor operations and conditional logic, including a custom compiler function that prints the FX graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport torch\nfrom torch import _dynamo as torchdynamo\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n\n@torchdynamo.optimize(my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n```\n\n----------------------------------------\n\nTITLE: Managing Default CUDA Device using torch.cuda.device Context Manager\nDESCRIPTION: This Python code illustrates the use of the `torch.cuda.device` context manager to control the default CUDA device for operations within its scope. It shows that operations outside the `with` block use the default device (usually 0), while operations inside `with torch.cuda.device(1):` are executed on CUDA device 1. The default device reverts back after exiting the context manager. This requires `torch` and typically multiple available GPUs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Outside device is 0\")  # On device 0 (default in most scenarios)\nwith torch.cuda.device(1):\n    print(\"Inside device is 1\")  # On device 1\nprint(\"Outside device is still 0\")  # On device 0\n```\n\n----------------------------------------\n\nTITLE: Creating and Using torch.device Objects in PyTorch\nDESCRIPTION: This code snippet illustrates how to create torch.device objects using different constructors, including string-based and combined string/ordinal approaches. It covers various device types like CUDA, CPU, and MPS.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('mps')\ndevice(type='mps')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantization-Aware Training in PyTorch\nDESCRIPTION: Demonstrates the process of implementing quantization-aware training (QAT) including model preparation, fusing modules, and converting to quantized format. Uses torch.ao.quantization APIs to prepare and convert the model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nmodel_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n\nmodel_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,\n    [['conv', 'bn', 'relu']])\n\nmodel_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train())\n\ntraining_loop(model_fp32_prepared)\n\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n\nres = model_int8(input_fp32)\n```\n\n----------------------------------------\n\nTITLE: Managing CUDA Devices and Tensor Allocation in PyTorch (Python)\nDESCRIPTION: Demonstrates how to define CUDA devices, allocate tensors on specific GPUs, and transfer tensors between devices using `torch.device`, `.cuda()`, `.to()`, and the `torch.cuda.device` context manager. It illustrates that operations generally occur on the tensor's device, regardless of the currently selected default device.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncuda = torch.device('cuda')     # Default CUDA device\ncuda0 = torch.device('cuda:0')\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch DataLoader for Reproducibility in Python\nDESCRIPTION: Provides a complete example for configuring a PyTorch `DataLoader` to achieve reproducible data loading, especially when using multiple workers. It defines a `worker_init_fn` (`seed_worker`) to individually seed each worker based on the main process's initial seed, ensuring consistent randomness (e.g., for augmentations) within each worker. It also uses a pre-seeded `torch.Generator` passed to the `DataLoader` to control shuffling and sampling randomness.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker,\n    generator=g,\n)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading PyTorch Tensors - PyTorch - Python\nDESCRIPTION: Demonstrates the basic usage of torch.save and torch.load to serialize and deserialize individual tensors. Assumes torch is imported; depends on the torch package. Takes a PyTorch tensor, saves it to a .pt file, and loads it back, preserving the tensor's data. File extensions such as .pt or .pth are conventionally used. The loaded tensor should be identical to the original; limitations relate to file permissions and format compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> t = torch.tensor([1., 2.])\n>>> torch.save(t, 'tensor.pt')\n>>> torch.load('tensor.pt')\ntensor([1., 2.])\n```\n\n----------------------------------------\n\nTITLE: Applying Custom Initialization to PyTorch Module Parameters\nDESCRIPTION: Shows how to define a custom initialization function and apply it recursively to a PyTorch module and its submodules using the 'apply' method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Define a function to initialize Linear weights.\n# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n@torch.no_grad()\ndef init_weights(m):\n  if isinstance(m, nn.Linear):\n    nn.init.xavier_normal_(m.weight)\n    m.bias.fill_(0.0)\n\n# Apply the function recursively on the module and its submodules.\ndynamic_net.apply(init_weights)\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantization Aware Training in PyTorch\nDESCRIPTION: Demonstrates quantization-aware training setup with fake quantization modules to model quantization effects during training. Includes model definition with quantization stubs and batch normalization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n# define a floating point model where some layers could benefit from QAT\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.ao.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.ao.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to eval for fusion to work\nmodel_fp32.eval()\n```\n\n----------------------------------------\n\nTITLE: Old API for Module Attributes in TorchScript\nDESCRIPTION: Demonstrates the older approach for defining attributes in TorchScript modules by inheriting from torch.jit.ScriptModule and using torch.jit.Attribute.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\nimport torch\n\nclass MyModule(torch.jit.ScriptModule):\n    def __init__(self):\n        super().__init__()\n        self.my_dict = torch.jit.Attribute({}, Dict[str, int])\n        self.my_int = torch.jit.Attribute(20, int)\n\nm = MyModule()\n```\n\n----------------------------------------\n\nTITLE: Loading Optimizer State Dict by Parameter Names\nDESCRIPTION: This example shows how to implement a custom hook for loading an optimizer state dict based on parameter names instead of their order. It ensures that parameters are correctly mapped even if their order changes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef names_matching(optimizer, state_dict):\n    assert len(state_dict['param_groups']) == len(optimizer.state_dict()['param_groups'])\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    for g_ind in range(len(state_dict['param_groups'])):\n        assert len(state_dict['param_groups'][g_ind]['params']) == len(\n            optimizer.state_dict()['param_groups'][g_ind]['params'])\n\n        for k, v in state_dict['param_groups'][g_ind].items():\n            if k not in ['params', 'param_names']:\n                adapted_state_dict['param_groups'][g_ind][k] = v\n\n        for param_id, param_name in zip(\n                optimizer.state_dict()['param_groups'][g_ind]['params'],\n                optimizer.state_dict()['param_groups'][g_ind]['param_names']):\n            index_in_loaded_list = state_dict['param_groups'][g_ind]['param_names'].index(param_name)\n            id_in_loaded = state_dict['param_groups'][g_ind]['params'][index_in_loaded_list]\n            # Copy the state of the corresponding parameter\n            if id_in_loaded in state_dict['state']:\n                adapted_state_dict['state'][param_id] = deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n```\n\n----------------------------------------\n\nTITLE: Graph Module Inspection Example\nDESCRIPTION: Shows various methods to inspect and debug a traced GraphModule including printing code, graph structure, and tabular representation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\nm = M()\ntraced = symbolic_trace(m)\n\nprint(traced)\nprint(traced.graph)\ntraced.graph.print_tabular()\n```\n\n----------------------------------------\n\nTITLE: Initializing and Sharding a Distributed Tensor in PyTorch\nDESCRIPTION: This snippet demonstrates how to create a device mesh, initialize a large tensor, and shard it across multiple devices using DTensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nfrom torch.distributed.tensor import init_device_mesh, Shard, distribute_tensor\n\nmesh = init_device_mesh(\"cuda\", (int(os.environ[\"WORLD_SIZE\"]),))\nbig_tensor = torch.randn(100000, 88)\nmy_dtensor = distribute_tensor(big_tensor, mesh, [Shard(dim=0)])\n```\n\n----------------------------------------\n\nTITLE: Defining TorchScript-Compatible Classes - PyTorch JIT - Python\nDESCRIPTION: This snippet shows how to define a simple class that is compatible with TorchScript using the @torch.jit.script decorator. The class Foo includes a constructor and a method that mutates state, demonstrating valid use cases within the TorchScript subset. All member functions must themselves be TorchScript-compatible. There are current limitations, including the need for all code paths and types to be statically resolvable. Requires the torch library and is only for illustrative or experimental use due to the experimental status of scripted classes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\nclass Foo:\n  def __init__(self, x, y):\n    self.x = x\n\n  def aug_add_x(self, inc):\n    self.x += inc\n```\n\n----------------------------------------\n\nTITLE: TorchDynamo Explain Usage Example\nDESCRIPTION: Shows how to use torch._dynamo.explain to identify and debug graph breaks in a PyTorch function. The example includes a toy function with intentional graph breaks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch._dynamo as dynamo\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    print(\"woo\")\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nexplanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))\n```\n\n----------------------------------------\n\nTITLE: Autograd with Tensors\nDESCRIPTION: Demonstrates creating a tensor with gradient tracking and performing automatic differentiation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n```\n\n----------------------------------------\n\nTITLE: Registering Custom PyTorch Operator Lambda Kernel (C++)\nDESCRIPTION: Shows how to register a stateless C++ lambda function directly as a kernel for a custom operator (`my_namespace::my_op`) associated with the CPU backend. The registration uses `torch::RegisterOperators` and passes the lambda to the `.kernel()` method along with the dispatch key (`CPU()`). The lambda must not have a closure.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nstatic auto registry = torch::RegisterOperators()\n    .op(\"my_namespace::my_op\", torch::RegisterOperators::options()\n        .kernel(CPU(), [] (const Tensor& a) -> Tensor{...}));\n```\n\n----------------------------------------\n\nTITLE: Gradient Clipping with torch.amp and GradScaler (Python)\nDESCRIPTION: This snippet shows how to perform gradient clipping with AMP in PyTorch by first unscaling gradients via GradScaler.unscale_ before calling torch.nn.utils.clip_grad_norm_. This ensures clipping is performed using unscaled gradients, preserving correct numerical thresholds. Requires PyTorch, model/optimizer set up for mixed precision, and clip_grad_norm_ utility. The scaler tracks if unscale_ was already called to avoid redundant unscaling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # Unscales the gradients of optimizer's assigned params in-place\n        scaler.unscale_(optimizer)\n\n        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n\n        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n        # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n        scaler.step(optimizer)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\n```\n\n----------------------------------------\n\nTITLE: Training Loop Memory Management in PyTorch\nDESCRIPTION: This snippet demonstrates a common pitfall in accumulating history across a training loop that can lead to excessive memory usage in PyTorch. It shows how to prevent memory buildup by converting a differentiable loss variable before aggregation to avoid retaining unnecessary computation history.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/faq.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n    total_loss = 0\n    for i in range(10000):\n        optimizer.zero_grad()\n        output = model(input)\n        loss = criterion(output)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss\n```\n\n----------------------------------------\n\nTITLE: Rewriting Data-Dependent Branching with torch.cond in PyTorch (Python)\nDESCRIPTION: These snippets compare two approaches to handling an if-statement with a data-dependent (dynamic) condition in a torch.nn.Module: using native Python branching vs. explicitly using torch.cond. The first module relies on x.sum() > 0 for branching, which is not permitted for dynamic shapes when tracing/exporting; the second rewrites the logic using torch.cond, allowing both branches to be traced. Both expect the PyTorch framework and proper import of torch.cond. The input x must be a Tensor, and the outputs are the result of either x.sin() or x.cos(), depending on the sum. No additional dependencies are required beyond PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass M_old(torch.nn.Module):\n    def forward(self, x):\n        if x.sum() > 0:\n            return x.sin()\n        else:\n            return x.cos()\n```\n\nLANGUAGE: python\nCODE:\n```\nclass M_new(torch.nn.Module):\n    def forward(self, x):\n        return torch.cond(\n            pred=x.sum() > 0,\n            true_fn=lambda x: x.sin(),\n            false_fn=lambda x: x.cos(),\n            operands=(x,),\n        )\n```\n\n----------------------------------------\n\nTITLE: Sparsifying Model Embeddings in PyTorch\nDESCRIPTION: Example showing how to use the data sparsifier to sparsify embeddings in a neural network model. The sparsifier is applied to the embedding layer and integrated into the model training process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Model(nn.Module):\n    def __init__(self, feature_dim, emb_dim, num_classes):\n        self.emb = nn.EmbeddingBag(feature_dim, emb_dim)\n        self.linear1 = nn.Linear(emb_dim, 32)\n        self.linear2 = nn.Linear(32, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.emb(x)\n        out = self.relu(self.linear1(out))\n        out = self.linear2(out)\n        return out\n\nmodel = Model(100, 32, 10)\nmy_sparsifier = ImplementedDataSparsifier(threshold=0.5)\nmy_sparsifier.add_data(name='emb', data=model.emb)\n\n...\n# Train model\n...\n\nmy_sparsifier.step()  # creates mask for embeddings\n\nmy_sparsifier.squash_mask()  # applies and removes mask\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Linear Function in PyTorch\nDESCRIPTION: This Python snippet demonstrates how to create a custom linear function by subclassing torch.autograd.Function. Dependencies include PyTorch's autograd module. The implementation includes a forward method for computation, setup_context for managing forward pass tensors, and backward for computing gradients. Ensure ctx is used to correctly mark gradients and input changes. Inputs include tensors for input, weight, and optional bias, while outputs are tensors resulting from matrix multiplication and addition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Inherit from Function\nclass LinearFunction(Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(input, weight, bias):\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    # inputs is a Tuple of all of the inputs passed to forward.\n    # output is the output of the forward().\n    def setup_context(ctx, inputs, output):\n        input, weight, bias = inputs\n        ctx.save_for_backward(input, weight, bias)\n\n    # This function has only a single output, so it gets only one gradient\n    @staticmethod\n    def backward(ctx, grad_output):\n        # This is a pattern that is very convenient - at the top of backward\n        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n        # None. Thanks to the fact that additional trailing Nones are\n        # ignored, the return statement is simple even when the function has\n        # optional inputs.\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        # These needs_input_grad checks are optional and there only to\n        # improve efficiency. If you want to make your code simpler, you can\n        # skip them. Returning gradients for inputs that don't require it is\n        # not an error.\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Named Parameters of a Module\nDESCRIPTION: Shows how to access and print the registered parameters (weight and bias) of the `MyLinear` module instance `m` using the `named_parameters()` method. This method yields tuples containing the parameter name and the parameter tensor itself, indicating they require gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfor parameter in m.named_parameters():\n  print(parameter)\n: ('weight', Parameter containing:\ntensor([[ 1.0597,  1.1796,  0.8247],\n        [-0.5080, -1.2635, -1.1045],\n        [ 0.0593,  0.2469, -1.4299],\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n('bias', Parameter containing:\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))\n```\n\n----------------------------------------\n\nTITLE: Integrating Traced Modules in Scripted nn.Module with torch.jit (Python)\nDESCRIPTION: Illustrates use of a traced torchvision model (ResNet18) within a custom nn.Module scripted by TorchScript. Dependencies are torch and torchvision. The 'MyScriptModule' class preprocesses input images using mean normalization and invokes the traced ResNet. Demonstrates submodule composition, parameter management, and scripting of modules; inputs are images (torch.Tensor), outputs are model predictions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\nimport torchvision\\n\\nclass MyScriptModule(torch.nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])\\n                                        .resize_(1, 3, 1, 1))\\n        self.resnet = torch.jit.trace(torchvision.models.resnet18(),\\n                                      torch.rand(1, 3, 224, 224))\\n\\n    def forward(self, input):\\n        return self.resnet(input - self.means)\\n\\nmy_script_module = torch.jit.script(MyScriptModule())\\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Hessian-Vector Product Using Forward-Mode AD in PyTorch\nDESCRIPTION: Demonstrates an efficient implementation of Hessian-vector products (HVP) by composing reverse-mode AD with forward-mode AD. This approach is memory efficient as it avoids constructing the full Hessian.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import jvp, grad, vjp\n\ndef hvp(f, primals, tangents):\n  return jvp(grad(f), primals, tangents)[1]\n```\n\n----------------------------------------\n\nTITLE: Vectorizing a Model with torch.func.vmap in Python\nDESCRIPTION: This snippet illustrates how `torch.func.vmap` can automatically vectorize a function designed for single inputs. A simple linear `model` function, which expects a 1D feature vector, is defined. `vmap(model)` is then used to apply this model across a batch of `examples` (a 2D tensor) without modifying the original `model` function to handle batches explicitly. The `vmap` transform effectively adds a batch dimension to the operations within the `model`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.func import vmap\nbatch_size, feature_size = 3, 5\nweights = torch.randn(feature_size, requires_grad=True)\n\ndef model(feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\nexamples = torch.randn(batch_size, feature_size)\nresult = vmap(model)(examples)\n```\n\n----------------------------------------\n\nTITLE: Moving DataLoader Batches to a Specific CUDA Device in PyTorch\nDESCRIPTION: This Python snippet shows a common pattern in PyTorch training loops for moving input data batches to a specific CUDA device. It defines a `torch.device` for the first CUDA GPU ('cuda:0') and then, within the loop iterating over a `train_loader`, uses the `.to(cuda0)` method to transfer each data batch `x` to that GPU before processing. It requires `torch` and an initialized `DataLoader` named `train_loader`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ncuda0 = torch.device('cuda:0')  # CUDA GPU 0\nfor i, x in enumerate(train_loader):\n    x = x.to(cuda0)\n```\n\n----------------------------------------\n\nTITLE: Typical Mixed Precision Training with torch.amp (Python)\nDESCRIPTION: This snippet demonstrates a standard workflow for mixed precision training in PyTorch using torch.amp.autocast and torch.amp.GradScaler. It initializes a model and optimizer, runs forward and backward passes with autocasting for float16 precision on CUDA, and uses a GradScaler to handle gradient scaling and updating across training epochs. Dependencies: PyTorch, CUDA or XPU device support, torch.amp (Python). Inputs are data batches (input, target); outputs are model parameter updates. Ensure GradScaler and autocast regions are used as shown for proper mixed precision handling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\n# Creates a GradScaler once at the beginning of training.\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Runs the forward pass with autocasting.\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        scaler.scale(loss).backward()\n\n        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n        # otherwise, optimizer.step() is skipped.\n        scaler.step(optimizer)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\n```\n\n----------------------------------------\n\nTITLE: Exporting Model with Dynamic Input Shapes - PyTorch - Python\nDESCRIPTION: This snippet illustrates creating a PyTorch module with two branches and a persistent buffer, preparing example inputs, and specifying dynamic input dimension using torch.export.Dim and the export dynamic_shapes argument. Demonstrates setting the first input dimension (typically batch) as dynamic for flexible exported models. Dependencies: PyTorch; inputs required: appropriately shaped tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.export import Dim, export\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\n\n# Create a dynamic batch size\nbatch = Dim(\"batch\")\n# Specify that the first dimension of each input is that batch size\ndynamic_shapes = {\"x1\": {0: batch}, \"x2\": {0: batch}}\n```\n\n----------------------------------------\n\nTITLE: Multiple Models and Optimizers with PyTorch AMP\nDESCRIPTION: Demonstrates using GradScaler with multiple models, losses, and optimizers in a training loop, including proper scaling, unscaling, and update steps.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nscaler = torch.amp.GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer0.zero_grad()\n        optimizer1.zero_grad()\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output0 = model0(input)\n            output1 = model1(input)\n            loss0 = loss_fn(2 * output0 + 3 * output1, target)\n            loss1 = loss_fn(3 * output0 - 5 * output1, target)\n\n        scaler.scale(loss0).backward(retain_graph=True)\n        scaler.scale(loss1).backward()\n\n        scaler.unscale_(optimizer0)\n\n        scaler.step(optimizer0)\n        scaler.step(optimizer1)\n\n        scaler.update()\n```\n\n----------------------------------------\n\nTITLE: Basic PyTorch Model Compilation Example\nDESCRIPTION: Demonstrates a simple example of using torch.compile() to compile a sequential neural network model with Linear, LayerNorm and ReLU layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile()\ndef test_model(x):\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 10),\n        torch.nn.LayerNorm(10),\n        torch.nn.ReLU(),\n    )\n    return model(x)\n\n\ny = test_model(torch.ones(10, 10))\n```\n\n----------------------------------------\n\nTITLE: Checking MPS Availability and Using the 'mps' Device in PyTorch (Python)\nDESCRIPTION: This Python snippet demonstrates checking for the availability of the PyTorch MPS backend using `torch.backends.mps.is_available()` and `torch.backends.mps.is_built()`. If MPS is available, it shows how to create a `torch.device('mps')`, allocate tensors directly on the MPS device, perform tensor operations, and move a PyTorch model to the MPS device using `.to()` for GPU acceleration on compatible macOS hardware. Requires the PyTorch library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/mps.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Check that MPS is available\nif not torch.backends.mps.is_available():\n    if not torch.backends.mps.is_built():\n        print(\"MPS not available because the current PyTorch install was not \"\n              \"built with MPS enabled.\")\n    else:\n        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n              \"and/or you do not have an MPS-enabled device on this machine.\")\n\nelse:\n    mps_device = torch.device(\"mps\")\n\n    # Create a Tensor directly on the mps device\n    x = torch.ones(5, device=mps_device)\n    # Or\n    x = torch.ones(5, device=\"mps\")\n\n    # Any operation happens on the GPU\n    y = x * 2\n\n    # Move your model to mps just like any other device\n    model = YourFavoriteNet()\n    model.to(mps_device)\n\n    # Now every call runs on the GPU\n    pred = model(x)\n```\n\n----------------------------------------\n\nTITLE: Custom Autograd Function with AMP Support\nDESCRIPTION: Example of implementing a custom autograd function with proper AMP support using custom_fwd and custom_bwd decorators.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass MyMM(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd\n    def forward(ctx, a, b):\n        ctx.save_for_backward(a, b)\n        return a.mm(b)\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad):\n        a, b = ctx.saved_tensors\n        return grad.mm(b.t()), a.t().mm(grad)\n```\n\n----------------------------------------\n\nTITLE: Computing Convolution Gradients with aten.convolution_backward.default\nDESCRIPTION: Shows the gradient computation for convolution operations during backpropagation. This includes gradients for weights and input tensors across various network layers, with particular focus on multi-scale feature extraction and detection heads.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([8, 255, 48, 64], f16), T([8, 256, 48, 64], f16), T([255, 256, 1, 1], f16), [255], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 11, ((T([8, 256, 48, 64], f16), T([8, 128, 48, 64], f16), T([256, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 10, ((T([8, 128, 48, 64], f16), T([8, 256, 48, 64], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 128, 48, 64], f16), T([8, 384, 48, 64], f16), T([128, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 128, 24, 32], f16), T([8, 256, 24, 32], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 255, 24, 32], f16), T([8, 512, 24, 32], f16), T([255, 512, 1, 1], f16), [255], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 11, ((T([8, 512, 24, 32], f16), T([8, 256, 24, 32], f16), T([512, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 10, ((T([8, 256, 24, 32], f16), T([8, 512, 24, 32], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 256, 24, 32], f16), T([8, 768, 24, 32], f16), T([256, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 256, 12, 16], f16), T([8, 512, 12, 16], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 255, 12, 16], f16), T([8, 1024, 12, 16], f16), T([255, 1024, 1, 1], f16), [255], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 7, ((T([8, 1024, 12, 16], f16), T([8, 512, 12, 16], f16), T([1024, 512, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 7, ((T([8, 512, 12, 16], f16), T([8, 1024, 12, 16], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 512, 12, 16], f16), T([8, 2048, 12, 16], f16), T([512, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 1024, 12, 16], f16), T([8, 512, 24, 32], f16), T([1024, 512, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 512, 24, 32], f16), T([8, 256, 48, 64], f16), T([512, 256, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 256, 48, 64], f16), T([8, 128, 96, 128], f16), T([256, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([8, 128, 96, 128], f16), T([8, 64, 96, 128], f16), T([128, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([8, 64, 96, 128], f16), T([8, 128, 96, 128], f16), T([64, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 128, 96, 128], f16), T([8, 64, 192, 256], f16), T([128, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 64, 192, 256], f16), T([8, 32, 192, 256], f16), T([64, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 32, 192, 256], f16), T([8, 64, 192, 256], f16), T([32, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 64, 192, 256], f16), T([8, 32, 384, 512], f16), T([64, 32, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([8, 32, 384, 512], f16), T([8, 3, 384, 512], f16), T([32, 3, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Seeding PyTorch Random Number Generator in Python\nDESCRIPTION: Seeds the global random number generator for all PyTorch devices (CPU and CUDA) to a specific value (0 in this case) using `torch.manual_seed()`. This helps control randomness in PyTorch operations for reproducible results, ensuring the same sequence of random numbers is generated each time the application runs in the same environment, provided other sources of nondeterminism are also addressed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\ntorch.manual_seed(0)\n```\n\n----------------------------------------\n\nTITLE: Using PassManager for Multiple Passes in PyTorch FX\nDESCRIPTION: This example shows how to use the PassManager to run multiple passes on a PyTorch FX graph module and set up checks after each pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.fx.passes.infra.pass_manager import PassManager\n\npm = PassManager(\n    passes=[replace_add_with_div, replace_div_with_mul],\n    run_checks_after_each_pass=True,\n    suppress_check_failures=False,\n)\ngraph_module_out = pm(graph_module)\n\npm = PassManager(passes=[replace_add_with_div, replace_div_with_mul])\n\ndef check_div_target(graph_module):\n```\n\n----------------------------------------\n\nTITLE: PyTorch CUDA Core Operations Function Declarations\nDESCRIPTION: Collection of C++ function declarations for PyTorch's core CUDA operations including tensor manipulation, matrix multiplication, dropout, and layer normalization. These are the low-level implementations that power PyTorch's GPU operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_22\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4_ops7permute4callERKNS_6TensorEN3c108ArrayRefIlEE\n_ZN2at4_ops2mm4callERKNS_6TensorES4_\n_ZN2at4_ops4silu4callERKNS_6TensorE\n_ZN2at6native7dropoutERKNS_6TensorEdb\n_ZN2at6native16embedding_symintERKNS_6TensorES3_N3c106SymIntEbb\n_ZN2at4_ops28scaled_dot_product_attention4callERKNS_6TensorES4_S4_RKSt8optionalIS2_EdbS5_IdE\n```\n\n----------------------------------------\n\nTITLE: Backward Convolution Operations in PyTorch MobileNetV3\nDESCRIPTION: Statistics for backward pass convolution operations used during training. These compute gradients with respect to inputs and weights for all the convolution layers in the network, following the same structure as the forward pass but in reverse order.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 1984, 7, 7], f16), T([128, 352, 7, 7], f16), T([1984, 352, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 352, 7, 7], f16), T([128, 1104, 7, 7], f16), T([352, 1104, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16), T([1104, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1104, [True, True, False]), {})\ncnt: 4, ((T([128, 1104, 7, 7], f16), T([128, 184, 7, 7], f16), T([1104, 184, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 184, 7, 7], f16), T([128, 1104, 7, 7], f16), T([184, 1104, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16), T([1104, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 1104, [True, True, False]), {})\ncnt: 1, ((T([128, 184, 7, 7], f16), T([128, 672, 7, 7], f16), T([184, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 3, ((T([128, 672, 14, 14], f16), T([128, 112, 14, 14], f16), T([672, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 112, 14, 14], f16), T([128, 336, 14, 14], f16), T([112, 336, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 336, 14, 14], f16), T([128, 336, 14, 14], f16), T([336, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 336, [True, True, False]), {})\ncnt: 1, ((T([128, 336, 14, 14], f16), T([128, 112, 14, 14], f16), T([336, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 112, 14, 14], f16), T([128, 672, 14, 14], f16), T([112, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 672, 14, 14], f16), T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 1, ((T([128, 112, 14, 14], f16), T([128, 384, 14, 14], f16), T([112, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 384, [True, True, False]), {})\ncnt: 3, ((T([128, 384, 14, 14], f16), T([128, 64, 14, 14], f16), T([384, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 64, 14, 14], f16), T([128, 384, 14, 14], f16), T([64, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 64, 14, 14], f16), T([128, 192, 14, 14], f16), T([64, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 14, 14], f16), T([128, 192, 14, 14], f16), T([192, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 192, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 14, 14], f16), T([128, 64, 14, 14], f16), T([192, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 14, 14], f16), T([128, 192, 28, 28], f16), T([192, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 192, [True, True, False]), {})\ncnt: 3, ((T([128, 192, 28, 28], f16), T([128, 32, 28, 28], f16), T([192, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 32, 28, 28], f16), T([128, 192, 28, 28], f16), T([32, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 192, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 192, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Embedding operation in PyTorch\nDESCRIPTION: The aten.embedding.default operator maps an input index tensor to embeddings from a weight matrix [128100, 1536], used widely in NLP tasks for converting indices to dense vectors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.embedding.default\ncnt: 1, ((T([128100, 1536], f16), T([1, 512], i64), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Using torch.cond for Control Flow in PyTorch\nDESCRIPTION: This snippet shows how to use torch.cond for expressing if-else like control flow in PyTorch when dealing with data-dependent control flow.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_17\n\nLANGUAGE: reStructuredText\nCODE:\n```\n:ref:`torch.cond <cond>`\n```\n\n----------------------------------------\n\nTITLE: Exporting Conv2D and BatchNorm Module for Training - PyTorch - Python\nDESCRIPTION: This snippet defines a neural network module combining Conv2D and BatchNorm layers, creates input data, and exports the model for training using torch.export.export_for_training. It demonstrates capturing general ATen ops (including non-functional batch_norm) in the produced IR suitable for eager training. Requires PyTorch 2.5+, and expects module definitions and tensor inputs; output is an ExportedProgram with IR as shown.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass ConvBatchnorm(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nmod = ConvBatchnorm()\ninp = torch.randn(1, 1, 3, 3)\n\nep_for_training = torch.export.export_for_training(mod, (inp,))\nprint(ep_for_training)\n```\n\n----------------------------------------\n\nTITLE: Creating a PyTorch Module with Persistent Buffer\nDESCRIPTION: Demonstrates how to create a PyTorch module with a persistent buffer using 'register_buffer', which is included in the state dictionary but not learnable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass RunningMean(nn.Module):\n  def __init__(self, num_features, momentum=0.9):\n    super().__init__()\n    self.momentum = momentum\n    self.register_buffer('mean', torch.zeros(num_features))\n  def forward(self, x):\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n    return self.mean\n\nm = RunningMean(4)\nfor _ in range(10):\n  input = torch.randn(4)\n  m(input)\n\nprint(m.state_dict())\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))])))\n\n# Serialized form will contain the 'mean' tensor\ntorch.save(m.state_dict(), 'mean.pt')\n\nm_loaded = RunningMean(4)\nm_loaded.load_state_dict(torch.load('mean.pt'))\nassert(torch.all(m.mean == m_loaded.mean))\n```\n\n----------------------------------------\n\nTITLE: Custom Gradient Rule with PyTorch autograd.Function (Python)\nDESCRIPTION: Defines a torch.autograd.Function (MyCube) for y = x ** 3 that moves part of the gradient computation (dx) to forward, allowing customized performance for backward passes. The backward method uses both output gradients and saved Tensors for higher-order differentiation, compatible with torch.func transforms. The example includes a my_cube convenience function and a test for correctness of second derivatives. Requires only torch. Inputs: single scalar tensor. Output: cubed tensor. Edge cases and higher-order gradients are supported by design.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        # In regular PyTorch, if we had just run y = x ** 3, then the backward\n        # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done\n        # that computation here in the forward pass instead.\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndef my_cube(x):\n    result, _ = MyCube.apply(x)\n    return result\n```\n\nLANGUAGE: python\nCODE:\n```\nx = torch.randn([])\nggx = torch.func.grad(torch.func.grad(my_cube))(x)\nassert torch.allclose(ggx, 6 * x)\n```\n\n----------------------------------------\n\nTITLE: PyTorch vmap with Different Randomness\nDESCRIPTION: Demonstrates how to use vmap with 'different' randomness mode where each element in the batch receives different random values. The example shows adding random noise to a tensor where each element gets unique random values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef add_noise(x):\n    y = torch.randn(())  # y will be different across the batch\n    return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"different\")(x)  # we get 3 different values\n```\n\n----------------------------------------\n\nTITLE: Implementing Post Training Dynamic Quantization in PyTorch\nDESCRIPTION: Demonstrates how to apply dynamic quantization to a simple linear layer model. The weights are quantized to int8 while activations remain in fp32, suitable for LSTM and Transformer models with small batch sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n# define a floating point model\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(4, 4)\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n# create a quantized model instance\nmodel_int8 = torch.ao.quantization.quantize_dynamic(\n    model_fp32,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8)  # the target dtype for quantized weights\n\n# run the model\ninput_fp32 = torch.randn(4, 4, 4, 4)\nres = model_int8(input_fp32)\n```\n\n----------------------------------------\n\nTITLE: NTK Computation using Jacobian Contraction\nDESCRIPTION: Implementation of empirical NTK computation using Jacobian contraction method, supporting full matrix, trace, and diagonal computations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/neural_tangent_kernels.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef empirical_ntk_jacobian_contraction(fnet_single, params, x1, x2, compute='full'):\n    # Compute J(x1)\n    jac1 = vmap(jacrev(fnet_single), (None, 0))(params, x1)\n    jac1 = [j.flatten(2) for j in jac1]\n    \n    # Compute J(x2)\n    jac2 = vmap(jacrev(fnet_single), (None, 0))(params, x2)\n    jac2 = [j.flatten(2) for j in jac2]\n    \n    # Compute J(x1) @ J(x2).T\n    einsum_expr = None\n    if compute == 'full':\n        einsum_expr = 'Naf,Mbf->NMab'\n    elif compute == 'trace':\n        einsum_expr = 'Naf,Maf->NM'\n    elif compute == 'diagonal':\n        einsum_expr = 'Naf,Maf->NMa'\n    else:\n        assert False\n        \n    result = torch.stack([torch.einsum(einsum_expr, j1, j2) for j1, j2 in zip(jac1, jac2)])\n    result = result.sum(0)\n    return result\n```\n\n----------------------------------------\n\nTITLE: Old API for Constants in TorchScript\nDESCRIPTION: Shows the older approach for defining constants in TorchScript modules using the __constants__ class variable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass MyModule(torch.jit.ScriptModule):\n    __constants__ = ['my_constant']\n\n    def __init__(self):\n        super().__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\nm = MyModule()\n```\n\n----------------------------------------\n\nTITLE: Manual Gradient Layout Control Example - Python\nDESCRIPTION: Example demonstrating how to reset gradients to None before accumulation phase in training loop. This approach can improve performance for some networks by allowing autograd to recreate gradients with optimal layout.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/autograd.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfor iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n```\n\n----------------------------------------\n\nTITLE: Gradient Accumulation with torch.amp and GradScaler (Python)\nDESCRIPTION: This snippet demonstrates using AMP with gradient accumulation across multiple batches. Each mini-batch performs forward/backward passes with scaled gradients, but updates (step, scale update, and zero_grad) are only performed every iters_to_accumulate batches. This pattern is critical for consistent scaling and step granularity during accumulation. Requires PyTorch, CUDA device, and established model and data iteration with accumulation logic.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for i, (input, target) in enumerate(data):\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n            loss = loss / iters_to_accumulate\n\n        # Accumulates scaled gradients.\n        scaler.scale(loss).backward()\n\n        if (i + 1) % iters_to_accumulate == 0:\n            # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n```\n\n----------------------------------------\n\nTITLE: PyTorch NLL Loss Operations\nDESCRIPTION: This snippet shows NLL loss forward and backward operations. The forward operation calculates loss between predicted logits [128, 1000] and target labels [128], while the backward operation computes gradients for the loss function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Matrix Multiplication for Attention Mechanisms\nDESCRIPTION: Shows batch matrix multiplication operations for computing attention scores and weighted values in transformer attention heads. These operations handle multiple attention heads simultaneously, with most inputs having dimensions [32, 576, 48] or similar.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 36, ((T([32, 576, 48], f16), T([32, 48, 576], f16)), {})\ncnt: 36, ((T([32, 576, 576], f16), T([32, 576, 48], f16)), {})\ncnt: 2, ((T([32, 1, 48], f16), T([32, 48, 577], f16)), {})\ncnt: 2, ((T([32, 1, 577], f16), T([32, 577, 48], f16)), {})\ncnt: 2, ((T([32, 577, 1], f16), T([32, 1, 48], f16)), {})\ncnt: 2, ((T([32, 1, 48], f16), T([32, 48, 577], f16, stride=(27696, 1, 48))), {})\ncnt: 2, ((T([32, 48, 1], f16), T([32, 1, 577], f16)), {})\ncnt: 2, ((T([32, 1, 577], f16), T([32, 577, 48], f16, stride=(27696, 1, 577))), {})\ncnt: 36, ((T([32, 576, 576], f16, stride=(331776, 1, 576)), T([32, 576, 48], f16)), {})\ncnt: 36, ((T([32, 576, 48], f16), T([32, 48, 576], f16, stride=(27648, 1, 48))), {})\ncnt: 36, ((T([32, 48, 576], f16, stride=(27648, 1, 48)), T([32, 576, 576], f16)), {})\ncnt: 36, ((T([32, 576, 576], f16), T([32, 576, 48], f16, stride=(27648, 1, 576))), {})\n```\n\n----------------------------------------\n\nTITLE: Compiling PyTorch Model for Inference\nDESCRIPTION: Example demonstrating how to apply torch.compile to a model for inference optimization in a training loop\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# inference\nmodel = ...\nopt_model = torch.compile(model)\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = opt_model(inp)\n```\n\n----------------------------------------\n\nTITLE: Marking Unused Functions in TorchScript with @torch.jit.unused Decorator\nDESCRIPTION: Decorator that indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. Allows exporting models with non-TorchScript compatible code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.unused\n```\n\n----------------------------------------\n\nTITLE: Using allow_in_graph for Unsupported Functions\nDESCRIPTION: Example of using torch._dynamo.allow_in_graph as an escape hatch for functions that don't work directly with torch.compile but can be symbolically traced.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef f(x):\n    return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x)\n\nx = torch.randn(2, 3)\nf(x)\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorch DataLoader with Various Options\nDESCRIPTION: Constructor signature for DataLoader class, showing all available parameters for configuring data loading behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)\n```\n\n----------------------------------------\n\nTITLE: Basic Optimization Loop with PyTorch\nDESCRIPTION: Standard training loop showing how to use an optimizer with backward pass. The loop zeros gradients, computes the forward pass, calculates loss, performs backpropagation, and updates parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Applying FSDP2 to a PyTorch Module\nDESCRIPTION: Demonstrates how to use the fully_shard function to apply Fully Sharded Data Parallelism to a PyTorch module. This function dynamically constructs a new class that subclasses the original module type and FSDPModule.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.fsdp.fully_shard.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfully_shard(module)\n```\n\n----------------------------------------\n\nTITLE: Transforming Modules using PyTorch FX in Python\nDESCRIPTION: This function illustrates the process of transforming a neural network module in PyTorch FX by acquiring its graph, modifying it, and returning a new module. Dependencies include the 'torch' and 'torch.fx' packages. The main parameter is 'm', a torch.nn.Module, and the function returns a modified module with the same interface. This process allows for custom tracing behaviors and is constrained by input and output types being torch.nn.Module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)\n```\n\n----------------------------------------\n\nTITLE: Implementing and Training a Neural Network with PyTorch C++\nDESCRIPTION: This code demonstrates how to define a neural network, create a data loader for the MNIST dataset, and train the network using SGD optimization. It showcases key components of the PyTorch C++ frontend including tensor operations, modules, optimizers, and data loading.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/frontend.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/torch.h>\n\n// Define a new Module.\nstruct Net : torch::nn::Module {\n  Net() {\n    // Construct and register two Linear submodules.\n    fc1 = register_module(\"fc1\", torch::nn::Linear(784, 64));\n    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));\n    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));\n  }\n\n  // Implement the Net's algorithm.\n  torch::Tensor forward(torch::Tensor x) {\n    // Use one of many tensor manipulation functions.\n    x = torch::relu(fc1->forward(x.reshape({x.size(0), 784})));\n    x = torch::dropout(x, /*p=*/0.5, /*train=*/is_training());\n    x = torch::relu(fc2->forward(x));\n    x = torch::log_softmax(fc3->forward(x), /*dim=*/1);\n    return x;\n  }\n\n  // Use one of many \"standard library\" modules.\n  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};\n};\n\nint main() {\n  // Create a new Net.\n  auto net = std::make_shared<Net>();\n\n  // Create a multi-threaded data loader for the MNIST dataset.\n  auto data_loader = torch::data::make_data_loader(\n      torch::data::datasets::MNIST(\"./data\").map(\n          torch::data::transforms::Stack<>()),\n      /*batch_size=*/64);\n\n  // Instantiate an SGD optimization algorithm to update our Net's parameters.\n  torch::optim::SGD optimizer(net->parameters(), /*lr=*/0.01);\n\n  for (size_t epoch = 1; epoch <= 10; ++epoch) {\n    size_t batch_index = 0;\n    // Iterate the data loader to yield batches from the dataset.\n    for (auto& batch : *data_loader) {\n      // Reset gradients.\n      optimizer.zero_grad();\n      // Execute the model on the input data.\n      torch::Tensor prediction = net->forward(batch.data);\n      // Compute a loss value to judge the prediction of our model.\n      torch::Tensor loss = torch::nll_loss(prediction, batch.target);\n      // Compute gradients of the loss w.r.t. the parameters of our model.\n      loss.backward();\n      // Update the parameters based on the calculated gradients.\n      optimizer.step();\n      // Output the loss and checkpoint every 100 batches.\n      if (++batch_index % 100 == 0) {\n        std::cout << \"Epoch: \" << epoch << \" | Batch: \" << batch_index\n                  << \" | Loss: \" << loss.item<float>() << std::endl;\n        // Serialize your model periodically as a checkpoint.\n        torch::save(net, \"net.pt\");\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Preserving Tensor View Relationships When Saving - PyTorch - Python\nDESCRIPTION: Illustrates that torch.save preserves view relationships between tensors when multiple related tensors are saved together. If tensors share underlying storage (e.g., views of the same tensor), loading them reconstructs the sharing. Requires torch. Two tensors (a base tensor and a strided view) are saved as a list; after loading, modifying the view also changes the base tensor, indicating correct storage sharing is preserved. Inputs are the original tensor and its view; output is a list with restored relationship; caveats arise with storage size (discussed later).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> numbers = torch.arange(1, 10)\n>>> evens = numbers[1::2]\n>>> torch.save([numbers, evens], 'tensors.pt')\n>>> loaded_numbers, loaded_evens = torch.load('tensors.pt')\n>>> loaded_evens *= 2\n>>> loaded_numbers\ntensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])\n```\n\n----------------------------------------\n\nTITLE: Basic RPC Model Computation with PyTorch\nDESCRIPTION: Example of a simple distributed model using PyTorch's RPC framework where computation is split across two worker nodes. This demonstrates the basic pattern that distributed autograd needs to support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed.rpc as rpc\n\ndef my_add(t1, t2):\n  return torch.add(t1, t2)\n\n# On worker 0:\nt1 = torch.rand((3, 3), requires_grad=True)\nt2 = torch.rand((3, 3), requires_grad=True)\n\n# Perform some computation remotely.\nt3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2))\n\n# Perform some computation locally based on remote result.\nt4 = torch.rand((3, 3), requires_grad=True)\nt5 = torch.mul(t3, t4)\n\n# Compute some loss.\nloss = t5.sum()\n```\n\n----------------------------------------\n\nTITLE: Calculating Standard Batch Gradient in PyTorch\nDESCRIPTION: Demonstrates the standard PyTorch workflow for calculating gradients averaged over a mini-batch. It instantiates the `SimpleCNN` model, moves it to the specified device, performs a forward pass with the entire batch of `data`, calculates the loss using the defined `loss_fn`, and then calls `loss.backward()` to compute gradients for all model parameters, averaged across the batch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = SimpleCNN().to(device=device)\npredictions = model(data) # move the entire mini-batch through the model\n\nloss = loss_fn(predictions, targets)\nloss.backward() # back propogate the 'average' gradient of this mini-batch\n```\n\n----------------------------------------\n\nTITLE: Training ResNet50 on CIFAR10 with Automatic Mixed Precision on XPU\nDESCRIPTION: Sets up and trains a ResNet50 model on the CIFAR10 dataset using automatic mixed precision (AMP) with GradScaler targeting an Intel XPU device. The code includes data preparation, model initialization, and a training loop with periodic loss reporting, ending with model checkpoint saving.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nuse_amp=True\n\ntransform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize((224, 224)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\ntrain_dataset = torchvision.datasets.CIFAR10(\n    root=DATA,\n    train=True,\n    transform=transform,\n    download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)\ntrain_len = len(train_loader)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\nscaler = torch.amp.GradScaler(device=\"xpu\", enabled=use_amp)\n\nmodel.train()\nmodel = model.to(\"xpu\")\ncriterion = criterion.to(\"xpu\")\n\nprint(f\"Initiating training\")\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    # set dtype=torch.bfloat16 for BF16\n    with torch.autocast(device_type=\"xpu\", dtype=torch.float16, enabled=use_amp):\n        output = model(data)\n        loss = criterion(output, target)\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad()\n    if (batch_idx + 1) % 10 == 0:\n         iteration_loss = loss.item()\n         print(f\"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}\")\n\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    },\n    \"checkpoint.pth\",\n)\n\nprint(\"Execution finished\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Data with PyTorch DataPipes\nDESCRIPTION: Demonstrates the use of the filter() method to selectively include or exclude data based on a condition. Shows filtering at different nesting levels and handling of empty batches.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).filter(lambda x: x % 2 == 0)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10)\ndp = dp.batch(3).filter(lambda x: len(x) > 2)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10)\ndp = dp.batch(3).filter(lambda x: x > 4, nesting_level = 1)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10)\ndp = dp.batch(3).filter(lambda x: x > 4, nesting_level = -1, drop_empty_batches = False)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(20)\ndp = dp.batch(3).batch(2).batch(2).filter(lambda x: x < 4 or x > 9 , nesting_level = -1, drop_empty_batches = False)\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Element-wise Addition Operations in Transformer Network\nDESCRIPTION: Records tensor addition operations used for residual connections and bias addition in transformer blocks. These operations have varying tensor shapes with the most common being [2, 576, 768] representing batch size, sequence length, and embedding dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([2, 576, 768], f16, stride=(442368, 1, 576)), T([1, 576, 768], f16)), {})\ncnt: 72, ((T([2, 576, 576, 16], f16), T([16], f16)), {})\ncnt: 72, ((T([2, 576, 768], f16, stride=(442368, 1, 576)), T([2, 576, 768], f16)), {})\ncnt: 1, ((T([2, 1, 768], f16, stride=(0, 768, 1)), T([2, 1, 768], f16)), {})\ncnt: 4, ((T([2, 1, 768], f16), T([2, 1, 768], f16)), {})\ncnt: 1, ((T([2, 1, 768], f16, stride=(443136, 768, 1)), T([2, 1, 768], f16)), {})\ncnt: 4, ((T([2, 577, 768], f16), T([2, 577, 768], f16)), {})\ncnt: 2, ((T([2, 1, 768], f16), T([2, 1, 768], f16, stride=(443136, 768, 1))), {})\ncnt: 1, ((T([2, 576, 768], f16, stride=(443136, 768, 1)), T([2, 576, 768], f16, stride=(443136, 768, 1))), {})\ncnt: 1, ((T([2, 576, 768], f16), T([2, 576, 768], f16, stride=(443136, 768, 1))), {})\ncnt: 72, ((T([2, 576, 768], f16), T([2, 576, 768], f16)), {})\ncnt: 72, ((T([3, 2, 16, 576, 48], f16), T([3, 2, 16, 576, 48], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Loss Calculation and Backpropagation in PyTorch\nDESCRIPTION: This snippet demonstrates the calculation of Negative Log Likelihood (NLL) loss and its backward pass in PyTorch. It shows the use of 16-bit float tensors and integer labels for loss computation in a classification task.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Saving Module State Dict and Loading State Into New Module - PyTorch - Python\nDESCRIPTION: Provides a concise example of saving a module's state_dict to disk and using torch.load and load_state_dict to apply the saved state to a new, freshly constructed module. This practice is essential for transferring learned parameters to models with identical architecture. Inputs are the module and filename; outputs are a state_dict loaded from disk and a module whose weights now match the previously saved state. Works only when module architectures align.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.save(bn.state_dict(), 'bn.pt')\n>>> bn_state_dict = torch.load('bn.pt')\n>>> new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> new_bn.load_state_dict(bn_state_dict)\n<All keys matched successfully>\n```\n\n----------------------------------------\n\nTITLE: Defining Custom OpInfo for PyTorch Operator Testing\nDESCRIPTION: Example of creating a custom OpInfo for testing a PyTorch operator (ops.aten.slice_scatter). Includes specifying dtypes, sample inputs, and other test parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nopinfo_core.OpInfo(\n    \"ops.aten.slice_scatter\",\n    aten_name=\"slice_scatter\",\n    dtypes=common_dtype.all_types_and(torch.bfloat16, torch.half, torch.bool),\n    sample_inputs_func=sample_inputs_slice_scatter,\n    supports_out=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Sparse to Dense for Operations in PyTorch\nDESCRIPTION: Shows how to handle unsupported operations by converting to dense format first.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nb_s.to_dense().cos()\n```\n\n----------------------------------------\n\nTITLE: Constructing Sparse COO Tensor in PyTorch\nDESCRIPTION: Creates a sparse COO tensor by providing indices and values, demonstrating the difference between COO and dense representations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ni = [[0, 1, 1],\n     [2, 0, 2]]\nv =  [3, 4, 5]\ns = torch.sparse_coo_tensor(i, v, (2, 3))\ns.to_dense()\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization in PyTorch\nDESCRIPTION: The aten.native_layer_norm.default operator executes a layer normalization on a tensor, adapting its data to zero mean, unit variance, crucial for model training stability across batch operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_layer_norm.default\ncnt: 49, ((T([1, 512, 1536], f16), [1536], T([1536], f16), T([1536], f16), 1e-07), {})\n```\n\n----------------------------------------\n\nTITLE: Complete SWA Training Loop Implementation\nDESCRIPTION: Full example of implementing SWA training including model creation, learning rate scheduling, and batch norm updates.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nloader, optimizer, model, loss_fn = ...\nswa_model = torch.optim.swa_utils.AveragedModel(model)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\nswa_start = 160\nswa_scheduler = SWALR(optimizer, swa_lr=0.05)\n\nfor epoch in range(300):\n      for input, target in loader:\n          optimizer.zero_grad()\n          loss_fn(model(input), target).backward()\n          optimizer.step()\n      if epoch > swa_start:\n          swa_model.update_parameters(model)\n          swa_scheduler.step()\n      else:\n          scheduler.step()\n\n# Update bn statistics for the swa_model at the end\ntorch.optim.swa_utils.update_bn(loader, swa_model)\n# Use swa_model to make predictions on test data\npreds = swa_model(test_input)\n```\n\n----------------------------------------\n\nTITLE: Vectorized Ensemble Prediction with vmap (Distinct Minibatch) - functorch/PyTorch - Python\nDESCRIPTION: Applies functorch's vmap to compute predictions for all models in one operation, using their respective stacked parameters, buffers, and a distinct minibatch per model. An assertion checks consistency with the traditional looping approach. Requires functorch, correctly prepared fmodel, params, buffers, and matching data dimensions for minibatches.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import vmap\n\npredictions1_vmap = vmap(fmodel)(params, buffers, minibatches)\n\n# verify the vmap predictions match the \nassert torch.allclose(predictions1_vmap, torch.stack(predictions_diff_minibatch_loop), atol=1e-3, rtol=1e-5)\n```\n\n----------------------------------------\n\nTITLE: Batching Data with PyTorch DataPipes\nDESCRIPTION: Demonstrates the use of the batch() method on a DataPipe to create batches of data. Shows examples of classic batching, dropping incomplete batches, and nested batching.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3, drop_last = True)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(30).batch(3).batch(2)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(30).batch(3).batch(2).batch(10, unbatch_level=-1)\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Registering and Implementing a Custom PyTorch Operator with Fake (Meta) Implementation (Python)\nDESCRIPTION: These snippets define a custom sin operator in the mylib namespace using torch.library.custom_op, with a NumPy-backed implementation for actual tensor data. A fake (meta) implementation is also registered with torch.library.register_fake for tracing with FakeTensor, returning an empty tensor of the correct shape. Dependencies: PyTorch and NumPy are required for the main operator. The primary input x is a Tensor. Outputs are tensors resulting from applying np.sin or torch.empty_like to x. The fake implementation should match the shape and metadata of the main operator, supporting tracing/export and custom op integration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@torch.library.custom_op(\"mylib::sin\", mutates_args=())\ndef sin(x: Tensor) -> Tensor:\n    x_np = x.numpy()\n    y_np = np.sin(x_np)\n    return torch.from_numpy(y_np)\n```\n\nLANGUAGE: python\nCODE:\n```\n@torch.library.register_fake(\"mylib::sin\")\ndef _(x: Tensor) -> Tensor:\n    return torch.empty_like(x)\n```\n\n----------------------------------------\n\nTITLE: Using collections.namedtuple and typing.NamedTuple with TorchScript (Python)\nDESCRIPTION: This snippet shows usage of both typing.NamedTuple and collections.namedtuple to define tuple-like data structures for TorchScript functions. The function inc takes an _AnnotatedNamedTuple and returns a tuple of incremented values. It demonstrates compatibility between annotated and unannotated named tuples in TorchScript. Prerequisites: torch, typing.NamedTuple, collections.namedtuple. Expects appropriate namedtuple input; outputs incremented tuple.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom typing import NamedTuple\nfrom typing import Tuple\nfrom collections import namedtuple\n\n_AnnotatedNamedTuple = NamedTuple('_NamedTupleAnnotated', [('first', int), ('second', int)])\n_UnannotatedNamedTuple = namedtuple('_NamedTupleAnnotated', ['first', 'second'])\n\ndef inc(x: _AnnotatedNamedTuple) -> Tuple[int, int]:\n    return (x.first+1, x.second+1)\n\nm = torch.jit.script(inc)\nprint(inc(_UnannotatedNamedTuple(1,2)))\n```\n\n----------------------------------------\n\nTITLE: Loss Function Computation for Model Training\nDESCRIPTION: Shows the negative log-likelihood loss calculation used for training the classification model. This operation takes logits of shape [2, 1000] and target labels of shape [2], ignoring entries with value -100.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([2, 1000], f16), T([2], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([2, 1000], f16), T([2], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Cloning Tensors with PyTorch\nDESCRIPTION: This example shows 'aten.clone.default', a PyTorch operation to create a copy of a tensor. It's often used to ensure tensor manipulations do not alter the original data. It supports cloning tensors with various dimensions prevalent in model layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([96, 3, 224, 224], f16),), {})\ncnt: 2, ((T([96, 32, 112, 112], f16),), {})\ncnt: 1, ((T([96, 96, 112, 112], f16),), {})\ncnt: 1, ((T([96, 96, 56, 56], f16),), {})\ncnt: 3, ((T([96, 144, 56, 56], f16),), {})\ncnt: 1, ((T([96, 144, 28, 28], f16),), {})\ncnt: 5, ((T([96, 192, 28, 28], f16),), {})\ncnt: 1, ((T([96, 192, 14, 14], f16),), {})\ncnt: 8, ((T([96, 384, 14, 14], f16),), {})\ncnt: 5, ((T([96, 576, 14, 14], f16),), {})\ncnt: 1, ((T([96, 576, 7, 7], f16),), {})\ncnt: 6, ((T([96, 960, 7, 7], f16),), {})\ncnt: 1, ((T([96, 1280, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Concatenation Operations in PyTorch (28x28 Feature Maps)\nDESCRIPTION: PyTorch's tensor concatenation operations for 28x28 feature maps along dimension 1 (channel dimension). These concatenations demonstrate the dense connectivity pattern of DenseNet, where each layer receives feature maps from all preceding layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, (([T([64, 128, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 128, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16), T([64, 32, 28, 28], f16)], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Freezing Compiled Functions in PyTorch\nDESCRIPTION: Example showing how to freeze compiled functions to prevent recompilation in production environments using torch.dynamo.run()\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrozen_toy_example = dynamo.run(toy_example)\nfrozen_toy_example(torch.randn(10), torch.randn(10))\n```\n\n----------------------------------------\n\nTITLE: Correct Use of In-Place Operations under vmap - PyTorch - Python\nDESCRIPTION: This snippet gives the correct pattern for using in-place operations under a vmap transform. Here, both tensors input to 'f' are batched, ensuring matching shapes for in-place modification. The code verifies correctness by comparing to expected output. Dependencies: PyTorch, torch.func. All tensors are batched consistently. Use this approach rather than applying in-place ops to non-batched tensors in vmap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(3, 1)\ny = torch.randn(3, 1)\nexpected = x + y\n\n# Does not raise an error because x is being vmapped over.\nvmap(f, in_dims=(0, 0))(x, y)\nassert torch.allclose(x, expected)\n```\n\n----------------------------------------\n\nTITLE: Computing Hessians using Composed Jacobian Transforms in PyTorch\nDESCRIPTION: Shows how to compute Hessians by composing jacrev with itself or jacfwd.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return x.sin().sum()\n\nx = torch.randn(5)\nhessian0 = jacrev(jacrev(f))(x)\nhessian1 = jacfwd(jacrev(f))(x)\n```\n\n----------------------------------------\n\nTITLE: Defining and Printing Graphs in PyTorch FX in Python\nDESCRIPTION: This snippet shows how to define a PyTorch nn.Module, trace it to acquire its graph, and print the graph's tabular structure. It relies on 'torch' and 'torch.fx', and inputs an instance of a PyTorch module without any constraints or transformations. This operation returns a printed representation of the graph with node details, aiding in graph analysis and debugging.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()\n```\n\n----------------------------------------\n\nTITLE: Defining Stateless Loss Computation Function for functorch\nDESCRIPTION: Defines a function `compute_loss_stateless_model` compatible with `functorch` transforms. It accepts the functional model's `params`, `buffers`, a single input `sample`, and a single `target`. Inside, it unsqueezes the sample and target to create a mini-batch of size 1 (as required by the original model structure), performs the forward pass using the stateless `fmodel`, and calculates the loss using the predefined `loss_fn`. The function returns the scalar loss value for the single sample.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef compute_loss_stateless_model (params, buffers, sample, target):\n    batch = sample.unsqueeze(0)\n    targets = target.unsqueeze(0)\n\n    predictions = fmodel(params, buffers, batch) \n    loss = loss_fn(predictions, targets)\n    return loss\n```\n\n----------------------------------------\n\nTITLE: Asynchronous All-Reduce with CUDA Streams - PyTorch (Python)\nDESCRIPTION: Demonstrates how to perform an asynchronous all_reduce operation in a distributed process group using CUDA tensors and streams in PyTorch. It initializes the process group with the 'nccl' backend, prepares a CUDA tensor on the local rank, creates a new CUDA stream, and kicks off an asynchronous all_reduce. The 'wait()' call ensures the operation is enqueued but does not guarantee completion on different CUDA streams; explicit synchronization may be needed for correct results. Requires 'torch' and 'torch.distributed', and assumes a valid 'rank' variable is defined.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n```\n\n----------------------------------------\n\nTITLE: Exporting a PyTorch Model with Dynamic Shapes\nDESCRIPTION: Example showing how to export a PyTorch model using torch.export API with dynamic shapes specified through the dynamic_shapes argument. The resulting ExportedProgram contains symbolic shapes for inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nexported_program: torch.export.ExportedProgram = export(\n    M(), args=example_args, dynamic_shapes=dynamic_shapes\n)\nprint(exported_program)\n```\n\n----------------------------------------\n\nTITLE: Disabling Gradient Computation in PyTorch\nDESCRIPTION: Demonstrates how to locally disable gradient computation using context managers like torch.no_grad() and torch.set_grad_enabled(). These are useful for temporarily turning off gradients during evaluation or inference.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n>>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n```\n\n----------------------------------------\n\nTITLE: Defining a Dynamic Network using ModuleList and ModuleDict\nDESCRIPTION: Defines a `DynamicNet` module that dynamically creates a variable number of linear layers using `nn.ModuleList` and stores different activation functions in `nn.ModuleDict`. The `forward` method demonstrates how to iterate through the `ModuleList` and select an activation from the `ModuleDict` based on an input argument (`act`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n      x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu')\n```\n\n----------------------------------------\n\nTITLE: Training ResNet50 on CIFAR10 with torch.compile on XPU\nDESCRIPTION: Demonstrates training a ResNet50 model on the CIFAR10 dataset using torch.compile optimization on an Intel XPU device. The example includes data preparation, model setup with compilation for performance acceleration, and a training loop with regular loss reporting.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision\n\nLR = 0.001\nDOWNLOAD = True\nDATA = \"datasets/cifar10/\"\n\ntransform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize((224, 224)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\ntrain_dataset = torchvision.datasets.CIFAR10(\n    root=DATA,\n    train=True,\n    transform=transform,\n    download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)\ntrain_len = len(train_loader)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\nmodel.train()\nmodel = model.to(\"xpu\")\ncriterion = criterion.to(\"xpu\")\nmodel = torch.compile(model)\n\nprint(f\"Initiating training with torch compile\")\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    if (batch_idx + 1) % 10 == 0:\n         iteration_loss = loss.item()\n         print(f\"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}\")\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    },\n    \"checkpoint.pth\",\n)\n\nprint(\"Execution finished\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Quantized Module in PyTorch\nDESCRIPTION: Example demonstrating how to create custom quantized modules using PyTorch's quantization API. Shows implementation of original module, observed module, and quantized module along with necessary conversion functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport torch.ao.nn.quantized as nnq\nfrom torch.ao.quantization import QConfigMapping\nimport torch.ao.quantization.quantize_fx\n\n# original fp32 module to replace\nclass CustomModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 3)\n\n    def forward(self, x):\n        return self.linear(x)\n\n# custom observed module, provided by user\nclass ObservedCustomModule(torch.nn.Module):\n    def __init__(self, linear):\n        super().__init__()\n        self.linear = linear\n\n    def forward(self, x):\n        return self.linear(x)\n\n    @classmethod\n    def from_float(cls, float_module):\n        assert hasattr(float_module, 'qconfig')\n        observed = cls(float_module.linear)\n        observed.qconfig = float_module.qconfig\n        return observed\n\n# custom quantized module, provided by user\nclass StaticQuantCustomModule(torch.nn.Module):\n    def __init__(self, linear):\n        super().__init__()\n        self.linear = linear\n\n    def forward(self, x):\n        return self.linear(x)\n\n    @classmethod\n    def from_observed(cls, observed_module):\n        assert hasattr(observed_module, 'qconfig')\n        assert hasattr(observed_module, 'activation_post_process')\n        observed_module.linear.activation_post_process = \\\n            observed_module.activation_post_process\n        quantized = cls(nnq.Linear.from_float(observed_module.linear))\n        return quantized\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Calling a Custom PyTorch Module\nDESCRIPTION: Demonstrates how to instantiate the previously defined `MyLinear` module with specific input and output feature sizes (4 and 3, respectively). It then shows how to call the module instance with a sample input tensor, invoking its `forward` method to perform the computation. The output tensor includes gradient tracking information (`grad_fn`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nm = MyLinear(4, 3)\nsample_input = torch.randn(4)\nm(sample_input)\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)\n```\n\n----------------------------------------\n\nTITLE: Exporting a Simple PyTorch Module\nDESCRIPTION: This code snippet demonstrates the usage of the torch.export function to export a basic PyTorch module by capturing a fully traced graph. Requires PyTorch and torch.export package. Sample inputs include random tensors which are used to execute and validate the exported program.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.export import export\n\nclass Mod(torch.nn.Module):\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        a = torch.sin(x)\n        b = torch.cos(y)\n        return a + b\n\nexample_args = (torch.randn(10, 10), torch.randn(10, 10))\n\nexported_program: torch.export.ExportedProgram = export(\n    Mod(), args=example_args\n)\nprint(exported_program)\n```\n\n----------------------------------------\n\nTITLE: Defining CNN Model Structure in PyTorch\nDESCRIPTION: Implementation of a simple Convolutional Neural Network (CNN) with 3 conv layers and 1 fully connected layer to demonstrate NTK computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/neural_tangent_kernels.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, (3, 3))\n        self.conv2 = nn.Conv2d(32, 32, (3, 3))\n        self.conv3 = nn.Conv2d(32, 32, (3, 3))\n        self.fc = nn.Linear(21632, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = x.relu()\n        x = self.conv2(x)\n        x = x.relu()\n        x = self.conv3(x)\n        x = x.flatten(1)\n        x = self.fc(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Exporting a Model to ONNX using TorchDynamo in Python\nDESCRIPTION: This Python code snippet demonstrates how to export an MLP model to ONNX format using the TorchDynamo engine. The function torch.onnx.export is used to convert the model with its input into an ONNX graph. The resulting ONNXProgram object allows further optimization and saving of the model. It encapsulates the ONNX graph and can optimize the model graph using onnx_program.optimize().\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n\nclass MLPModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc0 = nn.Linear(8, 8, bias=True)\n        self.fc1 = nn.Linear(8, 4, bias=True)\n        self.fc2 = nn.Linear(4, 2, bias=True)\n        self.fc3 = nn.Linear(2, 2, bias=True)\n\n    def forward(self, tensor_x: torch.Tensor):\n        tensor_x = self.fc0(tensor_x)\n        tensor_x = torch.sigmoid(tensor_x)\n        tensor_x = self.fc1(tensor_x)\n        tensor_x = torch.sigmoid(tensor_x)\n        tensor_x = self.fc2(tensor_x)\n        tensor_x = torch.sigmoid(tensor_x)\n        output = self.fc3(tensor_x)\n        return output\n\nmodel = MLPModel()\ntensor_x = torch.rand((97, 8), dtype=torch.float32)\nonnx_program = torch.onnx.export(model, (tensor_x,), dynamo=True)\n```\n\n----------------------------------------\n\nTITLE: Packaging a TorchScript Module\nDESCRIPTION: Demonstrates how to package a TorchScript model using the same save_pickle and load_pickle APIs as with any other object, including support for TorchScript objects as attributes or submodules.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# save TorchScript just like any other object\nwith PackageExporter(file_name) as e:\n    e.save_pickle(\"res\", \"script_model.pkl\", scripted_model)\n    e.save_pickle(\"res\", \"mixed_model.pkl\", python_model_with_scripted_submodule)\n# load as normal\nimporter = PackageImporter(file_name)\nloaded_script = importer.load_pickle(\"res\", \"script_model.pkl\")\nloaded_mixed = importer.load_pickle(\"res\", \"mixed_model.pkl\")\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients with grad Transform in PyTorch\nDESCRIPTION: Demonstrates using the grad transform to compute first and second-order gradients of trigonometric functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom functorch import grad\nx = torch.randn([])\ncos_x = grad(lambda x: torch.sin(x))(x)\nassert torch.allclose(cos_x, x.cos())\n\n# Second-order gradients\nneg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)\nassert torch.allclose(neg_sin_x, -x.sin())\n```\n\n----------------------------------------\n\nTITLE: Working with Non-Contiguous Tensor Views in PyTorch\nDESCRIPTION: Shows how operations like transpose() create views that might be non-contiguous. Demonstrates checking contiguity with is_contiguous() and converting to a contiguous tensor with contiguous() method when needed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_view.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> base = torch.tensor([[0, 1],[2, 3]])\n>>> base.is_contiguous()\nTrue\n>>> t = base.transpose(0, 1)  # `t` is a view of `base`. No data movement happened here.\n# View tensors might be non-contiguous.\n>>> t.is_contiguous()\nFalse\n# To get a contiguous tensor, call `.contiguous()` to enforce\n# copying data when `t` is not contiguous.\n>>> c = t.contiguous()\n```\n\n----------------------------------------\n\nTITLE: Manipulating Tensor Dimensions\nDESCRIPTION: Demonstrates how to permute and flatten tensor dimensions more expressively than standard approaches, using Tensor.align_to, Tensor.flatten, and Tensor.unflatten for dimension management.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ntensor = torch.randn(2, 2, 2, 2, 2, 2)\nnamed_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n# Move the F (dim 5) and E dimension (dim 4) to the front while keeping\ntensor.permute(5, 4, 0, 1, 2, 3)\nnamed_tensor.align_to('F', 'E', ...)\n\nimgs = torch.randn(32, 3, 128, 128)\nnamed_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n\nflat_imgs = imgs.view(32, -1)\nnamed_flat_imgs = named_imgs.flatten(['C', 'H', 'W'], 'features')\nnamed_flat_imgs.names\n\nunflattened_named_imgs = named_flat_imgs.unflatten('features', [('C', 3), ('H', 128), ('W', 128)])\nunflattened_named_imgs.names\n```\n\n----------------------------------------\n\nTITLE: Generating Dummy Data and Creating Model Ensemble - PyTorch - Python\nDESCRIPTION: Creates dummy image and label tensors mimicking an MNIST-like batch for an ensemble of models, specifying device as CUDA and constructing multiple instances of the SimpleMLP model. Requires a CUDA-enabled GPU and the previously defined SimpleMLP class. Outputs a data tensor of shape [100, 64, 1, 28, 28], a targets tensor, and a list of identical initialized models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndevice = 'cuda'\nnum_models = 10\n\ndata = torch.randn(100, 64, 1, 28, 28, device=device)\ntargets = torch.randint(10, (6400,), device=device)\n\nmodels = [SimpleMLP().to(device) for _ in range(num_models)]\n```\n\n----------------------------------------\n\nTITLE: Direct Graph Manipulation in PyTorch FX using Python\nDESCRIPTION: This snippet demonstrates direct manipulation of a PyTorch FX Graph by replacing torch.add operations with torch.mul within a traced module's graph. Dependencies are 'torch' and 'torch.fx'. The input is a PyTorch module that performs addition, and the output is a module modified to perform multiplication instead. Constraints include using suitable function replacements and ensuring graph consistency via 'graph.lint()'.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with FP32 on Intel GPU\nDESCRIPTION: An example of executing an FP32 inference workflow using ResNet50 model on Intel GPU. The code transfers the model and data to XPU and performs inference within a no-grad context.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\n\nwith torch.no_grad():\n    model(data)\n\nprint(\"Execution finished\")\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Operator Schema with Annotations and Defaults (C++)\nDESCRIPTION: Shows how to explicitly define an operator schema with type annotations, argument names, default values, and return value annotations using TorchScript syntax. This provides more detailed schema information than automatic inference. The C++ kernel signature must match the specified schema.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace {\n    Tensor my_kernel_cpu(const Tensor& a, int64_t b, std::optional<int64_t> c) {...}\n}\n\nstatic auto registry = torch::RegisterOperators()\n   .op(\"my_namespace::my_op(Tensor(a) x, int y = 3, int? z = None) -> Tensor(a|b)\",\n       torch::RegisterOperators::options()\n         .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()));\n```\n\n----------------------------------------\n\nTITLE: Enabling Reduced-Precision Reduction for FP16/BF16 in Scaled Dot Product Attention in PyTorch (Python)\nDESCRIPTION: This snippet enables reduced-precision reductions for Scaled Dot Product Attention (SDPA) when using FP16/BF16 tensors in PyTorch, by toggling a global backend setting. This is useful for improving speed when accuracy trade-offs are acceptable, or when performance is a higher priority. The key method is torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp, which accepts a boolean argument. Requires PyTorch with compatible CUDA hardware.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/numerical_accuracy.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using Basic Tensor Views in PyTorch\nDESCRIPTION: Demonstrates creating a tensor view with t.view() and shows how views share the same underlying data with the base tensor. When a view tensor is modified, the changes are reflected in the base tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_view.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> t = torch.rand(4, 4)\n>>> b = t.view(2, 8)\n>>> t.storage().data_ptr() == b.storage().data_ptr()  # `t` and `b` share the same underlying data.\nTrue\n# Modifying view tensor changes base tensor as well.\n>>> b[0][0] = 3.14\n>>> t[0][0]\ntensor(3.14)\n```\n\n----------------------------------------\n\nTITLE: Accessing Gradient Storage in PyTorch\nDESCRIPTION: Example showing how to access both the data and gradient storage of a tensor that requires gradients. Demonstrates that tensors with gradients maintain two separate storage objects.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/storage.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nt = torch.zeros(3, requires_grad=True)\nt.sum().backward()\nassert list(t.untyped_storage()) == [0] * 12  # the storage of the tensor is just 0s\nassert list(t.grad.untyped_storage()) != [0] * 12  # the storage of the gradient isn't\n```\n\n----------------------------------------\n\nTITLE: Training a PyTorch Neural Network with Optimizer\nDESCRIPTION: Illustrates a basic training loop for a PyTorch neural network, including creating an optimizer, computing loss, and updating parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Create the network (from previous section) and optimizer\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n# Run a sample training loop that \"teaches\" the network\n# to output the constant zero function\nfor _ in range(10000):\n  input = torch.randn(4)\n  output = net(input)\n  loss = torch.abs(output)\n  net.zero_grad()\n  loss.backward()\n  optimizer.step()\n\n# After training, switch the module to eval mode to do inference, compute performance metrics, etc.\n# (see discussion below for a description of training and evaluation modes)\n...\nnet.eval()\n...\n```\n\n----------------------------------------\n\nTITLE: Implementing Post Training Static Quantization in PyTorch\nDESCRIPTION: Shows implementation of static quantization where both weights and activations are quantized to int8. Includes model preparation, calibration, and conversion steps with observer attachment for activation tensor calibration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n# define a floating point model where some layers could be statically quantized\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.ao.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.ao.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\nmodel_fp32 = M()\nmodel_fp32.eval()\nmodel_fp32.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nmodel_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\nmodel_fp32_prepared = torch.ao.quantization.prepare(model_fp32_fused)\n\ninput_fp32 = torch.randn(4, 1, 4, 4)\nmodel_fp32_prepared(input_fp32)\n\nmodel_int8 = torch.ao.quantization.convert(model_fp32_prepared)\nres = model_int8(input_fp32)\n```\n\n----------------------------------------\n\nTITLE: Vectorized Jacobian Computation Using functorch vjp and vmap - Python\nDESCRIPTION: Uses functorch's 'vjp' for reverse-mode vector-Jacobian product and 'vmap' for vectorization to efficiently compute the full Jacobian matrix at once. Applies vjp to the partially-applied predict function and maps over identity vectors. Asserts equivalence with the manual row-by-row method. Requires functorch, predict, weight, bias, x, unit_vectors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import vmap, vjp\n\n_, vjp_fn = vjp(partial(predict, weight, bias), x)\n\nft_jacobian, = vmap(vjp_fn)(unit_vectors)\n\n# lets confirm both methods compute the same result\nassert torch.allclose(ft_jacobian, jacobian)\n```\n\n----------------------------------------\n\nTITLE: Training Model with FP32 on Intel GPU\nDESCRIPTION: Provides an example of training a ResNet50 model on Intel GPU using CIFAR-10 dataset. It includes data preprocessing, model setup, and training loop with step-by-step optimization and feedback.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport torchvision\n\nLR = 0.001\nDOWNLOAD = True\nDATA = \"datasets/cifar10/\"\n\ntransform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize((224, 224)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\ntrain_dataset = torchvision.datasets.CIFAR10(\n    root=DATA,\n    train=True,\n    transform=transform,\n    download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)\ntrain_len = len(train_loader)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\nmodel.train()\nmodel = model.to(\"xpu\")\ncriterion = criterion.to(\"xpu\")\n\nprint(f\"Initiating training\")\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    if (batch_idx + 1) % 10 == 0:\n        iteration_loss = loss.item()\n        print(f\"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}\")\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    },\n    \"checkpoint.pth\",\n)\n\nprint(\"Execution finished\")\n```\n\n----------------------------------------\n\nTITLE: Basic Implementation of a Custom Data Sparsifier in PyTorch\nDESCRIPTION: Example implementation of a custom data sparsifier that zeros out tensor values below a threshold. This sparsifier inherits from BaseDataSparsifier and implements the required update_mask method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass ImplementedDataSparsifier(BaseDataSparsifier):\n    def __init__(self, threshold):\n        super().__init__(threshold=threshold)\n\n    def update_mask(self, name, data, threshold):\n        mask = self.get_mask(name)\n        mask[torch.abs(data) < threshold] = 0.0\n```\n\n----------------------------------------\n\nTITLE: Defining SimpleCNN Model and NLL Loss Function in PyTorch\nDESCRIPTION: Defines a standard PyTorch `nn.Module` implementing a simple Convolutional Neural Network (CNN) named `SimpleCNN` with two convolutional layers and two fully connected layers, using ReLU activation and max pooling. It also defines a `loss_fn` using PyTorch's negative log-likelihood loss (`F.nll_loss`) intended for classification tasks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Here's a simple CNN and loss function:\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.log_softmax(x, dim=1)\n        output = x\n        return output\n\ndef loss_fn(predictions, targets):\n    return F.nll_loss(predictions, targets)\n```\n\n----------------------------------------\n\nTITLE: Attention Related Operations - PyTorch\nDESCRIPTION: Softmax and attention-related tensor operations including forward and backward passes. Shows matrix multiplications with typical transformer attention dimensions (batch=4, heads=12, seq_len=512).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_GPT2_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._softmax.default((T([4, 12, 512, 512], f16), -1, False), {})\naten._softmax_backward_data.default((T([4, 12, 512, 512], f16), T([4, 12, 512, 512], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Module Attributes with Type Annotations in TorchScript\nDESCRIPTION: This snippet illustrates how to define module attributes in TorchScript, including empty lists and dictionaries that require explicit type annotations. It also shows how to use these attributes in the forward method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Dict\n\nclass Foo(nn.Module):\n    # `words` is initialized as an empty list, so its type must be specified\n    words: List[str]\n\n    # The type could potentially be inferred if `a_dict` (below) was not\n    # empty, but this annotation ensures `some_dict` will be made into the\n    # proper type\n    some_dict: Dict[str, int]\n\n    def __init__(self, a_dict):\n        super().__init__()\n        self.words = []\n        self.some_dict = a_dict\n\n        # `int`s can be inferred\n        self.my_int = 10\n\n    def forward(self, input):\n        # type: (str) -> int\n        self.words.append(input)\n        return self.some_dict[input] + self.my_int\n\nf = torch.jit.script(Foo({'hi': 2}))\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations in PyTorch\nDESCRIPTION: This snippet shows matrix multiplication operations used in fully connected layers. These operations take input tensors and multiply them with weight matrices, transforming feature representations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([96, 65], f16), T([65, 512], f16)), {})\ncnt: 1, ((T([65, 96], f16, stride=(1, 65)), T([96, 512], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Computing Hessian using torch.func.hessian in Python\nDESCRIPTION: This snippet shows the usage of the convenience function `torch.func.hessian`. This function simplifies Hessian computation by internally combining forward-mode (`jacfwd`) and reverse-mode (`jacrev`) automatic differentiation. It calculates the Hessian of the function `f` (sum of sines) at the input `x` directly.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.func import hessian\n\ndef f(x):\n    return x.sin().sum()\n\nx = torch.randn(5)\nhess = hessian(f)(x)\n```\n\n----------------------------------------\n\nTITLE: Log Softmax Operations in PyTorch\nDESCRIPTION: Log softmax forward and backward operations on tensors with shape [32, 1000] using float16 precision. These are typically used in classification tasks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convmixer_768_32_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._log_softmax.default((T([32, 1000], f16), 1, False), {})\naten._log_softmax_backward_data.default((T([32, 1000], f16), T([32, 1000], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Inspecting TorchScript IR Graphs Using .graph Attribute (Python)\nDESCRIPTION: Shows how to print and inspect the TorchScript intermediate representation (IR) graph for a scripted function. Requires PyTorch. Uses a loop and conditional in the scripted function 'foo', and prints 'foo.graph' to observe the static single assignment (SSA) IR comprised of ATen and primitive operators. Useful for low-level debugging and performance analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\\ndef foo(len):\\n    # type: (int) -> torch.Tensor\\n    rv = torch.zeros(3, 4)\\n    for i in range(len):\\n        if i < 10:\\n            rv = rv - 1.0\\n        else:\\n            rv = rv + 1.0\\n    return rv\\n\\nprint(foo.graph)\\n\n```\n\n----------------------------------------\n\nTITLE: ReLU Activation in PyTorch\nDESCRIPTION: Applies the Rectified Linear Unit (ReLU) function element-wise to the input tensor, modifying it in-place.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\naten.relu_.default((T([64, 64, 224, 224], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.relu_.default((T([64, 128, 112, 112], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Computing Select Backward Gradients using aten.select_backward - Python\nDESCRIPTION: Documents usage of aten.select_backward.default to compute the gradient for select operations, often used in backward propagation. The operator takes input tensor, expanded shape, dimension, and index, often using f16 tensors with various strides. Dependencies: PyTorch, shape/index dimension compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.select_backward.default\ncnt: 24, ((T([2, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, -1), {})\ncnt: 12, ((T([2, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, -2), {})\ncnt: 12, ((T([2, 12, 192, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 14, 192, 64], 2, -1), {})\ncnt: 24, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, -1), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, -2), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, -3), {})\ncnt: 24, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, 0), {})\ncnt: 12, ((T([2, 12, 192, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 14, 192, 64], 2, -1), {})\ncnt: 24, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, -1), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, -2), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, -3), {})\ncnt: 24, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, 0), {})\ncnt: 24, ((T([2, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, 0), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(49152, 4096, 1, 64)), [2, 12, 16, 64, 64], 2, -1), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(49152, 4096, 1, 64)), [2, 12, 16, 64, 64], 2, 0), {})\ncnt: 12, ((T([2, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, 1), {})\ncnt: 12, ((T([2, 12, 192, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 14, 192, 64], 2, 0), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, 2), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [2, 12, 16, 64, 64], 2, 1), {})\ncnt: 12, ((T([2, 12, 192, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 14, 192, 64], 2, 0), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, 2), {})\ncnt: 12, ((T([2, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [2, 12, 16, 64, 64], 2, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing NN Modules on Meta Device using torch.device Context in Python\nDESCRIPTION: Illustrates initializing a `torch.nn.Module` (specifically `Linear`) on the meta device using the `torch.device('meta')` context manager. This is useful when module initialization doesn't directly accept a device argument, ensuring parameters are created as meta tensors. Requires `torch` and `torch.nn.modules`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> from torch.nn.modules import Linear\n>>> with torch.device('meta'):\n...     print(Linear(20, 30))\n...\nLinear(in_features=20, out_features=30, bias=True)\n```\n\n----------------------------------------\n\nTITLE: Building LibTorch Project with CMake\nDESCRIPTION: Commands to create a build directory, configure CMake with the LibTorch path, and build the project. This is the final step in compiling the LibTorch example application.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nmkdir build\ncd build\ncmake -DCMAKE_PREFIX_PATH=/absolute/path/to/libtorch ..\ncmake --build . --config Release\n```\n\n----------------------------------------\n\nTITLE: Implementing Batched Matrix Multiply with First-Class Dimensions in Python\nDESCRIPTION: This snippet demonstrates how to implement batched matrix multiplication (bmm) using first-class dimensions in PyTorch. It adds a batch dimension to the matrix multiply operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef bmm(A, B):\n    i = dims(1) # note: i here is a different value from i inside mm so it works\n    return mm(A[i], B[i]).order(i)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Sum Operation in PyTorch\nDESCRIPTION: This snippet shows the usage of sum.SymInt operator with a specific tensor shape and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swsl_resnext101_32x16d_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([32, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: ONNX Export Error Message Example\nDESCRIPTION: Example of error message shown when attempting to export an unsupported operator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nRuntimeError: ONNX export failed: Couldn't export operator foo\n```\n\n----------------------------------------\n\nTITLE: Integrating DistributedDataParallel with TorchDynamo in Python\nDESCRIPTION: Demonstrates the recommended way to use `DistributedDataParallel` (DDP) with `torch.compile` (TorchDynamo). The DDP wrapper should be applied to the model *before* compiling it. This allows TorchDynamo to apply `DDPOptimizer` graph-break optimizations based on the DDP bucket configuration, potentially improving performance.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/ddp.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    ddp_model = DDP(model, device_ids=[rank])\n    ddp_model = torch.compile(ddp_model)\n```\n\n----------------------------------------\n\nTITLE: Setting Thread Count for Optimal PyTorch Multiprocessing Performance in Python\nDESCRIPTION: Demonstrates how to set the optimal number of threads for each PyTorch process using torch.set_num_threads(). The formula divides available vCPUs by the number of processes to avoid CPU oversubscription.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/multiprocessing.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntorch.set_num_threads(floor(N/M))\n```\n\n----------------------------------------\n\nTITLE: Creating a Non-Contiguous NJT View in Python\nDESCRIPTION: This snippet demonstrates creating a non-contiguous NJT using `torch.nested.narrow` over an existing dense tensor. It shows how to specify the sequence lengths and use the method to avoid memory allocation and copying.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> padded = torch.randn(3, 5, 4)\n>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)\n>>> nt = torch.nested.narrow(padded, dim=1, start=0, length=seq_lens, layout=torch.jagged)\n>>> nt.shape\ntorch.Size([3, j1, 4])\n>>> nt.is_contiguous()\nFalse\n```\n\n----------------------------------------\n\nTITLE: Documenting torch.testing Module in reStructuredText\nDESCRIPTION: This snippet defines the documentation structure for the torch.testing module in PyTorch. It sets up the module reference and includes autofunction directives for three key functions: assert_close, make_tensor, and assert_allclose.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/testing.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\ntorch.testing\n=============\n\n.. automodule:: torch.testing\n.. currentmodule:: torch.testing\n\n.. autofunction:: assert_close\n.. autofunction:: make_tensor\n.. autofunction:: assert_allclose\n```\n\n----------------------------------------\n\nTITLE: Training Model with Hogwild in PyTorch (Python)\nDESCRIPTION: This snippet demonstrates the setup of parallel training using the Hogwild method in PyTorch. It involves sharing a model's parameters across multiple processes with each process performing updates to shared parameters asynchronously. The shared memory setting is crucial for using the fork method. Dependencies include torch.multiprocessing, and the proper setup of data loaders and optimizers. The expected input is a model with shared memory, and outputs are updates to the model parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/multiprocessing.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n```\n\n----------------------------------------\n\nTITLE: Creating and Using TestModule with TorchScript\nDESCRIPTION: This example demonstrates creating TestModule instances outside TorchScript scope and using them with different input types. It shows how forward() is automatically compiled while __init__() can contain arbitrary Python code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass TestModule(torch.nn.Module):\n    def __init__(self, v):\n        super().__init__()\n        self.x = v\n\n    def forward(self, inc: int):\n        return self.x + inc\n\nm = torch.jit.script(TestModule(1))\nprint(f\"First instance: {m(3)}\")\n\nm = torch.jit.script(TestModule(torch.ones([5])))\nprint(f\"Second instance: {m(3)}\")\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Slice Operations\nDESCRIPTION: Multiple slice operations on tensors with various shapes and dimensions, using half-precision floating point format. Includes operations on 2D, 3D and 4D tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n((T([12, 16], f16), [12, 16], 0, 0, 9223372036854775807, 1), {})\n((T([128, 4, 4, 256], f16), [128, 4, 7, 256], 2, 0, 9223372036854775807, 2), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with Named Tensors\nDESCRIPTION: Shows how matrix multiplication operations handle tensor names during computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.randn(3, 3, names=('N', 'D'))\n>>> y = torch.randn(3, 3, names=('in', 'out'))\n>>> x.mm(y).names\n('N', 'out')\n\n>>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))\n>>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F'))\n>>> torch.matmul(x, y).names\n('A', 'B', 'C', 'F')\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operation Statistics\nDESCRIPTION: Logs of matrix multiplication operations (aten.mm.default) showing tensor shapes and data types, primarily using float16 tensors with varying dimensions\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16), T([1000, 1280], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1280], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Customizing Torchvision Model Normalization with BatchNorm2d (No Running Stats) - PyTorch (Python)\nDESCRIPTION: Initializes a torchvision model with a BatchNorm2d layer that does not track running statistics via the 'norm_layer' parameter, using functools.partial to preset track_running_stats to False. This avoids vmap-related errors when running Functorch. Requires torchvision and functools as dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision\nfrom functools import partial\ntorchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))\n```\n\n----------------------------------------\n\nTITLE: Using torch.func.grad with torch.compile\nDESCRIPTION: Demonstrates compiling a gradient computation using torch.func.grad wrapped in torch.compile for optimization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef wrapper_fn(x):\n    return torch.func.grad(lambda x: x.sin().sum())(x)\n\nx = torch.randn(3, 3, 3)\ngrad_x = torch.compile(wrapper_fn)(x)\n```\n\n----------------------------------------\n\nTITLE: Ensembling Models using torch.func.stack_module_state and vmap in Python\nDESCRIPTION: Presents the PyTorch 2.0+ approach for model ensembling using `torch.func`. A stateless 'meta' device version of the base model is created. `torch.func.stack_module_state` stacks the parameters and buffers from the list of `models` into separate dictionaries. A helper function `call_single_model` wraps `torch.func.functional_call` to apply the `base_model` logic using a single set of parameters/buffers. `torch.vmap` maps this function over the stacked `params` and `buffers`, applying each model instance to the same `data`. Requires `torch`, `copy`, and `torch.func`. This replaces the `functorch.combine_state_for_ensemble` pattern.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nnum_models = 5\nbatch_size = 64\nin_features, out_features = 3, 3\nmodels = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]\ndata = torch.randn(batch_size, 3)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport copy\n\n# Construct a version of the model with no memory by putting the Tensors on\n# the meta device.\nbase_model = copy.deepcopy(models[0])\nbase_model.to('meta')\n\nparams, buffers = torch.func.stack_module_state(models)\n\n# It is possible to vmap directly over torch.func.functional_call,\n# but wrapping it in a function makes it clearer what is going on.\ndef call_single_model(params, buffers, data):\n    return torch.func.functional_call(base_model, (params, buffers), (data,))\n\noutput = torch.vmap(call_single_model, (0, 0, None))(params, buffers, data)\nassert output.shape == (num_models, batch_size, out_features)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating PyTorch Type Promotion with Different Tensor Types\nDESCRIPTION: This code snippet shows examples of PyTorch's type promotion rules when performing arithmetic operations between tensors of different data types. It includes examples with various tensor types including floating point, integer, boolean, and complex tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32\n```\n\n----------------------------------------\n\nTITLE: Tracing Conditional Logic with Dynamo\nDESCRIPTION: Shows how Dynamo handles conditional statements by tracing only the executed path based on the given inputs, demonstrating its input-dependent behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef fn(x, n):\n    y = x ** 2\n    if n >= 0:\n        return (n + 1) * y\n    else:\n        return y / n\n\nx = torch.randn(200)\nfn(x, 2)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef forward(l_x_: torch.Tensor):\n    # File: example.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n    # File: example.py:7, code: return (n + 1) * y\n    mul = 3 * y\n    return (mul,)\n```\n\n----------------------------------------\n\nTITLE: Computing Per-sample Gradients with vmap and grad in PyTorch\nDESCRIPTION: Demonstrates combining vmap and grad transforms to compute per-sample gradients in a simple linear model with MSE loss.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import vmap\nbatch_size, feature_size = 3, 5\n\ndef model(weights,feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\ndef compute_loss(weights, example, target):\n    y = model(weights, example)\n    return ((y - target) ** 2).mean()  # MSELoss\n\nweights = torch.randn(feature_size, requires_grad=True)\nexamples = torch.randn(batch_size, feature_size)\ntargets = torch.randn(batch_size)\ninputs = (weights,examples, targets)\ngrad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using DistributedDataParallel in Python\nDESCRIPTION: Provides a complete example of setting up and using `torch.nn.parallel.DistributedDataParallel` (DDP) for distributed training. It initializes a process group (`gloo` backend), creates a simple linear model, wraps it with DDP, performs a forward and backward pass, and updates parameters using an SGD optimizer. The example uses `torch.multiprocessing.spawn` to launch multiple processes and requires setting `MASTER_ADDR` and `MASTER_PORT` environment variables for the default `env://` initialization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/ddp.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\ndef example(rank, world_size):\n    # create default process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    # create local model\n    model = nn.Linear(10, 10).to(rank)\n    # construct DDP model\n    ddp_model = DDP(model, device_ids=[rank])\n    # define loss function and optimizer\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    # forward pass\n    outputs = ddp_model(torch.randn(20, 10).to(rank))\n    labels = torch.randn(20, 10).to(rank)\n    # backward pass\n    loss_fn(outputs, labels).backward()\n    # update parameters\n    optimizer.step()\n\ndef main():\n    world_size = 2\n    mp.spawn(example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True)\n\nif __name__==\"__main__\":\n    # Environment variables which need to be\n    # set when using c10d's default \"env\"\n    # initialization mode.\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    main()\n```\n\n----------------------------------------\n\nTITLE: Configuring Different Penalization for Bias Parameters in PyTorch\nDESCRIPTION: Example of how to apply different weight decay settings to different parameter groups. This approach prevents bias terms from being penalized by setting their weight_decay to 0.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbias_params = [p for name, p in self.named_parameters() if 'bias' in name]\nothers = [p for name, p in self.named_parameters() if 'bias' not in name]\n\noptim.SGD([\n                {'params': others},\n                {'params': bias_params, 'weight_decay': 0}\n            ], weight_decay=1e-2, lr=1e-2)\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Model for Automatic Splitting in Python\nDESCRIPTION: This code defines a simple model class with an embedding layer, multiple layers in a ModuleList, and a language model head. This model structure is used to demonstrate automatic splitting for pipeline parallelism.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Model(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.emb = torch.nn.Embedding(10, 3)\n        self.layers = torch.nn.ModuleList(\n            Layer() for _ in range(2)\n        )\n        self.lm = LMHead()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.emb(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.lm(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Converting Tensor to Python Number\nDESCRIPTION: Shows how to extract a Python number from a single-value tensor using the item() method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5\n```\n\n----------------------------------------\n\nTITLE: Creating Composable Backends with Fallback Support\nDESCRIPTION: This snippet shows how to create a composable backend that tries multiple backends in sequence with fallback support. It first attempts to use TensorRT, then falls back to Inductor if TensorRT fails, and finally returns the original module if both backends fail.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._dynamo import lookup_backend\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    try:\n        trt_compiled = lookup_backend(\"tensorrt\")(gm, example_inputs)\n        if trt_compiled is not None:\n            return trt_compiled\n    except Exception:\n        pass\n    # first backend failed, try something else...\n    try:\n        inductor_compiled = lookup_backend(\"inductor\")(gm, example_inputs)\n        if inductor_compiled is not None:\n            return inductor_compiled\n    except Exception:\n        pass\n    return gm.forward\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Quantized Models using state_dict in PyTorch\nDESCRIPTION: Demonstrates how to save and load a quantized model using state_dict. This method involves preparing the model, converting it, and then using torch.save and torch.load with BytesIO for serialization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(5, 5)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.relu(x)\n        return x\n\nm = M().eval()\nprepare_orig = prepare_fx(m, {'' : default_qconfig})\nprepare_orig(torch.rand(5, 5))\nquantized_orig = convert_fx(prepare_orig)\n\n# Save/load using state_dict\nb = io.BytesIO()\ntorch.save(quantized_orig.state_dict(), b)\n\nm2 = M().eval()\nprepared = prepare_fx(m2, {'' : default_qconfig})\nquantized = convert_fx(prepared)\nb.seek(0)\nquantized.load_state_dict(torch.load(b))\n```\n\n----------------------------------------\n\nTITLE: Importing Tensor Parallelism Module in Python\nDESCRIPTION: This snippet shows how to import the Tensor Parallelism module in PyTorch. It's the entry point for parallelizing nn.Module using Tensor Parallelism.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.parallel.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.distributed.tensor.parallel import parallelize_module\n```\n\n----------------------------------------\n\nTITLE: Converting a Dense Tensor to CSR Format in PyTorch\nDESCRIPTION: Shows how to convert a dense tensor to a sparse CSR tensor using the to_sparse_csr() method. The example converts a 3x4 dense tensor into CSR format, where zeros are treated as missing values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\na = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)\nsp = a.to_sparse_csr()\nsp\n```\n\n----------------------------------------\n\nTITLE: Using CUDA Graphs with torch.cuda.amp\nDESCRIPTION: This snippet shows how to use CUDA Graphs with torch.cuda.amp for mixed precision training. It includes a warmup phase and a capture phase, demonstrating how to handle the optimizer step outside the graph capture.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\n# warmup\n# In a real setting, use a few batches of real data.\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for i in range(3):\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast():\n            y_pred = model(static_input)\n            loss = loss_fn(y_pred, static_target)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\ntorch.cuda.current_stream().wait_stream(s)\n\n# capture\ng = torch.cuda.CUDAGraph()\noptimizer.zero_grad(set_to_none=True)\nwith torch.cuda.graph(g):\n    with torch.cuda.amp.autocast():\n        static_y_pred = model(static_input)\n        static_loss = loss_fn(static_y_pred, static_target)\n    scaler.scale(static_loss).backward()\n    # don't capture scaler.step(optimizer) or scaler.update()\n\nreal_inputs = [torch.rand_like(static_input) for _ in range(10)]\nreal_targets = [torch.rand_like(static_target) for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    static_input.copy_(data)\n    static_target.copy_(target)\n    g.replay()\n    # Runs scaler.step and scaler.update eagerly\n    scaler.step(optimizer)\n    scaler.update()\n```\n\n----------------------------------------\n\nTITLE: Integrating optimize_for_inference as a TorchDynamo Backend\nDESCRIPTION: This snippet shows how to create a custom TorchDynamo backend that uses torch.jit.optimize_for_inference for improved performance. It takes a GraphModule and example inputs, scripts the module, and then applies optimization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef optimize_for_inference_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    scripted = torch.jit.script(gm)\n    return torch.jit.optimize_for_inference(scripted)\n```\n\n----------------------------------------\n\nTITLE: Diagnosing Graph Breaks with torch._dynamo.explain\nDESCRIPTION: Example showing how to use dynamo.explain() to identify and analyze graph breaks in a PyTorch function. The explanation output includes graph count, break reasons, and operation details.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch._dynamo as dynamo\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    print(\"woo\")\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nexplanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))\nprint(explanation)\n```\n\n----------------------------------------\n\nTITLE: Installing Linux-Specific Dependencies and Triton - bash\nDESCRIPTION: This snippet installs Linux-specific dependencies for PyTorch builds, including MKL, GPU LAPACK support (magma-cuda), and optionally Triton for 'torch.compile' (inductor/triton). It illustrates both pip and conda-based installation. Triton is necessary when using certain features with Intel GPUs. Execute from the PyTorch directory. Ensure conda and pip are set up appropriately.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install mkl-static mkl-include\\n# CUDA only: Add LAPACK support for the GPU if needed\\nconda install -c pytorch magma-cuda121  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo\\n\\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\\n# Run from the pytorch directory after cloning\\n# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.\\nmake triton\n```\n\n----------------------------------------\n\nTITLE: vmap with torch.nonzero Causing Dynamic Shape Error - PyTorch - Python\nDESCRIPTION: This snippet illustrates the failure mode that occurs when vmap is used with PyTorch operations returning dynamic shapes (e.g., torch.nonzero). Since each 'example' may yield a tensor of a different shape, vmap cannot stack the results, leading to an error. Dependencies: PyTorch, torch.func. Limitation: Avoid using ops that may emit ragged or variably-shaped outputs inside vmap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nxs = torch.tensor([[0, 1, 2], [0, 0, 3]])\nvmap(torch.nonzero)(xs)\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorPipe RPC Backend in PyTorch\nDESCRIPTION: Example demonstrating how to initialize the TensorPipe RPC backend in PyTorch with custom options like worker threads and timeout settings. This is used for setting up distributed training environments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import os\n>>> from torch.distributed import rpc\n>>> os.environ['MASTER_ADDR'] = 'localhost'\n>>> os.environ['MASTER_PORT'] = '29500'\n>>>\n>>> rpc.init_rpc(\n>>>     \"worker1\",\n>>>     rank=0,\n>>>     world_size=2,\n>>>     rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n>>>         num_worker_threads=8,\n>>>         rpc_timeout=20 # 20 second timeout\n>>>     )\n>>> )\n>>>\n>>> # omitting init_rpc invocation on worker2\n```\n\n----------------------------------------\n\nTITLE: Python3-style Type Annotation in TorchScript\nDESCRIPTION: Example of Python3-style type annotation in TorchScript where parameter 'b' is explicitly typed as int while 'a' uses the default TensorType. The return type is inferred automatically based on the returned value.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef f(a, b: int):\n    return a+b\n\nm = torch.jit.script(f)\nprint(\"TorchScript:\", m(torch.ones([6]), 100))\n```\n\n----------------------------------------\n\nTITLE: Calling NumPy in torch.autograd.Function for Composable torch.func Transforms (Python)\nDESCRIPTION: Implements a torch.autograd.Function where forward and backward operations use NumPy for computation and are compatible with PyTorch's torch.func transforms due to proper implementation of forward, setup_context, and transformable backward methods. NumpySort saves intermediate indices as outputs for later use in backward, which relies on a helper NumpyTake function, itself implemented as a torch.autograd.Function. Dependencies: torch, numpy. Inputs: tensors and dimension integer. Outputs: sorted tensor and index intermediates. Limitations: assumes CPU compatibility and that inputs/outputs are convertible between torch and numpy as needed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\n\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    # Note that forward does not take ctx\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        # Any intermediates to be saved in backward must be returned as\n        # outputs.\n        return (\n            # The desired output\n            torch.tensor(result, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind_inv, device=device),\n        )\n\n    # setup_context is responsible for calling methods and/or assigning to\n    # the ctx object. Please do not do additional compute (e.g. add\n    # Tensors together) in setup_context.\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        # Note that output is whatever you returned from forward.\n        # If you returned multiple values, then output is a Tuple of multiple values.\n        # If you returned a single Tensor, then output is a Tensor.\n        # If you returned a Tuple with a single Tensor, then output is a\n        # Tuple with a single Tensor.\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        # Tensors must be saved via ctx.save_for_backward. Please do not\n        # assign them directly onto the ctx object.\n        ctx.save_for_backward(ind, ind_inv)\n        # Non-tensors may be saved by assigning them as attributes on the ctx object.\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        # For the autograd.Function to be arbitrarily composable with function\n        # transforms, all staticmethod other than forward and setup_context\n        # must be implemented in a \"transformable\" way; that is, they must\n        # only consist of PyTorch operations or autograd.Function.\n        #\n        # For example, this allows us to do double backwards and/or compute\n        # second order gradients.\n        #\n        # We've written the backward pass of NumpySort in terms of another\n        # autograd.Function, NumpyTake.\n        ind, ind_inv = ctx.saved_tensors\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None\n\n```\n\nLANGUAGE: python\nCODE:\n```\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result\n```\n\nLANGUAGE: python\nCODE:\n```\nx = torch.randn(2, 3)\ngrad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)\nassert torch.allclose(grad_x, torch.ones_like(x))\n```\n\n----------------------------------------\n\nTITLE: Invoking Tensor Addition with aten.add.Tensor - PyTorch - Python\nDESCRIPTION: These snippets describe tensor-tensor addition using aten.add.Tensor for inputs of shapes such as [4, 1024, 768] or [4, 1024, 3072] in f16. Both broadcasting and elementwise operations are covered. The resulting tensor matches the broadcasted shape. Dependencies: torch, half-precision support. Consider input dimensionality and broadcasting semantics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([4, 1024, 768], f16), T([1, 1024, 768], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 48, ((T([4, 1024, 768], f16), T([4, 1024, 768], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 36, ((T([4, 1024, 3072], f16), T([4, 1024, 3072], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 12, ((T([4, 1024, 3072], f16), 1.0), {})\n```\n\n----------------------------------------\n\nTITLE: Exporting a PyTorch Model with Conv2D and ReLU\nDESCRIPTION: Illustrates exporting a PyTorch model with Conv2D and ReLU using torch.export. Dependencies include PyTorch and torch.export. Inputs are sample tensors mimicking a 256x256 image with 3 channels. This provides a functional graph devoid of inplace operations, utilizing the aten operators.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.export import export\n\n# Simple module for demonstration\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels=3, out_channels=16, kernel_size=3, padding=1\n        )\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3)\n\n    def forward(self, x: torch.Tensor, *, constant=None) -> torch.Tensor:\n        a = self.conv(x)\n        a.add_(constant)\n        return self.maxpool(self.relu(a))\n\nexample_args = (torch.randn(1, 3, 256, 256),)\nexample_kwargs = {\"constant\": torch.ones(1, 16, 256, 256)}\n\nexported_program: torch.export.ExportedProgram = export(\n    M(), args=example_args, kwargs=example_kwargs\n)\nprint(exported_program)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with aten.mm in Python\nDESCRIPTION: The aten.mm.default operator is used for matrix multiplication between two 2D tensors. This is a fundamental operation in linear algebra applications, including machine learning models that require matrix transformations based on input and weight matrices. Compatibility in dimensions and data types (f16) is necessary for successful execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([256, 197951], f16), T([197951, 512], f16)), {})\ncnt: 1, ((T([197951, 256], f16, stride=(1, 197951)), T([256, 512], f16)), {})\ncnt: 2, ((T([256, 512], f16), T([512, 512], f16)), {})\ncnt: 2, ((T([512, 256], f16, stride=(1, 512)), T([256, 512], f16)), {})\ncnt: 1, ((T([256, 512], f16), T([512, 1024], f16)), {})\ncnt: 1, ((T([512, 256], f16, stride=(1, 512)), T([256, 1024], f16)), {})\ncnt: 1, ((T([256, 1024], f16), T([1024, 512], f16)), {})\ncnt: 1, ((T([1024, 256], f16, stride=(1, 1024)), T([256, 512], f16)), {})\ncnt: 1, ((T([512, 256], f16, stride=(1, 512)), T([256, 197951], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model in Eval Mode for vmap Compatibility - PyTorch (Python)\nDESCRIPTION: Runs a model in evaluation mode (model.eval()), which disables updates to running_mean and running_var, thereby enabling safe use of vmap with BatchNorm layers. This usage pattern is important for scenarios where inference or deterministic statistics are necessary. After evaluation, the model can be set back to training mode with model.train().\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel.eval()\nvmap(model)(x)\nmodel.train()\n```\n\n----------------------------------------\n\nTITLE: Backward Convolution Operations in PyTorch - Python\nDESCRIPTION: Utilizes aten.convolution_backward.default to compute gradients of input tensors used in convolution operations. Essential for training neural networks with backpropagation. Requires matching input and weight dimensions and returns gradient tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 3, ((T([32, 960, 7, 7], f16), T([32, 160, 7, 7], f16), T([960, 160, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([32, 160, 7, 7], f16), T([32, 960, 7, 7], f16), T([160, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([32, 960, 1, 1], f16), T([32, 240, 1, 1], f16), T([960, 240, 1, 1], f16), [960], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([32, 240, 1, 1], f16), T([32, 960, 1, 1], f16), T([240, 960, 1, 1], f16), [240], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([32, 960, 7, 7], f16), T([32, 960, 7, 7], f16), T([960, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 960, [True, True, False]), {})\ncnt: 1, ((T([32, 160, 7, 7], f16), T([32, 672, 7, 7], f16), T([160, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([32, 672, 1, 1], f16), T([32, 168, 1, 1], f16), T([672, 168, 1, 1], f16), [672], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([32, 168, 1, 1], f16), T([32, 672, 1, 1], f16), T([168, 672, 1, 1], f16), [168], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 2, ((T([32, 672, 14, 14], f16), T([32, 112, 14, 14], f16), T([672, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 112, 14, 14], f16), T([32, 672, 14, 14], f16), T([112, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16), T([672, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 1, ((T([32, 112, 14, 14], f16), T([32, 480, 14, 14], f16), T([112, 480, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 480, 1, 1], f16), T([32, 120, 1, 1], f16), T([480, 120, 1, 1], f16), [480], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 120, 1, 1], f16), T([32, 480, 1, 1], f16), T([120, 480, 1, 1], f16), [120], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16), T([480, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 480, [True, True, False]), {})\ncnt: 1, ((T([32, 480, 14, 14], f16), T([32, 80, 14, 14], f16), T([480, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n\n```\n\n----------------------------------------\n\nTITLE: Hessian Computation with functorch.hessian and Comparison - Python\nDESCRIPTION: Demonstrates use of functorch.hessian to compute second-order derivatives. Reduces matrix size to prevent excessive memory use. Computes the Hessian both via hessian() and via explicit composition jacfwd(jacfwd()), and notes jacrev(jacrev()) as an alternative (commented out). Requires functorch, torch, and initialized variables. Used to compare equivalence and API convenience.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import hessian\n\n# lets reduce the size in order not to blow out colab. Hessians require significant memory:\nDin = 512\nDout = 32\nweight = torch.randn(Dout, Din)\nbias = torch.randn(Dout)\nx = torch.randn(Din)\n\nhess_api = hessian(predict, argnums=2)(weight, bias, x)\nhess_fwdfwd = jacfwd(jacfwd(predict, argnums=2), argnums=2)(weight, bias, x)\n#hess_revrev = jacrev(jacrev(predict, argnums=2), argnums=2)(weight, bias, x)\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Method Export and Compilation in TorchScript Modules\nDESCRIPTION: Demonstrates how to use the @torch.jit.export decorator to explicitly mark methods for compilation. By default, TorchScript recursively compiles methods starting from forward().\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torchvision\n\nclass MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        means = torch.tensor([103.939, 116.779, 123.68])\n        self.means = torch.nn.Parameter(means.resize_(1, 3, 1, 1))\n        resnet = torchvision.models.resnet18()\n        self.resnet = torch.jit.trace(resnet, torch.rand(1, 3, 224, 224))\n\n    def helper(self, input):\n        return self.resnet(input - self.means)\n\n    def forward(self, input):\n        return self.helper(input)\n\n    # Since nothing in the model calls `top_level_method`, the compiler\n    # must be explicitly told to compile this method\n    @torch.jit.export\n    def top_level_method(self, input):\n        return self.other_helper(input)\n\n    def other_helper(self, input):\n        return input + 10\n\n# `my_script_module` will have the compiled methods `forward`, `helper`,\n# `top_level_method`, and `other_helper`\nmy_script_module = torch.jit.script(MyModule())\n```\n\n----------------------------------------\n\nTITLE: Decomposing PyTorch Operations using FX Proxy\nDESCRIPTION: This code demonstrates how to decompose PyTorch operations into smaller constituent operations using FX Proxy. It specifically shows how to transform F.relu(x) into (x > 0) * x by creating decomposition rules and applying them during graph transformation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    tracer = torch.fx.proxy.GraphAppendingTracer(new_graph)\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)\n```\n\n----------------------------------------\n\nTITLE: Using oneDNN Graph with Float32 Data Type\nDESCRIPTION: Example showing how to enable oneDNN Graph fusion globally in PyTorch and use it with a model in Float32 precision. This includes tracing the model and running inference.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/onednn/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# enable oneDNN graph fusion globally\ntorch.jit.enable_onednn_fusion(True)\n\n# define the model\ndef MyModel(torch.nn.Module):\n    ...\n\n# construct the model\nmodel = MyModel()\nwith torch.no_grad():\n    model.eval()\n    model = torch.jit.trace(model, torch.rand(args.batch_size, 3, 224, 224))\n\n# run the model\nwith torch.no_grad():\n    # oneDNN graph fusion will be triggered during runtime\n    output = model(images)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Input Mutation Support in CUDAGraph Trees\nDESCRIPTION: This code example shows how CUDAGraph Trees handle input mutations for different types of inputs, including tensors from eager execution, parameters and buffers, and outputs from previous CUDAGraph Tree operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile(mode=\"reduce-overhead\")\ndef foo(x):\n    return x + 1\n\n@torch.compile(mode=\"reduce-overhead\")\ndef mut(x):\n    return x.add_(2)\n\n# Enable input mutation support\ntorch._inductor.config.triton.cudagraph_support_input_mutation = True\n\nfor i in range(3):\n    torch.compiler.cudagraph_mark_step_begin()\n    inp = torch.rand([4], device=\"cuda\")\n\n    # CUDAGraph is applied since `foo` does not mutate `inp`\n    tmp = foo(inp)\n    # Although `mut` mutates `tmp`, which is an output of a CUDAGraph\n    # managed function. So CUDAGraph is still applied.\n    mut(tmp)\n\n\ntorch.compiler.cudagraph_mark_step_begin()\ninp = torch.rand([4], device=\"cuda\")\n\ntmp = foo(inp)\n# While `tmp` is a CUDAGraph Tree managed function's output, `tmp.clone()`\n# is not. So CUDAGraph is not applied to `mut` and there is a log\n# `skipping cudagraphs due to mutated inputs`\nmut(tmp.clone())\n```\n\n----------------------------------------\n\nTITLE: Dequantizing Tensors in PyTorch\nDESCRIPTION: Methods for converting quantized tensors back to floating point format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquantized_tensor.dequantize()\ntorch.dequantize(x)\n```\n\n----------------------------------------\n\nTITLE: Patching BatchNorm Modules In-Place with Functorch - PyTorch (Python)\nDESCRIPTION: Applies functorch's in-place patch function to replace all BatchNorm modules in a model such that they do not use running statistics. This is achieved with the replace_all_batch_norm_modules_ utility from torch.func. The module 'net' is modified directly to ensure compatibility with vmapped operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.func import replace_all_batch_norm_modules_\nreplace_all_batch_norm_modules_(net)\n```\n\n----------------------------------------\n\nTITLE: Specifying TorchScript Types With Python 3 Type Hints - PyTorch - Python\nDESCRIPTION: Here, standard Python 3 type hints from the typing module are used to annotate the types in a TorchScript function. The function foo is decorated with @torch.jit.script, specifying types for its int and tuple-of-tensor arguments and a tensor return type. This approach is compatible with newer versions of Python and is preferred for clearer type annotations. Dependencies include torch and typing, with the function expecting an int and a tuple of two tensors and returning a tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom typing import Tuple\n\n@torch.jit.script\ndef foo(x: int, tup: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n    t0, t1 = tup\n    return t0 + t1 + x\n\nprint(foo(3, (torch.rand(3), torch.rand(3))))\n```\n\n----------------------------------------\n\nTITLE: MyPy-style Type Annotation in TorchScript\nDESCRIPTION: Example of MyPy-style type annotation in TorchScript using a comment to specify types. All parameters must be annotated in this style, even if they use the default type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef f(a, b):\n    # type: (torch.Tensor, int)  torch.Tensor\n    return a+b\n\nm = torch.jit.script(f)\nprint(\"TorchScript:\", m(torch.ones([6]), 100))\n```\n\n----------------------------------------\n\nTITLE: Illustrating In-place Broadcasting Constraints in Python\nDESCRIPTION: Shows the limitation of in-place operations (like `add_`) regarding broadcasting. The in-place tensor cannot change its shape as a result of the broadcast. The first example succeeds because the shape of `x` matches the broadcasted result shape. The second example fails with a RuntimeError because the broadcast would require changing the shape of `x`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> x=torch.empty(5,3,4,1)\n>>> y=torch.empty(3,1,1)\n>>> (x.add_(y)).size()\ntorch.Size([5, 3, 4, 1])\n\n# but:\n>>> x=torch.empty(1,3,1)\n>>> y=torch.empty(3,1,7)\n>>> (x.add_(y)).size()\nRuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.\n```\n\n----------------------------------------\n\nTITLE: ReLU Activation Operations in PyTorch\nDESCRIPTION: This snippet shows ReLU activation operations applied to feature maps at different stages of the network. ReLU introduces non-linearity by replacing negative values with zero while keeping positive values unchanged.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu.default\ncnt: 1, ((T([96, 64, 64, 64], f16),), {})\ncnt: 4, ((T([96, 64, 32, 32], f16),), {})\ncnt: 4, ((T([96, 128, 16, 16], f16),), {})\ncnt: 4, ((T([96, 256, 8, 8], f16),), {})\ncnt: 4, ((T([96, 512, 4, 4], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Statements in TorchScript\nDESCRIPTION: Specifies the syntax for various simple statements in TorchScript, including expression statements, assignments, augmented assignments, and annotated assignments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nexpression_stmt    ::=  starred_expression\nstarred_expression ::=  expression | (starred_item \",\")* [starred_item]\nstarred_item       ::=  assignment_expression | \"*\" or_expr\n\nassignment_stmt ::=  (target_list \"=\")+ (starred_expression)\ntarget_list     ::=  target (\",\" target)* [\",\"]\ntarget          ::=  identifier\n                     | \"(\" [target_list] \")\"\n                     | \"[\" [target_list] \"]\"\n                     | attributeref\n                     | subscription\n                     | slicing\n                     | \"*\" target\n\naugmented_assignment_stmt ::= augtarget augop (expression_list)\naugtarget                 ::= identifier | attributeref | subscription\naugop                     ::= \"+=\" | \"-=\" | \"*=\" | \"/=\" | \"//=\" | \"%=\" |\n                              \"**=\"| \">>=\" | \"<<=\" | \"&=\" | \"^=\" | \"|=\"\n\nannotated_assignment_stmt ::= augtarget \":\" expression\n                              [\"=\" (starred_expression)]\n```\n\n----------------------------------------\n\nTITLE: Implementing Naive Per-Sample Gradient Calculation in PyTorch\nDESCRIPTION: Defines two functions for calculating per-sample gradients manually. `compute_grad` takes a single sample and target, unsqueezes them to add a batch dimension, computes the loss, and returns the gradients for model parameters using `torch.autograd.grad`. `compute_sample_grads` iterates through the input batch `data` and `targets`, calls `compute_grad` for each pair, and aggregates the gradients for each parameter across all samples using `zip` and `torch.stack`. This approach is functionally correct but inefficient.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef compute_grad(sample, target):\n    \n    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n    target = target.unsqueeze(0)\n\n    prediction = model(sample)\n    loss = loss_fn(prediction, target)\n\n    return torch.autograd.grad(loss, list(model.parameters()))\n\n\ndef compute_sample_grads(data, targets):\n    \"\"\" manually process each sample with per sample gradient \"\"\"\n    sample_grads = [compute_grad(data[i], targets[i]) for i in range(batch_size)]\n    sample_grads = zip(*sample_grads)\n    sample_grads = [torch.stack(shards) for shards in sample_grads]\n    return sample_grads\n\nper_sample_grads = compute_sample_grads(data, targets)\n```\n\n----------------------------------------\n\nTITLE: Optimized MulConstant with Gradient Materialization Control\nDESCRIPTION: Enhanced version of MulConstant that optimizes gradient computation by controlling gradient materialization and handling None gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        tensor, constant = inputs\n        ctx.set_materialize_grads(False)\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Here we must handle None grad_output tensor. In this case we\n        # can skip unnecessary computations and just return None.\n        if grad_output is None:\n            return None, None\n\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None\n```\n\n----------------------------------------\n\nTITLE: Customizing TorchVision Models with Modified BatchNorm in PyTorch\nDESCRIPTION: Configures a torchvision ResNet18 model with a custom norm_layer that doesn't track running statistics, making it compatible with functorch's vmap operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/batch_norm.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision\nfrom functools import partial\ntorchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))\n```\n\n----------------------------------------\n\nTITLE: Whole-Network CUDA Graph Capture with PyTorch\nDESCRIPTION: Shows how to capture an entire neural network training iteration including forward pass, loss computation, backward pass, and optimizer step in a CUDA graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nN, D_in, H, D_out = 640, 4096, 2048, 1024\nmodel = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n                            torch.nn.Dropout(p=0.2),\n                            torch.nn.Linear(H, D_out),\n                            torch.nn.Dropout(p=0.1)).cuda()\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n# Placeholders used for capture\nstatic_input = torch.randn(N, D_in, device='cuda')\nstatic_target = torch.randn(N, D_out, device='cuda')\n\n# warmup\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for i in range(3):\n        optimizer.zero_grad(set_to_none=True)\n        y_pred = model(static_input)\n        loss = loss_fn(y_pred, static_target)\n        loss.backward()\n        optimizer.step()\ntorch.cuda.current_stream().wait_stream(s)\n\n# capture\ng = torch.cuda.CUDAGraph()\noptimizer.zero_grad(set_to_none=True)\nwith torch.cuda.graph(g):\n    static_y_pred = model(static_input)\n    static_loss = loss_fn(static_y_pred, static_target)\n    static_loss.backward()\n    optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Creating Sparse BSR Tensor in PyTorch\nDESCRIPTION: Demonstrates the construction of a Block Sparse Row (BSR) tensor using crow_indices, col_indices, and values tensors. Shows both direct construction and conversion from dense tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\ncrow_indices = torch.tensor([0, 2, 4])\ncol_indices = torch.tensor([0, 1, 0, 1])\nvalues = torch.tensor([[[0, 1, 2], [6, 7, 8]],\n                      [[3, 4, 5], [9, 10, 11]],\n                      [[12, 13, 14], [18, 19, 20]],\n                      [[15, 16, 17], [21, 22, 23]]])\nbsr = torch.sparse_bsr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\n```\n\n----------------------------------------\n\nTITLE: DataParallel with PyTorch Autocast\nDESCRIPTION: Shows how to use autocast with DataParallel for multi-GPU training within a single process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nmodel = MyModel()\ndp_model = nn.DataParallel(model)\n\nwith autocast(device_type='cuda', dtype=torch.float16):\n    output = dp_model(input)\n    loss = loss_fn(output)\n```\n\n----------------------------------------\n\nTITLE: Manually Splitting a Transformer Model for Pipeline Parallelism in Python\nDESCRIPTION: This code demonstrates how to manually split a Transformer model into two stages for pipeline parallelism. It initializes the full model on a meta device, then selectively deletes parts for each stage before wrapping in a PipelineStage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith torch.device(\"meta\"):\n    assert num_stages == 2, \"This is a simple 2-stage example\"\n\n    # we construct the entire model, then delete the parts we do not need for this stage\n    # in practice, this can be done using a helper function that automatically divides up layers across stages.\n    model = Transformer()\n\n    if stage_index == 0:\n        # prepare the first stage model\n        del model.layers[\"1\"]\n        model.norm = None\n        model.output = None\n\n    elif stage_index == 1:\n        # prepare the second stage model\n        model.tok_embeddings = None\n        del model.layers[\"0\"]\n\n    from torch.distributed.pipelining import PipelineStage\n    stage = PipelineStage(\n        model,\n        stage_index,\n        num_stages,\n        device,\n    )\n```\n\n----------------------------------------\n\nTITLE: Computing Batch Jacobian with vmap in PyTorch\nDESCRIPTION: Demonstrates how to use vmap with jacrev to compute Jacobians for a batch of inputs. This approach applies jacrev to each input in the batch and stacks the results efficiently.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ncompute_batch_jacobian = vmap(jacrev(predict, argnums=2), in_dims=(None, None, 0))\nbatch_jacobian0 = compute_batch_jacobian(weight, bias, x)\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Model Entrypoint in Python\nDESCRIPTION: This snippet defines an entrypoint for the ResNet18 model in PyTorch, detailing how to load pretrained weights and dependencies using a Python function. It explains how the entrypoint integrates with torchvision and emphasizes the use of documentation within entrypoints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/hub.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndependencies = ['torch']\nfrom torchvision.models.resnet import resnet18 as _resnet18\n\n# resnet18 is the name of entrypoint\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    Resnet18 model\n    pretrained (bool): kwargs, load pretrained weights into the model\n    \"\"\"\n    # Call the model, load pretrained weights\n    model = _resnet18(pretrained=pretrained, **kwargs)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Composing Modules using nn.Sequential\nDESCRIPTION: Illustrates composing modules using `torch.nn.Sequential`. It creates a simple network by chaining two instances of the custom `MyLinear` module with a `torch.nn.ReLU` activation function in between. Input data flows sequentially through the defined layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnet = nn.Sequential(\n  MyLinear(4, 3),\n  nn.ReLU(),\n  MyLinear(3, 1)\n)\n\nsample_input = torch.randn(4)\nnet(sample_input)\n: tensor([-0.6749], grad_fn=<AddBackward0>)\n```\n\n----------------------------------------\n\nTITLE: Creating Named Tensors with PyTorch\nDESCRIPTION: This example demonstrates how to create a tensor with named dimensions using torch.zeros. The factory functions such as torch.empty, torch.rand, torch.randn, torch.ones, and torch.tensor support the names argument to associate names with tensor dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch.zeros(2, 3, names=('N', 'C'))\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Module Initialization\nDESCRIPTION: Shows different ways to initialize PyTorch modules with specific device placement and data types, including custom initialization techniques.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Initialize module directly onto GPU.\nm = nn.Linear(5, 3, device='cuda')\n\n# Initialize module with 16-bit floating point parameters.\nm = nn.Linear(5, 3, dtype=torch.half)\n\n# Skip default parameter initialization and perform custom initialization.\nm = torch.nn.utils.skip_init(nn.Linear, 5, 3)\nnn.init.orthogonal_(m.weight)\n```\n\n----------------------------------------\n\nTITLE: Creating Sparse Hybrid COO Tensor in PyTorch\nDESCRIPTION: Demonstrates the creation of a sparse hybrid COO tensor with tensor values instead of scalar values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ni = [[0, 1, 1],\n     [2, 0, 2]]\nv =  [[3, 4], [5, 6], [7, 8]]\ns = torch.sparse_coo_tensor(i, v, (2, 3, 2))\ns.to_dense()\n```\n\n----------------------------------------\n\nTITLE: Expressing Relationships Between Dynamic Dimensions in PyTorch Export\nDESCRIPTION: Example of using torch.export.Dim to define relationships between input shapes. Shows how to specify that one dimension is one larger than another dimension, with constraints on possible values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y[1:]\n\nx, y = torch.randn(5), torch.randn(6)\ndimx = torch.export.Dim(\"dimx\", min=3, max=6)\ndimy = dimx + 1\n\nexported_program = torch.export.export(\n    M(), (x, y), dynamic_shapes=({0: dimx}, {0: dimy}),\n)\nprint(exported_program)\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobian with Forward-Mode AD using torch.func.jacfwd in Python\nDESCRIPTION: This snippet introduces `torch.func.jacfwd` as an alternative to `jacrev` for computing Jacobians, but using forward-mode automatic differentiation. It computes the Jacobian of `torch.sin` for an input vector `x` and verifies the result against the expected diagonal matrix containing `torch.cos(x)`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.func import jacfwd\nx = torch.randn(5)\njacobian = jacfwd(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n```\n\n----------------------------------------\n\nTITLE: Using torch.compile with Different Backends in Python\nDESCRIPTION: Demonstrates how to use torch.compile with various backends for training and inference optimization. Each example shows the syntax for specifying a different backend.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntorch.compile(m, backend=\"inductor\")\ntorch.compile(m, backend=\"cudagraphs\")\ntorch.compile(m, backend=\"ipex\")\ntorch.compile(m, backend=\"onnxrt\")\ntorch.compile(m, backend=\"tensorrt\")\ntorch.compile(m, backend=\"ipex\")\ntorch.compile(m, backend=\"tvm\")\ntorch.compile(m, backend=\"openvino\")\n```\n\n----------------------------------------\n\nTITLE: Converting Scalars to Tensors in PyTorch FX Graph\nDESCRIPTION: This transformer converts scalar arguments to tensors in a PyTorch FX graph, based on the operator's schema.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef args_map(target, fn, args, kwargs):\n    assert isinstance(args, tuple)\n    assert isinstance(kwargs, dict)\n    args = list(args)\n    kwargs = kwargs.copy()\n\n    # Update the argument based on the function passed\n    def update(key, args, schema):\n        args[key] = fn(args[key], schema)\n\n    # Update each argument in the schema\n    for i, schema in enumerate(target._schema.arguments):\n        if schema.name in kwargs:\n            update(schema.name, kwargs, schema)\n        elif not schema.kwarg_only and i < len(args):\n            update(i, args, schema)\n    return tuple(args), kwargs\n\nclass ScalarToTensorPass(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        breakpoint()\n        def try_coerce(value, arg):\n            return (\n                torch.tensor(value)\n                if isinstance(value, (float, int, bool))\n                and type(arg.type) == torch.TensorType\n                else value\n            )\n\n        args, kwargs = args_map(target, try_coerce, args, kwargs)\n        return super().call_function(target, args, kwargs)\n\ntransformed_graph_module = ScalarToTensorPass(graph_module).transform()\n```\n\n----------------------------------------\n\nTITLE: Avoiding Data-Dependent Control Flow in PyTorch\nDESCRIPTION: Example showing how to refactor code to avoid data-dependent control flow that causes graph breaks. The new version evaluates the condition outside the compiled function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# old\nx = torch.randn(3, 3)\n@torch.compile\ndef fn(y):\n    if x.sum() > 0:\n        return y + x\n    else:\n        return y - x\n\n# new\nx = torch.randn(3, 3)\ncond = (x.sum() > 0).item()\n@torch.compile\ndef fn(y):\n    if cond:\n        return y + x\n    else:\n        return y - x\n```\n\n----------------------------------------\n\nTITLE: Creating and Inspecting Multi-dimensional Sparse COO Tensor in PyTorch\nDESCRIPTION: This snippet shows how to create a multi-dimensional sparse COO tensor and inspect its properties such as sparsity, layout, and dimensions. It also demonstrates accessing COO format data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n>>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n\n>>> isinstance(s, torch.Tensor)\nTrue\n>>> s.is_sparse\nTrue\n>>> s.layout == torch.sparse_coo\nTrue\n\n>>> s.sparse_dim(), s.dense_dim()\n(2, 1)\n\n>>> s._indices()\ntensor([[0, 1, 1],\n        [2, 0, 2]])\n```\n\n----------------------------------------\n\nTITLE: Creating Empty Tensors in PyTorch\nDESCRIPTION: These code snippets show various methods to create empty tensors in PyTorch. When fill_uninitialized_memory is True and deterministic algorithms are enabled, these operations will fill uninitialized memory with known values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/deterministic.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntorch.empty()\ntorch.empty_strided()\ntorch.empty_permuted()\ntorch.empty_like()\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobian-Vector Product (JVP) with torch.func.jvp in Python\nDESCRIPTION: This snippet demonstrates `torch.func.jvp` for computing Jacobian-vector products (forward-mode automatic differentiation). `jvp` takes the function `f`, the primal inputs `(x, y)`, and tangent vectors `(torch.ones(5), torch.ones(5))` corresponding to the inputs. It returns the original function's output (ignored here with `_`) and the JVP result (`out_tangent`). An assertion confirms the JVP for the product function `x*y` is `x+y` when tangents are ones.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.func import jvp\nx = torch.randn(5)\ny = torch.randn(5)\nf = lambda x, y: (x * y)\n_, out_tangent = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))\nassert torch.allclose(out_tangent, x + y)\n```\n\n----------------------------------------\n\nTITLE: Distributed Autograd Context Example in PyTorch\nDESCRIPTION: Shows basic distributed autograd usage including context creation, remote computation, backward pass and gradient retrieval. Uses RPC for remote operations and demonstrates core distributed autograd concepts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith dist_autograd.context() as context_id:\n    t1 = torch.rand((3, 3), requires_grad=True)\n    t2 = torch.rand((3, 3), requires_grad=True)\n\n    # Perform some computation remotely.\n    t3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2))\n\n    # Perform some computation locally based on remote result.\n    t4 = torch.rand((3, 3), requires_grad=True)\n    t5 = torch.mul(t3, t4)\n\n    # Compute some loss.\n    loss = t5.sum()\n\n    # Run the backward pass.\n    dist_autograd.backward(context_id, [loss])\n\n    # Retrieve the gradients from the context.\n    dist_autograd.get_gradients(context_id)\n```\n\n----------------------------------------\n\nTITLE: Basic Resnet18 Profiling with torch.compile\nDESCRIPTION: Example showing how to profile a compiled ResNet18 model using torch.profiler, including warm-up run and chrome trace export.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchvision.models import resnet18\n\ndevice = 'cuda'      # or 'cpu', 'xpu', etc.\nmodel = resnet18().to(device)\n\ninputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]\n\nmodel_c = torch.compile(model)\n\ndef fwd_bwd(inp):\n    out = model_c(inp)\n    out.sum().backward()\n\n# warm up\nfwd_bwd(inputs[0])\n\nwith torch.profiler.profile() as prof:\n    for i in range(1, 4):\n        fwd_bwd(inputs[i])\n        prof.step()\n\nprof.export_chrome_trace(\"trace.json\")\n```\n\n----------------------------------------\n\nTITLE: Calculating Broadcasted Tensor Size in Python\nDESCRIPTION: Illustrates how the size of the resulting tensor is determined when two broadcastable tensors are operated on. It shows examples where dimensions are prepended with 1s for alignment and the final dimension size is the maximum of the input sizes along that dimension. It also includes an example that results in a RuntimeError because the tensors are not broadcastable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# can line up trailing dimensions to make reading easier\n>>> x=torch.empty(5,1,4,1)\n>>> y=torch.empty(  3,1,1)\n>>> (x+y).size()\ntorch.Size([5, 3, 4, 1])\n\n# but not necessary:\n>>> x=torch.empty(1)\n>>> y=torch.empty(3,1,7)\n>>> (x+y).size()\ntorch.Size([3, 1, 7])\n\n>>> x=torch.empty(5,2,4,1)\n>>> y=torch.empty(3,1,1)\n>>> (x+y).size()\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n```\n\n----------------------------------------\n\nTITLE: Installing macOS-Specific Dependencies - bash\nDESCRIPTION: This snippet covers installing additional packages required for macOS builds of PyTorch. It includes MKL for x86 processors and packages for enabling torch.distributed using conda. Apply these steps only on macOS systems. Conda and pip are prerequisites.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n# Add this package on intel x86 processor machines only\\npip install mkl-static mkl-include\\n# Add these packages if torch.distributed is needed\\nconda install pkg-config libuv\n```\n\n----------------------------------------\n\nTITLE: Using CUDA Graphs with Multiple Streams\nDESCRIPTION: This snippet demonstrates how to use CUDA Graphs with multiple streams. It shows the correct way to branch out from and rejoin the initial capturing stream within the graph context.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\nwith torch.cuda.graph(g):\n    # at context manager entrance, torch.cuda.current_stream()\n    # is the initial capturing stream\n\n    # INCORRECT (does not branch out from or rejoin initial stream)\n    with torch.cuda.stream(s):\n        cuda_work()\n\n    # CORRECT:\n    # branches out from initial stream\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        cuda_work()\n    # rejoins initial stream before capture ends\n    torch.cuda.current_stream().wait_stream(s)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Torch Operations\nDESCRIPTION: Implementation of custom mean and add operations for ScalarTensor, including type conversion utilities and support for mixed tensor operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef ensure_tensor(data):\n    if isinstance(data, ScalarTensor):\n        return data.tensor()\n    return torch.as_tensor(data)\n\n@implements(torch.add)\ndef add(input, other):\n   try:\n       if input._N == other._N:\n           return ScalarTensor(input._N, input._value + other._value)\n       else:\n           raise ValueError(\"Shape mismatch!\")\n   except AttributeError:\n       return torch.add(ensure_tensor(input), ensure_tensor(other))\n\n@implements(torch.mean)\ndef mean(input):\n    return float(input._value) / input._N\n```\n\n----------------------------------------\n\nTITLE: Complete Activation Sparsification Example in PyTorch\nDESCRIPTION: Full example showing how to set up and use activation sparsification in a PyTorch model, including custom functions and configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Fetch model\nmodel = SomeModel()\n\n# define some aggregate, reduce and mask functions\ndef aggregate_fn(tensor1, tensor2):\n    return tensor1 + tensor2\n\ndef reduce_fn(tensor):\n    return tensor.mean(dim=0)\n\ndef mask_fn(data, threshold):\n    mask = torch.ones_like(tensor)\n    mask[torch.abs(tensor) < threshold] = 0.0\n    return mask)\n\n# sparse config\ndefault_sparse_config = {\"threshold\": 0.5}\n\n# define activation sparsifier\nact_sparsifier = ActivationSparsifier(model=model, aggregate_fn=aggregate_fn, reduce_fn=reduce_fn, mask_fn=mask_fn, **threshold)\n\n# register some layer to sparsify their activations\nact_sparsifier.register_layer(model.some_layer, threshold=0.8)  # custom sparse config\n\nfor epoch in range(EPOCHS):\n    for input, target in dataset:\n        ...\n        out = model(input)\n        ...\n    act_sparsifier.step()  # mask is computed\n\nact_sparsifier.squash_mask(attach_sparsify_hook=True)  # activations are multiplied with the computed mask before flowing through the layer\n```\n\n----------------------------------------\n\nTITLE: Using Named Tuples in TorchScript Functions\nDESCRIPTION: Example of using collections.namedtuple types in TorchScript. Named tuples work seamlessly with type annotations and can be passed to scripted functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport collections\n\nPoint = collections.namedtuple('Point', ['x', 'y'])\n\n@torch.jit.script\ndef total(point):\n    # type: (Point) -> Tensor\n    return point.x + point.y\n\np = Point(x=torch.rand(3), y=torch.rand(3))\nprint(total(p))\n```\n\n----------------------------------------\n\nTITLE: Accessing Saved Tensors in PyTorch Autograd\nDESCRIPTION: Demonstrates how to access intermediary results saved during forward pass through the grad_fn attribute. Shows tensor creation, exponential operation, and verification of saved results using torch.allclose().\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/autograd.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> b = a.exp()\n>>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))\nTrue\n>>> print(dir(b.grad_fn))\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n>>> print(torch.allclose(b.grad_fn._saved_result, b))\nTrue\n```\n\n----------------------------------------\n\nTITLE: Seeding Python's Standard Random Module in Python\nDESCRIPTION: Sets the seed for Python's built-in `random` module using `random.seed(0)`. This step is necessary for achieving reproducibility if custom operators or other parts of the application code rely on Python's standard library for random number generation, complementing PyTorch's own seeding mechanism.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport random\nrandom.seed(0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Batch with Memory Pinning in PyTorch\nDESCRIPTION: Demonstrates implementing a custom batch class with methods for memory pinning to speed up data transfer to CUDA devices. Utilizes PyTorch's DataLoader and includes a collate function that wraps batches into the custom class, enabling data transfer optimizations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass SimpleCustomBatch:\n    def __init__(self, data):\n        transposed_data = list(zip(*data))\n        self.inp = torch.stack(transposed_data[0], 0)\n        self.tgt = torch.stack(transposed_data[1], 0)\n\n    # custom memory pinning method on custom type\n    def pin_memory(self):\n        self.inp = self.inp.pin_memory()\n        self.tgt = self.tgt.pin_memory()\n        return self\n\ndef collate_wrapper(batch):\n    return SimpleCustomBatch(batch)\n\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ndataset = TensorDataset(inps, tgts)\n\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                    pin_memory=True)\n\nfor batch_ndx, sample in enumerate(loader):\n    print(sample.inp.is_pinned())\n    print(sample.tgt.is_pinned())\n```\n\n----------------------------------------\n\nTITLE: Creating Typed Tensors\nDESCRIPTION: Shows how to create tensors with specific data types and device placement using dtype and device parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n```\n\n----------------------------------------\n\nTITLE: Computing Vector-Jacobian Product (VJP) with torch.func.vjp in Python\nDESCRIPTION: This snippet shows how to use `torch.func.vjp` to compute vector-Jacobian products. `vjp` is applied to a function (`torch.sin`) and input tensors. It returns the original function's output and a new function (`vjp_fn`). This `vjp_fn` takes cotangent vectors (matching the structure of the function's output) and computes the VJP.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.func import vjp\n\ninputs = torch.randn(3)\nfunc = torch.sin\ncotangents = (torch.randn(3),)\n\noutputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)\n```\n\n----------------------------------------\n\nTITLE: Constructing Optimizers with Named Parameters in PyTorch\nDESCRIPTION: Examples of creating SGD and Adam optimizers using named parameters. This approach allows for more explicit parameter tracking during optimization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([('layer0', var1), ('layer1', var2)], lr=0.0001)\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Network Module Composed of Submodules\nDESCRIPTION: Defines a custom neural network module `Net` that internally uses two instances of the `MyLinear` module (`l0`, `l1`) as layers. The `forward` method explicitly defines the data flow: input passes through `l0`, then a ReLU activation (using `torch.nn.functional.relu`), and finally through `l1`. This demonstrates building complex modules by composing simpler ones.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l0 = MyLinear(4, 3)\n    self.l1 = MyLinear(3, 1)\n  def forward(self, x):\n    x = self.l0(x)\n    x = F.relu(x)\n    x = self.l1(x)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Partial-Network CUDA Graph Capture in PyTorch\nDESCRIPTION: Demonstrates how to capture parts of a neural network as CUDA graphs when the entire network cannot be captured due to dynamic control flow or other constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\nN, D_in, H, D_out = 640, 4096, 2048, 1024\n\nmodule1 = torch.nn.Linear(D_in, H).cuda()\nmodule2 = torch.nn.Linear(H, D_out).cuda()\nmodule3 = torch.nn.Linear(H, D_out).cuda()\n\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(chain(module1.parameters(),\n                                  module2.parameters(),\n                                  module3.parameters()),\n                            lr=0.1)\n\n# Sample inputs used for capture\nx = torch.randn(N, D_in, device='cuda')\nh = torch.randn(N, H, device='cuda', requires_grad=True)\n\nmodule1 = torch.cuda.make_graphed_callables(module1, (x,))\nmodule2 = torch.cuda.make_graphed_callables(module2, (h,))\n```\n\n----------------------------------------\n\nTITLE: Auto-vectorization with vmap Transform in PyTorch\nDESCRIPTION: Shows how to use vmap transform to add batch dimensions to tensor operations in a simple linear model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom functorch import vmap\nbatch_size, feature_size = 3, 5\nweights = torch.randn(feature_size, requires_grad=True)\n\ndef model(feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\nexamples = torch.randn(batch_size, feature_size)\nresult = vmap(model)(examples)\n```\n\n----------------------------------------\n\nTITLE: Distributing nn.Module Parameters Using DTensor in PyTorch\nDESCRIPTION: This example demonstrates how to use the high-level API to distribute parameters of a PyTorch nn.Module using DTensor, specifically sharding Linear layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nfrom torch.distributed.tensor import Shard, distribute_tensor, distribute_module, init_device_mesh\n\nclass MyModule(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.fc1 = nn.Linear(8, 8)\n        self.fc2 = nn.Linear(8, 8)\n        self.relu = nn.ReLU()\n\n    def forward(self, input):\n        return self.relu(self.fc1(input) + self.fc2(input))\n\nmesh = init_device_mesh(\"cuda\", (4,))\n\ndef shard_params(mod_name, mod, mesh):\n    col_linear_placement = [Shard(0)]\n    if isinstance(mod, nn.Linear):\n        for name, param in mod.named_parameters():\n            dist_param = nn.Parameter(\n                distribute_tensor(param, mesh, col_linear_placement)\n            )\n            mod.register_parameter(name, dist_param)\n\nsharded_module = distribute_module(MyModule(), mesh, partition_fn=shard_params)\n```\n\n----------------------------------------\n\nTITLE: Invoking Softmax and Log-Softmax Operations in PyTorch (Python)\nDESCRIPTION: These examples present the use of PyTorch's low-level softmax, log_softmax, and their backward counterparts, with clear tensor shapes and dtypes. Dependencies include PyTorch with ATen backend enabled; inputs and outputs are FP16 tensors, typically representing model activations or gradients over batches and feature dimensions. Key parameters are the tensor, the dimension for softmax, and whether the operation is stableoutput shapes match input shapes, with constraints based on tensor ranks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([508, 30522], f16), 1, False), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([508, 30522], f16), T([508, 30522], f16), 1, f16), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 12, ((T([4, 12, 128, 128], f16), -1, False), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([4, 12, 128, 128], f16), T([4, 12, 128, 128], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Gradient Penalty with AMP and GradScaler (Python)\nDESCRIPTION: This example illustrates combining gradient penalty computation with automatic mixed precision and gradient scaling. The penalty's autograd.grad outputs are scaled, so manual unscaling is performed before penalty calculation. All penalty operations are wrapped in autocast, and GradScaler manages scaling for numeric stability. This pattern is used for regularization techniques requiring gradient norms. Assumes model, optimizer, and torch.amp are available.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params\n        scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),\n                                                 inputs=model.parameters(),\n                                                 create_graph=True)\n\n        # Creates unscaled grad_params before computing the penalty. scaled_grad_params are\n        # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:\n        inv_scale = 1./scaler.get_scale()\n        grad_params = [p * inv_scale for p in scaled_grad_params]\n\n        # Computes the penalty term and adds it to the loss\n        with autocast(device_type='cuda', dtype=torch.float16):\n            grad_norm = 0\n            for grad in grad_params:\n                grad_norm += grad.pow(2).sum()\n            grad_norm = grad_norm.sqrt()\n            loss = loss + grad_norm\n\n        # Applies scaling to the backward call as usual.\n        # Accumulates leaf gradients that are correctly scaled.\n        scaler.scale(loss).backward()\n\n```\n\n----------------------------------------\n\nTITLE: Debugging MemPool with use_count and snapshot APIs\nDESCRIPTION: Example code demonstrating how to use torch.cuda.MemPool.use_count and torch.cuda.MemPool.snapshot APIs for debugging memory pool usage and understanding allocation patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npool = torch.cuda.MemPool(allocator)\n\n# pool's use count should be 1 at this point as MemPool object\n# holds a reference\nassert pool.use_count() == 1\n\nnelem_1mb = 1024 * 1024 // 4\n\nwith torch.cuda.use_mem_pool(pool):\n    out_0 = torch.randn(nelem_1mb, device=\"cuda\")\n\n    # pool's use count should be 2 at this point as use_mem_pool\n    # holds a reference\n    assert pool.use_count() == 2\n\n# pool's use count should be back to 1 at this point as use_mem_pool\n# released its reference\nassert pool.use_count() == 1\n\nwith torch.cuda.use_mem_pool(pool):\n    # pool should have 1 segment since we made a small allocation (1 MB)\n    # above and so the CUDACachingAllocator packed it into a 2 MB buffer\n    assert len(pool.snapshot()) == 1\n\n    out_1 = torch.randn(nelem_1mb, device=\"cuda\")\n\n    # pool should still have 1 segment since we made another small allocation\n    # (1 MB) that got packed into the existing 2 MB buffer\n    assert len(pool.snapshot()) == 1\n\n    out_2 = torch.randn(nelem_1mb, device=\"cuda\")\n\n    # pool now should have 2 segments since the CUDACachingAllocator had\n    # to make a new 2 MB buffer to accomodate out_2\n    assert len(pool.snapshot()) == 2\n```\n\n----------------------------------------\n\nTITLE: Enabling Reduced Precision Reduction for BF16 GEMMs in PyTorch (C++)\nDESCRIPTION: Provides the C++ method to control reduced precision reductions in BF16 GEMMs performed via CuBLAS. The example code shows setting `at::globalContext().setAllowBF16ReductionCuBLAS(true)`, which explicitly *enables* these reductions (this is the default state).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nat::globalContext().setAllowBF16ReductionCuBLAS(true);\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Patterns\nDESCRIPTION: This code snippet represents a comprehensive analysis of PyTorch operator usage in a neural network implementation. It includes operator names, input tensor shapes, data types, and call counts. The analysis covers various operations such as log_softmax, softmax, convolutions, matrix multiplications, and pooling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 4, ((T([64, 6, 196, 9, 9], f16, stride=(95256, 81, 486, 9, 1)), -1, False), {})\ncnt: 14, ((T([64, 12, 196, 196], f16), -1, False), {})\ncnt: 2, ((T([64, 12, 1, 197], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 2, ((T([64, 12, 1, 197], f16), T([64, 12, 1, 197], f16), -1, f16), {})\ncnt: 14, ((T([64, 12, 196, 196], f16), T([64, 12, 196, 196], f16), -1, f16), {})\ncnt: 4, ((T([64, 6, 196, 9, 9], f16), T([64, 6, 196, 9, 9], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([50176, 192], f16), [64, 28, 28, 192]), {})\ncnt: 4, ((T([12544, 486], f16), [64, 14, 14, 486]), {})\ncnt: 8, ((T([64, 6, 196, 9, 32], f16), [75264, 9, 32]), {})\ncnt: 4, ((T([75264, 9, 32], f16), [64, 6, 196, 9, 32]), {})\ncnt: 8, ((T([64, 6, 32, 9, 196], f16), [64, 1728, 196]), {})\ncnt: 16, ((T([64, 28, 28, 192], f16), [50176, 192]), {})\ncnt: 4, ((T([50176, 576], f16), [64, 28, 28, 576]), {})\ncnt: 28, ((T([12544, 1152], f16), [64, 14, 14, 1152]), {})\ncnt: 42, ((T([64, 12, 196, 32], f16), [768, 196, 32]), {})\ncnt: 14, ((T([64, 12, 32, 196], f16), [768, 32, 196]), {})\ncnt: 14, ((T([768, 196, 196], f16), [64, 12, 196, 196]), {})\ncnt: 14, ((T([768, 196, 32], f16), [64, 12, 196, 32]), {})\ncnt: 14, ((T([64, 196, 12, 32], f16), [64, 14, 14, 384]), {})\ncnt: 28, ((T([12544, 384], f16), [64, 14, 14, 384]), {})\ncnt: 2, ((T([12608, 768], f16), [64, 197, 768]), {})\ncnt: 2, ((T([64, 384], f16), [64, 1, 384]), {})\ncnt: 2, ((T([64, 12, 32, 197], f16), [768, 32, 197]), {})\ncnt: 2, ((T([768, 1, 197], f16), [64, 12, 1, 197]), {})\ncnt: 2, ((T([64, 12, 197, 32], f16), [768, 197, 32]), {})\ncnt: 2, ((T([768, 1, 32], f16), [64, 12, 1, 32]), {})\ncnt: 1, ((T([64, 196, 384], f16), [12544, 384]), {})\ncnt: 1, ((T([12544, 1000], f16), [64, 196, 1000]), {})\ncnt: 2, ((T([64, 197, 2, 12, 32], f16), [64, 197, 768]), {})\ncnt: 1, ((T([64, 14, 14, 384], f16), [12544, 384]), {})\ncnt: 14, ((T([64, 196, 3, 12, 32], f16), [64, 14, 14, 1152]), {})\ncnt: 4, ((T([64, 196, 6, 9, 9], f16), [64, 14, 14, 486]), {})\nOperator: aten.add.Tensor\ncnt: 4, ((T([64, 14, 14, 486], f16), T([486], f16)), {})\ncnt: 8, ((T([64, 28, 28, 192], f16), T([192], f16)), {})\ncnt: 16, ((T([64, 28, 28, 192], f16, stride=(150528, 28, 1, 784)), T([64, 28, 28, 192], f16)), {})\ncnt: 4, ((T([64, 28, 28, 576], f16), T([576], f16)), {})\ncnt: 1, ((T([64, 14, 14, 384], f16, stride=(75264, 14, 1, 196)), T([1, 14, 14, 384], f16)), {})\ncnt: 28, ((T([64, 14, 14, 384], f16), T([384], f16)), {})\ncnt: 28, ((T([64, 14, 14, 384], f16, stride=(75264, 14, 1, 196)), T([64, 14, 14, 384], f16)), {})\ncnt: 14, ((T([64, 14, 14, 1152], f16), T([1152], f16)), {})\ncnt: 4, ((T([64, 1, 384], f16, stride=(75648, 384, 1)), T([64, 1, 384], f16)), {})\ncnt: 2, ((T([64, 1, 384], f16), T([64, 1, 384], f16)), {})\ncnt: 1, ((T([64, 196, 1000], f16), T([1000], f16)), {})\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16)), {})\ncnt: 7, ((T([64, 197, 384], f16), T([64, 197, 384], f16)), {})\ncnt: 1, ((T([64, 14, 14, 384], f16, stride=(75648, 5376, 384, 1)), T([64, 14, 14, 384], f16)), {})\ncnt: 27, ((T([64, 14, 14, 384], f16), T([64, 14, 14, 384], f16)), {})\ncnt: 4, ((T([64, 28, 28, 192], f16), T([64, 28, 28, 192], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 3, ((T([], i64), 1), {})\nOperator: aten.addmm.default\ncnt: 2, ((T([384], f16), T([64, 384], f16), T([384, 384], f16, stride=(1, 384))), {})\ncnt: 2, ((T([1152], f16), T([64, 384], f16), T([384, 1152], f16, stride=(1, 384))), {})\ncnt: 2, ((T([384], f16), T([64, 1152], f16), T([1152, 384], f16, stride=(1, 1152))), {})\ncnt: 1, ((T([1000], f16), T([64, 384], f16, stride=(75648, 1)), T([384, 1000], f16, stride=(1, 384))), {})\nOperator: aten.avg_pool2d.default\ncnt: 4, ((T([64, 192, 28, 28], f16, stride=(150528, 1, 5376, 192)), [2, 2], [2, 2], [0, 0], True), {})\nOperator: aten.avg_pool2d_backward.default\ncnt: 4, ((T([64, 192, 14, 14], f16, stride=(37632, 1, 2688, 192)), T([64, 192, 28, 28], f16, stride=(150528, 1, 5376, 192)), [2, 2], [2, 2], [0, 0], True, True, None), {})\nOperator: aten.bmm.default\ncnt: 4, ((T([75264, 9, 9], f16), T([75264, 9, 32], f16)), {})\ncnt: 14, ((T([768, 196, 32], f16), T([768, 32, 196], f16)), {})\ncnt: 14, ((T([768, 196, 196], f16), T([768, 196, 32], f16)), {})\ncnt: 2, ((T([768, 1, 32], f16), T([768, 32, 197], f16)), {})\ncnt: 2, ((T([768, 1, 197], f16), T([768, 197, 32], f16)), {})\ncnt: 2, ((T([768, 197, 1], f16), T([768, 1, 32], f16)), {})\ncnt: 2, ((T([768, 1, 32], f16), T([768, 32, 197], f16, stride=(6304, 1, 32))), {})\ncnt: 2, ((T([768, 32, 1], f16), T([768, 1, 197], f16)), {})\ncnt: 2, ((T([768, 1, 197], f16), T([768, 197, 32], f16, stride=(6304, 1, 197))), {})\ncnt: 14, ((T([768, 196, 196], f16, stride=(38416, 1, 196)), T([768, 196, 32], f16)), {})\ncnt: 14, ((T([768, 196, 32], f16), T([768, 32, 196], f16, stride=(6272, 1, 32))), {})\ncnt: 14, ((T([768, 32, 196], f16, stride=(6272, 1, 32)), T([768, 196, 196], f16)), {})\ncnt: 14, ((T([768, 196, 196], f16), T([768, 196, 32], f16, stride=(6272, 1, 196))), {})\ncnt: 4, ((T([75264, 9, 9], f16, stride=(81, 1, 9)), T([75264, 9, 32], f16)), {})\ncnt: 4, ((T([75264, 9, 32], f16), T([75264, 32, 9], f16, stride=(288, 1, 32))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([64, 1, 384], f16, stride=(0, 384, 1)), T([64, 196, 384], f16, stride=(75264, 1, 196))], 1), {})\ncnt: 2, (([T([64, 1, 384], f16), T([64, 196, 384], f16, stride=(75648, 384, 1))], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 224, 224], f16),), {})\nOperator: aten.col2im.default\ncnt: 4, ((T([64, 1728, 196], f16), [28, 28], [3, 3], [1, 1], [1, 1], [2, 2]), {})\nOperator: aten.col2im_backward.default\ncnt: 4, ((T([64, 192, 28, 28], f16, stride=(150528, 1, 5376, 192)), [3, 3], [1, 1], [1, 1], [2, 2]), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([64, 64, 112, 112], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 64, 112, 112], f16), T([192, 64, 4, 4], f16), T([192], f16), [4, 4], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 192, 28, 28], f16), T([384, 192, 2, 2], f16), T([384], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 384, 14, 14], f16, stride=(75264, 1, 5376, 384)), T([64, 192, 28, 28], f16), T([384, 192, 2, 2], f16), [384], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([64, 192, 28, 28], f16), T([64, 64, 112, 112], f16), T([192, 64, 4, 4], f16), [192], [4, 4], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([64, 64, 112, 112], f16), T([64, 64, 112, 112], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 64, 112, 112], f16), T([64, 3, 224, 224], f16), T([64, 3, 7, 7], f16), [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\nOperator: aten.gelu.default\ncnt: 4, ((T([64, 28, 28, 576], f16),), {})\ncnt: 14, ((T([64, 14, 14, 1152], f16),), {})\ncnt: 2, ((T([64, 1, 1152], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 2, ((T([64, 1, 1152], f16), T([64, 1, 1152], f16)), {})\ncnt: 14, ((T([64, 14, 14, 1152], f16), T([64, 14, 14, 1152], f16)), {})\ncnt: 4, ((T([64, 28, 28, 576], f16), T([64, 28, 28, 576], f16)), {})\nOperator: aten.im2col.default\ncnt: 4, ((T([64, 192, 28, 28], f16, stride=(150528, 1, 5376, 192)), [3, 3], [1, 1], [1, 1], [2, 2]), {})\nOperator: aten.im2col_backward.default\ncnt: 4, ((T([64, 1728, 196], f16), [28, 28], [3, 3], [1, 1], [1, 1], [2, 2]), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([64], i64),), {})\nOperator: aten.max.dim\ncnt: 1, ((T([64, 196, 1000], f16), 1), {})\nOperator: aten.mm.default\ncnt: 8, ((T([50176, 192], f16), T([192, 192], f16, stride=(1, 192))), {})\ncnt: 4, ((T([12544, 192], f16), T([192, 486], f16, stride=(1, 192))), {})\ncnt: 4, ((T([50176, 192], f16), T([192, 576], f16, stride=(1, 192))), {})\ncnt: 4, ((T([50176, 576], f16), T([576, 192], f16, stride=(1, 576))), {})\ncnt: 28, ((T([12544, 384], f16), T([384, 1152], f16, stride=(1, 384))), {})\ncnt: 14, ((T([12544, 384], f16), T([384, 384], f16, stride=(1, 384))), {})\ncnt: 14, ((T([12544, 1152], f16), T([1152, 384], f16, stride=(1, 1152))), {})\ncnt: 2, ((T([12608, 384], f16), T([384, 768], f16, stride=(1, 384))), {})\ncnt: 2, ((T([64, 384], f16, stride=(75648, 1)), T([384, 384], f16, stride=(1, 384))), {})\ncnt: 1, ((T([12544, 384], f16), T([384, 1000], f16, stride=(1, 384))), {})\ncnt: 1, ((T([1000, 12544], f16, stride=(1, 1000)), T([12544, 384], f16)), {})\ncnt: 1, ((T([12544, 1000], f16), T([1000, 384], f16)), {})\ncnt: 1, ((T([64, 1000], f16), T([1000, 384], f16)), {})\ncnt: 1, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 384], f16, stride=(75648, 1))), {})\ncnt: 2, ((T([64, 384], f16, stride=(75648, 1)), T([384, 1152], f16)), {})\ncnt: 2, ((T([384, 64], f16, stride=(1, 75648)), T([64, 1152], f16)), {})\ncnt: 2, ((T([64, 1152], f16), T([1152, 384], f16)), {})\ncnt: 2, ((T([1152, 64], f16, stride=(1, 1152)), T([64, 384], f16)), {})\ncnt: 4, ((T([64, 384], f16), T([384, 384], f16)), {})\ncnt: 2, ((T([384, 64], f16, stride=(1, 384)), T([64, 384], f16)), {})\ncnt: 2, ((T([384, 64], f16, stride=(1, 384)), T([64, 384], f16, stride=(75648, 1))), {})\ncnt: 2, ((T([768, 12608], f16, stride=(1, 768)), T([12608, 384], f16)), {})\ncnt: 2, ((T([12608, 768], f16), T([768, 384], f16)), {})\ncnt: 14, ((T([384, 12544], f16, stride=(1, 384)), T([12544, 1152], f16)), {})\ncnt: 14, ((T([12544, 384], f16), T([384, 1152], f16)), {})\ncnt: 28, ((T([1152, 12544], f16, stride=(1, 1152)), T([12544, 384], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating comptime breakpoint usage in Python with torch.compile\nDESCRIPTION: This example shows a complete function using torch.compile with a comptime breakpoint. It illustrates how to trigger the breakpoint and use various debugging commands to inspect the Dynamo state.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch._dynamo.comptime import comptime\n\n@torch.compile(backend=\"eager\")\ndef f(x):\n    y = x + 1\n    comptime.breakpoint()\n    y = y + 1\n    return y\n\nf(torch.ones(3, 3))\n```\n\n----------------------------------------\n\nTITLE: Configuring DDP and Logging with torch.distributed and Multiprocessing - Python\nDESCRIPTION: This Python snippet sets up a two-layer linear model for distributed training using torch.nn.parallel.DistributedDataParallel (DDP), initializing distributed process groups, CUDA device assignment, and debug-level logging with environment variables. It demonstrates model creation, input preparation, and running 20 training iterations, using mp.spawn for multi-process launch. Dependencies include torch, torch.distributed, torch.multiprocessing, and CUDA, and the environment variables TORCH_CPP_LOG_LEVEL and TORCH_DISTRIBUTED_DEBUG control detailed logging and runtime checks. Outputs include debug logs for initialization, performance statistics, and error reporting for synchronization issues.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\nclass TwoLinLayerNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Linear(10, 10, bias=False)\n        self.b = torch.nn.Linear(10, 1, bias=False)\n\n    def forward(self, x):\n        a = self.a(x)\n        b = self.b(x)\n        return (a, b)\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    print(\"init model\")\n    model = TwoLinLayerNet().cuda()\n    print(\"init ddp\")\n    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n    inp = torch.randn(10, 10).cuda()\n    print(\"train\")\n\n    for _ in range(20):\n        output = ddp_model(inp)\n        loss = output[0] + output[1]\n        loss.sum().backward()\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\n        \"TORCH_DISTRIBUTED_DEBUG\"\n    ] = \"DETAIL\"  # set to DETAIL for runtime logging.\n    mp.spawn(worker, nprocs=2, args=())\n```\n\n----------------------------------------\n\nTITLE: Backward Computation for Dense Embeddings in PyTorch (Python)\nDESCRIPTION: Executes aten.embedding_dense_backward essential in backpropagation for gradient calculation of embedding layers, crucial to training phases of models incorporating dense data representations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\naten.embedding_dense_backward.default\ncnt: 1, ((T([16, 512, 768], f16), T([16, 512], i64), 2, -1, False), {})\ncnt: 2, ((T([16, 512, 768], f16), T([16, 512], i64), 1024, -1, False), {})\ncnt: 4, ((T([16, 512, 768], f16), T([16, 512], i64, stride=(2048, 4)), 1024, -1, False), {})\ncnt: 1, ((T([1, 512, 768], f16), T([1, 512], i64), 512, -1, False), {})\ncnt: 1, ((T([16, 512, 768], f16), T([16, 512], i64), 30522, 0, False), {})\n```\n\n----------------------------------------\n\nTITLE: Enforcing Deterministic cuDNN Convolution Algorithms in Python\nDESCRIPTION: Specifies two ways to ensure cuDNN uses deterministic convolution algorithms, which is necessary even if benchmarking is disabled. Setting `torch.use_deterministic_algorithms(True)` enforces determinism globally for many PyTorch operations, while `torch.backends.cudnn.deterministic = True` specifically targets cuDNN convolutions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntorch.use_deterministic_algorithms(True)\n```\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cudnn.deterministic = True\n```\n\n----------------------------------------\n\nTITLE: Avoiding CPU Oversubscription in PyTorch Training (Python)\nDESCRIPTION: This snippet shows how to adjust the number of threads in each subprocess to avoid CPU oversubscription during model training in PyTorch. It requires setting the number of threads using torch.set_num_threads() within each training process. Inputs include the number of vCPUs available (N) and the number of subprocesses (M), while outputs are efficient CPU utilization without oversubscription.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/multiprocessing.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef train(rank, args, model, device, dataset, dataloader_kwargs):\n    torch.manual_seed(args.seed + rank)\n\n    #### define the num threads used in current sub-processes\n    torch.set_num_threads(floor(N/M))\n\n    train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)\n\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    for epoch in range(1, args.epochs + 1):\n        train_epoch(epoch, args, model, device, train_loader, optimizer)\n```\n\n----------------------------------------\n\nTITLE: Ensembling Predictions with Distinct Minibatches via Loop - PyTorch - Python\nDESCRIPTION: Assigns a separate data minibatch to each model and computes predictions using a Python list comprehension. Minibatches must be generated by slicing the data to match the number of models. This for-loop method is the baseline for evaluating performance and correctness against vectorized alternatives.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nminibatches = data[:num_models]\npredictions_diff_minibatch_loop = [model(minibatch) for model, minibatch in zip(models, minibatches)]\n```\n\n----------------------------------------\n\nTITLE: Initializing Fuzzer for Tensor Generation in Python\nDESCRIPTION: This code snippet demonstrates how to create a Fuzzer object to generate random tensors with specific parameters. It creates two tensors, 'x' and 'y', where 'x' is a 2D tensor and 'y' is broadcastable to the shape of 'x'.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/benchmark/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfuzzer = Fuzzer(\n  parameters=[\n    FuzzedParameter(\"k0\", 16, 16 * 1024, \"loguniform\"),\n    FuzzedParameter(\"k1\", 16, 16 * 1024, \"loguniform\"),\n  ],\n  tensors=[\n    FuzzedTensor(\n      name=\"x\", size=(\"k0\", \"k1\"), probability_contiguous=0.75\n    ),\n    FuzzedTensor(\n      name=\"y\", size=(\"k0\", 1), probability_contiguous=0.75\n    ),\n  ],\n  seed=0,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Quantized XAND Function Logic in C++\nDESCRIPTION: Shows the C++ implementation for a quantized element-wise XAND operation (`quantized_xand`). It takes two quantized tensors (`qa`, `qb`), uses `TensorIterator` for element-wise processing, and employs `AT_DISPATCH_QINT_TYPES` to handle different quantized data types (like `qint8`). The output tensor `qc` maintains the quantization parameters (scale, zero-point) of the input `qa`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\nTensor quantized_xand(Tensor qa, Tensor qb) {\n  // Some type checks for qa and qb should be here...\n  Tensor qc;\n  double scale = qa.q_scale();\n  int64_t zero_point = qa.q_zero_point();\n\n  auto iter = TensorIterator::binary_op(qc, qa, qb);\n\n  AT_DISPATCH_QINT_TYPES(qa.scalar_type(), \"quantized_xand\", [&]() {\n    Tensor qc = at::_empty_affine_quantized(\n        qa.sizes(), at::device(kCPU).dtype(SCALAR_TYPE), scale, zero_point);\n    cpu_kernel(iter, [&](scalar_t a_value, scalar_t b_value) -> scalar_t {\n      return scalar_t(a_value.val_ & b_value.val_);\n    });\n  });\n  return qc;\n}\n```\n\n----------------------------------------\n\nTITLE: PyTorch Conv-BN Fusion Example\nDESCRIPTION: Example showing how to use torch.ao.quantization.fuse_modules API for fusing Conv-BN or Linear-BN patterns in Eager mode quantization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization-accuracy-debugging.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntorch.ao.quantization.fuse_modules\n```\n\n----------------------------------------\n\nTITLE: Python Primitive Specialization in torch.export\nDESCRIPTION: Example showing how torch.export specializes on Python primitive values like integers. The loop count and constant value are baked into the exported program rather than being treated as dynamic inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.export import export\n\nclass Mod(torch.nn.Module):\n    def forward(self, x: torch.Tensor, const: int, times: int):\n        for i in range(times):\n            x = x + const\n        return x\n\nexample_inputs = (torch.rand(2, 2), 1, 3)\nexported_program = export(Mod(), example_inputs)\nprint(exported_program)\n```\n\n----------------------------------------\n\nTITLE: Basic __torch_function__ Implementation for ScalarTensor\nDESCRIPTION: Example implementation of __torch_function__ that falls back to original torch functions when no override is available. Converts ScalarTensor arguments to regular tensors before calling the original function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef __torch_function__(cls, func, types, args=(), kwargs=None):\n    if kwargs is None:\n        kwargs = {}\n    if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n        args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n        return func(*args, **kwargs)\n    return HANDLED_FUNCTIONS[func](*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Compiling PyTorch Model with DistributedDataParallel\nDESCRIPTION: Example of applying torch.compile with DistributedDataParallel wrapper for distributed training\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# DistributedDataParallel\nmodel = ...\nopt_model = torch.compile(model)\nmodel_ddp = DistributedDataParallel(opt_model, ...)\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = model_ddp(inp)\n```\n\n----------------------------------------\n\nTITLE: Vectorizing a Function Using vmap - Pure Function Example - PyTorch - Python\nDESCRIPTION: This snippet illustrates how vmap in torch.func is conceptually similar to manually mapping a function over input tensors in a for-loop, provided the function has no side effects. It demonstrates using torch.stack and Python's list comprehension to achieve the same behavior as vmap, highlighting that under the hood, vmap(f)(x) ~ torch.stack([f(x_i) for x_i in x.unbind(0)]). Required dependencies: PyTorch. Functionality focuses on batch mapping of computations over tensor entries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntorch.stack([f(x_i) for x_i in x.unbind(0)])\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Allowed and Disallowed Casting in PyTorch Operations\nDESCRIPTION: This code snippet demonstrates the casting rules in PyTorch when the output tensor of an arithmetic operation is specified. It shows which operations are allowed (such as casting integers to float) and which are disallowed (such as casting float to integer).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# allowed:\n>>> float_tensor *= float_tensor\n>>> float_tensor *= int_tensor\n>>> float_tensor *= uint_tensor\n>>> float_tensor *= bool_tensor\n>>> float_tensor *= double_tensor\n>>> int_tensor *= long_tensor\n>>> int_tensor *= uint_tensor\n>>> uint_tensor *= int_tensor\n\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\n>>> int_tensor *= float_tensor\n>>> bool_tensor *= int_tensor\n>>> bool_tensor *= uint_tensor\n>>> float_tensor *= complex_float_tensor\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Cube Function with Auto-generated vmap\nDESCRIPTION: Example implementation of a custom autograd Function that computes the cube of a tensor with automatic vmap rule generation. Includes forward, backward, and setup_context methods with proper tensor handling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyCube(torch.autograd.Function):\n    generate_vmap_rule = True\n\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result\n\nx = torch.randn(3)\nresult = torch.vmap(my_cube)(x)\nassert torch.allclose(result, x ** 3)\n```\n\n----------------------------------------\n\nTITLE: Python ScriptModule Attribute Example\nDESCRIPTION: Shows an example of defining attributes on a ScriptModule that will be handled by the TorchScript compiler.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.x = 2\n\n    def forward(self):\n        return self.x\n\nm = torch.jit.script(Model())\n```\n\n----------------------------------------\n\nTITLE: Handling Import Errors in PyTorch\nDESCRIPTION: The Python snippet illustrates a typical ImportError caused by missing DLLs in PyTorch. It highlights a remedial script to install essential components via conda, including VC2017 runtime and MKL libraries, addressing missing dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._C import *\n\nImportError: DLL load failed: The specified module could not be found.\n```\n\n----------------------------------------\n\nTITLE: Loading a PyTorch Module in C++\nDESCRIPTION: Shows how to load a serialized PyTorch module in C++.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n>>> torch::jit::script::Module module;\n>>> module = torch::jit::load('controlflowmodule_scripted.pt');\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Neural Network\nDESCRIPTION: This code snippet represents a log or analysis output of PyTorch operator usage in a neural network. It shows various operators, their usage count, and the tensor shapes they operate on. This information is crucial for understanding the model's architecture and performance characteristics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mnasnet_100_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 52, ((T([], i64), 1), {})\ncnt: 4, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})\ncnt: 4, ((T([128, 40, 28, 28], f16), T([128, 40, 28, 28], f16)), {})\ncnt: 4, ((T([128, 80, 14, 14], f16), T([128, 80, 14, 14], f16)), {})\ncnt: 2, ((T([128, 96, 14, 14], f16), T([128, 96, 14, 14], f16)), {})\ncnt: 6, ((T([128, 192, 7, 7], f16), T([128, 192, 7, 7], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([16, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n# ... (truncated for brevity)\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 320, 7, 7], f16), T([128, 1152, 7, 7], f16), T([320, 1152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Registering Parameters and Buffers in a PyTorch Module\nDESCRIPTION: Showcases various ways to register parameters and buffers within a PyTorch module, including direct attribute assignment, string-based registration, and using ParameterList.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass StatefulModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n    # Setting a nn.Parameter as an attribute of the module automatically registers the tensor\n    # as a parameter of the module.\n    self.param1 = nn.Parameter(torch.randn(2))\n\n    # Alternative string-based way to register a parameter.\n    self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n\n    # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything\n    # except a parameter. \"None\" entries like this will not be present in the module's state_dict.\n    self.register_parameter('param3', None)\n\n    # Registers a list of parameters.\n    self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n\n    # Registers a dictionary of parameters.\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Producer Process Using Events in PyTorch Multiprocessing (Python)\nDESCRIPTION: Shows how a producer process can use an `event` object (presumably a `torch.multiprocessing.Event` or standard `multiprocessing.Event`) to wait until consumer processes have finished processing shared tensors. Calling `event.wait()` blocks the producer until the event is set by the consumers, ensuring resources aren't released prematurely.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n## producer\n# send tensors, do something\nevent.wait()\n```\n\n----------------------------------------\n\nTITLE: Implementing BUILD_LIST Bytecode in Python Symbolic Execution\nDESCRIPTION: This code snippet shows the implementation of the BUILD_LIST bytecode in Dynamo's symbolic executor. It pops a specified number of elements from the stack and pushes a new ListVariable to the stack.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef BUILD_LIST(self, inst):\n    items = self.popn(inst.argval)\n    self.push(ListVariable(items, mutation_type=ValueMutationNew()))\n```\n\n----------------------------------------\n\nTITLE: Asynchronous Execution with TorchScript Fork/Wait in Python\nDESCRIPTION: Demonstrates inter-op parallelism in PyTorch using TorchScript. The `forward` function uses `torch.jit._fork` to launch the `compute_z` function asynchronously, allowing parallel execution with the subsequent matrix multiplication (`torch.mm`). `torch.jit._wait` is used to synchronize and retrieve the result of the asynchronous task before combining it with the other result. This requires the `torch` library and utilizes TorchScript JIT compilation (`@torch.jit.script`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cpu_threading_torchscript_inference.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\ndef compute_z(x):\n    return torch.mm(x, self.w_z)\n\n@torch.jit.script\ndef forward(x):\n    # launch compute_z asynchronously:\n    fut = torch.jit._fork(compute_z, x)\n    # execute the next operation in parallel to compute_z:\n    y = torch.mm(x, self.w_y)\n    # wait for the result of compute_z:\n    z = torch.jit._wait(fut)\n    return y + z\n```\n\n----------------------------------------\n\nTITLE: In-place Operations with Named Tensors in PyTorch\nDESCRIPTION: Demonstrates how in-place operations propagate names from a named tensor to an unnamed tensor. When adding a named tensor to an unnamed tensor in-place, the unnamed tensor inherits the dimension names from the named tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.randn(3, 3)\n>>> y = torch.randn(3, 3, names=('N', 'C'))\n>>> x.names\n(None, None)\n\n>>> x += y\n>>> x.names\n('N', 'C')\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Module Hooks\nDESCRIPTION: Demonstrates the implementation of forward and backward hooks in PyTorch modules for custom behavior during forward and backward passes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef forward_pre_hook(m, inputs):\n    input = inputs[0]\n    return input + 1.\n\ndef forward_hook(m, inputs, output):\n    return output + inputs[0]\n\ndef backward_hook(m, grad_inputs, grad_outputs):\n    new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n    return new_grad_inputs\n\nm = nn.Linear(3, 3)\nx = torch.randn(2, 3, requires_grad=True)\n\nforward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\nforward_hook_handle = m.register_forward_hook(forward_hook)\nm.register_full_backward_hook(backward_hook)\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobian with Reverse-Mode AD using torch.func.jacrev in Python\nDESCRIPTION: This snippet showcases `torch.func.jacrev` to compute the Jacobian of a function using reverse-mode automatic differentiation. It calculates the Jacobian of `torch.sin` applied element-wise to a vector `x`. The result is compared against the expected Jacobian, which is a diagonal matrix with `torch.cos(x)` on the diagonal.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.func import jacrev\nx = torch.randn(5)\njacobian = jacrev(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n```\n\n----------------------------------------\n\nTITLE: Scripting a Function Operating on a NamedTuple in TorchScript (Python)\nDESCRIPTION: This snippet defines a NamedTuple MyTuple with two integer fields, a function inc that increments both fields, then scripts and invokes inc via torch.jit.script. It highlights use of typing.NamedTuple and tuple destructuring within TorchScript, returning a new tuple of incremented integers. Prerequisites: torch and typing.NamedTuple; expects a MyTuple input; outputs a 2-tuple of incremented ints. Use this pattern for structured, statically typed containers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom typing import NamedTuple\nfrom typing import Tuple\n\nclass MyTuple(NamedTuple):\n    first: int\n    second: int\n\ndef inc(x: MyTuple) -> Tuple[int, int]:\n    return (x.first+1, x.second+1)\n\nt = MyTuple(first=1, second=2)\nscripted_inc = torch.jit.script(inc)\nprint(\"TorchScript:\", scripted_inc(t))\n```\n\n----------------------------------------\n\nTITLE: Constructing Basic Optimizers in PyTorch\nDESCRIPTION: Examples of creating SGD and Adam optimizers with model parameters. The first example shows optimization of all model parameters, while the second optimizes specific variables.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001)\n```\n\n----------------------------------------\n\nTITLE: Creating Complex Tensors in PyTorch\nDESCRIPTION: Demonstrates how to create complex tensors using torch.randn with cfloat dtype.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nx = torch.randn(2,2, dtype=torch.cfloat)\nx\n```\n\n----------------------------------------\n\nTITLE: Logging Hierarchical Metrics to TensorBoard\nDESCRIPTION: Shows how to log multiple metrics with hierarchical naming for better organization in TensorBoard. Demonstrates logging of train/test metrics for loss and accuracy over multiple iterations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensorboard.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nwriter = SummaryWriter()\n\nfor n_iter in range(100):\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n```\n\n----------------------------------------\n\nTITLE: Replacing Add with Mul Using PyTorch FX Transformer\nDESCRIPTION: This example uses the PyTorch FX Transformer class to replace torch.ops.aten.add.Tensor calls with torch.ops.aten.mul.Tensor calls.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ReplaceAddWithMul(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        if target != torch.ops.aten.add.Tensor:\n            return super().call_function(target, args, kwargs)\n        return super().call_function(torch.ops.aten.mul.Tensor, args, kwargs)\n\ntransformed_graph_module = ReplaceAddWithMul(graph_module).transform()\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients using functorch.grad and make_functional in Python\nDESCRIPTION: Demonstrates the legacy `functorch` approach to calculate gradients for model parameters. It uses `functorch.make_functional` to obtain a stateless version of the model (`fmodel`) and its parameters (`params`). A loss function `compute_loss` is defined, taking these parameters as input. Finally, `functorch.grad` is applied to this loss function to compute the gradients with respect to the parameters. Requires `torch` and `functorch` libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# ---------------\n# using functorch\n# ---------------\nimport torch\nimport functorch\ninputs = torch.randn(64, 3)\ntargets = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nfmodel, params = functorch.make_functional(model)\n\ndef compute_loss(params, inputs, targets):\n    prediction = fmodel(params, inputs)\n    return torch.nn.functional.mse_loss(prediction, targets)\n\ngrads = functorch.grad(compute_loss)(params, inputs, targets)\n```\n\n----------------------------------------\n\nTITLE: Python Inference with Compiled Model\nDESCRIPTION: Shows how to load and run inference using the compiled model artifact in Python using the aoti_load_package utility function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = torch._inductor.aoti_load_package(os.path.join(os.getcwd(), \"model.pt2\"))\nprint(model(torch.randn(8, 10, device=device)))\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Pickled Objects with PackageExporter and PackageImporter\nDESCRIPTION: Demonstrates how to save pickled objects using PackageExporter and load them using PackageImporter. It also shows how to print the file structure of a package and access attributes of imported objects.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npe.save_pickle('foo_collection', 'foo1.pkl', foo_1)\npe.save_pickle('foo_collection', 'foo2.pkl', foo_2)\n\npi = PackageImporter('foo_package.pt')\nprint(pi.file_structure())\nimported_foo = pi.load_pickle('foo_collection', 'foo1.pkl')\nprint(f\"foo_1 string: '{imported_foo.my_string}'\")\nprint(f\"foo_1 export time: {imported_foo.time_exported}\")\nprint(f\"foo_1 import time: {imported_foo.time_imported}\")\n```\n\n----------------------------------------\n\nTITLE: Loading Batched Data from Map-style Dataset\nDESCRIPTION: Illustrates the equivalent operation of loading batched samples from a map-style dataset when automatic batching is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor indices in batch_sampler:\n    yield collate_fn([dataset[i] for i in indices])\n```\n\n----------------------------------------\n\nTITLE: Loading Pretrained Weights for PyTorch Models in Python\nDESCRIPTION: This snippet shows how to load pretrained weights for a model from a local checkpoint or a URL. It highlights the use of the 'torch' library for model weight management and provides a choice between local and web-hosted weights.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/hub.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif pretrained:\n    # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n    dirname = os.path.dirname(__file__)\n    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n    state_dict = torch.load(checkpoint)\n    model.load_state_dict(state_dict)\n\n    # For checkpoint saved elsewhere\n    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n```\n\n----------------------------------------\n\nTITLE: Benchmark Data: FP16 GEMM Performance with Reduced Precision Reduction\nDESCRIPTION: Displays sample benchmark results (in microseconds) comparing the performance of FP16 General Matrix Multiplications (GEMMs) on a V100 GPU with and without reduced precision reductions enabled. The data illustrates potential performance gains, especially for large 'k' dimensions, when allowing reduced precision (`allow_fp16_reduc=True`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n[--------------------------- bench_gemm_transformer --------------------------]\n      [  m ,  k  ,  n  ]    |  allow_fp16_reduc=True  |  allow_fp16_reduc=False\n1 threads: --------------------------------------------------------------------\n      [4096, 4048, 4096]    |           1634.6        |           1639.8\n      [4096, 4056, 4096]    |           1670.8        |           1661.9\n      [4096, 4080, 4096]    |           1664.2        |           1658.3\n      [4096, 4096, 4096]    |           1639.4        |           1651.0\n      [4096, 4104, 4096]    |           1677.4        |           1674.9\n      [4096, 4128, 4096]    |           1655.7        |           1646.0\n      [4096, 4144, 4096]    |           1796.8        |           2519.6\n      [4096, 5096, 4096]    |           2094.6        |           3190.0\n      [4096, 5104, 4096]    |           2144.0        |           2663.5\n      [4096, 5112, 4096]    |           2149.1        |           2766.9\n      [4096, 5120, 4096]    |           2142.8        |           2631.0\n      [4096, 9728, 4096]    |           3875.1        |           5779.8\n      [4096, 16384, 4096]   |           6182.9        |           9656.5\n(times in microseconds).\n```\n\n----------------------------------------\n\nTITLE: Setting up MNIST Training Loop with PyTorch Lazy Tensor\nDESCRIPTION: Configures the MNIST dataset, data loader, model, optimizer, and training loop using PyTorch's Lazy Tensor. The model is moved to the 'lazy' device for Lazy Tensor computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport os\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nimport torch._lazy\nimport torch._lazy.ts_backend\nimport torch._lazy.metrics\ntorch._lazy.ts_backend.init()\n\nif __name__  == '__main__':\n    bsz = 64\n    device = 'lazy'\n    epochs = 14\n    log_interval = 10\n    lr = 1\n    gamma = 0.7\n    train_kwargs = {'batch_size': bsz}\n    # if we want to use CUDA\n    if \"LTC_TS_CUDA\" in os.environ:\n        cuda_kwargs = {'num_workers': 1,\n                       'pin_memory': True,\n                       'shuffle': True,\n                       'batch_size': bsz}\n        train_kwargs.update(cuda_kwargs)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    dataset1 = datasets.MNIST('./data', train=True, download=True,\n                        transform=transform)\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n    for epoch in range(1, epochs + 1):\n        train(log_interval, model, device, train_loader, optimizer, epoch)\n        scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Vectorized Ensemble Prediction with vmap (Shared Minibatch) - functorch/PyTorch - Python\nDESCRIPTION: Uses vmap with custom in_dims to apply the same minibatch across the stacked ensemble, enabling fast prediction for multiple models using identical inputs. An assertion checks numerical equivalence with for-loop results. Requires parameters and buffers to be stacked appropriately and minibatch dimensions to match model input.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npredictions2_vmap = vmap(fmodel, in_dims=(0, 0, None))(params, buffers, minibatch)\n\nassert torch.allclose(predictions2_vmap, torch.stack(predictions2), atol=1e-3, rtol=1e-5)\n```\n\n----------------------------------------\n\nTITLE: Using ModelReport in Pytorch Quantization Workflow\nDESCRIPTION: Example code demonstrating how to use the ModelReport class within a PyTorch quantization workflow. This includes preparing a model, creating detector sets, inserting observers, calibrating the model, generating reports, and visualizing the results.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/_model_report/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# prep model\nqconfig_mapping = torch.ao.quantization.get_default_qconfig_mapping()\nmodel = Model() # TODO define model\nexample_input = torch.randn((*args)) # TODO get example data for callibration\nprepared_model = quantize_fx.prepare_fx(model, qconfig_mapping, example_input)\n\n# create ModelReport instance and insert observers\ndetector_set = set([DynamicStaticDetector()]) # TODO add all desired detectors\nmodel_report = ModelReport(model, detector_set)\nready_for_callibrate = model_report.prepare_detailed_callibration()\n\n# callibrate model and generate report\nready_for_callibrate(example_input) # TODO run callibration of model with relevant data\nreports = model_report.generate_model_report(remove_inserted_observers=True)\nfor report_name in report.keys():\n    text_report, report_dict = reports[report_name]\n    print(text_report, report_dict)\n\n# Optional: we get a ModelReportVisualizer instance to do any visualizations desired\nmod_rep_visualizer = tracer_reporter.generate_visualizer()\nmod_rep_visualizer.generate_table_visualization() # shows collected data as a table\n\n# TODO updated qconfig based on suggestions\n```\n\n----------------------------------------\n\nTITLE: Installing Windows-Specific Dependencies - bash\nDESCRIPTION: This snippet installs packages needed for a Windows build of PyTorch. It uses pip for MKL and conda for libuv, specifically when enabling torch.distributed. Note that distributed support is experimental on Windows. Run these commands within an activated conda environment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install mkl-static mkl-include\\n# Add these packages if torch.distributed is needed.\\n# Distributed package support on Windows is a prototype feature and is subject to changes.\\nconda install -c conda-forge libuv=1.39\n```\n\n----------------------------------------\n\nTITLE: BERT Model Optimization with HuggingFace\nDESCRIPTION: Demonstrates how to optimize a pre-trained BERT model from HuggingFace using torch.compile with the inductor backend for GPU inference.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\").to(device=\"cuda:0\")\nmodel = torch.compile(model, backend=\"inductor\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt').to(device=\"cuda:0\")\noutput = model(**encoded_input)\n```\n\n----------------------------------------\n\nTITLE: Configuring Backend for Linear-ReLU Fusion\nDESCRIPTION: This code snippet demonstrates how to configure the backend for Linear-ReLU fusion. It defines a fusion method and sets up the BackendPatternConfig with custom node getters and fusion method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef fuse_linear_relu(is_qat, linear, relu):\n    return nni.LinearReLU(linear, relu)\n\nBackendPatternConfig((torch.nn.Linear, torch.nn.ReLU))\n    .set_fuser_method(fuse_linear_relu)\n    ._set_root_node_getter(my_root_node_getter)\n    ._set_extra_inputs_getter(my_extra_inputs_getter)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Copy Epilogue in Python for FX Graph Pass\nDESCRIPTION: This code snippet shows a function with a clone operation followed by a copy operation. It illustrates that the clone operation cannot be eliminated due to the copy epilogue, as it would cause unintended aliasing between input and output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(x: Tensor):\n    y = x.clone()\n    x.copy_(y)\n    return y\n```\n\n----------------------------------------\n\nTITLE: MetadataTensor Wrapper Implementation\nDESCRIPTION: Example of a tensor wrapper class that attaches metadata to tensors and propagates it through torch operations. Implements __torch_function__ to handle the full torch API.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass MetadataTensor(object):\n    def __init__(self, data, metadata=None, **kwargs):\n        self._t = torch.as_tensor(data, **kwargs)\n        self._metadata = metadata\n\n    def __repr__(self):\n        return \"Metadata:\\n{}\\n\\ndata:\\n{}\".format(self._metadata, self._t)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))\n        args = [getattr(a, '_t', a) for a in args]\n        assert len(metadatas) > 0\n        ret = func(*args, **kwargs)\n        return MetadataTensor(ret, metadata=metadatas[0])\n```\n\n----------------------------------------\n\nTITLE: Configuring Per-Layer Learning Rates in PyTorch\nDESCRIPTION: Example of specifying different learning rates for different parameter groups. The base model uses a higher learning rate (1e-2) while the classifier uses the default rate (1e-3).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noptim.SGD([\n                {'params': model.base.parameters(), 'lr': 1e-2},\n                {'params': model.classifier.parameters()}\n            ], lr=1e-3, momentum=0.9)\n\noptim.SGD([\n                {'params': model.base.named_parameters(), 'lr': 1e-2},\n                {'params': model.classifier.named_parameters()}\n            ], lr=1e-3, momentum=0.9)\n```\n\n----------------------------------------\n\nTITLE: Simple Data Sparsification Example in PyTorch\nDESCRIPTION: Basic usage of a data sparsifier to sparsify a regular tensor and a parameter. Shows how to add data to the sparsifier, compute the mask, and apply the mask to the data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntensor1 = torch.randn(100, 100)\nparam1 = nn.Parameter(torch.randn(200, 32))\n\nmy_sparsifier = ImplementedDataSparsifier(threshold=0.2)\nmy_sparsifier.add_data(name='tensor1', data=tensor1, threshold=0.5)\nmy_sparsifier.add_data(name='param1', data=param1)\n\nmy_sparsifier.step()  # computes mask\n\nmy_sparsifier.squash_mask()  # applies and removes mask\n```\n\n----------------------------------------\n\nTITLE: Shape Propagation Interpreter for FX GraphModules\nDESCRIPTION: An implementation of the interpreter pattern for FX GraphModules that records tensor shapes and dtypes during execution. This class executes each node in the graph with given arguments and stores shape and dtype information on each node for analysis purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistent target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)\n```\n\n----------------------------------------\n\nTITLE: Optional Type Refinement in TorchScript With Conditionals and Asserts - PyTorch - Python\nDESCRIPTION: This code demonstrates how TorchScript refines Optional types based on None checks within conditionals and asserts. An nn.Module subclass defines a forward method using Optional[int] parameters; TorchScript infers variable types when conditions or asserts validate non-None values. Shows refinement for local variables and copying attributes to locals for refinement. Dependencies include torch, torch.nn, and typing. Inputs include Optional[int] types and torch.Tensor, returning an int.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nfrom typing import Optional\n\nclass M(nn.Module):\n    z: Optional[int]\n\n    def __init__(self, z):\n        super().__init__()\n        # If `z` is None, its type cannot be inferred, so it must\n        # be specified (above)\n        self.z = z\n\n    def forward(self, x, y, z):\n        # type: (Optional[int], Optional[int], Optional[int]) -> int\n        if x is None:\n            x = 1\n            x = x + 1\n\n        # Refinement for an attribute by assigning it to a local\n        z = self.z\n        if y is not None and z is not None:\n            x = y + z\n\n        # Refinement via an `assert`\n        assert z is not None\n        x += z\n        return x\n\nmodule = torch.jit.script(M(2))\nmodule = torch.jit.script(M(None))\n```\n\n----------------------------------------\n\nTITLE: Creating Stateless Functional Model with functorch\nDESCRIPTION: Imports necessary `functorch` functions (`make_functional_with_buffers`, `vmap`, `grad`). Uses `make_functional_with_buffers(model)` to transform the existing stateful `nn.Module` (`model`) into a stateless version. This process extracts the model's parameters and buffers into separate tuples (`params`, `buffers`) and returns a pure function `fmodel` representing the model's forward pass logic. This separation is required for using `functorch` transforms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import make_functional_with_buffers, vmap, grad\n\nfmodel, params, buffers = make_functional_with_buffers(model)\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobian over Model Parameters in PyTorch\nDESCRIPTION: This example shows how to compute the Jacobian with respect to the parameters of a PyTorch neural network model using the functional_call utility. The function f takes the model's parameters and inputs, and computes the forward pass. Dependencies include torch.func.functional_call, and the code uses torch.nn.Linear to define the model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.api.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nmodel = torch.nn.Linear(3, 3)\n\ndef f(params, x):\n    return torch.func.functional_call(model, params, x)\n\nx = torch.randn(3)\njacobian = jacrev(f)(dict(model.named_parameters()), x)\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization Operations - PyTorch\nDESCRIPTION: Layer normalization operations for transformer layers, including both forward and backward passes. Uses epsilon value of 1e-05 for numerical stability.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_GPT2_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.native_layer_norm.default((T([4, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-05), {})\naten.native_layer_norm_backward.default((T([4, 512, 768], f16), T([4, 512, 768], f16), [768], T([4, 512, 1], f32), T([4, 512, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: PT2 Post-AOTAutograd FakeTensor Usage\nDESCRIPTION: Demonstrates fake tensor usage in PT2 after AOTAutograd, where fake mode is typically already enabled and example inputs are usually fake.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Fake mode is enabled! example_inputs is typically fake already\n# TODO: we probably want to change this\n# Still do this to access fake mode\nfake_mode = detect_fake_mode(example_inputs)\n# But in general you don't have to turn it on\n```\n\n----------------------------------------\n\nTITLE: Creating and Binding First-class Dimensions\nDESCRIPTION: Examples showing how to create dimension objects and bind them to tensors, demonstrating dimension properties and indexing behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchdim import dims\n\nbatch, channel, width, height = dims(4)\n\ninput = torch.rand(2, 3, 224, 224)\nprint(input.ndim)\n\ninput_fc = input[batch, channel, width, height]\nprint(input_fc.dims)\nprint(input_fc.ndim)\n```\n\n----------------------------------------\n\nTITLE: Foreach Operations List in RestructuredText\nDESCRIPTION: A list of PyTorch's foreach operations that apply functions to multiple tensors simultaneously for better performance. These operations are in beta and do not support Forward-mode automatic differentiation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_5\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. warning::\n    This API is in beta and subject to future changes.\n    Forward-mode AD is not supported.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    _foreach_abs\n    _foreach_abs_\n    _foreach_acos\n    _foreach_acos_\n    _foreach_asin\n    _foreach_asin_\n    _foreach_atan\n    _foreach_atan_\n    _foreach_ceil\n    _foreach_ceil_\n    _foreach_cos\n    _foreach_cos_\n    _foreach_cosh\n    _foreach_cosh_\n    _foreach_erf\n    _foreach_erf_\n    _foreach_erfc\n    _foreach_erfc_\n    _foreach_exp\n    _foreach_exp_\n    _foreach_expm1\n    _foreach_expm1_\n    _foreach_floor\n    _foreach_floor_\n    _foreach_log\n    _foreach_log_\n    _foreach_log10\n    _foreach_log10_\n    _foreach_log1p\n    _foreach_log1p_\n    _foreach_log2\n    _foreach_log2_\n    _foreach_neg\n    _foreach_neg_\n    _foreach_tan\n    _foreach_tan_\n    _foreach_sin\n    _foreach_sin_\n    _foreach_sinh\n    _foreach_sinh_\n    _foreach_round\n    _foreach_round_\n    _foreach_sqrt\n    _foreach_sqrt_\n    _foreach_lgamma\n    _foreach_lgamma_\n    _foreach_frac\n    _foreach_frac_\n    _foreach_reciprocal\n    _foreach_reciprocal_\n    _foreach_sigmoid\n    _foreach_sigmoid_\n    _foreach_trunc\n    _foreach_trunc_\n    _foreach_zero_\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Nonzero Operation in PyTorch\nDESCRIPTION: Example implementation of a custom nonzero operation using PyTorch's library registration system. The function creates a dynamic-sized tensor with shape [nnz, x.dim()] where nnz is determined at runtime.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@torch.library.register_fake(\"mylib::custom_nonzero\")\ndef _(x):\n    nnz = torch.library.get_ctx().new_dynamic_size()\n    shape = [nnz, x.dim()]\n    return x.new_empty(shape, dtype=torch.int64)\n```\n\n----------------------------------------\n\nTITLE: Handling Exception and Return Statements in Python\nDESCRIPTION: Shows how conditional statements with exceptions and returns are handled during compilation. The compiler identifies unreachable code and simplifies the control flow.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nif i < 0:\n  raise Exception(\"Negative input\")\nelse:\n  return math.sqrt(i)\nprint(i)  # unreachable code\n```\n\n----------------------------------------\n\nTITLE: Using GPUDirect Storage in PyTorch\nDESCRIPTION: This example demonstrates how to use the GdsFile class from torch.cuda.gds for direct memory access transfers between GPU memory and storage. It requires CUDA 12.6 or higher and proper system configuration for GPUDirect Storage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.cuda.gds import GdsFile\n\n# Example usage of GdsFile\n# (Placeholder code, actual implementation may vary)\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobians using torch.func.jacrev and functional_call in Python\nDESCRIPTION: Demonstrates the PyTorch 2.0+ method for Jacobian computation using `torch.func`. Parameters are obtained via `model.named_parameters()`. `torch.func.jacrev` is applied directly to `torch.func.functional_call`. `argnums=1` specifies that the Jacobian should be computed with respect to the second argument passed to `functional_call` (the `params` dictionary), not the default first argument (`model`). Requires `torch` and functions from `torch.func`. This replaces the `functorch.jacrev` pattern shown previously.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport torch\nfrom torch.func import jacrev, functional_call\ninputs = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nparams = dict(model.named_parameters())\n# jacrev computes jacobians of argnums=0 by default.\n# We set it to 1 to compute jacobians of params\njacobians = jacrev(functional_call, argnums=1)(model, params, (inputs,))\n```\n\n----------------------------------------\n\nTITLE: PT2 Pre-AOTAutograd FakeTensor Usage\nDESCRIPTION: Shows how to handle fake tensors in PT2 before AOTAutograd, including detecting fake mode and converting arguments to fake tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Fake mode is not enabled!\nfrom torch._guards import detect_fake_mode\nfake_mode = detect_fake_mode(args)\n# if fake_mode isn't None\nconverter = fake_mode.fake_tensor_converter\nfake_args = [converter.from_real_tensor(fake_mode, arg) for arg in args]\nwith fake_mode:\n    ... # do stuff with the fake args, if needed ...\n```\n\n----------------------------------------\n\nTITLE: Creating Meta Tensors within torch.device Context in Python\nDESCRIPTION: Shows how to use the `torch.device('meta')` context manager to ensure that tensor creation operations like `torch.randn` within the context block produce meta tensors by default. This overrides the default device for tensor construction within the scope. Requires the `torch` library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> with torch.device('meta'):\n...     print(torch.randn(30, 30))\n...\ntensor(..., device='meta', size=(30, 30))\n```\n\n----------------------------------------\n\nTITLE: Aligning Tensor Dimensions by Names\nDESCRIPTION: Describes how to align tensor dimensions by their names using Tensor.align_as or Tensor.align_to methods, ensuring compatibility across tensor operations regardless of initial dimension ordering.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef scale_channels(input, scale):\n    scale = scale.refine_names('C')\n    return input * scale.align_as(input)\n\nnum_channels = 3\nscale = torch.randn(num_channels, names=('C',))\nimgs = torch.rand(3, 3, 3, num_channels, names=('N', 'H', 'W', 'C'))\nmore_imgs = torch.rand(3, num_channels, 3, 3, names=('N', 'C', 'H', 'W'))\nvideos = torch.randn(3, num_channels, 3, 3, 3, names=('N', 'C', 'H', 'W', 'D'))\n\nscale_channels(imgs, scale)\nscale_channels(more_imgs, scale)\nscale_channels(videos, scale)\n```\n\n----------------------------------------\n\nTITLE: Constant Folding with Static Inputs in torch.export (Python)\nDESCRIPTION: This snippet demonstrates how `torch.export.export` handles static primitive inputs. When the module `MyMod` is exported with a static integer `3` for `y`, the operation `y + 7` is constant-folded during tracing, resulting in the value `10` being directly embedded into the exported graph. The `add` operation in the final graph uses this constant `10`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass MyMod(torch.nn.Module):\n    def forward(self, x, y):\n        z = y + 7\n        return x + z\n\nm = torch.export.export(MyMod(), (torch.randn(1), 3))\nprint(m.graph_module.code)\n\n\"\"\"\ndef forward(self, arg0_1, arg1_1):\n    add = torch.ops.aten.add.Tensor(arg0_1, 10);  arg0_1 = None\n    return (add,)\n\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Saving and Reloading PowerSGD State and Hook in PyTorch\nDESCRIPTION: This code demonstrates how to save and reload PowerSGD state and hook for DDP models in PyTorch. It includes setting up a distributed environment, creating a simple model, applying PowerSGD hook, saving the state, and reloading it.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/ddp_comm_hooks.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook as powerSGD\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(24,24)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(24,12)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(\n        demo_fn,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True)\n\ndef demo_serialization(rank, world_size):\n    setup(rank, world_size)\n\n    CHECKPOINT = tempfile.gettempdir() + \"/checkpoint.pt\"\n\n    model = SimpleModel().to(rank)\n    ddp_model = DistributedDataParallel(model, device_ids=[rank])\n\n    powersgd_hook = powerSGD.powerSGD_hook\n    powersgd_state = powerSGD.PowerSGDState(process_group=None)\n\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n    ddp_model.register_comm_hook(powersgd_state, powersgd_hook)\n\n    state = {\n        'state_dict': ddp_model.state_dict(),\n        'comm_hook': powersgd_hook,\n        'comm_hook_state': powersgd_state}\n\n    if rank == 0:\n        torch.save(state, CHECKPOINT)\n\n    dist.barrier()\n    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n    checkpoint = torch.load(CHECKPOINT, map_location=map_location)\n\n    new_ddp_model = DistributedDataParallel(SimpleModel().to(rank), device_ids=[rank])\n    new_ddp_model.load_state_dict(checkpoint['state_dict'])\n    powersgd_hook = checkpoint['comm_hook']\n    powersgd_state = checkpoint['comm_hook_state']\n\n    new_ddp_model.register_comm_hook(powersgd_state, powersgd_hook)\n\n    if rank == 0:\n        os.remove(CHECKPOINT)\n\n    cleanup()\n\nif __name__ == \"__main__\":\n    n_gpus = torch.cuda.device_count()\n    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n    world_size = n_gpus\n    run_demo(demo_serialization, world_size)\n```\n\n----------------------------------------\n\nTITLE: Implementing Runtime Type Checking in Python DataPipes\nDESCRIPTION: Shows how to use the @runtime_validation decorator and runtime_validation_disabled context manager for runtime type checking in DataPipe classes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import runtime_validation, runtime_validation_disabled\n\nclass DP(IterDataPipe[Tuple[int, T_co]]):\n    def __init__(self, datasource):\n        self.ds = datasource\n\n    @runtime_validation\n    def __iter__(self):\n        for d in self.ds:\n            yield d\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP([(1, 1), (2, 2), ('3', 3)])\nfor d in dp:\n    print(d)\n```\n\nLANGUAGE: python\nCODE:\n```\nwith runtime_validation_disabled():\n    print(list(dp))\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP([(1, 1), (2, 2), [3, 3]])\nfor d in dp:\n    print(d)\n```\n\nLANGUAGE: python\nCODE:\n```\nwith runtime_validation_disabled():\n    print(list(dp))\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP([(1, 1), (2, '2'), (3, 3.)])\nprint(list(dp))\n```\n\n----------------------------------------\n\nTITLE: Specifying Module Patterns for PackageExporter\nDESCRIPTION: Example of specifying module patterns to intern or extern dependencies in torch.package's PackageExporter. This demonstrates how to include torchvision modules while marking numpy as an external dependency.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nmy_exporter.intern(\"torchvision.**\")\nmy_exporter.extern(\"numpy\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Eager Mode Quantization in PyTorch\nDESCRIPTION: Demonstrates how to implement eager mode quantization using QuantStub and DeQuantStub. The example shows a custom module with manual quantization and dequantization steps.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.ao.quantization.QuantStub()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1)\n        # this module will not be quantized (see `qconfig = None` logic below)\n        self.conv2 = torch.nn.Conv2d(1, 1, 1)\n        self.dequant = torch.ao.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv1(x)\n        # during the convert step, this will be replaced with a\n        # `dequantize` call\n        x = self.dequant(x)\n        x = self.conv2(x)\n        return x\n\nm = M()\nm.qconfig = some_qconfig\n# turn off quantization for conv2\nm.conv2.qconfig = None\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobian of a Function using PyTorch\nDESCRIPTION: This snippet demonstrates how to compute the Jacobian of a simple neural network function using the jacrev function from the torch.func module. It requires PyTorch to be installed and utilizes the torch.nn.Module and torch.randn for defining the model and input, respectively.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.api.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nmodel = torch.nn.Linear(3, 3)\n\ndef f(x):\n    return model(x)\n\nx = torch.randn(3)\njacobian = jacrev(f)(x)\nassert jacobian.shape == (3, 3)\n```\n\n----------------------------------------\n\nTITLE: Defining Constants in TorchScript Using Final Annotation\nDESCRIPTION: This example shows how to mark an attribute of a ScriptModule as constant using the Final[T] annotation. The constant value is computed in the constructor and can be used in the forward method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n\nclass Foo(nn.Module):\n    # `Final` from the `typing_extensions` module can also be used\n    a : torch.jit.Final[int]\n\n    def __init__(self):\n        super().__init__()\n        self.a = 1 + 4\n\n    def forward(self, input):\n        return self.a + input\n\nf = torch.jit.script(Foo())\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Operator Tests with pytest\nDESCRIPTION: Commands to run all tests or specific operator tests using pytest. Shows how to test individual operators like torch.ceil or nn.functional.scaled_dot_product_attention.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# All\npython -m pytest test_ops.py\n\n# To run tests on a specific operator (e.g. torch.ceil):\npython -m pytest test_ops.py -k ceil\n\n# To run tests on a nn operator (e.g. nn.functional.scaled_dot_product_attention):\npython -m pytest test_ops.py -k nn_functional_scaled_dot_product_attention\n```\n\n----------------------------------------\n\nTITLE: Symbolic Tracing of PyTorch Module\nDESCRIPTION: Demonstrates how to use torch.fx.symbolic_trace to create a GraphModule from a custom PyTorch Module. The example shows the creation of a simple Module with various operations and its symbolic tracing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/fx/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass MyModule(torch.nn.Module):\n  def __init__(self) -> None:\n    super().__init__()\n    self.param = torch.nn.Parameter(\n        torch.rand(3, 4))\n    self.linear = torch.nn.Linear(4, 5)\n\n  def forward(self, x):\n    return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\nfrom torch.fx import symbolic_trace\nmodule = MyModule()\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\ninput = torch.rand(3, 4)\ntorch.testing.assert_close(symbolic_traced(input), module(input))\n```\n\n----------------------------------------\n\nTITLE: Gram Matrix Implementation with Dimensions\nDESCRIPTION: Implements a Gram matrix calculation using dimension objects for style transfer applications.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef gram_matrix_new(y):\n    b, c, c2, h, w = dims()\n    r = (y[b, c, h, w] * y[b, c2, h, w]).sum((h, w))\n    r = r / (h.size * w.size)\n    return r.order(b, c, c2)\n\ngram_matrix_new(torch.rand(1, 2, 3, 4))\n```\n\n----------------------------------------\n\nTITLE: Using ATen Tanh and Its Backward Operator in PyTorch: Python\nDESCRIPTION: This code exemplifies the application of ATen's 'tanh' and 'tanh_backward' operators to process tensors with specific dimensions and data types (f16). It focuses on evaluating the tanh function and its backward pass, crucial for neural network activation and backpropagation tasks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.tanh.default\ncnt: 1, ((T([3, 1, 512, 512], f16),), {})\nOperator: aten.tanh_backward.default\ncnt: 1, ((T([3, 1, 512, 512], f16, stride=(0, 0, 0, 0)), T([3, 1, 512, 512], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Executing aten.sum Operator on Tensors in PyTorch\nDESCRIPTION: The aten.sum.SymInt operation is employed for aggregating tensor elements over specified dimensions, significant for computed data summation in PyTorch. Dependents comprise PyTorchs versatile tensor functionalities. Inputs involve target tensors and dimensions for reduction. Outputs result in reduced-dimension tensors summarizing values in listed axes, and inputs must match the expected format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Linear Function Aliases in PyTorch\nDESCRIPTION: Shows two ways to make custom operations easier to use - either through direct aliasing or wrapping in a function with default arguments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Option 1: alias\nlinear = LinearFunction.apply\n\n# Option 2: wrap in a function, to support default args and keyword args.\ndef linear(input, weight, bias=None):\n    return LinearFunction.apply(input, weight, bias)\n```\n\n----------------------------------------\n\nTITLE: Setting up Initial CMake Configuration for PyTorch Edge Test\nDESCRIPTION: Initializes CMake minimum version, sets essential path variables (TORCH_ROOT, TEST_ROOT, OUTPUT_DIRECTORY), finds Python source files using GLOB_RECURSE, includes utility CMake functions, and appends a C++ compiler flag (`-Wno-unused-private-field`) if supported.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.15)\n\nset(TORCH_ROOT ${CMAKE_CURRENT_LIST_DIR}/../..)\nset(TEST_ROOT ${TORCH_ROOT}/test/edge)\nset(OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/out)\nfile(GLOB_RECURSE all_python \"${TORCH_ROOT}/torchgen/*.py\")\ninclude(${TORCH_ROOT}/cmake/public/utils.cmake)\nappend_cxx_flag_if_supported(\"-Wno-unused-private-field\" CMAKE_CXX_FLAGS)\n```\n\n----------------------------------------\n\nTITLE: Creating Asynchronous Tasks with torch.jit.fork() in TorchScript\nDESCRIPTION: Creates an asynchronous task executing func and a reference to the value of the result of this execution. Fork will return immediately.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ntorch.jit.fork()\n```\n\n----------------------------------------\n\nTITLE: FakeTensorProp Usage with GraphModule\nDESCRIPTION: Demonstrates various ways to use FakeTensorProp for propagating fake tensors through a GraphModule, including handling both real and fake inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport FakeTensorProp from torch.fx.passes.fake_tensor_prop\ngm: GraphModule\nreal_inputs: List[Tensor]\nFakeTensorProp(gm).propagate(*real_inputs)\n# This will populate meta['val'] on all the FX nodes with a fake tensor\n# or if you have a preexisting fake mode, you should use it\nFakeTensorProp(gm, mode=fake_mode).propagate(*real_inputs)\n# There is also propagate_dont_convert_inputs if your inputs are already fake\nfake_inputs: List[FakeTensor]\nFakeTensorProp(gm, mode=fake_mode).propagate_dont_convert_inputs(*fake_inputs)\n```\n\n----------------------------------------\n\nTITLE: Vector-Jacobian Product (VJP) Transform in PyTorch\nDESCRIPTION: Shows how to use the vjp transform to compute vector-Jacobian products with cotangents.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import vjp\n\ninputs = torch.randn(3)\nfunc = torch.sin\ncotangents = (torch.randn(3),)\n\noutputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)\n```\n\n----------------------------------------\n\nTITLE: Registering a Symbolic Function for ONNX Export in PyTorch\nDESCRIPTION: Example of how to register a new symbolic function for ONNX export using the internal registration.onnx_symbolic decorator. This snippet demonstrates the registration of the 'reshape' function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/onnx/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@registration.onnx_symbolic\ndef reshape(g, self, shape):\n```\n\n----------------------------------------\n\nTITLE: Correctly Passing Received Shared Tensors via Cloning (Python)\nDESCRIPTION: Shows the correct way to pass the data content of a received shared tensor to another part of the application or another process via a queue. Instead of passing the shared tensor `x` directly, a process-local copy `x_clone` is created using `x.clone()`, and this independent copy is then put into `queue_2`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# you need to create a process-local copy\nx = queue.get()\nx_clone = x.clone()\nqueue_2.put(x_clone)\n```\n\n----------------------------------------\n\nTITLE: Computing Batch Hessian with vmap in PyTorch\nDESCRIPTION: Demonstrates computing batched Hessians using vmap combined with the hessian function. This applies the hessian computation to each input in the batch and stacks the results.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ncompute_batch_hessian = vmap(hessian(predict, argnums=2), in_dims=(None, None, 0))\n\nbatch_hess = compute_batch_hessian(weight, bias, x)\nbatch_hess.shape\n```\n\n----------------------------------------\n\nTITLE: Moving PyTorch Module Parameters to Different Devices\nDESCRIPTION: Demonstrates how to move all parameters of a PyTorch module to a CUDA device and change their precision using the 'to' method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n```\n\n----------------------------------------\n\nTITLE: Out of Memory Exception Handling in PyTorch\nDESCRIPTION: This example demonstrates a strategy to overcome issues in handling out of memory exceptions due to Python's exception handling mechanism retaining stack frames. It suggests moving recovery code outside of the exception block to properly free memory before retrying the operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/faq.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n    oom = False\n    try:\n        run_model(batch_size)\n    except RuntimeError: # Out of memory\n        oom = True\n\n    if oom:\n        for _ in range(batch_size):\n            run_model(1)\n```\n\n----------------------------------------\n\nTITLE: Basic PyTorch Tensor Operation Example\nDESCRIPTION: Demonstrates a simple PyTorch script with tensor operations that can be fused, including addition and ReLU activation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef forward(x):\n    o = x + 1.0\n    o = o.relu()\n    return o\n\nshape = (2, 32, 128, 512)\ninput = torch.rand(*shape).cuda()\nt = torch.jit.script(forward)\n\nwith torch.jit.fuser(\"fuser2\"):\n    for k in range(4):\n        o = t(input)\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a GPipe Schedule in Python\nDESCRIPTION: This snippet demonstrates how to create a GPipe schedule using a PipelineStage and execute it with input data. It shows the process of creating the schedule, preparing input data, and running the pipeline step for different ranks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.pipelining import ScheduleGPipe\n\n# Create a schedule\nschedule = ScheduleGPipe(stage, n_microbatches)\n\n# Input data (whole batch)\nx = torch.randn(batch_size, in_dim, device=device)\n\n# Run the pipeline with input `x`\n# `x` will be divided into microbatches automatically\nif rank == 0:\n    schedule.step(x)\nelse:\n    output = schedule.step()\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading PyTorch Module State\nDESCRIPTION: Shows how to save a trained PyTorch module's state to disk and load it into a new instance of the same module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Save the module\ntorch.save(net.state_dict(), 'net.pt')\n\n...\n\n# Load the module later on\nnew_net = Net()\nnew_net.load_state_dict(torch.load('net.pt'))\n: <All keys matched successfully>\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formulation of Quantization Process in PyTorch\nDESCRIPTION: This snippet defines the mathematical formulation of how data is quantized and dequantized in PyTorch's quantization framework, including the equations for scale and zero point calculation in both symmetric and asymmetric quantization schemes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization-support.rst#2025-04-22_snippet_0\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\begin{aligned}\n    \\text{Quantization:}&\\\\\n    &Q_\\text{out} = \\text{clamp}(x_\\text{input}/s+z, Q_\\text{min}, Q_\\text{max})\\\\\n    \\text{Dequantization:}&\\\\\n    &x_\\text{out} = (Q_\\text{input}-z)*s\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Compiling Function with Shape-Dependent Control Flow in Python\nDESCRIPTION: This snippet demonstrates a function compiled with `torch.compile(dynamic=True)` where the control flow (`if/else`) depends on a calculation involving a tensor's shape (`a.shape[0]`). Dynamo traces this condition, resulting in a guard (`2*L['a'].size()[0] >= 16`) that involves symbolic integers and is checked during execution to select the correct path or trigger a recompile if the condition's outcome changes relative to the trace.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile(dynamic=True)\ndef fn(a):\n    if a.shape[0] * 2 < 16:\n        return a\n    else:\n        return a + 1\n\nfn(torch.randn(8))\n```\n\n----------------------------------------\n\nTITLE: Debugging TorchScript Execution Using Scripting and Tracing (Python)\nDESCRIPTION: Provides an example of integrating scripted, traced, and native Python functions for debugging. Requires PyTorch. Demonstrates how to use the PYTORCH_JIT environment variable to disable JIT ahead-of-time compilation, allowing use of Python debugging tools (pdb) in scripted code. Shows tracing over a function that internally calls a scripted function; illustrates input/output type constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\\ndef scripted_fn(x : torch.Tensor):\\n    for i in range(12):\\n        x = x + x\\n    return x\\n\\ndef fn(x):\\n    x = torch.neg(x)\\n    import pdb; pdb.set_trace()\\n    return scripted_fn(x)\\n\\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\\ntraced_fn(torch.rand(3, 4))\\n\n```\n\n----------------------------------------\n\nTITLE: Simple PyTorch Function Compilation Example\nDESCRIPTION: A basic example showing a function decorated with torch.compile that contains a graph break (the print statement). This demonstrates how Dynamo handles operations before and after a non-PyTorch operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef fn(a):\n    b = a + 2\n    print(\"Hi\")\n    return b + a\n\nfn(torch.randn(4))\n```\n\n----------------------------------------\n\nTITLE: Analytical Evaluation of Complex Gradients in PyTorch\nDESCRIPTION: Mathematical formulation showing the analytical approach to compute the same scalar quantity as the numerical method for complex input gradcheck in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/gradcheck.rst#2025-04-22_snippet_2\n\nLANGUAGE: math\nCODE:\n```\n\\begin{aligned}\n    s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\\\n      &= v^T real(2 * CW) ur + i * v^T imag(2 * CW) ui) \\\\\n      &= real(v^T (2 * CW)) ur + i * imag(v^T (2 * CW)) ui\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Relationship-Based Dynamic Shape Representation in Exported Program\nDESCRIPTION: Output showing how relationships between dynamic dimensions are preserved in the exported program. The example demonstrates one dimension being defined as one larger than another dimension, with corresponding range constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nExportedProgram:\nclass GraphModule(torch.nn.Module):\n    def forward(self, x: \"f32[s0]\", y: \"f32[s0 + 1]\"):\n        # code: return x + y[1:]\n        slice_1: \"f32[s0]\" = torch.ops.aten.slice.Tensor(y, 0, 1, 9223372036854775807)\n        add: \"f32[s0]\" = torch.ops.aten.add.Tensor(x, slice_1)\n        return (add,)\n\nRange constraints: {s0: VR[3, 6], s0 + 1: VR[4, 7]}\n```\n\n----------------------------------------\n\nTITLE: Re-exporting an Imported Object\nDESCRIPTION: Shows how to re-export an object that was previously imported by a PackageImporter, making the new PackageExporter aware of the original PackageImporter to find source code for dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimporter = PackageImporter(f)\nobj = importer.load_pickle(\"model\", \"model.pkl\")\n\n# re-export obj in a new package\nwith PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\n    exporter.save_pickle(\"model\", \"model.pkl\", obj)\n```\n\n----------------------------------------\n\nTITLE: Complete PyTorch Add Operator Benchmark Implementation\nDESCRIPTION: Full implementation of a benchmark for torch.add showing input configuration, tensor creation, and the forward computation. Includes both 'short' and 'long' configurations using different helper methods.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport operator_benchmark as op_bench\nimport torch\n\nadd_long_configs = op_bench.cross_product_configs(\n    M=[8, 64, 128],\n    N=range(2, 10, 3),\n    K=[2 ** x for x in range(0, 3)],\n    tags=[\"long\"]\n)\n\nadd_short_configs = op_bench.config_list(\n    attr_names=[\"M\", \"N\", \"K\"],\n    attrs=[\n        [8, 16, 32],\n        [16, 16, 64],\n        [64, 64, 128],\n    ],\n    tags=[\"short\"],\n)\n\nclass AddBenchmark(op_bench.TorchBenchmarkBase):\n    def init(self, M, N, K, device):\n        self.inputs = {\n            \"input_one\": torch.rand(M, N, K, device=device, requires_grad=self.auto_set()),\n            \"input_two\": torch.rand(M, N, K, device=device, requires_grad=self.auto_set())\n        }\n        self.set_module_name(\"add\")\n\n    def forward(self, input_one, input_two):\n        return torch.add(input_one, input_two)\n\nop_bench.generate_pt_test(add_long_configs + add_short_configs, AddBenchmark)\n\nif __name__ == \"__main__\":\n    op_bench.benchmark_runner.main()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Naive vs. functorch Per-Sample Gradient Calculation\nDESCRIPTION: Uses `torch.utils.benchmark.Timer` to measure the execution time of the naive per-sample gradient calculation (`compute_sample_grads`) and the `functorch`-based calculation (`ft_compute_sample_grad`). `Timer` objects are created for each function call, and `.timeit(100)` runs each statement 100 times to get reliable timing measurements. The results (median times) are stored in `no_vmap_timing` and `with_vmap_timing` and printed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.benchmark import Timer\n\nwithout_vmap = Timer( stmt=\"compute_sample_grads(data, targets)\", globals=globals())\nwith_vmap = Timer(stmt=\"ft_compute_sample_grad(params, buffers, data, targets)\",globals=globals())\nno_vmap_timing = without_vmap.timeit(100)\nwith_vmap_timing = with_vmap.timeit(100)\n\nprint(f'Per-sample-grads without vmap {no_vmap_timing}')\nprint(f'Per-sample-grads with vmap {with_vmap_timing}')\n```\n\n----------------------------------------\n\nTITLE: Using the Hessian Convenience Function in PyTorch\nDESCRIPTION: Demonstrates using the hessian convenience function that combines jacfwd and jacrev transforms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import hessian\n\ndef f(x):\n  return x.sin().sum()\n\nx = torch.randn(5)\nhess = hessian(f)(x)\n```\n\n----------------------------------------\n\nTITLE: Resolving Multiprocessing Errors in Python\nDESCRIPTION: This Python script advises on resolving multiprocessing errors on Windows by wrapping code in an if-statement (protecting main execution) to avoid RuntimeErrors that arise due to Windows's use of 'spawn'. It suggests modifying code structure to set num_worker to zero if needed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef main()\n    for i, data in enumerate(dataloader):\n        # do something here\n\nif __name__ == '__main__':\n    main()\n```\n\n----------------------------------------\n\nTITLE: Using Custom Quantization with Eager Mode\nDESCRIPTION: Example showing how to use custom quantization modules with PyTorch's eager mode quantization workflow including prepare and convert steps.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nm = torch.nn.Sequential(CustomModule()).eval()\nprepare_custom_config_dict = {\n    \"float_to_observed_custom_module_class\": {\n        CustomModule: ObservedCustomModule\n    }\n}\nconvert_custom_config_dict = {\n    \"observed_to_quantized_custom_module_class\": {\n        ObservedCustomModule: StaticQuantCustomModule\n    }\n}\nm.qconfig = torch.ao.quantization.default_qconfig\nmp = torch.ao.quantization.prepare(\n    m, prepare_custom_config_dict=prepare_custom_config_dict)\n# calibration (not shown)\nmq = torch.ao.quantization.convert(\n    mp, convert_custom_config_dict=convert_custom_config_dict)\n```\n\n----------------------------------------\n\nTITLE: Adding CUTLASS Includes and Conditional Static CUDA Dependencies in CMake\nDESCRIPTION: Appends CUTLASS include directories to the ATen CUDA include path list. Conditionally, if the environment variable `ATEN_STATIC_CUDA` is set, it appends static CUDA libraries (cuSPARSE, cuFFT, cuSolver, LAPACK) to the `ATen_CUDA_DEPENDENCY_LIBS` list. The specific cuSolver and LAPACK libraries depend on the CUDA version (<=11 or >=12) and whether `BUILD_LAZY_CUDA_LINALG` is disabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\n  list(APPEND ATen_CUDA_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/cutlass/include)\n  list(APPEND ATen_CUDA_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/cutlass/tools/util/include)\n  if($ENV{ATEN_STATIC_CUDA})\n    list(APPEND ATen_CUDA_DEPENDENCY_LIBS\n      ${CUDA_LIBRARIES}\n      CUDA::cusparse_static\n      CUDA::cufft_static_nocallback\n    )\n   if(NOT BUILD_LAZY_CUDA_LINALG)\n     if(CUDA_VERSION_MAJOR LESS_EQUAL 11)\n       list(APPEND ATen_CUDA_DEPENDENCY_LIBS\n         CUDA::cusolver_static\n         ${CUDAToolkit_LIBRARY_DIR}/liblapack_static.a     # needed for libcusolver_static\n       )\n     elseif(CUDA_VERSION_MAJOR GREATER_EQUAL 12)\n       list(APPEND ATen_CUDA_DEPENDENCY_LIBS\n         CUDA::cusolver_static\n         ${CUDAToolkit_LIBRARY_DIR}/libcusolver_lapack_static.a     # needed for libcusolver_static\n       )\n     endif()\n   endif()\n```\n\n----------------------------------------\n\nTITLE: Accessing and Renaming Tensor Dimensions\nDESCRIPTION: Illustrates how to access and rename named dimensions of a tensor using the Tensor.names and Tensor.rename methods. Supports partial renaming of dimensions and is essential for rearranging dimensions when necessary.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W'))\nimgs.names\nrenamed_imgs = imgs.rename(H='height', W='width')\nrenamed_imgs.names\n```\n\n----------------------------------------\n\nTITLE: Example of DifferentiableGraph in PyTorch JIT\nDESCRIPTION: Shows a graph representation with a DifferentiableGraph node that implements an LSTM cell computation. The graph is split into a forward pass and a complementary backward pass to safely apply optimizations in models requiring gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ngraph(%x : Float(*, *),\n      %hx : Float(*, *),\n      %cx : Float(*, *),\n      %w_ih : Float(*, *),\n      %w_hh : Float(*, *),\n      %b_ih : Float(*),\n      %b_hh : Float(*)):\n  %8 : int = prim::Constant[value=1]()\n  %hy : Float(*, *), %cy : Float(*, *) = prim::DifferentiableGraph_0(%cx, %b_hh, %b_ih, %hx, %w_hh, %x, %w_ih)\n  %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)\n  return (%30)\nwith prim::DifferentiableGraph_0 = graph(%13 : Float(*, *),\n      %29 : Float(*),\n      %33 : Float(*),\n      %40 : Float(*, *),\n      %43 : Float(*, *),\n      %45 : Float(*, *),\n      %48 : Float(*, *)):\n  %49 : Float(*, *) = aten::t(%48)\n  %47 : Float(*, *) = aten::mm(%45, %49)\n  %44 : Float(*, *) = aten::t(%43)\n  %42 : Float(*, *) = aten::mm(%40, %44)\n  %38 : int = prim::Constant[value=1]()\n  %39 : Float(*, *) = aten::add(%47, %42, %38)\n  %35 : Float(*, *) = aten::add(%39, %33, %38)\n  %gates : Float(*, *) = aten::add(%35, %29, %38)\n  %24 : Float(*, *), %25 : Float(*, *), %26 : Float(*, *), %27 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%gates)\n  %ingate : Float(*, *) = aten::sigmoid(%24)\n  %forgetgate : Float(*, *) = aten::sigmoid(%25)\n  %cellgate : Float(*, *) = aten::tanh(%26)\n  %outgate : Float(*, *) = aten::sigmoid(%27)\n  %14 : Float(*, *) = aten::mul(%forgetgate, %13)\n  %11 : Float(*, *) = aten::mul(%ingate, %cellgate)\n  %cy : Float(*, *) = aten::add(%14, %11, %38)\n  %4 : Float(*, *) = aten::tanh(%cy)\n  %hy : Float(*, *) = aten::mul(%outgate, %4)\n  return (%hy, %cy)\n```\n\n----------------------------------------\n\nTITLE: Decomposing BatchNorm for Inference - PyTorch - Python\nDESCRIPTION: Here, the previously exported training program is converted to an inference IR by decomposing operators (such as batch_norm) using a decomposition table. The decomposition table is modified to keep conv2d but decompose other ops to their functional equivalents. Requires a previously exported ExportedProgram and PyTorch's torch.export.default_decompositions API.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Lower to core aten inference IR, but keep conv2d\ndecomp_table = torch.export.default_decompositions()\ndel decomp_table[torch.ops.aten.conv2d.default]\nep_for_inference = ep_for_training.run_decompositions(decomp_table)\n\nprint(ep_for_inference)\n```\n\n----------------------------------------\n\nTITLE: Exporting AlexNet Model to ONNX Format\nDESCRIPTION: Example showing how to export a pretrained AlexNet model to ONNX format using torch.onnx.export. Includes setting input/output names and using CUDA device.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport torchvision\n\ndummy_input = torch.randn(10, 3, 224, 224, device=\"cuda\")\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\n\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\noutput_names = [ \"output1\" ]\n\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\n```\n\n----------------------------------------\n\nTITLE: Using torch.Size to Get and Access Tensor Dimensions in Python\nDESCRIPTION: This example demonstrates how to obtain a torch.Size object from a tensor and how to interact with it using common sequence operations. It shows creating a tensor, getting its size, displaying the full dimensions, accessing individual dimensions, and checking the length of the Size object.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/size.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.ones(10, 20, 30)\n>>> s = x.size()\n>>> s\ntorch.Size([10, 20, 30])\n>>> s[1]\n20\n>>> len(s)\n3\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobian with PyTorch Linear Module\nDESCRIPTION: Demonstrates how to compute the Jacobian of a PyTorch Linear layer using jacrev transform. The example shows creating a 3x3 linear layer, defining a wrapper function, and computing its Jacobian with respect to the input.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/functorch.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = torch.nn.Linear(3, 3)\n\ndef f(x):\n    return model(x)\n\nx = torch.randn(3)\njacobian = jacrev(f)(x)\nassert jacobian.shape == (3, 3)\n```\n\n----------------------------------------\n\nTITLE: Testing Versioned Operators in PyTorch\nDESCRIPTION: Provides an example of how to write a test for a versioned operator, comparing the behavior of old and new models with upgraders applied.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n@settings(max_examples=10, deadline=200000)  # A total of 10 examples will be generated\n@given(\n    sample_input=st.tuples(st.integers(min_value=5, max_value=199), st.floats(min_value=5.0, max_value=199.0))\n)  # Generate a pair (integer, float)\n@example((2, 3, 2.0, 3.0))  # Ensure this example will be covered\ndef test_versioned_div_scalar(self, sample_input):\n    # Step 1. Write down the old behavior of this operator, if possible\n    def historic_div_scalar_float(self, other: float):\n        return torch.true_divide(self, other)\n\n    # Step 2. Write down how current module should look like\n    class MyModuleFloat(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n\n        def forward(self, a, b: float):\n            return a / b\n    try:\n        # Step 3. Load the old model and it will apply upgrader\n        v3_mobile_module_float = _load_for_lite_interpreter(\n            pytorch_test_dir + \"/jit/fixtures/test_versioned_div_scalar_float_v2.ptl\")\n        v3_server_module_float = torch.jit.load(\n            pytorch_test_dir + \"/jit/fixtures/test_versioned_div_scalar_float_v2.ptl\")\n    except Exception as e:\n        self.skipTest(\"Failed to load fixture!\")\n\n    # Step4. Load the new model and it won't apply the upgrader\n    current_mobile_module_float = self._save_load_mobile_module(MyModuleFloat)\n    current_server_module_float = self._save_load_module(MyModuleFloat)\n\n    for val_a, val_b in product(sample_input, sample_input):\n        a = torch.tensor((val_a,))\n        b = val_b\n\n        def _helper(m, fn):\n            m_result = self._try_fn(m, a, b)\n            fn_result = self._try_fn(fn, a, b)\n\n            if isinstance(m_result, Exception):\n                self.assertTrue(fn_result, Exception)\n            else:\n                self.assertEqual(m_result, fn_result)\n\n        # Ensure the module loaded from the old model with upgrader\n        # has the same result as the module loaded from the new model\n        _helper(v3_mobile_module_float, current_mobile_module_float)\n        _helper(v3_mobile_module_float, current_server_module_float)\n\n        # Ensure the module loaded from the new model with upgrader\n        # has the same result as the module loaded from the new model\n        _helper(current_mobile_module_float, torch.div)\n        _helper(current_server_module_float, torch.div)\n```\n\n----------------------------------------\n\nTITLE: Converting Between Real and Complex Representations\nDESCRIPTION: Shows how to convert between real tensors and complex tensors using view_as_complex and view_as_real functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nx = torch.randn(3, 2)\nx\ny = torch.view_as_complex(x)\ny\ntorch.view_as_real(y)\n```\n\n----------------------------------------\n\nTITLE: Shape Specialization in torch.export with Branching Logic\nDESCRIPTION: Demonstration of how torch.export specializes on input tensor shapes by default. This example shows how shape-dependent control flow is specialized based on the example inputs, resulting in only one branch being included in the exported program.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.export import export\n\nclass Mod(torch.nn.Module):\n    def forward(self, x):\n        if x.shape[0] > 5:\n            return x + 1\n        else:\n            return x - 1\n\nexample_inputs = (torch.rand(10, 2),)\nexported_program = export(Mod(), example_inputs)\nprint(exported_program)\n```\n\n----------------------------------------\n\nTITLE: Controlling TF32 Usage for Matmul and cuDNN in PyTorch (Python)\nDESCRIPTION: Shows how to enable or disable the use of TensorFloat-32 (TF32) tensor cores for matrix multiplications (matmul) and cuDNN operations in PyTorch. Setting these flags allows trading precision for performance on compatible NVIDIA GPUs (Ampere and later).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n# in PyTorch 1.12 and later.\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\ntorch.backends.cudnn.allow_tf32 = True\n```\n\n----------------------------------------\n\nTITLE: Logging PyTorch Model and Images to TensorBoard\nDESCRIPTION: Demonstrates basic TensorBoard integration by logging MNIST dataset images and a ResNet model graph. Shows initialization of SummaryWriter, data loading with transforms, model setup, and logging images and model graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensorboard.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\n\n# Writer will output to ./runs/ directory by default\nwriter = SummaryWriter()\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\nmodel = torchvision.models.resnet50(False)\n# Have ResNet model take in grayscale rather than RGB\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nimages, labels = next(iter(trainloader))\n\ngrid = torchvision.utils.make_grid(images)\nwriter.add_image('images', grid, 0)\nwriter.add_graph(model, images)\nwriter.close()\n```\n\n----------------------------------------\n\nTITLE: Function with Non-Data-Dependent Control Flow under vmap (Supported) - PyTorch - Python\nDESCRIPTION: This example shows a function using control flow that depends on meta-data (tensor shape) rather than tensor values, which works correctly under vmap. custom_dot checks the dimension of its input and applies different operations accordingly. Dependencies: PyTorch, torch.func. This pattern is safe under vmap since the control flow does not depend on batched data values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef custom_dot(x):\n  if x.dim() == 1:\n    return torch.dot(x, x)\n  return (x * x).sum()\n\nx = torch.randn(3)\nvmap(custom_dot)(x)\n```\n\n----------------------------------------\n\nTITLE: Inspecting TorchScript-Compiled Code Using .code Attribute (Python)\nDESCRIPTION: Demonstrates inspection of the Python-like compiled TorchScript code using the 'code' attribute of a scripted function. Requires PyTorch. The scripted function 'foo' applies a loop and conditional logic on a tensor. Printing 'foo.code' outputs the synthetic, pretty-printed representation of TorchScript's internal code for debugging and verification.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\\ndef foo(len):\\n    # type: (int) -> torch.Tensor\\n    rv = torch.zeros(3, 4)\\n    for i in range(len):\\n        if i < 10:\\n            rv = rv - 1.0\\n        else:\\n            rv = rv + 1.0\\n    return rv\\n\\nprint(foo.code)\\n\n```\n\n----------------------------------------\n\nTITLE: Dynamic Shape Configuration for PyTorch JIT\nDESCRIPTION: Example code showing how to configure PyTorch's JIT fusion strategy to control static vs dynamic shape handling. This affects how the graph executor specializes graphs for different input shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ntorch._C._jit_set_fusion_strategy([\n    (\"STATIC\", 2),\n    (\"DYNAMIC\", 20),\n])\n```\n\n----------------------------------------\n\nTITLE: TorchScript Aliasing Example\nDESCRIPTION: Demonstrates alias analysis in TorchScript with a function containing tensor operations, control flow, and potential aliasing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\ndef foo(a : Tensor, b : Tensor):\n  c = 2 * b\n  a += 1\n  if a.max() > 4:\n    r = a[0]\n  else:\n    r = b[0]\n  return c, r\n```\n\n----------------------------------------\n\nTITLE: Python In-place Operation Tracing Example\nDESCRIPTION: Shows how to handle in-place tensor operations during tracing by replacing them with out-of-place alternatives.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef fill_row_zero(x):\n    x[0] = torch.rand(*x.shape[1:2])\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph)\n\n# Fixed version:\ndef fill_row_zero(x):\n    x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)\n    return x\n```\n\n----------------------------------------\n\nTITLE: Creating Tensor Views in TorchScript\nDESCRIPTION: Demonstrates how TorchScript creates tensor views through indexing operations. Shows how multiple tensor variables can reference the same underlying data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nt = torch.rand(3, 4)\nt2 =  t[0] # view of one slice of t\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with Bias Addition in PyTorch\nDESCRIPTION: This code shows the addmm operation which performs matrix multiplication followed by addition with a bias vector. This operation is typically used in fully connected layers of neural networks, combining weights, input, and bias in one operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([65], f16), T([96, 512], f16), T([512, 65], f16, stride=(1, 512))), {})\n```\n\n----------------------------------------\n\nTITLE: Conceptual Implementation of Dynamo's Compilation Process\nDESCRIPTION: This snippet provides a conceptual implementation of how Dynamo transforms a user function. It shows the high-level logic of how guard functions are checked and compiled code is executed, falling back to recompilation when necessary.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef compiled_example(a, b):\n    L = {'a': a, 'b': b}\n    for guard, code in get_cache_entries():\n        if guard(L):\n            return code(a, b)\n    recompile_and_add_another_cache_entry()\n```\n\n----------------------------------------\n\nTITLE: Intel ITT Integration Documentation\nDESCRIPTION: RST documentation for Intel Instrumentation and Tracing Technology (ITT) related functions in PyTorch profiler.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/profiler.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\nIntel Instrumentation and Tracing Technology APIs\n-------------------------------------------------\n\n.. autofunction:: torch.profiler.itt.is_available\n\n.. autofunction:: torch.profiler.itt.mark\n\n.. autofunction:: torch.profiler.itt.range_push\n\n.. autofunction:: torch.profiler.itt.range_pop\n\n.. This module needs to be documented. Adding here in the meantime\n.. for tracking purposes\n.. py:module:: torch.profiler.itt\n.. py:module:: torch.profiler.profiler\n.. py:module:: torch.profiler.python_tracer\n```\n\n----------------------------------------\n\nTITLE: Dimension Index Arithmetic in PyTorch\nDESCRIPTION: Creates a matrix mask using dimension indices to generate an upper triangular matrix pattern.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torchdim import dims\ni, j = dims(sizes=[4, 4])\nprint(i <= j)\n```\n\n----------------------------------------\n\nTITLE: Implementing DataPipe Constructor in Python\nDESCRIPTION: Constructor implementation for a MapperIterDataPipe class that takes a source DataPipe and mapping function as arguments. Demonstrates proper initialization pattern for DataPipes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MapperIterDataPipe(IterDataPipe):\n    def __init__(self, dp, fn):\n        super().__init__()\n        self.dp = dp\n        self.fn = fn\n```\n\n----------------------------------------\n\nTITLE: Basic ATen Tensor Operations in C++\nDESCRIPTION: Demonstrates fundamental tensor operations using ATen library including tensor creation and arithmetic operations. Shows usage of ones(), randn() and type conversion with to().\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <ATen/ATen.h>\n\nat::Tensor a = at::ones({2, 2}, at::kInt);\nat::Tensor b = at::randn({2, 2});\nauto c = a + b.to(at::kInt);\n```\n\n----------------------------------------\n\nTITLE: Using Annotations for Tensor Mutation and Aliasing\nDESCRIPTION: This code snippet illustrates the use of annotations to denote mutation and aliasing of Tensor objects. It covers different cases such as in-place operations and output arguments and explains the requirements for using annotations like '(a!)'. Examples of function schemas with annotations are provided, detailing changes as PyTorch moves towards schema unification.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nabs(Tensor self) -> Tensor\n```\n\nLANGUAGE: C++\nCODE:\n```\nabs_(Tensor(a!) self) -> Tensor(a!)\n```\n\nLANGUAGE: C++\nCODE:\n```\nabs(Tensor self, *, Tensor(a!) out) -> Tensor(a!)\n```\n\nLANGUAGE: C++\nCODE:\n```\ntranspose(Tensor(a) self, int dim0, int dim1) -> Tensor(a)\n```\n\nLANGUAGE: C++\nCODE:\n```\nfunc: chunk(Tensor(a -> *) self, int chunks, int dim=0) -> Tensor(a)[]\n```\n\n----------------------------------------\n\nTITLE: Implementing Hessian-Vector Product Using Double Reverse-Mode AD in PyTorch\nDESCRIPTION: Provides an alternative HVP implementation using reverse-mode AD twice. This approach is useful when PyTorch forward-AD doesn't have coverage for specific operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef hvp_revrev(f, primals, tangents):\n  _, vjp_fn = vjp(grad(f), *primals)\n  return vjp_fn(*tangents)\n```\n\n----------------------------------------\n\nTITLE: Handling Dispatch Operations in PyTorch\nDESCRIPTION: This snippet logs a sequence of dispatch operations performed during the backward pass of neural networks in PyTorch. It includes creating initial gradients, expanding them, multiplying and detaching tensors. These operations rely on the PyTorch library and require tensor objects. The input includes tensors and certain flags such as 'pin_memory', outputs are intermediary results stored in tensors. Constraints include maintaining correct tensor shapes and memory formats.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n# Initial gradient creation using ones_like\\nDispatch Log: aten.ones_like.default(*(tensor(11.4637, grad_fn=<SumBackward0>),), **{'pin_memory': False, 'memory_format': torch.preserve_format})\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Expanding tensor during backward\\nDispatch Log: aten.expand.default(*(tensor(1.), [10]), **{})\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Multiplying tensor elements\\nDispatch Log: aten.mul.Tensor(*(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 2), **{})\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Detaching tensor to stop gradient tracking\\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})\n```\n\nLANGUAGE: Python\nCODE:\n```\n# Detaching tensor to stop gradient tracking\\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})\n```\n\n----------------------------------------\n\nTITLE: Using CUDA Graphs with PyTorch Modules\nDESCRIPTION: This snippet demonstrates how to use CUDA Graphs with PyTorch modules. It shows the process of creating graphed callables, running forward and backward passes, and updating the optimizer.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\nmodule3 = torch.cuda.make_graphed_callables(module3, (h,))\n\nreal_inputs = [torch.rand_like(x) for _ in range(10)]\nreal_targets = [torch.randn(N, D_out, device=\"cuda\") for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    optimizer.zero_grad(set_to_none=True)\n\n    tmp = module1(data)  # forward ops run as a graph\n\n    if tmp.sum().item() > 0:\n        tmp = module2(tmp)  # forward ops run as a graph\n    else:\n        tmp = module3(tmp)  # forward ops run as a graph\n\n    loss = loss_fn(tmp, target)\n    # module2's or module3's (whichever was chosen) backward ops,\n    # as well as module1's backward ops, run as graphs\n    loss.backward()\n    optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Registering PyTorch Kernels for Multiple Backends (C++)\nDESCRIPTION: Shows how to register different kernel implementations for the same operator (`my_namespace::my_op`) targeting different backends (CPU and CUDA). Two separate `.op()` calls (or chained calls) are used, each specifying a different kernel function and the corresponding dispatch key (`CPU()` or `CUDA()`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace {\nTensor my_kernel_cpu(const Tensor& a, const Tensor& b) {...}\nTensor my_kernel_cuda(const Tensor& a, const Tensor& b) {...}\n}\n\nstatic auto registry = torch::RegisterOperators()\n   .op(\"my_namespace::my_op\",  torch::RegisterOperators::options()\n       .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()))\n   .op(\"my_namespace::my_op\",  torch::RegisterOperators::options()\n       .kernel<decltype(my_kernel_cuda), &my_kernel_cuda>(CUDA()));\n```\n\n----------------------------------------\n\nTITLE: Aggregation Function Implementation in PyTorch\nDESCRIPTION: Example implementation of an aggregation function that combines two tensors by addition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef aggregate_fn(tensor1, tensor2):\n    return tensor1 + tensor2\n```\n\n----------------------------------------\n\nTITLE: Using Functorch's Batch Norm Patching Utility in PyTorch\nDESCRIPTION: Applies functorch's utility function to automatically replace all BatchNorm modules in a network with versions that don't track running statistics, modifying the network in-place.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/batch_norm.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch.experimental import replace_all_batch_norm_modules_\nreplace_all_batch_norm_modules_(net)\n```\n\n----------------------------------------\n\nTITLE: Implementing Iterator for MapperIterDataPipe\nDESCRIPTION: Implementation of the __iter__ method for MapperIterDataPipe that consumes data from source DataPipe and applies the mapping function before yielding results.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MapperIterDataPipe(IterDataPipe):\n    ...\n\n    def __iter__(self):\n        for d in self.dp:\n            yield self.fn(d)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Concatenation\nDESCRIPTION: Concatenation operations on tensors along dimension 1 with varying shapes and channels\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.cat.default([T([64, 96, 112, 112], f16), T([64, 96, 112, 112], f16)], 1)\n```\n\n----------------------------------------\n\nTITLE: TorchScript Optimized Graph Output\nDESCRIPTION: Example of optimized TorchScript IR showing fused CUDA operations in a CudaFusionGroup.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n  Optimized Graph:\n  graph(%x.1 : Tensor):\n    %12 : bool = prim::CudaFusionGuard[types=[Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0)]](%x.1)\n    %11 : Tensor = prim::If(%12)\n      block0():\n        %o.8 : Tensor = prim::CudaFusionGroup_0[cache_id=0](%x.1)\n        -> (%o.8)\n      block1():\n        %18 : Function = prim::Constant[name=\"fallback_function\", fallback=1]()\n        %19 : (Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0)) = prim::CallFunction(%18, %x.1)\n        %20 : Float(2, 32, 128, 512, strides=[2097152, 65536, 512, 1], requires_grad=0, device=cuda:0) = prim::TupleUnpack(%19)\n        -> (%20)\n    return (%11)\n```\n\n----------------------------------------\n\nTITLE: Mixed Dimension Transposition in PyTorch\nDESCRIPTION: Demonstrates working with mixed positional and first-class dimensions during transposition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nB = torch.rand(3, 4, 5)\nB_T = B[i, j].order(j, i)\nassert torch.allclose(B.permute(1, 0, 2), B_T)\n```\n\n----------------------------------------\n\nTITLE: FX Graph Mode Quantization Example\nDESCRIPTION: Shows implementation of post-training quantization and quantization-aware training using FX Graph Mode. Includes dynamic, static, and QAT approaches with proper configuration setup.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom torch.ao.quantization import (\n  get_default_qconfig_mapping,\n  get_default_qat_qconfig_mapping,\n  QConfigMapping,\n)\nimport torch.ao.quantization.quantize_fx as quantize_fx\nimport copy\n\nmodel_fp = UserModel()\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_to_quantize.eval()\nqconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_dynamic_qconfig)\nexample_inputs = (input_fp32)\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n```\n\n----------------------------------------\n\nTITLE: Installing Decompilation Hooks with Depyf\nDESCRIPTION: Shows how to install decompilation hooks using the depyf library to convert Python bytecode into human-readable source code for better debugging of TorchDynamo's behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport depyf\ndepyf.install()\n```\n\n----------------------------------------\n\nTITLE: Logging Tensor Subclass Implementation\nDESCRIPTION: Implementation of a LoggingTensor subclass that logs all function and method calls except __repr__ to avoid infinite recursion.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass LoggingTensor(torch.Tensor):\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion\n        if func is not torch.Tensor.__repr__:\n            logging.info(f\"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}\")\n        if kwargs is None:\n            kwargs = {}\n        return super().__torch_function__(func, types, args, kwargs)\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Streams in PyTorch C++\nDESCRIPTION: This snippet shows how to set the current CUDA stream using the setCurrentCUDAStream function. It also mentions the CUDAStreamGuard as a recommended alternative for managing stream and device context.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nvoid setCurrentCUDAStream(CUDAStream stream);\n```\n\n----------------------------------------\n\nTITLE: Serializing and Loading Exported PyTorch Programs\nDESCRIPTION: Example of saving an ExportedProgram to disk using torch.export.save and loading it back with torch.export.load. The convention is to use the .pt2 file extension for exported programs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport io\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nexported_program = torch.export.export(MyModule(), torch.randn(5))\n\ntorch.export.save(exported_program, 'exported_program.pt2')\nsaved_exported_program = torch.export.load('exported_program.pt2')\n```\n\n----------------------------------------\n\nTITLE: Registering Tensor Hooks Example in PyTorch\nDESCRIPTION: Demonstrates how to register pack and unpack hooks on a saved tensor using grad_fn attributes in PyTorch's autograd system.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\ny.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)\n```\n\n----------------------------------------\n\nTITLE: Using torch.device as a Context Manager for Default Device Selection\nDESCRIPTION: This code snippet demonstrates how to use torch.device as a context manager to change the default device that tensors are allocated on within a specific code block, and verifies the device assignment with the device property.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> with torch.device('cuda:1'):\n...     r = torch.randn(2, 3)\n>>> r.device\ndevice(type='cuda', index=1)\n```\n\n----------------------------------------\n\nTITLE: Converting Dense to COO Sparse Tensor in PyTorch\nDESCRIPTION: Demonstrates converting a 2D dense tensor to a COO (Coordinate Format) sparse tensor, storing only non-zero elements and their indices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\na = torch.tensor([[0, 2.], [3, 0]])\na.to_sparse()\n```\n\n----------------------------------------\n\nTITLE: AOTAutograd Backend Implementation\nDESCRIPTION: Example of implementing a custom backend with AOTAutograd support for model training, using boxed functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._dynamo.backends.common import aot_autograd\nfrom functorch.compile import make_boxed_func\n\ndef my_compiler(gm, example_inputs):\n    return make_boxed_func(gm.forward)\n\nmy_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n\nmodel_opt = torch.compile(model, backend=my_backend)\n```\n\n----------------------------------------\n\nTITLE: L2 Gradient Penalty without AMP or GradScaler (Python)\nDESCRIPTION: This snippet presents a reference implementation of L2 gradient penalty calculation during training without the use of automatic mixed precision or gradient scaling. It uses torch.autograd.grad to compute the gradients, sums their norms to add as a penalty to the loss, and performs a standard backward pass. This is useful for comparison or as a baseline when AMP is not required.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n\n        # Creates gradients\n        grad_params = torch.autograd.grad(outputs=loss,\n                                          inputs=model.parameters(),\n                                          create_graph=True)\n\n        # Computes the penalty term and adds it to the loss\n        grad_norm = 0\n        for grad in grad_params:\n            grad_norm += grad.pow(2).sum()\n        grad_norm = grad_norm.sqrt()\n        loss = loss + grad_norm\n\n        loss.backward()\n\n        # clip gradients here, if desired\n\n        optimizer.step()\n\n```\n\n----------------------------------------\n\nTITLE: Using Learning Rate Scheduler in PyTorch Training Loop\nDESCRIPTION: This snippet demonstrates the typical usage of a learning rate scheduler within a PyTorch training loop. It shows how to initialize the scheduler and call its step() method after each epoch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nscheduler = ...\nfor epoch in range(100):\n    train(...)\n    validate(...)\n    scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Creating a DTensor from a logical torch.Tensor\nDESCRIPTION: Uses the distribute_tensor function to create a DTensor from a logical torch.Tensor on each rank. This is useful for sharding model parameters, buffers, and inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndistribute_tensor(tensor, device_mesh, placements)\n```\n\n----------------------------------------\n\nTITLE: Example of FSDP2 Application to nn.Linear\nDESCRIPTION: Illustrates how FSDP2 is applied to a specific PyTorch module (nn.Linear). It shows that calling fully_shard on a Linear module creates a new FSDPLinear class.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.fsdp.fully_shard.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfully_shard(linear)\n```\n\n----------------------------------------\n\nTITLE: Defining BackendConfig for Custom Quantization Backend in Python\nDESCRIPTION: This snippet demonstrates how to create a BackendConfig object with multiple BackendPatternConfigs for different operator patterns. It includes configurations for Linear, Conv2d+ReLU fusion, and quantized ConvReLU2d.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/backend_config/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.ao.quantization.backend_config import (\n    BackendConfig,\n    BackendPatternConfig,\n    DTypeConfig,\n    ObservationType,\n)\n\nweighted_int8_dtype_config = DTypeConfig(\n    input_dtype=torch.quint8,\n    output_dtype=torch.quint8,\n    weight_dtype=torch.qint8,\n    bias_dtype=torch.float)\n\ndef fuse_conv2d_relu(is_qat, conv, relu):\n    \"\"\"Return a fused ConvReLU2d from individual conv and relu modules.\"\"\"\n    return torch.ao.nn.intrinsic.ConvReLU2d(conv, relu)\n\n# For quantizing Linear\nlinear_config = BackendPatternConfig(torch.nn.Linear) \\\n    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\n    .add_dtype_config(weighted_int8_dtype_config) \\\n    .set_root_module(torch.nn.Linear) \\\n    .set_qat_module(torch.ao.nn.qat.Linear) \\\n    .set_reference_quantized_module(torch.ao.nn.quantized.reference.Linear)\n\n# For fusing Conv2d + ReLU into ConvReLU2d\nconv_relu_config = BackendPatternConfig((torch.nn.Conv2d, torch.nn.ReLU)) \\\n    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\n    .add_dtype_config(weighted_int8_dtype_config) \\\n    .set_fused_module(torch.ao.nn.intrinsic.ConvReLU2d) \\\n    .set_fuser_method(fuse_conv2d_relu)\n\n# For quantizing ConvReLU2d\nfused_conv_relu_config = BackendPatternConfig(torch.ao.nn.intrinsic.ConvReLU2d) \\\n    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\n    .add_dtype_config(weighted_int8_dtype_config) \\\n    .set_root_module(torch.nn.Conv2d) \\\n    .set_qat_module(torch.ao.nn.intrinsic.qat.ConvReLU2d) \\\n    .set_reference_quantized_module(torch.ao.nn.quantized.reference.Conv2d)\n\nbackend_config = BackendConfig(\"my_backend\") \\\n    .set_backend_pattern_config(linear_config) \\\n    .set_backend_pattern_config(conv_relu_config) \\\n    .set_backend_pattern_config(fused_conv_relu_config)\n```\n\n----------------------------------------\n\nTITLE: TorchScript Compilation with AOT Function\nDESCRIPTION: Shows how to use TorchScript as a compiler backend for AOT Autograd, including scripting and freezing the model for inference optimization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    return x.cos().cos()\n\ndef ts_compiler(fx_g: fx.GraphModule, inps):\n    f = torch.jit.script(fx_g)\n    print(f.graph)\n    f = torch.jit.freeze(f.eval())\n    return f\n\naot_function(f, ts_compiler, ts_compiler)(torch.randn(3, requires_grad=True))\n```\n\n----------------------------------------\n\nTITLE: Manipulating Package Contents with Python's zipfile Module\nDESCRIPTION: Demonstrates how to read and modify the contents of a torch package using Python's zipfile module. This allows programmatic access to read files and write modified content back into the package.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom zipfile import ZipFile\nwith ZipFile(\"my_package.pt\") as myzip:\n    file_bytes = myzip.read(\"torchvision/models/resnet.py\")\n    # edit file_bytes in some way\n    myzip.writestr(\"torchvision/models/resnet.py\", new_file_bytes)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Events Handler for TorchElastic in Python\nDESCRIPTION: This snippet shows how to implement a custom events handler by extending the EventHandler class and configuring it in a custom launcher to process and record events during TorchElastic execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/customization.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# my_launcher.py\n\nimport torch.distributed.elastic.events as events\n\nclass MyEventHandler(events.EventHandler):\n    def record(self, event: events.Event):\n        # process event\n\ndef main():\n  events.configure(MyEventHandler())\n\n  spec = WorkerSpec(...)\n  agent = LocalElasticAgent(spec)\n  agent.run()\n```\n\n----------------------------------------\n\nTITLE: Unbatching Data with PyTorch DataPipes\nDESCRIPTION: Shows how to use the unbatch() method to reverse batching operations. Demonstrates unbatching at different levels using the unbatch_level argument.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3).shuffle().unbatch()\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(40).batch(2).batch(4).batch(3).unbatch(unbatch_level = 2)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(40).batch(2).batch(4).batch(3).unbatch(unbatch_level = -1)\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Clearing cuFFT Plan Cache in PyTorch CUDA Backend\nDESCRIPTION: This code snippet shows how to clear the cuFFT plan cache using the clear() method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/backends.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclear()\n```\n\n----------------------------------------\n\nTITLE: Creating and Using RRef with Remote Call in PyTorch\nDESCRIPTION: This code demonstrates creating an RRef using torch.distributed.rpc.remote to execute torch.add on worker B, then retrieving the result using to_here(). This snippet shows the basic RRef usage pattern where a UserRRef is created on worker A and an OwnerRRef is created on worker B.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/rref.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed.rpc as rpc\n\n# on worker A\nrref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n# say the rref has RRefId 100 and ForkId 1\nrref.to_here()\n```\n\n----------------------------------------\n\nTITLE: Compiling PyTorch Training Loop\nDESCRIPTION: Example showing how to optimize a training step with torch.compile, including gradient computation and optimization\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# training\nmodel = ...\nopt = torch.optim.Adam(model.parameters())\n\n@torch.compile\ndef train(mod, data):\n    opt.zero_grad(True)\n    pred = mod(data[0])\n    loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n    loss.backward()\n    opt.step()\n\nfor _ in range(N_ITERS):\n    inp = ...\n    train(model, inp)\n```\n\n----------------------------------------\n\nTITLE: Sparse Tensor Operations in PyTorch\nDESCRIPTION: Demonstrates operation support for sparse tensors, showing both supported (sin) and unsupported (cos) operations on CSR format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nb = torch.tensor([[0, 0, 1, 2, 3, 0], [4, 5, 0, 6, 0, 0]])\nb_s = b.to_sparse_csr()\nb_s.cos()\nb_s.sin()\n```\n\n----------------------------------------\n\nTITLE: Basic functorch Sanity Check Example\nDESCRIPTION: Simple example demonstrating basic functorch usage with vmap transform on sine function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom functorch import vmap\nx = torch.randn(3)\ny = vmap(torch.sin)(x)\nassert torch.allclose(y, x.sin())\n```\n\n----------------------------------------\n\nTITLE: Calling Registered Quantized Operator from C++ via Dispatcher (Unsupported)\nDESCRIPTION: Illustrates an *officially unsupported* method for calling a registered quantized operator (`quantized::xand`) directly from C++ code. It involves finding the operator schema using `c10::Dispatcher::singleton().findSchema` and then invoking it using `op.call<Tensor, Tensor, Tensor>()`. This approach bypasses the standard Python API and interacts directly with the dispatcher.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_9\n\nLANGUAGE: c++\nCODE:\n```\n  Tensor quantized_xand(Tensor qa, Tensor qb) {\n    static const c10::OperatorHandle op = c10::Dispatcher::singleton().findSchema({\"quantized::xand\", \"\"}).value();\n    return op.call<Tensor, Tensor, Tensor>(qa, qb);\n  }\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Tensor Strides in PyTorch\nDESCRIPTION: This code snippet demonstrates how to create a tensor and inspect its strides. It also shows how the strides change when the tensor is transposed. This illustrates the concept of strided memory layout in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)\n```\n\n----------------------------------------\n\nTITLE: Removing Detach Operations in PyTorch FX Graph\nDESCRIPTION: This transformer removes torch.ops.aten.detach.default and torch.ops.aten.detach_copy.default operations from a PyTorch FX graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass RemoveDetachPass(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        if target not in (\n            torch.ops.aten.detach.default,\n            torch.ops.aten.detach_copy.default,\n        ):\n            return super().call_function(target, args, kwargs, meta)\n\n        assert len(args) == 1\n        return args[0]\n\ntransformed_graph_module = RemoveDetachPass(graph_module).transform()\n```\n\n----------------------------------------\n\nTITLE: Performing Single-GPU Offline GEMM Tuning with PyTorch (Python)\nDESCRIPTION: Python script for the second step of offline tuning on a single GPU. It programmatically sets environment variables to enable TunableOp (`PYTORCH_TUNABLEOP_ENABLED=1`) and tuning (`PYTORCH_TUNABLEOP_TUNING=1`), disables further recording of untuned ops (`PYTORCH_TUNABLEOP_RECORD_UNTUNED=0`), and then calls `torch.cuda.tunable.tune_gemm_in_file` to read untuned GEMM configurations from the specified input CSV (`tunableop_untuned0.csv`), perform the tuning benchmarks, and write the results to an output CSV (`tunableop_results0.csv`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch.cuda.tunable as tunable\nimport os\n\nos.putenv('PYTORCH_TUNABLEOP_ENABLED', '1')\nos.putenv('PYTORCH_TUNABLEOP_TUNING', '1')\nos.putenv('PYTORCH_TUNABLEOP_RECORD_UNTUNED', '0')\ntunable.tune_gemm_in_file(\"tunableop_untuned0.csv\")\n```\n\n----------------------------------------\n\nTITLE: Linear Model with vmap Transform\nDESCRIPTION: Example showing how to use vmap to handle batch dimensions in a simple linear model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import vmap\nbatch_size, feature_size = 3, 5\nweights = torch.randn(feature_size, requires_grad=True)\n\ndef model(feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\nexamples = torch.randn(batch_size, feature_size)\nresult = vmap(model)(examples)\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch, functorch, and Setting Random Seed - Python\nDESCRIPTION: This snippet imports essential PyTorch and functorch modules, including neural network layers and functional tools, and sets a manual random seed for reproducibility. Dependencies required include PyTorch and functorch. This is the setup required for all subsequent model, data, and transformation operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\ntorch.manual_seed(0);\n```\n\n----------------------------------------\n\nTITLE: Defining and Compiling a Function with torch.compile in Python\nDESCRIPTION: This snippet defines a simple Python function `fn` that performs element-wise multiplication scaled by the first dimension of the input tensor 'a'. It then uses `torch.compile` to optimize this function. The function is called twice with tensors of different shapes to demonstrate Dynamo's tracing behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a.shape[0] * a * b\n\nfn(torch.randn(4, 3), torch.randn(4, 3))\nfn(torch.randn(8, 3), torch.randn(8, 3))\n```\n\n----------------------------------------\n\nTITLE: Basic GradScaler Usage with PyTorch AMP\nDESCRIPTION: Shows the basic usage of GradScaler for stepping and updating optimizers in mixed precision training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nscaler.step(optimizer)\nscaler.update()\n```\n\n----------------------------------------\n\nTITLE: Graph Breaks Profiling Example\nDESCRIPTION: Shows how to identify graph breaks using torch.profiler by creating synthetic breaks in a model's execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch._dynamo\n\ndevice = 'cuda'\n\nclass ModelWithBreaks(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        def create_sequential():\n            return torch.nn.Sequential(\n                torch.nn.Linear(128, 128),\n                torch.nn.ReLU(),\n                torch.nn.Linear(128, 128),\n                torch.nn.ReLU(),\n            )\n        self.mod1 = create_sequential()\n        self.mod2 = create_sequential()\n        self.mod3 = create_sequential()\n        self.mod4 = create_sequential()\n\n    def forward(self, inp):\n        mod1 = self.mod1(inp)\n        torch._dynamo.graph_break()\n        mod2 = self.mod2(mod1)\n        torch._dynamo.graph_break()\n        mod3 = self.mod3(mod2)\n        torch._dynamo.graph_break()\n        mod4 = self.mod4(mod3)\n        return mod4\n\nmodel = ModelWithBreaks().to(device)\ninputs = [torch.randn((128, 128), device=device) for _ in range(10)]\n\nmodel_c = torch.compile(model)\n\ndef fwd_bwd(inp):\n    out = model_c(inp)\n    out.sum().backward()\n\n# warm up\nfwd_bwd(inputs[0])\n\nwith torch.profiler.profile() as prof:\n    for i in range(1, 4):\n        fwd_bwd(inputs[i])\n        prof.step()\n\nprof.export_chrome_trace(\"trace_break.json\")\n```\n\n----------------------------------------\n\nTITLE: Gradient Checking Example\nDESCRIPTION: Shows how to verify custom gradient implementations using PyTorch's gradcheck utility with finite difference approximations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.autograd import gradcheck\n\n# gradcheck takes a tuple of tensors as input, check if your gradient\n# evaluated with these tensors are close enough to numerical\n# approximations and returns True if they all verify this condition.\ninput = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\ntest = gradcheck(linear, input, eps=1e-6, atol=1e-4)\nprint(test)\n```\n\n----------------------------------------\n\nTITLE: Illustrating Error with Nondeterministic Operation in Deterministic Mode (Python)\nDESCRIPTION: Demonstrates the effect of enabling deterministic algorithms using `torch.use_deterministic_algorithms(True)`. When a nondeterministic operation like `index_add_` on a CUDA tensor (which lacks a deterministic CUDA implementation) is called, PyTorch raises a `RuntimeError`, enforcing the deterministic requirement.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\n'torch.use_deterministic_algorithms(True)'. ...\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Various DTensor Distribution Strategies in PyTorch\nDESCRIPTION: This code snippet shows different ways to construct DTensors, including row-wise sharding, column-wise sharding, replication, and combined sharding and replication strategies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.distributed.tensor import DTensor, Shard, Replicate, distribute_tensor, distribute_module, init_device_mesh\n\ndevice_mesh = init_device_mesh(\"cuda\", (4,))\nrowwise_placement=[Shard(0)]\ncolwise_placement=[Shard(1)]\n\nbig_tensor = torch.randn(888, 12)\nrowwise_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=rowwise_placement)\n\nreplica_placement = [Replicate()]\nreplica_tensor = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=replica_placement)\n\ndevice_mesh = init_device_mesh(\"cuda\", (2, 2))\nspec=[Replicate(), Shard(0)]\npartial_replica = distribute_tensor(big_tensor, device_mesh=device_mesh, placements=spec)\n\nlocal_tensor = torch.randn((8, 8), requires_grad=True)\nrowwise_tensor = DTensor.from_local(local_tensor, device_mesh, rowwise_placement)\n\ncolwise_tensor = rowwise_tensor.redistribute(device_mesh, colwise_placement)\nreplica_tensor = colwise_tensor.redistribute(device_mesh, replica_placement)\n```\n\n----------------------------------------\n\nTITLE: ATen Tensor API Examples in C++\nDESCRIPTION: Examples of tensor operations available in ATen's API including mathematical functions, in-place operations (denoted by underscore suffix), and various tensor manipulation methods.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nTensor atan2(const Tensor & other) const;\nTensor & atan2_(const Tensor & other);\nTensor pow(Scalar exponent) const;\nTensor pow(const Tensor & exponent) const;\nTensor & pow_(Scalar exponent);\nTensor & pow_(const Tensor & exponent);\nTensor lerp(const Tensor & end, Scalar weight) const;\nTensor & lerp_(const Tensor & end, Scalar weight);\nTensor histc() const;\nTensor histc(int64_t bins) const;\nTensor histc(int64_t bins, Scalar min) const;\nTensor histc(int64_t bins, Scalar min, Scalar max) const;\n```\n\n----------------------------------------\n\nTITLE: Registering Overloaded PyTorch Operators in C++\nDESCRIPTION: This snippet demonstrates how to register multiple kernels under the same operator name using overloads. It defines two CPU kernels with different signatures and registers them as separate overloads of the same operator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace {\n  Tensor my_kernel_cpu_1(const Tensor& a) {...}\n  Tensor my_kernel_cpu_2(const Tensor& a, const Tensor& b) {...}\n}\n\nstatic auto registry = torch::RegisterOperators()\n   .op(\"my_namespace::my_op.overload1(Tensor a) -> Tensor\",\n       torch::RegisterOperators::options()\n         .kernel<decltype(my_kernel_cpu_1), &my_kernel_cpu>(CPU()))\n   .op(\"my_namespace::my_op.overload2(Tensor a, Tensor b) -> Tensor\",\n       torch::RegisterOperators::options()\n         .kernel<decltype(my_kernel_cpu_2), &my_kernel_cpu>(CPU()));\n```\n\n----------------------------------------\n\nTITLE: Enumerating PyTorch sum Operator Arguments for SymInt and IntList - Python\nDESCRIPTION: These snippets detail structured argument lists for the 'sum' operator with both symbolic integer dimension and explicit integer list variants. They show float16 tensors with various shapes, axes or dimensions to sum over, and boolean keepdim flags. The SymInt version involves symbolic indices suitable for shape-inferred operations, while dim_IntList enumerates specific dimensions. Used for verifying reduction operations in testing or kernel dispatch. Relies on compatible input tensors and PyTorch sum operator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([32, 1000], f16), [0], True), {})\ncnt: 2, ((T([32, 2, 512, 8, 8], f16), [3, 4], True), {})\ncnt: 1, ((T([32, 2, 512, 16, 16], f16), [3, 4], True), {})\ncnt: 22, ((T([32, 2, 256, 16, 16], f16), [3, 4], True), {})\ncnt: 1, ((T([32, 2, 256, 32, 32], f16), [3, 4], True), {})\ncnt: 3, ((T([32, 2, 128, 32, 32], f16), [3, 4], True), {})\ncnt: 1, ((T([32, 2, 128, 64, 64], f16), [3, 4], True), {})\ncnt: 3, ((T([32, 2, 64, 64, 64], f16), [3, 4], True), {})\nOperator: aten.sum.dim_IntList\ncnt: 6, ((T([32, 2, 64, 64, 64], f16), [1]), {})\ncnt: 2, ((T([32, 2, 128, 64, 64], f16), [1]), {})\ncnt: 6, ((T([32, 2, 128, 32, 32], f16), [1]), {})\ncnt: 2, ((T([32, 2, 256, 32, 32], f16), [1]), {})\ncnt: 44, ((T([32, 2, 256, 16, 16], f16), [1]), {})\ncnt: 2, ((T([32, 2, 512, 16, 16], f16), [1]), {})\ncnt: 4, ((T([32, 2, 512, 8, 8], f16), [1]), {})\n```\n\n----------------------------------------\n\nTITLE: Incompatible Operators for PyTorch CUDAGraph\nDESCRIPTION: List of PyTorch operators that are incompatible with CUDAGraph Trees, causing the compiler to skip graph recording for functions containing these operators.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\naten._fused_moving_avg_obs_fq_helper.default\naten._fused_moving_avg_obs_fq_helper_functional.default\naten.multinomial.default\nfbgemm.dense_to_jagged.default\nfbgemm.jagged_to_padded_dense.default\nrun_and_save_rng_state\nrun_with_rng_state\naten._local_scalar_dense\naten._assert_scalar\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CUDAGraph Tree Execution Flow in PyTorch\nDESCRIPTION: This code snippet illustrates how CUDAGraph Trees handle different execution paths and graph breaks in a PyTorch function. It shows the process of graph warming, recording, and replay for optimized execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile(mode=\"reduce-overhead\")\ndef foo(x):\n    # GRAPH 1\n    y = x * x * x\n    # graph break triggered here\n    if y.sum() > 0:\n        # GRAPH 2\n        z = y ** y\n    else:\n        # GRAPH 3\n        z = (y.abs() ** y.abs())\n    torch._dynamo.graph_break()\n    # GRAPH 4\n    return z * torch.rand_like(z)\n\n# the first run warms up each graph, which does things like CuBlas or Triton benchmarking\nfoo(torch.arange(0, 10, device=\"cuda\"))\n# The second run does a CUDA Graph recording, and replays it\nfoo(torch.arange(0, 10, device=\"cuda\"))\n# Finally we hit the optimized, CUDA Graph replay path\nfoo(torch.arange(0, 10, device=\"cuda\"))\n```\n\n----------------------------------------\n\nTITLE: Post-Specialization LSTM Graph Representation in PyTorch\nDESCRIPTION: Initial specialized graph representation of an LSTM cell with typed inputs. This shows the computational flow with operations like matrix multiplication, tensor addition, and activation functions that make up the LSTM gates.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# post specialization, inputs are now specialized types\ngraph(%x : Float(*, *),\n      %hx : Float(*, *),\n      %cx : Float(*, *),\n      %w_ih : Float(*, *),\n      %w_hh : Float(*, *),\n      %b_ih : Float(*),\n      %b_hh : Float(*)):\n  %7 : int = prim::Constant[value=4]()\n  %8 : int = prim::Constant[value=1]()\n  %9 : Tensor = aten::t(%w_ih)\n  %10 : Tensor = aten::mm(%x, %9)\n  %11 : Tensor = aten::t(%w_hh)\n  %12 : Tensor = aten::mm(%hx, %11)\n  %13 : Tensor = aten::add(%10, %12, %8)\n  %14 : Tensor = aten::add(%13, %b_ih, %8)\n  %gates : Tensor = aten::add(%14, %b_hh, %8)\n  %16 : Tensor[] = aten::chunk(%gates, %7, %8)\n  %ingate.1 : Tensor, %forgetgate.1 : Tensor, %cellgate.1 : Tensor, %outgate.1 : Tensor = prim::ListUnpack(%16)\n  %ingate : Tensor = aten::sigmoid(%ingate.1)\n  %forgetgate : Tensor = aten::sigmoid(%forgetgate.1)\n  %cellgate : Tensor = aten::tanh(%cellgate.1)\n  %outgate : Tensor = aten::sigmoid(%outgate.1)\n  %25 : Tensor = aten::mul(%forgetgate, %cx)\n  %26 : Tensor = aten::mul(%ingate, %cellgate)\n  %cy : Tensor = aten::add(%25, %26, %8)\n  %28 : Tensor = aten::tanh(%cy)\n  %hy : Tensor = aten::mul(%outgate, %28)\n  %30 : (Tensor, Tensor) = prim::TupleConstruct(%hy, %cy)\n  return (%30)\n```\n\n----------------------------------------\n\nTITLE: Tensor Concatenation Operations in PyTorch (56x56 Feature Maps)\nDESCRIPTION: PyTorch's tensor concatenation operations for 56x56 feature maps along dimension 1 (channel dimension). These patterns are typical of DenseNet architecture where feature maps from previous layers are concatenated with the current layer's output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.cat.default\ncnt: 1, (([T([64, 64, 56, 56], f16)], 1), {})\ncnt: 1, (([T([64, 64, 56, 56], f16), T([64, 32, 56, 56], f16)], 1), {})\ncnt: 1, (([T([64, 64, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16)], 1), {})\ncnt: 1, (([T([64, 64, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16)], 1), {})\ncnt: 1, (([T([64, 64, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16)], 1), {})\ncnt: 1, (([T([64, 64, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16)], 1), {})\ncnt: 1, (([T([64, 64, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16)], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients using torch.func.grad and functional_call in Python\nDESCRIPTION: Shows the PyTorch 2.0+ approach for computing model parameter gradients using `torch.func`. Parameters are extracted using `model.named_parameters()` into a dictionary. A loss function `compute_loss` uses `torch.func.functional_call` to execute the original `model` with the provided `params` dictionary and inputs. `torch.func.grad` is then used to compute gradients with respect to the `params` dictionary. Requires the `torch` library. This is the recommended replacement for the `functorch.make_functional` pattern.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport torch\ninputs = torch.randn(64, 3)\ntargets = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nparams = dict(model.named_parameters())\n\ndef compute_loss(params, inputs, targets):\n    prediction = torch.func.functional_call(model, params, (inputs,))\n    return torch.nn.functional.mse_loss(prediction, targets)\n\ngrads = torch.func.grad(compute_loss)(params, inputs, targets)\n```\n\n----------------------------------------\n\nTITLE: Custom Operator Decomposition for conv2d - PyTorch - Python\nDESCRIPTION: This code customizes the decomposition of the conv2d operator during IR lowering by assigning a custom function to torch.ops.aten.conv2d.default in the decomposition table. It then runs decompositions on the exported program to obtain an IR where conv2d is replaced with twice the result of a convolution op. Demonstrates extension of PyTorch's export and decomposition system with user-defined behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Lower to core aten inference IR, but customize conv2d\ndecomp_table = torch.export.default_decompositions()\n\ndef my_awesome_custom_conv2d_function(x, weight, bias, stride=[1, 1], padding=[0, 0], dilation=[1, 1], groups=1):\n    return 2 * torch.ops.aten.convolution(x, weight, bias, stride, padding, dilation, False, [0, 0], groups)\n\ndecomp_table[torch.ops.aten.conv2d.default] = my_awesome_conv2d_function\nep_for_inference = ep_for_training.run_decompositions(decomp_table)\n\nprint(ep_for_inference)\n```\n\n----------------------------------------\n\nTITLE: Computing First and Second Order Gradients with torch.func.grad in Python\nDESCRIPTION: This snippet demonstrates the basic usage of `torch.func.grad` to compute the gradient of a function. It first calculates the first derivative of `torch.sin(x)` with respect to `x`. It then shows how `grad` can be composed with itself to compute second-order gradients, calculating the second derivative of `torch.sin(x)`. Assertions verify the results against analytical derivatives (`cos(x)` and `-sin(x)`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.func import grad\nx = torch.randn([])\ncos_x = grad(lambda x: torch.sin(x))(x)\nassert torch.allclose(cos_x, x.cos())\n\n# Second-order gradients\nneg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)\nassert torch.allclose(neg_sin_x, -x.sin())\n```\n\n----------------------------------------\n\nTITLE: Applying Native Batch Normalization in PyTorch\nDESCRIPTION: The native_batch_norm operator normalizes tensor outputs across the specified dimensions. It configures training mode and momentum, factors crucial for adapting model parameters based on batch statistics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([32, 128, 112, 112], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Checking Supported Ops in Lazy Tensor\nDESCRIPTION: Demonstrates how to check which operations are supported by Lazy Tensor for a given model, helping identify potential performance bottlenecks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntorch._lazy.metrics.reset()\ntrain(...)\nprint(torch._lazy.metrics.counter_names())\n```\n\n----------------------------------------\n\nTITLE: Internal Graph Representation of a TorchScript Module\nDESCRIPTION: Shows the internal IR (Intermediate Representation) graph of a TorchScript module. This represents how PyTorch understands the computational flow defined in the Python code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_38\n\nLANGUAGE: text\nCODE:\n```\ngraph(%x.1 : Tensor,\n      %y : int,\n      %z : float):\n  %5 : int = prim::Constant[value=1]()\n  %3 : int = prim::Constant[value=2]()\n  %4 : bool = aten::gt(%y, %3)\n  %x : Tensor = prim::If(%4)\n    block0():\n      %x.2 : Tensor = aten::add(%x.1, %z, %5)\n      -> (%x.2)\n    block1():\n      %x.3 : Tensor = aten::add(%x.1, %y, %5)\n      -> (%x.3)\n  return (%x)\n```\n\n----------------------------------------\n\nTITLE: Modified Bytecode for Function with Graph Break\nDESCRIPTION: The bytecode generated by Dynamo after compilation, showing how it splits the function at the graph break point. It creates two separate functions - the original and a continuation function - and executes them in sequence.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nMODIFIED BYTECODE fn script.py line 3\n 0 LOAD_GLOBAL              1 (__compiled_fn_0)\n 2 LOAD_FAST                0 (a)\n 4 CALL_FUNCTION            1\n 6 STORE_FAST               3 (graph_out_0)\n 8 LOAD_GLOBAL              0 (print)\n10 LOAD_CONST               2 ('Hi')\n12 LOAD_FAST                3 (graph_out_0)\n14 LOAD_CONST               3 (0)\n16 BINARY_SUBSCR\n18 STORE_FAST               1 (b)\n\n20 CALL_FUNCTION            1\n22 LOAD_GLOBAL              2 (__resume_at_14_1)\n24 ROT_TWO\n26 LOAD_FAST                0 (a)\n28 LOAD_FAST                1 (b)\n30 CALL_FUNCTION            3\n32 RETURN_VALUE\n\nMODIFIED BYTECODE resume_in_fn script.py line 6\n 0 LOAD_GLOBAL              1 (__compiled_fn_2)\n 2 LOAD_FAST                2 (b)\n 4 LOAD_FAST                1 (a)\n 6 CALL_FUNCTION            2\n 8 UNPACK_SEQUENCE          1\n10 RETURN_VALUE\n```\n\n----------------------------------------\n\nTITLE: Equivalent Methods for Device Specification in PyTorch Tensor Creation\nDESCRIPTION: This code snippet illustrates three equivalent ways to specify a device when creating a random tensor: using a torch.device object, a properly formatted string, or a legacy integer device ordinal.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy\n```\n\n----------------------------------------\n\nTITLE: Relative Performance Comparison (Wide Matrix) - Python\nDESCRIPTION: Uses get_perf to compare jacrev and jacfwd performance for wide matrix scenario. Requires variables from previous benchmarks. Simply prints relative improvement.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nget_perf(jacrev_timing, \"jacrev\", jacfwd_timing, \"jacfwd\")\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors with Inherited Device/Dtype using new_* Methods in PyTorch\nDESCRIPTION: This Python snippet demonstrates creating new tensors while preserving the device (CPU or CUDA) and data type (dtype) of an existing tensor using `torch.Tensor.new_*` methods like `new_full()` and `new_tensor()`. This is useful for writing device-agnostic code within modules, ensuring new tensors match the context of existing ones. The example shows creating tensors based on existing CPU and GPU tensors, automatically placing the new tensors on the correct device and matching the original dtype. It requires the `torch` library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ncuda = torch.device('cuda')\nx_cpu = torch.empty(2)\nx_gpu = torch.empty(2, device=cuda)\nx_cpu_long = torch.empty(2, dtype=torch.int64)\n\ny_cpu = x_cpu.new_full([3, 2], fill_value=0.3)\nprint(y_cpu)\n\n    # tensor([[ 0.3000,  0.3000],\n    #         [ 0.3000,  0.3000],\n    #         [ 0.3000,  0.3000]])\n\ny_gpu = x_gpu.new_full([3, 2], fill_value=-5)\nprint(y_gpu)\n\n    # tensor([[-5.0000, -5.0000],\n    #         [-5.0000, -5.0000],\n    #         [-5.0000, -5.0000]], device='cuda:0')\n\ny_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])\nprint(y_cpu_long)\n\n    # tensor([[ 1,  2,  3]])\n```\n\n----------------------------------------\n\nTITLE: Setting Up TorchScript for Doctest Compatibility With Monkeypatching - PyTorch - Python\nDESCRIPTION: This snippet demonstrates monkeypatching of torch.jit.script and torch.jit.trace to set a custom module attribute, ensuring compatibility with the doctest environment. It is used to circumvent issues with the inspect module and doctest execution when preparing test setups for documentation. Both the script and trace functions are wrapped so that any scripted or traced object has its __module__ set to 'FakeMod', which helps doctest resolve symbol origins correctly. Requires PyTorch as a dependency and assumes doctest is running as the test driver.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\noriginal_script = torch.jit.script\ndef script_wrapper(obj, *args, **kwargs):\n    obj.__module__ = 'FakeMod'\n    return original_script(obj, *args, **kwargs)\n\ntorch.jit.script = script_wrapper\n\noriginal_trace = torch.jit.trace\ndef trace_wrapper(obj, *args, **kwargs):\n    obj.__module__ = 'FakeMod'\n    return original_trace(obj, *args, **kwargs)\n\ntorch.jit.trace = trace_wrapper\n```\n\n----------------------------------------\n\nTITLE: Transforming Tensors using the Tanh Function\nDESCRIPTION: Applies the hyperbolic tangent function to a tensor, transforming input values to a range between -1 and 1. Requires an input tensor of shape [16, 3, 128, 128]. The operation facilitates non-linear transformations in activation functions, crucial for deep model inference.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.tanh.default\ncnt: 1, ((T([16, 3, 128, 128], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Partitionable Transformer Model in Python\nDESCRIPTION: This code snippet shows how to define a Transformer model that can be easily partitioned for pipeline parallelism. It uses a ModuleDict for layers and handles None values to enable easy splitting.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Transformer(nn.Module):\n    def __init__(self, model_args: ModelArgs):\n        super().__init__()\n\n        self.tok_embeddings = nn.Embedding(...)\n\n        # Using a ModuleDict lets us delete layers without affecting names,\n        # ensuring checkpoints will correctly save and load.\n        self.layers = torch.nn.ModuleDict()\n        for layer_id in range(model_args.n_layers):\n            self.layers[str(layer_id)] = TransformerBlock(...)\n\n        self.output = nn.Linear(...)\n\n    def forward(self, tokens: torch.Tensor):\n        # Handling layers being 'None' at runtime enables easy pipeline splitting\n        h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\n\n        for layer in self.layers.values():\n            h = layer(h, self.freqs_cis)\n\n        h = self.norm(h) if self.norm else h\n        output = self.output(h).float() if self.output else h\n        return output\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Tensor Promotion in TorchScript If Statements\nDESCRIPTION: Shows how 1-dimensional tensors are promoted to bool in if statements, while multi-dimensional tensors raise an error.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.jit.script\ndef fn(x: torch.Tensor):\n    if x: # The tensor gets promoted to bool\n        return True\n    return False\nprint(fn(torch.rand(1)))\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.clone.default Calls in PyTorch Text Trace\nDESCRIPTION: Describes clone operations performed on half-precision (f16) tensors of various shapes, logging the count and size of clone invocations. Cloning is common when a tensor needs to be duplicated for further in-place operations or to avoid side effects. The trace provides insight into memory usage trends and intermediate tensor lifetimes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([32, 3, 224, 224], f16),), {})\ncnt: 2, ((T([32, 32, 112, 112], f16),), {})\ncnt: 1, ((T([32, 8, 1, 1], f16),), {})\ncnt: 1, ((T([32, 96, 112, 112], f16),), {})\ncnt: 1, ((T([32, 96, 56, 56], f16),), {})\ncnt: 1, ((T([32, 4, 1, 1], f16),), {})\ncnt: 3, ((T([32, 144, 56, 56], f16),), {})\ncnt: 2, ((T([32, 6, 1, 1], f16),), {})\ncnt: 1, ((T([32, 144, 28, 28], f16),), {})\ncnt: 3, ((T([32, 240, 28, 28], f16),), {})\ncnt: 2, ((T([32, 10, 1, 1], f16),), {})\ncnt: 1, ((T([32, 240, 14, 14], f16),), {})\ncnt: 6, ((T([32, 480, 14, 14], f16),), {})\ncnt: 3, ((T([32, 20, 1, 1], f16),), {})\ncnt: 5, ((T([32, 672, 14, 14], f16),), {})\ncnt: 3, ((T([32, 28, 1, 1], f16),), {})\ncnt: 1, ((T([32, 672, 7, 7], f16),), {})\ncnt: 8, ((T([32, 1152, 7, 7], f16),), {})\ncnt: 4, ((T([32, 48, 1, 1], f16),), {})\ncnt: 1, ((T([32, 1280, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Decompiling Dynamo Generated Code in Python\nDESCRIPTION: This snippet demonstrates how to decompile the compiled code object generated by Dynamo using the depyf library. This allows inspection of the structure of the compiled function, including how it handles graph breaks and calls to compiled subgraphs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom depyf import decompile\nprint(decompile(code))\n```\n\n----------------------------------------\n\nTITLE: In-Place Addition of Tensors in PyTorch - Python\nDESCRIPTION: Illustrates the use of aten.add_.Tensor for in-place addition of tensors. Modifies the original tensor by adding another tensor. Useful for operations where memory optimization is crucial. Limitations include maintaining compatible tensor shapes for in-place operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add_.Tensor\ncnt: 1, ((T([32, 16, 112, 112], f16), T([32, 16, 112, 112], f16)), {})\ncnt: 1, ((T([32, 24, 56, 56], f16), T([32, 24, 56, 56], f16)), {})\ncnt: 2, ((T([32, 40, 28, 28], f16), T([32, 40, 28, 28], f16)), {})\ncnt: 3, ((T([32, 80, 14, 14], f16), T([32, 80, 14, 14], f16)), {})\ncnt: 1, ((T([32, 112, 14, 14], f16), T([32, 112, 14, 14], f16)), {})\ncnt: 2, ((T([32, 160, 7, 7], f16), T([32, 160, 7, 7], f16)), {})\n\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Statistics in a Vision Transformer Model\nDESCRIPTION: This code represents operator statistics from a PyTorch model, showing each tensor operation with its count and tensor shapes. The model appears to be a Vision Transformer variant with attention mechanisms and convolutional layers operating on a batch of 128 images with 3 channels at 224x224 resolution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 4, ((T([128, 6, 196, 196], f16), -1, False), {})\ncnt: 4, ((T([128, 6, 49, 49], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 4, ((T([128, 6, 49, 49], f16), T([128, 6, 49, 49], f16), -1, f16), {})\ncnt: 4, ((T([128, 6, 196, 196], f16), T([128, 6, 196, 196], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 8, ((T([128, 6, 196, 64], f16), [768, 196, 64]), {})\ncnt: 4, ((T([128, 6, 64, 196], f16), [768, 64, 196]), {})\ncnt: 4, ((T([768, 196, 196], f16), [128, 6, 196, 196]), {})\ncnt: 4, ((T([768, 196, 64], f16), [128, 6, 196, 64]), {})\ncnt: 4, ((T([128, 6, 64, 196], f16), [128, 384, 14, 14]), {})\ncnt: 8, ((T([128, 6, 49, 128], f16), [768, 49, 128]), {})\ncnt: 4, ((T([128, 6, 128, 49], f16), [768, 128, 49]), {})\ncnt: 4, ((T([768, 49, 49], f16), [128, 6, 49, 49]), {})\ncnt: 4, ((T([768, 49, 128], f16), [128, 6, 49, 128]), {})\ncnt: 4, ((T([128, 6, 128, 49], f16), [128, 768, 7, 7]), {})\ncnt: 4, ((T([128, 3, 6, 128, 49], f16), [128, 2304, 7, 7]), {})\ncnt: 4, ((T([128, 3, 6, 64, 196], f16), [128, 1152, 14, 14]), {})\n```\n\n----------------------------------------\n\nTITLE: Understanding PyTorch Tensors Retrieval Mechanism\nDESCRIPTION: This Python code snippet provides an insight into how PyTorch packs and unpacks tensors during autograd operations. Using '.exp()' function, the code shows that the unpacked tensor retrieved through 'grad_fn._saved_result' might be a different object while maintaining the same storage. Requires 'torch' library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nx = torch.randn(5, requires_grad=True)\ny = x.exp()\nprint(y.equal(y.grad_fn._saved_result))  # True\nprint(y is y.grad_fn._saved_result)  # False\n```\n\n----------------------------------------\n\nTITLE: Data-Dependent Operations Example with Graph Breaks\nDESCRIPTION: Example demonstrating how data-dependent operations cause graph breaks in torch.compile\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef fn(x):\n    y = x.sum()\n    if y > 0:\n        return x + y.item()\n    return x - y.item()\n\nfn(torch.ones(3, 3))\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors with Inherited Properties using ones_like/zeros_like in PyTorch\nDESCRIPTION: This Python code showcases the use of `torch.ones_like()` and `torch.zeros_like()` helper functions. These functions create new tensors filled with ones or zeros, respectively, while automatically inheriting the size, data type (dtype), and device (CPU or CUDA) from a provided input tensor (`x_cpu` or `x_gpu`). This simplifies creating tensors with matching properties in device-agnostic code. It requires the `torch` library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nx_cpu = torch.empty(2, 3)\nx_gpu = torch.empty(2, 3) # Assuming default device is CUDA or x_gpu is created on CUDA\n\ny_cpu = torch.ones_like(x_cpu)\ny_gpu = torch.zeros_like(x_gpu)\n```\n\n----------------------------------------\n\nTITLE: Displaying Shape of Calculated Per-Sample Gradients\nDESCRIPTION: Prints the shape of the first element in the `per_sample_grads` list, which corresponds to the per-sample gradients computed for the first parameter of the model (likely `model.conv1.weight`). The output shape (e.g., `[64, 32, 1, 3, 3]`) demonstrates that a separate gradient tensor has been computed for each of the 64 samples in the batch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(per_sample_grads[0].shape)\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Autograd Benchmarks Workflow\nDESCRIPTION: Complete workflow for running autograd benchmarks before and after code changes. Includes environment setup, PyTorch compilation, dependency installation, and benchmark execution steps.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/functional_autograd_benchmark/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Make sure you compile pytorch in release mode and with the same flags before/after\nexport DEBUG=0\n# When running on CPU, it might be required to limit the number of cores to avoid oversubscription\nexport OMP_NUM_THREADS=10\n\n# Compile pytorch with the base revision\ngit checkout master\npython setup.py develop\n\n# Install dependencies:\n# Scipy is required by detr\npip install scipy\n\n# Run the benchmark for the base\n# This will use the GPU if available.\npushd benchmarks/functional_autograd_benchmark\npython functional_autograd_benchmark.py --output before.txt\n\n# Compile pytorch with your change\npopd\ngit checkout your_feature_branch\npython setup.py develop\n\n# Run the benchmark for the new version\npushd benchmarks/functional_autograd_benchmark\npython functional_autograd_benchmark.py --output after.txt\n\n# Get the markdown table that you can paste in your github PR\npython compare.py\n\npopd\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobians with jacrev Transform in PyTorch\nDESCRIPTION: Shows how to compute Jacobians using reverse-mode AD and batch them using vmap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import jacrev\nx = torch.randn(5)\njacobian = jacrev(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n\nx = torch.randn(64, 5)\njacobian = vmap(jacrev(torch.sin))(x)\nassert jacobian.shape == (64, 5, 5)\n```\n\n----------------------------------------\n\nTITLE: Initializing TCP-based Distributed Process Group in PyTorch\nDESCRIPTION: This snippet demonstrates how to initialize a distributed process group using TCP in PyTorch. It specifies the backend, initialization method with an IP address and port, rank, and world size.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n```\n\n----------------------------------------\n\nTITLE: Passing RRef as Argument in RPC Call\nDESCRIPTION: This code shows how to pass an RRef as an argument to another RPC call. Worker A creates an RRef pointing to an object on worker B, then passes this RRef back to worker B in a subsequent RPC call. This demonstrates the pattern of sharing RRefs between workers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/rref.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed.rpc as rpc\n\n# on worker A and worker B\ndef func(rref):\n  pass\n\n# on worker A\nrref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n# say the rref has RRefId 100 and ForkId 1\nrpc.rpc_async('B', func, args=(rref, ))\n```\n\n----------------------------------------\n\nTITLE: Disabling Reduced-Precision Reduction for FP16/BF16 GEMMs in PyTorch (Python)\nDESCRIPTION: This snippet provides settings to disable reduced-precision accumulation for GEMM operations with FP16 and BF16 data types on PyTorchs CUDA backend. Disabling these features can avoid numerical overflows by ensuring intermediate accumulation uses single-precision, at the cost of performance. This code is relevant when numerical reproducibility is preferred over speed and is only meaningful on GPU backends supporting these configuration flags. The main configuration properties are torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction and torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction, which are set to False to disable reduced precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/numerical_accuracy.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n```\n\n----------------------------------------\n\nTITLE: Creating Named Tensors Using Factory Functions in PyTorch\nDESCRIPTION: Demonstrates how to create a tensor with named dimensions using the torch.zeros factory function. The 'names' parameter associates names 'N' and 'C' with the first and second dimensions respectively.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.zeros(2, 3, names=('N', 'C'))\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], names=('N', 'C'))\n```\n\n----------------------------------------\n\nTITLE: Quantizing Tensors in PyTorch\nDESCRIPTION: Functions for converting floating point tensors to quantized format with specified scale, zero point, and data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntorch.quantize_per_tensor(x, scale, zero_point, dtype)\ntorch.quantize_per_channel(x, scales, zero_points, axis, dtype)\ntorch.quantize_per_tensor_dynamic(x, dtype, reduce_range)\nto(torch.float16)\n```\n\n----------------------------------------\n\nTITLE: Extended ScalarTensor with Torch Function Support\nDESCRIPTION: Enhanced implementation of ScalarTensor that includes __torch_function__ support for integrating with torch operations. Includes a global dispatch table for handling torch functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nHANDLED_FUNCTIONS = {}\nclass ScalarTensor(object):\n    def __init__(self, N, value):\n        self._N = N\n        self._value = value\n\n    def __repr__(self):\n        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n    def tensor(self):\n        return self._value * torch.eye(self._N)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n            return NotImplemented\n        return HANDLED_FUNCTIONS[func](*args, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: PyTorch 2 Export Quantization Implementation\nDESCRIPTION: Demonstrates implementation of PyTorch 2 export quantization with XNNPACK backend. Includes model definition, calibration setup, and quantization process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nfrom torch.ao.quantization.quantize_pt2e import prepare_pt2e\nfrom torch.export import export_for_training\nfrom torch.ao.quantization.quantizer import (\n    XNNPACKQuantizer,\n    get_symmetric_quantization_config,\n)\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 10)\n\n    def forward(self, x):\n        return self.linear(x)\n\nfloat_model = M().eval()\n\ndef calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)\n\nm = export_for_training(m, *example_inputs).module()\nquantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config())\nm = prepare_pt2e(m, quantizer)\nm = convert_pt2e(m)\n```\n\n----------------------------------------\n\nTITLE: Proper Use of Function Returns with functorch\nDESCRIPTION: This example shows the correct approach to refactor a function to be compatible with functorch transforms by returning intermediate values as part of the function's output instead of using global variables.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z, intermediate\n\ngrad_x, intermediate = grad(f, has_aux=True)(x)\n```\n\n----------------------------------------\n\nTITLE: Using Data Sparsification Callbacks with PyTorch Lightning\nDESCRIPTION: Example showing how to use data sparsification callbacks with PyTorch Lightning Trainer. It demonstrates the workflow of creating a Lightning module, adding the callback to the trainer, and accessing the sparsified model after training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/lightning/callbacks/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npl_module = SomePLModule()  # pl_module.model should specify the pytorch model\n\nds_callback = SomeDataSparsifierCallback(data_sparsifier_class=..., data_sparsifier_args=..., ...)  # add scheduler if TrainingAwareDataSparsifier\ntrainer = Trainer(callbacks=[ds_callback])\n\ntrainer.fit(pl_module, train_data_loader, val_data_loader)\n\n# NOTE: pl_module.model is not sparsified\n\n# access sparsified model\nsparsified_model = ds_callback.sparsified\n```\n\n----------------------------------------\n\nTITLE: Grouping Data with PyTorch DataPipes\nDESCRIPTION: Illustrates the use of the groupby() method to group data based on a key function. Shows examples of grouping with different parameters and handling of buffer sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).shuffle().groupby(lambda x: x % 3)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3).groupby(lambda x: len(x))\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3).groupby(lambda x: x % 3, unbatch_level = 1)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, buffer_size = 5)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(18).shuffle().groupby(lambda x: x % 3, group_size = 3)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, group_size = 3, guaranteed_group_size = 2)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, guaranteed_group_size = 2)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(15).groupby(lambda x: x % 3, guaranteed_group_size = 2, buffer_size = 6)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, guaranteed_group_size = 2, buffer_size = 6)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(15).shuffle().groupby(lambda x: x % 3, guaranteed_group_size = 2, buffer_size = 6, drop_remaining = True)\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Displaying Functional Model Representation\nDESCRIPTION: Prints the representation of the `fmodel` object obtained from `make_functional_with_buffers`. This output confirms that `fmodel` is an instance of `FunctionalModuleWithBuffers`, indicating the successful transformation of the original model into a stateless functional form suitable for `functorch` operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfmodel\n```\n\n----------------------------------------\n\nTITLE: Performing Multi-GPU Distributed Offline GEMM Tuning with PyTorch (Python)\nDESCRIPTION: Python script for performing offline GEMM tuning distributed across multiple GPUs. It requires the main logic to be within `if __name__ == \"__main__\":` due to using `concurrent.futures`. It calls `tunable.mgpu_tune_gemm_in_file`, providing a wildcard pattern for input untuned CSV files (`tunableop_untuned?.csv`) and the number of GPUs (`num_gpus`) to use. The function gathers unique GEMMs, distributes tuning across the specified GPUs, collects results, and saves them to aggregated files (e.g., `tunableop_results_full0.csv`) and per-GPU files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    num_gpus = 8 # number of GPUs that will be used during the tuning process\n    tunable.mgpu_tune_gemm_in_file(\"tunableop_untuned?.csv\", num_gpus)\n```\n\n----------------------------------------\n\nTITLE: Implementing __torch_dispatch__ Method\nDESCRIPTION: Defines the signature and calling convention for the __torch_dispatch__ method used to override PyTorch's native operations. This method receives normalized arguments and can intercept all calls into the aten native API.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining a TorchScript Class with Initialization Constraints\nDESCRIPTION: Example showing how TorchScript classes require member declaration in __init__(). Attempting to assign to self outside of __init__() will result in a runtime error.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n@torch.jit.script\nclass Foo:\n  def assign_x(self):\n    self.x = torch.rand(2, 3)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Graph Break in PyTorch Compilation\nDESCRIPTION: Example showing how a graph break occurs when using Python builtin functions (open) within a torch.compile decorated function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef fn(x):\n    x = x + 1\n    with open(\"test.txt\", \"r\") as f:\n        return x + len(f.read())\n\nfn(torch.ones(3, 3))\n```\n\n----------------------------------------\n\nTITLE: Initializing Owner to User RRef Sharing in PyTorch\nDESCRIPTION: Demonstrates how an owner shares an RRef with a user through RPC calls. The owner can update reference counting locally without additional control messages. Shows creation of a local RRef and passing it to a remote worker.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/rref.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed.rpc as RRef, rpc\n\n# on worker B and worker C\ndef func(rref):\n    pass\n\n# on worker B, creating a local RRef\nrref = RRef(\"data\")\n# say the rref has RRefId 100\ndist.rpc_async('C', func, args=(rref, ))\n```\n\n----------------------------------------\n\nTITLE: Pinpointing Problematic Kernel/Fusion Patterns\nDESCRIPTION: This complex command combines multiple debug options to log TorchScript IR, codegen IR, and executed kernels, helping to identify failing fusions in larger models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\nPYTORCH_NVFUSER_DISABLE=fallback \\\nPYTORCH_JIT_LOG_LEVEL=\">partition:graph_fuser:>>kernel_cache\" \\\npython your_script.py &> log\n```\n\n----------------------------------------\n\nTITLE: Saving Resources to a Package\nDESCRIPTION: Demonstrates how to save various types of resources (pickle objects, text, and binary data) to a torch package using save_pickle, save_text, and save_binary methods.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith torch.PackageExporter(\"package.pt\") as exporter:\n    # Pickles the object and saves to `my_resources/tensor.pkl` in the archive.\n    exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4))\n    exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\")\n    exporter.save_binary(\"raw_data\", \"binary\", my_bytes)\n```\n\n----------------------------------------\n\nTITLE: Performing Matrix Multiplication with Sparse Tensor in PyTorch\nDESCRIPTION: Demonstrates how to use a semi-structured sparse tensor in matrix multiplication operation, comparing results with dense tensor multiplication.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\na = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).half().cuda()\nb = torch.rand(64, 64).half().cuda()\nc = torch.mm(a, b)\na_sparse = to_sparse_semi_structured(a)\ntorch.allclose(c, torch.mm(a_sparse, b))\n```\n\n----------------------------------------\n\nTITLE: Saving Dictionaries of Tensors with Pickle - PyTorch - Python\nDESCRIPTION: Shows how torch.save and torch.load, which use Python's pickle module by default, can serialize dictionaries containing multiple tensors. Requires that both the keys and the values in the dictionary are pickle-able and that torch is imported. The example creates a dictionary of two tensors, saves it to a file, and reconstructs the dictionary upon loading. Inputs are the dictionary and a filename; the output is a reconstituted dictionary of tensors. Limitations include ensuring all dictionary elements are serializable with pickle.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}\n>>> torch.save(d, 'tensor_dict.pt')\n>>> torch.load('tensor_dict.pt')\n{'a': tensor([1., 2.]), 'b': tensor([3., 4.])}\n```\n\n----------------------------------------\n\nTITLE: Basic Custom Backend Implementation in Python\nDESCRIPTION: Shows how to implement a basic custom backend function and use it with torch.compile. The backend function receives a GraphModule and example inputs, returning the forward function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef my_custom_backend(gm, example_inputs):\n    return gm.forward\n\ndef f(...):\n    ...\n\nf_opt = torch.compile(f, backend=my_custom_backend)\n\n@torch.compile(backend=my_custom_backend)\ndef g(...):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Syntactic Sugar for Catch-All Function Kernel Registration (C++)\nDESCRIPTION: Demonstrates a simplified syntax for registering a catch-all kernel function (`my_kernel_cpu`). Instead of using `.options().catchAllKernel()`, the function pointer is passed directly as the second argument to `.op()`. This implicitly registers the function as a catch-all kernel.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace { Tensor my_kernel_cpu(const Tensor& a, const Tensor& b) {...}\n\nstatic auto registry = torch::RegisterOperators()\n .op(\"my_namespace::my_op\", &my_kernel_cpu);\n```\n\n----------------------------------------\n\nTITLE: Creating and Inspecting Uncoalesced Sparse COO Tensor in PyTorch\nDESCRIPTION: This snippet demonstrates how to create an uncoalesced sparse COO tensor and inspect its properties. It shows the tensor representation with duplicate indices and how coalescing combines duplicate values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n>>> i = [[1, 1]]\n>>> v =  [3, 4]\n>>> s=torch.sparse_coo_tensor(i, v, (3,))\n>>> s\ntensor(indices=tensor([[1, 1]]),\n       values=tensor(  [3, 4]),\n       size=(3,), nnz=2, layout=torch.sparse_coo)\n\n>>> s.coalesce()\ntensor(indices=tensor([[1]]),\n       values=tensor([7]),\n       size=(3,), nnz=1, layout=torch.sparse_coo)\n```\n\n----------------------------------------\n\nTITLE: Configuring ATen Build Settings and Dependencies\nDESCRIPTION: Main CMake configuration script that sets up build environment for ATen. Handles conditional compilation for different hardware backends (CPU, CUDA, ROCm), manages source files, include paths, and dependencies. Includes error checking for incompatible configurations like CUDA and ROCm together.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT INTERN_BUILD_ATEN_OPS)\n  return()\nendif()\n\n# Find modules\nif(NOT INTERN_BUILD_MOBILE)\n  list(APPEND CMAKE_MODULE_PATH /usr/lib/x86_64-linux-gnu/)\n  list(APPEND CMAKE_LIBRARY_PATH /usr/lib/x86_64-linux-gnu/ /usr/lib/aarch64-linux-gnu/)\nendif()\n\nlist(APPEND CMAKE_MODULE_PATH\n  ${CMAKE_CURRENT_SOURCE_DIR}/../cmake/Modules\n  ${CMAKE_CURRENT_SOURCE_DIR}/../cmake/public\n  ${CMAKE_CURRENT_SOURCE_DIR}/../cmake/Modules_CUDA_fix)\n\ncmake_policy(SET CMP0012 NEW)\n\n#############################################\n\nset(ATen_CPU_SRCS)\nset(ATen_XPU_SRCS)\nset(ATen_XPU_INCLUDE)\nset(ATen_CPU_TEST_SRCS)\nset(ATen_CPU_INCLUDE)\nset(ATen_THIRD_PARTY_INCLUDE)\nset(ATen_CUDA_CPP_SRCS)\nset(ATen_CUDA_CU_SRCS)\nset(ATen_CUDA_LINALG_SRCS)\nset(ATen_CUDA_SRCS_W_SORT_BY_KEY)\nset(ATen_CUDA_TEST_SRCS)\nset(ATen_CUDA_INCLUDE)\nset(ATen_NVRTC_STUB_SRCS)\nset(ATen_HIP_SRCS)\nset(ATen_HIP_SRCS_W_SORT_BY_KEY)\nset(ATen_HIP_TEST_SRCS)\nset(ATen_HIP_INCLUDE)\nset(ATen_MPS_SRCS)\nset(ATen_MPS_TEST_SRCS)\nset(ATen_XPU_SRCS)\nset(ATen_XPU_INCLUDE)\nset(ATen_XPU_TEST_SRCS)\nset(ATen_VULKAN_TEST_SRCS)\nset(ATen_CPU_DEPENDENCY_LIBS)\nset(ATen_XPU_DEPENDENCY_LIBS)\nset(ATen_CUDA_DEPENDENCY_LIBS)\nset(ATen_HIP_DEPENDENCY_LIBS)\nset(ATen_PUBLIC_CUDA_DEPENDENCY_LIBS)\nset(ATen_PUBLIC_HIP_DEPENDENCY_LIBS)\nset(ATEN_INSTALL_BIN_SUBDIR \"bin\" CACHE PATH \"ATen install binary subdirectory\")\nset(ATEN_INSTALL_LIB_SUBDIR \"lib\" CACHE PATH \"ATen install library subdirectory\")\nset(ATEN_INSTALL_INCLUDE_SUBDIR \"include\" CACHE PATH \"ATen install include subdirectory\")\nset(MEM_EFF_ATTENTION_CUDA_SOURCES)\n\nset(TH_LINK_STYLE STATIC)\nset(TH_CPU_INCLUDE\n  ${CMAKE_CURRENT_SOURCE_DIR}/src\n  ${CMAKE_CURRENT_BINARY_DIR}/src\n  ${CMAKE_BINARY_DIR}/aten/src)\nlist(APPEND ATen_CPU_INCLUDE ${TH_CPU_INCLUDE})\n\nif(USE_VULKAN)\n  list(APPEND ATen_CPU_INCLUDE ${CMAKE_BINARY_DIR}/vulkan ${CMAKE_CURRENT_SOURCE_DIR}/../third_party/VulkanMemoryAllocator)\nendif()\n\n# Find the HIP package, set the HIP paths, load the HIP CMake.\nif(USE_ROCM)\n  include(LoadHIP)\n  if(NOT PYTORCH_FOUND_HIP)\n    set(USE_ROCM OFF)\n  endif()\nendif()\n\n# Both CUDA and ROCM are enabled and found. Report an error.\nif(USE_CUDA AND USE_ROCM)\n  message(FATAL_ERROR \"Both CUDA and ROCm are enabled and found. PyTorch can only be built with either of them. Please turn one off by using either USE_CUDA=OFF or USE_ROCM=OFF.\")\nendif()\n\nif(USE_ROCM)\n  # TODO: AT_HIP_ENABLED (change this once we represent HIP as HIP in\n  # ATen proper)\n  set(AT_CUDA_ENABLED 1)\n  add_subdirectory(src/THH)\n  message(\"ROCm is enabled.\")\nelseif(USE_CUDA)\n  set(AT_CUDA_ENABLED 1)\n  add_subdirectory(src/THC)\nelse()\n  message(\"disabling CUDA because USE_CUDA is set false\")\n  set(AT_CUDA_ENABLED 0)\nendif()\n\nif(NOT USE_NNPACK)\n  set(AT_NNPACK_ENABLED 0)\nelse()\n  set(AT_NNPACK_ENABLED 1)\nendif()\n\nif(NOT USE_CUSPARSELT)\n  set(AT_CUSPARSELT_ENABLED 0)\nelse()\n  set(AT_CUSPARSELT_ENABLED 1)\nendif()\n\nlist(APPEND ATen_CPU_INCLUDE\n  ${CMAKE_CURRENT_SOURCE_DIR}/src)\nadd_subdirectory(src/ATen)\n\n# Pass source, includes, and libs to parent\nset(ATen_CPU_SRCS ${ATen_CPU_SRCS} PARENT_SCOPE)\nset(ATen_CORE_SRCS ${ATen_CORE_SRCS} PARENT_SCOPE)\nset(ATen_XPU_SRCS ${ATen_XPU_SRCS} PARENT_SCOPE)\nset(ATen_XPU_INCLUDE ${ATen_XPU_INCLUDE} PARENT_SCOPE)\nset(ATen_CUDA_CU_SRCS ${ATen_CUDA_CU_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_CPP_SRCS ${ATen_CUDA_CPP_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_LINALG_SRCS ${ATen_CUDA_LINALG_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_SRCS_W_SORT_BY_KEY ${ATen_CUDA_SRCS_W_SORT_BY_KEY} PARENT_SCOPE)\nset(ATen_CUDA_CU_SRCS_W_SORT_BY_KEY ${ATen_CUDA_CU_SRCS_W_SORT_BY_KEY} PARENT_SCOPE)\nset(ATen_HIP_SRCS ${ATen_HIP_SRCS} PARENT_SCOPE)\nset(ATen_MPS_SRCS ${ATen_MPS_SRCS} PARENT_SCOPE)\nset(ATen_MPS_TEST_SRCS ${ATen_MPS_TEST_SRCS} PARENT_SCOPE)\nset(ATen_HIP_SRCS_W_SORT_BY_KEY ${ATen_HIP_SRCS_W_SORT_BY_KEY} PARENT_SCOPE)\nset(ATen_XPU_SRCS ${ATen_XPU_SRCS} PARENT_SCOPE)\nset(ATen_XPU_TEST_SRCS ${ATen_XPU_TEST_SRCS} PARENT_SCOPE)\nset(ATen_NVRTC_STUB_SRCS ${ATen_NVRTC_STUB_SRCS} PARENT_SCOPE)\nset(ATen_CPU_TEST_SRCS ${ATen_CPU_TEST_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_TEST_SRCS ${ATen_CUDA_TEST_SRCS} PARENT_SCOPE)\nset(ATen_HIP_TEST_SRCS ${ATen_HIP_TEST_SRCS} PARENT_SCOPE)\nset(ATen_VULKAN_TEST_SRCS ${ATen_VULKAN_TEST_SRCS} PARENT_SCOPE)\nset(ATen_MOBILE_BENCHMARK_SRCS ${ATen_MOBILE_BENCHMARK_SRCS} PARENT_SCOPE)\nset(ATen_MOBILE_TEST_SRCS ${ATen_MOBILE_TEST_SRCS} PARENT_SCOPE)\nset(ATen_VEC_TEST_SRCS ${ATen_VEC_TEST_SRCS} PARENT_SCOPE)\nset(ATen_CPU_INCLUDE ${ATen_CPU_INCLUDE} PARENT_SCOPE)\nset(ATen_CUDA_INCLUDE ${ATen_CUDA_INCLUDE} PARENT_SCOPE)\nset(ATen_HIP_INCLUDE ${ATen_HIP_INCLUDE} PARENT_SCOPE)\nset(ATen_XPU_INCLUDE ${ATen_XPU_INCLUDE} PARENT_SCOPE)\nset(ATen_THIRD_PARTY_INCLUDE ${ATen_THIRD_PARTY_INCLUDE} PARENT_SCOPE)\nset(ATen_CPU_DEPENDENCY_LIBS ${ATen_CPU_DEPENDENCY_LIBS} PARENT_SCOPE)\nset(ATen_XPU_DEPENDENCY_LIBS ${ATen_XPU_DEPENDENCY_LIBS} PARENT_SCOPE)\nset(ATen_CUDA_DEPENDENCY_LIBS ${ATen_CUDA_DEPENDENCY_LIBS} PARENT_SCOPE)\nset(ATen_HIP_DEPENDENCY_LIBS ${ATen_HIP_DEPENDENCY_LIBS} PARENT_SCOPE)\nset(ATen_CORE_TEST_SRCS ${ATen_CORE_TEST_SRCS} PARENT_SCOPE)\nset(FLASH_ATTENTION_CUDA_SOURCES ${FLASH_ATTENTION_CUDA_SOURCES} PARENT_SCOPE)\nset(MEM_EFF_ATTENTION_CUDA_SOURCES ${MEM_EFF_ATTENTION_CUDA_SOURCES} PARENT_SCOPE)\nset(ATen_ATTENTION_KERNEL_SRCS ${ATen_ATTENTION_KERNEL_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Custom Cube Function with Intermediate Value Handling\nDESCRIPTION: Implements a custom cube operation that demonstrates how to handle intermediate values and support higher-order gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        # We wish to save dx for backward. In order to do so, it must\n        # be returned as an output.\n        dx = 3 * x ** 2\n        result = x ** 3\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`,\n        # which is grad_dx * 6 * x.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\n# Wrap MyCube in a function so that it is clearer what the output is\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Trace\nDESCRIPTION: A comprehensive trace of PyTorch tensor operations showing operator names, input tensors with shapes and types, execution counts, and additional parameters. The trace appears to be from a transformer model implementation with operations like softmax, matrix multiplication, and layer normalization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fastNLP_Bert_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._index_put_impl_.default\ncnt: 1, ((T([6, 474, 768], f16), [T([6, 474], i64, stride=(1, 0)), T([6, 474], i64, stride=(475, 1))], T([6, 474, 768], f16), True, True), {})\nOperator: aten._softmax.default\ncnt: 12, ((T([6, 12, 476, 476], f16), -1, False), {})\n# ... [Additional operations truncated for brevity]\n```\n\n----------------------------------------\n\nTITLE: Implementing Logging Modes Example\nDESCRIPTION: Demonstrates implementing custom logging modes using TorchFunctionMode and TorchDispatchMode to intercept and log PyTorch operations at different levels of the execution stack.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.overrides import TorchFunctionMode, resolve_name\nfrom torch.utils._python_dispatch import TorchDispatchMode\n\nclass FunctionLog(TorchFunctionMode):\n    def __torch_function__(self, func, types, args, kwargs=None):\n        print(f\"Function Log: {resolve_name(func)}(*{args}, **{kwargs})\")\n        return func(*args, **(kwargs or {}))\n\nclass DispatchLog(TorchDispatchMode):\n    def __torch_dispatch__(self, func, types, args, kwargs=None):\n        print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n        return func(*args, **(kwargs or {}))\n\ndef f():\n    a = torch.rand(10, requires_grad=True)\n    b = a * 2\n    b.sum().backward()\n\nprint(\"TorchFunctionMode logging:\")\nwith FunctionLog():\n    f()\n\nprint(\"TorchDispatchMode logging:\")\nwith DispatchLog():\n    f()\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Scripted Quantized Models in PyTorch\nDESCRIPTION: Shows how to save and load a scripted quantized model using torch.jit.save and torch.jit.load. This method involves preparing the model, converting it, scripting it, and then using BytesIO for serialization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Note: using the same model M from previous example\nm = M().eval()\nprepare_orig = prepare_fx(m, {'' : default_qconfig})\nprepare_orig(torch.rand(5, 5))\nquantized_orig = convert_fx(prepare_orig)\n\n# save/load using scripted model\nscripted = torch.jit.script(quantized_orig)\nb = io.BytesIO()\ntorch.jit.save(scripted, b)\nb.seek(0)\nscripted_quantized = torch.jit.load(b)\n```\n\n----------------------------------------\n\nTITLE: Type Annotation with torch.jit.annotate() in TorchScript\nDESCRIPTION: Provides a type hint to TorchScript where Python 3 style type hints do not work well. Used for annotating types of expressions like empty lists.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ntorch.jit.annotate(List[int], [])\n```\n\n----------------------------------------\n\nTITLE: Fixing CUDA Stream Synchronization Error\nDESCRIPTION: Corrected version of the code that properly synchronizes streams by making the new stream wait for the default stream to complete.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda._sanitizer.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith torch.cuda.stream(torch.cuda.Stream()):\n    torch.cuda.current_stream().wait_stream(torch.cuda.default_stream())\n    torch.mul(a, 5, out=a)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Quantization with FX Graph Mode\nDESCRIPTION: Example demonstrating custom quantization module usage with PyTorch's FX graph mode quantization including configuration setup and execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nm = torch.nn.Sequential(CustomModule()).eval()\nqconfig_mapping = QConfigMapping().set_global(torch.ao.quantization.default_qconfig)\nprepare_custom_config_dict = {\n    \"float_to_observed_custom_module_class\": {\n        \"static\": {\n            CustomModule: ObservedCustomModule,\n        }\n    }\n}\nconvert_custom_config_dict = {\n    \"observed_to_quantized_custom_module_class\": {\n        \"static\": {\n            ObservedCustomModule: StaticQuantCustomModule,\n        }\n    }\n}\nmp = torch.ao.quantization.quantize_fx.prepare_fx(\n    m, qconfig_mapping, torch.randn(3,3), prepare_custom_config=prepare_custom_config_dict)\n# calibration (not shown)\nmq = torch.ao.quantization.quantize_fx.convert_fx(\n    mp, convert_custom_config=convert_custom_config_dict)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CUDA Stream Synchronization Error\nDESCRIPTION: Example showing a data race condition where a tensor is modified on different CUDA streams without proper synchronization, potentially causing race conditions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda._sanitizer.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\na = torch.rand(4, 2, device=\"cuda\")\n\nwith torch.cuda.stream(torch.cuda.Stream()):\n    torch.mul(a, 5, out=a)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking jacfwd vs jacrev for Wide Matrix Case - Python\nDESCRIPTION: Runs the same jacfwd/jacrev timing experiment on the reverse case (much more inputs than outputs; wide matrix). Initializes appropriate random tensors and prints both times. Requires functorch and Timer. Compares performance implications of matrix shape for autodiff.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nDin = 2048\nDout = 32\nweight = torch.randn(Dout, Din)\nbias = torch.randn(Dout)\nx = torch.randn(Din)\n\nusing_fwd = Timer(stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\nusing_bwd = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\njacfwd_timing = using_fwd.timeit(500)\njacrev_timing = using_bwd.timeit(500)\n\nprint(f'jacfwd time: {jacfwd_timing}')\nprint(f'jacrev time: {jacrev_timing}')\n```\n\n----------------------------------------\n\nTITLE: Forward-mode Jacobian Computation with jacfwd in PyTorch\nDESCRIPTION: Demonstrates computing Jacobians using forward-mode AD as an alternative to reverse-mode.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import jacfwd\nx = torch.randn(5)\njacobian = jacfwd(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n```\n\n----------------------------------------\n\nTITLE: Managing CUDA Streams on Multiple Devices (Skeleton Example)\nDESCRIPTION: This skeleton example demonstrates various approaches to handle CUDA streams on multiple devices, including stream acquisition, setting current streams, and using device guards.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n// Usage 0: acquire CUDA stream and set current CUDA stream with `setCurrentCUDAStream`\n// Create a CUDA stream vector `streams0` on device 0\nstd::vector<at::cuda::CUDAStream> streams0 =\n  {at::cuda::getDefaultCUDAStream(), at::cuda::getStreamFromPool()};\n// set current stream as `streams0[0]` on device 0\nat::cuda::setCurrentCUDAStream(streams0[0]);\n\n// create a CUDA stream vector `streams1` on device using CUDA device guard\nstd::vector<at::cuda::CUDAStream> streams1;\n{\n  // device index is set to 1 within this scope\n  at::cuda::CUDAGuard device_guard(1);\n  streams1.push_back(at::cuda::getDefaultCUDAStream());\n  streams1.push_back(at::cuda::getStreamFromPool());\n}\n// device index is reset to 0 after device_guard is destroyed\n\n// set current stream as `streams1[0]` on device 1\n```\n\n----------------------------------------\n\nTITLE: Conditional Tensor Storage with DataParallel in PyTorch\nDESCRIPTION: Implementation of size-based tensor storage optimization using saved tensor hooks within a PyTorch model using DataParallel.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Only save on disk tensors that have size >= 1000\nSAVE_ON_DISK_THRESHOLD = 1000\n\ndef pack_hook(x):\n    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n        return x\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(tensor_or_sctf):\n    if isinstance(tensor_or_sctf, torch.Tensor):\n        return tensor_or_sctf\n    return torch.load(tensor_or_sctf.name)\n\nclass Model(nn.Module):\n    def forward(self, x):\n        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n          # ... compute output\n          output = x\n        return output\n\nmodel = Model()\nnet = nn.DataParallel(model)\n```\n\n----------------------------------------\n\nTITLE: Attention Mechanism with Dimensions in PyTorch\nDESCRIPTION: Implements an attention mechanism using dimension objects for batch, channel, key, and query operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom torchdim import softmax\ndef attention(K, Q, V):\n    batch, channel, key, query = dims(4)\n    k = K[batch, channel, key]\n    q = Q[batch, channel, query]\n    v = V[batch, channel, key]\n\n    a = (k * q).sum(channel) # matrix multiply\n    a = softmax(a * (channel.size ** -0.5), dim=key)\n    r = (v * a).sum(key) # matrix multiply\n    return torch.cat((r.order(batch, channel, query), Q), dim=1)\n\ninputs = (torch.rand(2, 3, 4) for _ in range(3))\nattention(*inputs)\n```\n\n----------------------------------------\n\nTITLE: Tracing and Saving a PyTorch Module with Control Flow\nDESCRIPTION: Shows how to trace a module with control flow, save it, and load it back. Demonstrates the limitations of tracing compared to scripting.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# A module with control flow\n>>> class ControlFlowModule(torch.nn.Module):\n      def __init__(self):\n        super().__init__()\n        self.l0 = torch.nn.Linear(4, 2)\n        self.l1 = torch.nn.Linear(2, 1)\n\n      def forward(self, input):\n        if input.dim() > 1:\n            return torch.tensor(0)\n\n        out0 = self.l0(input)\n        out0_relu = torch.nn.functional.relu(out0)\n        return self.l1(out0_relu)\n\n>>> traced_module = torch.jit.trace(ControlFlowModule(), torch.randn(4))\n>>> torch.jit.save(traced_module, 'controlflowmodule_traced.pt')\n>>> loaded = torch.jit.load('controlflowmodule_traced.pt')\n>>> loaded(torch.randn(2, 4)))\ntensor([[-0.1571], [-0.3793]], grad_fn=<AddBackward0>)\n\n>>> scripted_module = torch.jit.script(ControlFlowModule(), torch.randn(4))\n>>> torch.jit.save(scripted_module, 'controlflowmodule_scripted.pt')\n>>> loaded = torch.jit.load('controlflowmodule_scripted.pt')\n>> loaded(torch.randn(2, 4))\ntensor(0)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Composable C++ Operator for PyTorch\nDESCRIPTION: Defines a simple C++ function `my_op` that takes two `at::Tensor` objects and returns a new tensor computed as `self + 2 * other`. This example illustrates a function composed of existing PyTorch operators (`+`, `*`) that can potentially be registered with `CompositeImplicitAutograd` for automatic differentiation support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nat::Tensor my_op(const Tensor& self, const Tensor& other) {\n  return self + 2 * other;\n}\n```\n\n----------------------------------------\n\nTITLE: Registering Custom PyTorch Operator Kernel Function (C++)\nDESCRIPTION: Demonstrates registering a C++ function (`my_kernel_cpu`) as a kernel implementation for a custom operator named `my_namespace::my_op`. The registration uses `torch::RegisterOperators` and specifies the kernel function via `decltype` and function pointer, associating it with the CPU dispatch key. It's recommended to place the kernel function within an anonymous namespace.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace { Tensor my_kernel_cpu(const Tensor& a, const Tensor& b) {...} }\n\nstatic auto registry = torch::RegisterOperators()\n   .op(\"my_namespace::my_op\",  torch::RegisterOperators::options()\n       .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()));\n```\n\n----------------------------------------\n\nTITLE: Basic torch.compile Usage with Trigonometric Functions\nDESCRIPTION: Demonstrates basic usage of torch.compile with simple trigonometric operations (cos and sin) on GPU. Shows how to compile and run a function with the inductor backend.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\ndef fn(x):\n   a = torch.cos(x)\n   b = torch.sin(a)\n   return b\nnew_fn = torch.compile(fn, backend=\"inductor\")\ninput_tensor = torch.randn(10000).to(device=\"cuda:0\")\na = new_fn(input_tensor)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Control Flow Example in PyTorch FX\nDESCRIPTION: Demonstrates a failed symbolic tracing attempt due to dynamic control flow where the condition depends on input values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.sum.SymInt Calls - PyTorch - Python\nDESCRIPTION: Presents structure for tensor sum reductions using symbolically-typed axes (aten.sum.SymInt) in PyTorch. Inputs are float16 tensors and sum dimensions (usually a list), with keepdim option. Outputs are reduced tensors or scalars, used to identify reduction hotspots. Requires PyTorch with symbolic shape support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ReLU Activation Usage in PyTorch\nDESCRIPTION: This code snippet shows the input tensor shapes for the ReLU activation function in PyTorch. It includes counts of how many times each unique shape is used, both for the default ReLU and the in-place ReLU_ operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu.default\ncnt: 1, ((T([32, 24, 1, 1], f16),), {})\ncnt: 2, ((T([32, 32, 1, 1], f16),), {})\ncnt: 1, ((T([32, 120, 1, 1], f16),), {})\ncnt: 2, ((T([32, 168, 1, 1], f16),), {})\ncnt: 2, ((T([32, 240, 1, 1], f16),), {})\n\nOperator: aten.relu_.default\ncnt: 1, ((T([32, 16, 112, 112], f16),), {})\ncnt: 1, ((T([32, 64, 112, 112], f16),), {})\ncnt: 1, ((T([32, 64, 56, 56], f16),), {})\ncnt: 3, ((T([32, 72, 56, 56], f16),), {})\ncnt: 1, ((T([32, 72, 28, 28], f16),), {})\ncnt: 4, ((T([32, 120, 28, 28], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Forward Convolution Operations in PyTorch\nDESCRIPTION: Forward pass convolution operations with varying input tensor shapes (ranging from 7x7 to 28x28) and filter sizes (1x1 and 3x3). Operations use half-precision (f16) format with consistent stride and padding patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 128, 28, 28], f16), T([32, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling ATen Activation and Pooling Operators in PyTorch Model (Python)\nDESCRIPTION: Provides a log of activation (ReLU) and pooling (max_pool2d_with_indices) operator calls with argument shapes, dtypes, and kernel/stride settings as present in a PyTorch model trace. Dependencies: PyTorch, tensor inputs matching shape/dtype. Inputs are typically multi-dimensional tensors; outputs are activated or pooled tensors (and indices for pooling). Highlights parameter choices (kernel/stride/padding).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([16, 64, 112, 112], f16),), {})\ncnt: 4, ((T([16, 64, 56, 56], f16),), {})\ncnt: 4, ((T([16, 128, 28, 28], f16),), {})\ncnt: 4, ((T([16, 256, 14, 14], f16),), {})\ncnt: 4, ((T([16, 512, 7, 7], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([16, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([16, 64, 56, 56], f16), T([16, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([16, 64, 56, 56], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Generating CUDA Memory Snapshot in PyTorch\nDESCRIPTION: This snippet demonstrates how to enable memory history recording, run code to be observed, and save a pickled snapshot of CUDA memory usage in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch_cuda_memory.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# enable memory history, which will\n# add tracebacks and event history to snapshots\ntorch.cuda.memory._record_memory_history()\n\nrun_your_code()\ntorch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n```\n\n----------------------------------------\n\nTITLE: Implementing AOT Autograd with NNC Compiler\nDESCRIPTION: Uses PyTorch Neural Network Compiler (NNC) for operator fusion optimization\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch.compile import ts_compile\n\naot_nnc_fn = aot_function(fn, fw_compiler=ts_compile, bw_compiler=ts_compile)\n\ncloned_inputs = [x.clone().detach().requires_grad_(True) for x in (a, b, c, d)]\ncloned_a, cloned_b, cloned_c, cloned_d = cloned_inputs\n\nres = aot_nnc_fn(*cloned_inputs)\nloss = res.sum()\nloss.backward()\nassert torch.allclose(ref, res)\nassert torch.allclose(a.grad, cloned_a.grad)\nassert torch.allclose(b.grad, cloned_b.grad)\nassert torch.allclose(c.grad, cloned_c.grad)\nassert torch.allclose(d.grad, cloned_d.grad)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Sum Operations in PyTorch\nDESCRIPTION: This code snippet shows the usage of sum operations in PyTorch, including both the SymInt and default versions. It includes the input tensor shapes and the dimensions along which the sum is computed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})\ncnt: 1, ((T([32, 1280], f16), [0], True), {})\ncnt: 2, ((T([32, 960, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 672, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 480, 14, 14], f16), [2, 3], True), {})\ncnt: 2, ((T([32, 120, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 72, 28, 28], f16), [2, 3], True), {})\n\nOperator: aten.sum.default\ncnt: 1, ((T([32, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Releasing Shared CUDA Tensor Memory Promptly in Consumer Process (Python)\nDESCRIPTION: Demonstrates the recommended practice for handling shared CUDA tensors received via a multiprocessing queue. The tensor `x` obtained from `queue.get()` should be explicitly deleted using `del x` as soon as it's no longer needed to release the underlying shared memory. This prevents the producer process from unnecessarily holding onto the memory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n## Good\nx = queue.get()\n# do somethings with x\ndel x\n```\n\n----------------------------------------\n\nTITLE: Defining a Constant in TorchScript Using Python Module Attribute\nDESCRIPTION: This snippet demonstrates how to use a constant value from a Python module (math.pi) in a TorchScript function. Values looked up as attributes of a module are assumed to be constant in TorchScript.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport torch\n\n@torch.jit.script\ndef fn():\n    return math.pi\n```\n\n----------------------------------------\n\nTITLE: Exported Program Graph Representation with Dynamic Shapes\nDESCRIPTION: The textual representation of an ExportedProgram showing forward method with symbolic shape 's0'. Includes input parameters, operations, and range constraints for dynamic dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nExportedProgram:\nclass GraphModule(torch.nn.Module):\n    def forward(self, p_branch1_0_weight: \"f32[32, 64]\", p_branch1_0_bias: \"f32[32]\", p_branch2_0_weight: \"f32[64, 128]\", p_branch2_0_bias: \"f32[64]\", c_buffer: \"f32[32]\", x1: \"f32[s0, 64]\", x2: \"f32[s0, 128]\"):\n\n         # code: out1 = self.branch1(x1)\n        linear: \"f32[s0, 32]\" = torch.ops.aten.linear.default(x1, p_branch1_0_weight, p_branch1_0_bias)\n        relu: \"f32[s0, 32]\" = torch.ops.aten.relu.default(linear)\n\n         # code: out2 = self.branch2(x2)\n        linear_1: \"f32[s0, 64]\" = torch.ops.aten.linear.default(x2, p_branch2_0_weight, p_branch2_0_bias)\n        relu_1: \"f32[s0, 64]\" = torch.ops.aten.relu.default(linear_1)\n\n         # code: return (out1 + self.buffer, out2)\n        add: \"f32[s0, 32]\" = torch.ops.aten.add.Tensor(relu, c_buffer)\n        return (add, relu_1)\n\nRange constraints: {s0: VR[0, int_oo]}\n```\n\n----------------------------------------\n\nTITLE: Python Model Device Conversion Example\nDESCRIPTION: Demonstrates best practices for training models on GPU and performing inference on CPU using TorchScript.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncpu_model = gpu_model.cpu()\nsample_input_cpu = sample_input_gpu.cpu()\ntraced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\ntraced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\ntorch.jit.save(traced_gpu, \"gpu.pt\")\n\n# ... later, when using the model:\n\nif use_gpu:\n  model = torch.jit.load(\"gpu.pt\")\nelse:\n  model = torch.jit.load(\"cpu.pt\")\n\nmodel(input)\n```\n\n----------------------------------------\n\nTITLE: Backpropagating ELU with aten.elu_backward in Python\nDESCRIPTION: This operator, aten.elu_backward.default, computes gradients of the ELU activation function for backpropagation in training. The function requires an input tensor, parameters to determine activation shape, and a gradient tensor for the backward pass, ensuring efficient gradient updates under different tensor strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.elu_backward.default\ncnt: 1, ((T([256, 197951], f16, stride=(0, 0)), 1.6732632423543772, 1.0507009873554805, 1, False, T([256, 197951], f16)), {})\ncnt: 4, ((T([256, 512], f16), 1.6732632423543772, 1.0507009873554805, 1, False, T([256, 512], f16)), {})\ncnt: 1, ((T([256, 1024], f16), 1.6732632423543772, 1.0507009873554805, 1, False, T([256, 1024], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Customizing Class Packaging with __reduce_package__\nDESCRIPTION: Demonstrates how to customize the packaging of a class by implementing __reduce_package__ and a corresponding unpackaging function. This allows for custom serialization behavior for class instances.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# foo.py [Example of customizing how class Foo is packaged]\nfrom torch.package import PackageExporter, PackageImporter\nimport time\n\n\nclass Foo:\n    def __init__(self, my_string: str):\n        super().__init__()\n        self.my_string = my_string\n        self.time_imported = 0\n        self.time_exported = 0\n\n    def __reduce_package__(self, exporter: PackageExporter):\n        \"\"\"\n        Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when\n        saving an instance of this object. This method should do the work to save this\n        object inside of the ``torch.package`` archive.\n\n        Returns function w/ arguments to load the object from a\n        ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.\n        \"\"\"\n\n        # use this pattern to ensure no naming conflicts with normal dependencies,\n        # anything saved under this module name shouldn't conflict with other\n        # items in the package\n        generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\"\n        exporter.save_text(\n            generated_module_name,\n            \"foo.txt\",\n            self.my_string + \", with exporter modification!\",\n        )\n        time_exported = time.clock_gettime(1)\n\n        # returns de-packaging function w/ arguments to invoke with\n        return (unpackage_foo, (generated_module_name, time_exported,))\n\n\ndef unpackage_foo(\n    importer: PackageImporter, generated_module_name: str, time_exported: float\n) -> Foo:\n    \"\"\"\n    Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function\n    when depickling a Foo object.\n    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\n    \"\"\"\n    time_imported = time.clock_gettime(1)\n    foo = Foo(importer.load_text(generated_module_name, \"foo.txt\"))\n    foo.time_imported = time_imported\n    foo.time_exported = time_exported\n    return foo\n```\n\n----------------------------------------\n\nTITLE: Forcing Metal Kernels over MPS Graph API (Environment Variable)\nDESCRIPTION: Set `PYTORCH_MPS_PREFER_METAL` to `1` to force the use of Metal kernels instead of the MPS Graph APIs. Currently, this primarily affects the matrix multiplication (matmul) operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nPYTORCH_MPS_PREFER_METAL\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Neural Network\nDESCRIPTION: This snippet shows the usage patterns of various PyTorch operators in a neural network implementation. It includes operator names, usage counts, and tensor shapes for different operations like softmax, convolutions, and matrix multiplications.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/jx_nest_base_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\n\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\n\nOperator: aten._softmax.default\ncnt: 2, ((T([64, 4, 16, 196, 196], f16), -1, False), {})\ncnt: 2, ((T([64, 8, 4, 196, 196], f16), -1, False), {})\ncnt: 20, ((T([64, 16, 1, 196, 196], f16), -1, False), {})\n\nOperator: aten._softmax_backward_data.default\ncnt: 20, ((T([64, 16, 1, 196, 196], f16), T([64, 16, 1, 196, 196], f16), -1, f16), {})\ncnt: 2, ((T([64, 8, 4, 196, 196], f16), T([64, 8, 4, 196, 196], f16), -1, f16), {})\ncnt: 2, ((T([64, 4, 16, 196, 196], f16), T([64, 4, 16, 196, 196], f16), -1, f16), {})\n\n# ... (truncated for brevity)\n\nOperator: aten.div_.Tensor\ncnt: 2, ((T([64, 1, 1, 1], f16), 0.9782608691602945), {})\ncnt: 2, ((T([64, 1, 1, 1], f16), 0.9565217383205891), {})\ncnt: 2, ((T([64, 1, 1, 1], f16), 0.9347826093435287), {})\n# ... (more entries)\n```\n\n----------------------------------------\n\nTITLE: Computing Batched Jacobians using torch.func.vmap and torch.func.jacrev in Python\nDESCRIPTION: This snippet demonstrates how to compute Jacobians for a batch of inputs by composing `torch.func.vmap` with `torch.func.jacrev`. `jacrev(torch.sin)` creates a function that computes the Jacobian for a single input vector. `vmap` then maps this Jacobian computation over a batch of input vectors `x` (shape 64x5), producing a batch of Jacobians with the expected shape (64, 5, 5).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nx = torch.randn(64, 5)\njacobian = vmap(jacrev(torch.sin))(x)\nassert jacobian.shape == (64, 5, 5)\n```\n\n----------------------------------------\n\nTITLE: Named Tensors with Unnamed Dimensions\nDESCRIPTION: Demonstrates the coexistence of named and unnamed dimensions within tensors, highlighting that unnamed dimensions are given 'None' as a default name.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimgs = torch.randn(1, 2, 2, 3 , names=(None, 'C', 'H', 'W'))\nimgs.names\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.native_layer_norm and Backward Operator - PyTorch - Python\nDESCRIPTION: This documents forward and backward calls to layer normalization using aten.native_layer_norm and aten.native_layer_norm_backward. Forward receives a 3D f16 tensor and norm dim specs, weight, bias, and epsilon. Backward takes gradients plus input/weight/bias/other tensors and computes parameter updates. Outputs: normalized activations/gradients. Prereqs: correct tensor shapes, support for half precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_layer_norm.default\ncnt: 25, ((T([4, 1024, 768], f16), [768], T([768], f16), T([768], f16), 1e-05), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_layer_norm_backward.default\ncnt: 25, ((T([4, 1024, 768], f16), T([4, 1024, 768], f16), [768], T([4, 1024, 1], f32), T([4, 1024, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Using torch.jit.annotate for Type Annotation\nDESCRIPTION: Shows how to use torch.jit.annotate to explicitly specify a type for an expression. This is useful when the default type inference would be incorrect, like initializing an empty list of a specific type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom typing import List\n\ndef g(l: List[int], val: int):\n    l.append(val)\n    return l\n\ndef f(val: int):\n    l = g(torch.jit.annotate(List[int], []), val)\n    return l\n\nm = torch.jit.script(f)\nprint(\"Eager:\", f(3))\nprint(\"TorchScript:\", m(3))\n```\n\n----------------------------------------\n\nTITLE: Tracing a Simple PyTorch Function with Dynamo\nDESCRIPTION: Demonstrates how Dynamo traces a basic mean squared error function, converting it into a linear sequence of PyTorch operations without control flow.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef mse(x, y):\n    z = (x - y) ** 2\n    return z.sum()\n\nx = torch.randn(200)\ny = torch.randn(200)\nmse(x, y)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef forward(l_x_: torch.Tensor, l_y_: torch.Tensor):\n    # File: example.py:5, code: z = (x - y) ** 2\n    sub = l_x_ - l_y_\n    z = sub ** 2\n    # File: example.py:6, code: return z.sum()\n    sum_1 = z.sum()\n    return (sum_1,)\n```\n\n----------------------------------------\n\nTITLE: Defining Convolution Test Parameters for aten.convolution (Python)\nDESCRIPTION: This snippet outlines multiple test case parameter definitions for the forward aten.convolution operator in PyTorch using Python tuple notation. Each case provides tensor shapes, data types (f16), optional strides, stride lists, padding lists, dilation lists, group settings, and an integer for either out_channels or related convolution property. The configuration is intended to serve as synthetic input data for parameterized unit or integration tests, allowing systematic coverage of different convolution settings, tensor sizes, and layer dimensionalities.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nasnetalarge_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 7, ((T([16, 42, 83, 83], f16), T([42, 42, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 42, 83, 83], f16), T([42, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 42), {})\ncnt: 2, ((T([16, 96, 171, 171], f16), T([96, 1, 7, 7], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 96), {})\ncnt: 5, ((T([16, 96, 83, 83], f16), T([42, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 42, 83, 83], f16), T([42, 1, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 42), {})\ncnt: 1, ((T([16, 96, 169, 169], f16), T([96, 1, 5, 5], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 96), {})\ncnt: 2, ((T([16, 42, 83, 83], f16), T([42, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 42), {})\ncnt: 1, ((T([16, 168, 83, 83], f16), T([84, 168, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 84, 87, 87], f16), T([84, 1, 5, 5], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 84), {})\ncnt: 10, ((T([16, 84, 42, 42], f16), T([84, 84, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 84, 42, 42], f16), T([84, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 84), {})\ncnt: 2, ((T([16, 84, 89, 89], f16), T([84, 1, 7, 7], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 84), {})\ncnt: 2, ((T([16, 84, 42, 42], f16), T([84, 1, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 84), {})\ncnt: 2, ((T([16, 84, 42, 42], f16), T([84, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 84), {})\ncnt: 2, ((T([16, 168, 42, 42], f16), T([84, 168, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 336, 42, 42], f16), T([168, 336, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 24, ((T([16, 168, 42, 42], f16), T([168, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 168), {})\ncnt: 60, ((T([16, 168, 42, 42], f16), T([168, 168, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 36, ((T([16, 168, 42, 42], f16), T([168, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 168), {})\ncnt: 9, ((T([16, 1008, 42, 42], f16), T([168, 1008, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 1008, 42, 42], f16), T([336, 1008, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 336, 45, 45], f16), T([336, 1, 5, 5], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 336), {})\ncnt: 70, ((T([16, 336, 21, 21], f16), T([336, 336, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 26, ((T([16, 336, 21, 21], f16), T([336, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 336), {})\ncnt: 2, ((T([16, 336, 47, 47], f16), T([336, 1, 7, 7], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 336), {})\ncnt: 2, ((T([16, 336, 21, 21], f16), T([336, 1, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 336), {})\ncnt: 38, ((T([16, 336, 21, 21], f16), T([336, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 336), {})\ncnt: 2, ((T([16, 1008, 21, 21], f16), T([168, 1008, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 1344, 21, 21], f16), T([336, 1344, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 9, ((T([16, 2016, 21, 21], f16), T([336, 2016, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 2016, 21, 21], f16), T([672, 2016, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 672, 25, 25], f16), T([672, 1, 5, 5], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 672), {})\ncnt: 70, ((T([16, 672, 11, 11], f16), T([672, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 26, ((T([16, 672, 11, 11], f16), T([672, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 2, ((T([16, 672, 27, 27], f16), T([672, 1, 7, 7], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 672), {})\ncnt: 2, ((T([16, 672, 11, 11], f16), T([672, 1, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 672), {})\ncnt: 38, ((T([16, 672, 11, 11], f16), T([672, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 672), {})\ncnt: 2, ((T([16, 2016, 11, 11], f16), T([336, 2016, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([16, 2688, 11, 11], f16), T([672, 2688, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 9, ((T([16, 4032, 11, 11], f16), T([672, 4032, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Summing All Elements with aten.sum in PyTorch\nDESCRIPTION: With the aten.sum.default operator, the sum of all elements of a given tensor is computed, resulting in a scalar. This method is crucial for obtaining aggregate values for further computations or evaluations in models involving tensor elements, relying on single-input, whole-tensor aggregation mechanisms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.default\ncnt: 1, ((T([256, 197951], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Using torch.distributed.rpc.rpc_async() in TorchScript\nDESCRIPTION: Makes a non-blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.rpc.rpc_async()\n```\n\n----------------------------------------\n\nTITLE: Custom ONNX Symbolic Python Operation Handler\nDESCRIPTION: Implementation of a symbolic Python operation handler for ONNX export that processes custom operations like MyClip and MyRelu.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef symbolic_python_op(g: \"GraphContext\", *args, **kwargs):\n    n = ctx.cur_node\n    print(\"original node: \", n)\n    for i, out in enumerate(n.outputs()):\n        print(\"original output {}: {}, requires grad: {}\".format(i, out, out.requiresGrad()))\n    import torch.onnx.symbolic_helper as sym_helper\n    for i, arg in enumerate(args):\n        requires_grad = arg.requiresGrad() if sym_helper._is_value(arg) else False\n        print(\"arg {}: {}, requires grad: {}\".format(i, arg, requires_grad))\n\n    name = kwargs[\"name\"]\n    ret = None\n    if name == \"MyClip\":\n        ret = g.op(\"Clip\", args[0], args[1])\n    elif name == \"MyRelu\":\n        ret = g.op(\"Relu\", args[0])\n    else:\n        return _unimplemented(\"prim::PythonOp\", \"unknown node kind: \" + name)\n    ret.setType(n.type())\n    return ret\n```\n\n----------------------------------------\n\nTITLE: Applying ELU Activation with aten.elu in PyTorch\nDESCRIPTION: The aten.elu.default operator implements the Exponential Linear Unit (ELU) activation function. This non-linear transformation is widely used in neural networks, requiring specific alpha and scale parameters for precise control over the shape of the activation curve. The tensors involved must be of type f16.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.elu.default\ncnt: 4, ((T([256, 512], f16), 1.6732632423543772, 1.0507009873554805), {})\ncnt: 1, ((T([256, 1024], f16), 1.6732632423543772, 1.0507009873554805), {})\ncnt: 1, ((T([256, 197951], f16), 1.6732632423543772, 1.0507009873554805), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring X86 Default Quantization Settings\nDESCRIPTION: Shows how to configure PyTorch quantization settings for x86 backends using either post-training quantization (PTQ) or quantization-aware training (QAT) configs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# set the qconfig for PTQ\n# Note: the old 'fbgemm' is still available but 'x86' is the recommended default on x86 CPUs\nqconfig = torch.ao.quantization.get_default_qconfig('x86')\n# or, set the qconfig for QAT\nqconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n# set the qengine to control weight packing\ntorch.backends.quantized.engine = 'x86'\n```\n\n----------------------------------------\n\nTITLE: Fast Real-to-Real Numerical Gradient Approximation\nDESCRIPTION: Mathematical formula for efficiently computing the Jacobian-vector product for real-to-real functions using finite differences, which is a key component of the fast gradcheck implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/gradcheck.rst#2025-04-22_snippet_4\n\nLANGUAGE: math\nCODE:\n```\nJ_f u \\approx \\frac{f(x + u * eps) - f(x - u * eps)}{2 * eps}.\n```\n\n----------------------------------------\n\nTITLE: Summing Tensor Elements in PyTorch\nDESCRIPTION: This snippet shows the operation for summing elements along a specific dimension of a tensor, commonly used in loss calculation or feature aggregation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobians with Respect to Multiple Parameters Using jacrev - Python\nDESCRIPTION: Demonstrates functorch.jacrev's 'argnums' argument to compute Jacobians of both model parameters (weight and bias) instead of input. Returns two Jacobians (for weight and bias). Prerequisite: functorch.jacrev and all variables in scope. No output, just computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# note the change in input via argnums params of 0,1 to map to weight and bias\nft_jac_weight, ft_jac_bias = jacrev(predict, argnums=(0, 1))(weight, bias, x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Data-Dependent Control Flow with torch.cond\nDESCRIPTION: An example showcasing how to express data-dependent control flow using torch.cond in a PyTorch module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DataDependentCondPredicate(torch.nn.Module):\n    \"\"\"\n    A basic usage of cond based on data dependent predicate.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return torch.cond(x.sum() > 4.0, true_fn, false_fn, (x,))\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Dataclass as PyTree Input in torch.export (Python)\nDESCRIPTION: This example shows how to use a custom dataclass as an input type for a module being exported with `torch.export.export`. The `Input` dataclass is registered using `torch.export.register_dataclass`, allowing instances of it to be treated as PyTrees. The module `M` accepts an instance of `Input`, and the export process correctly handles this custom input type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nimport torch\n\n@dataclass\nclass Input:\n    f: torch.Tensor\n    p: torch.Tensor\n\ntorch.export.register_dataclass(Input)\n\nclass M(torch.nn.Module):\n    def forward(self, x: Input):\n        return x.f + 1\n\ntorch.export.export(M(), (Input(f=torch.ones(10, 4), p=torch.zeros(10, 4)),))\n```\n\n----------------------------------------\n\nTITLE: Conditional Branch Example with Dynamic Shapes in PyTorch\nDESCRIPTION: Example demonstrating how PyTorch handles conditional branching based on dynamic tensor sizes. Shows a function that performs different operations (multiplication or addition) based on the size of a concatenated tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamic_shapes.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, y):\n    z = torch.cat([x, y])\n    if z.size(0) > 2:\n        return z.mul(2)\n    else:\n        return z.add(2)\n```\n\n----------------------------------------\n\nTITLE: Checking Tensor Broadcastability in Python\nDESCRIPTION: Demonstrates the rules for determining if two PyTorch tensors are broadcastable. It shows examples of tensors with the same shape (always broadcastable), tensors where one lacks dimensions (not broadcastable), and tensors where trailing dimensions align according to the rules (broadcastable and not broadcastable cases).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> x=torch.empty(5,7,3)\n>>> y=torch.empty(5,7,3)\n# same shapes are always broadcastable (i.e. the above rules always hold)\n\n>>> x=torch.empty((0,))\n>>> y=torch.empty(2,2)\n# x and y are not broadcastable, because x does not have at least 1 dimension\n\n# can line up trailing dimensions\n>>> x=torch.empty(5,3,4,1)\n>>> y=torch.empty(  3,1,1)\n# x and y are broadcastable.\n# 1st trailing dimension: both have size 1\n# 2nd trailing dimension: y has size 1\n# 3rd trailing dimension: x size == y size\n# 4th trailing dimension: y dimension doesn't exist\n\n# but:\n>>> x=torch.empty(5,2,4,1)\n>>> y=torch.empty(  3,1,1)\n# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n```\n\n----------------------------------------\n\nTITLE: Implementing torch.cond in Python\nDESCRIPTION: A logical implementation of torch.cond showing its basic structure and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef cond(\n    pred: Union[bool, torch.Tensor],\n    true_fn: Callable,\n    false_fn: Callable,\n    operands: Tuple[torch.Tensor]\n):\n    if pred:\n        return true_fn(*operands)\n    else:\n        return false_fn(*operands)\n```\n\n----------------------------------------\n\nTITLE: Inserting ReLU After Add in PyTorch FX Graph\nDESCRIPTION: This code inserts a torch.ops.aten.relu.default call after each torch.ops.aten.add.Tensor call in a PyTorch FX graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef insert_relu_after_add(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n    for node in gm.graph.nodes:\n        if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor:\n\n            # Specifies the insertion point. Any nodes added to the graph within\n            # this scope will be inserted after `node`\n            with gm.graph.inserting_after(node):\n                # Insert a new `call_function` node with op `torch.ops.aten.relu.default`\n                new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, args=(node,))\n                # Replace all the places that use `node` to now use the `new_relu_node`\n                node.replace_all_uses_with(new_relu_node)\n```\n\n----------------------------------------\n\nTITLE: Avoiding Segfaults with Single-Process Queue Put/Get (Python)\nDESCRIPTION: Highlights a potential pitfall when using multiprocessing queues within the *same* process. Putting a tensor into a queue and then immediately getting it back from the same queue in the same process can lead to segmentation faults. This pattern should be avoided; queues are intended for inter-process communication.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# putting and getting from the same queue in the same process will likely end up with segfault\nqueue.put(tensor)\nx = queue.get()\n```\n\n----------------------------------------\n\nTITLE: PyTorch Dtype Conversion Flow\nDESCRIPTION: Example showing performance impact of dtype conversions in quantized models, comparing inefficient and efficient conversion patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization-accuracy-debugging.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfp32_op -> quant -> int8_op -> dequant -> fp32_op -> quant -> int8_op -> dequant\n```\n\nLANGUAGE: python\nCODE:\n```\nfp32_op -> fp32_op -> quant -> int8_op -> int8_op -> dequant\n```\n\n----------------------------------------\n\nTITLE: Custom AutoGrad Function with ONNX Support\nDESCRIPTION: Example of implementing a custom AutoGrad Function (MyRelu) with static symbolic method for ONNX export support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nclass MyRelu(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input: torch.Tensor) -> torch.Tensor:\n        ctx.save_for_backward(input)\n        return input.clamp(min=0)\n\n    @staticmethod\n    def symbolic(g: torch.Graph, input: torch.Value) -> torch.Value:\n        return g.op(\"Clip\", input, g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.float)))\n```\n\n----------------------------------------\n\nTITLE: Zipping DataPipes in PyTorch\nDESCRIPTION: Shows how to use the zip() method to combine multiple DataPipes into a single DataPipe of tuples.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n_dp = ExampleIterPipe(5).shuffle()\ndp = ExampleIterPipe(5).zip(_dp)\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Implementing Torch Function Decorator\nDESCRIPTION: Decorator for registering torch function implementations in the global dispatch table for ScalarTensor operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport functools\ndef implements(torch_function):\n    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n    def decorator(func):\n        functools.update_wrapper(func, torch_function)\n        HANDLED_FUNCTIONS[torch_function] = func\n        return func\n    return decorator\n```\n\n----------------------------------------\n\nTITLE: Module-based Debugging Backend Example\nDESCRIPTION: Shows how to use a custom debugging backend with torch.nn.Module implementations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\nclass MockModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n    def forward(self, x):\n        return self.relu(torch.cos(x))\nmod = MockModule()\noptimized_mod = torch.compile(mod, backend=my_compiler)\noptimized_mod(torch.randn(10))\n```\n\n----------------------------------------\n\nTITLE: Verifying ONNX Model with ONNX Library\nDESCRIPTION: Code snippet demonstrating how to load and verify an exported ONNX model using the ONNX library, including model structure validation and graph visualization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport onnx\n\n# Load the ONNX model\nmodel = onnx.load(\"alexnet.onnx\")\n\n# Check that the model is well formed\nonnx.checker.check_model(model)\n\n# Print a human readable representation of the graph\nprint(onnx.helper.printable_graph(model.graph))\n```\n\n----------------------------------------\n\nTITLE: Tensor Indexing and Modification\nDESCRIPTION: Demonstrates accessing and modifying tensor elements using Python indexing notation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch with Intel GPU Support on Windows - cmd\nDESCRIPTION: This batch script configures the CMAKE_PREFIX_PATH environment variable to point to the appropriate Conda libraries when building PyTorch with Intel GPU support on Windows. It then invokes the develop-mode installation. Visual Studio 2022 is required. Make sure to install all Intel GPU prerequisites and activate your Conda environment beforehand.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_12\n\nLANGUAGE: cmd\nCODE:\n```\n:: CMD Commands:\\n:: Set the CMAKE_PREFIX_PATH to help find corresponding packages\\n:: %CONDA_PREFIX% only works after `conda activate custom_env`\\n\\nif defined CMAKE_PREFIX_PATH (\\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library;%CMAKE_PREFIX_PATH%\"\\n) else (\\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library\"\\n)\\n\\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Using CUDAStreamGuard to Change Device and Stream in PyTorch (C++)\nDESCRIPTION: This snippet shows how to use at::cuda::CUDAStreamGuard to change both the current device index and CUDA stream within a scope. It sets the device index to 1 and the stream to streams1[1].\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n{\n  at::cuda::CUDAStreamGuard stream_guard(streams1[1]);\n\n  // current device index and current CUDA stream are set to 1 and `streams1[1]` within scope\n}\n// current device index and current CUDA stream are reset to 0 and `streams0[0]` after\n// stream_guard is destroyed\n```\n\n----------------------------------------\n\nTITLE: Type Annotations for Variables in TorchScript\nDESCRIPTION: Demonstrates how to use Python 3 style type hints for variable annotations in TorchScript, including containers and optional types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom typing import Dict, Optional\n\n@torch.jit.script\ndef make_dict(flag: bool):\n    x: Dict[str, int] = {}\n    x['hi'] = 2\n    b: Optional[int] = None\n    if flag:\n        b = 2\n    return x, b\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors with Default Options in PyTorch C++\nDESCRIPTION: This example shows how to create tensors using factory functions without specifying TensorOptions, which results in default properties (32-bit float, strided, CPU tensor that does not require a gradient).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n// A 32-bit float, strided, CPU tensor that does not require a gradient.\ntorch::Tensor tensor = torch::randn({3, 4});\ntorch::Tensor range = torch::arange(5, 10);\n```\n\n----------------------------------------\n\nTITLE: Disabling TF32 for Matmul and cuDNN in PyTorch (Python)\nDESCRIPTION: Shows how to explicitly disable the use of TensorFloat-32 (TF32) for both matrix multiplications (matmul) and cuDNN operations in PyTorch to ensure full FP32 precision. This is done by setting the respective flags in `torch.backends.cuda.matmul` and `torch.backends.cudnn` to `False`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.allow_tf32 = False\n```\n\n----------------------------------------\n\nTITLE: Exposing Custom PyTorch Operators to Caffe2 CPU\nDESCRIPTION: This code demonstrates how to make a custom PyTorch operator available to the Caffe2 frontend for CPU execution. It uses a macro to expose the operator under a specified name in Caffe2.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\n// Expose \"my_namespace::my_op\" custom operator to caffe2.\n// In caffe2, the operator will be called \"MyCaffe2OperatorName\".\nC10_EXPORT_C10_OP_TO_CAFFE2_CPU(\n    MyCaffe2OperatorName, \"my_namespace::my_op\")\n```\n\n----------------------------------------\n\nTITLE: Monitored Barrier Example Showing Failure in Python\nDESCRIPTION: Illustrates using a monitored barrier for debugging distributed application faults like synchronization issues, using PyTorch's `monitored_barrier` to identify and report unresponsive ranks. This feature requires PyTorch version 1.10 or greater.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom datetime import timedelta\n\nimport torch\nimport torch.distributed as dist\n```\n\n----------------------------------------\n\nTITLE: Building a Stage Runtime\nDESCRIPTION: This snippet shows the construction of a distributed stage runtime on a specified device using a ProcessGroup. It requires a pipe object, a stage index, device, and group as inputs and outputs a stage object.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nstage = pipe.build_stage(stage_idx, device, group)\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations with Tensor Shapes in PyTorch\nDESCRIPTION: Records of convolution operations with detailed tensor shapes, showing input, output, weights, stride, padding and other parameters. The operations are performed with half-precision (f16) tensors of various dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 48, 112, 112], f16), T([128, 16, 112, 112], f16), T([48, 16, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 32, 112, 112], f16), T([16, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 32, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Preserving Autograd History with torch.as_nested_tensor in Python\nDESCRIPTION: This code showcases the use of `torch.nested.as_nested_tensor` to maintain autograd history while constructing nested tensors. It demonstrates backpropagation through the nested tensor and highlights memory copying involved in the process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> a = torch.randn(12, 512, requires_grad=True)\n>>> b = torch.randn(23, 512, requires_grad=True)\n>>> nt = torch.nested.as_nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.sum().backward()\n>>> a.grad\ntensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])\n>>> b.grad\ntensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])\n```\n\n----------------------------------------\n\nTITLE: Using CPU Tensor Accessors in C++\nDESCRIPTION: Example of creating and using CPU tensor accessors for efficient element-wise access. The accessor provides a type-safe view of tensor data with simplified indexing, ideal for operations where performance is critical.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor foo = torch::rand({12, 12});\n\n// assert foo is 2-dimensional and holds floats.\nauto foo_a = foo.accessor<float,2>();\nfloat trace = 0;\n\nfor(int i = 0; i < foo_a.size(0); i++) {\n  // use the accessor foo_a to get tensor data.\n  trace += foo_a[i][i];\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Monitored Barrier with torch.distributed - Python\nDESCRIPTION: This Python snippet demonstrates initializing a distributed process group using both NCCL and Gloo backends, and performing a monitored barrier with timeout to detect synchronization issues between two ranks. The code depends on torch.distributed, torch.multiprocessing, and Python\\'s os module for process management and environment variable configuration. Key parameters include backend type, rank, world_size, and barrier timeout, and the output consists of error messages if synchronization fails. Requires at least two GPUs and proper NCCL/Gloo installation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    # monitored barrier requires gloo process group to perform host-side sync.\n    group_gloo = dist.new_group(backend=\"gloo\")\n    if rank not in [1]:\n        dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    mp.spawn(worker, nprocs=2, args=())\n```\n\n----------------------------------------\n\nTITLE: Profiling Collective Communication with torch.profiler in Python\nDESCRIPTION: Set up PyTorch's collective communication profiling using `torch.profiler`, ensuring insights into operations like `all_reduce` for tensors distributed across devices. Requires PyTorch version 1.8.1 or later. The profiling output integrates collective communication findings with the GLOO, NCCL, and MPI backends.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor)\n```\n\n----------------------------------------\n\nTITLE: Backend Compiler Error Example\nDESCRIPTION: Example demonstrating how to debug backend compiler errors using a toy compiler that fails on ReLU operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch._dynamo as dynamo\n\nmodel = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n# toy compiler which fails if graph contains relu\ndef toy_compiler(gm: torch.fx.GraphModule, _):\n    for node in gm.graph.nodes:\n        if node.target == torch.relu:\n            assert False\n    return gm\n\ndef test_backend_error():\n    y = torch.ones(200, 200)\n    x = torch.ones(200, 200)\n    z = x + y\n    a = torch.relu(z)\n    return model(a)\n\ncompiled_test_backend_error = torch.compile(test_backend_error, backend=toy_compiler)\ncompiled_test_backend_error()\n```\n\n----------------------------------------\n\nTITLE: Using Hessian-Vector Product with Example Function in PyTorch\nDESCRIPTION: Shows how to use the HVP function with a simple sine function. This example demonstrates applying the HVP to compute the product of the Hessian of the sum of sine with a tangent vector.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return x.sin().sum()\n\nx = torch.randn(2048)\ntangent = torch.randn(2048)\n\nresult = hvp(f, (x,), (tangent,))\n```\n\n----------------------------------------\n\nTITLE: Zip Examples in TorchScript\nDESCRIPTION: Demonstrates valid and invalid uses of zip() function in TorchScript with different container types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\na = [1, 2] # List\nb = [2, 3, 4] # List\nzip(a, b) # works\n```\n\nLANGUAGE: python\nCODE:\n```\na = (1, 2) # Tuple\nb = [2, 3, 4] # List\nzip(a, b) # Runtime error\n```\n\nLANGUAGE: python\nCODE:\n```\na = [1.3, 2.4]\nb = [2, 3, 4]\nzip(a, b) # Works\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for PyTorch Development\nDESCRIPTION: Specifies required Python packages and their version constraints for PyTorch development environment. Includes essential packages for building, testing, and development like cmake, ninja, numpy, and other utility packages. Some dependencies have specific version requirements and platform constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nastunparse\ncmake\nexpecttest>=0.3.0\nfilelock\nfsspec\nhypothesis\njinja2\nlintrunner ; platform_machine != \"s390x\"\nnetworkx\nninja\nnumpy\noptree>=0.13.0\npackaging\npsutil\npyyaml\nrequests\nsetuptools>=62.3.0,<75.9\nsympy>=1.13.3\ntypes-dataclasses\ntyping-extensions>=4.10.0\n```\n\n----------------------------------------\n\nTITLE: Tuning Intra-op Threads for Matrix Multiplication in Python\nDESCRIPTION: Illustrates how to measure the performance impact of varying the number of intra-op threads for a PyTorch operation (matrix multiplication). It iterates through different thread counts, sets the number of threads using `torch.set_num_threads(t)`, and times the execution of `torch.mm` using `timeit.timeit`. This script helps in finding the optimal number of threads for CPU-bound operations by measuring runtimes. Dependencies include the `torch` and `timeit` libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cpu_threading_torchscript_inference.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport timeit\nruntimes = []\nthreads = [1] + [t for t in range(2, 49, 2)]\nfor t in threads:\n    torch.set_num_threads(t)\n    r = timeit.timeit(setup = \"import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)\", stmt=\"torch.mm(x, y)\", number=100)\n    runtimes.append(r)\n# ... plotting (threads, runtimes) ...\n```\n\n----------------------------------------\n\nTITLE: Running Sparse Matrix-Matrix Multiplication Benchmark\nDESCRIPTION: Command line usage for running SPMM benchmarks using matmul_bench.py. Supports sparse@sparse and sparse@dense operations with optional CUDA and backward testing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/sparse/dlmc/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmatmul_bench.py --operation sparse@sparse|sparse@dense --backward-test --with-cuda\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.native_batch_norm.default\nDESCRIPTION: Logs native batch normalization operations (`aten.native_batch_norm.default`). These examples show normalization applied to 4D float16 input tensors, using float16 weight, bias, running mean, and running variance tensors. The `True` flag likely indicates training mode, with momentum 0.1 and epsilon 1e-05.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 24, 112, 112], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 56, 56, 56], f16), T([56], f16), T([56], f16), T([56], f16), T([56], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 56, 28, 28], f16), T([56], f16), T([56], f16), T([56], f16), T([56], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 152, 28, 28], f16), T([152], f16), T([152], f16), T([152], f16), T([152], f16), True, 0.1, 1e-05), {})\ncnt: 12, ((T([128, 152, 14, 14], f16), T([152], f16), T([152], f16), T([152], f16), T([152], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 368, 14, 14], f16), T([368], f16), T([368], f16), T([368], f16), T([368], f16), True, 0.1, 1e-05), {})\ncnt: 21, ((T([128, 368, 7, 7], f16), T([368], f16), T([368], f16), T([368], f16), T([368], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Graph Node List Structure in Python\nDESCRIPTION: This snippet defines the fundamental structure for a Graph in Export IR using Python, where a graph contains a list of Node instances. It prescribes the base schema for representing graphs within torch.fx and PyTorch's Export IR internals. There are no external dependencies needed beyond standard Python for the class skeleton; actual usages assume torch.fx.Node compatibility. The Graph expects a non-empty list of Node objects as its nodes property.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Graph:\n  nodes: List[Node]\n```\n\n----------------------------------------\n\nTITLE: Defining Operator Schema in PyTorch\nDESCRIPTION: Demonstrates the old and new schema for the 'linspace' operator, highlighting the change in the 'steps' parameter from optional to required.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nlinspace(start: Union[int, float, complex], end: Union[int, float, complex], steps: Optional[int], dtype: Optional[int], layout: Optional[int],\n                    device: Optional[Device], pin_memory: Optional[bool]):\n\nlinspace(start: Union[int, float, complex], end: Union[int, float, complex], steps: int, dtype: Optional[int], layout: Optional[int],\n                    device: Optional[Device], pin_memory: Optional[bool]):\n```\n\n----------------------------------------\n\nTITLE: Exporting Model with Context Manager - PyTorch - Python\nDESCRIPTION: This snippet defines a simple neural network module using PyTorch, which includes a custom context manager in the forward pass. It shows usage of strict and non-strict tracing modes with torch.export, highlighting that non-strict mode can bypass TorchDynamo limitations around context managers. Requires PyTorch (torch), and demonstrates exporting with and without strict mode by passing a strict flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport contextlib\nimport torch\n\nclass ContextManager():\n    def __init__(self):\n        self.count = 0\n    def __enter__(self):\n        self.count += 1\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.count -= 1\n\nclass M(torch.nn.Module):\n    def forward(self, x):\n        with ContextManager():\n            return x.sin() + x.cos()\n\nexport(M(), (torch.ones(3, 3),), strict=False)  # Non-strict traces successfully\nexport(M(), (torch.ones(3, 3),))  # Strict mode fails with torch._dynamo.exc.Unsupported: ContextManager\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch nll_loss Forward and Backward Operators - Python\nDESCRIPTION: This snippet enumerates example input arguments for the PyTorch ATen nll_loss_forward.default and nll_loss_backward.default operators. Inputs include tensors representing logits/probabilities, target indices, optional weights, reduction/fill parameters, and labels. These are relevant for loss and gradient checking in multiclass classification setups. Knowledge of PyTorch ATen interface and tensor dtype/shape conventions is required.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Computing Gradients through NumPy Code\nDESCRIPTION: Shows how to compute gradients through NumPy code using torch.compile and wrap_numpy decorator with backpropagation support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile(fullgraph=True)\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    return np.mean(np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1)))\n\nX = torch.randn(1024, 64, device=\"cuda\", requires_grad=True)\nY = torch.randn(1024, 64, device=\"cuda\")\nZ = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nZ.backward()\nprint(X.grad)\n```\n\n----------------------------------------\n\nTITLE: CUDA Test Sources Configuration\nDESCRIPTION: Appends CUDA-specific test source files and conditionally includes cuDNN tests if enabled\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/test/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND ATen_CUDA_TEST_SRCS\n  ${CMAKE_CURRENT_SOURCE_DIR}/cuda_allocator_test.cpp\n  [...additional files...])\nif(CAFFE2_USE_CUDNN)\n  list(APPEND ATen_CUDA_TEST_SRCS\n    ${CMAKE_CURRENT_SOURCE_DIR}/cuda_cudnn_test.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specifying Non-Tensor Argument Types With Type Annotations - PyTorch TorchScript - Python\nDESCRIPTION: This snippet demonstrates how to specify argument types for TorchScript functions using MyPy-style type annotations in a comment. The function foo takes an int and a tuple of two Tensors, returning their sum plus the int. This facilitates proper type checking and compilation in TorchScript. Requires torch and a valid PyTorch runtime. Expected inputs are an integer and a tuple of two tensors; the output is a tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.jit.script\ndef foo(x, tup):\n    # type: (int, Tuple[Tensor, Tensor]) -> Tensor\n    t0, t1 = tup\n    return t0 + t1 + x\n\nprint(foo(3, (torch.rand(3), torch.rand(3))))\n```\n\n----------------------------------------\n\nTITLE: Implementing Elastic Training Script in PyTorch\nDESCRIPTION: This code snippet demonstrates the structure of a PyTorch training script compatible with elastic training. It includes checkpoint loading and saving, process group initialization, and a training loop that resumes from the last saved epoch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/train_script.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n     args = parse_args(sys.argv[1:])\n     state = load_checkpoint(args.checkpoint_path)\n     initialize(state)\n\n     # torch.distributed.run ensures that this will work\n     # by exporting all the env vars needed to initialize the process group\n     torch.distributed.init_process_group(backend=args.backend)\n\n     for i in range(state.epoch, state.total_num_epochs)\n          for batch in iter(state.dataset)\n              train(batch, state.model)\n\n          state.epoch += 1\n          save_checkpoint(state)\n```\n\n----------------------------------------\n\nTITLE: Describing aten.add.Tensor, add_, addmm Usage - PyTorch - Python\nDESCRIPTION: Summarizes element-wise addition (add/add_) and matrix multiplication with addition (addmm) operators, often seen in residual and dense layers. Input and output tensor shapes and formats indicate use within deep convolutional blocks. add_ ops denote in-place modifications for memory efficiency, while addmm combines bias addition and matrix multiplication. Quantities reflect heavy usage throughout the model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 5, ((T([64, 2048, 7, 7], f16), T([64, 2048, 7, 7], f16)), {})\ncnt: 46, ((T([64, 1024, 14, 14], f16), T([64, 1024, 14, 14], f16)), {})\ncnt: 8, ((T([64, 512, 28, 28], f16), T([64, 512, 28, 28], f16)), {})\ncnt: 6, ((T([64, 256, 56, 56], f16), T([64, 256, 56, 56], f16)), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 106, ((T([], i64), 1), {})\ncnt: 3, ((T([64, 256, 56, 56], f16), T([64, 256, 56, 56], f16)), {})\ncnt: 4, ((T([64, 512, 28, 28], f16), T([64, 512, 28, 28], f16)), {})\ncnt: 23, ((T([64, 1024, 14, 14], f16), T([64, 1024, 14, 14], f16)), {})\ncnt: 3, ((T([64, 2048, 7, 7], f16), T([64, 2048, 7, 7], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([64, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})\n```\n\n----------------------------------------\n\nTITLE: Compiling NumPy Code with torch.compile\nDESCRIPTION: Demonstrates how to use torch.compile with native NumPy code, showing integration between PyTorch compilation and NumPy operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\n\n@torch.compile\ndef numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = np.random.randn(1024, 64)\nY = np.random.randn(1024, 64)\nZ = numpy_fn(X, Y)\nassert isinstance(Z, np.ndarray)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Rendezvous Handler for TorchElastic in Python\nDESCRIPTION: This snippet shows how to implement a custom rendezvous handler by extending the RendezvousHandler class and passing it to the worker specification when creating an agent.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/customization.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nspec = WorkerSpec(\n    rdzv_handler=MyRendezvousHandler(params),\n    ...\n)\nelastic_agent = LocalElasticAgent(spec, start_method=start_method)\nelastic_agent.run(spec.role)\n```\n\n----------------------------------------\n\nTITLE: Updating Specific Test Models in PyTorch\nDESCRIPTION: These commands demonstrate how to update specific test models (reduction_ops and mobilenet_v2) using the gen_test_model.py script. This is useful when a test model needs to be regenerated due to changes in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/model_test/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython test/mobile/model_test/gen_test_model.py reduction_ops\npython test/mobile/model_test/gen_test_model.py mobilenet_v2\n```\n\n----------------------------------------\n\nTITLE: Tensor Cloning Operation in PyTorch Neural Network\nDESCRIPTION: Summary of tensor cloning operation (aten.clone.default) in the model, showing count and tensor shape. This represents the initial cloning of the input image tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([32, 3, 224, 224], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Concatenating DataPipes in PyTorch\nDESCRIPTION: Demonstrates the use of the concat() method to combine multiple DataPipes sequentially, yielding elements from each DataPipe in order.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(4)\ndp2 = ExampleIterPipe(3)\ndp = dp.concat(dp2)\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Ensembling Models using functorch.combine_state_for_ensemble and vmap in Python\nDESCRIPTION: Shows how to perform model ensembling using the legacy `functorch` library. `functorch.combine_state_for_ensemble` takes a list of models and stacks their parameters and buffers, returning a functional model `fmodel` and the stacked state (`params`, `buffers`). `functorch.vmap` is then used to apply `fmodel` in parallel across the ensemble dimension (first dimension of `params` and `buffers`), processing the same input `data`. Requires `torch` and `functorch`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nnum_models = 5\nbatch_size = 64\nin_features, out_features = 3, 3\nmodels = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]\ndata = torch.randn(batch_size, 3)\n\n# ---------------\n# using functorch\n# ---------------\nimport functorch\nfmodel, params, buffers = functorch.combine_state_for_ensemble(models)\noutput = functorch.vmap(fmodel, (0, 0, None))(params, buffers, data)\nassert output.shape == (num_models, batch_size, out_features)\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Transformer Model\nDESCRIPTION: This code snippet represents a summary of PyTorch operator usage in a deep learning model. It includes tensor operations, loss calculations, and various tensor manipulations typical in transformer architectures. The snippet shows the operator name, count of usage, and the input parameters for each operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BigBird_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 26, ((T([1, 1024, 768], f16), T([1, 1024, 768], f16), [768], T([1, 1024, 1], f32), T([1, 1024, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\nOperator: aten.new_empty_strided.default\ncnt: 36, ((T([144, 64, 64], f16), [144, 64, 64], [4096, 64, 1]), {})\nOperator: aten.new_ones.default\ncnt: 24, ((T([1, 1, 1, 1024], f16), [1, 1, 1, 192]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\ncnt: 24, ((T([1, 12, 14, 64, 192], f32), [1, 12, 64, 256]), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\nOperator: aten.new_zeros.default\ncnt: 12, ((T([12, 12, 64, 64], f16, stride=(64, 49152, 768, 1)), [589824]), {})\ncnt: 24, ((T([504, 64, 64], f16), [192, 64, 64]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([1024, 50358], f16), T([1024], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([1024, 50358], f16), T([1024], i64), None, 1, -100), {})\nOperator: aten.pow.Tensor_Scalar\ncnt: 12, ((T([1, 1024, 3072], f16), 3.0), {})\ncnt: 1, ((T([1, 1024, 768], f16), 3.0), {})\ncnt: 1, ((T([1, 1024, 768], f16), 2.0), {})\ncnt: 12, ((T([1, 1024, 3072], f16), 2.0), {})\nOperator: aten.rsub.Scalar\ncnt: 24, ((T([1, 1, 1, 1024], f16), 1.0), {})\ncnt: 24, ((T([1, 12, 64, 448], f32), 1.0), {})\ncnt: 12, ((T([1, 1, 12, 64, 192], f16), 1.0), {})\ncnt: 24, ((T([1, 1, 1, 1, 64], f16), 1.0), {})\ncnt: 12, ((T([1, 12, 12, 64, 192], f32, stride=(2064384, 172032, 12288, 192, 1)), 1.0), {})\nOperator: aten.select_backward.default\ncnt: 24, ((T([1, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, -1), {})\ncnt: 12, ((T([1, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, -2), {})\ncnt: 12, ((T([1, 12, 192, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 14, 192, 64], 2, -1), {})\ncnt: 24, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, -1), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, -2), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, -3), {})\ncnt: 24, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, 0), {})\ncnt: 12, ((T([1, 12, 192, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 14, 192, 64], 2, -1), {})\ncnt: 24, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, -1), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, -2), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, -3), {})\ncnt: 24, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, 0), {})\ncnt: 24, ((T([1, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, 0), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(64, 4096, 1, 64)), [1, 12, 16, 64, 64], 2, -1), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(64, 4096, 1, 64)), [1, 12, 16, 64, 64], 2, 0), {})\ncnt: 12, ((T([1, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, 1), {})\ncnt: 12, ((T([1, 12, 192, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 14, 192, 64], 2, 0), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, 2), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 64, 1)), [1, 12, 16, 64, 64], 2, 1), {})\ncnt: 12, ((T([1, 12, 192, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 14, 192, 64], 2, 0), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, 2), {})\ncnt: 12, ((T([1, 12, 64, 64], f16, stride=(344064, 28672, 1, 448)), [1, 12, 16, 64, 64], 2, 1), {})\nOperator: aten.slice_backward.default\ncnt: 372, ((T([1, 12, 16, 64, 64], f16), [1, 12, 16, 64, 64], 1, 0, 9223372036854775807, 1), {})\ncnt: 372, ((T([1, 12, 16, 64, 64], f16), [1, 12, 16, 64, 64], 0, 0, 9223372036854775807, 1), {})\ncnt: 72, ((T([1, 12, 14, 192, 64], f16), [1, 12, 14, 192, 64], 1, 0, 9223372036854775807, 1), {})\ncnt: 72, ((T([1, 12, 14, 192, 64], f16), [1, 12, 14, 192, 64], 0, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 64], f16), [1, 12, 12, 64, 512], 4, -64, 9223372036854775807, 1), {})\ncnt: 48, ((T([1, 12, 12, 64, 512], f16), [1, 12, 12, 64, 512], 3, 0, 9223372036854775807, 1), {})\ncnt: 48, ((T([1, 12, 12, 64, 512], f16), [1, 12, 12, 64, 512], 2, 0, 9223372036854775807, 1), {})\ncnt: 48, ((T([1, 12, 12, 64, 512], f16), [1, 12, 12, 64, 512], 1, 0, 9223372036854775807, 1), {})\ncnt: 48, ((T([1, 12, 12, 64, 512], f16), [1, 12, 12, 64, 512], 0, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 64], f16), [1, 12, 12, 64, 512], 4, 0, 64, 1), {})\ncnt: 12, ((T([1, 12, 12, 192, 64], f16), [1, 12, 14, 192, 64], 2, 1, -1, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 192], f16), [1, 12, 12, 64, 512], 4, 256, -64, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 192], f16), [1, 12, 12, 64, 512], 4, 64, 256, 1), {})\ncnt: 12, ((T([1, 12, 12, 192, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [1, 12, 14, 192, 64], 2, 1, -1, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 64], f16), [1, 12, 16, 64, 64], 2, 2, -2, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [1, 12, 16, 64, 64], 2, 3, -1, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [1, 12, 16, 64, 64], 2, 2, -2, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [1, 12, 16, 64, 64], 2, 1, -3, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [1, 12, 16, 64, 64], 2, 3, -1, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [1, 12, 16, 64, 64], 2, 2, -2, 1), {})\ncnt: 12, ((T([1, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [1, 12, 16, 64, 64], 2, 1, -3, 1), {})\nOperator: aten.stack.default\ncnt: 12, (([T([504, 64], f32)],), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([1024, 50358], f16), [0], True), {})\ncnt: 49, ((T([1024, 768], f16), [0], True), {})\ncnt: 12, ((T([1024, 3072], f16), [0], True), {})\ncnt: 12, ((T([1024, 768], f16, stride=(1, 1024)), [0], True), {})\nOperator: aten.tanh.default\ncnt: 12, ((T([1, 1024, 3072], f16),), {})\ncnt: 1, ((T([1, 768], f16),), {})\ncnt: 1, ((T([1, 1024, 768], f16),), {})\nOperator: aten.tanh_backward.default\ncnt: 1, ((T([1, 1024, 768], f16), T([1, 1024, 768], f16)), {})\ncnt: 12, ((T([1, 1024, 3072], f16), T([1, 1024, 3072], f16)), {})\nOperator: aten.unbind.int\ncnt: 12, ((T([1, 16, 64], f32),), {})\ncnt: 12, ((T([1, 12, 14, 3], i64),), {})\nOperator: aten.unsqueeze_.default\ncnt: 1, ((T([1, 12, 64, 192], f32), 1), {})\ncnt: 12, ((T([12, 14, 3], i64), 0), {})\ncnt: 48, ((T([1, 12, 64, 64], f16), 2), {})\n```\n\n----------------------------------------\n\nTITLE: Generating A100 Mixed MM Heuristics\nDESCRIPTION: Generates the mixed matrix multiplication heuristic specifically for NVIDIA A100 GPU using pre-collected data\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mixed_mm/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash gen_mixedmm_heuristic_a100.sh\n```\n\n----------------------------------------\n\nTITLE: Moving Meta Modules to CPU using to_empty in Python\nDESCRIPTION: Demonstrates using the `to_empty(device=\"cpu\")` method on an `nn.Module` previously initialized on the 'meta' device. This converts the module structure to the target device ('cpu') but leaves its parameters uninitialized, requiring explicit reinitialization afterward. Requires `torch` and `torch.nn.modules`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> from torch.nn.modules import Linear\n>>> with torch.device('meta'):\n...     m = Linear(20, 30)\n>>> m.to_empty(device=\"cpu\")\nLinear(in_features=20, out_features=30, bias=True)\n```\n\n----------------------------------------\n\nTITLE: Adding Tensors in PyTorch\nDESCRIPTION: The aten.add.Tensor operator demonstrates how addition is performed on two tensors, both shaped [1, 512, 1536] with float16 precision. Addition operators like this are fundamental for many tensor operations throughout model calculations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 144, ((T([1, 512, 1536], f16), T([1, 512, 1536], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Using cuDNN Header Guards in C++\nDESCRIPTION: Example showing proper guarding of cuDNN header includes and definition usage with AT_CUDNN_ENABLED() macro in PyTorch implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cudnn/README.md#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n#if AT_CUDNN_ENABLED()\n#include<ATen/cudnn/*.h>\n// cuDNN-dependent code here\n#endif\n```\n\n----------------------------------------\n\nTITLE: Defining Torch Library in C++\nDESCRIPTION: Macro for defining a new Torch library. Used to create custom operators and data types that can be used in PyTorch's eager API and TorchScript.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/library.rst#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nTORCH_LIBRARY\n```\n\n----------------------------------------\n\nTITLE: Collecting TIMM Models Training Data\nDESCRIPTION: Command to collect matrix multiplication training data from TIMM models using TorchInductor\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mm/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_AUTOHEURISTIC_USE=\"\" TORCHINDUCTOR_AUTOHEURISTIC_COLLECT=\"mm\" TORCHINDUCTOR_AUTOHEURISTIC_LOG_PATH=\"timm_train_mm.txt\" TORCHINDUCTOR_MAX_AUTOTUNE=1 time python ../../../benchmarks/dynamo/timm_models.py --ci --performance --timing --explain --inductor --device cuda --train --amp\n```\n\n----------------------------------------\n\nTITLE: Enabling OpInfo Test Case in PyTorch Testing\nDESCRIPTION: Example of adding a TorchLibOpInfo entry to enable a new OpInfo test case in the PyTorch testing framework.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nTorchLibOpInfo(\"ops.aten.slice_scatter\", core_ops.aten_slice_scatter)\n```\n\n----------------------------------------\n\nTITLE: Disabling Reduced Precision Reduction for BF16 GEMMs in PyTorch (Python)\nDESCRIPTION: Shows how to disable reduced precision reductions for BFloat16 (BF16) General Matrix Multiplications (GEMMs). Setting `torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction` to `False` can improve numerical stability, although BF16 reductions are enabled by default.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.nll_loss_backward.default in PyTorch ATen\nDESCRIPTION: Logs calls to the `aten.nll_loss_backward.default` operator, computing the gradient for the negative log-likelihood loss. Arguments include the gradient w.r.t the loss output (scalar `[]`, f16), the model's output predictions (log probabilities, `[128, 1000]`, f16), target labels (`[128]`, i64), an optional weight tensor (`None`), the reduction mode (`1` indicating mean), the index to ignore (`-100`), and the total weight (scalar `[]`, f16). The `cnt` indicates call frequency.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Nonlinear Linear Model Function - Python\nDESCRIPTION: Defines the 'predict' function that computes a linear transformation (with weight and bias) followed by a tanh activation. Used as a test function for Jacobian and Hessian computations. Input parameters: weight (matrix), bias (vector), x (feature vector). Returns: activated linear output. Requires torch.nn.functional and input tensors of matching shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef predict(weight, bias, x):\n    return F.linear(x, weight, bias).tanh()\n```\n\n----------------------------------------\n\nTITLE: NTK Computation using Vector Products\nDESCRIPTION: Alternative implementation of empirical NTK computation using NTK-vector products method, which can be more efficient for certain model architectures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/neural_tangent_kernels.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef empirical_ntk_ntk_vps(func, params, x1, x2, compute='full'):\n    def get_ntk(x1, x2):\n        def func_x1(params):\n            return func(params, x1)\n\n        def func_x2(params):\n            return func(params, x2)\n\n        output, vjp_fn = vjp(func_x1, params)\n\n        def get_ntk_slice(vec):\n            vjps = vjp_fn(vec)\n            _, jvps = jvp(func_x2, (params,), vjps)\n            return jvps\n\n        basis = torch.eye(output.numel(), dtype=output.dtype, device=output.device).view(output.numel(), -1)\n        return vmap(get_ntk_slice)(basis)\n        \n    result = vmap(vmap(get_ntk, (None, 0)), (0, None))(x1, x2)\n    \n    if compute == 'full':\n        return result\n    if compute == 'trace':\n        return torch.einsum('NMKK->NM', result)\n    if compute == 'diagonal':\n        return torch.einsum('NMKK->NMK', result)\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple Linear-ReLU Module in PyTorch\nDESCRIPTION: This code snippet defines a simple PyTorch module that combines a linear layer with a ReLU activation. It serves as the starting point for the quantization process example.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LinearReLUModule(torch.nn.Module):\n   def __init__(self) -> None:\n       super().__init__()\n       self.linear = torch.nn.Linear(5, 10).float()\n       self.relu = torch.nn.ReLU()\n\n   def forward(self, x):\n       return self.relu(self.linear(x))\n```\n\n----------------------------------------\n\nTITLE: Handling Errors in PyBind11 Bindings\nDESCRIPTION: This snippet shows how to handle errors when using PyBind11 for Python bindings. It uses HANDLE_TH_ERRORS and END_HANDLE_TH_ERRORS_PYBIND macros to raise PyTorch errors and warnings natively while letting PyBind11 handle other errors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/README.md#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// Function given to the pybind binding\nat::Tensor foo(at::Tensor x) {\n  HANDLE_TH_ERRORS\n  ...\n  if (!x) throw python_error();\n  // pybind native error\n  if (!x) throw py::value_error();\n  // From c10/Exception.h\n  TORCH_CHECK(cond, \"cond was false here\");\n  TORCH_WARN(\"Warning message\");\n  ...\n  END_HANDLE_TH_ERRORS_PYBIND\n}\n```\n\n----------------------------------------\n\nTITLE: Tracking Convolution Forward Operations in Neural Network\nDESCRIPTION: Lists all convolution operations in the forward pass, showing kernel sizes, strides, padding, and tensor shapes. The network uses a combination of standard convolutions, depthwise separable convolutions (with groups parameter), and 1x1 pointwise convolutions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([16, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([48, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 48, 112, 112], f16), T([48, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 48), {})\ncnt: 1, ((T([128, 48, 56, 56], f16), T([24, 48, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 24, 56, 56], f16), T([72, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 72, 56, 56], f16), T([72, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 72), {})\ncnt: 1, ((T([128, 72, 1, 1], f16), T([24, 72, 1, 1], f16), T([24], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 24, 1, 1], f16), T([72, 24, 1, 1], f16), T([72], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 72, 56, 56], f16), T([24, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 72, 56, 56], f16), T([72, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 72), {})\ncnt: 1, ((T([128, 72, 28, 28], f16), T([40, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 40, 28, 28], f16), T([240, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([240, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 240), {})\ncnt: 2, ((T([128, 240, 1, 1], f16), T([64, 240, 1, 1], f16), T([64], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 64, 1, 1], f16), T([240, 64, 1, 1], f16), T([240], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([40, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([240, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 240), {})\ncnt: 1, ((T([128, 240, 14, 14], f16), T([80, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 80, 14, 14], f16), T([480, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([480, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 480), {})\ncnt: 2, ((T([128, 480, 1, 1], f16), T([120, 480, 1, 1], f16), T([120], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 120, 1, 1], f16), T([480, 120, 1, 1], f16), T([480], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 480, 14, 14], f16), T([80, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 480, 14, 14], f16), T([112, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 112, 14, 14], f16), T([672, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 2, ((T([128, 672, 1, 1], f16), T([168, 672, 1, 1], f16), T([168], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 168, 1, 1], f16), T([672, 168, 1, 1], f16), T([672], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 672, 14, 14], f16), T([112, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), T([192, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 7, 7], f16), T([1152, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1152, 7, 7], f16), T([1152, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1152), {})\ncnt: 1, ((T([128, 1152, 1, 1], f16), T([288, 1152, 1, 1], f16), T([288], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 288, 1, 1], f16), T([1152, 288, 1, 1], f16), T([1152], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1152, 7, 7], f16), T([192, 1152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 7, 7], f16), T([960, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 960, 1, 1], f16), T([1280, 960, 1, 1], f16), T([1280], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Dynamic Shapes Usage in PyTorch\nDESCRIPTION: Demonstrates using dynamic shapes to handle varying tensor dimensions without recompilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile(dynamic=True)\ndef fn(x):\n    return x + 1\n\nfn(torch.ones(3, 3))\nfn(torch.ones(4, 4))\n```\n\n----------------------------------------\n\nTITLE: Using torch.use_deterministic_algorithms() in PyTorch\nDESCRIPTION: This code snippet shows how to enable deterministic algorithms in PyTorch using the torch.use_deterministic_algorithms() method. When set to True, it activates the fill_uninitialized_memory feature if it's enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/deterministic.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ntorch.use_deterministic_algorithms(True)\n```\n\n----------------------------------------\n\nTITLE: CapabilityBasedPartitioner Usage Example\nDESCRIPTION: Demonstrates using CapabilityBasedPartitioner with custom operator support for add and mul operations\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.fx.passes.infra.partitioner import CapabilityBasedPartitioner\nfrom torch.fx.passes.operator_support import any_chain, OperatorSupportBase\n\nclass AddMulOperatorSupport(OperatorSupportBase):\n    def is_node_supported(self, submodules, node: torch.fx.Node) -> bool:\n        return node.op == \"call_function\" and node.target in [\n            torch.ops.aten.add.Tensor, torch.ops.aten.mul.Tensor,\n        ]\n\ncapability_partitioner = CapabilityBasedPartitioner(\n    graph_module,\n    op_support,\n)\n\n# Returns a list of partitions (list of nodes that belong in each partition)\npartition_list = capability_partitioner.propose_partitions()\n# Fuses the partitions into graph modules and inserts `call_module` nodes in the graph\nfused_graph_module = capability_partitioner.fuse_partitions(partition_list)\n```\n\n----------------------------------------\n\nTITLE: Generating Python Code from FX IR in PyTorch\nDESCRIPTION: This code snippet demonstrates how FX generates valid Python source code based on the IR it is instantiated with. It shows the forward method of a traced module, including operations like addition, linear transformation, and clamping.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/fx/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x):\n    param = self.param\n    add_1 = x + param;  x = param = None\n    linear_1 = self.linear(add_1);  add_1 = None\n    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None\n    return clamp_1\n```\n\n----------------------------------------\n\nTITLE: Per-Sample Gradients Computation\nDESCRIPTION: Example showing how to compute per-sample gradients using vmap and grad transforms together.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import vmap\nbatch_size, feature_size = 3, 5\n\ndef model(weights,feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\ndef compute_loss(weights, example, target):\n    y = model(weights, example)\n    return ((y - target) ** 2).mean()  # MSELoss\n\nweights = torch.randn(feature_size, requires_grad=True)\nexamples = torch.randn(batch_size, feature_size)\ntargets = torch.randn(batch_size)\ninputs = (weights,examples, targets)\ngrad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n```\n\n----------------------------------------\n\nTITLE: Initializing Tensors for Batch Jacobian Computation in PyTorch\nDESCRIPTION: Creates the necessary tensors for batch Jacobian computation: a weight matrix, bias vector, and batch of input features. The weight dimensions connect Din input features to Dout output features.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nbatch_size = 64\nDin = 31\nDout = 33\n\nweight = torch.randn(Dout, Din)\nprint(f\"weight shape = {weight.shape}\")\n\nbias = torch.randn(Dout)\n\nx = torch.randn(batch_size, Din)\n```\n\n----------------------------------------\n\nTITLE: Creating a Stage Module\nDESCRIPTION: This snippet demonstrates how to retrieve a model partition as a nn.Module using the Pipe object. The stage_mod can be utilized for creating an optimizer or loading checkpoints. No special prerequisites other than an initialized pipe object are needed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nstage_mod : nn.Module = pipe.get_stage_module(stage_idx)\n```\n\n----------------------------------------\n\nTITLE: Implementing TorchScript Enums with the Enum Class\nDESCRIPTION: Example showing how to define and use Python enums in TorchScript. Enums can be used without additional annotations, but all values must be of the same type (int, float, or str).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nfrom enum import Enum\n\n\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n\n@torch.jit.script\ndef enum_fn(x: Color, y: Color) -> bool:\n    if x == Color.RED:\n        return True\n\n    return x == y\n```\n\n----------------------------------------\n\nTITLE: Documenting Compare Class in PyTorch Benchmark Utils\nDESCRIPTION: This snippet documents the Compare class from the torch.utils.benchmark module, including all its members.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_6\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: Compare\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Static Control Flow Example in PyTorch FX\nDESCRIPTION: Shows successful symbolic tracing of static control flow where conditions depend on module parameters rather than inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Paths Based on Target Platform in CMake\nDESCRIPTION: Sets the main C++ source directory `pytorch_android_DIR`. Based on whether `ANDROID_ABI` is defined (Android build), `BUILD_LIBTORCH_WITH_JNI` is true (internal PyTorch build), or neither (using pre-built LibTorch), it configures `USE_VULKAN`, `libtorch_include_DIR`, `BUILD_SUBDIR`, and potentially linking directories (`LIBTORCH_HOME`). Requires `LIBTORCH_HOME` for non-Android builds using pre-built LibTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(pytorch_android_DIR ${CMAKE_CURRENT_LIST_DIR}/src/main/cpp)\n\nif(ANDROID_ABI)\n  set(USE_VULKAN ON)\n  set(libtorch_include_DIR ${pytorch_android_DIR}/libtorch_include/${ANDROID_ABI})\n  set(BUILD_SUBDIR ${ANDROID_ABI})\nelsif(BUILD_LIBTORCH_WITH_JNI)\n  # Don't need LIBTORCH_HOME if we're building from within PyTorch.\nelse()\n  # Building against a pre-built libtorch.\n  if(NOT LIBTORCH_HOME)\n    message(FATAL_ERROR\n      \"pytorch_android requires LIBTORCH_HOME to be defined for non-Android builds.\")\n  endif()\n  set(libtorch_include_DIR ${LIBTORCH_HOME}/include)\n  link_directories(${LIBTORCH_HOME}/lib)\n  set(BUILD_SUBDIR host)\nendif()\n\nmessage(STATUS \"libtorch dir:${libtorch_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorOptions in PyTorch C++\nDESCRIPTION: This example shows how to create a TensorOptions object with custom dtype, layout, device, and requires_grad settings. It configures a 64-bit float, strided tensor that requires a gradient and lives on CUDA device 1.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nauto options =\n  torch::TensorOptions()\n    .dtype(torch::kFloat32)\n    .layout(torch::kStrided)\n    .device(torch::kCUDA, 1)\n    .requires_grad(true);\n```\n\n----------------------------------------\n\nTITLE: Defining TorchScript Type System Grammar\nDESCRIPTION: Specifies the grammar for TorchScript's type system, including primitive types, structural types, and nominal types. It defines the syntax for various type constructs like tuples, lists, dictionaries, and custom classes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_17\n\nLANGUAGE: TorchScript\nCODE:\n```\nTSPrimitiveType ::= \"int\" | \"float\" | \"double\" | \"complex\" | \"bool\" | \"str\" | \"None\"\n\nTSStructuralType ::= TSTuple | TSNamedTuple | TSList | TSDict | TSOptional |\n                     TSUnion | TSFuture | TSRRef | TSAwait\nTSTuple         ::= \"Tuple\" \"[\" (TSType \",\")* TSType \"]\"\nTSNamedTuple    ::= \"namedtuple\" \"(\" (TSType \",\")* TSType \")\"\nTSList          ::= \"List\" \"[\" TSType \"]\"\nTSOptional      ::= \"Optional\" \"[\" TSType \"]\"\nTSUnion         ::= \"Union\" \"[\" (TSType \",\")* TSType \"]\"\nTSFuture        ::= \"Future\" \"[\" TSType \"]\"\nTSRRef          ::= \"RRef\" \"[\" TSType \"]\"\nTSAwait         ::= \"Await\" \"[\" TSType \"]\"\nTSDict          ::= \"Dict\" \"[\" KeyType \",\" TSType \"]\"\nKeyType         ::= \"str\" | \"int\" | \"float\" | \"bool\" | TensorType | \"Any\"\n\nTSNominalType   ::= TSBuiltinClasses | TSCustomClass | TSEnum\nTSBuiltinClass  ::= TSTensor | \"torch.device\" | \"torch.stream\"|\n                    \"torch.dtype\" | \"torch.nn.ModuleList\" |\n                    \"torch.nn.ModuleDict\" | ...\nTSTensor        ::= \"torch.tensor\" and subclasses\n```\n\n----------------------------------------\n\nTITLE: Dynamic Shapes Support with Nested Jagged Tensors in PyTorch\nDESCRIPTION: Demonstrates how NJTs support dynamic shapes with torch.compile to avoid unnecessary recompiles when the ragged structure changes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(4, 3)\n>>> c = torch.randn(5, 3)\n>>> d = torch.randn(6, 3)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt2 = torch.nested.nested_tensor([c, d], layout=torch.jagged)\n>>> def f(x): return x.sin() + 1\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output1 = compiled_f(nt1)\n>>> output2 = compiled_f(nt2)  # NB: No recompile needed even though ragged structure differs\n```\n\n----------------------------------------\n\nTITLE: ResNet50 Optimization Example\nDESCRIPTION: Shows how to apply torch.compile to optimize a pre-trained ResNet50 model from PyTorch hub using the inductor backend.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(1,3,64,64))\n```\n\n----------------------------------------\n\nTITLE: Enabling Watchdog in LocalElasticAgent\nDESCRIPTION: Sets up environment variables to enable a named pipe based watchdog in LocalElasticAgent. The TORCHELASTIC_ENABLE_FILE_TIMER variable must be set to 1, and optionally TORCHELASTIC_TIMER_FILE can specify a unique file name for the named pipe.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/agent.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nos.environ[\"TORCHELASTIC_ENABLE_FILE_TIMER\"] = \"1\"\nos.environ[\"TORCHELASTIC_TIMER_FILE\"] = \"unique_pipe_name\"  # Optional\n```\n\n----------------------------------------\n\nTITLE: Using oneDNN Graph with BFloat16 Data Type\nDESCRIPTION: Example demonstrating how to use oneDNN Graph with BFloat16 precision. This includes enabling oneDNN Graph fusion, disabling JIT AMP, and using CPU AMP autocast for BFloat16 operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/onednn/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Assuming we have a model of the name 'model'\n\nexample_input = torch.rand(1, 3, 224, 224)\n\n# enable oneDNN Graph\ntorch.jit.enable_onednn_fusion(True)\n# Disable AMP for JIT\ntorch._C._jit_set_autocast_mode(False)\nwith torch.no_grad(), torch.cpu.amp.autocast():\n    model = torch.jit.trace(model, (example_input))\n    model = torch.jit.freeze(model)\n     # 2 warm-ups (2 for tracing/scripting with an example, 3 without an example)\n    model(example_input)\n    model(example_input)\n\n    # speedup would be observed in subsequent runs.\n    model(example_input)\n```\n\n----------------------------------------\n\nTITLE: Using Nested Jagged Tensors with torch.compile in PyTorch\nDESCRIPTION: Shows how to use nested jagged tensors with torch.compile for optimal performance. Demonstrates compiling functions that work with NJTs directly or create them from values and offsets.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(4, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> def f(x): return x.sin() + 1\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output = compiled_f(nt)\n>>> output.shape\ntorch.Size([2, j1, 3])\n>>> def g(values, offsets): return torch.nested.nested_tensor_from_jagged(values, offsets) * 2.\n...\n>>> compiled_g = torch.compile(g, fullgraph=True)\n>>> output2 = compiled_g(nt.values(), nt.offsets())\n>>> output2.shape\ntorch.Size([2, j1, 3])\n```\n\n----------------------------------------\n\nTITLE: Importing Core PyTorch and Utility Modules - Python\nDESCRIPTION: This snippet imports PyTorch modules, the neural network functional interface, and the partial function utility from functools. It sets a manual random seed for reproducibility. No external dependencies beyond PyTorch and standard library. Ensures code reproducibility and provides necessary functionality for deep learning computations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\n_ = torch.manual_seed(0)\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorch Module State Management\nDESCRIPTION: Demonstrates state management in PyTorch modules including parameter dictionaries, persistent and non-persistent buffers, and state saving/loading.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nself.param_dict = nn.ParameterDict({\n  'foo': nn.Parameter(torch.randn(3)),\n  'bar': nn.Parameter(torch.randn(4))\n})\n\nself.register_buffer('buffer1', torch.randn(4), persistent=True)\nself.register_buffer('buffer2', torch.randn(5), persistent=False)\nself.register_buffer('buffer3', None)\nself.linear = nn.Linear(2, 3)\n\nm = StatefulModule()\ntorch.save(m.state_dict(), 'state.pt')\nm_loaded = StatefulModule()\nm_loaded.load_state_dict(torch.load('state.pt'))\n```\n\n----------------------------------------\n\nTITLE: Defining Valid DataPipe Classes in Python\nDESCRIPTION: Shows valid DataPipe class definitions with correct type hints and subtyping for the __iter__ method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe[Tuple]):\n    def __iter__(self) -> Iterator[Tuple[int, str]]:\n        pass\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe):\n    def __iter__(self) -> Iterator[int]:\n        pass\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe):\n    def __iter__(self):\n        pass\nprint(DP.type)\nclass DP(IterDataPipe):\n    def __iter__(self) -> Iterator:\n        pass\nprint(DP.type)\nclass DP(IterDataPipe):\n    def __iter__(self) -> Iterator[T_co]:\n        pass\nprint(DP.type)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe[Tuple[T_co, str]]):\n    def __iter__(self) -> Iterator[Tuple[T_co, str]]:\n        pass\nprint(DP.type)\n\nT = TypeVar('T', int, str)  # equals to Union[int, str]\nclass DP(IterDataPipe[Tuple[T, str]]):\n    def __iter__(self) -> Iterator[Tuple[Union[int, str], str]]:\n        pass\nprint(DP.type)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Python Extension in CMake\nDESCRIPTION: This snippet sets up the torch_python shared library, including source files, compile options, and dependencies. It also configures platform-specific settings and handles various build options.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(torch_python SHARED ${TORCH_PYTHON_SRCS})\ntorch_compile_options(torch_python)\nif(APPLE)\n  target_compile_options(torch_python PRIVATE\n      $<$<COMPILE_LANGUAGE:CXX>: -fvisibility=default>)\nendif()\n\nif(CAFFE2_USE_MKL AND BUILD_LIBTORCHLESS)\n  set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)\n  target_link_libraries(torch_python PRIVATE caffe2::mkl)\nendif()\n\nadd_dependencies(torch_python onnx_proto)\nif(USE_NUMPY)\n  target_link_libraries(torch_python PRIVATE Python::NumPy)\n  target_compile_definitions(torch_python PRIVATE USE_NUMPY)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Complete Distributed Training Example with PyTorch\nDESCRIPTION: End-to-end example demonstrating distributed autograd and optimizer implementation. Includes RPC initialization, remote tensor operations, distributed backward pass, and optimizer step execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.multiprocessing as mp\nimport torch.distributed.autograd as dist_autograd\nfrom torch.distributed import rpc\nfrom torch import optim\nfrom torch.distributed.optim import DistributedOptimizer\n\ndef random_tensor():\n    return torch.rand((3, 3), requires_grad=True)\n\ndef _run_process(rank, dst_rank, world_size):\n    name = \"worker{}\".format(rank)\n    dst_name = \"worker{}\".format(dst_rank)\n\n    # Initialize RPC.\n    rpc.init_rpc(\n        name=name,\n        rank=rank,\n        world_size=world_size\n    )\n\n    # Use a distributed autograd context.\n    with dist_autograd.context() as context_id:\n        # Forward pass (create references on remote nodes).\n        rref1 = rpc.remote(dst_name, random_tensor)\n        rref2 = rpc.remote(dst_name, random_tensor)\n        loss = rref1.to_here() + rref2.to_here()\n\n        # Backward pass (run distributed autograd).\n        dist_autograd.backward(context_id, [loss.sum()])\n\n        # Build DistributedOptimizer.\n        dist_optim = DistributedOptimizer(\n        optim.SGD,\n        [rref1, rref2],\n        lr=0.05,\n        )\n\n        # Run the distributed optimizer step.\n        dist_optim.step(context_id)\n\ndef run_process(rank, world_size):\n    dst_rank = (rank + 1) % world_size\n    _run_process(rank, dst_rank, world_size)\n    rpc.shutdown()\n\nif __name__ == '__main__':\n  # Run world_size workers\n  world_size = 2\n  mp.spawn(run_process, args=(world_size,), nprocs=world_size)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tensor Allocations for aten.new_empty_strided.default - Python\nDESCRIPTION: Each snippet defines example input shapes, dtypes, and striding information for the 'aten.new_empty_strided.default' PyTorch operator. Inputs describe the base tensor, target allocation shape, and the physical memory strides for layout testing. Used to verify tensor memory layouts, these testcases require PyTorch installed and can be limited by CUDA/CPU capabilities when running large or exotic shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.new_empty_strided.default\ncnt: 5, ((T([128, 160, 7, 7], f16), [128, 160, 7, 7], [7840, 49, 7, 1]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 112, 14, 14], f16), [128, 112, 14, 14], [21952, 196, 14, 1]), {})\n```\n\n----------------------------------------\n\nTITLE: Computing Hessians by Composing Jacobian Transforms in Python\nDESCRIPTION: This snippet demonstrates how to compute the Hessian matrix (matrix of second-order partial derivatives) by composing Jacobian transforms. It defines a function `f` that sums the sine of input elements. The Hessian is computed in two ways: first by composing `jacrev` with itself (`jacrev(jacrev(f))`), and second by composing `jacfwd` with `jacrev` (`jacfwd(jacrev(f))`). Both methods yield the Hessian matrix for the function `f` evaluated at `x`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.whirlwind_tour.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    return x.sin().sum()\n\nx = torch.randn(5)\nhessian0 = jacrev(jacrev(f))(x)\nhessian1 = jacfwd(jacrev(f))(x)\n```\n\n----------------------------------------\n\nTITLE: Detecting Shape Mismatches in torch.distributed Collectives - Python\nDESCRIPTION: This Python snippet illustrates setting up a distributed job using NCCL with torch.distributed, intentionally passing mismatched tensor sizes to dist.all_reduce to provoke runtime shape consistency checks. It highlights initialization of the process group, CUDA device allocation per rank, and setting TORCH_DISTRIBUTED_DEBUG to DETAIL for enhanced debug logging. Requires torch, torch.multiprocessing, torch.distributed, and CUDA; expected output is a RuntimeError when input shapes into a collective differ, allowing easier root-cause analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    tensor = torch.randn(10 if rank == 0 else 20).cuda()\n    dist.all_reduce(tensor)\n    torch.cuda.synchronize(device=rank)\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n    mp.spawn(worker, nprocs=2, args=())\n```\n\n----------------------------------------\n\nTITLE: Using DataPipes for CSV Processing\nDESCRIPTION: Example usage of DataPipes to create a pipeline for processing CSV files, demonstrating chaining of multiple DataPipes using functional API.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch.utils.data.datapipes as dp\n\nFOLDER = 'path/2/csv/folder'\ndatapipe = dp.iter.FileLister([FOLDER]).filter(fn=lambda filename: filename.endswith('.csv'))\ndatapipe = dp.iter.FileOpener(datapipe, mode='rt')\ndatapipe = datapipe.parse_csv_files(delimiter=' ')\n\nfor d in datapipe: # Start loading data\n    pass\n```\n\n----------------------------------------\n\nTITLE: Constraining Integer Tensor Ranges in PyTorch using constrain_range API\nDESCRIPTION: The constrain_range API allows users to specify known upper and lower bounds for integer tensor data. This is useful when the system cannot automatically determine that values are non-negative.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamic_shapes.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconstrain_range\n```\n\n----------------------------------------\n\nTITLE: Using torch.compiler.disable with Decorator Syntax\nDESCRIPTION: Example showing how to disable TorchDynamo compilation on a specific function and its recursive calls using the decorator syntax. This is recommended when you can modify the source code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fine_grain_apis.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@torch.compiler.disable\ndef problematic_function():\n    # Function implementation\n    pass\n```\n\n----------------------------------------\n\nTITLE: Registering Single Operator Benchmark in PyTorch\nDESCRIPTION: This snippet shows how to register a single operator benchmark with the PyTorch benchmark suite using the generate_pt_test function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nop_bench.generate_pt_test(add_long_configs + add_short_configs, AddBenchmark)\n```\n\n----------------------------------------\n\nTITLE: Converting Between Nested Jagged Tensors and Padded Tensors in PyTorch\nDESCRIPTION: Shows how to convert a nested jagged tensor to a padded dense tensor using to_padded_tensor() with a specified padding value, and how to convert back using nested.narrow() with sequence lengths.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(6, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> padded = torch.nested.to_padded_tensor(nt, padding=4.2)\n>>> padded\ntensor([[[ 1.6107,  0.5723,  0.3913],\n         [ 0.0700, -0.4954,  1.8663],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000]],\n        [[-0.0479, -0.7610, -0.3484],\n         [ 1.1345,  1.0556,  0.3634],\n         [-1.7122, -0.5921,  0.0540],\n         [-0.5506,  0.7608,  2.0606],\n         [ 1.5658, -1.1934,  0.3041],\n         [ 0.1483, -1.1284,  0.6957]]])\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> padded = torch.randn(3, 5, 4)\n>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)\n>>> nt = torch.nested.narrow(padded, dim=1, length=seq_lens, layout=torch.jagged)\n>>> nt.shape\ntorch.Size([3, j1, 4])\n>>> nt = nt.contiguous()\n>>> nt.shape\ntorch.Size([3, j2, 4])\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Storage Bloat from Shared Tensor Storage - PyTorch - Python\nDESCRIPTION: Highlights an issue that can occur when saving a view of a larger tensor: torch.save preserves the original storage, so the resulting file contains all values from the shared storage, not just the elements in the view. Inputs are a large tensor and a view; output demonstrates how loading the view yields a storage size equal to the original large tensor. Useful for advanced users considering file size and efficiency when saving tensor subsets. No additional dependencies required except torch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small, 'small.pt')\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n999\n```\n\n----------------------------------------\n\nTITLE: Mixing Tracing and Scripting with torch.jit in PyTorch (Python)\nDESCRIPTION: Demonstrates how to trace a simple function and subsequently utilize it within a scripted TorchScript function. Requires PyTorch. The example uses a traced function 'traced_foo' and a scripted function 'bar', showing how scripted functions can call traced ones. Input and output are both torch.Tensors, illustrating function composition in TorchScript.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\n\\ndef foo(x, y):\\n    return 2 * x + y\\n\\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\\n\\n@torch.jit.script\\ndef bar(x):\\n    return traced_foo(x, x)\\n\n```\n\n----------------------------------------\n\nTITLE: Creating torch.device Objects Using String and Device Ordinal\nDESCRIPTION: This code snippet shows how to create torch.device objects by passing a string for the device type and an integer for the device ordinal. This approach works for CUDA, MPS, and CPU devices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('mps', 0)\ndevice(type='mps', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n```\n\n----------------------------------------\n\nTITLE: Adding Data-Dependent Shape Assertions with torch._check in PyTorch (Python)\nDESCRIPTION: These code snippets illustrate handling a data-dependent dynamic shape (result of torch.nonzero) when tracing a torch.nn.Module. The old version uses a standard Python if-statement to branch based on nz.shape[0] > 0, which is not export-safe. The new version introduces torch._check(nz.shape[0] > 0) as an explicit assertion, making the branch traceable under the condition checked. Both modules require PyTorch, and the input x is a Tensor. The output is x.sin() if the assertion passes, otherwise x.cos(). torch._check is required for the assertion. Inputs should be tensors suitable for nonzero().\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass M_old(torch.nn.Module):\n    def forward(self, x):\n        nz = x.nonzero()\n        if nz.shape[0] > 0:\n            return x.sin()\n        else:\n            return x.cos()\n```\n\nLANGUAGE: python\nCODE:\n```\nclass M_new(torch.nn.Module):\n    def forward(self, x):\n        nz = x.nonzero()\n        torch._check(nz.shape[0] > 0)\n        if nz.shape[0] > 0:\n            return x.sin()\n        else:\n            return x.cos()\n```\n\n----------------------------------------\n\nTITLE: Adding Sparse and Dense Tensors in PyTorch\nDESCRIPTION: Demonstrates addition between sparse and dense tensors, showing how the result defaults to dense format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\na + b.to_sparse()\n```\n\n----------------------------------------\n\nTITLE: Creating SWA Model in PyTorch\nDESCRIPTION: Example showing how to create a Stochastic Weight Averaging (SWA) model using AveragedModel class.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\naveraged_model = AveragedModel(model)\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations with Half Precision\nDESCRIPTION: Multiple convolution operations with half-precision (fp16) tensors of various dimensions, including 1x1, 3x3, 5x5, and 7x1 convolutions with different stride and padding configurations\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([192, 192, 7, 1], f16), [0], [1, 1], [3, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Name Propagation in Operations\nDESCRIPTION: Shows how tensor operations can handle named dimensions automatically through name inference, checking matches and propagating names, as demonstrated with tensor absolute value computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nx = torch.randn(3, 3, names=('N', 'C'))\nx.abs().names\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Patterns\nDESCRIPTION: This snippet demonstrates the usage patterns of various PyTorch operators in a deep learning model. It includes information about tensor shapes, data types (primarily float16), and the number of times each operator is called with specific configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilevit_s_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\n\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\n\nOperator: aten._softmax.default\ncnt: 2, ((T([256, 4, 256, 256], f16), -1, False), {})\ncnt: 4, ((T([256, 4, 64, 64], f16), -1, False), {})\ncnt: 3, ((T([256, 4, 16, 16], f16), -1, False), {})\n\nOperator: aten._softmax_backward_data.default\ncnt: 3, ((T([256, 4, 16, 16], f16), T([256, 4, 16, 16], f16), -1, f16), {})\ncnt: 4, ((T([256, 4, 64, 64], f16), T([256, 4, 64, 64], f16), -1, f16), {})\ncnt: 2, ((T([256, 4, 256, 256], f16), T([256, 4, 256, 256], f16), -1, f16), {})\n\nOperator: aten._unsafe_view.default\ncnt: 2, ((T([147456, 16, 2, 2], f16), [64, 144, 256, 4]), {})\ncnt: 2, ((T([64, 4, 256, 144], f16), [256, 256, 144]), {})\ncnt: 6, ((T([256, 4, 256, 36], f16), [1024, 256, 36]), {})\n# ... (more entries)\n\nOperator: aten.add.Tensor\ncnt: 32, ((T([], i64), 1), {})\ncnt: 4, ((T([64, 64, 64, 64], f16), T([64, 64, 64, 64], f16)), {})\ncnt: 8, ((T([256, 256, 144], f16), T([256, 256, 144], f16)), {})\n# ... (more entries)\n\nOperator: aten.addmm.default\ncnt: 2, ((T([432], f16), T([65536, 144], f16), T([144, 432], f16, stride=(1, 144))), {})\ncnt: 2, ((T([144], f16), T([65536, 144], f16), T([144, 144], f16, stride=(1, 144))), {})\n# ... (more entries)\n\nOperator: aten.bmm.default\ncnt: 2, ((T([1024, 256, 36], f16), T([1024, 36, 256], f16)), {})\ncnt: 2, ((T([1024, 256, 256], f16), T([1024, 256, 36], f16)), {})\n# ... (more entries)\n\nOperator: aten.cat.default\ncnt: 1, (([T([64, 96, 32, 32], f16), T([64, 96, 32, 32], f16)], 1), {})\ncnt: 1, (([T([64, 128, 16, 16], f16), T([64, 128, 16, 16], f16)], 1), {})\ncnt: 1, (([T([64, 160, 8, 8], f16), T([64, 160, 8, 8], f16)], 1), {})\n\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 256, 256], f16),), {})\ncnt: 1, ((T([64, 16, 128, 128], f16),), {})\n# ... (more entries)\n\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 256, 256], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 16, 128, 128], f16), T([64, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n# ... (more entries)\n```\n\n----------------------------------------\n\nTITLE: Installing and Configuring CCache\nDESCRIPTION: This bash snippet provides commands for installing and configuring ccache across various package managers to optimize build times by caching previous compilation results.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\nconda install ccache -c conda-forge\nsudo apt install ccache\nsudo yum install ccache\nbrew install ccache\n\n# config: cache dir is ~/.ccache, conf file ~/.ccache/ccache.conf\n# max size of cache\nccache -M 25Gi  # -M 0 for unlimited\n# unlimited number of files\nccache -F 0\n```\n\n----------------------------------------\n\nTITLE: Combined Forward and Context Linear Function Implementation\nDESCRIPTION: Demonstrates how to implement a custom Linear function with combined forward and context setup, including full backward pass implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass LinearFunction(Function):\n    @staticmethod\n    # ctx is the first argument to forward\n    def forward(ctx, input, weight, bias=None):\n        # The forward pass can use ctx.\n        ctx.save_for_backward(input, weight, bias)\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias\n```\n\n----------------------------------------\n\nTITLE: Setting ATen HIP Source Files and NVRTC Stubs in CMake\nDESCRIPTION: Conditionally executes if `USE_ROCM` is true. It sets the `ATen_HIP_SRCS` variable to the list of all HIP C++ source files (`all_hip_cpp`). It also sets the `ATen_NVRTC_STUB_SRCS` variable to use HIP-specific NVRTC stub source files (`hip_nvrtc_stub_cpp`), reusing stubs originally intended for CUDA NVRTC driver APIs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_23\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_ROCM)\n  set(ATen_HIP_SRCS ${all_hip_cpp})\n  # caffe2_nvrtc's stubs to driver APIs are useful for HIP.\n  # See NOTE [ ATen NVRTC Stub and HIP ]\n  set(ATen_NVRTC_STUB_SRCS ${hip_nvrtc_stub_cpp})\n  # NB: Instead of adding it to this list, we add it by hand\n  # to caffe2_hip, because it needs to be a PRIVATE dependency\n  # list(APPEND ATen_HIP_DEPENDENCY_LIBS ATEN_CUDA_FILES_GEN_LIB)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Guards Generated for Dynamic Shape Input in PyTorch Dynamo\nDESCRIPTION: These are the guards generated by Dynamo after retracing due to a shape change. The guards for size now use `None` for the first dimension, indicating it's dynamic. Additional guards ensure that the dynamic dimension (`L['a'].size()[0]`) is equal for both tensors ('duck shaping') and that it meets certain constraints (e.g., `2 <= L['a'].size()[0]`, avoiding specialization for 0 and 1).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Guards second call\ncheck_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])\ncheck_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])\n\nL['b'].size()[0] == L['a'].size()[0]\n2 <= L['a'].size()[0]\n```\n\n----------------------------------------\n\nTITLE: Using ATen LogSoftmax Backward Data Operator in PyTorch\nDESCRIPTION: This operator computes the gradient of the log softmax operation applied in a forward pass. Dependencies include PyTorch with ATen support. Key parameters include input tensor shapes, dimension, and data type. Output is a tensor with gradients calculated based on the forward softmax output. Ensure that both input tensors have matching dimensions for correct gradient computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/ElectraForCausalLM_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([511, 30522], f16), T([511, 30522], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch Sigmoid and Sigmoid Backward Operators - Python\nDESCRIPTION: This snippet records typical input shapes and argument combinations for testing the ATen sigmoid.default and sigmoid_backward.default operators in PyTorch. It exercises both elementwise activation and its backward pass with diverse tensor sizes and dtypes (float16), covering forward and gradient propagation scenarios. Inputs are tuples of tensors with matching shapes. No external dependencies needed besides PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 1, 64], f16),), {})\ncnt: 2, ((T([128, 1, 128], f16),), {})\ncnt: 1, ((T([128, 1, 256], f16),), {})\ncnt: 1, ((T([128, 1, 256], f16), T([128, 1, 256], f16)), {})\ncnt: 2, ((T([128, 1, 128], f16), T([128, 1, 128], f16)), {})\ncnt: 2, ((T([128, 1, 64], f16), T([128, 1, 64], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Copy Operations with Type Casting\nDESCRIPTION: Highlights the use of the aten._to_copy.default operator for tensor data type conversion and copying. Key parameters include the source tensor and the desired target data type; optional device and layout specifications provide flexibility in GPU-accelerated environments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForQuestionAnswering_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 25, ((T([4, 512, 768], f16),), {'dtype': f32})\ncnt: 25, ((T([4, 512, 768], f32),), {'dtype': f16})\ncnt: 1, ((T([4, 512, 1], f32),), {'dtype': f16})\ncnt: 1, ((T([4, 1, 512, 512], f32),), {'dtype': torch.uint8})\ncnt: 12, ((T([], f32),), {'dtype': f16, 'device': \"torch.device('cpu')\"})\ncnt: 12, ((T([4, 1, 512, 512], u8),), {'dtype': torch.bool})\ncnt: 25, ((T([4, 512, 768], f16),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 25, ((T([4, 512, 768], f32),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\n```\n\n----------------------------------------\n\nTITLE: Custom Tracer Implementation in FX\nDESCRIPTION: Shows how to customize tracing behavior by subclassing the Tracer class.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass MyCustomTracer(torch.fx.Tracer):\n    pass\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\ntraced_graph = MyCustomTracer().trace(mod)\n```\n\n----------------------------------------\n\nTITLE: Operator Decomposition for Trace Function in C++\nDESCRIPTION: Shows how to implement a batching rule by decomposing an operator into simpler operations. This example demonstrates decomposing the trace operation into diagonal and sum operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/writing_batching_rules.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nTensor trace_decomp(const Tensor& self) {\n  return at::sum(at::diagonal(self));\n}\n...\nm.impl(\"trace\", trace_decomp);\n```\n\n----------------------------------------\n\nTITLE: Complete Structured Pruning Implementation Example\nDESCRIPTION: A comprehensive example showing how to prune 50% of rows in linear layers of a neural network using SaliencyPruner. Includes model definition, pruning configuration, and pruning execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.ao.pruning._experimental.pruner import SaliencyPruner\n\n# Define model\nclass Model(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.seq = nn.Sequential(\n            nn.Linear(700, 500, bias=True),\n            nn.ReLU(),\n            nn.Linear(500, 800, bias=False),\n            nn.ReLU(),\n            nn.Linear(800, 600, bias=True),\n            nn.ReLU(),\n        )\n        self.linear = nn.Linear(600, 4, bias=False)\n\n    def forward(self, x):\n        x = self.seq(x)\n        x = self.linear(x)\n        return x\n\n# Define pruning_config, which specifies which tensors you wish to prune.\n# The SaliencyPruner also needs a sparsity_level parameter to specify what % of rows to prune.\npruning_config = [\n    {\"tensor_fqn\": \"seq.0.weight\", \"sparsity_level\": 0.5},\n    {\"tensor_fqn\": \"seq.2.weight\", \"sparsity_level\": 0.5},\n    {\"tensor_fqn\": \"seq.4.weight\", \"sparsity_level\": 0.5},\n    {\"tensor_fqn\": \"linear.weight\", \"sparsity_level\": 0.5},\n]\n\noriginal = Model()\n# define defaults\n# for structured pruning, we also prune biases by default.\ndefaults = {\"prune_bias\": True}\n# any configs passed in here are defaults that are propagated\n# Your selection criteria is decided by which pruner you use\npruner = SaliencyPruner(defaults, patterns=patterns)\n\n# Next we call `prepare`, which will attach `FakeStructuredSparsity` parameterizations\n# to the tensors specified in the config. These parameterizations will zero out\n# the appropriate weights in order to make the model behave as if it has been pruned.\npruner.prepare(original, sparse_config)\n\n# take one pruning step. This will update the masks\npruner.enable_mask_update = True\npruner.step()\n\n# pruner.prune() will find patterns and apply that patterns pruning function to it's matching nodes.\n# The output of pruner.prune() is a model with resized weights and the masks / parametrizations removed.\npruned_model = pruner.prune()\n```\n\n----------------------------------------\n\nTITLE: Basic Windows CMake Configuration\nDESCRIPTION: A minimal CMake configuration demonstrating how to set up two dynamic libraries with proper linking on Windows. Shows the basic structure needed for Windows DLL exports.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_37\n\nLANGUAGE: cmake\nCODE:\n```\nproject(myproject CXX)\nset(CMAKE_CXX_STANDARD 14)\nadd_library(foo SHARED foo.cpp)\nadd_library(bar SHARED bar.cpp)\ntarget_link_libraries(bar PUBLIC foo)\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.native_batch_norm_backward.default in PyTorch ATen\nDESCRIPTION: Logs calls to the `aten.native_batch_norm_backward.default` operator, computing gradients for batch normalization. Arguments include the gradient w.r.t the output (4D, f16), the original input tensor (4D, f16), weight (1D, f16), running mean (1D, f16), running variance (1D, f16), saved mean (1D, f32), saved inverse std deviation (1D, f32), training mode (`True`), epsilon (`1e-05`), and a list of booleans indicating which inputs require gradients (`[True, True, True]`). The `cnt` indicates call frequency.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 3, ((T([128, 256, 7, 7], f16), T([128, 256, 7, 7], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 128, 7, 7], f16), T([128, 128, 7, 7], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 11, ((T([128, 128, 14, 14], f16), T([128, 128, 14, 14], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 64, 14, 14], f16), T([128, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 64, 28, 28], f16), T([128, 64, 28, 28], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 32, 28, 28], f16), T([128, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 16, 56, 56], f16), T([128, 16, 56, 56], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 8, 112, 112], f16), T([128, 8, 112, 112], f16), T([8], f16), T([8], f16), T([8], f16), T([8], f32), T([8], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Autograd Example with Tensor Differentiation\nDESCRIPTION: Shows how to use PyTorch's autograd system for automatic differentiation. Creates differentiable tensors and computes gradients through backward propagation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/index.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/csrc/autograd/variable.h>\n#include <torch/csrc/autograd/function.h>\n\ntorch::Tensor a = torch::ones({2, 2}, torch::requires_grad());\ntorch::Tensor b = torch::randn({2, 2});\nauto c = a + b;\nc.backward(); // a.grad() will now hold the gradient of c w.r.t. a.\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Sigmoid Operations\nDESCRIPTION: This snippet demonstrates the usage of the sigmoid activation function in PyTorch. It shows the tensor shapes and data types for both forward and backward passes across different layers of the network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sigmoid.default\ncnt: 1, ((T([32, 32, 1, 1], f16),), {})\ncnt: 1, ((T([32, 96, 1, 1], f16),), {})\ncnt: 2, ((T([32, 144, 1, 1], f16),), {})\n# ... (truncated for brevity)\n\nOperator: aten.sigmoid_backward.default\ncnt: 4, ((T([32, 1152, 1, 1], f16), T([32, 1152, 1, 1], f16)), {})\ncnt: 3, ((T([32, 672, 1, 1], f16), T([32, 672, 1, 1], f16)), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Modeling Control Flow and Attribute Access in an FX Graph (Python)\nDESCRIPTION: This extended Python snippet represents a full FX graph for a control flow model using get_attr and call_function nodes. It illustrates creating placeholders for inputs, retrieving subgraph attributes, invoking a higher-order conditional using those graph pieces, and returning the result. The graph provides a realistic sketch of FX node interconnections when modeling functional control flow as in functorch's cond operator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ngraph():\n    %x_1 : [num_users=1] = placeholder[target=x_1]\n    %y_1 : [num_users=1] = placeholder[target=y_1]\n    %true_graph_0 : [num_users=1] = get_attr[target=true_graph_0]\n    %false_graph_0 : [num_users=1] = get_attr[target=false_graph_0]\n    %conditional : [num_users=1] = call_function[target=torch.ops.higher_order.cond](args = (%y_1, %true_graph_0, %false_graph_0, [%x_1]), kwargs = {})\n    return conditional\n```\n\n----------------------------------------\n\nTITLE: PyTorch ReLU In-place Activation Operations\nDESCRIPTION: This snippet shows in-place ReLU activation operations applied to tensors of various shapes in f16 precision. The operations modify tensors directly without creating new copies, indicated by the trailing underscore in 'relu_'.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 3, ((T([128, 16, 112, 112], f16),), {})\ncnt: 1, ((T([128, 96, 112, 112], f16),), {})\ncnt: 1, ((T([128, 96, 56, 56], f16),), {})\ncnt: 4, ((T([128, 24, 56, 56], f16),), {})\ncnt: 1, ((T([128, 144, 56, 56], f16),), {})\ncnt: 1, ((T([128, 144, 28, 28], f16),), {})\ncnt: 2, ((T([128, 96, 28, 28], f16),), {})\ncnt: 5, ((T([128, 192, 28, 28], f16),), {})\ncnt: 3, ((T([128, 192, 14, 14], f16),), {})\ncnt: 6, ((T([128, 384, 14, 14], f16),), {})\ncnt: 5, ((T([128, 672, 14, 14], f16),), {})\ncnt: 2, ((T([128, 336, 14, 14], f16),), {})\ncnt: 1, ((T([128, 672, 7, 7], f16),), {})\ncnt: 8, ((T([128, 1104, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1984, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Disabling TF32 for CuBLAS and CuDNN in PyTorch (C++)\nDESCRIPTION: Demonstrates the C++ method for disabling TensorFloat-32 (TF32) usage within PyTorch's backend operations. It uses the `at::globalContext` to set the flags `allowTF32CuBLAS` and `allowTF32CuDNN` to `false`, ensuring full FP32 precision for underlying CuBLAS and CuDNN calls.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nat::globalContext().setAllowTF32CuBLAS(false);\nat::globalContext().setAllowTF32CuDNN(false);\n```\n\n----------------------------------------\n\nTITLE: Creating CFFI Extension in Python\nDESCRIPTION: The Python snippet outlines how to create a CFFI extension for PyTorch with experimental support. It emphasizes the need to specify additional libraries in the Extension object for successful builds on Windows. The code requires specifying inputs like headers, sources, and whether CUDA support is needed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nffi = create_extension(\n    '_ext.my_lib',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_compile_args=[\"-std=c99\"],\n    libraries=['ATen', '_C'] # Append cuda libraries when necessary, like cudart\n)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Neural Network Layer Operations\nDESCRIPTION: Neural network specific operations including embedding, layer normalization, and GELU activation functions. These operations are typical in transformer-based architectures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/AllenaiLongformerBase_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Neural network operations\naten.embedding.default(T([50265, 768], f16), T([1, 1024], i64), 1)\naten.native_layer_norm.default(T([1, 1024, 768], f16), [768], T([768], f16), T([768], f16), 1e-05)\naten.gelu.default(T([1, 1024, 3072], f16))\n```\n\n----------------------------------------\n\nTITLE: Replacing Nodes in PyTorch FX Graph in Python\nDESCRIPTION: This example provides a method to replace node operations in a PyTorch FX graph using the 'inserting_after' context. It depends on 'torch' and 'torch.fx'. The snippet replaces node uses with a new operation, like adding a ReLU function, using the 'replace_all_uses_with' API. This is useful for modifying computation flow in traced graphs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)\n```\n\n----------------------------------------\n\nTITLE: Enabling Full FP16 Accumulation for FP16 GEMMs in PyTorch (C++)\nDESCRIPTION: Provides the C++ method to enable full FP16 accumulation within FP16 GEMMs handled by CuBLAS. Using `at::globalContext().setAllowFP16AccumulationCuBLAS(true)` activates this behavior, potentially boosting performance on compatible hardware at the expense of precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nat::globalContext().setAllowFP16AccumulationCuBLAS(true);\n```\n\n----------------------------------------\n\nTITLE: Other Operations List in RestructuredText\nDESCRIPTION: A comprehensive list of miscellaneous tensor operations in PyTorch, including shape manipulation, element-wise operations, and mathematical functions, formatted as a RestructuredText autosummary.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    atleast_1d\n    atleast_2d\n    atleast_3d\n    bincount\n    block_diag\n    broadcast_tensors\n    broadcast_to\n    broadcast_shapes\n    bucketize\n    cartesian_prod\n    cdist\n    clone\n    combinations\n    corrcoef\n    cov\n    cross\n    cummax\n    cummin\n    cumprod\n    cumsum\n    diag\n    diag_embed\n    diagflat\n    diagonal\n    diff\n    einsum\n    flatten\n    flip\n    fliplr\n    flipud\n    kron\n    rot90\n    gcd\n    histc\n    histogram\n    histogramdd\n    meshgrid\n    lcm\n    logcumsumexp\n    ravel\n    renorm\n    repeat_interleave\n    roll\n    searchsorted\n    tensordot\n    trace\n    tril\n    tril_indices\n    triu\n    triu_indices\n    unflatten\n    vander\n    view_as_real\n    view_as_complex\n    resolve_conj\n    resolve_neg\n```\n\n----------------------------------------\n\nTITLE: Manipulating Tensor Storage in PyTorch\nDESCRIPTION: Example demonstrating how to access and modify a tensor's underlying storage, including cloning storage, filling with zeros, and reassigning storage to a tensor. Shows the low-level relationship between tensors and their storage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/storage.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nt = torch.ones(3)\ns0 = t.untyped_storage()\ns0\n 0\n 0\n 128\n 63\n 0\n 0\n 128\n 63\n 0\n 0\n 128\n 63\n[torch.storage.UntypedStorage(device=cpu) of size 12]\ns1 = s0.clone()\ns1.fill_(0)\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n[torch.storage.UntypedStorage(device=cpu) of size 12]\n# Fill the tensor with a zeroed storage\nt.set_(s1, storage_offset=t.storage_offset(), stride=t.stride(), size=t.size())\ntensor([0., 0., 0.])\n```\n\n----------------------------------------\n\nTITLE: Defining MNIST MLP Model Architecture in PyTorch\nDESCRIPTION: Defines a multi-level perceptron model with two convolutions, two linear layers, and activations for MNIST digit classification.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass Net(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n```\n\n----------------------------------------\n\nTITLE: Implementing MNIST Training Loop with Lazy Tensor mark_step()\nDESCRIPTION: Defines the training loop function using Lazy Tensor, including the mark_step() call to break up the current trace and start asynchronous execution after each iteration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef train(log_interval, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        torch._lazy.mark_step()\n\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n```\n\n----------------------------------------\n\nTITLE: Computing Batch Jacobian Using Output Summing in PyTorch\nDESCRIPTION: Shows an alternative approach to compute batch Jacobians by summing outputs and using jacrev. This method works when each input produces an independent output, avoiding the need for vmap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef predict_with_output_summed(weight, bias, x):\n    return predict(weight, bias, x).sum(0)\n\nbatch_jacobian1 = jacrev(predict_with_output_summed, argnums=2)(weight, bias, x).movedim(1, 0)\nassert torch.allclose(batch_jacobian0, batch_jacobian1)\n```\n\n----------------------------------------\n\nTITLE: Neural Network Module Transformation Example\nDESCRIPTION: Shows how to compute per-sample gradients using a functional version of nn.Linear.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom functorch import make_functional, vmap, grad\n\nmodel = torch.nn.Linear(3, 3)\ndata = torch.randn(64, 3)\ntargets = torch.randn(64, 3)\n\nfunc_model, params = make_functional(model)\n\ndef compute_loss(params, data, targets):\n    preds = func_model(params, data)\n    return torch.mean((preds - targets) ** 2)\n\nper_sample_grads = vmap(grad(compute_loss), (None, 0, 0))(params, data, targets)\n```\n\n----------------------------------------\n\nTITLE: Defining a Placeholder Node in FX Graph (Python)\nDESCRIPTION: This Python code snippet demonstrates the creation of a placeholder node within an FX graph, following the syntax conventions used in PyTorch's FX framework. The placeholder target designates the input variable's name, and the node can optionally hold a default argument. This node structure is essential for marking graph inputs and should appear at the top of the nodes list in a graph block.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%name = placeholder[target = name](args = ())\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Documentation Prerequisites Using Bash\nDESCRIPTION: The Bash snippet details installing required prerequisites for building the PyTorch documentation. It suggests installing Python packages using pip and additional tools like katex via npm or yarn. The prerequisites must be installed inside the docs directory. The snippet is dependent on properly configured Python and Node.js environments. It outputs the necessary setup for building documentation sources.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_20\n\nLANGUAGE: Bash\nCODE:\n```\ncd docs\npip install -r requirements.txt\n# `katex` must also be available in your PATH.\n# You can either install katex globally if you have properly configured npm:\n# npm install -g katex\n# Or if you prefer an uncontaminated global executable environment or do not want to go through the node configuration:\n# npm install katex && export PATH=\"$PATH:$(pwd)/node_modules/.bin\"\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Python Bindings in CMake\nDESCRIPTION: Configures the build system for PyTorch Python bindings. It checks for prerequisites, sets up build paths, and determines whether to proceed with the build based on the BUILD_PYTHON option.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT CAFFE2_CMAKE_BUILDING_WITH_MAIN_REPO)\n  cmake_minimum_required(VERSION 3.18 FATAL_ERROR)\n  project(torch CXX C)\n  find_package(torch REQUIRED)\n  option(USE_CUDA \"Use CUDA\" ON)\n  set(CMAKE_EXPORT_COMPILE_COMMANDS ON)\nendif()\n\nif(NOT BUILD_PYTHON)\n  return()\nendif()\n\nset(TORCH_SRC_DIR \"${CMAKE_CURRENT_SOURCE_DIR}\")\nset(TORCH_ROOT \"${TORCH_SRC_DIR}/..\")\n\nif(NOT TORCH_INSTALL_LIB_DIR)\n  set(TORCH_INSTALL_LIB_DIR lib)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using CUDA Packed Tensor Accessors in C++\nDESCRIPTION: Example of creating and using CUDA packed tensor accessors for efficient element-wise access in GPU kernels. Packed accessors copy tensor metadata rather than pointing to it, making them suitable for use in CUDA device code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n__global__ void packed_accessor_kernel(\n    torch::PackedTensorAccessor64<float, 2> foo,\n    float* trace) {\n  int i = threadIdx.x;\n  gpuAtomicAdd(trace, foo[i][i]);\n}\n\ntorch::Tensor foo = torch::rand({12, 12});\n\n// assert foo is 2-dimensional and holds floats.\nauto foo_a = foo.packed_accessor64<float,2>();\nfloat trace = 0;\n\npacked_accessor_kernel<<<1, 12>>>(foo_a, &trace);\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Pre-Built Docker Image with GPU - bash\nDESCRIPTION: This command runs a pre-built PyTorch Docker image with GPU support. It sets the container to use all available GPUs, removes the container after exit, and ensures shared memory is configured for high-performance multiprocessing. Docker version 19.03+ and NVIDIA drivers supporting GPUs are prerequisites. Adjust shared memory size as needed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Tests with pytest and Filtering\nDESCRIPTION: Uses pytest to run PyTorch tests, filtering for tests containing 'Loss' in their name.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npytest test/test_nn.py -k Loss -v\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with Bias for Linear Layers\nDESCRIPTION: Documents the addmm operator calls for linear layer transformations in the neural network. These operations perform matrix multiplication with an added bias term, used primarily in feed-forward layers and projection operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 36, ((T([2304], f16), T([1152, 768], f16), T([768, 2304], f16, stride=(1, 768))), {})\ncnt: 36, ((T([768], f16), T([1152, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 36, ((T([3072], f16), T([1152, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 36, ((T([768], f16), T([1152, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 2, ((T([768], f16), T([2, 768], f16, stride=(443136, 1)), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 4, ((T([768], f16), T([1154, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 2, ((T([768], f16), T([2, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 2, ((T([3072], f16), T([2, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 2, ((T([768], f16), T([2, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 1, ((T([1000], f16), T([2, 768], f16, stride=(443136, 1)), T([768, 1000], f16, stride=(1, 768))), {})\n```\n\n----------------------------------------\n\nTITLE: Running Hugging Face Benchmark with PyTorch Inductor\nDESCRIPTION: This command runs the Hugging Face benchmark suite using PyTorch Inductor backend with specific settings for performance testing. It includes options for cold-start latency, inference, AMP, and CUDA device.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_performance_dashboard.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython benchmarks/dynamo/huggingface.py --performance --cold-start-latency --inference --amp --backend inductor --disable-cudagraphs --device cuda\n```\n\n----------------------------------------\n\nTITLE: Declaring ATen Functions with Argument and Return Type Details\nDESCRIPTION: The snippet illustrates the declaration format of ATen functions, highlighting the permissible argument and return types. It outlines the syntax for specifying function signatures, including overloaded functions and the use of namespaces. Dependencies include familiarity with C++ and PyTorch's code generation conventions. Key parameters include function names, argument types (like Tensor, int, float), and return types (like Tensor, Tensor[]). The functions are primarily designed for C++ integration in PyTorch, enabling complex type mappings and function overloading.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n\\n- func: func_name[.overload_name](ArgType arg0[=default], ArgType arg1[=default], ...) -> Return\\n\n```\n\n----------------------------------------\n\nTITLE: In-place Tensor Operations in TorchScript\nDESCRIPTION: Shows how to perform in-place tensor operations using trailing underscore convention and out parameter. These operations modify the tensor data directly without creating new memory allocations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nt2.relu_() # inplace relu operator, note t is modified as well!\ntorch.add(t, t, out=t) # update t, without using temporary memory if possible\n```\n\n----------------------------------------\n\nTITLE: Exporting with Optional Input Omitted using torch.export (Python)\nDESCRIPTION: This snippet shows the behavior of `torch.export.export` when an optional argument is *not* provided during tracing. The module `M` has an optional argument `y`. When exported with only `x` provided, `torch.export.export` traces the default path (where `y` is `None`, leading to `return x + x`). The resulting exported graph is specialized for this case, and its function signature only requires `x`, effectively losing the optional defaulting behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass M(torch.nn.Module):\n    def forward(self, x, y=None):\n        if y is not None:\n            return y * x\n        return x + x\n\n# Optional input is not passed in\nep = torch.export.export(M(), (torch.randn(3, 3),))\nprint(ep)\n\"\"\"\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 3]\", y):\n            # File: /data/users/angelayi/pytorch/moo.py:16 in forward, code: return x + x\n            add: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating Compressed Sparse Tensors in PyTorch\nDESCRIPTION: Demonstrates creating CSR and CSC tensors using the same input data by specifying different layouts with sparse_compressed_tensor function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\ncompressed_indices = torch.tensor([0, 2, 4])\nplain_indices = torch.tensor([0, 1, 0, 1])\nvalues = torch.tensor([1, 2, 3, 4])\ncsr = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csr)\ncsc = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csc)\n```\n\n----------------------------------------\n\nTITLE: Creating an Integer Tensor with Range in PyTorch C++\nDESCRIPTION: This example illustrates how to create a 5x5 square matrix with integers between 0 and 10 using the randint() factory function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor tensor = torch::randint(/*high=*/10, {5, 5});\n```\n\n----------------------------------------\n\nTITLE: PyTorch Utils Module Interface\nDESCRIPTION: Lists the main functions available in torch.utils module including tensor operations, backend management, and utility functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/utils.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nrename_privateuse1_backend\ngenerate_methods_for_privateuse1_backend\nget_cpp_backtrace\nset_module\nswap_tensors\n```\n\n----------------------------------------\n\nTITLE: Sparse COO Tensor Operations in PyTorch\nDESCRIPTION: Series of sparse COO tensor operations with shape [965, 192] using half-precision (f16) format on CUDA device. Each operation contains counter (cnt) indicating number of occurrences.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((1, 1, [965, 192], T([1, 54827], i64), T([54827, 192], f16)), {'dtype': f16, 'layout': torch.sparse_coo, 'device': 'cuda', 'pin_memory': None})\n```\n\n----------------------------------------\n\nTITLE: AOT Function with Backpropagation Example\nDESCRIPTION: Shows how to use AOT function with backpropagation, demonstrating that regular PyTorch operations can be mixed with compiled functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninp = torch.randn(3, requires_grad=True)\ninp = inp.cos()\nout = nf(inp)\nout = out.sin().sum().backward()\n```\n\n----------------------------------------\n\nTITLE: Shape Manipulations with Nested Jagged Tensors in PyTorch\nDESCRIPTION: Demonstrates various shape manipulation operations on nested jagged tensors, including unsqueeze, unflatten, cat, stack, and transpose.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> a = torch.randn(2, 6)\n>>> b = torch.randn(4, 6)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt.shape\ntorch.Size([2, j1, 6])\n>>> nt.unsqueeze(-1).shape\ntorch.Size([2, j1, 6, 1])\n>>> nt.unflatten(-1, [2, 3]).shape\ntorch.Size([2, j1, 2, 3])\n>>> torch.cat([nt, nt], dim=2).shape\ntorch.Size([2, j1, 12])\n>>> torch.stack([nt, nt], dim=2).shape\ntorch.Size([2, j1, 2, 6])\n>>> nt.transpose(-1, -2).shape\ntorch.Size([2, 6, j1])\n```\n\n----------------------------------------\n\nTITLE: Saving Only Tensor Data by Cloning to New Storage - PyTorch - Python\nDESCRIPTION: Describes a technique to reduce saved file size by cloning a tensor before saving, breaking its view relationship and storing only its raw data. Suitable when the view relationship is not required after loading. Inputs are a tensor view; output is a smaller file and a loaded tensor whose storage contains only its own elements. Uses torch's clone() method; same requirements as other snippets.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small.clone(), 'small.pt')  # saves a clone of small\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n5\n```\n\n----------------------------------------\n\nTITLE: Disabling Running Stats in BatchNorm2d - PyTorch (Python)\nDESCRIPTION: Configures a BatchNorm2d layer with track_running_stats set to False, preventing updates to running_mean and running_var. This is useful when using vmapped modules in Functorch, as it avoids in-place tensor modification errors. \"64\" is the channel count for this example, and no additional dependencies are needed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBatchNorm2d(64, track_running_stats=False)\n```\n\n----------------------------------------\n\nTITLE: Creating a CSC Tensor in PyTorch from Explicit Components\nDESCRIPTION: Shows how to create a sparse CSC (Compressed Sparse Column) tensor by providing ccol_indices, row_indices, and values tensors. The example creates a 2x2 sparse matrix with all elements defined.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nccol_indices = torch.tensor([0, 2, 4])\nrow_indices = torch.tensor([0, 1, 0, 1])\nvalues = torch.tensor([1, 2, 3, 4])\ncsc = torch.sparse_csc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)\ncsc\n```\n\n----------------------------------------\n\nTITLE: Using Scalars with Tensors in ATen\nDESCRIPTION: Examples of ATen Scalar usage with tensor operations. Scalars are dynamically typed single numbers that can be implicitly constructed from C++ number types and are used for functions that take both tensors and scalar values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace torch {\nTensor addmm(Scalar beta, const Tensor & self,\n             Scalar alpha, const Tensor & mat1,\n             const Tensor & mat2);\nScalar sum(const Tensor & self);\n} // namespace torch\n\n// Usage.\ntorch::Tensor a = ...\ntorch::Tensor b = ...\ntorch::Tensor c = ...\ntorch::Tensor r = torch::addmm(1.0, a, .5, b, c);\n```\n\n----------------------------------------\n\nTITLE: Custom C++ Operator ONNX Export Example\nDESCRIPTION: Example of exporting a model with custom C++ operators to ONNX format with custom symbolic function registration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n@symbolic_helper.parse_args(\"v\", \"v\", \"f\", \"i\")\ndef symbolic_foo_forward(g, input1, input2, attr1, attr2):\n    return g.op(\"custom_domain::Foo\", input1, input2, attr1_f=attr1, attr2_i=attr2)\n\nclass FooModel(torch.nn.Module):\n    def __init__(self, attr1, attr2):\n        super().__init__()\n        self.attr1 = attr1\n        self.attr2 = attr2\n\n    def forward(self, input1, input2):\n        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)\n```\n\n----------------------------------------\n\nTITLE: Migrating Code from CUDA to Intel XPU\nDESCRIPTION: Shows how to migrate CUDA code to use Intel XPU by changing device references from 'cuda' to 'xpu'. It maintains the logic intact while switching the device target.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# CUDA CODE\ntensor = torch.tensor([1.0, 2.0]).to(\"cuda\")\n\n# CODE for Intel GPU\ntensor = torch.tensor([1.0, 2.0]).to(\"xpu\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting Compiled Subgraph Functions in Dynamo\nDESCRIPTION: This code inspects the source code of compiled subgraph functions generated by Dynamo. It prints the source code for the main compiled function and the resume functions that handle graph breaks, providing insight into how Dynamo transforms the original function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(\"source code of __compiled_fn_0:\")\nprint(innermost_fn(__compiled_fn_0).__self__.code)\nprint(\"=\" * 60)\nprint(\"source code of __resume_at_30_1:\")\nprint(decompile(__resume_at_30_1))\nprint(\"=\" * 60)\nprint(\"source code of __resume_at_38_2:\")\nprint(decompile(__resume_at_38_2))\n```\n\n----------------------------------------\n\nTITLE: Applying In-Place ReLU Activation with PyTorch\nDESCRIPTION: Applies the ReLU activation function in-place on the input tensor, converting all negative values to zero, crucial in deep learning. Requires input tensors of shapes like [16, 64, 128, 128]. Outputs the same modified tensor, highlighting efficiency in memory usage by avoiding tensor duplication.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 2, ((T([16, 64, 128, 128], f16),), {})\ncnt: 2, ((T([16, 128, 64, 64], f16),), {})\ncnt: 7, ((T([16, 256, 32, 32], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Performing Convolution Operations in PyTorch\nDESCRIPTION: This snippet shows various convolution operations with different input and output tensor shapes. It includes operations for the initial layer, downsampling, and feature extraction layers of a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# Initial convolution layer\naten.convolution.default(T([64, 3, 224, 224], f16), T([64, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1)\n\n# Downsampling convolution\naten.convolution.default(T([64, 64, 56, 56], f16), T([128, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1)\n\n# Feature extraction convolutions\naten.convolution.default(T([64, 128, 56, 56], f16), T([32, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1)\n\n# Various 1x1 convolutions for channel adjustment\naten.convolution.default(T([64, 96, 56, 56], f16), T([128, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1)\naten.convolution.default(T([64, 128, 56, 56], f16), T([128, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1)\naten.convolution.default(T([64, 160, 56, 56], f16), T([128, 160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1)\naten.convolution.default(T([64, 192, 56, 56], f16), T([128, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1)\naten.convolution.default(T([64, 224, 56, 56], f16), T([128, 224, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1)\naten.convolution.default(T([64, 256, 56, 56], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1)\n\n# Final convolution layers\naten.convolution.default(T([64, 128, 28, 28], f16), T([128, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Count for aten._log_softmax Operations\nDESCRIPTION: Shows the call pattern for the _log_softmax.default operator with tensor shape [128, 1000] in half precision (f16), operating along dimension 1 with False as the third parameter.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Basic FakeTensorMode Usage in PyTorch\nDESCRIPTION: Demonstrates how to create and use fake tensors outside of PT2 context. Shows creation of FakeTensorMode, converting real tensors to fake tensors, and performing operations within the fake mode context.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Create a fake mode\nfrom torch._subclasses.fake_tensor import FakeTensorMode\nfake_mode = FakeTensorMode()\nconverter = fake_mode.fake_tensor_converter\n# Fakeify some real tensors\nfake_x = converter.from_real_tensor(fake_mode, x)\nwith fake_mode:\n    # Do some operations on the fake tensors\n    fake_y = fake_x * 2\n    # Factory operations automatically get fakeified in the context manager\n    fake_z = torch.empty(20)\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Unsqueeze Batching Rule in C++\nDESCRIPTION: Demonstrates implementation of a basic batching rule for the unsqueeze operator. The rule handles moving batch dimensions and adjusting the unsqueeze dimension parameter to account for the batch dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/writing_batching_rules.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstd::tuple<Tensor,optional<int64_t>> unsqueeze_batch_rule(\n    const Tensor& self,\n    optional<int64_t> self_bdim,\n    int64_t dim) {\n  auto self_ = moveBatchDimToFront(self, self_bdim);\n  auto rank = rankWithoutBatchDim(self, self_bdim);\n  dim = maybe_wrap_dim(dim, rank + 1) + 1;\n  return std::make_tuple(self_.unsqueeze(dim), 0);\n}\n```\n\n----------------------------------------\n\nTITLE: Gradient Backpropagation for Tensor Selection Operations\nDESCRIPTION: Records the backward pass operations for tensor selection and slicing used in the model. These operations compute gradients with respect to indexed or sliced tensors during backpropagation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.select_backward.default\ncnt: 3, ((T([2, 768], f16), [2, 577, 768], 1, 0), {})\ncnt: 36, ((T([2, 16, 576, 48], f16), [3, 2, 16, 576, 48], 0, 2), {})\ncnt: 36, ((T([2, 16, 576, 48], f16, stride=(442368, 27648, 1, 576)), [3, 2, 16, 576, 48], 0, 1), {})\ncnt: 36, ((T([2, 16, 576, 48], f16), [3, 2, 16, 576, 48], 0, 0), {})\nOperator: aten.slice_backward.default\ncnt: 3, ((T([2, 577, 768], f16), [2, 577, 768], 0, 0, 9223372036854775807, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Minified TorchInductor Error Reproduction\nDESCRIPTION: Auto-generated minimal reproduction code that isolates the exact operation causing the error in TorchInductor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nfrom torch.fx.experimental.proxy_tensor import make_fx\n\nclass Repro(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, add):\n        _foobar = torch.ops.aten._foobar.default(add);  add = None\n        return (_foobar,)\n\nargs = [((200, 200), (200, 1), torch.float32, 'cpu')]\nargs = [rand_strided(shape, stride, dtype, device) for shape, stride, dtype, device in args]\nmod = make_fx(Repro())(*args)\nfrom torch._inductor.compile_fx import compile_fx_inner\n\ncompiled = compile_fx_inner(mod, args)\ncompiled(*args)\n```\n\n----------------------------------------\n\nTITLE: Accelerating nn.Linear with Semi-Structured Sparsity in PyTorch\nDESCRIPTION: Shows how to accelerate linear layers in a model by converting weights to semi-structured sparse format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ninput = torch.rand(64, 64).half().cuda()\nmask = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool()\nlinear = nn.Linear(64, 64).half().cuda()\nlinear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0)))\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch Slice Backward Operator - Python\nDESCRIPTION: This snippet provides batches of input configurations for the ATen slice_backward.default operator, simulating common slicing and gradient propagation operations. Each test case specifies the base tensor, output shape, slice dimension, start/stop/step, and supporting parameters, reflecting realistic backpropagation slicing scenarios in neural networks. Dependencies: PyTorch tensor API.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([8192, 8, 12], f16), [8192, 8, 23], 2, 11, 9223372036854775807, 1), {})\ncnt: 2, ((T([8192, 8, 23], f16), [8192, 9, 23], 1, 0, 8, 1), {})\ncnt: 2, ((T([8192, 9, 23], f16), [8192, 9, 23], 0, 0, 9223372036854775807, 1), {})\ncnt: 2, ((T([16384, 4, 12], f16), [16384, 4, 23], 2, 11, 9223372036854775807, 1), {})\ncnt: 2, ((T([16384, 4, 23], f16), [16384, 5, 23], 1, 0, 4, 1), {})\ncnt: 2, ((T([16384, 5, 23], f16), [16384, 5, 23], 0, 0, 9223372036854775807, 1), {})\ncnt: 2, ((T([32768, 8, 12], f16), [32768, 8, 23], 2, 11, 9223372036854775807, 1), {})\ncnt: 2, ((T([32768, 8, 23], f16), [32768, 9, 23], 1, 0, 8, 1), {})\ncnt: 2, ((T([32768, 9, 23], f16), [32768, 9, 23], 0, 0, 9223372036854775807, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Utilize aten.split_with_sizes in Python\nDESCRIPTION: Splits a tensor into parts of specified sizes along a defined dimension using PyTorch. Prerequisites are compatible tensor operations provided by PyTorch. This function needs a tensor, a sizes list indicating partition extents, and a dimension index. The result is a tuple of segmented tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 32, 112, 112], f16), [16, 16], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Reporting ATen Operator Usage Patterns - PyTorch - Python\nDESCRIPTION: Lists a structured tally of PyTorch ATen operator invocations, including tensor shapes, data types, and parameters per operator instance. This covers elementwise (add, div), memory ops (clone, copy), convolutions, normalizations, and activations, showing usage diversity and architectural depth. Inputs specify tensor type abbreviations (T), shapes, dtypes, and parameter lists, supporting downstream analysis for optimization, kernel testing, or code generation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_CycleGAN_and_pix2pix_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 18, ((T([1, 256, 64, 64], f16), T([1, 256, 64, 64], f16)), {})\nOperator: aten.clone.default\ncnt: 1, ((T([1, 3, 256, 256], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([1, 3, 262, 262], f16), T([64, 3, 7, 7], f16), T([64], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([1, 64, 256, 256], f16), T([128, 64, 3, 3], f16), T([128], f16), [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([1, 128, 128, 128], f16), T([256, 128, 3, 3], f16), T([256], f16), [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 18, ((T([1, 256, 66, 66], f16), T([256, 256, 3, 3], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([1, 256, 64, 64], f16), T([256, 128, 3, 3], f16), T([128], f16), [2, 2], [1, 1], [1, 1], True, [1, 1], 1), {})\ncnt: 1, ((T([1, 128, 128, 128], f16), T([128, 64, 3, 3], f16), T([64], f16), [2, 2], [1, 1], [1, 1], True, [1, 1], 1), {})\ncnt: 1, ((T([1, 64, 262, 262], f16), T([3, 64, 7, 7], f16), T([3], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([1, 3, 256, 256], f16), T([1, 64, 262, 262], f16), T([3, 64, 7, 7], f16), [3], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([1, 64, 256, 256], f16), T([1, 128, 128, 128], f16), T([128, 64, 3, 3], f16), [64], [2, 2], [1, 1], [1, 1], True, [1, 1], 1, [True, True, True]), {})\ncnt: 1, ((T([1, 128, 128, 128], f16), T([1, 256, 64, 64], f16), T([256, 128, 3, 3], f16), [128], [2, 2], [1, 1], [1, 1], True, [1, 1], 1, [True, True, True]), {})\ncnt: 18, ((T([1, 256, 64, 64], f16), T([1, 256, 66, 66], f16), T([256, 256, 3, 3], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([1, 256, 64, 64], f16), T([1, 128, 128, 128], f16), T([256, 128, 3, 3], f16), [256], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([1, 128, 128, 128], f16), T([1, 64, 256, 256], f16), T([128, 64, 3, 3], f16), [128], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([1, 64, 256, 256], f16), T([1, 3, 262, 262], f16), T([64, 3, 7, 7], f16), [64], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([1, 3, 256, 256], f16), T([1, 3, 256, 256], f16)), {})\ncnt: 2, ((T([64, 256, 256], f16), T([64, 256, 256], f16)), {})\ncnt: 4, ((T([1, 64, 256, 256], f16), T([1, 64, 256, 256], f16)), {})\ncnt: 2, ((T([128, 128, 128], f16), T([128, 128, 128], f16)), {})\ncnt: 4, ((T([1, 128, 128, 128], f16), T([1, 128, 128, 128], f16)), {})\ncnt: 10, ((T([256, 64, 64], f16), T([256, 64, 64], f16)), {})\ncnt: 20, ((T([1, 256, 64, 64], f16), T([1, 256, 64, 64], f16)), {})\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 196608), {})\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([1, 64, 256, 256], f16), None, None, None, None, True, 0.1, 1e-05), {})\ncnt: 2, ((T([1, 128, 128, 128], f16), None, None, None, None, True, 0.1, 1e-05), {})\ncnt: 19, ((T([1, 256, 64, 64], f16), None, None, None, None, True, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 2, ((T([1, 64, 256, 256], f16), T([1, 64, 256, 256], f16), None, None, None, T([64], f32), T([64], f32), True, 1e-05, [True, False, False]), {})\ncnt: 2, ((T([1, 128, 128, 128], f16), T([1, 128, 128, 128], f16), None, None, None, T([128], f32), T([128], f32), True, 1e-05, [True, False, False]), {})\ncnt: 19, ((T([1, 256, 64, 64], f16), T([1, 256, 64, 64], f16), None, None, None, T([256], f32), T([256], f32), True, 1e-05, [True, False, False]), {})\nOperator: aten.new_empty_strided.default\ncnt: 2, ((T([1, 64, 256, 256], f16), [1, 64, 256, 256], [4194304, 65536, 256, 1]), {})\ncnt: 2, ((T([1, 128, 128, 128], f16), [1, 128, 128, 128], [2097152, 16384, 128, 1]), {})\ncnt: 10, ((T([1, 256, 64, 64], f16), [1, 256, 64, 64], [1048576, 4096, 64, 1]), {})\nOperator: aten.new_zeros.default\ncnt: 2, ((T([64, 256, 256], f16), [4194304]), {})\ncnt: 2, ((T([128, 128, 128], f16), [2097152]), {})\ncnt: 10, ((T([256, 64, 64], f16), [1048576]), {})\nOperator: aten.reflection_pad2d.default\ncnt: 1, ((T([1, 3, 256, 256], f16), [3, 3, 3, 3]), {})\ncnt: 18, ((T([1, 256, 64, 64], f16), [1, 1, 1, 1]), {})\ncnt: 1, ((T([1, 64, 256, 256], f16), [3, 3, 3, 3]), {})\nOperator: aten.reflection_pad2d_backward.default\ncnt: 1, ((T([1, 64, 262, 262], f16), T([1, 64, 256, 256], f16), [3, 3, 3, 3]), {})\ncnt: 18, ((T([1, 256, 66, 66], f16), T([1, 256, 64, 64], f16), [1, 1, 1, 1]), {})\nOperator: aten.relu_.default\ncnt: 2, ((T([1, 64, 256, 256], f16),), {})\ncnt: 2, ((T([1, 128, 128, 128], f16),), {})\ncnt: 10, ((T([1, 256, 64, 64], f16),), {})\nOperator: aten.sum.default\ncnt: 1, ((T([1, 3, 256, 256], f16),), {})\nOperator: aten.tanh.default\ncnt: 1, ((T([1, 3, 256, 256], f16),), {})\nOperator: aten.tanh_backward.default\ncnt: 1, ((T([1, 3, 256, 256], f16, stride=(0, 0, 0, 0)), T([1, 3, 256, 256], f16)), {})\nOperator: aten.threshold_backward.default\ncnt: 2, ((T([1, 64, 256, 256], f16), T([1, 64, 256, 256], f16), 0), {})\ncnt: 2, ((T([1, 128, 128, 128], f16), T([1, 128, 128, 128], f16), 0), {})\ncnt: 10, ((T([1, 256, 64, 64], f16), T([1, 256, 64, 64], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Illustrating Immutable Function in Python for FX Graph Pass\nDESCRIPTION: This code snippet demonstrates a simple function that returns a clone of the input tensor. It's used to illustrate that such operations cannot be turned into no-ops in graph passes, as it would change the semantics of the compiled graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef f(x: Tensor):\n    return x.clone()\n```\n\n----------------------------------------\n\nTITLE: Defining a FakeTensor Data Structure in PyTorch (Python)\nDESCRIPTION: This Python class outlines the structure for a FakeTensor, which stores tensor metadata (such as size, dtype, device, and optionally dimension order) for use in symbolic or fake tracing contexts. All fields are explicitly typed, with dim_order denoted as a work-in-progress feature. The class facilitates shape/dtype inferences for graph construction without real data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass FakeTensor:\n  size: List[SymInt]\n  dtype: torch.dtype\n  device: torch.device\n  dim_order: List[int]  # This doesn't exist yet\n```\n\n----------------------------------------\n\nTITLE: Installing Miniz using vcpkg in C\nDESCRIPTION: This code snippet demonstrates how to download and install Miniz using the vcpkg dependency manager. It includes steps for cloning the vcpkg repository, bootstrapping, integrating, and installing Miniz.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/third_party/miniz-3.0.2/readme.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Microsoft/vcpkg.git\ncd vcpkg\n./bootstrap-vcpkg.sh\n./vcpkg integrate install\n./vcpkg install miniz\n```\n\n----------------------------------------\n\nTITLE: Invoking aten._to_copy Operator for Type and Device Casting - PyTorch - Python\nDESCRIPTION: These snippets illustrate calls to aten._to_copy for tensor dtype or device conversion, such as u8 to bool or f16 on CUDA. Inputs include tensor objects and destination dtype/device specifications. Output is a tensor with transformed type/layout/device. Requires torch and target device availability. Limitations include device compatibility and datatype support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 12, ((T([1, 1, 1024, 1024], u8),), {'dtype': torch.bool})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 12, ((T([], f16),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\n```\n\n----------------------------------------\n\nTITLE: Gradient Transform Examples\nDESCRIPTION: Demonstrates using grad transform for computing first and second-order gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import grad\nx = torch.randn([])\ncos_x = grad(lambda x: torch.sin(x))(x)\nassert torch.allclose(cos_x, x.cos())\n\n# Second-order gradients\nneg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)\nassert torch.allclose(neg_sin_x, -x.sin())\n```\n\n----------------------------------------\n\nTITLE: Signaling Completion from Consumer Process Using Events (Python)\nDESCRIPTION: Demonstrates how a consumer process, after receiving and using shared tensors, signals its completion to the producer process by calling `event.set()` on a shared event object. This allows the waiting producer process (using `event.wait()`) to proceed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n## consumer\n# receive tensors and use them\nevent.set()\n```\n\n----------------------------------------\n\nTITLE: Profiling ATen Operator Calls in PyTorch (Python)\nDESCRIPTION: This snippet logs counts and input/output patterns for a wide variety of ATen operators within a Python program, likely via PyTorch's tracing or profiling mechanisms. Dependencies include PyTorch, and familiarity with tensor shapes, strides, and data types is required. Each entry groups calls by the operator, lists the number of occurrences (cnt), and details argument examples and optional keyword settings. The expected input is a model execution trace, and the output enumerates how operators such as softmax, addmm, bmm, embedding, and loss-related operations are being used, which is crucial for optimization and debugging efforts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistillGPT2_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([511, 50257], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([511, 50257], f16), T([511, 50257], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 6, ((T([1, 12, 512, 512], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 6, ((T([1, 12, 512, 512], f16), T([1, 12, 512, 512], f16), -1, f16), {})\nOperator: aten._to_copy.default\ncnt: 6, ((T([1, 1, 512, 512], u8, stride=(1048576, 1048576, 1024, 1)),), {'dtype': torch.bool})\ncnt: 6, ((T([], f16),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten._unsafe_view.default\ncnt: 6, ((T([12, 512, 512], f16), [1, 12, 512, 512]), {})\ncnt: 6, ((T([12, 512, 64], f16), [1, 12, 512, 64]), {})\ncnt: 1, ((T([512, 50257], f16), [1, 512, 50257]), {})\ncnt: 12, ((T([1, 512, 12, 64], f16), [1, 512, 768]), {})\nOperator: aten.add.Tensor\ncnt: 25, ((T([1, 512, 768], f16), T([1, 512, 768], f16)), {})\ncnt: 18, ((T([1, 512, 3072], f16), T([1, 512, 3072], f16)), {})\ncnt: 6, ((T([1, 512, 3072], f16), 1.0), {})\ncnt: 1, ((T([50257, 768], f16), T([50257, 768], f16)), {})\nOperator: aten.addmm.default\ncnt: 6, ((T([2304], f16), T([512, 768], f16), T([768, 2304], f16)), {})\ncnt: 6, ((T([768], f16), T([512, 768], f16), T([768, 768], f16)), {})\ncnt: 6, ((T([3072], f16), T([512, 768], f16), T([768, 3072], f16)), {})\ncnt: 6, ((T([768], f16), T([512, 3072], f16), T([3072, 768], f16)), {})\nOperator: aten.bmm.default\ncnt: 6, ((T([12, 512, 64], f16, stride=(64, 2304, 1)), T([12, 64, 512], f16, stride=(64, 1, 2304))), {})\ncnt: 12, ((T([12, 512, 512], f16), T([12, 512, 64], f16, stride=(64, 2304, 1))), {})\ncnt: 6, ((T([12, 512, 512], f16, stride=(262144, 1, 512)), T([12, 512, 64], f16, stride=(64, 768, 1))), {})\ncnt: 6, ((T([12, 512, 64], f16, stride=(64, 768, 1)), T([12, 64, 512], f16, stride=(64, 1, 2304))), {})\ncnt: 6, ((T([12, 64, 512], f16, stride=(64, 1, 2304)), T([12, 512, 512], f16)), {})\nOperator: aten.cat.default\ncnt: 6, (([T([1, 512, 768], f16), T([1, 512, 768], f16, stride=(512, 1, 512)), T([1, 512, 768], f16)], 2), {})\nOperator: aten.clone.default\ncnt: 2, ((T([1, 512], i64),), {})\nOperator: aten.copy_.default\ncnt: 2, ((T([1, 512], i64), T([1, 512], i64)), {})\nOperator: aten.div.Tensor\ncnt: 12, ((T([1, 12, 512, 512], f16), T([], f16)), {})\nOperator: aten.embedding.default\ncnt: 1, ((T([50257, 768], f16), T([1, 512], i64)), {})\ncnt: 1, ((T([1024, 768], f16), T([1, 512], i64)), {})\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([1, 512, 768], f16), T([1, 512], i64), 1024, -1, False), {})\ncnt: 1, ((T([1, 512, 768], f16), T([1, 512], i64), 50257, -1, False), {})\nOperator: aten.mm.default\ncnt: 1, ((T([512, 768], f16), T([768, 50257], f16, stride=(1, 768))), {})\ncnt: 1, ((T([50257, 512], f16, stride=(1, 50257)), T([512, 768], f16)), {})\ncnt: 1, ((T([512, 50257], f16), T([50257, 768], f16)), {})\ncnt: 6, ((T([512, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 6, ((T([3072, 512], f16, stride=(1, 3072)), T([512, 768], f16)), {})\ncnt: 6, ((T([512, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 6, ((T([768, 512], f16, stride=(1, 768)), T([512, 3072], f16)), {})\ncnt: 6, ((T([512, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 6, ((T([768, 512], f16, stride=(1, 768)), T([512, 768], f16)), {})\ncnt: 6, ((T([512, 2304], f16), T([2304, 768], f16, stride=(1, 2304))), {})\ncnt: 6, ((T([768, 512], f16, stride=(1, 768)), T([512, 2304], f16)), {})\nOperator: aten.mul.Scalar\ncnt: 6, ((T([1, 512, 3072], f16), 3.0), {})\nOperator: aten.mul.Tensor\ncnt: 12, ((T([1, 512, 3072], f16), 0.5), {})\ncnt: 12, ((T([1, 512, 3072], f16), 0.044715), {})\ncnt: 12, ((T([1, 512, 3072], f16), 0.7978845608028654), {})\ncnt: 24, ((T([1, 512, 3072], f16), T([1, 512, 3072], f16)), {})\nOperator: aten.native_layer_norm.default\ncnt: 13, ((T([1, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-05), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 13, ((T([1, 512, 768], f16), T([1, 512, 768], f16), [768], T([1, 512, 1], f32), T([1, 512, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([511, 50257], f16), T([511], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([511, 50257], f16), T([511], i64), None, 1, -100), {})\nOperator: aten.pow.Tensor_Scalar\ncnt: 6, ((T([1, 512, 3072], f16), 3.0), {})\ncnt: 6, ((T([1, 512, 3072], f16), 2.0), {})\nOperator: aten.slice_backward.default\ncnt: 1, ((T([1, 511, 50257], f16), [1, 511, 50257], 2, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([1, 511, 50257], f16), [1, 512, 50257], 1, 0, -1, 1), {})\nOperator: aten.split.Tensor\ncnt: 6, ((T([1, 512, 2304], f16), 768, 2), {})\nOperator: aten.sum.SymInt\ncnt: 12, ((T([512, 768], f16), [0], True), {})\ncnt: 6, ((T([512, 3072], f16), [0], True), {})\ncnt: 6, ((T([512, 2304], f16), [0], True), {})\nOperator: aten.tanh.default\ncnt: 6, ((T([1, 512, 3072], f16),), {})\nOperator: aten.tanh_backward.default\ncnt: 6, ((T([1, 512, 3072], f16), T([1, 512, 3072], f16)), {})\nOperator: aten.where.self\ncnt: 12, ((T([1, 1, 512, 512], b8), T([1, 12, 512, 512], f16), T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Backward Convolution Operations\nDESCRIPTION: Backward pass convolution operations computing gradients with respect to input, weight and bias tensors. Operations maintain same precision and configuration as forward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 1536, 7, 7], f16), T([64, 264, 7, 7], f16), T([1536, 264, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Matrix Multiplication via aten.bmm\nDESCRIPTION: The aten.bmm.default performs batch matrix multiplication on sets of 3D tensors. PyTorch dependency ensures that inputs comprising compatible tensor shapes perform batched matrix multiplications with resulting output tensors having batched products.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 36, ((T([96, 128, 64], f16), T([96, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 36, ((T([96, 128, 128], f16), T([96, 128, 64], f16)), {})\ncnt: 18, ((T([96, 128, 128], f16, stride=(16384, 1, 128)), T([96, 128, 64], f16)), {})\ncnt: 18, ((T([96, 64, 128], f16, stride=(8192, 1, 64)), T([96, 128, 128], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Setting up Type-Wrapped JIT Scripting for Doctest in PyTorch (Python)\nDESCRIPTION: This snippet reassigns torch.jit.script and torch.jit.trace within the test environment to inject a synthetic __module__ attribute, ensuring that objects scripted or traced during doctests have the correct module context. This is necessary because inspect-based introspection in doctest environments can otherwise misbehave, particularly for TorchScript-annotated code. Prerequisites: PyTorch installed. No inputs, directly modifies global torch.jit attributes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# These are hidden from the docs, but these are necessary for `doctest`\n# since the `inspect` module doesn't play nicely with the execution\n# environment for `doctest`\nimport torch\n\noriginal_script = torch.jit.script\ndef script_wrapper(obj, *args, **kwargs):\n    obj.__module__ = 'FakeMod'\n    return original_script(obj, *args, **kwargs)\n\ntorch.jit.script = script_wrapper\n\noriginal_trace = torch.jit.trace\ndef trace_wrapper(obj, *args, **kwargs):\n    obj.__module__ = 'FakeMod'\n    return original_trace(obj, *args, **kwargs)\n\ntorch.jit.trace = trace_wrapper\n```\n\n----------------------------------------\n\nTITLE: Temporarily Disabling FakeTensor Mode\nDESCRIPTION: Shows how to temporarily disable fake tensor mode using unset_fake_temporarily context manager, useful for cases like constant propagation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._subclasses.fake_tensor import unset_fake_temporarily\nwith unset_fake_temporarily():\n    ... # fake mode is disabled here, you can do real tensor compute\n```\n\n----------------------------------------\n\nTITLE: Profiling Triton and CPU Kernels in PyTorch\nDESCRIPTION: Explains three key components visible in profiling traces: CPU-side events (prefixed with 'triton_'), kernel launches (cuLaunchKernel), and GPU-side events. Covers both Inductor-generated and non-Inductor Triton kernels, as well as CPU kernels and performance considerations around launch overhead.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Implementing Custom Pruning Pattern for Conv2D with Pooling\nDESCRIPTION: Example showing how to create a custom pruning function for a Conv2D-Pool-Activation-Conv2D pattern and integrate it with the default pruning patterns. The code demonstrates pattern definition and registration with SaliencyPruner.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.ao.pruning._experimental.pruner.prune_functions import prune_conv2d_activation_conv2d\n\ndef prune_conv2d_pool_activation_conv2d(\n    c1: nn.Conv2d,\n    pool: nn.Module,\n    activation: Optional[Callable[[Tensor], Tensor]],\n    c2: nn.Conv2d,\n) -> None:\n    prune_conv2d_activation_conv2d(c1, activation, c2)\n\n# note how the pattern defined in the key will be passed to the pruning function as args\nmy_patterns = {(nn.Conv2d, nn.MaxPool2d, nn.ReLU, nn.Conv2d): prune_conv2d_activation_conv2d}\n\npruning_patterns = _get_default_structured_pruning_patterns()\npruning_patterns.update(my_patterns)\n\npruner = SaliencyPruner({}, patterns=pruning_patterns)\n```\n\n----------------------------------------\n\nTITLE: Printing Optimized Graph of jit-traced Function in PyTorch\nDESCRIPTION: This code prints the optimized graph of the jit-traced function, revealing that the if statement has been optimized out, leading to incorrect behavior with changed inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprint(torch.jit.last_executed_optimized_graph())\n\n# graph(%t : Tensor,\n#       %maybe : Tensor):\n#   %2 : Tensor = prim::profile[profiled_type=Float(1, strides=[1], requires_grad=0, device=cpu), seen_none=0](%t)\n#    = prim::profile()\n#   return (%2)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Namespaced Functions in ATen\nDESCRIPTION: The snippet demonstrates how to register functions in custom namespaces within ATen by prefixing the function name with the desired namespace. This is useful for functions that do not belong to the default ATen namespace but are commonly used and thus require a shared registration space. It provides a structure for specifying function signatures, variants, and dispatch mechanisms for different architectures like CPU and CUDA.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n\\n- func: custom::my_op(Tensor(a) self, ...) -> Tensor(a)\\n  variants: function, method\\n  dispatch:\\n    CPU: my_op_cpu\\n    CUDA: my_op_cuda\\n\n```\n\n----------------------------------------\n\nTITLE: Executing Softmax in PyTorch\nDESCRIPTION: Computes the softmax of a tensor along the specified dimension, essential for probability distributions in classification problems. This function operates on FP16 tensors and allows specification of the dimension for computation. The result is a tensor with values normalized between 0 and 1.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\naten._softmax.default(T([256, 128, 128], f16), -1, False)\n```\n\n----------------------------------------\n\nTITLE: PyTorch vmap with Same Randomness\nDESCRIPTION: Shows how to use vmap with 'same' randomness mode where all elements in the batch receive the same random values. The example demonstrates adding random noise to a tensor where all elements get the same random value.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef add_noise(x):\n    y = torch.randn(())  # y will be the same across the batch\n    return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"same\")(x)  # we get the same value, repeated 3 times\n```\n\n----------------------------------------\n\nTITLE: Calling Scripted Functions from Traced Functions in torch.jit (Python)\nDESCRIPTION: Shows how a scripted TorchScript function can be called from within a traced function. Requires PyTorch. The scripted function 'foo' uses control-flow to select between two tensors. The outer function 'bar' is then traced; when invoked, it will preserve the control-flow logic from 'foo'. Both inputs and outputs are torch.Tensors, useful for models with mixed computation paradigms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\n\\n@torch.jit.script\\ndef foo(x, y):\\n    if x.max() > y.max():\\n        r = x\\n    else:\\n        r = y\\n    return r\\n\\ndef bar(x, y, z):\\n    return foo(x, y) + z\\n\\ntraced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3)))\\n\n```\n\n----------------------------------------\n\nTITLE: Enabling or Disabling TF32 Precision for CUDA Backend in PyTorch (Python)\nDESCRIPTION: This snippet shows how to configure the use of TensorFloat32 (TF32) tensor cores for matrix multiplications and convolutions in PyTorch by setting the allow_tf32 property. The operation requires PyTorch with a CUDA-enabled Nvidia Ampere (or later) GPU. Enabling TF32 can improve performance at the cost of reduced precision. The key configuration parameters are torch.backends.cuda.matmul.allow_tf32 and torch.backends.cudnn.allow_tf32, accepting boolean values to enable or disable TF32 usage in matmul and convolution routines.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/numerical_accuracy.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = False\n```\n\n----------------------------------------\n\nTITLE: Verifying Hessian Results Match Between API and Manual - Python\nDESCRIPTION: Confirms that both functorch.hessian and manual jacfwd(jacfwd()) approaches produce the same Hessian output, using torch.allclose. Requires hess_api and hess_fwdfwd populated. Returns Boolean result.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntorch.allclose(hess_api, hess_fwdfwd)\n```\n\n----------------------------------------\n\nTITLE: Launching Fault-Tolerant Distributed Training Job with torchrun in Bash\nDESCRIPTION: Command to launch a fault-tolerant distributed training job using torchrun. It specifies the number of nodes, processes per node, maximum restarts, rendezvous ID, backend, and endpoint. The command should be run on all participating nodes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/quickstart.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun\n   --nnodes=NUM_NODES\n   --nproc-per-node=TRAINERS_PER_NODE\n   --max-restarts=NUM_ALLOWED_FAILURES\n   --rdzv-id=JOB_ID\n   --rdzv-backend=c10d\n   --rdzv-endpoint=HOST_NODE_ADDR\n   YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n```\n\n----------------------------------------\n\nTITLE: Debugging Backend with Graph Printing\nDESCRIPTION: Implements a debugging backend that prints the FX graph structure before execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef fn(x, y):\n    a = torch.cos(x)\n    b = torch.sin(y)\n    return a + b\nfn(torch.randn(10), torch.randn(10))\n```\n\n----------------------------------------\n\nTITLE: Seeding Global NumPy Random Number Generator in Python\nDESCRIPTION: Seeds the global random number generator provided by the NumPy library using `np.random.seed(0)`. This is crucial for reproducibility if the application or any of its dependencies utilize `numpy.random` functions. Note that this does not affect NumPy `Generator` objects, which require separate seeding.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nnp.random.seed(0)\n```\n\n----------------------------------------\n\nTITLE: Launching Multiple Processes for Pipeline Parallelism in Bash\nDESCRIPTION: This bash command demonstrates how to use torchrun to launch multiple processes for pipeline parallelism, specifically launching 2 processes per node to run the example script.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node=2 example.py\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Numerical Overflow in PyTorch Norm Computation (Python)\nDESCRIPTION: This Python code snippet illustrates how the .norm() method in PyTorch can produce overflow (inf) results when using FP32 precision for large input values, and how converting the tensor to double precision (float64) increases the range and avoids the overflow. The snippet requires the torch library and demonstrates the effect of numerical precision on operations. The main input parameter is a torch tensor with large numbers and outputs are the results of .norm() in fp32 and float64. Limitations involve possible overflows in fp32 for large values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/numerical_accuracy.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\na=torch.tensor([1e20, 1e20]) # fp32 type by default\na.norm() # produces tensor(inf)\na.double().norm() # produces tensor(1.4142e+20, dtype=torch.float64), representable in fp32\n```\n\n----------------------------------------\n\nTITLE: Listing Available Backends for torch.compile in Python\nDESCRIPTION: Shows how to view the list of supported backends for torch.compile using the torch.compiler.list_backends() function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntorch.compiler.list_backends()\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison Helper Function for Benchmarks - Python\nDESCRIPTION: Defines 'get_perf' to compare timing results between two torch.benchmark Timer runs. Prints percent improvement for the faster method. Parameters: two timing objects and descriptor strings. Prerequisite: requires Torch Benchmark library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef get_perf(first, first_descriptor, second, second_descriptor):\n  \"\"\"  takes torch.benchmark objects and compares delta of second vs first. \"\"\"\n  faster = second.times[0]\n  slower = first.times[0]\n  gain = (slower-faster)/slower\n  if gain < 0: gain *=-1 \n  final_gain = gain*100\n  print(f\" Performance delta: {final_gain:.4f} percent improvement with {second_descriptor} \")\n```\n\n----------------------------------------\n\nTITLE: Reclaiming Memory from MemPool in PyTorch\nDESCRIPTION: Code snippet showing how to reclaim memory from a MemPool by deleting tensor references and the pool itself, which triggers the internal empty_cache method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndel tensor, del pool\n```\n\n----------------------------------------\n\nTITLE: Debug Compile Usage\nDESCRIPTION: Example showing how to use debug_compile to dump graph information and constants to disk for debugging.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch.compile import memory_efficient_fusion, debug_compile\n\nmemory_efficient_fusion(foo, bw_compiler=debug_compile)(inp)\n```\n\n----------------------------------------\n\nTITLE: Creating a CSR Tensor in PyTorch from Explicit Components\nDESCRIPTION: Demonstrates how to create a sparse CSR tensor by directly providing crow_indices, col_indices, and values tensors. The example creates a 2x2 sparse matrix with all elements defined, showing how to specify the dtype.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncrow_indices = torch.tensor([0, 2, 4])\ncol_indices = torch.tensor([0, 1, 0, 1])\nvalues = torch.tensor([1, 2, 3, 4])\ncsr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\ncsr\n```\n\n----------------------------------------\n\nTITLE: Local Autograd Example for Dependency Computation\nDESCRIPTION: Example of local autograd computation on a single machine to demonstrate how dependencies are calculated during the backward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\na = torch.rand((3, 3), requires_grad=True)\nb = torch.rand((3, 3), requires_grad=True)\nc = torch.rand((3, 3), requires_grad=True)\nd = a + b\ne = b * c\nd.sum.().backward()\n```\n\n----------------------------------------\n\nTITLE: Complete Distributed Autograd Example (Incomplete)\nDESCRIPTION: Beginning of a complete example showing how to use distributed autograd with the RPC framework. This snippet sets up the imports and defines the add function but is incomplete in the provided text.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed.autograd as dist_autograd\nimport torch.distributed.rpc as rpc\n\ndef my_add(t1, t2):\n  return torch.add(t1, t2)\n\n# On worker 0:\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations\nDESCRIPTION: Convolution operations with input tensors of various sizes using half-precision (f16) format. Includes stride, padding and other convolution parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_xception65_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n((T([32, 64, 150, 150], f16), T([32, 32, 150, 150], f16), T([64, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n((T([32, 32, 150, 150], f16), T([32, 3, 299, 299], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch API Documentation Structure with Sphinx toctree\nDESCRIPTION: A toctree directive in Sphinx documentation that defines the navigation structure for PyTorch's Python API documentation. It includes links to core modules, utilities, and specialized components with a maxdepth of 1.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/pytorch-api.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{toctree}\n:glob:\n:maxdepth: 1\n\ntorch\nnn\nnn.functional\ntensors\ntensor_attributes\ntensor_view\ntorch.amp <amp>\ntorch.autograd <autograd>\ntorch.library <library>\naccelerator\ncpu\ncuda\ntorch.cuda.memory <torch_cuda_memory>\nmps\nxpu\nmtia\nmtia.memory\nmeta\ntorch.backends <backends>\ntorch.export <export>\ntorch.distributed <distributed>\ntorch.distributed.tensor <distributed.tensor>\ntorch.distributed.algorithms.join <distributed.algorithms.join>\ntorch.distributed.elastic <distributed.elastic>\ntorch.distributed.fsdp <fsdp>\ntorch.distributed.fsdp.fully_shard <distributed.fsdp.fully_shard>\ntorch.distributed.tensor.parallel <distributed.tensor.parallel>\ntorch.distributed.optim <distributed.optim>\ntorch.distributed.pipelining <distributed.pipelining>\ntorch.distributed.checkpoint <distributed.checkpoint>\ntorch.distributions <distributions>\ntorch.compiler <torch.compiler>\ntorch.fft <fft>\ntorch.func <func>\nfutures\nfx\nfx.experimental\ntorch.hub <hub>\ntorch.jit <jit>\ntorch.linalg <linalg>\ntorch.monitor <monitor>\ntorch.signal <signal>\ntorch.special <special>\ntorch.overrides\ntorch.package <package>\nprofiler\nnn.init\nnn.attention\nonnx\noptim\ncomplex_numbers\nddp_comm_hooks\nquantization\nrpc\ntorch.random <random>\nmasked\ntorch.nested <nested>\nsize\nsparse\nstorage\ntorch.testing <testing>\ntorch.utils <utils>\ntorch.utils.benchmark <benchmark_utils>\ntorch.utils.bottleneck <bottleneck>\ntorch.utils.checkpoint <checkpoint>\ntorch.utils.cpp_extension <cpp_extension>\ntorch.utils.data <data>\ntorch.utils.deterministic <deterministic>\ntorch.utils.jit <jit_utils>\ntorch.utils.dlpack <dlpack>\ntorch.utils.mobile_optimizer <mobile_optimizer>\ntorch.utils.model_zoo <model_zoo>\ntorch.utils.tensorboard <tensorboard>\ntorch.utils.module_tracker <module_tracker>\ntype_info\nnamed_tensor\nname_inference\ntorch.__config__ <config_mod>\ntorch.__future__ <future_mod>\nlogging\ntorch_environment_variables\n```\n\n----------------------------------------\n\nTITLE: NCCL Integration with CUDAGraph Trees in PyTorch\nDESCRIPTION: Example of a function using NCCL operators with torch.compile in \"reduce-overhead\" mode for cross-device communication within CUDAGraph Trees.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile(mode=\"reduce-overhead\")\ndef func(x):\n    y = x * x\n    y = torch.distributed.all_reduce(y, op=torch.distributed.ReduceOp.SUM)\n    x = torch.nn.functional.silu(x)\n    return x * y\n```\n\n----------------------------------------\n\nTITLE: Parallel Execution with Fork and Wait in TorchScript\nDESCRIPTION: Demonstrates how to execute functions in parallel using TorchScript's fork and wait primitives. The fork primitive starts parallel execution while wait synchronizes the threads.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef fn(arg0, arg1, ...):\n  ...\n  return v\n\nfut = torch.jit.fork(fn, arg0, arg1, ...)\n...\nv = torch.jit.wait(fut)\n```\n\n----------------------------------------\n\nTITLE: Specialized Shape-Based Control Flow in Exported PyTorch Program\nDESCRIPTION: The exported program resulting from shape specialization, showing how conditional branches based on tensor shapes are removed during export. Only the branch matching the example input's shape is preserved in the final graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nExportedProgram:\nclass GraphModule(torch.nn.Module):\n    def forward(self, x: \"f32[10, 2]\"):\n        # code: return x + 1\n        add: \"f32[10, 2]\" = torch.ops.aten.add.Tensor(x, 1)\n        return (add,)\n```\n\n----------------------------------------\n\nTITLE: Loading Optimizer State Dict with Custom Parameter Mapping\nDESCRIPTION: This example shows how to implement a custom hook for loading an optimizer state dict when the model architecture has changed. It demonstrates adapting the state dict to map parameters from a single-layer model to a two-layer model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef adapt_state_dict_ids(optimizer, state_dict):\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n    for k, v in state_dict['param_groups'][0].items():\n        if k not in ['params', 'param_names']:\n            adapted_state_dict['param_groups'][0][k] = v\n\n    lookup_dict = {\n        'fc1.weight': 'fc.weight',\n        'fc1.bias': 'fc.bias',\n        'fc2.weight': 'fc.weight',\n        'fc2.bias': 'fc.bias'\n    }\n    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}\n    for param_id, param_name in zip(\n            optimizer.state_dict()['param_groups'][0]['params'],\n            optimizer.state_dict()['param_groups'][0]['param_names']):\n        name_in_loaded = lookup_dict[param_name]\n        index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)\n        id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]\n        # Copy the state of the corresponding parameter\n        if id_in_loaded in state_dict['state']:\n            adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\noptimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)\noptimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict\n```\n\n----------------------------------------\n\nTITLE: Optimization with Closure Function in PyTorch\nDESCRIPTION: Example for advanced optimizers like Conjugate Gradient and LBFGS that need to re-evaluate the function multiple times. A closure function encapsulates the gradient computation process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n```\n\n----------------------------------------\n\nTITLE: Downloading LibTorch Distribution using wget and unzip\nDESCRIPTION: Commands to download and extract the LibTorch ZIP archive for CPU-only version. This is the first step in setting up a C++ project with PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nwget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip\nunzip libtorch-shared-with-deps-latest.zip\n```\n\n----------------------------------------\n\nTITLE: Using torch.iinfo Properties Example\nDESCRIPTION: Example properties available through torch.iinfo class for integer data types. These properties include bits, max, and min values for numeric analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/type_info.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbits        int     # The number of bits occupied by the type\nmax         int     # The largest representable number\nmin         int     # The smallest representable number\n```\n\n----------------------------------------\n\nTITLE: Stacking Tensors using aten.stack - Python\nDESCRIPTION: Captures an example of using aten.stack.default to combine a sequence of tensors along a new axis, resulting in higher-rank tensors. The inputs are typically lists of identically shaped tensors (e.g., f32 with shape [504, 64]), and outputs are stacked tensors. Prerequisite: PyTorch with input shapes matching exactly.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.stack.default\ncnt: 12, (([T([504, 64], f32), T([504, 64], f32)],), {})\n```\n\n----------------------------------------\n\nTITLE: Recursively Iterating Through All Modules in a Nested Network\nDESCRIPTION: Defines a `BigNet` module which contains another module (`Net`) as a submodule. It then demonstrates using `named_modules()` to recursively iterate through all modules within `big_net`, including the top-level module itself, its direct children (`l1`, `net`), and the children of nested modules (`net.l0`, `net.l1`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass BigNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l1 = MyLinear(5, 4)\n    self.net = Net()\n  def forward(self, x):\n    return self.net(self.l1(x))\n\nbig_net = BigNet()\nfor module in big_net.named_modules():\n  print(module)\n: ('', BigNet(\n  (l1): MyLinear()\n  (net): Net(\n    (l0): MyLinear()\n    (l1): MyLinear()\n  )\n))\n('l1', MyLinear())\n('net', Net(\n  (l0): MyLinear()\n  (l1): MyLinear()\n))\n('net.l0', MyLinear())\n('net.l1', MyLinear())\n```\n\n----------------------------------------\n\nTITLE: Optimizers with Complex Parameters\nDESCRIPTION: Shows how to use optimizers with complex parameters, demonstrating equivalence with real parameter optimization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nparams = [torch.rand(2, 3, dtype=torch.complex64) for _ in range(5)]\nreal_params = [torch.view_as_real(p) for p in params]\n\ncomplex_optim = torch.optim.AdamW(params)\nreal_optim = torch.optim.AdamW(real_params)\n```\n\n----------------------------------------\n\nTITLE: Free Intermediate Tensors in PyTorch\nDESCRIPTION: This code example illustrates how intermediate variables in PyTorch can unnecessarily extend their scope and retain memory. The snippet provides a way to free memory earlier by explicitly deleting intermediate objects once they are no longer needed within a loop.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/faq.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n    for i in range(5):\n        intermediate = f(input[i])\n        result += g(intermediate)\n    output = h(result)\n    return output\n```\n\n----------------------------------------\n\nTITLE: Comparing Dropout Behavior with Symbolic Trace in PyTorch FX\nDESCRIPTION: This code demonstrates a difference in behavior between functional and module-based dropout when using torch.fx.symbolic_trace. The functional version (torch.nn.functional.dropout) maintains its training flag during tracing, while the nn.Dropout module version properly respects the module's training state.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False);  x = None\nreturn dropout\n```\n\nLANGUAGE: python\nCODE:\n```\ntraced.eval()\n\nx = torch.randn(5, 3)\ntorch.testing.assert_close(traced(x), x)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DropoutRepro2(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.drop = torch.nn.Dropout()\n\n  def forward(self, x):\n    return self.drop(x)\n\ntraced = torch.fx.symbolic_trace(DropoutRepro2())\nprint(traced.code)\n\"\"\"\ndef forward(self, x):\n  drop = self.drop(x);  x = None\n  return drop\n\"\"\"\n\ntraced.eval()\n\nx = torch.randn(5, 3)\ntorch.testing.assert_close(traced(x), x)\n```\n\n----------------------------------------\n\nTITLE: Batch Matrix Multiplication (bmm) for 3D Tensors in PyTorch (Python)\nDESCRIPTION: Usage of batched matrix-matrix (bmm) multiplications for sequences of matrices (3D tensors), commonly used in transformer models for attention score computation. Requires matching batch and inner dimensions; supports explicit stride specifications for efficiency. Inputs/outputs are FP16 tensors, and improper dimensions will raise runtime exceptions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 12, ((T([48, 128, 64], f16), T([48, 64, 128], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 12, ((T([48, 128, 128], f16), T([48, 128, 64], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 12, ((T([48, 128, 128], f16, stride=(16384, 1, 128)), T([48, 128, 64], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 12, ((T([48, 128, 64], f16), T([48, 64, 128], f16, stride=(8192, 1, 64))), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 12, ((T([48, 64, 128], f16, stride=(8192, 1, 64)), T([48, 128, 128], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 12, ((T([48, 128, 128], f16), T([48, 128, 64], f16, stride=(8192, 1, 128))), {})\n```\n\n----------------------------------------\n\nTITLE: Backward Convolution in PyTorch\nDESCRIPTION: Calculates the gradients for inputs based on the convolution operation, supporting backpropagation in neural networks. Requires tensors representing inputs, weights, and biases with relevant gradient configurations. Outputs include gradients for input and weight tensors, crucial for parameter updates during training phases.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([16, 3, 128, 128], f16), T([16, 64, 128, 128], f16), T([3, 64, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 64, 128, 128], f16), T([16, 128, 64, 64], f16), T([128, 64, 4, 4], f16), [0], [2, 2], [1, 1], [1, 1], True, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 128, 64, 64], f16), T([16, 256, 32, 32], f16), T([256, 128, 4, 4], f16), [0], [2, 2], [1, 1], [1, 1], True, [0, 0], 1, [True, True, False]), {})\ncnt: 12, ((T([16, 256, 32, 32], f16), T([16, 256, 32, 32], f16), T([256, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 256, 32, 32], f16), T([16, 128, 64, 64], f16), T([256, 128, 4, 4], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 128, 64, 64], f16), T([16, 64, 128, 128], f16), T([128, 64, 4, 4], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 64, 128, 128], f16), T([16, 8, 128, 128], f16), T([64, 8, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Graph Visualization with AOT Function\nDESCRIPTION: Implements a graph drawer compiler that saves forward and backward graphs as SVG files using the draw_graph utility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    return x.cos().cos()\n\ndef graph_drawer(name):\n    def f(fx_g: fx.GraphModule, inps):\n        draw_graph(fx_g, name)\n        return fx_g\n    return f\n\naot_function(f, fw_compiler=graph_drawer(\"forward\"), bw_compiler=graph_drawer(\"backward\"))(torch.randn(3, requires_grad=True))\n```\n\n----------------------------------------\n\nTITLE: Implementing vmap for Custom PyTorch Autograd Function\nDESCRIPTION: This snippet shows the implementation of vmap support for a custom PyTorch autograd Function. It handles expanding batched dimensions and applies the NumpyTake operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nreturn x.expand(info.batch_size, *x.shape)\nreturn x.movedim(x_bdim, 0)\n\n# If the Tensor doesn't have the dimension being vmapped over,\n# expand it out. Otherwise, move it to the front of the Tensor\nx = maybe_expand_bdim_at_front(x, x_bdim)\nind = maybe_expand_bdim_at_front(ind, ind_bdim)\nind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)\n\n# The return is a tuple (output, out_dims). Since output is a Tensor,\n# then out_dims is an Optional[int] (instead of being a Tuple).\nreturn NumpyTake.apply(x, ind, ind_inv, dim + 1), 0\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Load Options in Python\nDESCRIPTION: Configuration options for torch.load including mmap settings, endianness control, mmap flags, and storage offset calculations. These settings affect how PyTorch deserializes data when loading saved models and tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntorch.utils.serialization.config.load = {\n    \"mmap\": False,  # Controls memory mapping behavior\n    \"endianness\": torch.serialization.LoadEndianness.NATIVE,  # Controls byte order\n    \"mmap_flags\": \"MAP_PRIVATE\",  # Memory mapping access flags\n    \"calculate_storage_offsets\": False  # Controls storage offset calculation method\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Python Errors in C++ Code\nDESCRIPTION: This snippet demonstrates how to handle errors when writing Python bindings by hand. It uses HANDLE_TH_ERRORS and END_HANDLE_TH_ERRORS macros to catch exceptions and convert them to appropriate Python signals.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/README.md#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n// Entry point from Python interpreter\nPyObject* run(PyObject* arg) {\n  HANDLE_TH_ERRORS\n  ...\n  if (!x) throw python_error();\n  // From c10/Exception.h\n  TORCH_CHECK(cond, \"cond was false here\");\n  TORCH_WARN(\"Warning message\");\n  ...\n  END_HANDLE_TH_ERRORS\n}\n```\n\n----------------------------------------\n\nTITLE: Convolution Tensor Operations in PyTorch\nDESCRIPTION: Series of convolution operations with different tensor shapes and parameters. Operations use float16 precision and include varying input/output channels and spatial dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n((T([1, 1920, 1, 1], f16), T([1, 80, 1, 1], f16), T([1920, 80, 1, 1], f16), [1920], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: FX Graph Minifier Function Definition\nDESCRIPTION: Core minifier function that takes an FX GraphModule, inputs, and a failure checker function. It implements two main strategies: suffix truncation and delta debugging to minimize the graph while maintaining the failure condition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef minifier(fail_f: fx.GraphModule, inps, module_fails):\n    \"\"\"\n    Minimizes a FX graph with given inputs, such that the resulting FX graph still returns True for module_fails.\n\n    Does 2 main strategies:\n    1. Truncates suffix: Removes some suffix from the graph and sets a new output.\n    2. Delta Debugging: Tries replacing half of the graph with inputs. If fails,\n        tries replacing quarter of the graph, etc.\n\n    >>> failing_function = fx.symbolic_trace(f)\n    >>> minimize(failing_function, [torch.randn(5)], lambda fx_g, inps: fx_g(*inps))\n\n    note: module_fails returns True if it fails.\n    ...\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Capturing NLL Loss Backward Operator Usage (aten.nll_loss_backward) - PyTorch - Python\nDESCRIPTION: This snippet lists usage for the negative log likelihood loss backward operator, aten.nll_loss_backward, specifying empty and shaped tensors for predictions, targets, and gradient inputs, including ignore index and weight parameters. It exemplifies call signatures found in supervised deep learning tasks and assumes tensors provided conform to expected classification output and label shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom CUDA Memory Allocator in C++\nDESCRIPTION: This code snippet demonstrates how to create a simple custom CUDA memory allocator in C++ that traces memory operations. It defines malloc and free functions that can be compiled into a shared library for use with PyTorch's CUDAPluggableAllocator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\n#include <sys/types.h>\n#include <cuda_runtime_api.h>\n#include <iostream>\n// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC\nextern \"C\" {\nvoid* my_malloc(ssize_t size, int device, cudaStream_t stream) {\n   void *ptr;\n   cudaMalloc(&ptr, size);\n   std::cout<<\"alloc \"<<ptr<<size<<std::endl;\n   return ptr;\n}\n\nvoid my_free(void* ptr, ssize_t size, int device, cudaStream_t stream) {\n   std::cout<<\"free \"<<ptr<< \" \"<<stream<<std::endl;\n   cudaFree(ptr);\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting a TorchScript Function Handling Any in Tuple (Python)\nDESCRIPTION: This snippet demonstrates creating and scripting a function with @torch.jit.export that takes a tuple where the second element has the Any type, returning both the incremented first element and the second element unchanged. It illustrates how the Any type allows passing arbitrary types in the parameter tuple. Requires torch and typing.Any, expects tuple inputs; output is a tuple of incremented int and unmodified Any. This function can be scripted and invoked with different types bound to Any.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom typing import Tuple\nfrom typing import Any\n\n@torch.jit.export\ndef inc_first_element(x: Tuple[int, Any]):\n    return (x[0]+1, x[1])\n\nm = torch.jit.script(inc_first_element)\nprint(m((1,2.0)))\nprint(m((1,(100,200))))\n```\n\n----------------------------------------\n\nTITLE: Autograd and Tensor Python Interoperability (PyTorch Autograd/Python Bindings C++)\nDESCRIPTION: These symbol names correspond to functions providing Python/C++ interoperability for PyTorch's automatic differentiation (autograd) infrastructure. They include Python argument parsing/utilities, type casters for pybind11, and bespoke Python method bindings for Tensor view and manipulation operations. Dependencies require a Python interpreter, the pybind11 library, and torch/ATen tensor types. Inputs commonly include PyObject pointers and output the result of tensor operations or property queries, supporting mixed-language workflows.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_19\n\nLANGUAGE: C++\nCODE:\n```\n_ZN8pybind116detail9load_typeINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEENS0_11type_casterINS0_14intrinsic_typeIT_E4typeEvEERKNS_6handleE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch10PythonArgs18intlistWithDefaultEiSt6vectorIlSaIlEE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch10PythonArgs20memoryformatOptionalEi.isra.0\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autogradL19THPVariable_permuteEP7_objectS2_S2_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autogradL19THPVariable_reshapeEP7_objectS2_S2_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autogradL16THPVariable_viewEP7_objectS2_S2_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autogradL23THPVariable_bool_scalarEP7_objectS2_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autogradL16THPVariable_sizeEP7_objectS2_S2_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autogradL22THPVariable_contiguousEP7_objectS2_S2_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autogradL16THPVariable_siluEP7_objectS2_S2_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autogradL40THPVariable_scaled_dot_product_attentionEP7_objectS2_S2_\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Headed Attention with First-Class Dimensions in Python\nDESCRIPTION: This snippet shows how to implement multi-headed attention using first-class dimensions in PyTorch. It demonstrates reshaping inputs, batching over attention heads, and using matrix products for attention scores.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom torchdim import softmax\ndef multiheadattention(q, k, v, num_attention_heads, dropout_prob, use_positional_embedding):\n    batch, query_sequence, key_sequence, heads, features = dims(5)\n    heads.size = num_attention_heads\n\n    # binding dimensions, and unflattening the heads from the feature dimension\n    q = q[batch, query_sequence, [heads, features]]\n    k = k[batch, key_sequence, [heads, features]]\n    v = v[batch, key_sequence, [heads, features]]\n\n    # einsum-style operators to calculate scores,\n    attention_scores = (q*k).sum(features) * (features.size ** -0.5)\n\n    # use first-class dim to specify dimension for softmax\n    attention_probs = softmax(attention_scores, dim=key_sequence)\n\n    # dropout work pointwise, following Rule #1\n    attention_probs = torch.nn.functional.dropout(attention_probs, p=dropout_prob)\n\n    # another matrix product\n    context_layer = (attention_probs*v).sum(key_sequence)\n\n    # flatten heads back into features\n    return context_layer.order(batch, query_sequence, [heads, features])\n```\n\n----------------------------------------\n\nTITLE: Basic Usage Examples of First-class Dimensions in PyTorch\nDESCRIPTION: Demonstrates key use cases including matrix multiplication with einsum, pixel shuffling with rearrange, batched matrix multiplication, and embedding bag operations using first-class dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torchdim import dims\n\n# einsum\ndef mm(A: torch.Tensor, B: torch.Tensor):\n    i, j, k = dims(3)\n    r = (A[i, k] * B[k, j]).sum(k)\n    return r.order(i, j)\n\n# rearrange\ndef pixel_shuffle(img: torch.Tensor, upscale_factor=2):\n    h2, w2, c, b, h, w = dims(6)\n    h2.size = w2.size = upscale_factor\n    return img[b, (c, h2, w2), h, w].order(b, c, (h, h2), (w, w2))\n\n# batching\ndef bmm(A: torch.Tensor, B: torch.Tensor):\n    i = dims(1)\n    return mm(A[i], B[i]).order(i)\n\n# indexing\ndef embedding_bag(input: torch.Tensor, embedding_weights: torch.Tensor):\n    batch, sequence, features = dims(3)\n    r = embedding_weights[input[batch, sequence], features].sum(sequence)\n    return r.order(batch, features)\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Inspecting Dynamo Cache Entries in Python\nDESCRIPTION: This snippet demonstrates how to retrieve compiled code and guards from a function using Dynamo's debug API. It shows how to access the first cache entry and examine both the guard function and compiled code using Python's disassembler.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._dynamo.eval_frame import _debug_get_cache_entry_list, innermost_fn\ncache_entries = _debug_get_cache_entry_list(innermost_fn(toy_example))\ncache_entry = cache_entries[0]\nguard, code = cache_entry.check_fn, cache_entry.code\n# the guard takes the local variables of an input frame, and tells whether a re-compilation should be triggered.\nimport dis\ndis.dis(guard)\ndis.dis(code)\n```\n\n----------------------------------------\n\nTITLE: Performing Log Softmax in PyTorch (Python)\nDESCRIPTION: Executes the aten._log_softmax operation, which applies the logarithm of the softmax function on a tensor. This is often used in the final layer of a neural network for classification purposes. The operation includes specifying the input tensor shape and dimension to apply the function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\naten._log_softmax.default\ncnt: 1, ((T([16, 2], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Creating Backend Compiler Runtime Shared Library in CMake\nDESCRIPTION: Defines a shared library target named `backend_with_compiler_runtime` using specified C++ source files related to JIT backend testing and interfaces. This library serves as a dependency for the main test executable. Requires `TORCH_ROOT` variable and existence of source files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lite_interpreter_runtime/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(backend_with_compiler_runtime SHARED\n        ${TORCH_ROOT}/test/cpp/jit/test_backend_compiler_lib.cpp\n        ${TORCH_ROOT}/torch/csrc/jit/backends/backend_interface.cpp\n        )\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch aten Operator Usage Patterns\nDESCRIPTION: This data structure shows the usage patterns of various PyTorch aten operators in a model, displaying operator names, counts, and tensor shapes. It provides insights into tensor dimensions, data types (primarily f16, f32, i64, b8), and operation frequency across the model execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.exp.default\ncnt: 2, ((T([5000], f32, stride=(4,)),), {})\nOperator: aten.floor_divide.default\ncnt: 1, ((T([1, 5000], i64), 90), {})\nOperator: aten.gather.default\ncnt: 1, ((T([1, 76725, 4], f16), 1, T([1, 5000, 4], i64, stride=(5000, 1, 0))), {})\ncnt: 1, ((T([1, 76725, 90], f16), 1, T([1, 5000, 90], i64, stride=(5000, 1, 0))), {})\ncnt: 1, ((T([1, 5000, 90], f16), 2, T([1, 5000, 1], i64)), {})\nOperator: aten.ge.Scalar\ncnt: 1, ((T([5000, 4], f32), 0), {})\nOperator: aten.gt.Tensor\ncnt: 1, ((T([5000, 4], f32), T([4], f16)), {})\nOperator: aten.index.Tensor\ncnt: 1, ((T([76725, 4], f16, stride=(1, 76725)), [T([5000], i64)]), {})\ncnt: 1, ((T([5000, 4], f32), [T([100], i64)]), {})\ncnt: 1, ((T([5000, 1], f32), [T([100], i64)]), {})\ncnt: 1, ((T([5000, 1], i64), [T([100], i64)]), {})\nOperator: aten.masked_fill_.Scalar\ncnt: 1, ((T([5000, 4], f32), T([5000, 4], b8), 0), {})\nOperator: aten.max.default\ncnt: 1, ((T([5000, 4], f32),), {})\nOperator: aten.max_pool2d_with_indices.default\ncnt: 5, ((T([1, 88, 21, 21], f16), [3, 3], [2, 2]), {})\ncnt: 5, ((T([1, 88, 11, 11], f16), [3, 3], [2, 2]), {})\ncnt: 4, ((T([1, 88, 81, 81], f16), [3, 3], [2, 2]), {})\ncnt: 4, ((T([1, 88, 41, 41], f16), [3, 3], [2, 2]), {})\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 5, ((T([1, 88, 5, 5], f16), T([1, 88, 11, 11], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([1, 88, 5, 5], i64)), {})\ncnt: 5, ((T([1, 88, 10, 10], f16), T([1, 88, 21, 21], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([1, 88, 10, 10], i64)), {})\ncnt: 4, ((T([1, 88, 20, 20], f16), T([1, 88, 41, 41], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([1, 88, 20, 20], i64)), {})\ncnt: 4, ((T([1, 88, 40, 40], f16), T([1, 88, 81, 81], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([1, 88, 40, 40], i64)), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([1, 32, 320, 320], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 16, 320, 320], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 96, 160, 160], f16), [2, 3], True), {})\ncnt: 2, ((T([1, 144, 160, 160], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 144, 80, 80], f16), [2, 3], True), {})\ncnt: 2, ((T([1, 240, 80, 80], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 240, 40, 40], f16), [2, 3], True), {})\ncnt: 4, ((T([1, 480, 40, 40], f16), [2, 3], True), {})\ncnt: 3, ((T([1, 672, 40, 40], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 672, 20, 20], f16), [2, 3], True), {})\ncnt: 5, ((T([1, 1152, 20, 20], f16), [2, 3], True), {})\ncnt: 1, ((T([1, 1920, 20, 20], f16), [2, 3], True), {})\nOperator: aten.minimum.default\ncnt: 1, ((T([5000, 4], f32), T([4], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 2, ((T([1, 32, 320, 320], f16), T([1, 32, 1, 1], f16)), {})\ncnt: 2, ((T([1, 16, 320, 320], f16), T([1, 16, 1, 1], f16)), {})\ncnt: 2, ((T([1, 96, 160, 160], f16), T([1, 96, 1, 1], f16)), {})\ncnt: 4, ((T([1, 144, 160, 160], f16), T([1, 144, 1, 1], f16)), {})\ncnt: 2, ((T([1, 144, 80, 80], f16), T([1, 144, 1, 1], f16)), {})\ncnt: 4, ((T([1, 240, 80, 80], f16), T([1, 240, 1, 1], f16)), {})\ncnt: 2, ((T([1, 240, 40, 40], f16), T([1, 240, 1, 1], f16)), {})\ncnt: 8, ((T([1, 480, 40, 40], f16), T([1, 480, 1, 1], f16)), {})\ncnt: 6, ((T([1, 672, 40, 40], f16), T([1, 672, 1, 1], f16)), {})\ncnt: 2, ((T([1, 672, 20, 20], f16), T([1, 672, 1, 1], f16)), {})\ncnt: 10, ((T([1, 1152, 20, 20], f16), T([1, 1152, 1, 1], f16)), {})\ncnt: 2, ((T([1, 1920, 20, 20], f16), T([1, 1920, 1, 1], f16)), {})\ncnt: 40, ((T([1, 88, 10, 10], f16), T([], f16)), {})\ncnt: 40, ((T([1, 88, 20, 20], f16), T([], f16)), {})\ncnt: 40, ((T([1, 88, 40, 40], f16), T([], f16)), {})\ncnt: 16, ((T([1, 88, 80, 80], f16), T([], f16)), {})\ncnt: 16, ((T([1, 88, 5, 5], f16), T([], f16)), {})\ncnt: 6, ((T([5000], f32), T([5000], f16)), {})\ncnt: 2, ((T([5000], f32, stride=(4,)), T([5000], f16)), {})\ncnt: 1, ((T([5000], f32), T([], f32)), {})\ncnt: 1, ((T([100, 4], f32), T([], f16)), {})\ncnt: 1, ((T([100, 4], f32, stride=(0, 0)), T([], f16)), {})\ncnt: 2, ((T([5000], f32), T([5000], f32)), {})\ncnt: 16, ((T([1, 88, 5, 5], f16), T([1, 88, 5, 5], f16)), {})\ncnt: 40, ((T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16)), {})\ncnt: 40, ((T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16)), {})\ncnt: 40, ((T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16)), {})\ncnt: 16, ((T([1, 88, 80, 80], f16), T([1, 88, 80, 80], f16)), {})\ncnt: 1, ((T([1, 1920, 20, 20], f16), T([1, 1920, 20, 20], f16)), {})\ncnt: 5, ((T([1, 1152, 20, 20], f16), T([1, 1152, 20, 20], f16)), {})\ncnt: 1, ((T([1, 672, 20, 20], f16), T([1, 672, 20, 20], f16)), {})\ncnt: 3, ((T([1, 672, 40, 40], f16), T([1, 672, 40, 40], f16)), {})\ncnt: 4, ((T([1, 480, 40, 40], f16), T([1, 480, 40, 40], f16)), {})\ncnt: 1, ((T([1, 240, 40, 40], f16), T([1, 240, 40, 40], f16)), {})\ncnt: 2, ((T([1, 240, 80, 80], f16), T([1, 240, 80, 80], f16)), {})\ncnt: 1, ((T([1, 144, 80, 80], f16), T([1, 144, 80, 80], f16)), {})\ncnt: 2, ((T([1, 144, 160, 160], f16), T([1, 144, 160, 160], f16)), {})\ncnt: 1, ((T([1, 96, 160, 160], f16), T([1, 96, 160, 160], f16)), {})\ncnt: 1, ((T([1, 16, 320, 320], f16), T([1, 16, 320, 320], f16)), {})\ncnt: 1, ((T([1, 32, 320, 320], f16), T([1, 32, 320, 320], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([1, 32, 320, 320], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 0.001), {})\ncnt: 3, ((T([1, 16, 320, 320], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), False, 0.1, 0.001), {})\ncnt: 1, ((T([1, 96, 320, 320], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 0.001), {})\ncnt: 1, ((T([1, 96, 160, 160], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 0.001), {})\ncnt: 3, ((T([1, 24, 160, 160], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), False, 0.1, 0.001), {})\ncnt: 5, ((T([1, 144, 160, 160], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 0.001), {})\ncnt: 1, ((T([1, 144, 80, 80], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 0.001), {})\ncnt: 3, ((T([1, 40, 80, 80], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f16), False, 0.1, 0.001), {})\ncnt: 5, ((T([1, 240, 80, 80], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.1, 0.001), {})\ncnt: 1, ((T([1, 240, 40, 40], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.1, 0.001), {})\ncnt: 4, ((T([1, 80, 40, 40], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f16), False, 0.1, 0.001), {})\ncnt: 8, ((T([1, 480, 40, 40], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), False, 0.1, 0.001), {})\ncnt: 4, ((T([1, 112, 40, 40], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), False, 0.1, 0.001), {})\ncnt: 7, ((T([1, 672, 40, 40], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), False, 0.1, 0.001), {})\ncnt: 1, ((T([1, 672, 20, 20], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), False, 0.1, 0.001), {})\ncnt: 5, ((T([1, 192, 20, 20], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), False, 0.1, 0.001), {})\ncnt: 10, ((T([1, 1152, 20, 20], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), False, 0.1, 0.001), {})\ncnt: 2, ((T([1, 320, 20, 20], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), False, 0.1, 0.001), {})\ncnt: 2, ((T([1, 1920, 20, 20], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f16), False, 0.1, 0.001), {})\ncnt: 17, ((T([1, 88, 20, 20], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f16), False, 0.01, 0.001), {})\ncnt: 14, ((T([1, 88, 10, 10], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f16), False, 0.01, 0.001), {})\ncnt: 16, ((T([1, 88, 40, 40], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f16), False, 0.01, 0.001), {})\ncnt: 11, ((T([1, 88, 80, 80], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f16), False, 0.01, 0.001), {})\ncnt: 10, ((T([1, 88, 5, 5], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f16), False, 0.01, 0.001), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 10, ((T([1, 88, 5, 5], f16), T([1, 88, 5, 5], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f32), T([88], f32), False, 0.001, [True, True, True]), {})\ncnt: 14, ((T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f32), T([88], f32), False, 0.001, [True, True, True]), {})\ncnt: 17, ((T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f32), T([88], f32), False, 0.001, [True, True, True]), {})\ncnt: 16, ((T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f32), T([88], f32), False, 0.001, [True, True, True]), {})\ncnt: 11, ((T([1, 88, 80, 80], f16), T([1, 88, 80, 80], f16), T([88], f16), T([88], f16), T([88], f16), T([88], f32), T([88], f32), False, 0.001, [True, True, True]), {})\ncnt: 2, ((T([1, 320, 20, 20], f16), T([1, 320, 20, 20], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), False, 0.001, [True, True, True]), {})\ncnt: 2, ((T([1, 1920, 20, 20], f16), T([1, 1920, 20, 20], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f32), T([1920], f32), False, 0.001, [True, True, True]), {})\ncnt: 10, ((T([1, 1152, 20, 20], f16), T([1, 1152, 20, 20], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f32), T([1152], f32), False, 0.001, [True, True, True]), {})\ncnt: 5, ((T([1, 192, 20, 20], f16), T([1, 192, 20, 20], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([1, 672, 20, 20], f16), T([1, 672, 20, 20], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), False, 0.001, [True, True, True]), {})\ncnt: 7, ((T([1, 672, 40, 40], f16), T([1, 672, 40, 40], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), False, 0.001, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Device and Memory Management Utilities (PyTorch CUDA/C++)\nDESCRIPTION: These entries represent device management routines, memory querying, and special CUDA integration points in the PyTorch backend. Symbols like SetTargetDevice, getCurrentCUDABlasLtHandle, and NUMA moves are responsible for managing the active device, handling NUMA configuration, and providing integration with CUDA libraries. Prerequisites include a properly-initialized CUDA environment and correct device properties. Inputs/outputs are typically device or handle types and may affect the global state of PyTorch device settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_18\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104cuda15SetTargetDeviceEv\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at18GetCurrentNUMANodeEv\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c1013IsNUMAEnabledEv\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c1022memoryProfilingEnabledEv\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c108NUMAMoveEPvmi\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c108free_cpuEPv\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4cuda26getCurrentCUDABlasLtHandleEv\n```\n\n----------------------------------------\n\nTITLE: Utilizing aten.silu_ Activation in PyTorch\nDESCRIPTION: The aten.silu_.default is applied in these snippets to use the Swish activation function, suggested as 'silu', on tensors within PyTorchs deep learning framework. Key prerequisites involve proper PyTorch setup. Tensors input for the Swish activation target non-traditional activation purposes. Output retains tensor shape with enhanced activation advantages, ideal for deep learning architectures requiring Swish.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 56, 56], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Map and Filter Operations with Nesting\nDESCRIPTION: Examples of applying map and filter operations with nesting levels on both DataFrame and regular DataPipes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndp = get_dataframes_pipe(range = 30, dataframe_size = 3)\ndp = dp.filter(lambda x: x.i > 5)\ndp = dp.batch(5).filter(lambda x: x.i < 13, nesting_level = 1)\nprint(\"Iterate over DataFrame batches\")\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Accessing Non-Strict Export Reference in PyTorch\nDESCRIPTION: This code snippet demonstrates how to reference the non-strict export functionality in PyTorch documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_16\n\nLANGUAGE: reStructuredText\nCODE:\n```\n:ref:`non-strict export <Non-Strict Export>`\n```\n\n----------------------------------------\n\nTITLE: Defining a Function with Dynamic Control Flow in Python\nDESCRIPTION: This snippet defines a function 'add_two_maybe' that conditionally adds 2 to a tensor based on a boolean input. It demonstrates a case where jit.trace can fail due to dynamic control flow.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef add_two_maybe(t: torch.Tensor, maybe: torch.Tensor):\n    if maybe:\n        return t + 2\n    return t\n```\n\n----------------------------------------\n\nTITLE: Initializing ATen MPS Sources in CMake\nDESCRIPTION: Conditionally executes if `USE_MPS` is true. It includes Metal-specific CMake functions from `../../../cmake/Metal.cmake`. Then, it initializes the `ATen_MPS_SRCS` list variable by appending various MPS-related source and header files (`mps_cpp`, `mps_mm`, `mps_h`, `native_mps_cpp`, `native_mps_mm`, `native_mps_h`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_20\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_MPS)\n    include(../../../cmake/Metal.cmake)\n\n    set(ATen_MPS_SRCS ${ATen_MPS_SRCS} ${mps_cpp} ${mps_mm} ${mps_h} ${native_mps_cpp} ${native_mps_mm} ${native_mps_h})\n```\n\n----------------------------------------\n\nTITLE: Cloning Tensor in PyTorch\nDESCRIPTION: This snippet demonstrates the cloning of a tensor, which is often used to create a copy of the input data for further processing or to preserve the original data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\naten.clone.default(T([64, 3, 224, 224], f16),)\n```\n\n----------------------------------------\n\nTITLE: Building an FX Graph for Tensor Addition in PyTorch (Python)\nDESCRIPTION: This Python snippet contains both a functional example and its corresponding FX intermediate graph. The code defines a function performing tensor addition, while the graph block illustrates how placeholders and call_function nodes are used and returned in FX. This demonstrates basic graph tracing for simple tensor operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef add_one(x):\n  return torch.ops.aten(x, 1)\n```\n\nLANGUAGE: python\nCODE:\n```\ngraph():\n  %ph_0 : [#users=1] = placeholder[target=ph_0]\n  %add_tensor : [#users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%ph_0, 1), kwargs = {})\n  return [add_tensor]\n```\n\n----------------------------------------\n\nTITLE: Dividing Tensors with div Tensor in PyTorch (Python)\nDESCRIPTION: Involves the aten.div operation to divide a tensor by a scalar. This operation is essential for normalizing data or scaling down tensor values, especially necessary in adjusting activation outputs or gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\naten.div.Tensor\ncnt: 24, ((T([16, 12, 512, 512], f16), 8.0), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing CSV Parser DataPipe\nDESCRIPTION: Complete implementation of CSVParserIterDataPipe that processes CSV file streams and yields rows. Shows how to handle file streams and use external libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@functional_datapipe(\"parse_csv_files\")\nclass CSVParserIterDataPipe(IterDataPipe):\n    def __init__(self, dp, **fmtparams):\n        self.dp = dp\n        self.fmtparams = fmtparams\n\n    def __iter__(self):\n        for filename, stream in self.dp:\n            reader = csv.reader(stream, **self.fmtparams)\n            for row in reader:\n                yield filename, row\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations with Shape Information\nDESCRIPTION: Collection of PyTorch tensor operations showing operator type, count, tensor shapes, and parameters. Operations include batch normalization, leaky ReLU, convolutions, and loss calculations. All operations use half precision (f16) tensors with various dimensions and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cspdarknet53_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Batch normalization operation examples\ncnt: 1, ((T([64, 32, 256, 256], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\n\n# Leaky ReLU operation examples\ncnt: 1, ((T([64, 32, 256, 256], f16),), {})\n\n# Matrix multiplication examples\ncnt: 1, ((T([64, 1000], f16), T([1000, 1024], f16)), {})\n\n# Loss calculation examples\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for PyTorch NNAPI\nDESCRIPTION: This snippet demonstrates the setup of a CMake project either as a standalone build for PyTorch NNAPI or as part of the PyTorch tree. It includes setting the C++ standard, finding the Torch package, and compiling source files into a shared library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/nnapi/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Define this to build the NNAPI binding out of tree.\nif(PYTORCH_NNAPI_STANDALONE)\n  cmake_minimum_required(VERSION 3.5 FATAL_ERROR)\n  project(pytorch_nnapi)\n\n  set(CMAKE_CXX_STANDARD 14 CACHE STRING \"The C++ standard whose features are requested to build this target.\")\n  find_package(Torch REQUIRED)\n\n  set(NNAPI_SRCS\n    nnapi_bind.cpp\n    nnapi_wrapper.cpp\n    nnapi_model_loader.cpp\n    )\n\n  add_library(pytorch_nnapi SHARED ${NNAPI_SRCS})\n  target_link_libraries(pytorch_nnapi torch)\nelse()\n  # Building within the PyTorch tree.\n  file(GLOB ATen_NNAPI_SRCS \"*.cpp\")\n  set(ATen_NNAPI_SRCS ${ATen_NNAPI_SRCS} PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Backward Pass for Tanh Activation in PyTorch (Python)\nDESCRIPTION: Computes the derivatives for the tanh function during backpropagation with aten.tanh_backward, crucial for adjusting weights based on the tanh activation response during training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\naten.tanh_backward.default\ncnt: 1, ((T([16, 768], f16), T([16, 768], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Computing Sums Along an Axis with aten.sum in Python\nDESCRIPTION: The aten.sum.SymInt operator calculates the sum of elements along a specified axis, while preserving dimensions and supporting integer output size. This operation allows for aggregation across axes, maintaining broadcastable shapes due to the 'keepdims=True' setting, typically summing elements along the first axis (axis 0).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([256, 197951], f16), [0], True), {})\ncnt: 4, ((T([256, 512], f16), [0], True), {})\ncnt: 1, ((T([256, 1024], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: TorchScript-based ONNX Export Memory Analysis\nDESCRIPTION: Script demonstrating memory usage tracking for TorchScript-based ONNX export of a HighResNet model. Records memory allocation history and generates a snapshot for visualization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo_memory_usage.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom monai.networks.nets import (\n    HighResNet,\n)\n\ntorch.cuda.memory._record_memory_history()\n\nmodel = HighResNet(\n    spatial_dims=3, in_channels=1, out_channels=3, norm_type=\"batch\"\n).eval()\n\nmodel = model.to(\"cuda\")\ndata = torch.randn(30, 1, 48, 48, 48, dtype=torch.float32).to(\"cuda\")\n\nwith torch.no_grad():\n    onnx_program = torch.onnx.export(\n        model,\n        data,\n        \"torchscript_exporter_highresnet.onnx\",\n        dynamo=False,\n    )\n\nsnapshot_name = \"torchscript_exporter_example.pickle\"\nprint(f\"generate {snapshot_name}\")\n\ntorch.cuda.memory._dump_snapshot(snapshot_name)\nprint(\"Export is done.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Backend for QAT Module Swap\nDESCRIPTION: This code snippet demonstrates how to configure the backend for QAT module swapping. It sets up the BackendPatternConfig to specify the QAT module corresponding to the fused LinearReLU module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nBackendPatternConfig(nni.LinearReLU)\n    .set_qat_module(nniqat.LinearReLU)\n```\n\n----------------------------------------\n\nTITLE: Converting a Dense Tensor to CSC Format in PyTorch\nDESCRIPTION: Demonstrates how to convert a dense tensor to a sparse CSC tensor using the to_sparse_csc() method. The example converts a 3x4 dense tensor into CSC format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\na = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)\nsp = a.to_sparse_csc()\nsp\n```\n\n----------------------------------------\n\nTITLE: Wirtinger Derivative Chain Rule\nDESCRIPTION: Chain rule expressions relating partial derivatives with respect to real and imaginary components to Wirtinger derivatives.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_5\n\nLANGUAGE: math\nCODE:\n```\n\\begin{aligned}\n    \\frac{\\partial }{\\partial z} &= 1/2 * \\left(\\frac{\\partial }{\\partial x} - 1j * \\frac{\\partial }{\\partial y}\\right)   \\\\\n    \\frac{\\partial }{\\partial z^*} &= 1/2 * \\left(\\frac{\\partial }{\\partial x} + 1j * \\frac{\\partial }{\\partial y}\\right)\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Tensor Copy Operations in DenseNet Input Processing\nDESCRIPTION: This snippet shows the tensor copy operation used for input processing in DenseNet. It copies the input tensor from one storage location to another while preserving the dimensions [32, 3, 224, 224] and half-precision (f16) data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Practical Example Using Minifier\nDESCRIPTION: Complete example demonstrating how to use the minifier with a failing function that contains a multiplication operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx as fx\nfrom functorch.compile import minifier\n\ndef failing_f(x, y):\n    y = torch.ops.aten.div(x, y)\n    x = torch.ops.aten.add(x, 3)\n    x = torch.ops.aten.mul(x, y)\n    return torch.ops.aten.sub(x, y)\n\ninps = [torch.randn(3), torch.randn(3)]\n\ndef pass_checker(fx_g, inps):\n    return (torch.ops.aten.mul in {i.target for i in fx_g.graph.nodes})\n\nmin_f, inps = minifier(fx.symbolic_trace(failing_f), inps, pass_checker)\n```\n\n----------------------------------------\n\nTITLE: Importing jacrev and jacfwd for Reverse and Forward Mode - Python\nDESCRIPTION: Imports functorch.jacrev and jacfwd, exposing both the reverse-mode and forward-mode Jacobian transform APIs. These are used for later function and performance comparisons. No input/output except module imports.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import jacrev, jacfwd\n```\n\n----------------------------------------\n\nTITLE: Alternative Ways to Specify Devices in PyTorch Functions\nDESCRIPTION: This code snippet demonstrates different ways to specify devices when creating tensors, including using torch.device objects, device strings, and legacy integer device ordinals, which are all equivalent approaches.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> # Example of a function that takes in a torch.device\n>>> cuda1 = torch.device('cuda:1')\n>>> torch.randn((2,3), device=cuda1)\n\n>>> # You can substitute the torch.device with a string\n>>> torch.randn((2,3), device='cuda:1')\n```\n\n----------------------------------------\n\nTITLE: Correct Use of Tensor.new_zeros for batched initialization - PyTorch - Python\nDESCRIPTION: This snippet demonstrates the preferred approach for batched tensor creation inside vmap-compatible functions. diag_embed uses vec.new_zeros to create a tensor aligned with batching, enabling correct in-place operations. Dependencies: PyTorch, torch.func. Inputs and outputs are batched tensors; method can be generalized for any similar batched-tensor factory needs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef diag_embed(vec):\n  assert vec.dim() == 1\n  result = vec.new_zeros(vec.shape[0], vec.shape[0])\n  result.diagonal().copy_(vec)\n  return result\n\nvecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\nvmap(diag_embed)(vecs)\n```\n\n----------------------------------------\n\nTITLE: Complete EMA Training Loop Implementation\nDESCRIPTION: Full example of implementing EMA training including model creation and batch norm updates.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nloader, optimizer, model, loss_fn = ...\nema_model = torch.optim.swa_utils.AveragedModel(model, \\\n            multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n\nfor epoch in range(300):\n      for input, target in loader:\n          optimizer.zero_grad()\n          loss_fn(model(input), target).backward()\n          optimizer.step()\n          ema_model.update_parameters(model)\n\n# Update bn statistics for the ema_model at the end\ntorch.optim.swa_utils.update_bn(loader, ema_model)\n# Use ema_model to make predictions on test data\npreds = ema_model(test_input)\n```\n\n----------------------------------------\n\nTITLE: Disabling Tensor Device Consistency Checks in PyTorch Kernels (native_functions.yaml)\nDESCRIPTION: Shows the `device_check: NoCheck` setting within `native_functions.yaml`. This option disables the default behavior of ATen's code generation, which checks if all input tensor arguments reside on the same device. It's used for kernels specifically designed to operate on tensors across multiple devices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ndevice_check: NoCheck\n```\n\n----------------------------------------\n\nTITLE: Automodule Documentation for torch.distributed.optim\nDESCRIPTION: Generates documentation for the torch.distributed.optim module, including DistributedOptimizer, PostLocalSGDOptimizer, and ZeroRedundancyOptimizer classes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.optim.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: torch.distributed.optim\n    :members: DistributedOptimizer, PostLocalSGDOptimizer, ZeroRedundancyOptimizer\n```\n\n----------------------------------------\n\nTITLE: Custom Float32 Autograd Function with AMP\nDESCRIPTION: Implementation of a custom autograd function that requires float32 inputs, using custom_fwd and custom_bwd decorators with specific dtype requirements.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/amp_examples.rst#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nclass MyFloat32Func(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda', cast_inputs=torch.float32)\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        ...\n        return fwd_output\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch CUDA Module\nDESCRIPTION: This code snippet shows how to import the torch.cuda module, which provides CUDA-related functionality in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch.cuda\n```\n\n----------------------------------------\n\nTITLE: Implementing Construct-time Type Checking in Python DataPipes\nDESCRIPTION: Demonstrates how to use the @argument_validation decorator for construct-time type checking in DataPipe classes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import argument_validation\n\nclass DP(IterDataPipe):\n    @argument_validation\n    def __init__(self, dp: IterDataPipe[Union[int, tuple]]):\n        self.dp = dp\n\n    def __iter__(self):\n        for d in self.dp:\n            yield d\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP(range(10))\n```\n\nLANGUAGE: python\nCODE:\n```\nclass Temp(IterDataPipe[str]):\n    def __iter__(self):\n        pass\ndp = DP(Temp())\n```\n\nLANGUAGE: python\nCODE:\n```\nclass Temp(IterDataPipe[Tuple[int, T_co]]):\n    def __iter__(self):\n        pass\ndp = DP(Temp())\n```\n\n----------------------------------------\n\nTITLE: Configuring BatchNorm2d Without Running Stats in PyTorch\nDESCRIPTION: Creates a BatchNorm2d layer with track_running_stats set to False, preventing the module from using running mean and variance statistics that cause issues with functorch's vmap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/batch_norm.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nBatchNorm2d(64, track_running_stats=False)\n```\n\n----------------------------------------\n\nTITLE: Debugging PyTorch with LLDB After Adding Debug Info\nDESCRIPTION: Example showing the improved debugging experience after rebuilding a file with debug symbols. The LLDB output now provides source code context and variable information.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_35\n\nLANGUAGE: bash\nCODE:\n```\n% lldb -o \"b applySelect\" -o \"process launch\" -- python3 -c \"import torch;print(torch.rand(5)[3])\"\n(lldb) target create \"python\"\nCurrent executable set to '/usr/bin/python3' (arm64).\n(lldb) settings set -- target.run-args  \"-c\" \"import torch;print(torch.rand(5)[3])\"\n(lldb) b applySelect\nBreakpoint 1: no locations (pending).\nWARNING:  Unable to resolve breakpoint to any actual locations.\n(lldb) process launch\n2 locations added to breakpoint 1\nProcess 87741 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x00000001024e2628 libtorch_python.dylib`at::indexing::impl::applySelect(self=0x00000001004ee8a8, dim=0, index=(data_ = 3), real_dim=0, (null)=0x000000016fdfe535, self_sizes= Has Value=true ) at TensorIndexing.h:239:7\n   236         const at::Device& /*self_device*/,\n   237         const std::optional<SymIntArrayRef>& self_sizes) {\n   238       // See NOTE [nested tensor size for indexing]\n-> 239       if (self_sizes.has_value()) {\n   240         auto maybe_index = index.maybe_as_int();\n   241         if (maybe_index.has_value()) {\n   242           TORCH_CHECK_INDEX(\nTarget 0: (python) stopped.\nProcess 87741 launched: '/usr/bin/python3' (arm64)\n```\n\n----------------------------------------\n\nTITLE: GELU Activation Computation in PyTorch (Python)\nDESCRIPTION: Involves the computation of the Gaussian Error Linear Unit (GELU) activation function using aten.gelu, preferred in transformer models for its better performance in comparison to other activations like ReLU.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\naten.gelu.default\ncnt: 12, ((T([16, 512, 3072], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Forward Convolution Operations in PyTorch\nDESCRIPTION: Multiple forward convolution operations with different tensor shapes, kernel sizes, and parameters. These operations are likely part of a neural network model, possibly a variant of MobileNet or EfficientNet.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 672, 1, 1], f16), T([168, 672, 1, 1], f16), T([168], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring and Installing ATenConfig.cmake in CMake\nDESCRIPTION: Defines the installation include directory path in the `ATEN_INCLUDE_DIR` variable. Uses `configure_file` to generate `ATenConfig.cmake` from the template `ATenConfig.cmake.in` in the build directory's `cmake-exports` subdirectory. Finally, it installs the generated `ATenConfig.cmake` file to the CMake package share directory for ATen (`${AT_INSTALL_SHARE_DIR}/cmake/ATen`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\nset(ATEN_INCLUDE_DIR \"${CMAKE_INSTALL_PREFIX}/${AT_INSTALL_INCLUDE_DIR}\")\nconfigure_file(ATenConfig.cmake.in \"${CMAKE_CURRENT_BINARY_DIR}/cmake-exports/ATenConfig.cmake\")\ninstall(FILES \"${CMAKE_CURRENT_BINARY_DIR}/cmake-exports/ATenConfig.cmake\"\n  DESTINATION \"${AT_INSTALL_SHARE_DIR}/cmake/ATen\")\n```\n\n----------------------------------------\n\nTITLE: Defining If Statements in TorchScript\nDESCRIPTION: Specifies the syntax for if statements in TorchScript, including both basic if/else and ternary if/else forms. Shows how tensors are handled in conditionals.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nif_stmt ::= \"if\" assignment_expression \":\" suite\n            (\"elif\" assignment_expression \":\" suite)\n            [\"else\" \":\" suite]\n\nif_stmt ::= return [expression_list] \"if\" assignment_expression \"else\" [expression_list]\n```\n\n----------------------------------------\n\nTITLE: Type Isolation in Package Importing\nDESCRIPTION: Example showing how classes imported from a PackageImporter are specific to that importer, illustrating the isolation of types between packages and the loading environment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom foo import MyClass\n\nmy_class_instance = MyClass()\n\nwith PackageExporter(f) as exporter:\n    exporter.save_module(\"foo\")\n\nimporter = PackageImporter(f)\n```\n\n----------------------------------------\n\nTITLE: Specialized Python Primitives in Exported PyTorch Program\nDESCRIPTION: The exported program showing how Python primitives are specialized. The loop has been unrolled with the constant value of 1 hard-coded into each operation, and the original inputs 'const' and 'times' are no longer used.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[2, 2]\", const, times):\n            # code: x = x + const\n            add: \"f32[2, 2]\" = torch.ops.aten.add.Tensor(x, 1)\n            add_1: \"f32[2, 2]\" = torch.ops.aten.add.Tensor(add, 1)\n            add_2: \"f32[2, 2]\" = torch.ops.aten.add.Tensor(add_1, 1)\n            return (add_2,)\n```\n\n----------------------------------------\n\nTITLE: Using Custom CUDA Memory Allocator in PyTorch\nDESCRIPTION: This Python code demonstrates how to load and use a custom CUDA memory allocator in PyTorch using the CUDAPluggableAllocator class. It shows how to swap the current allocator and allocate memory using the new allocator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\n\n# Load the allocator\nnew_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n    'alloc.so', 'my_malloc', 'my_free')\n# Swap the current allocator\ntorch.cuda.memory.change_current_allocator(new_alloc)\n# This will allocate memory in the device using the new allocator\nb = torch.zeros(10, device='cuda')\n```\n\n----------------------------------------\n\nTITLE: Project Definition and Platform Detection\nDESCRIPTION: Sets up the Torch project, defines Linux platform detection, and configures C++17 and C11 standards for compilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nproject(Torch CXX C)\n\nif(${CMAKE_SYSTEM_NAME} STREQUAL \"Linux\")\n  set(LINUX TRUE)\nelse()\n  set(LINUX FALSE)\nendif()\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_C_STANDARD 11)\n```\n\n----------------------------------------\n\nTITLE: Documenting LSTM Optimization Conditions in reStructuredText\nDESCRIPTION: This snippet outlines the conditions required for selecting a persistent algorithm to improve LSTM performance in PyTorch. It specifies requirements related to cuDNN, GPU usage, data type, hardware, and input format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cudnn_persistent_rnn.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. note::\n\n    If the following conditions are satisfied:\n    1) cudnn is enabled,\n    2) input data is on the GPU\n    3) input data has dtype ``torch.float16``\n    4) V100 GPU is used,\n    5) input data is not in ``PackedSequence`` format\n    persistent algorithm can be selected to improve performance.\n```\n\n----------------------------------------\n\nTITLE: Function Decoration in TorchScript\nDESCRIPTION: Shows different ways to decorate functions in TorchScript, including making functions ignored, unused, or explicitly exported.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Same behavior as pre-PyTorch 1.2\n@torch.jit.script\ndef some_fn():\n    return 2\n\n# Marks a function as ignored, if nothing\n# ever calls it then this has no effect\n@torch.jit.ignore\ndef some_fn2():\n    return 2\n\n# As with ignore, if nothing calls it then it has no effect.\n# If it is called in script it is replaced with an exception.\n@torch.jit.unused\ndef some_fn3():\n  import pdb; pdb.set_trace()\n  return 4\n\n# Doesn't do anything, this function is already\n# the main entry point\n@torch.jit.export\ndef some_fn4():\n    return 2\n```\n\n----------------------------------------\n\nTITLE: Implementing aten.sigmoid Operator in PyTorch\nDESCRIPTION: This snippet captures the execution of the aten.sigmoid.default operator in PyTorch, applying the sigmoid activation function on input tensors. Dependencies are essential from PyTorch's library. Inputs involve tensors that need a sigmoid transformation, and outputs are the tensors converted to the probability space between 0 and 1. It is well-suited for binary classification models and expects float inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Replacing Add with Mul and Sub Using PyTorch FX Transformer\nDESCRIPTION: This transformer replaces a single add operation with a multiplication followed by a subtraction in a PyTorch FX graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass ReplaceAddWithMulSub(torch.fx.Transformer):\n    \"\"\"\n    Original:\n        def f(x, y):\n            return x + y\n\n    After pass:\n        def f(x, y):\n            z = x * y\n            return z - y\n    \"\"\"\n    def call_function(self, target, args, kwargs):\n        if target != torch.ops.aten.add.Tensor:\n            return super().call_function(target, args, kwargs)\n\n        x, y = args\n\n        mul_res = super().call_function(torch.ops.aten.mul.Tensor, args, {})\n        return super().call_function(torch.ops.aten.sub.Tensor, (mul_res, y), {})\n\ntransformed_graph_module = ReplaceAddWithMulSub(graph_module).transform()\n```\n\n----------------------------------------\n\nTITLE: Compiling torch.vmap with torch.compile\nDESCRIPTION: Shows how to use torch.compile with torch.vmap for vectorized operations on tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef my_fn(x):\n    return torch.vmap(lambda x: x.sum(1))(x)\n\nx = torch.randn(3, 3, 3)\noutput = torch.compile(my_fn)(x)\n```\n\n----------------------------------------\n\nTITLE: Functional Stage Runtime Construction\nDESCRIPTION: This code demonstrates using the functional version of the build_stage API to create a distributed stage runtime after modifications. It requires importing necessary modules and a distributed ProcessGroup, with outputs including a stage runtime.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.pipelining import build_stage\nfrom torch.nn.parallel import DistributedDataParallel\n\ndp_mod = DistributedDataParallel(stage_mod)\ninfo = pipe.info()\nstage = build_stage(dp_mod, stage_idx, info, device, group)\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple PyTorch Operators for Benchmarking\nDESCRIPTION: This snippet demonstrates how to configure multiple PyTorch operators (abs and acos) for benchmarking using op_bench.config_list and op_bench.op_list.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport operator_benchmark as op_bench\nimport torch\n\nunary_ops_configs = op_bench.config_list(\n    attrs=[\n        [128, 128],\n        [256, 256],\n        [1024, 1024],\n    ],\n    attr_names=[\"M\", \"N\"],\n    tags=[\"short\"]\n)\n\nunary_ops_list = op_bench.op_list(\n    attr_names=[\"op_name\", \"op_func\"],\n    attrs=[\n        [\"abs\", torch.abs],\n        [\"acos\", torch.acos],\n    ],\n)\n```\n\n----------------------------------------\n\nTITLE: Using Type-Annotated Empty Lists and Dicts in TorchScript Modules - PyTorch - Python\nDESCRIPTION: This example shows how to create empty list and dict objects with explicit type annotations in a TorchScript-compatible nn.Module. It uses Python 3 type annotations and the typing module to declare a module that generates a list of (int, float) tuples and a dict mapping string to int in its forward method. The code highlights the need for type hints to disambiguate empty data structures. Requires torch, torch.nn, and typing modules.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, List, Tuple\n\nclass EmptyDataStructures(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -> Tuple[List[Tuple[int, float]], Dict[str, int]]:\n        # This annotates the list to be a `List[Tuple[int, float]]`\n        my_list: List[Tuple[int, float]] = []\n        for i in range(10):\n            my_list.append((i, x.item()))\n\n        my_dict: Dict[str, int] = {}\n        return my_list, my_dict\n\nx = torch.jit.script(EmptyDataStructures())\n```\n\n----------------------------------------\n\nTITLE: Preparing Data and Computing Jacobian Using compute_jac - Python\nDESCRIPTION: Clones and sets requirement for gradient on input vector xp, defines unit vectors as identity matrix, and computes Jacobian using 'compute_jac'. Prints shape and first row of the resulting Jacobian for inspection. Dependencies: torch, predict, compute_jac, weight, bias. Outputs: Jacobian matrix and sample row.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nxp = x.clone().requires_grad_()\nunit_vectors = torch.eye(D)\n\njacobian = compute_jac(xp)\n\nprint(jacobian.shape)\nprint(jacobian[0])  # show first row\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Spawn Tests in Python\nDESCRIPTION: Command to run the distributed spawn test suite, which includes tests for Distributed Data Parallel, using Python's test runner.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython test/run_test.py --verbose -i distributed/test_distributed_spawn\n```\n\n----------------------------------------\n\nTITLE: Cataloging Convolution-related ATen Operator Calls - PyTorch - Python\nDESCRIPTION: This snippet captures frequency and signatures of various PyTorch convolution-related operator calls, where each tuple encodes argument types, tensor shapes, datatypes (notably half precision), paddings, strides, groups, and flags. Dependencies assume PyTorch and knowledge of its ATen operator signature conventions. Inputs include tuples of tensors with precise shapes and config lists, and outputs are not included; the focus is on argument usage pattern enumeration, not on computing results.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 256, 56, 56], f16), T([128, 64, 56, 56], f16), T([256, 64, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), T([64, 64, 3, 3], f16), [64], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 128, 56, 56], f16), T([64, 128, 1, 1], f16), [64], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 56, 56], f16), T([128, 128, 56, 56], f16), T([256, 128, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 128, 56, 56], f16), T([128, 64, 112, 112], f16), T([128, 64, 3, 3], f16), [128], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([128, 32, 112, 112], f16), T([64, 32, 3, 3], f16), [64], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 16, 112, 112], f16), T([32, 16, 3, 3], f16), [32], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), [16], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication and Addition in PyTorch - Python\nDESCRIPTION: This snippet demonstrates using aten.addmm.default to perform matrix multiplication followed by an addition. Prerequisites include compatible matrix dimensions. Enhances performance on large-scale linear operations common in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1280], f16), T([32, 960], f16), T([960, 1280], f16, stride=(1, 960))), {})\ncnt: 1, ((T([1000], f16), T([32, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})\n\n```\n\n----------------------------------------\n\nTITLE: Working with Data-Dependent Operations in torch.compile\nDESCRIPTION: Demonstrates how to configure torch.compile to handle data-dependent operations on nested jagged tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> def f(nt): return nt.chunk(2, dim=0)[0]\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output = compiled_f(nt)\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> torch._dynamo.config.capture_dynamic_output_shape_ops = True\n>>> torch._dynamo.config.capture_scalar_outputs = True\n```\n\n----------------------------------------\n\nTITLE: Registering Hook After In-place Tensor Modification in PyTorch\nDESCRIPTION: Demonstrates the proper way to register a hook after performing an in-place modification on a tensor. The hook will receive gradients with respect to the modified tensor value rather than the original value.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nt = torch.tensor(1., requires_grad=True).sin()\nt.cos_()\nt.register_hook(fn)\nt.backward()\n```\n\n----------------------------------------\n\nTITLE: Appending and Setting Caffe2 CPU Source Files in CMake\nDESCRIPTION: This CMake snippet appends 'common.cc' to the Caffe2_CPU_SRCS list and sets the updated list in the parent scope. It's part of the build configuration for PyTorch's Caffe2 CPU components.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/core/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND Caffe2_CPU_SRCS\n  \"${CMAKE_CURRENT_SOURCE_DIR}/common.cc\"\n)\nset(Caffe2_CPU_SRCS ${Caffe2_CPU_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Enumerating Composite Tensor Input/Weight Signatures for Miscellaneous PyTorch Operators - Python\nDESCRIPTION: This block lists composite argument tuples frequently representing PyTorch operator invocations with multiple input, weight, bias, and parameter tensor specifications (sizes and dtypes). The used notation suggests these are from automated operator signature logging, likely for convolution layers, normalization, or linear layers, involving batch features and parameter tensors in half and single precision. Each entry is a mapping from count and parameter tuple to an (optional) configuration dict. Inputs mix tensors and Python scalars or lists, so this is intended for test suite, log, or analysis datanot as executable code. Limitations: Operator names are omitted in these blocks; usage context is inferred from signature patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 75, 1, 1], f16), T([128, 75, 1, 1], f16), T([75], f16), T([75], f16), T([75], f16), T([75], f32), T([75], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 906, 7, 7], f16), T([128, 906, 7, 7], f16), T([906], f16), T([906], f16), T([906], f16), T([906], f32), T([906], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 151, 7, 7], f16), T([128, 151, 7, 7], f16), T([151], f16), T([151], f16), T([151], f16), T([151], f32), T([151], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 70, 1, 1], f16), T([128, 70, 1, 1], f16), T([70], f16), T([70], f16), T([70], f16), T([70], f32), T([70], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 840, 7, 7], f16), T([128, 840, 7, 7], f16), T([840], f16), T([840], f16), T([840], f16), T([840], f32), T([840], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 140, 7, 7], f16), T([128, 140, 7, 7], f16), T([140], f16), T([140], f16), T([140], f16), T([140], f32), T([140], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 64, 1, 1], f16), T([128, 64, 1, 1], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 768, 7, 7], f16), T([128, 768, 7, 7], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f32), T([768], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 768, 14, 14], f16), T([128, 768, 14, 14], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f32), T([768], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 128, 14, 14], f16), T([128, 128, 14, 14], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoHeuristic Features\nDESCRIPTION: Example of adding numerical and categorical features to AHContext for heuristic decision making. Shows both direct feature addition and feature augmentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncontext = AHContext()\n\n# adding numerical features\ncontext.add_feature(\"m\", mat1.shape[0])\ncontext.add_feature(\"k\", mat1.shape[1])\n\n# adding a categorical feture\ncontext.add_feature(\"mat1_dtype\", mat1.dtype, is_categorical=True)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations\nDESCRIPTION: Collection of convolution operations with various tensor shapes and parameters using half-precision (f16) floating point format. Operations include forward and backward convolutions with different stride and padding configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 1024, 74, 76], f16), T([256, 1024, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Exporting a Model using torch.cond\nDESCRIPTION: Shows how to export a model that uses torch.cond for further transformations and deployment, including dynamic shape handling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ninp = torch.randn(4, 3)\ndim_batch = torch.export.Dim(\"batch\", min=2)\nep = torch.export.export(DynamicShapeCondPredicate(), (inp,), {}, dynamic_shapes={\"x\": {0: dim_batch}})\nprint(ep)\n```\n\n----------------------------------------\n\nTITLE: Defining BatchNorm Backward Test Cases in PyTorch - Python\nDESCRIPTION: This snippet enumerates input descriptions for the backward pass of the PyTorch batch normalization operator, following a similar tuple notation to the forward test cases. Each case provides a tuple containing the input tensors (including gradients), affine parameters, running statistics, and flags required for executing and testing the backward computation. The final boolean list controls which gradients are computed. Required for full operator coverage in automated test frameworks. Inputs include (input, grad_output, weight, running_mean, running_var, save_mean, save_invstd, training, eps, [flags for gradients]), and outputs are the corresponding input and parameter gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([64, 1024, 7, 7], f16), T([64, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\ncnt: 16, ((T([64, 128, 7, 7], f16), T([64, 128, 7, 7], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 992, 7, 7], f16), T([64, 992, 7, 7], f16), T([992], f16), T([992], f16), T([992], f16), T([992], f32), T([992], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 960, 7, 7], f16), T([64, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f32), T([960], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 928, 7, 7], f16), T([64, 928, 7, 7], f16), T([928], f16), T([928], f16), T([928], f16), T([928], f32), T([928], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 896, 7, 7], f16), T([64, 896, 7, 7], f16), T([896], f16), T([896], f16), T([896], f16), T([896], f32), T([896], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 864, 7, 7], f16), T([64, 864, 7, 7], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f32), T([864], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 832, 7, 7], f16), T([64, 832, 7, 7], f16), T([832], f16), T([832], f16), T([832], f16), T([832], f32), T([832], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 800, 7, 7], f16), T([64, 800, 7, 7], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f32), T([800], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 768, 7, 7], f16), T([64, 768, 7, 7], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f32), T([768], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 736, 7, 7], f16), T([64, 736, 7, 7], f16), T([736], f16), T([736], f16), T([736], f16), T([736], f32), T([736], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 704, 7, 7], f16), T([64, 704, 7, 7], f16), T([704], f16), T([704], f16), T([704], f16), T([704], f32), T([704], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 672, 7, 7], f16), T([64, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 640, 7, 7], f16), T([64, 640, 7, 7], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f32), T([640], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 608, 7, 7], f16), T([64, 608, 7, 7], f16), T([608], f16), T([608], f16), T([608], f16), T([608], f32), T([608], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 576, 7, 7], f16), T([64, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 544, 7, 7], f16), T([64, 544, 7, 7], f16), T([544], f16), T([544], f16), T([544], f16), T([544], f32), T([544], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 512, 7, 7], f16), T([64, 512, 7, 7], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Max Pooling Operations\nDESCRIPTION: Max pooling operations with different kernel sizes (5x5, 9x9, 13x13) and corresponding padding and stride configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n((T([8, 512, 12, 16], f16), [5, 5], [1, 1], [2, 2]), {})\n```\n\n----------------------------------------\n\nTITLE: Describing aten._log_softmax.default Usage - PyTorch - Python\nDESCRIPTION: Shows an invocation of the _log_softmax ATen operator on a [64, 1000] half-precision (f16) tensor, typically used as the final activation for classification logits in neural networks. The call also indicates usage of dimension 1 without keeping dimension, and maps to the computation \\nlog(softmax(x))\\n for efficient numerical stability. Input must be a tensor of specified shape and type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Building C10 CUDA Library\nDESCRIPTION: Creates the C10 CUDA library target with appropriate compilation options and dependencies. This section sets up the library build process, including visibility settings and linking to dependencies like C10 and CUDA runtime.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/cuda/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_LIBTORCHLESS)\n  torch_cuda_based_add_library(c10_cuda ${C10_CUDA_SRCS} ${C10_CUDA_HEADERS})\n  torch_compile_options(c10_cuda)\n  set(CUDA_LINK_LIBRARIES_KEYWORD)\n  # If building shared library, set dllimport/dllexport proper.\n  target_compile_options(c10_cuda PRIVATE \"-DC10_CUDA_BUILD_MAIN_LIB\")\n  # Enable hidden visibility if compiler supports it.\n  if(${COMPILER_SUPPORTS_HIDDEN_VISIBILITY})\n    target_compile_options(c10_cuda PRIVATE \"-fvisibility=hidden\")\n  endif()\n\n  # ---[ Dependency of c10_cuda\n  target_link_libraries(c10_cuda PUBLIC ${C10_LIB} torch::cudart)\n\n  if(NOT WIN32)\n  target_link_libraries(c10_cuda PRIVATE dl)\n  target_compile_options(c10_cuda PRIVATE \"-DPYTORCH_C10_DRIVER_API_SUPPORTED\")\n  endif()\n\n  target_include_directories(\n      c10_cuda PUBLIC\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/../..>\n      $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}>\n      $<INSTALL_INTERFACE:include>)\n\n# ---[ Installation\n# Note: for now, we will put all export path into one single Caffe2Targets group\n# to deal with the cmake deployment need. Inside the Caffe2Targets set, the\n# individual libraries like libc10.so and libcaffe2.so are still self-contained.\ninstall(TARGETS c10_cuda EXPORT Caffe2Targets DESTINATION lib)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Testing FastRNNs correctness with default settings\nDESCRIPTION: Commands to test the FastRNNs benchmarking scripts for correctness validation on all RNN implementations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/fastrnns/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m fastrnns.test\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Forward vs Reverse Mode Jacobian - Taller Matrix Case - Python\nDESCRIPTION: Tests jacfwd (forward mode) and jacrev (reverse mode) on a case with many more outputs than inputs (tall matrix). Initializes random weights, bias, and input, runs Timer benchmarks for both Jacobian transform calls, and prints timings. Requires functorch, Timer and appropriate shapes for tall matrix. Outputs: timing for each method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nDin = 32\nDout = 2048\nweight = torch.randn(Dout, Din)\n\nbias = torch.randn(Dout)\nx = torch.randn(Din)\n\n# remember the general rule about taller vs wider...here we have a taller matrix:\nprint(weight.shape)\n\nusing_fwd = Timer(stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\nusing_bwd = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\njacfwd_timing = using_fwd.timeit(500)\njacrev_timing = using_bwd.timeit(500)\n\nprint(f'jacfwd time: {jacfwd_timing}')\nprint(f'jacrev time: {jacrev_timing}')\n\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding MAGMA Dependencies for CUDA/HIP/MSVC in CMake\nDESCRIPTION: This block handles MAGMA library dependencies based on backend and platform. If `USE_MAGMA` is true, it adds `torch::magma` to CUDA dependencies (if `USE_CUDA` is true and `BUILD_LAZY_CUDA_LINALG` is false) and HIP dependencies (if `USE_ROCM` is true). On MSVC, if it's a binary build (`$ENV{TH_BINARY_BUILD}`), it also adds BLAS libraries to CUDA dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\n  if(USE_MAGMA)\n    if(USE_CUDA AND NOT BUILD_LAZY_CUDA_LINALG)\n      list(APPEND ATen_CUDA_DEPENDENCY_LIBS torch::magma)\n    endif(USE_CUDA AND NOT BUILD_LAZY_CUDA_LINALG)\n    if(USE_ROCM)\n      list(APPEND ATen_HIP_DEPENDENCY_LIBS torch::magma)\n    endif(USE_ROCM)\n    if(MSVC)\n      if($ENV{TH_BINARY_BUILD})\n        # Do not do this on Linux: see Note [Extra MKL symbols for MAGMA in torch_cpu]\n        # in caffe2/CMakeLists.txt\n        list(APPEND ATen_CUDA_DEPENDENCY_LIBS ${BLAS_LIBRARIES})\n      endif($ENV{TH_BINARY_BUILD})\n    endif(MSVC)\n  endif(USE_MAGMA)\n```\n\n----------------------------------------\n\nTITLE: Tracking NLL Loss Calculations in PyTorch for Classification\nDESCRIPTION: Records function calls for negative log-likelihood loss in forward and backward passes. The operations use a batch size of 128 with 1000 classes, indicating a standard image classification task.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Custom NumPy Take Function with vmap Support\nDESCRIPTION: Implementation of a custom take operation using NumPy with vmap support. Includes forward, backward, setup_context, and partial vmap implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None\n\n    @staticmethod\n    def vmap(info, in_dims, x, ind, ind_inv, dim):\n        x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims\n        logical_dim = x.dim() if x_bdim is None else x_bdim - 1\n        dim = dim if dim >= 0 else dim + logical_dim\n\n        def maybe_expand_bdim_at_front(x, x_bdim):\n```\n\n----------------------------------------\n\nTITLE: Applying Tanh Activation and its Backward using aten.tanh and aten.tanh_backward - Python\nDESCRIPTION: Showcases the use of the aten.tanh.default and aten.tanh_backward.default operators, which perform element-wise tanh activation and propagate gradients through tanh in backward passes. Operates on both high-rank and lower-rank f16 tensors, returning outputs of the same shape. Dependencies: PyTorch and input tensors with compatible dtype and shape.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.tanh.default\ncnt: 12, ((T([2, 1024, 3072], f16),), {})\ncnt: 1, ((T([2, 768], f16),), {})\ncnt: 1, ((T([2, 1024, 768], f16),), {})\nOperator: aten.tanh_backward.default\ncnt: 1, ((T([2, 1024, 768], f16), T([2, 1024, 768], f16)), {})\ncnt: 12, ((T([2, 1024, 3072], f16), T([2, 1024, 3072], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Accessing Symbolic Sizes of TensorImpl in C++\nDESCRIPTION: This function retrieves the symbolic sizes of a TensorImpl object. It's used for handling tensors with dynamic or symbolic dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\n_ZNK3c1010TensorImpl9sym_sizesEv\n```\n\n----------------------------------------\n\nTITLE: Listing ATen Operator Usage Counts and Signatures - PyTorch - Python\nDESCRIPTION: This snippet provides a structured overview of ATen operators invoked within a PyTorch project, including operator names, usage counts, input/output tensor shapes, data types (e.g., f16, f32, i64), and supplementary parameters (such as strides and options). It documents both forward and backward passes for typical neural network components, highlighting the prevalence of operations (like matmul, elementwise arithmetic, embedding, layer normalization) on common tensor shapes. No direct code is present, but the documentation format and content assume Python usage and focus on PyTorch's ATen backend. The content is intended for analysis and profiling, expects prior knowledge of PyTorch ATen operators, and may not be readily executable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MegatronBertForQuestionAnswering_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 2, ((T([8, 128], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 2, ((T([8, 128], f16), T([8, 128], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 24, ((T([8, 16, 128, 128], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 24, ((T([8, 16, 128, 128], f16), T([8, 16, 128, 128], f16), -1, f16), {})\nOperator: aten._to_copy.default\ncnt: 1, ((T([8, 1, 1, 128], f32),), {'dtype': f16})\nOperator: aten._unsafe_view.default\ncnt: 72, ((T([8, 16, 128, 64], f16), [128, 128, 64]), {})\ncnt: 24, ((T([8, 16, 64, 128], f16), [128, 64, 128]), {})\ncnt: 24, ((T([128, 128, 128], f16), [8, 16, 128, 128]), {})\ncnt: 24, ((T([128, 128, 64], f16), [8, 16, 128, 64]), {})\ncnt: 48, ((T([8, 128, 16, 64], f16), [8, 128, 1024]), {})\ncnt: 24, ((T([8, 128, 1024], f16), [1024, 1024]), {})\nOperator: aten.add.Tensor\ncnt: 145, ((T([8, 128, 1024], f16), T([8, 128, 1024], f16)), {})\ncnt: 24, ((T([8, 16, 128, 128], f16), T([8, 1, 1, 128], f16)), {})\ncnt: 1, ((T([], f16), T([], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 1, ((T([8, 128, 1024], f16), T([1, 128, 1024], f16)), {})\nOperator: aten.addmm.default\ncnt: 96, ((T([1024], f16), T([1024, 1024], f16), T([1024, 1024], f16, stride=(1, 1024))), {})\ncnt: 24, ((T([4096], f16), T([1024, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})\ncnt: 24, ((T([1024], f16), T([1024, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})\ncnt: 1, ((T([2], f16), T([1024, 1024], f16), T([1024, 2], f16, stride=(1, 1024))), {})\nOperator: aten.bmm.default\ncnt: 24, ((T([128, 128, 64], f16), T([128, 64, 128], f16)), {})\ncnt: 24, ((T([128, 128, 128], f16), T([128, 128, 64], f16)), {})\ncnt: 24, ((T([128, 128, 128], f16, stride=(16384, 1, 128)), T([128, 128, 64], f16)), {})\ncnt: 24, ((T([128, 128, 64], f16), T([128, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 24, ((T([128, 64, 128], f16, stride=(8192, 1, 64)), T([128, 128, 128], f16)), {})\ncnt: 24, ((T([128, 128, 128], f16), T([128, 128, 64], f16, stride=(8192, 1, 128))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([8, 128, 1], f16), T([8, 128, 1], f16)], 2), {})\nOperator: aten.clamp.default\ncnt: 2, ((T([8], i64), 0, 128), {})\nOperator: aten.clone.default\ncnt: 1, ((T([8, 128], i64),), {})\ncnt: 2, ((T([8], i64),), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([8, 128], i64), T([8, 128], i64)), {})\ncnt: 2, ((T([8], i64), T([8], i64)), {})\nOperator: aten.div.Tensor\ncnt: 48, ((T([8, 16, 128, 128], f16), 8.0), {})\ncnt: 2, ((T([], f16), 2), {})\nOperator: aten.embedding.default\ncnt: 1, ((T([29056, 1024], f16), T([8, 128], i64), 0), {})\ncnt: 1, ((T([2, 1024], f16), T([8, 128], i64)), {})\ncnt: 1, ((T([512, 1024], f16), T([1, 128], i64)), {})\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([1, 128, 1024], f16), T([1, 128], i64), 512, -1, False), {})\ncnt: 1, ((T([8, 128, 1024], f16), T([8, 128], i64), 2, -1, False), {})\ncnt: 1, ((T([8, 128, 1024], f16), T([8, 128], i64), 29056, 0, False), {})\nOperator: aten.gelu.default\ncnt: 24, ((T([8, 128, 4096], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 24, ((T([8, 128, 4096], f16), T([8, 128, 4096], f16)), {})\nOperator: aten.mm.default\ncnt: 1, ((T([1024, 2], f16), T([2, 1024], f16)), {})\ncnt: 1, ((T([2, 1024], f16, stride=(1, 2)), T([1024, 1024], f16)), {})\ncnt: 24, ((T([1024, 1024], f16), T([1024, 4096], f16)), {})\ncnt: 24, ((T([1024, 1024], f16, stride=(1, 1024)), T([1024, 4096], f16)), {})\ncnt: 24, ((T([1024, 4096], f16), T([4096, 1024], f16)), {})\ncnt: 24, ((T([4096, 1024], f16, stride=(1, 4096)), T([1024, 1024], f16)), {})\ncnt: 96, ((T([1024, 1024], f16), T([1024, 1024], f16)), {})\ncnt: 96, ((T([1024, 1024], f16, stride=(1, 1024)), T([1024, 1024], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 1, ((T([8, 1, 1, 128], f16), -65504.0), {})\nOperator: aten.native_layer_norm.default\ncnt: 49, ((T([8, 128, 1024], f16), [1024], T([1024], f16), T([1024], f16), 1e-12), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 49, ((T([8, 128, 1024], f16), T([8, 128, 1024], f16), [1024], T([8, 128, 1], f32), T([8, 128, 1], f32), T([1024], f16), T([1024], f16), [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 2, ((T([], f16), T([8, 128], f16), T([8], i64), None, 1, 128, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 2, ((T([8, 128], f16), T([8], i64), None, 1, 128), {})\nOperator: aten.rsub.Scalar\ncnt: 1, ((T([8, 1, 1, 128], f16), 1.0), {})\nOperator: aten.split.Tensor\ncnt: 1, ((T([8, 128, 2], f16), 1, -1), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([1024, 2], f16), [0], True), {})\ncnt: 120, ((T([1024, 1024], f16), [0], True), {})\ncnt: 24, ((T([1024, 4096], f16), [0], True), {})\ncnt: 1, ((T([8, 128, 1024], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in PyTorch\nDESCRIPTION: Batch normalization forward operations with batch size 128 and varying feature dimensions. Uses half-precision (f16) tensors with momentum 0.1 and epsilon 0.001.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 0.001), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring CLOG Runtime Library Type\nDESCRIPTION: Macro that sets compile options for MSVC runtime library type (shared or static) based on the CLOG_RUNTIME_TYPE option.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nmacro(CLOG_TARGET_RUNTIME_LIBRARY target)\n  if(MSVC AND NOT CLOG_RUNTIME_TYPE STREQUAL \"default\")\n    if(CLOG_RUNTIME_TYPE STREQUAL \"shared\")\n      target_compile_options(${target} PRIVATE\n        \"/MD$<$<CONFIG:Debug>:d>\")\n    elseif(CLOG_RUNTIME_TYPE STREQUAL \"static\")\n      target_compile_options(${target} PRIVATE\n        \"/MT$<$<CONFIG:Debug>:d>\")\n    endif()\n  endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Backward Pass in PyTorch\nDESCRIPTION: This code snippet shows the input tensor shapes and parameters for the native_batch_norm_backward operation in PyTorch. It includes counts of how many times each unique shape combination is used in the backward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 5, ((T([32, 960, 7, 7], f16), T([32, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f32), T([960], f32), False, 0.001, [True, True, True]), {})\ncnt: 3, ((T([32, 160, 7, 7], f16), T([32, 160, 7, 7], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), False, 0.001, [True, True, True]), {})\ncnt: 3, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), False, 0.001, [True, True, True]), {})\ncnt: 2, ((T([32, 112, 14, 14], f16), T([32, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f32), T([112], f32), False, 0.001, [True, True, True]), {})\ncnt: 2, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), False, 0.001, [True, True, True]), {})\ncnt: 4, ((T([32, 80, 14, 14], f16), T([32, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), False, 0.001, [True, True, True]), {})\ncnt: 4, ((T([32, 184, 14, 14], f16), T([32, 184, 14, 14], f16), T([184], f16), T([184], f16), T([184], f16), T([184], f32), T([184], f32), False, 0.001, [True, True, True]), {})\ncnt: 2, ((T([32, 200, 14, 14], f16), T([32, 200, 14, 14], f16), T([200], f16), T([200], f16), T([200], f16), T([200], f32), T([200], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), T([32, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([32, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 0.001, [True, True, True]), {})\ncnt: 3, ((T([32, 40, 28, 28], f16), T([32, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), False, 0.001, [True, True, True]), {})\ncnt: 4, ((T([32, 120, 28, 28], f16), T([32, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f32), T([120], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([32, 72, 28, 28], f16), T([32, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), False, 0.001, [True, True, True]), {})\ncnt: 3, ((T([32, 72, 56, 56], f16), T([32, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), False, 0.001, [True, True, True]), {})\ncnt: 2, ((T([32, 24, 56, 56], f16), T([32, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 0.001, [True, True, True]), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 0.001, [True, True, True]), {})\ncnt: 3, ((T([32, 16, 112, 112], f16), T([32, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), False, 0.001, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Enumerating BatchNorm, Matrix Multiplication, and Reduction Cases - Python\nDESCRIPTION: This bundle includes sample arguments for N-dimensional batch normalization, matrix multiplication (mm), and reduction operations (mean, mul) typical in deep network pipelines. Each tuple provides tensor shapes meant for functionally significant bottlenecks in training and inference, with batchnorm requiring parallel scale/bias/mean/variance vectors of size matching the normalized axis. Supported operations utilize scalar or per-element broadcasting and reduce dimensions as needed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 3, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 1280], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 72, 28, 28], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Enabling Logging for Fusion Pass in Python\nDESCRIPTION: This command enables logging for the fusion pass in PyTorch, useful for verifying if fusion is occurring in a script model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\nPYTORCH_JIT_LOG_LEVEL=\"graph_fuser\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Package Contents from Packaged Code\nDESCRIPTION: Shows how to use the importlib.resources API and the torch_package_importer to access resources and pickled objects from within packaged code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# foo.py:\nimport importlib.resources\nimport my_resource\n\n# returns \"hello world!\"\ndef get_my_resource():\n    return importlib.resources.read_text(my_resource, \"a.txt\")\n\n# bar.py:\nimport torch_package_importer # this is the PackageImporter that imported this module.\n\n# Prints \"hello world!\", equivalent to importlib.resources.read_text\ndef get_my_resource():\n    return torch_package_importer.load_text(\"my_resource\", \"a.txt\")\n\n# You also do things that the importlib.resources API does not support, like loading\n# a pickled object from the package.\ndef get_my_pickle():\n    return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")\n```\n\n----------------------------------------\n\nTITLE: Graph Representation of LSTM Cell in PyTorch\nDESCRIPTION: An example of a graph representation for an LSTM cell operation, showing the internal graph format that PyTorch uses before code generation. The graph represents tensor operations with explicit data flow between operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_20\n\nLANGUAGE: graph\nCODE:\n```\ngraph(%x : Tensor,\n      %hx : Tensor,\n      %cx : Tensor,\n      %w_ih : Tensor,\n      %w_hh : Tensor,\n      %b_ih : Tensor,\n      %b_hh : Tensor):\n  %7 : int = prim::Constant[value=4]()\n  %8 : int = prim::Constant[value=1]()\n  %9 : Tensor = aten::t(%w_ih)\n  %10 : Tensor = aten::mm(%x, %9)\n  %11 : Tensor = aten::t(%w_hh)\n  %12 : Tensor = aten::mm(%hx, %11)\n  %13 : Tensor = aten::add(%10, %12, %8)\n  %14 : Tensor = aten::add(%13, %b_ih, %8)\n  %gates : Tensor = aten::add(%14, %b_hh, %8)\n  %16 : Tensor[] = aten::chunk(%gates, %7, %8)\n  %ingate.1 : Tensor, %forgetgate.1 : Tensor, %cellgate.1 : Tensor, %outgate.1 : Tensor = prim::ListUnpack(%16)\n  %ingate : Tensor = aten::sigmoid(%ingate.1)\n  %forgetgate : Tensor = aten::sigmoid(%forgetgate.1)\n  %cellgate : Tensor = aten::tanh(%cellgate.1)\n  %outgate : Tensor = aten::sigmoid(%outgate.1)\n  %25 : Tensor = aten::mul(%forgetgate, %cx)\n  %26 : Tensor = aten::mul(%ingate, %cellgate)\n  %cy : Tensor = aten::add(%25, %26, %8)\n  %28 : Tensor = aten::tanh(%cy)\n  %hy : Tensor = aten::mul(%outgate, %28)\n  %30 : (Tensor, Tensor) = prim::TupleConstruct(%hy, %cy)\n  return (%30)\n```\n\n----------------------------------------\n\nTITLE: Building for AMD ROCm on Linux - Run Build Script - bash\nDESCRIPTION: This script is required when compiling PyTorch with AMD ROCm support. It invokes the ROCm-specific build process by running a Python build helper. Only execute if targeting AMD GPUs with ROCm on Linux. ROCm and its prerequisites must be installed prior to running.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Only run this if you're compiling for ROCm\\npython tools/amd_build/build_amd.py\n```\n\n----------------------------------------\n\nTITLE: Using ATen _softmax Operator in PyTorch\nDESCRIPTION: Demonstrates the `aten._softmax` operator which normalizes input data across specified dimensions. Various tensor shapes such as [256, 1024, 1024], [256, 256, 256] with data type f16 and dimension -1 are used, showcasing adaptability in tensor operations. Dependencies include the input tensor and the dimension along which to apply the softmax.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 1, ((T([256, 1024, 1024], f16), -1, False), {})\ncnt: 2, ((T([256, 256, 256], f16), -1, False), {})\ncnt: 1, ((T([256, 64, 64], f16), -1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Adaptive Average Pooling in PyTorch\nDESCRIPTION: Performs 2D adaptive average pooling on input tensors. The forward and backward operations are shown with their respective input shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\naten._adaptive_avg_pool2d.default((T([64, 512, 7, 7], f16), [7, 7]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten._adaptive_avg_pool2d_backward.default((T([64, 512, 7, 7], f16), T([64, 512, 7, 7], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Creating a Multi-Dimensional Tensor with PyTorch C++\nDESCRIPTION: This snippet shows how to create a three-dimensional tensor filled with values from a unit normal distribution using the randn() factory function. It also demonstrates how to verify the tensor's size.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor tensor = torch::randn({3, 4, 5});\nassert(tensor.sizes() == std::vector<int64_t>{3, 4, 5});\n```\n\n----------------------------------------\n\nTITLE: Generated Triton Kernel Example\nDESCRIPTION: Shows the automatically generated Triton kernel code that implements the fused cos and sin operations. This is the optimized code produced by the TorchInductor backend.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pointwise(size_hints=[16384], filename=__file__, triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': 0, 'constants': {}, 'mutated_arg_names': [], 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n@triton.jit\ndef triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n   xnumel = 10000\n   xoffset = tl.program_id(0) * XBLOCK\n   xindex = xoffset + tl.arange(0, XBLOCK)[:]\n   xmask = xindex < xnumel\n   x0 = xindex\n   tmp0 = tl.load(in_ptr0 + (x0), xmask, other=0.0)\n   tmp1 = tl.cos(tmp0)\n   tmp2 = tl.sin(tmp1)\n   tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp2, xmask)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Symbolic Function for ONNX Export\nDESCRIPTION: Template for implementing a symbolic function to support custom PyTorch operators in ONNX export. Shows function signature and documentation requirements.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef foo(\n  g,\n  input_0: torch._C.Value,\n  input_1: torch._C.Value) -> Union[None, torch._C.Value, List[torch._C.Value]]:\n  \"\"\"\n  Adds the ONNX operations representing this PyTorch function by updating the\n  graph g with `g.op()` calls.\n\n  Args:\n    g (Graph): graph to write the ONNX representation into.\n    input_0 (Value): value representing the variables which contain\n        the first input for this operator.\n    input_1 (Value): value representing the variables which contain\n        the second input for this operator.\n\n  Returns:\n    A Value or List of Values specifying the ONNX nodes that compute something\n    equivalent to the original PyTorch operator with the given inputs.\n\n    None if it cannot be converted to ONNX.\n  \"\"\"\n  ...\n```\n\n----------------------------------------\n\nTITLE: Distinguishing Between Packaged and Non-Packaged Code\nDESCRIPTION: Demonstrates how to use torch.package.is_from_package() to check if an object's code is from a torch.package, with exceptions for extern modules and stdlib.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimporter = PackageImporter(f)\nmod = importer.import_module('foo')\nobj = importer.load_pickle('model', 'model.pkl')\ntxt = importer.load_text('text', 'my_test.txt')\n\nassert is_from_package(mod)\nassert is_from_package(obj)\nassert not is_from_package(txt) # str is from stdlib, so this will return False\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch Android from Source\nDESCRIPTION: This bash script outlines the process of building PyTorch Android from source. It includes cloning the repository, updating submodules, and running the build script.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/pytorch/pytorch.git\ncd pytorch\ngit submodule update --init --recursive\nbash ./scripts/build_pytorch_android.sh\n```\n\n----------------------------------------\n\nTITLE: Collecting Raw CUDA Memory Usage in PyTorch\nDESCRIPTION: This code snippet shows how to use the device_memory_used function to collect raw CUDA memory usage information for a specific device in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch_cuda_memory.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\ndevice_idx = ...\nprint(torch.cuda.device_memory_used(device_idx))\n```\n\n----------------------------------------\n\nTITLE: Jacobians Using functorch.jacrev with Argument Selection - Python\nDESCRIPTION: Uses functorch's convenient 'jacrev' transform to compute the Jacobian of 'predict' with respect to the specified function argument (argnums=2 refers to 'x'). Confirms that this method matches previous Jacobian computations. Requires functorch.jacrev and all variables in scope.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import jacrev\n\nft_jacobian = jacrev(predict, argnums=2)(weight, bias, x)\n\n# confirm \nassert torch.allclose(ft_jacobian, jacobian)\n```\n\n----------------------------------------\n\nTITLE: Constructing PyTorch Tensor\nDESCRIPTION: Documents the constructor behavior for torch.Tensor class. When provided with a torch.Size argument, it creates an empty tensor of the specified size. This constructor does not support explicit dtype or device specification.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndata (array_like)  # The tensor to construct from\ndevice (torch.device, optional)  # The desired device of returned tensor. Default: if None, uses same device as source tensor\n```\n\n----------------------------------------\n\nTITLE: Using CUDAMultiStreamGuard for Multiple Streams in PyTorch C++\nDESCRIPTION: This example shows how to use CUDAMultiStreamGuard to set and manage multiple CUDA streams on different devices simultaneously.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\n// create two tensor, one on device 0, one on device 1\ntorch::Tensor tensor0 = torch::ones({2, 2}, torch::device({torch::kCUDA, 0}));\ntorch::Tensor tensor1 = torch::ones({2, 2}, torch::device({torch::kCUDA, 1}));\n\n// acquire new CUDA streams from CUDA stream pool on device 0 and device 1\nat::cuda::CUDAStream myStream0 = at::cuda::getStreamFromPool(false, 0);\nat::cuda::CUDAStream myStream1 = at::cuda::getStreamFromPool(false, 1);\n\n// set current CUDA stream on device 0 to `myStream0` and\n// set current CUDA stream on device 1 to `myStream1` CUDA using multistream guard\n{\n  at::cuda::CUDAMultiStreamGuard multi_guard({myStream0, myStream1});\n\n  // sum() on tensor0 uses `myStream0` as current CUDA stream on device 0\n  tensor0.sum();\n  // sum() on tensor1 uses `myStream1` as current CUDA stream on device 1\n  tensor1.sum();\n}\n\n// current CUDA stream on device 0 is reset to default CUDA stream on device 0\n// current CUDA stream on device 1 is reset to default CUDA stream on device 1\n\n// sum() on tensor0 uses default CUDA stream as current CUDA stream on device 0\ntensor0.sum();\n// sum() on tensor1 uses default CUDA stream as current CUDA stream on device 1\ntensor1.sum();\n```\n\n----------------------------------------\n\nTITLE: Saving Custom Class Instances in a Package\nDESCRIPTION: Shows how to save instances of a custom class in a torch package using PackageExporter. This demonstrates that the custom serialization happens automatically when instances are encountered.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# example of saving instances of class Foo\n\nimport torch\nfrom torch.package import PackageImporter, PackageExporter\nimport foo\n\nfoo_1 = foo.Foo(\"foo_1 initial string\")\nfoo_2 = foo.Foo(\"foo_2 initial string\")\nwith PackageExporter('foo_package.pt') as pe:\n    # save as normal, no extra work necessary\n```\n\n----------------------------------------\n\nTITLE: Generated Graph with Symbolic Shape (SymInt) in PyTorch Dynamo\nDESCRIPTION: This shows the FX graph generated by Dynamo after a retrace triggered by a shape change (second call to `fn` with shape `(8, 3)`). The varying dimension is now represented symbolically using `torch.SymInt` (aliased as `s0` in the graph code logs, though shown explicitly here) allowing the graph to be generic for that dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, s0: torch.SymInt, l_a_: torch.Tensor, l_b_: torch.Tensor):\n    size = l_a_.size()\n    getitem = size[0]\n    mul = getitem * l_a_\n    mul_1 = mul * l_b_\n    return (mul_1,)\n```\n\n----------------------------------------\n\nTITLE: Running C10D Unit Tests in Python\nDESCRIPTION: Commands to run various C10D (Communication Library) unit tests using Python. These tests cover common functionality, Gloo backend, and NCCL backend.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython test/distributed/test_c10d_common.py\npython test/distributed/test_c10d_gloo.py\npython test/distributed/test_c10d_nccl.py\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Operations\nDESCRIPTION: Matrix multiplication and transformation operations including bmm (batch matrix multiply), mm (matrix multiply), and addmm (matrix multiply with addition). Operations handle various tensor shapes and use float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/AllenaiLongformerBase_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Matrix multiplication operations\naten.bmm.default(T([36, 512, 64], f16), T([36, 64, 512], f16))\naten.mm.default(T([1024, 768], f16), T([768, 3072], f16))\naten.addmm.default(T([768], f16), T([1024, 768], f16), T([768, 768], f16))\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Tensor Operations\nDESCRIPTION: Batch normalization operations on tensors with various dimensions, using half precision (f16) and single precision (f32) data types. Operations include processing of feature maps with batch size 64 and varying channel dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 1024, 14, 14], f16), T([64, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Installing Caffe2 CMake Configuration Files\nDESCRIPTION: Handles installation of CMake configuration files for shared library builds. Installs various CMake modules, configuration files, and export targets for downstream projects.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_47\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_SHARED_LIBS)\n  configure_file(${PROJECT_SOURCE_DIR}/cmake/Caffe2Config.cmake.in\n                 ${PROJECT_BINARY_DIR}/Caffe2Config.cmake @ONLY)\n  install(\n    FILES ${PROJECT_BINARY_DIR}/Caffe2Config.cmake\n    DESTINATION share/cmake/Caffe2\n    COMPONENT dev)\n  install(\n    FILES ${PROJECT_SOURCE_DIR}/cmake/public/cuda.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/xpu.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/glog.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/gflags.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/mkl.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/mkldnn.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/protobuf.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/utils.cmake\n          ${PROJECT_SOURCE_DIR}/cmake/public/LoadHIP.cmake\n    DESTINATION share/cmake/Caffe2/public\n    COMPONENT dev)\nendif()\n```\n\n----------------------------------------\n\nTITLE: TorchScript Custom Class Example\nDESCRIPTION: Basic example of creating a custom class in TorchScript with initialization and method definition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\nclass MyClass:\n    def __init__(self, x: int):\n        self.x = x\n\n    def inc(self, val: int):\n        self.x += val\n```\n\n----------------------------------------\n\nTITLE: Registering PyTorch Kernel with Multiple Outputs (C++)\nDESCRIPTION: Illustrates how a C++ kernel function can return multiple values by returning a `std::tuple`. The types within the tuple correspond to the multiple outputs of the operator. The operator schema will be inferred to reflect these multiple return values unless explicitly specified.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace {\n  std::tuple<Tensor, int64_t, Tensor>\n     my_kernel_cpu(const Tensor& a, const Tensor& b, int64_t c) {...}\n}\n\nstatic auto registry = torch::RegisterOperators()\n   .op(\"my_namespace::my_op\", torch::RegisterOperators::options()\n       .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()));\n```\n\n----------------------------------------\n\nTITLE: Automatic Batching with Dimensions in PyTorch\nDESCRIPTION: Demonstrates automatic batching of unbatched code using dimension objects.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nbatch_size, feature_size = 3, 5\nweights = torch.randn(feature_size)\n\ndef model(feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\nexamples = torch.randn(batch_size, feature_size)\nbatch = dims(1)\nr = model(examples[batch])\nprint(r)\n```\n\n----------------------------------------\n\nTITLE: Implementing Embedding Bag with First-Class Dimensions in Python\nDESCRIPTION: This snippet shows how to implement an embedding bag operation using first-class dimensions in PyTorch. It performs an embedding table lookup followed by a sum.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef embedding_bag(input, embedding_weights):\n    batch, sequence, features = dims(3)\n    r = embedding_weights[input[batch, sequence], features].sum(sequence)\n    return r.order(batch, features)\n\ninput = torch.tensor([[1, 0, 4, 3]])\nW = torch.rand(5,2)\nembedding_bag(input, W)\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization in PyTorch with Half-Precision Tensors\nDESCRIPTION: Multiple batch normalization operations on tensors of various shapes. Operations use half-precision (f16) tensors and include running mean and variance calculations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 32, 149, 149], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 0.001), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing TrainingAwareDataSparsity Callback in Python\nDESCRIPTION: Example of creating a TrainingAwareDataSparsity callback that sparsifies model parameters during training. It configures both a DataNormSparsifier and a StepSLScheduler with their respective parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/lightning/callbacks/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom data_sparsity import TrainingAwareDataSparsity\nsparsifier_args = {\n    'sparsity_level': 0.5,\n    'sparse_block_shape': (1, 4),\n    'zeros_per_block': 4\n}\nscheduler_args = {\n    'gamma': 2,\n    'step_size': 1\n}\n\nta_callback = TrainingAwareDataSparsity(\n    data_sparsifier_class=DataNormSparsifier,\n    data_sparsifier_args=sparsifier_args,\n    data_scheduler_class=StepSLScheduler,\n    data_scheduler_args=scheduler_args\n)\n```\n\n----------------------------------------\n\nTITLE: FakeTensor Implementation Patterns Description\nDESCRIPTION: The FakeTensor implementation uses both a subclass and mode tensor subclass pattern. It relies on FakeTensor.__torch_dispatch__ to enable the associated FakeTensorMode and handle operation redispatching. The implementation includes special handling for unrecognized subclass arguments by returning NotImplemented to allow other subclasses to process first.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_6\n\n\n\n----------------------------------------\n\nTITLE: Checking PyTorch HIP or CUDA Availability\nDESCRIPTION: Illustrates how to conditionally execute code based on whether PyTorch is using HIP or CUDA. This snippet requires PyTorch with GPU support to check for hardware availability and differentiate between using HIP or CUDA. The dependencies include torch.cuda.is_available(), torch.version.hip, and torch.version.cuda. Its useful in scenarios where specific code paths need to be executed under different GPU acceleration libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/hip.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nif torch.cuda.is_available() and torch.version.hip:\n    # do something specific for HIP\nelif torch.cuda.is_available() and torch.version.cuda:\n    # do something specific for CUDA\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.sigmoid.default\nDESCRIPTION: Logs Sigmoid activation function calls (`aten.sigmoid.default`). These are out-of-place operations applied to 4D float16 tensors where the spatial dimensions are singletons ([128, C, 1, 1]).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.sigmoid.default\ncnt: 1, ((T([128, 24, 1, 1], f16),), {})\ncnt: 1, ((T([128, 56, 1, 1], f16),), {})\ncnt: 4, ((T([128, 152, 1, 1], f16),), {})\ncnt: 7, ((T([128, 368, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with Torch Compile on Intel GPU\nDESCRIPTION: Illustrates inference with and without torch.compile optimization for ResNet50 model on Intel GPU, measuring performance before and after applying torch.compile.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport torchvision.models as models\nimport time\n\nmodel = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\nITERS = 10\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\n\nfor i in range(ITERS):\n    start = time.time()\n    with torch.no_grad():\n        model(data)\n        torch.xpu.synchronize()\n    end = time.time()\n    print(f\"Inference time before torch.compile for iteration {i}: {(end-start)*1000} ms\")\n\nmodel = torch.compile(model)\nfor i in range(ITERS):\n    start = time.time()\n    with torch.no_grad():\n        model(data)\n        torch.xpu.synchronize()\n    end = time.time()\n    print(f\"Inference time after torch.compile for iteration {i}: {(end-start)*1000} ms\")\n\nprint(\"Execution finished\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Verbose MPS Allocator Logging (Environment Variable)\nDESCRIPTION: Set `PYTORCH_DEBUG_MPS_ALLOCATOR` to `1` to enable verbose logging for the MPS memory allocator in PyTorch. This is useful for debugging memory allocation issues on Apple Silicon.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nPYTORCH_DEBUG_MPS_ALLOCATOR\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Metric Handler for TorchElastic in Python\nDESCRIPTION: This snippet demonstrates how to implement a custom metric handler by extending the MetricHandler class and configuring it in a custom launcher to process and emit metrics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/customization.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# my_launcher.py\n\nimport torch.distributed.elastic.metrics as metrics\n\nclass MyMetricHandler(metrics.MetricHandler):\n    def emit(self, metric_data: metrics.MetricData):\n        # push metric_data to your metric sink\n\ndef main():\n  metrics.configure(MyMetricHandler())\n\n  spec = WorkerSpec(...)\n  agent = LocalElasticAgent(spec)\n  agent.run()\n```\n\n----------------------------------------\n\nTITLE: Using MemPool with Tensor Allocation in PyTorch\nDESCRIPTION: Example demonstrating how to use the torch.cuda.use_mem_pool context manager to allocate tensors with a custom allocator and perform collective operations with NVLS reductions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nwith torch.cuda.use_mem_pool(pool):\n    # tensor gets allocated with ncclMemAlloc passed in the pool\n    tensor = torch.arange(1024 * 1024 * 2, device=device)\n    print(f\"tensor ptr on rank {rank} is {hex(tensor.data_ptr())}\")\n\n# register user buffers using ncclCommRegister (called under the hood)\nbackend.register_mem_pool(pool)\n\n# Collective uses Zero Copy NVLS\ndist.all_reduce(tensor[0:4])\ntorch.cuda.synchronize()\nprint(tensor[0:4])\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Gamma Scheduler for Data Sparsification in PyTorch\nDESCRIPTION: A custom data scheduler that gradually increases the sparsity level by multiplying it with gamma every epoch until a threshold is reached. This scheduler inherits from the BaseDataScheduler class and implements the required get_schedule_param() method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_scheduler/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass GammaScheduler(BaseDataScheduler):\n    def __init__(self, data_sparsifier, gamma, threshold_sl):\n        super().__init__(data_sparsifier, \"sparsity_level\")\n        self.gamma = gamma\n        self.threshold_sl = threshold_sl\n\n    def get_schedule_param(self):\n        if self.last_epoch > 0:\n            return {name: min(self.threshold_sl, config[\"sparsity_level\"] * self.gamma) for name, config in self.data_sparsifier.data_groups.items()}\n        else:\n            return {name: 0.0 for name, config in self.data_sparsifier.data_groups.items()}\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobians using functorch.jacrev and make_functional in Python\nDESCRIPTION: Illustrates computing Jacobians of model outputs with respect to parameters using the legacy `functorch` library. `functorch.make_functional` converts the `model` into a functional form (`fmodel`) and extracts its parameters (`params`). `functorch.jacrev` is then applied to the functional model `fmodel` to compute the Jacobians. Requires `torch` and `functorch`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.migrating.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# ---------------\n# using functorch\n# ---------------\nimport torch\nimport functorch\ninputs = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nfmodel, params = functorch.make_functional(model)\njacobians = functorch.jacrev(fmodel)(params, inputs)\n```\n\n----------------------------------------\n\nTITLE: Guards Generated for Static Shape Input in PyTorch Dynamo\nDESCRIPTION: These are the guards generated by Dynamo for the first execution of the compiled function. They check specific properties of the input tensors `a` and `b`, including their dtype, device, grad status, and importantly, their exact fixed size `[4, 3]` and stride `[3, 1]`. If these conditions hold on a subsequent call, the compiled static graph can be reused.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Guards first call\ncheck_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])\ncheck_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])\n```\n\n----------------------------------------\n\nTITLE: Modifying Existing GraphModule in PyTorch FX using Python\nDESCRIPTION: This code snippet shows how to modify an existing PyTorch FX GraphModule by tracing a given module, altering its graph, and recompiling it. Dependencies include 'torch' and 'torch.fx'. The input is a torch.nn.Module, and it returns a modified GraphModule. The main constraint is the requirement to recompile to synchronize the 'forward()' method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm\n```\n\n----------------------------------------\n\nTITLE: Stream Semantics Examples in Backward Pass\nDESCRIPTION: Illustrates various patterns of stream usage with backward passes, showing both safe and unsafe implementations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ns = torch.cuda.Stream()\n\n# Safe, grads are used in the same stream context as backward()\nwith torch.cuda.stream(s):\n    loss.backward()\n    use grads\n\n# Unsafe\nwith torch.cuda.stream(s):\n    loss.backward()\nuse grads\n\n# Safe, with synchronization\nwith torch.cuda.stream(s):\n    loss.backward()\ntorch.cuda.current_stream().wait_stream(s)\nuse grads\n\n# Safe, populating initial grad and invoking backward are in the same stream context\nwith torch.cuda.stream(s):\n    loss.backward(gradient=torch.ones_like(loss))\n\n# Unsafe, populating initial_grad and invoking backward are in different stream contexts,\n# without synchronization\ninitial_grad = torch.ones_like(loss)\nwith torch.cuda.stream(s):\n    loss.backward(gradient=initial_grad)\n\n# Safe, with synchronization\ninitial_grad = torch.ones_like(loss)\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    initial_grad.record_stream(s)\n    loss.backward(gradient=initial_grad)\n```\n\n----------------------------------------\n\nTITLE: TorchScript Enum Example\nDESCRIPTION: Demonstrates defining and using enum types in TorchScript with comparison operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom enum import Enum\n\nclass Color(Enum):\n    RED = 1\n    GREEN = 2\n\ndef enum_fn(x: Color, y: Color) -> bool:\n    if x == Color.RED:\n        return True\n    return x == y\n\nm = torch.jit.script(enum_fn)\n\nprint(\"Eager: \", enum_fn(Color.RED, Color.GREEN))\nprint(\"TorchScript: \", m(Color.RED, Color.GREEN))\n```\n\n----------------------------------------\n\nTITLE: Accessing a Submodule Attribute via get_attr Node in FX Graph (Python)\nDESCRIPTION: This FX graph snippet shows how to add a get_attr node, which reads a submodule or attribute from an enclosing GraphModule. In PyTorch FX, get_attr is used to reference submodules, parameters, or buffers by their names, and to fetch components needed as inputs for subsequent graph nodes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%name = get_attr[target = name](args = ())\n```\n\n----------------------------------------\n\nTITLE: Embedding Lookup with Dimensions in PyTorch\nDESCRIPTION: Demonstrates looking up features in an embedding table using dimension objects for sequence and feature indices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsequence, features = dims(2)\nembeddings = torch.rand(8, 128)\nwords = torch.tensor([5, 4, 0,])\n\nstate = embeddings[words[sequence], features]\nprint(state.dims)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations\nDESCRIPTION: Batch matrix multiplication operations for attention mechanism calculations, operating on 16-bit floating point tensors with various shapes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_DistilBert_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.bmm.default(T([96, 512, 64], f16), T([96, 64, 512], f16))\naten.mm.default(T([4096, 768], f16), T([768, 768], f16))\naten.addmm.default(T([768], f16), T([4096, 768], f16), T([768, 768], f16, stride=(1, 768)))\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Named Children of a Module\nDESCRIPTION: Demonstrates iterating through the immediate submodules (children) of the `Net` instance using `named_children()`. This method yields tuples containing the attribute name assigned to the child module (`'l0'`, `'l1'`) and the child module instance itself.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnet = Net()\nfor child in net.named_children():\n  print(child)\n: ('l0', MyLinear())\n('l1', MyLinear())\n```\n\n----------------------------------------\n\nTITLE: Control Flow Documentation in RestructuredText\nDESCRIPTION: Documentation for PyTorch's control flow operations, specifically the 'cond' function which is in prototype stage and may have breaking changes in future releases.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_9\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. warning::\n    This feature is a prototype and may have compatibility breaking changes in the future.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    cond\n```\n\n----------------------------------------\n\nTITLE: Convolution Backward Pass in PyTorch\nDESCRIPTION: Computes gradients for convolution operations. Includes input gradients, weight gradients, and bias gradients for various layer configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\naten.convolution_backward.default((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), T([512, 512, 3, 3], f16), [512], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.convolution_backward.default((T([64, 512, 28, 28], f16), T([64, 512, 28, 28], f16), T([512, 512, 3, 3], f16), [512], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Broadcasting Backward Compatibility Changes in Python\nDESCRIPTION: Highlights a backward compatibility change introduced by broadcasting. Previously, operations on tensors with different shapes but the same number of elements might have worked by viewing them as 1D. Now, broadcasting rules apply. The example shows `torch.add(torch.ones(4,1), torch.randn(4))` resulting in a `[4,4]` tensor due to broadcasting, whereas it might have previously resulted in `[4,1]`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.add(torch.ones(4,1), torch.randn(4))\n```\n\n----------------------------------------\n\nTITLE: Function with Data-Dependent Control Flow under vmap (Unsupported) - PyTorch - Python\nDESCRIPTION: This Python snippet shows a function using data-dependent control flow (if-statement with tensor value) and how it fails under vmap. relu uses a Tensor in an if-condition, which isn't supported by vmap due to ambiguity over batching. Dependencies: PyTorch, torch.func. Limitation: Avoid using Tensors as conditions in control flow when using vmap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef relu(x):\n  if x > 0:\n    return x\n  return 0\n\nx = torch.randn(3)\nvmap(relu)(x)\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations in PyTorch with Half-Precision Tensors\nDESCRIPTION: Multiple convolution operations with different input/output shapes, kernel sizes, and strides. Operations use half-precision (f16) tensors and include various padding and dilation settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([192, 192, 7, 1], f16), [0], [1, 1], [3, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in ResNet-like Model\nDESCRIPTION: This code snippet provides a comprehensive analysis of PyTorch operator usage in what appears to be a ResNet or similar convolutional neural network architecture. It includes operator names, input tensor shapes, and frequency counts for various operations like convolutions, additions, and backward passes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_senet154_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([32, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([32, 1000], f16), T([32, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 5, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16)), {})\ncnt: 72, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16)), {})\ncnt: 16, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16)), {})\ncnt: 6, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)), {})\ncnt: 1, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 157, ((T([], i64), 1), {})\ncnt: 3, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)), {})\ncnt: 8, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16)), {})\ncnt: 36, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16)), {})\ncnt: 3, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([32, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})\nOperator: aten.clone.default\ncnt: 1, ((T([32, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([64, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\n# ... [additional convolution operations omitted for brevity]\nOperator: aten.convolution_backward.default\ncnt: 3, ((T([32, 2048, 1, 1], f16), T([32, 128, 1, 1], f16), T([2048, 128, 1, 1], f16), [2048], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n# ... [additional backward convolution operations omitted for brevity]\nOperator: aten.copy_.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\n```\n\n----------------------------------------\n\nTITLE: Using torch.cond with Dynamic Shape Predicate\nDESCRIPTION: An example of using torch.cond to branch based on input shape, demonstrating its ability to handle dynamic shape predicates.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef true_fn(x: torch.Tensor):\n    return x.cos() + x.sin()\n\ndef false_fn(x: torch.Tensor):\n    return x.sin()\n\nclass DynamicShapeCondPredicate(torch.nn.Module):\n    \"\"\"\n    A basic usage of cond based on dynamic shape predicate.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        def true_fn(x: torch.Tensor):\n            return x.cos()\n\n        def false_fn(x: torch.Tensor):\n            return x.sin()\n\n        return torch.cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\ndyn_shape_mod = DynamicShapeCondPredicate()\n```\n\n----------------------------------------\n\nTITLE: Resizing a Non-Quantized Tensor in PyTorch\nDESCRIPTION: This code snippet demonstrates the use of the resize_() method on a PyTorch tensor. When fill_uninitialized_memory is True and deterministic algorithms are enabled, this operation will fill uninitialized memory for non-quantized tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/deterministic.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntorch.Tensor.resize_()\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations\nDESCRIPTION: Forward convolution operations with various kernel sizes, strides and channels using float16 precision\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.convolution.default(T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1)\n```\n\n----------------------------------------\n\nTITLE: Supported In-place Operations with vmap\nDESCRIPTION: This example shows a case where in-place operations are supported by vmap, specifically when the tensor being modified has the same number of elements as the result of the operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(3)\ny = torch.randn(3)\nexpected = x + y\n\n# Does not raise an error because x and y have the same number of elements.\nvmap(f, in_dims=(0, 0))(x, y)\nassert torch.allclose(x, expected)\n```\n\n----------------------------------------\n\nTITLE: Applying HardTanh Function in PyTorch\nDESCRIPTION: The 'aten.hardtanh_.default' operator applies a Hard Tanh activation function in-place on tensors, ensuring outputs remain within specified limits [0.0, 6.0]. This operator is crucial for keeping outputs standardized during processing in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.hardtanh_.default\ncnt: 2, ((T([96, 32, 112, 112], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 96, 112, 112], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 96, 56, 56], f16), 0.0, 6.0), {})\ncnt: 3, ((T([96, 144, 56, 56], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 144, 28, 28], f16), 0.0, 6.0), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Example DataPipe Classes\nDESCRIPTION: Definition of an example IterDataPipe class and helper functions to create DataFrame and regular pipes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example IterDataPipe\nclass ExampleIterPipe(IterDataPipe):\n    def __init__(self, range = 20):\n        self.range = range\n    def __iter__(self):\n        for i in range(self.range):\n            yield i\n\ndef get_dataframes_pipe(range = 10, dataframe_size = 7):\n    return ExampleIterPipe(range = range).map(lambda i: (i, i % 3))._to_dataframes_pipe(columns = ['i','j'], dataframe_size = dataframe_size)\n\ndef get_regular_pipe(range = 10):\n    return ExampleIterPipe(range = range).map(lambda i: (i, i % 3))\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building PyTorch C++ RPC Tests using CMake\nDESCRIPTION: This CMake script sets up the build process for the PyTorch C++ RPC tests. It defines the test directory (`TORCH_RPC_TEST_DIR`) and source files (`TORCH_RPC_TEST_SOURCES`). It specifies dependencies (`TORCH_RPC_TEST_DEPENDENCY_LIBS`) including `torch` and `gtest`. Sources and dependencies related to Gloo and TensorPipe backends are conditionally added based on `USE_GLOO` and `USE_TENSORPIPE` variables. An executable target `test_cpp_rpc` is created, linked against the necessary libraries, and configured with appropriate include directories. Conditional compilation for CUDA support (`USE_CUDA`) is handled, and optional installation (`INSTALL_TEST`) rules are defined, including setting RPATH and installing PDB files on MSVC.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/rpc/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TORCH_RPC_TEST_DIR \"${TORCH_ROOT}/test/cpp/rpc\")\nset(TORCH_RPC_TEST_SOURCES\n  ${TORCH_ROOT}/test/cpp/common/main.cpp\n  ${TORCH_RPC_TEST_DIR}/e2e_test_base.cpp\n  ${TORCH_RPC_TEST_DIR}/test_wire_serialization.cpp\n)\nset(TORCH_RPC_TEST_DEPENDENCY_LIBS\n  torch gtest\n)\n\nif(USE_GLOO)\n  list(APPEND TORCH_RPC_TEST_SOURCES\n    ${TORCH_RPC_TEST_DIR}/test_e2e_tensorpipe.cpp\n  )\nendif()\n\nif(USE_TENSORPIPE)\n  list(APPEND TORCH_RPC_TEST_SOURCES\n    ${TORCH_RPC_TEST_DIR}/test_tensorpipe_serialization.cpp\n  )\n  list(APPEND TORCH_RPC_TEST_DEPENDENCY_LIBS\n    tensorpipe\n  )\nendif()\n\nadd_executable(test_cpp_rpc ${TORCH_RPC_TEST_SOURCES})\ntarget_include_directories(\n  test_cpp_rpc PRIVATE\n  ${ATen_CPU_INCLUDE})\ntarget_include_directories(\n  test_cpp_rpc PRIVATE\n  $<BUILD_INTERFACE:${TORCH_SRC_DIR}/csrc/distributed>)\ntarget_link_libraries(test_cpp_rpc PRIVATE ${TORCH_RPC_TEST_DEPENDENCY_LIBS})\n\nif(USE_CUDA)\n  target_compile_definitions(test_cpp_rpc PRIVATE \"USE_CUDA\")\nendif()\n\nif(INSTALL_TEST)\n  set_target_properties(test_cpp_rpc PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n  install(TARGETS test_cpp_rpc DESTINATION bin)\n  # Install PDB files for MSVC builds\n  if(MSVC AND BUILD_SHARED_LIBS)\n    install(FILES $<TARGET_PDB_FILE:test_cpp_rpc> DESTINATION bin OPTIONAL)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: CUDA Graph Profiling Fix\nDESCRIPTION: Workaround for CUDA graph profiling issues on older driver versions, showing initialization setup.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ntorch.profiler._utils._init_for_cuda_graphs()\n\n# ... rest of program\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch Libraries (Python)\nDESCRIPTION: The build_pytorch_libs.py script is a cross-platform tool that builds all constituent libraries of PyTorch, excluding the PyTorch Python extension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npython build_pytorch_libs.py\n```\n\n----------------------------------------\n\nTITLE: LSTM Graph After Type and Shape Inference in PyTorch\nDESCRIPTION: The LSTM graph after constant propagation and type inference. The graph now contains more specific type information for tensors and has replaced the generic chunk operation with a specialized ConstantChunk operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ngraph(%x : Float(*, *),\n      %hx : Float(*, *),\n      %cx : Float(*, *),\n      %w_ih : Float(*, *),\n      %w_hh : Float(*, *),\n      %b_ih : Float(*),\n      %b_hh : Float(*)):\n  %8 : int = prim::Constant[value=1]()\n  %9 : Float(*, *) = aten::t(%w_ih)\n  %10 : Float(*, *) = aten::mm(%x, %9)\n  %11 : Float(*, *) = aten::t(%w_hh)\n  %12 : Float(*, *) = aten::mm(%hx, %11)\n  %13 : Float(*, *) = aten::add(%10, %12, %8)\n  %14 : Float(*, *) = aten::add(%13, %b_ih, %8)\n  %gates : Float(*, *) = aten::add(%14, %b_hh, %8)\n  %31 : Float(*, *), %32 : Float(*, *), %33 : Float(*, *), %34 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%gates)\n  %ingate : Float(*, *) = aten::sigmoid(%31)\n  %forgetgate : Float(*, *) = aten::sigmoid(%32)\n  %cellgate : Float(*, *) = aten::tanh(%33)\n  %outgate : Float(*, *) = aten::sigmoid(%34)\n  %25 : Float(*, *) = aten::mul(%forgetgate, %cx)\n  %26 : Float(*, *) = aten::mul(%ingate, %cellgate)\n  %cy : Float(*, *) = aten::add(%25, %26, %8)\n  %28 : Float(*, *) = aten::tanh(%cy)\n  %hy : Float(*, *) = aten::mul(%outgate, %28)\n  %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)\n  return (%30)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Allocator Change Restrictions in PyTorch\nDESCRIPTION: This Python code illustrates that changing the CUDA memory allocator is not possible after an initial memory allocation has been made. It shows the error that occurs when attempting to change the allocator after using CUDA memory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\n\n# Do an initial memory allocator\nb = torch.zeros(10, device='cuda')\n# Load the allocator\nnew_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n    'alloc.so', 'my_malloc', 'my_free')\n# This will error since the current allocator was already instantiated\ntorch.cuda.memory.change_current_allocator(new_alloc)\n```\n\n----------------------------------------\n\nTITLE: Configuring NCCL Support for PyTorch Python\nDESCRIPTION: Adds NVIDIA Collective Communications Library (NCCL) support to PyTorch Python bindings when enabled on non-Windows platforms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_NCCL AND NOT WIN32)\n    list(APPEND TORCH_PYTHON_SRCS\n      ${TORCH_SRC_DIR}/csrc/cuda/python_nccl.cpp)\n    list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_NCCL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Profiling ATen Threshold Backward Operations for ReLU Gradients in PyTorch (Python)\nDESCRIPTION: Tracks ATen threshold_backward operator calls, corresponding to gradient computations for ReLU during backward passes. Dependencies: PyTorch. Inputs are tensors of same shape (input and grad_output) plus the threshold scalar; output is the computed gradient. Matches each activation backward with input/output tensor shapes, for correct gradient flow.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 4, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16), 0), {})\ncnt: 4, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16), 0), {})\ncnt: 4, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16), 0), {})\ncnt: 4, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16), 0), {})\ncnt: 1, ((T([16, 64, 112, 112], f16), T([16, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Unsupported In-place Operations with vmap\nDESCRIPTION: This example demonstrates how vmap raises an error when encountering an unsupported in-place operation where a tensor with fewer elements would be overwritten by a tensor with more elements due to broadcasting.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(1)\ny = torch.randn(3)\n\n# Raises an error because `y` has fewer elements than `x`.\nvmap(f, in_dims=(None, 0))(x, y)\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Reflection Padding Backward Operations\nDESCRIPTION: This snippet demonstrates the usage of the aten.reflection_pad2d_backward.default operator for computing gradients in reflection padding operations with various shapes and padding values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.reflection_pad2d_backward.default\ncnt: 2, ((T([3, 64, 518, 518], f16), T([3, 64, 512, 512], f16), [3, 3, 3, 3]), {})\ncnt: 26, ((T([3, 256, 130, 130], f16), T([3, 256, 128, 128], f16), [1, 1, 1, 1]), {})\n```\n\n----------------------------------------\n\nTITLE: Defining EventSource Class in PyTorch Distributed Elastic\nDESCRIPTION: This class represents the source of an event in the PyTorch distributed elastic system. It's used to identify where an event originated from.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.elastic.events.api.EventSource\n```\n\n----------------------------------------\n\nTITLE: Tensor Addition Operations with Strides in PyTorch\nDESCRIPTION: Collection of tensor addition operations with various strides. These operations show different tensor shapes and stride patterns for 16-bit floating point tensors, likely from a neural network computation graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([64, 64, 56, 56], f16, stride=(802816, 3136, 56, 1)), T([64, 64, 56, 56], f16, stride=(702464, 3136, 56, 1))), {})\ncnt: 5, ((T([64, 32, 56, 56], f16, stride=(802816, 3136, 56, 1)), T([64, 32, 56, 56], f16, stride=(702464, 3136, 56, 1))), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16, stride=(602112, 3136, 56, 1))), {})\ncnt: 4, ((T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16, stride=(602112, 3136, 56, 1))), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16, stride=(501760, 3136, 56, 1))), {})\ncnt: 3, ((T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16, stride=(501760, 3136, 56, 1))), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16, stride=(401408, 3136, 56, 1))), {})\ncnt: 2, ((T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16, stride=(401408, 3136, 56, 1))), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16, stride=(301056, 3136, 56, 1))), {})\ncnt: 1, ((T([64, 32, 56, 56], f16), T([64, 32, 56, 56], f16, stride=(301056, 3136, 56, 1))), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Inspecting Shapes and Type of Extracted Model Parameters\nDESCRIPTION: Iterates through the `params` tuple (containing the model's learnable parameters extracted by `make_functional_with_buffers`) and prints the shape of each parameter tensor (weights and biases). It also prints the Python type of the `params` variable itself, confirming it is a tuple.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor x in params:\n  print(f\"{x.shape}\")\n\nprint(f\"\\n{type(params)}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing index_select for CUDA in PyTorch\nDESCRIPTION: This function implements the index_select operation for CUDA tensors in PyTorch. It's specifically optimized for Half precision data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_38\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native44_GLOBAL__N__9e5ddf9f_11_Indexing_cu_89862edb26index_select_out_cuda_implIN3c104HalfEEEvRNS_6TensorERKS5_lS8_\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.mul and aten.mul.Scalar for Elementwise and Scalar Multiplication - PyTorch - Python\nDESCRIPTION: These snippets document both elementwise (tensor-tensor) and scalar (tensor-scalar) multiplication of f16 tensors using aten.mul.Tensor and aten.mul.Scalar. Parameters include input tensors or broadcasting scalars. Output matches broadcasting rules, producing f16 tensors of original or broadcasted shape. Dependencies: torch, careful management of precision and broadcasting semantics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mul.Scalar\ncnt: 12, ((T([4, 1024, 3072], f16), 3.0), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 24, ((T([4, 1024, 3072], f16), 0.5), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 24, ((T([4, 1024, 3072], f16), 0.044715), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 24, ((T([4, 1024, 3072], f16), 0.7978845608028654), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 48, ((T([4, 1024, 3072], f16), T([4, 1024, 3072], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Negative Log Likelihood Loss Forward in PyTorch\nDESCRIPTION: Records usage of \\\"aten.nll_loss_forward.default\\\", for forward pass applications of NLL Loss computation in network models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_22\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([1024, 50265], f16), T([1024], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing pow Kernel for Tensor-Tensor Operations in PyTorch\nDESCRIPTION: This CUDA kernel implements the power operation for tensor-tensor operations in PyTorch. It operates on TensorIteratorBase to handle various tensor shapes and types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_42\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native51_GLOBAL__N__e9b6561f_12_PowKernel_cu_40e48458_3413224pow_tensor_tensor_kernelERNS_18TensorIteratorBaseE\n```\n\n----------------------------------------\n\nTITLE: Combining Ensemble States for functorch vmap - PyTorch - Python\nDESCRIPTION: Uses functorch's combine_state_for_ensemble to stack parameters and buffers from a list of models, producing a stateless functional model, a tuple of stacked parameters, and a tuple of stacked buffers. This transformation is required before applying vmap. All models must have the same architecture and device alignment. Parameters are set as require_grad_ for potential use in gradient computations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import combine_state_for_ensemble\n\nfmodel, params, buffers = combine_state_for_ensemble(models)\n[p.requires_grad_() for p in params];\n\n```\n\n----------------------------------------\n\nTITLE: Verifying Model and Minibatch Dimensions Before vmap - PyTorch - Python\nDESCRIPTION: Displays the leading dimension (number of models) of each parameter tensor after stacking and asserts the minibatch tensor has correct leading shape for vmap. Ensures compatibility between model parameters and input data dimensions before vectorized mapping.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint([p.size(0) for p in params]) # show the leading 'num_models' dimension\n\nassert minibatches.shape == (num_models, 64, 1, 28, 28) # verify minibatch has leading dimension of size 'num_models'\n```\n\n----------------------------------------\n\nTITLE: Demonstrating a Simple TorchScript Module in Python\nDESCRIPTION: Shows how to define a basic ScriptModule with a forward method that includes conditional logic. This example is used to illustrate the relationship between Python code and its internal graph representation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nclass M(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x, y, z):\n        # type: (Tensor, int, float) -> Tensor\n        if y > 2:\n            x = x + z\n        else:\n            x = x + y\n        return x\n\nm = M()\n```\n\n----------------------------------------\n\nTITLE: Incorrectly Passing Received Shared Tensors Between Queues (Python)\nDESCRIPTION: Illustrates an incorrect pattern for inter-process communication using shared tensors. Directly putting a tensor `x` received from one queue (`queue`) into another queue (`queue_2`) will not work as intended due to the underlying shared memory mechanisms and reference counting. The shared tensor cannot simply be re-shared this way.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# not going to work\nx = queue.get()\nqueue_2.put(x)\n```\n\n----------------------------------------\n\nTITLE: Vectorizing Gradient Computation with functorch.vmap\nDESCRIPTION: Applies `functorch.vmap` to the single-sample gradient function `ft_compute_grad` to create a vectorized version `ft_compute_sample_grad`. `vmap` maps the function over batches of inputs. The `in_dims=(None, None, 0, 0)` argument specifies that the transformation should map over the 0th dimension (the batch dimension) of the `sample` (3rd arg) and `target` (4th arg), while the `params` (1st arg) and `buffers` (2nd arg) should be broadcasted (not mapped over, used as constants for each sample).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n```\n\n----------------------------------------\n\nTITLE: Function using Tensor.item() under vmap (Unsupported) - PyTorch - Python\nDESCRIPTION: This snippet shows a function calling .item() on a batched tensor inside a vmap transform, which is not supported because .item() squeezes the batch dimension into a scalar, breaking batching. Dependencies: PyTorch, torch.func. Limitation: Do not use .item() on tensors inside functions passed to vmap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return x.item()\n\nx = torch.randn(3)\nvmap(f)(x)\n```\n\n----------------------------------------\n\nTITLE: Loading Resources from a Package\nDESCRIPTION: Shows how to load resources from a torch package using load_pickle, load_text, and load_binary methods. This demonstrates accessing saved artifacts after the package is created.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimporter = torch.PackageImporter(\"package.pt\")\nmy_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\")\ntext = importer.load_text(\"config_stuff\", \"words.txt\")\nbinary = importer.load_binary(\"raw_data\", \"binary\")\n```\n\n----------------------------------------\n\nTITLE: Spectral Operations List in RestructuredText\nDESCRIPTION: A list of spectral processing operations available in PyTorch, including Fourier transforms and window functions, formatted as a RestructuredText autosummary.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    stft\n    istft\n    bartlett_window\n    blackman_window\n    hamming_window\n    hann_window\n    kaiser_window\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Runtime Environment Variables in reStructuredText\nDESCRIPTION: This snippet defines a table of CUDA runtime and libraries environment variables using reStructuredText syntax. It includes variables for GPU device visibility, synchronous CUDA calls, workspace configurations for cuBLAS and cuDNN, and TF32 controls.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda_environment_variables.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. list-table::\n  :header-rows: 1\n\n  * - Variable\n    - Description\n  * - ``CUDA_VISIBLE_DEVICES``\n    - Comma-separated list of GPU device IDs that should be made available to CUDA runtime. If set to ``-1``, no GPUs are made available.\n  * - ``CUDA_LAUNCH_BLOCKING``\n    - If set to ``1``, makes CUDA calls synchronous. This can be useful for debugging.\n  * - ``CUBLAS_WORKSPACE_CONFIG``\n    - This environment variable is used to set the workspace configuration for cuBLAS per allocation. The format is ``:[SIZE]:[COUNT]``.\n      As an example, the default workspace size per allocation is ``CUBLAS_WORKSPACE_CONFIG=:4096:2:16:8`` which specifies a total size of ``2 * 4096 + 8 * 16 KiB``.\n      To force cuBLAS to avoid using workspaces, set ``CUBLAS_WORKSPACE_CONFIG=:0:0``.\n  * - ``CUDNN_CONV_WSCAP_DBG``\n    - Similar to ``CUBLAS_WORKSPACE_CONFIG``, this environment variable is used to set the workspace configuration for cuDNN per allocation.\n  * - ``CUBLASLT_WORKSPACE_SIZE``\n    - Similar to ``CUBLAS_WORKSPACE_CONFIG``, this environment variable is used to set the workspace size for cuBLASLT.\n  * - ``CUDNN_ERRATA_JSON_FILE``\n    - Can be set to a file path for an errata filter that can be passed to cuDNN to avoid specific engine configs, used primarily for debugging or to hardcode autotuning.\n  * - ``NVIDIA_TF32_OVERRIDE``\n    - If set to ``0``, disables TF32 globally across all kernels, overriding all PyTorch settings.\n```\n\n----------------------------------------\n\nTITLE: Creating an Integer Tensor with Custom Range in PyTorch C++\nDESCRIPTION: This snippet demonstrates how to create a 5x5 square matrix with integers between 3 and 10 using the randint() factory function with both lower and upper bounds specified.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor tensor = torch::randint(/*low=*/3, /*high=*/10, {5, 5});\n```\n\n----------------------------------------\n\nTITLE: Solving Tensor Puzzlers with First-Class Dimensions in Python\nDESCRIPTION: This section includes multiple snippets demonstrating how to solve various tensor puzzles using first-class dimensions in PyTorch. It covers operations like outer product, diagonal vector, identity matrix, upper triangular matrix, running difference, vector stacking, circular shift, and sequence masking.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef outer(a, b):\n    i, j = dims(2)\n    return (a[i] * b[j]).order(i, j)\n\ndef diag(a):\n    i = dims(1)\n    return a[i, i].order(i)\n\nfrom torch import where\ndef eye(j: int):\n    i,j = dims(sizes=[j, j])\n    return where(i == j, 1, 0).order(i, j)\n\ndef triu(j: int):\n    i,j = dims(sizes=[j, j])\n    return where(i <= j, 1, 0).order(i, j)\n\ndef diff(a, i: int):\n    i = dims(1)\n    d = a[i] - a[i - 1]\n    return where(i - 1 >= 0, d, a[i]).order(i)\n\ndef vstack(a, b):\n    v, i = dims(sizes=[2, None])\n    return where(v == 0,  a[i], b[i]).order(v, i)\n\ndef roll(a, i: int):\n    i = dims(sizes=[a.size(0)])\n    return a[where(i + 1 < i.size, i + 1, 0)].order(i)\n\ndef flip(a, i: int):\n    i = dims(sizes=[a.size(0)])\n    return a[i.size - i - 1].order(i)\n\ndef sequence_mask(values, length):\n    j, i = dims()\n    v = values[i, j]\n    return where(j < length[i], v, 0).order(i, j)\n```\n\n----------------------------------------\n\nTITLE: Building and Benchmarking on ARMv7 Android\nDESCRIPTION: This snippet describes how to build QNNPACK for ARMv7 Android devices and push the binaries using ADB. It explains the steps to compile the library, prepare model files, and execute speed benchmarks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Clone PyTorch 1.0 repo\ngit clone --recursive https://github.com/pytorch/pytorch.git\ncd pytorch\n\n# Optional: update QNNPACK submodule to latest revision\ngit submodule update --remote third_party/QNNPACK\n\n# Build Caffe2 (including binaries) for Android, and push to device\nscripts/build_android.sh -DANDROID_TOOLCHAIN=clang -DBUILD_BINARY=ON\nadb push build_android/bin/speed_benchmark /data/local/tmp/speed_benchmark\n\n# Download model weights and copy them to Android device\nwget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/init_net.pb\nadb push init_net.pb /data/local/tmp/init_net.pb\n\n# Download model graph and copy it to Android device\nwget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/predict_net.pb\nadb push predict_net.pb /data/local/tmp/predict_net.pb\n\n# Run speed benchmark with 50 warm-up iterations and 10 measurement iterations\nadb shell /data/local/tmp/speed_benchmark \\\n    --net /data/local/tmp/predict_net.pb \\\n    --init_net /data/local/tmp/init_net.pb \\\n    --input data --input_dims 1,3,224,224 --input_type float \\\n    --warmup 50 --iter 10\n```\n\n----------------------------------------\n\nTITLE: Linking Prebuilt libtorch Library in Gradle\nDESCRIPTION: This Gradle configuration shows how to link a prebuilt libtorch library from a PyTorch Android gradle dependency. It includes task setup for extracting AAR contents and configuring the native build.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_4\n\nLANGUAGE: Groovy\nCODE:\n```\nandroid {\n...\n    configurations {\n       extractForNativeBuild\n    }\n...\n    compileOptions {\n        externalNativeBuild {\n            cmake {\n                arguments \"-DANDROID_STL=c++_shared\"\n            }\n        }\n    }\n...\n    externalNativeBuild {\n        cmake {\n            path \"CMakeLists.txt\"\n        }\n    }\n}\n\ndependencies {\n    extractForNativeBuild('org.pytorch:pytorch_android:1.10.0')\n}\n\ntask extractAARForNativeBuild {\n    doLast {\n        configurations.extractForNativeBuild.files.each {\n            def file = it.absoluteFile\n            copy {\n                from zipTree(file)\n                into \"$buildDir/$file.name\"\n                include \"headers/**\"\n                include \"jni/**\"\n            }\n        }\n    }\n}\n\ntasks.whenTaskAdded { task ->\n  if (task.name.contains('externalNativeBuild')) {\n    task.dependsOn(extractAARForNativeBuild)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Demonstrating TorchInductor Error Example\nDESCRIPTION: Example showing how to trigger and debug a TorchInductor error using a dummy function that deliberately fails.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch._dynamo as dynamo\n\nmodel = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n\ndef test_backend_error():\n    y = torch.ones(200, 200)\n    x = torch.ones(200, 200)\n    z = x + y\n    a = torch.ops.aten._foobar(z)  # dummy function which errors\n    return model(a)\n\ncompiled_test_backend_error = torch.compile(test_backend_error, backend=\"inductor\")\ncompiled_test_backend_error()\n```\n\n----------------------------------------\n\nTITLE: Loading Tensors onto Meta Device using torch.load in Python\nDESCRIPTION: Demonstrates how to load a saved PyTorch tensor directly onto the 'meta' device using `torch.load` by specifying `map_location='meta'`. This loads the tensor's metadata (like size) without loading its actual data, resulting in a meta tensor. Requires the `torch` library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.save(torch.randn(2), 'foo.pt')\n>>> torch.load('foo.pt', map_location='meta')\ntensor(..., device='meta', size=(2,))\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization Operations\nDESCRIPTION: This snippet shows batch normalization operations on tensors with various shapes in f16 precision. Each operation includes input tensor, scale, bias, running mean, running variance, training mode flag, momentum (0.1), and epsilon (1e-05).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 4, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 2, ((T([128, 96, 28, 28], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})\ncnt: 5, ((T([128, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 6, ((T([128, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), True, 0.1, 1e-05), {})\ncnt: 5, ((T([128, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})\ncnt: 2, ((T([128, 336, 14, 14], f16), T([336], f16), T([336], f16), T([336], f16), T([336], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 184, 7, 7], f16), T([184], f16), T([184], f16), T([184], f16), T([184], f16), True, 0.1, 1e-05), {})\ncnt: 8, ((T([128, 1104, 7, 7], f16), T([1104], f16), T([1104], f16), T([1104], f16), T([1104], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 352, 7, 7], f16), T([352], f16), T([352], f16), T([352], f16), T([352], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 1984, 7, 7], f16), T([1984], f16), T([1984], f16), T([1984], f16), T([1984], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Softmax Operations in PyTorch\nDESCRIPTION: Shows the usage of softmax and log_softmax operations with their tensor shapes and parameters. The counts indicate these are primarily used in attention mechanisms with 16 attention heads and sequence lengths of 576 or 577.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([2, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([2, 1000], f16), T([2, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 36, ((T([2, 16, 576, 576], f16, stride=(5308416, 1, 9216, 16)), -1, False), {})\ncnt: 2, ((T([2, 16, 1, 577], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 2, ((T([2, 16, 1, 577], f16), T([2, 16, 1, 577], f16), -1, f16), {})\ncnt: 36, ((T([2, 16, 576, 576], f16, stride=(5308416, 1, 9216, 16)), T([2, 16, 576, 576], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Target for Code Generation Dependency in CMake\nDESCRIPTION: Adds a custom target named `unbox_target`. This target depends on the successful completion of the source code generation defined by the `add_custom_command` associated with `GEN_COMMAND_sources`. Other targets can depend on `unbox_target` to ensure sources are generated before they are used.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(unbox_target DEPENDS ${GEN_COMMAND_sources})\n```\n\n----------------------------------------\n\nTITLE: Using Constant ModuleList in TorchScript\nDESCRIPTION: Example showing how to use nn.ModuleList in TorchScript by marking it as a constant with __constants__. This allows the module list to be used in for loops which will unroll at compile time.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nclass SubModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(2))\n\n    def forward(self, input):\n        return self.weight + input\n\nclass MyModule(torch.nn.Module):\n    __constants__ = ['mods']\n\n    def __init__(self):\n        super().__init__()\n        self.mods = torch.nn.ModuleList([SubModule() for i in range(10)])\n\n    def forward(self, v):\n        for module in self.mods:\n            v = module(v)\n        return v\n\n\nm = torch.jit.script(MyModule())\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Vision Transformer Implementation\nDESCRIPTION: This code snippet represents a log of PyTorch operators used in a Vision Transformer model execution, showing call counts (cnt) and tensor shapes for each operation. The operators include matrix multiplications, convolutions, normalizations, and various tensor manipulations with predominantly float16 (f16) precision tensors. The pattern reveals 24 transformer blocks with self-attention operations and feed-forward networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gmixer_24_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 24, ((T([64, 384, 384], f16), [64, 384, 384]), {})\ncnt: 24, ((T([64, 384, 196], f16), [24576, 196]), {})\nOperator: aten.add.Tensor\ncnt: 24, ((T([64, 384, 384], f16), T([384], f16)), {})\ncnt: 24, ((T([64, 196, 384], f16, stride=(75264, 1, 196)), T([64, 196, 384], f16, stride=(75264, 1, 196))), {})\ncnt: 24, ((T([64, 196, 384], f16, stride=(75264, 1, 196)), T([64, 196, 384], f16)), {})\ncnt: 24, ((T([64, 196, 384], f16), T([64, 196, 384], f16)), {})\ncnt: 24, ((T([64, 196, 384], f16), T([64, 196, 384], f16, stride=(75264, 1, 196))), {})\nOperator: aten.addmm.default\ncnt: 24, ((T([196], f16), T([24576, 192], f16), T([192, 196], f16, stride=(1, 192))), {})\ncnt: 24, ((T([1536], f16), T([12544, 384], f16), T([384, 1536], f16, stride=(1, 384))), {})\ncnt: 24, ((T([384], f16), T([12544, 768], f16), T([768, 384], f16, stride=(1, 768))), {})\ncnt: 1, ((T([1000], f16), T([64, 384], f16), T([384, 1000], f16, stride=(1, 384))), {})\nOperator: aten.bmm.default\ncnt: 24, ((T([64, 384, 196], f16, stride=(75264, 1, 384)), T([64, 196, 384], f16, stride=(0, 1, 196))), {})\ncnt: 24, ((T([64, 196, 384], f16), T([64, 384, 384], f16)), {})\ncnt: 24, ((T([64, 384, 384], f16), T([64, 384, 196], f16, stride=(0, 196, 1))), {})\nOperator: aten.cat.default\ncnt: 24, (([T([64, 196, 768], f16), T([64, 196, 768], f16)], 2), {})\ncnt: 24, (([T([64, 384, 192], f16), T([64, 384, 192], f16)], 2), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([384, 3, 16, 16], f16), T([384], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 384, 14, 14], f16, stride=(75264, 1, 5376, 384)), T([64, 3, 224, 224], f16), T([384, 3, 16, 16], f16), [384], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\ncnt: 24, ((T([384, 196], f16), T([384, 196], f16, stride=(1, 384))), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([64, 196, 384], f16, stride=(384, 0, 1)), 196), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([64], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([64, 196, 384], f16), [1]), {})\nOperator: aten.mm.default\ncnt: 1, ((T([64, 1000], f16), T([1000, 384], f16)), {})\ncnt: 1, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 384], f16)), {})\ncnt: 24, ((T([12544, 384], f16), T([384, 768], f16)), {})\ncnt: 24, ((T([384, 12544], f16, stride=(1, 384)), T([12544, 768], f16)), {})\ncnt: 24, ((T([12544, 1536], f16), T([1536, 384], f16)), {})\ncnt: 24, ((T([1536, 12544], f16, stride=(1, 1536)), T([12544, 384], f16)), {})\ncnt: 24, ((T([24576, 196], f16), T([196, 192], f16)), {})\ncnt: 24, ((T([196, 24576], f16, stride=(1, 196)), T([24576, 192], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 24, ((T([64, 384, 192], f16, stride=(147456, 384, 1)), T([64, 384, 192], f16)), {})\ncnt: 24, ((T([64, 196, 768], f16, stride=(301056, 1536, 1)), T([64, 196, 768], f16)), {})\ncnt: 24, ((T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(301056, 1536, 1))), {})\ncnt: 24, ((T([64, 196, 768], f16), T([64, 196, 768], f16)), {})\ncnt: 24, ((T([64, 384, 192], f16), T([64, 384, 192], f16, stride=(147456, 384, 1))), {})\ncnt: 24, ((T([64, 384, 192], f16), T([64, 384, 192], f16)), {})\nOperator: aten.native_layer_norm.default\ncnt: 49, ((T([64, 196, 384], f16, stride=(75264, 1, 196)), [384], T([384], f16), T([384], f16), 1e-06), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 25, ((T([64, 196, 384], f16), T([64, 196, 384], f16, stride=(75264, 1, 196)), [384], T([64, 196, 1], f32), T([64, 196, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})\ncnt: 24, ((T([64, 196, 384], f16, stride=(75264, 1, 196)), T([64, 196, 384], f16, stride=(75264, 1, 196)), [384], T([64, 196, 1], f32), T([64, 196, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})\nOperator: aten.new_empty_strided.default\ncnt: 24, ((T([384, 196], f16, stride=(1, 384)), [384, 196], [196, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\nOperator: aten.silu.default\ncnt: 24, ((T([64, 384, 192], f16, stride=(147456, 384, 1)),), {})\ncnt: 24, ((T([64, 196, 768], f16, stride=(301056, 1536, 1)),), {})\nOperator: aten.silu_backward.default\ncnt: 24, ((T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(301056, 1536, 1))), {})\ncnt: 24, ((T([64, 384, 192], f16), T([64, 384, 192], f16, stride=(147456, 384, 1))), {})\nOperator: aten.split.Tensor\ncnt: 24, ((T([64, 384, 384], f16), 192, -1), {})\ncnt: 24, ((T([64, 196, 1536], f16), 768, -1), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\ncnt: 24, ((T([12544, 384], f16), [0], True), {})\ncnt: 24, ((T([12544, 1536], f16), [0], True), {})\ncnt: 24, ((T([24576, 196], f16), [0], True), {})\ncnt: 24, ((T([64, 384, 384], f16), [0, 1], True), {})\ncnt: 24, ((T([64, 196, 384], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Specifying Dispatch for Tensor Functions in PyTorch\nDESCRIPTION: This snippet specifies how to define the dispatch rules for Tensor operations in PyTorch C++, allowing different backend functions to be used depending on the Tensor's backend (e.g., CPU, CUDA). It details the format for dispatch specifications and supports custom namespaces up to two levels deep, enhancing function resolution in diverse namespaces.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\ndispatch:\n    CPU: func_cpu\n    CUDA: func_cuda\n```\n\nLANGUAGE: C++\nCODE:\n```\ndispatch:\n    CPU: custom::ns::func_cpu\n```\n\n----------------------------------------\n\nTITLE: Accessing Nested Tensor Components in Python\nDESCRIPTION: This example illustrates how to directly access the values and offsets of a nested tensor using `values()` and `offsets()` methods. It also shows constructing an NJT from jagged `values` and `offsets` directly.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> a = torch.randn(50, 128) # text 1\n>>> b = torch.randn(32, 128) # text 2\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.values().shape  # note the \"packing\" of the ragged dimension; no padding needed\ntorch.Size([82, 128])\n>>> nt.offsets()\ntensor([ 0, 50, 82])\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> values = torch.randn(82, 128)\n>>> offsets = torch.tensor([0, 50, 82], dtype=torch.int64)\n>>> nt = torch.nested.nested_tensor_from_jagged(values=values, offsets=offsets)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Multiplication Operations\nDESCRIPTION: Matrix multiplication operations showing various tensor shapes and stride patterns commonly used in neural network layers. Documents the linear transformations between different dimensional spaces.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 4, ((T([25088, 128], f16), T([128, 128], f16, stride=(1, 128))), {})\ncnt: 4, ((T([25088, 256], f16), T([256, 128], f16, stride=(1, 256))), {})\n```\n\n----------------------------------------\n\nTITLE: Detecting Execution Inside a Package\nDESCRIPTION: Shows how to check if code is being executed inside a package by checking for the __torch_package__ attribute. This allows for different behavior based on the execution context.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# In foo/bar.py:\n\nif \"__torch_package__\" in dir():  # true if the code is being loaded from a package\n    def is_in_package():\n        return True\n\n    UserException = Exception\nelse:\n    def is_in_package():\n        return False\n\n    UserException = UnpackageableException\n```\n\n----------------------------------------\n\nTITLE: Documenting Measurement Class in PyTorch Benchmark Utils\nDESCRIPTION: This snippet documents the Measurement class from the torch.utils.benchmark module, including all its members.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: Measurement\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Calculating Threshold Backward in PyTorch\nDESCRIPTION: This snippet demonstrates the usage of threshold backward operations on tensors with various shapes and strides. It shows the count of operations for each unique tensor configuration and includes details on input gradients and thresholds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 2, ((T([32, 1024, 7, 7], f16), T([32, 1024, 7, 7], f16), 0), {})\ncnt: 1, ((T([32, 224, 7, 7], f16, stride=(105056, 49, 7, 1)), T([32, 224, 7, 7], f16), 0), {})\ncnt: 8, ((T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), 0), {})\ncnt: 1, ((T([32, 224, 7, 7], f16, stride=(92512, 49, 7, 1)), T([32, 224, 7, 7], f16), 0), {})\ncnt: 2, ((T([32, 768, 14, 14], f16), T([32, 768, 14, 14], f16), 0), {})\ncnt: 1, ((T([32, 192, 14, 14], f16, stride=(338688, 196, 14, 1)), T([32, 192, 14, 14], f16), 0), {})\ncnt: 8, ((T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), 0), {})\ncnt: 1, ((T([32, 192, 14, 14], f16, stride=(288512, 196, 14, 1)), T([32, 192, 14, 14], f16), 0), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16), 0), {})\ncnt: 1, ((T([32, 160, 28, 28], f16, stride=(827904, 784, 28, 1)), T([32, 160, 28, 28], f16), 0), {})\ncnt: 4, ((T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), 0), {})\ncnt: 1, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16), 0), {})\ncnt: 1, ((T([32, 128, 56, 56], f16, stride=(2408448, 3136, 56, 1)), T([32, 128, 56, 56], f16), 0), {})\ncnt: 5, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), 0), {})\ncnt: 2, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Testing torch.cond with Different Input Shapes\nDESCRIPTION: Demonstrates how to test the DynamicShapeCondPredicate model with different input shapes to verify the conditional behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cond.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ninp = torch.randn(3)\ninp2 = torch.randn(5)\nassert torch.equal(dyn_shape_mod(inp), false_fn(inp))\nassert torch.equal(dyn_shape_mod(inp2), true_fn(inp2))\n```\n\n----------------------------------------\n\nTITLE: Initializing Kernel Dispatch in C++\nDESCRIPTION: This snippet showcases how to declare a kernel dispatch using the DECLARE_DISPATCH macro in a header file. The macro requires a function pointer type and a dispatch registry name to ensure the kernel is correctly registered. The snippet highlights the necessary steps to set up dispatch for multiple CPU architectures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cpu/README.md#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nDECLARE_DISPATCH(fn_type, fnNameImpl)\n```\n\n----------------------------------------\n\nTITLE: Computing Backward Through Threshold (aten.threshold_backward.default) in PyTorch (Python)\nDESCRIPTION: This snippet denotes calls to the threshold_backward operation, typically arising from autograd support for thresholded activations like ReLU. It accepts input and grad_output tensors (matched in shape and dtype) and a threshold value, processing many shapes/sizes. Uses float16, and the integer threshold argument is often zero for ReLU; relies on PyTorch's autograd system.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([32, 1280, 7, 7], f16), T([32, 1280, 7, 7], f16), 0), {})\ncnt: 8, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16), 0), {})\ncnt: 1, ((T([32, 576, 7, 7], f16), T([32, 576, 7, 7], f16), 0), {})\ncnt: 3, ((T([32, 576, 14, 14], f16), T([32, 576, 14, 14], f16), 0), {})\ncnt: 6, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16), 0), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), T([32, 240, 14, 14], f16), 0), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([32, 240, 28, 28], f16), 0), {})\ncnt: 4, ((T([32, 120, 28, 28], f16), T([32, 120, 28, 28], f16), 0), {})\ncnt: 1, ((T([32, 72, 28, 28], f16), T([32, 72, 28, 28], f16), 0), {})\ncnt: 5, ((T([32, 72, 56, 56], f16), T([32, 72, 56, 56], f16), 0), {})\ncnt: 1, ((T([32, 48, 56, 56], f16), T([32, 48, 56, 56], f16), 0), {})\ncnt: 1, ((T([32, 48, 112, 112], f16), T([32, 48, 112, 112], f16), 0), {})\ncnt: 2, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Handling Weights Only torch.load Failure and Whitelisting Classes - PyTorch - Python\nDESCRIPTION: Shows the error message and steps necessary to recover from a torch.load(weights_only=True) failure, including re-running with weights_only=False or allowlisting required classes and functions using torch.serialization.safe_globals. No execution context; relevant for debugging safe loading of PyTorch model checkpoints and managing deserialization security. No execution dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded,\nto do so you have two options, do those steps only if you trust the source of the checkpoint.\n    1. Re-running `torch.load` with `weights_only` set to `False` will likely succeed,\n        but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n    2. Alternatively, to load with `weights_only=True` please check the recommended\n       steps in the following error message.\n       WeightsUnpickler error: Unsupported global: GLOBAL {__module__}.{__name__} was not an allowed global by\n       default. Please use `torch.serialization.add_safe_globals([{__name__}])` or the\n       `torch.serialization.safe_globals([{__name__}])` context manager to allowlist this global\n       if you trust this class/function.\n```\n\n----------------------------------------\n\nTITLE: Using Autocast in JIT Scripted Functions with CUDA\nDESCRIPTION: Demonstrates how to use an autocast context manager within a JIT scripted function to automatically convert operations from float32 to float16 on CUDA devices for performance optimization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.cuda.amp import autocast\n\n@torch.jit.script\ndef func(a, b):\n    with autocast():\n        return torch.mm(a, b)\n\na_float32 = torch.rand((8, 8), dtype=torch.float32, device=\"cuda\")\nb_float32 = torch.rand((8, 8), dtype=torch.float32, device=\"cuda\")\nresult = func(a_float32, b_float32)\nprint(result.dtype) # expecting torch.float16\n```\n\n----------------------------------------\n\nTITLE: Generated Graph with Static Shape in PyTorch Dynamo\nDESCRIPTION: This shows the FX graph generated by Dynamo for the first call to the compiled function `fn` where the input tensor shape `(4, 3)` is treated as static. The shape dimension `4` is hardcoded into the graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, l_a_: torch.Tensor, l_b_: torch.Tensor):\n    mul = 4 * l_a_\n    mul_1 = mul * l_b_\n    return (mul_1,)\n```\n\n----------------------------------------\n\nTITLE: Parallel addition with aten.add_.Tensor in PyTorch\nDESCRIPTION: The aten.add_.Tensor operator handles in-place tensor addition, modifying one tensor by adding another to it, with focus on large f16-based tensor dimensions. The operation requires PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\naten.add_.Tensor, ((T([16, 128, 30522], f16), T([30522], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Using file_structure() API to Explore Package Contents\nDESCRIPTION: Demonstrates how to use the file_structure() method of PackageImporter to explore and print the contents of a torch package with filtering options. Shows both filtered and unfiltered output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwith PackageExporter('my_package.pt') as pe:\n    pe.save_pickle('models', 'model_1.pkl', mod)\n\nimporter = PackageImporter('my_package.pt')\n# can limit printed items with include/exclude args\nprint(importer.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storage\"))\nprint(importer.file_structure()) # will print out all files\n```\n\n----------------------------------------\n\nTITLE: Implementing Upgrader Logic in Python\nDESCRIPTION: Shows the Python implementation of the upgrader logic for the 'linspace' operator, handling cases where 'steps' is None by defaulting to 100 steps.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef linspace_0_7(start: Union[int, float, complex], end: Union[int, float, complex], steps: Optional[int], *, dtype: Optional[int], layout: Optional[int],\n                    device: Optional[Device], pin_memory: Optional[bool]):\n    if (steps is None):\n      return torch.linspace(start=start, end=end, steps=100, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory)\n    return torch.linspace(start=start, end=end, steps=steps, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory)\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.layer_norm with Tensor Arguments (Text)\nDESCRIPTION: This section logs calls to an implicit operator, likely `aten::layer_norm`, based on the argument structure. It shows various invocations with different input tensor shapes (e.g., [1, 3072, 1536]), weight/bias shapes, and data types (f16 for inputs/weights, f32 for running stats). Key arguments include the input tensor, normalized shape (implicitly derived from weight/bias), weight, bias, running mean, running variance, elementwise_affine flag (True), epsilon (1e-05), and potentially cudnn_enable flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 3072, 1536], f16), T([1, 3072, 1536], f16), T([3072], f16), None, None, T([3072], f32), T([3072], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 9, ((T([1, 1536, 768], f16), T([1, 1536, 768], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 18, ((T([1, 768, 1152], f16), T([1, 768, 1152], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 8, ((T([1, 768, 1536], f16), T([1, 768, 1536], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 1536, 1536], f16), T([1, 1536, 1536], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 768, 512], f16), T([1, 768, 512], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 1536, 512], f16), T([1, 1536, 512], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([1, 512, 256], f16), T([1, 512, 256], f16), T([512], f16), None, None, T([512], f32), T([512], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 4, ((T([1, 256, 1152], f16), T([1, 256, 1152], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 256, 512], f16), T([1, 256, 512], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 256, 256], f16), T([1, 256, 256], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([1, 256, 128], f16), T([1, 256, 128], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([1, 128, 1152], f16), T([1, 128, 1152], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 128, 128], f16), T([1, 128, 128], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 128, 576], f16), T([1, 128, 576], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 64, 288], f16), T([1, 64, 288], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 32, 144], f16), T([1, 32, 144], f16), T([32], f16), None, None, T([32], f32), T([32], f32), True, 1e-05, [True, True, False]), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1, 16, 27], f16), T([1, 16, 27], f16), T([16], f16), None, None, T([16], f32), T([16], f32), True, 1e-05, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Backpropagation Operations in PyTorch CNN Model\nDESCRIPTION: This code shows the backpropagation operations for the convolutional layers of the model. It demonstrates gradient flow through convolutions with various kernel sizes, handling residual connections, and propagating gradients across different feature scales.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 4, ((T([128, 768, 7, 7], f16), T([128, 3072, 7, 7], f16), T([768, 3072, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 3072, 7, 7], f16), T([128, 768, 7, 7], f16), T([3072, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 768, 7, 7], f16), T([128, 768, 7, 7], f16), T([768, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 2304, 7, 7], f16), T([128, 768, 7, 7], f16), T([2304, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 768, 7, 7], f16), T([128, 384, 14, 14], f16), T([768, 384, 2, 2], f16), [768], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 4, ((T([128, 384, 14, 14], f16), T([128, 1536, 14, 14], f16), T([384, 1536, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 1536, 14, 14], f16), T([128, 384, 14, 14], f16), T([1536, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 1152, 14, 14], f16), T([128, 384, 14, 14], f16), T([1152, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 384, 14, 14], f16), T([128, 192, 28, 28], f16), T([384, 192, 2, 2], f16), [384], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 7, ((T([128, 192, 28, 28], f16), T([128, 384, 28, 28], f16), T([192, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 7, ((T([128, 384, 28, 28], f16), T([128, 384, 28, 28], f16), T([384, 48, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 8, [True, True, False]), {})\ncnt: 7, ((T([128, 384, 28, 28], f16), T([128, 192, 28, 28], f16), T([384, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([128, 32, 112, 112], f16), T([192, 32, 4, 4], f16), [192], [4, 4], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 3, 224, 224], f16), T([32, 3, 7, 7], f16), [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Using experimental context_parallel feature\nDESCRIPTION: Applies context-based parallelism to a module using the experimental context_parallel function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ncontext_parallel(module, device_mesh, parallel_mode=\"tensor\", sharding_strategy=None)\n```\n\n----------------------------------------\n\nTITLE: Accessing cuFFT Plan Cache in PyTorch CUDA Backend\nDESCRIPTION: This code snippet demonstrates how to access the cuFFT plan cache for a specific CUDA device using torch.backends.cuda.cufft_plan_cache.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/backends.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch.backends.cuda.cufft_plan_cache[i]\n```\n\n----------------------------------------\n\nTITLE: PyTorch Nearest Neighbor Upsampling Operations\nDESCRIPTION: Usage of aten.upsample_nearest2d.vec operator showing progressive upsampling of feature maps with specified output sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.upsample_nearest2d.vec\ncnt: 1, ((T([4, 256, 37, 38], f16), [74, 76], None), {})\ncnt: 1, ((T([4, 256, 74, 76], f16), [148, 152], None), {})\ncnt: 1, ((T([4, 256, 148, 152], f16), [296, 304], None), {})\n```\n\n----------------------------------------\n\nTITLE: Basic Name Inference for Tensor Addition\nDESCRIPTION: Explains name inference rules through adding one-dimensional tensors, showcasing scenarios of matching and unifying names, where tensors x, y, and z are combined with match checks and potential errors if names do not align.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nx = torch.randn(3, names=('X',))\ny = torch.randn(3)\nz = torch.randn(3, names=('Z',))\n# x + y  # match('X', None) is True\n# x + z  # match('X', 'Z') is False\n# x + x  # match('X', 'X') is True\n\nx + z\n\n(x + y).names\n(x + x).names\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom IterDataPipe in Python\nDESCRIPTION: Creates an example IterDataPipe class called ExampleIterPipe that yields a range of integers. This class is used in subsequent examples to demonstrate various DataPipe operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Example IterDataPipe\nclass ExampleIterPipe(IterDataPipe):\n    def __init__(self, range = 20):\n        self.range = range\n    def __iter__(self):\n        for i in range(self.range):\n            yield i\n```\n\n----------------------------------------\n\nTITLE: Library Requirements Documentation\nDESCRIPTION: Specifies the required library versions for running the differential privacy examples with ResNet18. Requires Opacus version 1.0.1 and torchvision 0.11.2.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/examples/dp_cifar10/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n### Requirements\\nThese examples use Opacus version 1.0.1 and torchvision 0.11.2\n```\n\n----------------------------------------\n\nTITLE: Initializing Local AutoHeuristic Constructor in PyTorch\nDESCRIPTION: Constructor call for local autotuning use case where feedback function returns immediate results. Used for pad_mm optimization with fallback, choices, feedback function and context parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nautoheuristic = AutoHeuristic(\n    fallback=fallback,\n    choices=choices,\n    feedback=feedback,\n    context=context,\n    name=name,\n    augment_context=pad_mm_operations(),\n    precondition=pad_mm_precondition,\n)\n```\n\n----------------------------------------\n\nTITLE: Operators Incompatible with Deterministic Algorithms in PyTorch CUDAGraph\nDESCRIPTION: PyTorch operators that become incompatible with CUDAGraph when deterministic algorithms are enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\naten._fused_moving_avg_obs_fq_helper.default\naten._fused_moving_avg_obs_fq_helper_functional.default\naten.multinomial.default\nfbgemm.dense_to_jagged.default\nfbgemm.jagged_to_padded_dense.default\nrun_and_save_rng_state\nrun_with_rng_state\naten._local_scalar_dense\naten._assert_scalar\n```\n\n----------------------------------------\n\nTITLE: Launching Elastic Distributed Training Job with torchrun in Bash\nDESCRIPTION: Command to launch an elastic distributed training job using torchrun. It specifies a range of nodes, processes per node, maximum restarts, rendezvous ID, backend, and endpoint. The command should be run on at least MIN_SIZE and at most MAX_SIZE nodes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/quickstart.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun\n    --nnodes=MIN_SIZE:MAX_SIZE\n    --nproc-per-node=TRAINERS_PER_NODE\n    --max-restarts=NUM_ALLOWED_FAILURES_OR_MEMBERSHIP_CHANGES\n    --rdzv-id=JOB_ID\n    --rdzv-backend=c10d\n    --rdzv-endpoint=HOST_NODE_ADDR\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files for backend_with_compiler Library in CMake\nDESCRIPTION: Sets a CMake variable `BACKEND_WITH_COMPILER_SRCS` containing the list of source files for the `backend_with_compiler` library. It conditionally appends `profiler_edge.cpp` to the list if the `USE_KINETO` flag is enabled, allowing for edge profiler testing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(BACKEND_WITH_COMPILER_SRCS\n  ${JIT_TEST_ROOT}/test_backend_compiler_lib.cpp\n  ${JIT_TEST_ROOT}/test_backend_compiler_preprocess.cpp\n)\nif(USE_KINETO)\n  # Testing edge profiler for backend use\n  # profiler_edge should only be aded when USE_KINETO flag is on\n  list(APPEND BACKEND_WITH_COMPILER_SRCS\n    ${TORCH_SRC_DIR}/csrc/jit/mobile/profiler_edge.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Describing PyTorch nll_loss_forward and nll_loss_backward Calls - Python\nDESCRIPTION: These snippets specify argument patterns for negative log-likelihood loss operators in PyTorch, showing combinations of predicted logits, target indices, optional weights, reduction type, ignore index, and output tensors. Arguments include float16 tensors for prediction and output, int64 tensors for indices, and constants matching operator signatures. Intended for coverage of varied NLL loss forward and backward usages in PyTorch. Assumes dependency on PyTorch operators, with shapes matching typical classification tasks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([32, 1000], f16), T([32], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([32, 1000], f16), T([32], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Loops with prim::Loop in TorchScript IR\nDESCRIPTION: Demonstrates the structure of a prim::Loop node in TorchScript IR, which is used to implement both while and for loops. It shows the general structure and explains the semantics using Python-like pseudocode.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%y_1, ..., %y_r = prim::Loop(%max_trip_count, %initial_condition, %x_1, ..., %x_r)\n  block0(%i, %a_1, ..., %a_r):\n    %b_1, ..., %b_m = some::node(%a_value_from_outer_block, %a_1)\n    %iter_condition = some::other_node(%a_2)\n    -> (%iter_condition, %b_1, ..., %b_r)\n```\n\nLANGUAGE: python\nCODE:\n```\ny_1, ..., y_r = x_1, ..., x_r\ncondition = initial_condition\ni = 0\nwhile condition and i < max_trip_count:\n    a_1, ..., a_r = y_1, ..., y_r\n\n    ############################################################\n    # Actual body of the loop\n    b_1, ..., b_m = some::node(a_value_from_outside_of_the_loop, a_1)\n    iter_condition = some::node(a_2)\n    ############################################################\n\n    y_1, ..., y_r = b_1, ..., b_r\n    condition = iter_condition\n    i += 1\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific Library Dependencies\nDESCRIPTION: Handles Unix-specific library dependencies, particularly for rt (real-time) library and pthread support on non-Apple systems.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/lib/libshm/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(UNIX AND NOT APPLE)\n  include(CheckLibraryExists)\n  find_package(Threads REQUIRED)\n  check_library_exists(rt clock_gettime \"time.h\" NEED_LIBRT)\n  if(NEED_LIBRT)\n    target_link_libraries(shm PUBLIC rt)\n  else()\n    message(STATUS \"Checking if rt requires pthread\")\n    set(CMAKE_REQUIRED_LIBRARIES Threads::Threads)\n    check_library_exists(rt shm_open \"sys/mman.h\" NEED_RT_AND_PTHREAD)\n    unset(CMAKE_REQUIRED_LIBRARIES)\n    if(NEED_RT_AND_PTHREAD)\n      message(STATUS \"Needs it, linking against pthread and rt\")\n      target_link_libraries(shm PUBLIC rt Threads::Threads)\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TF32 vs FP32 Matmul Precision and Speed in PyTorch (Python)\nDESCRIPTION: Provides an example comparing the speed and precision of matrix multiplication using TensorFloat-32 (TF32) versus standard FP32. It demonstrates enabling TF32 via `torch.backends.cuda.matmul.allow_tf32`, performing the matmul, calculating the error relative to a double-precision baseline, and then repeating with TF32 disabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\na_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\nb_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\nab_full = a_full @ b_full\nmean = ab_full.abs().mean()  # 80.7277\n\na = a_full.float()\nb = b_full.float()\n\n# Do matmul at TF32 mode.\ntorch.backends.cuda.matmul.allow_tf32 = True\nab_tf32 = a @ b  # takes 0.016s on GA100\nerror = (ab_tf32 - ab_full).abs().max()  # 0.1747\nrelative_error = error / mean  # 0.0022\n\n# Do matmul with TF32 disabled.\ntorch.backends.cuda.matmul.allow_tf32 = False\nab_fp32 = a @ b  # takes 0.11s on GA100\nerror = (ab_fp32 - ab_full).abs().max()  # 0.0031\nrelative_error = error / mean  # 0.000039\n```\n\n----------------------------------------\n\nTITLE: Analyzing NLL Loss Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of nll_loss_backward.default and nll_loss_forward.default operators with specific tensor shapes and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swsl_resnext101_32x16d_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([], f16), T([32, 1000], f16), T([32], i64), None, 1, -100, T([], f16)), {})\ncnt: 1, ((T([32, 1000], f16), T([32], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Using Result on Non-default CUDA Stream in Python\nDESCRIPTION: Demonstrates using a specified CUDA stream for operations in PyTorch, ensuring synchronization with the default stream to prevent non-deterministic results when performing operations like `output.add_(100)`. Requires `torch.cuda` functionalities enabled with a CUDA-capable device.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n```\n\n----------------------------------------\n\nTITLE: Calculating Wirtinger and Conjugate Wirtinger Gradients for Complex Gradcheck (PyTorch, Python)\nDESCRIPTION: This code snippet numerically estimates the Wirtinger (w_d) and Conjugate Wirtinger (conj_w_d) derivatives for a scalar complex-to-real function, in the context of PyTorch's gradcheck. It applies finite differences in both real and imaginary directions, combines them to get the required Wirtinger forms, and computes the corresponding gradient contribution using grad_out values. Dependencies include a compute_gradient callable, and it assumes grad_out=1 as commonly used in PyTorch gradcheck logic. The snippet outputs the conjugate Wirtinger gradient scaled appropriately for the gradcheck mechanism.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/gradcheck.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105\\n# Notation changes in this code block:\\n# s here is y above\\n# x, y here are a, b above\\n\\nds_dx = compute_gradient(eps)\\nds_dy = compute_gradient(eps * 1j)\\n# conjugate wirtinger derivative\\nconj_w_d = 0.5 * (ds_dx + ds_dy * 1j)\\n# wirtinger derivative\\nw_d = 0.5 * (ds_dx - ds_dy * 1j)\\nd[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj()\\n\\n# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.\n```\n\n----------------------------------------\n\nTITLE: Defining Tree View for Apply Node in C++\nDESCRIPTION: Definition of the Apply tree view in C++, which provides named accessors for the subtrees of an apply node, including the callee, inputs, and attributes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nstruct Apply : public Expr {\n  Expr callee() const {\n    return Expr(subtree(0));\n  }\n  List<Expr> inputs() const {\n    return List<Expr>(subtree(1));\n  }\n  List<Attribute> attributes() const {\n    return List<Attribute>(subtree(2));\n  ...\n};\n```\n\n----------------------------------------\n\nTITLE: QConfig Mapping and Backend Configuration\nDESCRIPTION: Example of QConfig mapping and backend configuration for qat_linear_relu node, showing dtype specifications for weights, input and output activations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# qconfig_mapping (simplified, shown as dict)\n{'qat_linear_relu': QConfig(\n  weight=MinMaxObserver.with_args(dtype=torch.qint8),\n  input_activation=HistogramObserver.with_args(dtype=torch.quint8),\n  output_activation=PlaceholderObserver.with_args(dtype=torch.float32),\n)}\n\n# backend_config (simplified)\n{\n  'pattern': nnqat.LinearReLU,\n  'dtype_configs': [{input: torch.quint8, output: torch.float32, weight: torch.qint8}],\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Convolution with ATen Convolution Operator\nDESCRIPTION: Details `aten.convolution`, crucial for feature extraction in neural networks using filters of various sizes. Fueled by input-output combinations like [64, 3, 256, 256] and [24, 3, 3, 3], supporting multiple kernel sizes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 256, 256], f16), T([24, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Handling Graph Breaks in CUDA Execution\nDESCRIPTION: Example showing how to handle graph breaks when executing NumPy code on CUDA using device context manager.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    prod = X[:, :, None] * Y[:, None, :]\n    print(\"oops, a graph break!\")\n    return np.sum(prod, axis=(-2, -1))\n\nX = torch.randn(1024, 64, device=\"cuda\")\nY = torch.randn(1024, 64, device=\"cuda\")\n\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nassert Z.device.type == \"cuda\"\n```\n\n----------------------------------------\n\nTITLE: Importing checkpoint module in PyTorch\nDESCRIPTION: Shows how to import the checkpoint module from torch.utils. This module provides functions and classes for implementing checkpointing in PyTorch models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/checkpoint.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch.utils.checkpoint\n```\n\n----------------------------------------\n\nTITLE: Saving and Restoring Module State Dicts - PyTorch - Python\nDESCRIPTION: Demonstrates extracting the state_dict of a torch.nn.Module (here, BatchNorm1d), which includes both parameters and buffers, saving it to disk, and restoring it to a freshly initialized module. Relies on torch and its nn submodule. Inputs include a module with learnable parameters and persistent buffers; outputs are a file containing the state_dict and a restored module state. This method is more robust and compatible than saving entire modules directly. Inputs and outputs are dictionaries mapping parameter/buffer names to tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n>>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> list(bn.named_parameters())\n[('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),\n ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]\n\n>>> list(bn.named_buffers())\n[('running_mean', tensor([0., 0., 0.])),\n ('running_var', tensor([1., 1., 1.])),\n ('num_batches_tracked', tensor(0))]\n\n>>> bn.state_dict()\nOrderedDict([('weight', tensor([1., 1., 1.])),\n             ('bias', tensor([0., 0., 0.])),\n             ('running_mean', tensor([0., 0., 0.])),\n             ('running_var', tensor([1., 1., 1.])),\n             ('num_batches_tracked', tensor(0))])\n```\n\n----------------------------------------\n\nTITLE: Checking Intel GPU Availability\nDESCRIPTION: Demonstrates how to check if an Intel GPU is available using PyTorch's API. If unavailable, users should double-check their Intel GPU driver installation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\ntorch.xpu.is_available()  # torch.xpu is the API for Intel GPU support\n```\n\n----------------------------------------\n\nTITLE: Legacy Device Construction Using Device Ordinals in PyTorch\nDESCRIPTION: This code snippet shows the legacy method of constructing a device by providing a single device ordinal, which is treated as the current accelerator type (typically CUDA).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.device(1)\ndevice(type='cuda', index=1)\n```\n\n----------------------------------------\n\nTITLE: Interning Multiple Module Patterns\nDESCRIPTION: Example of interning multiple module patterns at once in torch.package, which allows selecting specific submodules from a package to include in the export.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nexporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"])\n```\n\n----------------------------------------\n\nTITLE: Computing Jacobian Row-by-Row Using Autograd - Python\nDESCRIPTION: Implements 'compute_jac' to calculate the Jacobian of the 'predict' function with respect to input using PyTorch's autograd.grad in a row-wise fashion. Expects xp (input vector) and uses a global 'unit_vectors' variable. Returns the Jacobian matrix by stacking gradients for each unit vector. Requires 'predict', 'weight', 'bias' in scope, and 'unit_vectors' to be initialized as identity matrix.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef compute_jac(xp):\n    jacobian_rows = [torch.autograd.grad(predict(weight, bias, xp), xp, vec)[0]\n                     for vec in unit_vectors]\n    return torch.stack(jacobian_rows)\n```\n\n----------------------------------------\n\nTITLE: New API for Module Attributes in TorchScript\nDESCRIPTION: Shows the newer approach for defining module attributes in TorchScript using Python type annotations instead of torch.jit.Attribute.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict\n\nclass MyModule(torch.nn.Module):\n    my_dict: Dict[str, int]\n\n    def __init__(self):\n        super().__init__()\n        # This type cannot be inferred and must be specified\n        self.my_dict = {}\n\n        # The attribute type here is inferred to be `int`\n        self.my_int = 20\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())\n```\n\n----------------------------------------\n\nTITLE: Defining Matrix Multiplication Benchmark with GroupedStmts\nDESCRIPTION: Example of defining a benchmark using GroupedStmts to test matrix multiplication in both Python and C++. It includes setup code, specifies a function signature, and enables TorchScript and autograd variants.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# `GroupedStmts` is an alias of `GroupedBenchmark.init_from_stmts`\nbenchmark = GroupedStmts(\n    py_stmt=r\"y = x * w\",\n    cpp_stmt=r\"auto y = x * w;\",\n\n    setup=GroupedSetup(\n        py_setup=\"\"\"\n            x = torch.ones((4, 4))\n            w = torch.ones((4, 4), requires_grad=True)\n        \"\"\",\n        cpp_setup=\"\"\"\n            auto x = torch::ones((4, 4));\n            auto w = torch::ones((4, 4));\n            w.set_requires_grad(true);\n        \"\"\",\n    ),\n\n    signature=\"f(x, w) -> y\",\n    torchscript=True,\n    autograd=True,\n),\n```\n\n----------------------------------------\n\nTITLE: Using TorchScript Classes with Function Definitions\nDESCRIPTION: Demonstrates how to define and use a TorchScript class with the torch.jit.script decorator. The example creates a Pair class with two tensor members and a function to sum them.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# Declare a TorchScript class\n@torch.jit.script\nclass Pair:\n  def __init__(self, first, second):\n    self.first = first\n    self.second = second\n\n@torch.jit.script\ndef sum_pair(p):\n  # type: (Pair) -> Tensor\n  return p.first + p.second\n\np = Pair(torch.rand(2, 3), torch.rand(2, 3))\nprint(sum_pair(p))\n```\n\n----------------------------------------\n\nTITLE: Non-batchwise Operation Fallback for PyTorch NJT\nDESCRIPTION: This snippet suggests using an 'unbind()'-based fallback for non-batchwise operations on Nested Jagged Tensors in PyTorch. This approach can help quickly implement operations that don't operate over the batch dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# For non-batchwise operations\n# Use unbind()-based fallback\n# Example:\n# result = torch.stack([op(tensor) for tensor in njt.unbind()])\n```\n\n----------------------------------------\n\nTITLE: vmap with Different Randomness Behavior\nDESCRIPTION: This example demonstrates using vmap with 'different' randomness flag, where random values are generated independently for each element in the batch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef add_noise(x):\n  y = torch.randn(())  # y will be different across the batch\n  return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"different\")(x)  # we get 3 different values\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations in PyTorch\nDESCRIPTION: This extensive snippet shows various convolution operations with different kernel sizes, strides, and padding. The operations process tensors through a typical CNN architecture with decreasing spatial dimensions and increasing channel counts (64128256512).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([96, 9, 128, 128], f16), T([64, 9, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 64, 64, 64], f16), T([64, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([96, 64, 32, 32], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 64, 64, 64], f16), T([64, 64, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 64, 32, 32], f16), T([128, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([96, 128, 16, 16], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 64, 32, 32], f16), T([128, 64, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 128, 16, 16], f16), T([256, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([96, 256, 8, 8], f16), T([256, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 128, 16, 16], f16), T([256, 128, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 256, 8, 8], f16), T([512, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([96, 512, 4, 4], f16), T([512, 512, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 256, 8, 8], f16), T([512, 256, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows the configuration of batch normalization operations used in the model. It includes input tensor shapes, normalization parameters, and the frequency of these operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 8, 112, 112], f16), T([8], f16), T([8], f16), T([8], f16), T([8], f16), True, 0.1, 1e-05), {})\ncnt: 2, ((T([128, 24, 112, 112], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Applying PyTorch relu_ In-place Activation on Tensors - Python\nDESCRIPTION: These code blocks provide example argument patterns for in-place relu_ operator calls on float16 tensors of various shapes. Cases include large input tensors for batch processing (e.g., images or features). Useful for verifying in-place modification and broadcasting in PyTorch for relu_ activation. Requires float16 tensor support and basic PyTorch invocation compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 2, ((T([32, 64, 128, 128], f16),), {})\ncnt: 1, ((T([32, 128, 128, 128], f16),), {})\ncnt: 3, ((T([32, 64, 64, 64], f16),), {})\ncnt: 4, ((T([32, 128, 64, 64], f16),), {})\ncnt: 3, ((T([32, 32, 1, 1], f16),), {})\ncnt: 4, ((T([32, 256, 64, 64], f16),), {})\ncnt: 4, ((T([32, 64, 1, 1], f16),), {})\ncnt: 5, ((T([32, 512, 32, 32], f16),), {})\ncnt: 3, ((T([32, 128, 32, 32], f16),), {})\ncnt: 4, ((T([32, 256, 32, 32], f16),), {})\ncnt: 23, ((T([32, 128, 1, 1], f16),), {})\ncnt: 24, ((T([32, 1024, 16, 16], f16),), {})\ncnt: 22, ((T([32, 256, 16, 16], f16),), {})\ncnt: 23, ((T([32, 512, 16, 16], f16),), {})\ncnt: 3, ((T([32, 256, 1, 1], f16),), {})\ncnt: 3, ((T([32, 2048, 8, 8], f16),), {})\ncnt: 2, ((T([32, 512, 8, 8], f16),), {})\ncnt: 2, ((T([32, 1024, 8, 8], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Inserting Tracking and Module Declarations - reStructuredText\nDESCRIPTION: This snippet consists of reStructuredText comments indicating that certain modules still need documentation and inserting corresponding module declaration directives for Sphinx. It doesnt generate user-facing documentation but assists editors and build processes by marking intended module scope. No other dependencies except Sphinx; inputs are module paths, outputs are internal module declarations for doc build tools.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps.rst#2025-04-22_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. This module needs to be documented. Adding here in the meantime\n.. for tracking purposes\n.. py:module:: torch.mps.event\n.. py:module:: torch.mps.profiler\n```\n\n----------------------------------------\n\nTITLE: Advanced Tensor Reshaping with aten._unsafe_view in PyTorch (Python)\nDESCRIPTION: Demonstrates use of the _unsafe_view method for creating tensor views with new shapes, affecting memory structure without copy. Users must ensure reshaped tensors are compatible; improper usage can result in undefined behavior. Examples highlight reshaping for multi-head attention models, with key parameters being the input tensor and target shape.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([4, 12, 128, 64], f16), [48, 128, 64]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([4, 12, 64, 128], f16), [48, 64, 128]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([48, 128, 128], f16), [4, 12, 128, 128]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([48, 128, 64], f16), [4, 12, 128, 64]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 24, ((T([4, 128, 12, 64], f16), [4, 128, 768]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([4, 128, 768], f16), [512, 768]), {})\n```\n\n----------------------------------------\n\nTITLE: CMake Build Configuration\nDESCRIPTION: CMake configuration file for building the C++ inference example, including automatic model compilation and proper LibTorch linking.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\nproject(aoti_example)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(aoti_example inference.cpp model.pt2)\n\nadd_custom_command(\n    OUTPUT model.pt2\n    COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/model.py\n    DEPENDS model.py\n)\n\ntarget_link_libraries(aoti_example \"${TORCH_LIBRARIES}\")\nset_property(TARGET aoti_example PROPERTY CXX_STANDARD 17)\n```\n\n----------------------------------------\n\nTITLE: Exposing Custom PyTorch Operators to Caffe2 CUDA\nDESCRIPTION: This code shows how to make a custom PyTorch operator available to the Caffe2 frontend for CUDA execution. It uses a macro to expose the CUDA implementation of the operator to Caffe2.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\nC10_EXPORT_C10_OP_TO_CAFFE2_CUDA(\n    MyCaffe2OperatorName, \"my_namespace::my_op\")\n```\n\n----------------------------------------\n\nTITLE: Defining Complex Function Derivatives\nDESCRIPTION: Mathematical representation of complex function derivatives using Wirtinger calculus, showing the relationship between real and imaginary components.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_3\n\nLANGUAGE: math\nCODE:\n```\nf(z=x+yj) = u(x, y) + v(x, y)j\n```\n\n----------------------------------------\n\nTITLE: Specifying Static Linking for Android Builds in CMake\nDESCRIPTION: Inside the `if(ANDROID_ABI)` block, this section uses the previously defined `import_static_lib` function to import several core PyTorch and dependency static libraries (like libtorch, libc10, libnnpack, etc.). It then defines the `pytorch_jni_LIBS` variable with the list of libraries to link against, including fbjni and the imported static libraries. Linker flags (`-Wl,--gc-sections`, `-Wl,--whole-archive`, `-Wl,--no-whole-archive`) are used to control static linking behavior and potentially reduce binary size.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\n  import_static_lib(libtorch)\n  import_static_lib(libtorch_cpu)\n  import_static_lib(libc10)\n  import_static_lib(libnnpack)\n  import_static_lib(libXNNPACK)\n  import_static_lib(libmicrokernels-prod)\n  import_static_lib(libpytorch_qnnpack)\n  import_static_lib(libpthreadpool)\n  import_static_lib(libeigen_blas)\n  import_static_lib(libcpuinfo)\n  import_static_lib(libclog)\n\n  # Link most things statically on Android.\n  set(pytorch_jni_LIBS\n      fbjni\n      -Wl,--gc-sections\n      -Wl,--whole-archive\n      libtorch\n      libtorch_cpu\n      -Wl,--no-whole-archive\n      libc10\n      libnnpack\n      libXNNPACK\n      libmicrokernels-prod\n      libpytorch_qnnpack\n      libpthreadpool\n      libeigen_blas\n      libcpuinfo\n      libclog\n  )\n```\n\n----------------------------------------\n\nTITLE: Ragged Dimension Operations for PyTorch NJT\nDESCRIPTION: This snippet outlines a strategy for operations on the ragged dimension of Nested Jagged Tensors. It involves converting to padded dense format, running the operation, and converting back to NJT. It also mentions the potential for fusion in torch.compile to avoid materializing padded intermediates.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# For operations on ragged dimension\n# 1. Convert to padded dense with appropriate padding value\n# 2. Run the operation\n# 3. Convert back to NJT\n# Note: In torch.compile, these conversions can be fused\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Test Binary Compilation in CMake\nDESCRIPTION: Sets up test binary compilation for CUDA-related tests in PyTorch. Handles both Windows and non-Windows environments, configures test linking with CUDA libraries, and manages test installation settings. Includes special handling for HIP files on Windows platforms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/cuda/test/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(C10_CUDA_ALL_TEST_FILES\n    impl/CUDAAssertionsTest_1_var_test.cu\n    impl/CUDAAssertionsTest_catches_stream.cu\n    impl/CUDAAssertionsTest_catches_thread_and_block_and_device.cu\n    impl/CUDAAssertionsTest_from_2_processes.cu\n    impl/CUDAAssertionsTest_multiple_writes_from_blocks_and_threads.cu\n    impl/CUDAAssertionsTest_multiple_writes_from_multiple_blocks.cu\n    impl/CUDAAssertionsTest_multiple_writes_from_same_block.cu\n    impl/CUDATest.cpp\n)\nif(BUILD_TEST)\n  foreach(test_src ${C10_CUDA_ALL_TEST_FILES})\n    get_filename_component(test_file_name ${test_src} NAME_WE)\n    set(test_name \"c10_cuda_${test_file_name}\")\n    if(WIN32 AND test_src MATCHES \"^.*\\.hip$\")\n      set_source_files_properties(${test_src} PROPERTIES HIP_SOURCE_PROPERTY_FORMAT 1)\n      hip_add_executable(${test_name} \"${test_src}\")\n      set_target_properties(${test_name} PROPERTIES LINKER_LANGUAGE CXX HIP_ARCHITECTURES ${PYTORCH_ROCM_ARCH})\n    else()\n      add_executable(${test_name} \"${test_src}\")\n    endif()\n    target_link_libraries(${test_name} ${C10_CUDA_LIB} ${C10_LIB} gtest_main)\n    add_test(NAME ${test_name} COMMAND $<TARGET_FILE:${test_name}>)\n    if(INSTALL_TEST)\n      set_target_properties(${test_name} PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n      install(TARGETS ${test_name} DESTINATION test)\n    endif()\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using wrap_numpy Decorator for CUDA Tensor Operations\nDESCRIPTION: Demonstrates using torch.compiler.wrap_numpy decorator to handle CUDA tensors directly without data movement between CPU and GPU memory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile(fullgraph=True)\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = torch.randn(1024, 64, device=\"cuda\")\nY = torch.randn(1024, 64, device=\"cuda\")\nZ = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nassert Z.device.type == \"cuda\"\n```\n\n----------------------------------------\n\nTITLE: Tensor Cloning in PyTorch\nDESCRIPTION: This snippet shows a tensor clone operation used to create a copy of the input tensor. This is typically done to ensure the tensor is contiguous in memory or to prevent unexpected in-place modifications.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([96, 9, 128, 128], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dynamo's Dynamic Shape Handling\nDESCRIPTION: Illustrates Dynamo's ability to handle dynamic shapes by tracing integer inputs symbolically, allowing for flexible input sizes without recompilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_deepdive.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef fn(x, n):\n    y = x ** 2\n    if n >= 0:\n        return (n + 1) * y\n    else:\n        return y / n\n\nx = torch.randn(200)\nfn(x, 2)\nfn(x, 3)\nfn(x, -2)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):\n    # File: a.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n\n    # File: a.py:7, code: return (n + 1) * y\n    add = l_n_ + 1\n    mul = add * y\n    return (mul,)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):\n    # File: a.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n\n    # File: a.py:9, code: return y / n\n    truediv = y / l_n_\n    return (truediv,)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Serialization for TorchScript Modules\nDESCRIPTION: Example of implementing __getstate__ and __setstate__ methods for custom serialization behavior in a TorchScript module. These methods allow users to control how their class or module is pickled and unpickled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        self.a = torch.rand(2, 3)\n        self.b = torch.nn.Linear(10, 10)\n\n    def __getstate__(self):\n        # Compiler infers that this is a tuple of (Tensor, Linear)\n        return (self.a, self.b)\n\n    def __setstate__(self, state):\n        # Don't need to annotate this, we know what type `state` is!\n        self.a = state[0]\n        self.b = state[1]\n```\n\n----------------------------------------\n\nTITLE: PyTorch JIT C++ Fuser API Examples\nDESCRIPTION: C++ APIs for controlling fusion optimizations in PyTorch's JIT. These functions allow C++ code to enable or disable various fusion mechanisms like NNC and NVFuser, providing the same functionality as the Python APIs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_30\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::jit::setTensorExprFuserEnabled(bool); // NNC enable/disable\ntorch::jit::overrideCanFuseOnCPU(bool); // NNC on CPU\ntorch::jit::overrideCanFuseOnGPU(bool); // NNC on GPU\ntorch::jit::fuser::cuda::setEnabled(bool); // NVFuser enable/disable (deprecated)\n```\n\n----------------------------------------\n\nTITLE: Comparing Python and C++ Tensor Creation\nDESCRIPTION: Compares tensor creation syntax between Python and C++ APIs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntorch.randn(3, 4, dtype=torch.float32, device=torch.device('cuda', 1), requires_grad=True)\n```\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::randn({3, 4}, torch::dtype(torch::kFloat32).device(torch::kCUDA, 1).requires_grad(true))\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Count for aten.convolution.default Operations\nDESCRIPTION: Extensive list of convolution operations used throughout the network with various kernel sizes, strides, and parameters. Shows depthwise separable convolutions (groups > 1) and regular convolutions, primarily using half precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 16, 112, 112], f16), T([16, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 16), {})\ncnt: 2, ((T([128, 16, 112, 112], f16), T([16, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([64, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([64, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 64), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([24, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 24, 56, 56], f16), T([48, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 48, 56, 56], f16), T([48, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 48), {})\ncnt: 3, ((T([128, 48, 56, 56], f16), T([24, 48, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 24, 56, 56], f16), T([120, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 120, 56, 56], f16), T([120, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 120), {})\ncnt: 1, ((T([128, 120, 1, 1], f16), T([8, 120, 1, 1], f16), T([8], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 8, 1, 1], f16), T([120, 8, 1, 1], f16), T([120], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 120, 28, 28], f16), T([40, 120, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 40, 28, 28], f16), T([120, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 120, 28, 28], f16), T([120, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 120), {})\ncnt: 4, ((T([128, 120, 1, 1], f16), T([16, 120, 1, 1], f16), T([16], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 16, 1, 1], f16), T([120, 16, 1, 1], f16), T([120], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 40, 28, 28], f16), T([200, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 200, 28, 28], f16), T([200, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 200), {})\ncnt: 1, ((T([128, 200, 14, 14], f16), T([72, 200, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 72, 14, 14], f16), T([216, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 216, 14, 14], f16), T([216, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 216), {})\ncnt: 4, ((T([128, 216, 14, 14], f16), T([72, 216, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 72, 14, 14], f16), T([360, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 360, 14, 14], f16), T([360, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 360), {})\ncnt: 1, ((T([128, 360, 1, 1], f16), T([24, 360, 1, 1], f16), T([24], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 24, 1, 1], f16), T([360, 24, 1, 1], f16), T([360], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 6, ((T([128, 360, 14, 14], f16), T([120, 360, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 120, 14, 14], f16), T([360, 120, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 360, 14, 14], f16), T([360, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 360), {})\ncnt: 5, ((T([128, 360, 1, 1], f16), T([32, 360, 1, 1], f16), T([32], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 32, 1, 1], f16), T([360, 32, 1, 1], f16), T([360], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 120, 14, 14], f16), T([720, 120, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 720, 14, 14], f16), T([720, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 720), {})\ncnt: 1, ((T([128, 720, 1, 1], f16), T([32, 720, 1, 1], f16), T([32], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 1, 1], f16), T([720, 32, 1, 1], f16), T([720], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 720, 7, 7], f16), T([184, 720, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 184, 7, 7], f16), T([736, 184, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 736, 7, 7], f16), T([736, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 736), {})\ncnt: 5, ((T([128, 736, 1, 1], f16), T([48, 736, 1, 1], f16), T([48], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 48, 1, 1], f16), T([736, 48, 1, 1], f16), T([736], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 736, 7, 7], f16), T([184, 736, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 184, 7, 7], f16), T([1104, 184, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1104, 7, 7], f16), T([1104, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1104), {})\ncnt: 1, ((T([128, 1104, 1, 1], f16), T([48, 1104, 1, 1], f16), T([48], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 48, 1, 1], f16), T([1104, 48, 1, 1], f16), T([1104], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1104, 7, 7], f16), T([224, 1104, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 224, 7, 7], f16), T([1344, 224, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1344, 1, 1], f16), T([1984, 1344, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Structuring a Complete Quantized Operator Source File (qxand.cpp) in C++\nDESCRIPTION: Presents the complete structure for the `ATen/native/quantized/cpu/qxand.cpp` file. It includes necessary headers, wraps the `quantized_xand` function implementation (shown as a stub returning `qc`) and its `TORCH_LIBRARY_IMPL` registration within the `at::native` namespace. This represents the final consolidated code for the new quantized operator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_5\n\nLANGUAGE: c++\nCODE:\n```\n#include <ATen/ATen.h>\n#include <ATen/NativeFunctions.h> // Need that for the `native_functions.yaml`\n#include <ATen/core/Type.h>\n#include <torch/library.h>\n#include <ATen/native/TensorIterator.h>\n#include <ATen/native/cpu/Loops.h>\n\nnamespace at {\n  namespace native {\n  Tensor quantized_xand(Tensor qa, Tensor qb) {\n    // The awesome op implementation...\n    Tensor qc; // Placeholder for return value based on description\n    // Actual implementation from Snippet 1 would create and compute qc\n    // For structure example, we assume qc is created and returned\n    return qc;\n  }\n\n  TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {\n    m.impl(\"xand\", TORCH_FN(quantized_xand));\n  }\n}}  // namespace at::native\n```\n\n----------------------------------------\n\nTITLE: Reinforcing Types for DataPipe Instances in Python\nDESCRIPTION: Demonstrates how to use the reinforce_type method to specify more precise types for DataPipe instances at runtime.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nT = TypeVar('T', int, str)\nds = list(range(10))\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe[T]):\n    def __init__(self, ds):\n        self.ds = ds\n\n    def __iter__(self):\n        for d in self.ds:\n            yield d\ndp = DP(ds).reinforce_type(int)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe[T]):\n    def __init__(self, ds):\n        self.ds = ds\n\n    @runtime_validation\n    def __iter__(self):\n        for d in self.ds:\n            yield d\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP(ds).reinforce_type(float)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP(ds).reinforce_type(str)\nlist(dp)\n```\n\nLANGUAGE: python\nCODE:\n```\nwith runtime_validation_disabled():\n    print(list(dp))\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP(ds).reinforce_type(int)\nprint(list(dp))\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe[Union[int, str]]):\n    def __init__(self, label):\n        if label == 'int':\n            self.reinforce_type(int)\n        elif label == 'str':\n            self.reinforce_type(str)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP('int')\nprint(dp.type)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP('str')\nprint(dp.type)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = DP('')\nprint(dp.type)\n```\n\n----------------------------------------\n\nTITLE: Running TorchBench Performance Benchmarks (Python)\nDESCRIPTION: Commands to run TorchBench performance benchmarks for both training and inference using TorchInductor backend. The commands specify device, precision, and output format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n./benchmarks/dynamo/torchbench.py --performance --training --amp --backend=inductor --output=torchbench_training.csv\n./benchmarks/dynamo/torchbench.py --performance --inference --bfloat16 --backend=inductor --output=torchbench_inference.csv\n```\n\n----------------------------------------\n\nTITLE: Using Data Scheduler with Data Sparsifier in PyTorch Training Loop\nDESCRIPTION: Example code demonstrating how to integrate a data scheduler with a data sparsifier in a PyTorch training loop. The scheduler adjusts sparsification parameters after each epoch, while the sparsifier processes the input data during training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_scheduler/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = SomeModel()\noptimizer = SomeOptimizer(model.parameters(), lr=...)\ndata_sparsifier = SomeDataSparsifier(...)\n\n\ndata_scheduler = SomeDataScheduler(data_sparsifier, ...)\n\n\ndata_name = 'train_data'\n\nfor epoch in range(EPOCHS):\n    for input, target in dataset:\n        input = data_sparsifier.add_data(name=data_name, data=input)\n\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n        data_sparsifier.step()\n\n    data_scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Observer Insertion Result for Linear-ReLU Module\nDESCRIPTION: This code snippet shows the result of observer insertion for the Linear-ReLU module. It demonstrates how MinMaxObservers are added before and after the LinearReLU module to collect statistics for quantization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nGraphModule(\n  (activation_post_process_0): MinMaxObserver(min_val=inf, max_val=-inf)\n  (linear): LinearReLU(\n    (0): Linear(in_features=5, out_features=10, bias=True)\n    (1): ReLU()\n  )\n  (activation_post_process_1): MinMaxObserver(min_val=inf, max_val=-inf)\n)\n\ndef forward(self, x):\n    activation_post_process_0 = self.activation_post_process_0(x);  x = None\n    linear = self.linear(activation_post_process_0);  activation_post_process_0 = None\n    activation_post_process_1 = self.activation_post_process_1(linear);  linear = None\n    return activation_post_process_1\n```\n\n----------------------------------------\n\nTITLE: Forward Convolution Operations in PyTorch MobileNetV3\nDESCRIPTION: Statistics for all forward convolution operations in the model. These form the backbone of the network and include standard convolutions, depthwise separable convolutions (indicated by groups parameter), and pointwise (1x1) convolutions for channel projection and expansion.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 16, 112, 112], f16), T([16, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([16, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 16), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([96, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 112, 112], f16), T([96, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 96), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([24, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 24, 56, 56], f16), T([24, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 24, 56, 56], f16), T([24, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 24), {})\ncnt: 1, ((T([128, 24, 56, 56], f16), T([144, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([144, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 144), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([32, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 28, 28], f16), T([96, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 28, 28], f16), T([96, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 96), {})\ncnt: 1, ((T([128, 96, 28, 28], f16), T([32, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 32, 28, 28], f16), T([192, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([192, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 192), {})\ncnt: 2, ((T([128, 192, 28, 28], f16), T([32, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 192), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([192, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 192), {})\ncnt: 2, ((T([128, 192, 14, 14], f16), T([64, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 14, 14], f16), T([192, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 14, 14], f16), T([192, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 192), {})\ncnt: 3, ((T([128, 64, 14, 14], f16), T([384, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 384, 14, 14], f16), T([384, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 384), {})\ncnt: 2, ((T([128, 384, 14, 14], f16), T([64, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 384, 14, 14], f16), T([112, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 112, 14, 14], f16), T([672, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 2, ((T([128, 672, 14, 14], f16), T([112, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 112, 14, 14], f16), T([336, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 336, 14, 14], f16), T([336, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 336), {})\ncnt: 1, ((T([128, 336, 14, 14], f16), T([112, 336, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), T([184, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 184, 7, 7], f16), T([1104, 184, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 1104, 7, 7], f16), T([1104, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1104), {})\ncnt: 3, ((T([128, 1104, 7, 7], f16), T([184, 1104, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1104, 7, 7], f16), T([1104, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1104), {})\ncnt: 1, ((T([128, 1104, 7, 7], f16), T([352, 1104, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 352, 7, 7], f16), T([1984, 352, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Vim to View Package Contents\nDESCRIPTION: Shows how to configure Vim to natively read torch package archives by treating .pt files as zip files. This enables viewing and editing package contents directly from Vim.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_2\n\nLANGUAGE: vim\nCODE:\n```\n# add this to your .vimrc to treat `*.pt` files as zip files\nau BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\"))\n\n~ vi my_package.pt\n```\n\n----------------------------------------\n\nTITLE: Defining Control Flow Statements in TorchScript\nDESCRIPTION: Specifies the syntax for control flow statements in TorchScript, including raise, assert, return, del, pass, print, break, and continue.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nraise_stmt ::=  \"raise\" [expression [\"from\" expression]]\n\nassert_stmt ::=  \"assert\" expression [\",\" expression]\n\nreturn_stmt ::=  \"return\" [expression_list]\n\ndel_stmt ::=  \"del\" target_list\n\npass_stmt ::= \"pass\"\n\nprint_stmt ::= \"print\" \"(\" expression  [, expression] [.format{expression_list}] \")\"\n\nbreak_stmt ::= \"break\"\n\ncontinue_stmt ::= \"continue\"\n```\n\n----------------------------------------\n\nTITLE: Elementwise Multiplication with mul in PyTorch (Python)\nDESCRIPTION: Conducts aten.mul to perform element-wise multiplication of tensor elements, applied in scenarios where scaling specific tensor values is needed for normalization or adjustment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\naten.mul.Tensor\ncnt: 1, ((T([16, 1, 1, 512], f16), -65504.0), {})\n```\n\n----------------------------------------\n\nTITLE: Subtracting Scalar from Tensor using aten.rsub.Scalar - Python\nDESCRIPTION: Illustrates calls to aten.rsub.Scalar to subtract a scalar from all elements of the tensor, covering both regular and strided tensors in f16 and f32 formats. Used in residual or normalization connections in neural networks. Requirements: PyTorch, appropriate tensor and scalar type compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.rsub.Scalar\ncnt: 24, ((T([2, 1, 1, 1024], f16), 1.0), {})\ncnt: 24, ((T([2, 12, 64, 448], f32), 1.0), {})\ncnt: 12, ((T([2, 1, 12, 64, 192], f16), 1.0), {})\ncnt: 24, ((T([2, 1, 1, 1, 64], f16, stride=(1024, 1024, 1024, 64, 1)), 1.0), {})\ncnt: 12, ((T([2, 12, 12, 64, 192], f32, stride=(2064384, 172032, 12288, 192, 1)), 1.0), {})\n```\n\n----------------------------------------\n\nTITLE: Example of Structured Pruned Weight Tensor\nDESCRIPTION: Demonstrates structured pruning where entire rows of a weight tensor are zeroed out, which allows for matrix resizing and computational efficiency gains.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nW_pruned = [[0 0 0] = [[4, 5, 6],\n            [4 5 6]    [7, 1, 9]]\n            [7 1 9]]\n```\n\n----------------------------------------\n\nTITLE: Debug Tracing Example with TorchInductor\nDESCRIPTION: Sample code demonstrating how to use TORCH_COMPILE_DEBUG for tracing compilation and debugging TorchInductor internals.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile()\ndef test_model(x):\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 10),\n        torch.nn.LayerNorm(10),\n        torch.nn.ReLU(),\n    )\n    return model(x)\n\ny = test_model(torch.ones(10, 10))\n```\n\n----------------------------------------\n\nTITLE: Using torch.finfo Properties Example\nDESCRIPTION: Example properties available through torch.finfo class for floating point data types. These properties include bits, eps, max, min, tiny, smallest_normal, and resolution values for numeric analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/type_info.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbits                   int     # The number of bits occupied by the type\neps                    float   # The smallest representable number such that 1.0 + eps != 1.0\nmax                    float   # The largest representable number\nmin                    float   # The smallest representable number (typically -max)\ntiny                   float   # The smallest positive normal number\nsmallest_normal        float   # The smallest positive normal number\nresolution             float   # The approximate decimal resolution of this type\n```\n\n----------------------------------------\n\nTITLE: PyTorch Log Softmax Operations\nDESCRIPTION: Log softmax forward and backward operations on 64x1000 tensors using float16 dtype\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._log_softmax.default(T([64, 1000], f16), 1, False)\naten._log_softmax_backward_data.default(T([64, 1000], f16), T([64, 1000], f16), 1, f16)\n```\n\n----------------------------------------\n\nTITLE: Executing and Verifying Lazy Tensor Results in PyTorch\nDESCRIPTION: This snippet shows how to trigger the execution of Lazy Tensor computations and verify the results against the original function for both false and true conditions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlazy_result = add_two_maybe(t_lazy, maybe_false_lazy)\nprint(lazy_result)\nassert lazy_result.cpu() == add_two_maybe(t, maybe_false)\n\nmaybe_true_lazy = torch.BoolTensor([1]).to(dev)\nlazy_result = add_two_maybe(t_lazy, maybe_true_lazy)\nassert lazy_result.cpu() == add_two_maybe(t, maybe_true)\n```\n\n----------------------------------------\n\nTITLE: Applying aten.sum.SymInt Operator in PyTorch\nDESCRIPTION: Multiple instances of the aten.sum.SymInt operator being used to sum tensors along dimension 0. The third parameter (True) indicates that the operation keeps the dimension in the output tensor, producing a tensor with a singleton dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/coat_lite_mini_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 4, ((T([6400, 512], f16), [0], True), {})\ncnt: 2, ((T([6400, 2048], f16), [0], True), {})\ncnt: 2, ((T([6400, 1536], f16), [0], True), {})\ncnt: 1, ((T([128, 1, 512], f16, stride=(25600, 512, 1)), [0], True), {})\ncnt: 4, ((T([25216, 320], f16), [0], True), {})\ncnt: 2, ((T([25216, 1280], f16), [0], True), {})\ncnt: 2, ((T([25216, 960], f16), [0], True), {})\ncnt: 1, ((T([128, 1, 320], f16, stride=(63040, 320, 1)), [0], True), {})\ncnt: 4, ((T([100480, 128], f16), [0], True), {})\ncnt: 2, ((T([100480, 1024], f16), [0], True), {})\ncnt: 2, ((T([100480, 384], f16), [0], True), {})\ncnt: 1, ((T([128, 1, 128], f16, stride=(100480, 128, 1)), [0], True), {})\ncnt: 4, ((T([401536, 64], f16), [0], True), {})\ncnt: 2, ((T([401536, 512], f16), [0], True), {})\ncnt: 2, ((T([401536, 192], f16), [0], True), {})\ncnt: 1, ((T([128, 1, 64], f16, stride=(200768, 64, 1)), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.sigmoid_backward.default\nDESCRIPTION: Logs the backward pass for the Sigmoid activation function (`aten.sigmoid_backward.default`). Takes the output gradient and the output of the forward sigmoid operation as inputs, both being 4D float16 tensors with singleton spatial dimensions ([128, C, 1, 1]).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.sigmoid_backward.default\ncnt: 7, ((T([128, 368, 1, 1], f16), T([128, 368, 1, 1], f16)), {})\ncnt: 4, ((T([128, 152, 1, 1], f16), T([128, 152, 1, 1], f16)), {})\ncnt: 1, ((T([128, 56, 1, 1], f16), T([128, 56, 1, 1], f16)), {})\ncnt: 1, ((T([128, 24, 1, 1], f16), T([128, 24, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Convolution Operations in PyTorch\nDESCRIPTION: This snippet shows various convolution operations with different tensor shapes, strides, and padding. The operations are represented as tuples containing tensor shapes and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 5, ((T([128, 736, 7, 7], f16), T([128, 184, 7, 7], f16), T([736, 184, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 184, 7, 7], f16), T([128, 720, 7, 7], f16), T([184, 720, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 720, 1, 1], f16), T([128, 32, 1, 1], f16), T([720, 32, 1, 1], f16), [720], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Convolution Operations in PyTorch\nDESCRIPTION: This snippet shows various convolution operations with different tensor shapes, strides, and parameters. It demonstrates the use of 16-bit floating point (f16) tensors and group convolutions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 5, ((T([128, 512, 14, 14], f16), T([128, 1024, 14, 14], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1024, 14, 14], f16), T([128, 512, 28, 28], f16), T([1024, 512, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 128, 14, 14], f16), T([128, 128, 28, 28], f16, stride=(401408, 784, 28, 1)), T([128, 16, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 8, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Using ATen _log_softmax Operator in PyTorch\nDESCRIPTION: The code snippet demonstrates the use of the `aten._log_softmax` operator with specific input parameters: a tensor of size [64, 1000] with data type f16, dimension 1, and a boolean False. The operator is used to apply the log-softmax function, which normalizes input data for classification tasks. The expected input is a tensor, and the output is a tensor with log-softmax applied.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Function with Docstring Linter Exclusion\nDESCRIPTION: A function that uses a noqa comment to indicate it should be excluded from docstring linting.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef long_with_omit():  # noqa: docstring_linter\n    #\n    #\n    #\n    #\n    pass\n```\n\n----------------------------------------\n\nTITLE: Average Pooling Operations in PyTorch\nDESCRIPTION: PyTorch's average pooling operations with 2x2 kernels and stride 2. These operations reduce the spatial dimensions of the input tensors by a factor of 2, commonly used in CNN architectures for downsampling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([64, 128, 56, 56], f16), [2, 2], [2, 2]), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), [2, 2], [2, 2]), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), [2, 2], [2, 2]), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Mobile Build Settings for PyTorch\nDESCRIPTION: Sets up specific build configurations for mobile platforms (Android, iOS). Disables certain components not applicable to mobile and adds compilation flags for code size optimization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\n# Set INTERN_BUILD_MOBILE for all mobile builds. Components that are not\n# applicable to mobile are disabled by this variable. Setting\n# `BUILD_PYTORCH_MOBILE_WITH_HOST_TOOLCHAIN` environment variable can force it\n# to do mobile build with host toolchain - which is useful for testing purpose.\nif(ANDROID\n   OR IOS\n   OR DEFINED ENV{BUILD_PYTORCH_MOBILE_WITH_HOST_TOOLCHAIN})\n  set(INTERN_BUILD_MOBILE ON)\n  message(WARNING \"INTERN_BUILD_MOBILE is on, disabling BUILD_LAZY_TS_BACKEND\")\n  set(BUILD_LAZY_TS_BACKEND OFF)\n\n  set(USE_KLEIDIAI OFF)\n  message(WARNING \"KleidiAI cannot be used on Mobile builds. Set it to OFF\")\n\n  # Set -ffunction-sections and -fdata-sections so that each method has its own\n  # text section. This allows the linker to remove unused section when the flag\n  # -Wl,-gc-sections is provided at link time.\n  string(APPEND CMAKE_CXX_FLAGS \" -ffunction-sections\")\n  string(APPEND CMAKE_C_FLAGS \" -ffunction-sections\")\n  string(APPEND CMAKE_CXX_FLAGS \" -fdata-sections\")\n  string(APPEND CMAKE_C_FLAGS \" -fdata-sections\")\n\n  # Please note that the use of the following flags is required when linking\n  # against libtorch_cpu.a for mobile builds. -Wl,--whole-archive -ltorch_cpu\n  # -Wl,--no-whole-archive\n  #\n  # This allows global constructors to be included and run. Global constructors\n  # are used for operator/kernel registration with the PyTorch Dispatcher.\n\n  if(DEFINED ENV{BUILD_PYTORCH_MOBILE_WITH_HOST_TOOLCHAIN})\n    # C10_MOBILE is derived from Android/iOS toolchain macros in\n    # c10/macros/Macros.h, so it needs to be explicitly set here.\n    string(APPEND CMAKE_CXX_FLAGS \" -DC10_MOBILE\")\n  endif()\n\n  if(DEFINED ENV{PYTORCH_MOBILE_TRIM_DISPATCH_KEY_SET})\n    # If PYTORCH_MOBILE_TRIM_DISPATCH_KEY_SET is defined (env var), then define\n    # C10_MOBILE_TRIM_DISPATCH_KEYS, which limits the number of dispatch keys in\n    # OperatorEntry::dispatchTable_ to reduce peak memory during library\n    # initialization.\n    string(APPEND CMAKE_CXX_FLAGS \" -DC10_MOBILE_TRIM_DISPATCH_KEYS\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Nightly PyTorch with CUDA Support\nDESCRIPTION: Uses the tools/nightly.py script to check out a nightly PyTorch branch and install binaries built with CUDA support using the --cuda flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n./tools/nightly.py checkout -b my-nightly-branch --cuda\nsource venv/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Count for aten.clone.default Operations\nDESCRIPTION: Lists tensor cloning operations across the network for various tensor shapes, used to create copies of tensors for operations that might modify the original data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\ncnt: 3, ((T([128, 16, 112, 112], f16),), {})\ncnt: 1, ((T([128, 64, 112, 112], f16),), {})\ncnt: 1, ((T([128, 64, 56, 56], f16),), {})\ncnt: 6, ((T([128, 48, 56, 56], f16),), {})\ncnt: 1, ((T([128, 120, 56, 56], f16),), {})\ncnt: 9, ((T([128, 120, 28, 28], f16),), {})\ncnt: 1, ((T([128, 8, 1, 1], f16),), {})\ncnt: 4, ((T([128, 16, 1, 1], f16),), {})\ncnt: 1, ((T([128, 200, 28, 28], f16),), {})\ncnt: 1, ((T([128, 200, 14, 14], f16),), {})\ncnt: 8, ((T([128, 216, 14, 14], f16),), {})\ncnt: 12, ((T([128, 360, 14, 14], f16),), {})\ncnt: 1, ((T([128, 24, 1, 1], f16),), {})\ncnt: 6, ((T([128, 32, 1, 1], f16),), {})\ncnt: 1, ((T([128, 720, 14, 14], f16),), {})\ncnt: 1, ((T([128, 720, 7, 7], f16),), {})\ncnt: 10, ((T([128, 736, 7, 7], f16),), {})\ncnt: 6, ((T([128, 48, 1, 1], f16),), {})\ncnt: 2, ((T([128, 1104, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1344, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1984, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: FakeTensor Operator Implementation Cases\nDESCRIPTION: Documents the various ways operators are implemented in FakeTensor, including constant propagation for small tensors, fastpath implementations, custom ops with impl_abstract, device-converting operations, and fallback behavior for missing meta implementations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_7\n\n\n\n----------------------------------------\n\nTITLE: Tensor Copy Operation in PyTorch\nDESCRIPTION: Copies the contents of one tensor to another with the same shape and data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\naten.copy_.default((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Accessing fill_uninitialized_memory Attribute in PyTorch\nDESCRIPTION: This snippet demonstrates how to access the fill_uninitialized_memory attribute in the torch.utils.deterministic module. When set to True, it causes uninitialized memory to be filled with known values when using deterministic algorithms. This affects performance but ensures deterministic behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/deterministic.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch.utils.deterministic.fill_uninitialized_memory\n```\n\n----------------------------------------\n\nTITLE: Defining a Short Class without Docstring\nDESCRIPTION: A minimal class with no implementation or documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Short:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Creating a Nested Tensor with torch.jagged Layout in Python\nDESCRIPTION: This snippet demonstrates constructing a nested tensor using the `torch.nested.nested_tensor` method with a `torch.jagged` layout. It highlights the requirement for input tensors to have the same number of dimensions when constructing a nested tensor, and showcases how to specify dtype, device, and autograd parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> a, b = torch.arange(3), torch.arange(5) + 3\n>>> a\ntensor([0, 1, 2])\n>>> b\ntensor([3, 4, 5, 6, 7])\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> print([component for component in nt])\n[tensor([0, 1, 2]), tensor([3, 4, 5, 6, 7])]\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> a = torch.randn(50, 128) # 2D tensor\n>>> b = torch.randn(2, 50, 128) # 3D tensor\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n...\nRuntimeError: When constructing a nested tensor, all tensors in list must have the same dim\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32, device='cuda', requires_grad=True)\n>>> print([component for component in nt])\n[tensor([0., 1., 2.], device='cuda:0', grad_fn=<UnbindBackwardAutogradNestedTensor0>), tensor([3., 4., 5., 6., 7.], device='cuda:0', grad_fn=<UnbindBackwardAutogradNestedTensor0>)]\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Backward Operations\nDESCRIPTION: Backward pass operations for convolution layers with gradient computations. Shows various tensor shapes and parameters used in the backward propagation of convolution operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([32, 1000, 1, 1], f16), T([32, 2688, 1, 1], f16), T([1000, 2688, 1, 1], f16), [1000], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Python Loop Tracing Example\nDESCRIPTION: Demonstrates a problematic case of tracing a loop that depends on input tensor shape, showing how the trace differs across different input shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef loop_in_traced_fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Launcher for TorchElastic in Python\nDESCRIPTION: This snippet demonstrates how to create a custom launcher for TorchElastic by programmatically creating an agent and passing worker specifications. It includes error handling and result processing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/customization.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# my_launcher.py\n\nif __name__ == \"__main__\":\n  args = parse_args(sys.argv[1:])\n  rdzv_handler = RendezvousHandler(...)\n  spec = WorkerSpec(\n      local_world_size=args.nproc_per_node,\n      fn=trainer_entrypoint_fn,\n      args=(trainer_entrypoint_fn args.fn_args,...),\n      rdzv_handler=rdzv_handler,\n      max_restarts=args.max_restarts,\n      monitor_interval=args.monitor_interval,\n  )\n\n  agent = LocalElasticAgent(spec, start_method=\"spawn\")\n  try:\n      run_result = agent.run()\n      if run_result.is_failed():\n          print(f\"worker 0 failed with: run_result.failures[0]\")\n      else:\n          print(f\"worker 0 return value is: run_result.return_values[0]\")\n  except Exception ex:\n      # handle exception\n```\n\n----------------------------------------\n\nTITLE: Performing Inference with AMP on Intel GPU\nDESCRIPTION: Executes an AMP-based inference using ResNet50 on Intel GPU, utilizing PyTorch's autocast to handle data types efficiently for accelerated processing on available XPU.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\n\nwith torch.no_grad():\n    d = torch.rand(1, 3, 224, 224)\n    d = d.to(\"xpu\")\n    # set dtype=torch.bfloat16 for BF16\n    with torch.autocast(device_type=\"xpu\", dtype=torch.float16, enabled=True):\n        model(data)\n\nprint(\"Execution finished\")\n```\n\n----------------------------------------\n\nTITLE: Module Parameter Declaration in Serialized Python Code\nDESCRIPTION: This example shows how TorchScript represents module parameters in the serialized Python code. It uses a special __parameters__ list to track which attributes are parameters, along with type annotations for all attributes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyModule(Module):\n    __parameters__ = [\"foo\", \"bar\", ]\n    foo : Tensor\n    bar : Tensor\n    attribute_but_not_param : Tensor\n```\n\n----------------------------------------\n\nTITLE: Slicing and Indexing Sparse COO Tensor in PyTorch\nDESCRIPTION: This example demonstrates slicing and indexing operations on a sparse COO tensor. It shows that slicing is supported only for dense dimensions, while indexing is supported for both sparse and dense dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n>>> s[1]\ntensor(indices=tensor([[0, 2]]),\n       values=tensor([[5, 6],\n                      [7, 8]]),\n       size=(3, 2), nnz=2, layout=torch.sparse_coo)\n>>> s[1, 0, 1]\ntensor(6)\n>>> s[1, 0, 1:]\ntensor([6])\n```\n\n----------------------------------------\n\nTITLE: Backend Registration in setup.py\nDESCRIPTION: Shows how to register a backend through Python package entry points in setup.py configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsetup(\n    ...\n    'torch_dynamo_backends': [\n        'my_compiler = your_module.submodule:my_compiler',\n    ]\n    ...\n)\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Bottleneck Profiler Command\nDESCRIPTION: Command line instruction for running the bottleneck profiler on a Python script. The utility accepts a source script path and optional arguments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/bottleneck.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m torch.utils.bottleneck /path/to/source/script.py [args]\n```\n\n----------------------------------------\n\nTITLE: Incorrect CUDA Stream Usage Example\nDESCRIPTION: Shows an incorrect implementation of CUDA stream operations where proper synchronization is missing between operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ncuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\nwith torch.cuda.stream(s):\n    # sum() may start execution before normal_() finishes!\n    B = torch.sum(A)\n```\n\n----------------------------------------\n\nTITLE: Invoking PyTorch aten Operators with Tensor Arguments - Python\nDESCRIPTION: This snippet demonstrates the notation for invoking PyTorch aten operators such as max pool, mean reduction, batch norm, and elementwise operations using tensor argument specifications, shapes, types, and basic test setup patterns. Dependencies include a Python environment with PyTorch and the aten operator namespace. The notation encodes operator names, input/output tensor shapes (T), datatypes, strides, argument lists, and operation counts (cnt), intended for test, coverage, or profiling. Inputs expected are tensor metadata and operator arguments; outputs are implicit results per operator specification. This is typically a technical artifact and not standalone runnable code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_senet154_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((T([32, 2048, 7, 7], f16, stride=(2048, 1, 0, 0)), 49), {})\ncnt: 36, ((T([32, 1024, 14, 14], f16, stride=(1024, 1, 0, 0)), 196), {})\ncnt: 8, ((T([32, 512, 28, 28], f16, stride=(512, 1, 0, 0)), 784), {})\ncnt: 3, ((T([32, 256, 56, 56], f16, stride=(256, 1, 0, 0)), 3136), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([32], i64),), {})\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([32, 128, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([32, 128, 56, 56], f16), T([32, 128, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([32, 128, 56, 56], i64)), {})\nOperator: aten.mean.dim\ncnt: 3, ((T([32, 256, 56, 56], f16), [2, 3], True), {})\ncnt: 8, ((T([32, 512, 28, 28], f16), [2, 3], True), {})\ncnt: 36, ((T([32, 1024, 14, 14], f16), [2, 3], True), {})\ncnt: 3, ((T([32, 2048, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 2048, 7, 7], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([32, 1000], f16), T([1000, 2048], f16)), {})\ncnt: 1, ((T([1000, 32], f16, stride=(1, 1000)), T([32, 2048], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 6, ((T([32, 256, 56, 56], f16), T([32, 256, 1, 1], f16)), {})\ncnt: 16, ((T([32, 512, 28, 28], f16), T([32, 512, 1, 1], f16)), {})\ncnt: 72, ((T([32, 1024, 14, 14], f16), T([32, 1024, 1, 1], f16)), {})\ncnt: 6, ((T([32, 2048, 7, 7], f16), T([32, 2048, 1, 1], f16)), {})\ncnt: 3, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16)), {})\ncnt: 36, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16)), {})\ncnt: 8, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16)), {})\ncnt: 3, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 128, 112, 112], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 8, ((T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 18, ((T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([32, 256, 28, 28], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 74, ((T([32, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 1e-05), {})\ncnt: 35, ((T([32, 512, 14, 14], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([32, 2048, 7, 7], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f16), True, 0.1, 1e-05), {})\ncnt: 2, ((T([32, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 7, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([32, 1024, 7, 7], f16), T([32, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\ncnt: 74, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\ncnt: 35, ((T([32, 512, 14, 14], f16), T([32, 512, 14, 14], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 18, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 7, ((T([32, 256, 28, 28], f16), T([32, 256, 28, 28], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 8, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 128, 112, 112], f16), T([32, 128, 112, 112], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([32, 1000], f16), T([32], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([32, 1000], f16), T([32], i64), None, 1, -100), {})\nOperator: aten.relu_.default\ncnt: 2, ((T([32, 64, 112, 112], f16),), {})\ncnt: 1, ((T([32, 128, 112, 112], f16),), {})\ncnt: 3, ((T([32, 128, 56, 56], f16),), {})\ncnt: 7, ((T([32, 256, 56, 56], f16),), {})\ncnt: 3, ((T([32, 16, 1, 1], f16),), {})\ncnt: 17, ((T([32, 512, 28, 28], f16),), {})\ncnt: 8, ((T([32, 32, 1, 1], f16),), {})\ncnt: 7, ((T([32, 256, 28, 28], f16),), {})\ncnt: 73, ((T([32, 1024, 14, 14], f16),), {})\ncnt: 36, ((T([32, 64, 1, 1], f16),), {})\ncnt: 35, ((T([32, 512, 14, 14], f16),), {})\ncnt: 6, ((T([32, 2048, 7, 7], f16),), {})\ncnt: 3, ((T([32, 128, 1, 1], f16),), {})\ncnt: 2, ((T([32, 1024, 7, 7], f16),), {})\nOperator: aten.sigmoid.default\ncnt: 3, ((T([32, 256, 1, 1], f16),), {})\ncnt: 8, ((T([32, 512, 1, 1], f16),), {})\ncnt: 36, ((T([32, 1024, 1, 1], f16),), {})\ncnt: 3, ((T([32, 2048, 1, 1], f16),), {})\nOperator: aten.sigmoid_backward.default\ncnt: 3, ((T([32, 2048, 1, 1], f16), T([32, 2048, 1, 1], f16)), {})\ncnt: 36, ((T([32, 1024, 1, 1], f16), T([32, 1024, 1, 1], f16)), {})\ncnt: 8, ((T([32, 512, 1, 1], f16), T([32, 512, 1, 1], f16)), {})\ncnt: 3, ((T([32, 256, 1, 1], f16), T([32, 256, 1, 1], f16)), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([32, 1000], f16), [0], True), {})\ncnt: 3, ((T([32, 2048, 7, 7], f16), [2, 3], True), {})\ncnt: 36, ((T([32, 1024, 14, 14], f16), [2, 3], True), {})\ncnt: 8, ((T([32, 512, 28, 28], f16), [2, 3], True), {})\ncnt: 3, ((T([32, 256, 56, 56], f16), [2, 3], True), {})\nOperator: aten.threshold_backward.default\ncnt: 6, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), 0), {})\ncnt: 3, ((T([32, 128, 1, 1], f16), T([32, 128, 1, 1], f16), 0), {})\ncnt: 2, ((T([32, 1024, 7, 7], f16), T([32, 1024, 7, 7], f16), 0), {})\ncnt: 73, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16), 0), {})\ncnt: 36, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), 0), {})\ncnt: 35, ((T([32, 512, 14, 14], f16), T([32, 512, 14, 14], f16), 0), {})\ncnt: 17, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16), 0), {})\ncnt: 8, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), 0), {})\ncnt: 7, ((T([32, 256, 28, 28], f16), T([32, 256, 28, 28], f16), 0), {})\ncnt: 7, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16), 0), {})\ncnt: 3, ((T([32, 16, 1, 1], f16), T([32, 16, 1, 1], f16), 0), {})\ncnt: 3, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), 0), {})\ncnt: 1, ((T([32, 128, 112, 112], f16), T([32, 128, 112, 112], f16), 0), {})\ncnt: 2, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Forward Operations in PyTorch\nDESCRIPTION: Statistics for the aten.convolution.default operator showing forward convolution operations with various configurations. These operations implement different convolutional layers in a neural network, including different kernel sizes, strides, and channel dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 193, 193], f16), T([16, 3, 3, 3], f16), T([16], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 16, 96, 96], f16), T([32, 16, 3, 3], f16), T([32], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 96, 96], f16), T([64, 32, 3, 3], f16), T([64], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 97, 97], f16), T([128, 64, 3, 3], f16), T([128], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 128, 48, 48], f16), T([256, 128, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 48, 48], f16), T([128, 128, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 128, 48, 48], f16), T([128, 128, 3, 3], f16), T([128], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 1, 1], f16), T([256, 128, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 256, 24, 24], f16), T([512, 256, 1, 1], f16), T([512], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 48, 48], f16), T([256, 256, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 49, 49], f16), T([256, 128, 3, 3], f16), T([256], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 2), {})\ncnt: 3, ((T([128, 256, 24, 24], f16), T([256, 128, 3, 3], f16), T([256], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})\ncnt: 2, ((T([128, 512, 1, 1], f16), T([256, 512, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 256, 1, 1], f16), T([512, 256, 1, 1], f16), T([512], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 24, 24], f16), T([256, 512, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 12, 12], f16), T([1536, 512, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 24, 24], f16), T([768, 512, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 768, 25, 25], f16), T([768, 128, 3, 3], f16), T([768], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 6), {})\ncnt: 11, ((T([128, 768, 12, 12], f16), T([768, 128, 3, 3], f16), T([768], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 6), {})\ncnt: 6, ((T([128, 768, 12, 12], f16), T([1536, 768, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 9, ((T([128, 1536, 1, 1], f16), T([768, 1536, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 9, ((T([128, 768, 1, 1], f16), T([1536, 768, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 6, ((T([128, 1536, 12, 12], f16), T([768, 1536, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1536, 6, 6], f16), T([1536, 1536, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 768, 13, 13], f16), T([768, 128, 3, 3], f16), T([768], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 6), {})\ncnt: 5, ((T([128, 768, 6, 6], f16), T([768, 128, 3, 3], f16), T([768], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 6), {})\ncnt: 3, ((T([128, 768, 6, 6], f16), T([1536, 768, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 1536, 6, 6], f16), T([768, 1536, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1536, 6, 6], f16), T([3072, 1536, 1, 1], f16), T([3072], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic AOT Function with Graph Printing\nDESCRIPTION: Demonstrates the basic usage of aot_function with custom compiler functions that print the forward and backward FX graphs of a simple cosine operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch.compile import aot_function, aot_module, draw_graph\nimport torch.fx as fx\nimport torch\n\ndef print_graph(name):\n    def f(fx_g: fx.GraphModule, inps):\n        print(name)\n        print(fx_g.code)\n        return fx_g\n    return f\n\ndef f(x):\n    return x.cos().cos()\n\nnf = aot_function(f, fw_compiler=print_graph(\"forward\"), bw_compiler=print_graph(\"backward\"))\nnf(torch.randn(3, requires_grad=True))\n```\n\n----------------------------------------\n\nTITLE: Guard Generation Example in PyTorch\nDESCRIPTION: Demonstrates how guards are generated for a simple torch.compile function to check input tensor properties.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef fn(x):\n    return x + 1\n\nfn(torch.ones(3, 3))\n```\n\n----------------------------------------\n\nTITLE: Textual FX Graph with Placeholder, Call Function, and Output Nodes in PyTorch\nDESCRIPTION: This snippet demonstrates the textual representation of an Export IR graph as output from torch.export, showing nodes of type placeholder (inputs), call_function (ATen operator, here add), and output (return value node). The format reveals node names, typing/usage information, operation types, ATen targets, argument lists, and graph output structure. The code is illustrative and is meant for visualization/debugging, not for direct execution; it expects the context of torch.export and torch.fx.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ngraph():\n  %x : [num_users=1] = placeholder[target=x]\n  %y : [num_users=1] = placeholder[target=y]\n  %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x, %y), kwargs = {})\n  return (add,)\n```\n\n----------------------------------------\n\nTITLE: Conducting Convolutions in ATen with PyTorch\nDESCRIPTION: Performs 2D convolutions on input tensors, essential for feature extraction in CNNs. Dependencies involve input and weight tensors with specific shapes, strides, padding options, and other parameters. Outputs are processed tensors based on the input and kernel configurations. Limitations include the requirement for structured input shapes and parameter configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([16, 8, 128, 128], f16), T([64, 8, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 64, 128, 128], f16), T([128, 64, 4, 4], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 128, 64, 64], f16), T([256, 128, 4, 4], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 12, ((T([16, 256, 32, 32], f16), T([256, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 256, 32, 32], f16), T([256, 128, 4, 4], f16), None, [2, 2], [1, 1], [1, 1], True, [0, 0], 1), {})\ncnt: 1, ((T([16, 128, 64, 64], f16), T([128, 64, 4, 4], f16), None, [2, 2], [1, 1], [1, 1], True, [0, 0], 1), {})\ncnt: 1, ((T([16, 64, 128, 128], f16), T([3, 64, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This code snippet represents a summary of PyTorch operator usage in a neural network. It includes operator names, call counts, and tensor shapes for various operations. This information is crucial for understanding the network architecture and computational requirements.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net101_26w_4s_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 4, ((T([64, 26, 56, 56], f16), T([64, 26, 56, 56], f16, stride=(326144, 3136, 56, 1))), {})\ncnt: 6, ((T([64, 52, 28, 28], f16), T([64, 52, 28, 28], f16, stride=(163072, 784, 28, 1))), {})\ncnt: 44, ((T([64, 104, 14, 14], f16), T([64, 104, 14, 14], f16, stride=(81536, 196, 14, 1))), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Backward Layer Norm Calculation with native_layer_norm in PyTorch (Python)\nDESCRIPTION: Executes the backward computation for layer normalization via aten.native_layer_norm_backward, computing gradients necessary for parameter updates within the layer normalization step.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\naten.native_layer_norm_backward.default\ncnt: 25, ((T([16, 512, 768], f16), T([16, 512, 768], f16), [768], T([16, 512, 1], f32), T([16, 512, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Indexing and Copying Operations: clone, copy_, cumsum in PyTorch (Python)\nDESCRIPTION: Shows standard tensor deep copy (clone), value replacement (copy_), and cumulative sum (cumsum) ops, taking integer or float tensors with single or multi-dimensional shapes. cumsum requires a dimension argument; copy_ operates in-place, and clone produces an object with the same content but a new memory address.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 2, ((T([4, 128], i64),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 2, ((T([4, 128], i64), T([4, 128], i64)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.cumsum.default\ncnt: 1, ((T([4, 128], i32), 1), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Multiplication with aten.addmm in PyTorch\nDESCRIPTION: Applies aten.addmm.default for performing matrix multiplication followed by addition in f16 precision, essential for neural network layer computations. Dependencies include PyTorch and potentially CUDA for better performance.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\naten.addmm.default, ((T([512], f16), T([2048, 384], f16), T([384, 512], f16, stride=(1, 384))), {})\n```\n\n----------------------------------------\n\nTITLE: Testing FX Symbolic Traceability for Structured Pruning\nDESCRIPTION: Code snippet for verifying if a model is compatible with structured pruning by testing if it can be symbolically traced with torch.fx.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.fx import symbolic_trace\nmodel = MyModel()\nsymbolic_trace(model)\n```\n\n----------------------------------------\n\nTITLE: Preprocessor Conditional Code Path for CUDA and HIP\nDESCRIPTION: Shows how to handle conditional compilation paths based on CUDA and HIP versions in C++. It requires you to define certain macros when the code needs to be selective based on GPU library availability. This snippet is essential for developers transitioning codebases or writing cross-compatible GPU-accelerated code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/hip.rst#2025-04-22_snippet_2\n\nLANGUAGE: Preprocessor\nCODE:\n```\n#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(USE_ROCM)\n\n// Code path that excludes ROCm/HIP.\n\n#elif (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || defined(USE_ROCM)\n\n// Code path that includes ROCm/HIP.\n\n#endif\n```\n\n----------------------------------------\n\nTITLE: Implementing Relative Positional Embedding with First-Class Dimensions in Python\nDESCRIPTION: This snippet demonstrates how to implement relative positional embeddings using first-class dimensions in PyTorch. It uses dimensions for indexing arithmetic and embedding lookup.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef relative_positional_embedding(q, k, distance_embedding_weight):\n    batch, query_sequence, key_sequence, heads, features = dims(5)\n    q = q[batch, query_sequence, [heads, features]]\n    k = k[batch, key_sequence, [heads, features]]\n\n    distance = query_sequence - key_sequence\n    n_embeddings = distance_embedding_weight.size(0)\n    index_bias = n_embeddings // 2\n\n    assert key_sequence.size + bias <= n_embeddings\n\n    # indexing with dims\n    positional_embedding = distance_embedding_weight[distance + index_bias, features]\n\n    # matrix multiplies with dims\n    relative_position_scores_query = (q*positional_embedding).sum(features)\n    relative_position_scores_key = (k*positional_embedding).sum(features)\n    return  (relative_position_scores_query + relative_position_scores_key).order(batch, heads, key_sequence, query_sequence)\n```\n\n----------------------------------------\n\nTITLE: Setting Selected Operator List Configuration for PyTorch\nDESCRIPTION: Defines a CMake variable for specifying a YAML file that contains a list of operators to include for custom build. By default, all operators are included.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nset(SELECTED_OP_LIST\n    \"\"\n    CACHE\n      STRING\n      \"Path to the yaml file that contains the list of operators to include for custom build. Include all operators by default.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a MemPool with Custom Allocator\nDESCRIPTION: Code showing how to define a new memory pool by passing a custom allocator to torch.cuda.MemPool. This enables specialized memory allocation strategies for distributed training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\npool = torch.cuda.MemPool(allocator)\n```\n\n----------------------------------------\n\nTITLE: ReLU Backward Pass in PyTorch\nDESCRIPTION: Computes the gradients for the ReLU activation function during the backward pass of the neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\naten.threshold_backward.default((T([64, 4096], f16), T([64, 4096], f16), 0), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.threshold_backward.default((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Indexing Implementation in PyTorch Python\nDESCRIPTION: This snippet reflects the usage of the 'aten._index_put_impl_' operator, which handles in-place assignments to a tensor based on index arrays. It requires the PyTorch library for execution. The input includes a target tensor, index arrays, the values to assign, and flags dictating operational behavior. The operation outputs the modified tensor. This method is efficient for sparse update operations on tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._index_put_impl_.default\ncnt: 1, ((T([5000, 1], f32), [T([100], i64)], T([100, 1], f32, stride=(0, 0)), True, True), {})\ncnt: 1, ((T([5000, 4], f32), [T([100], i64)], T([100, 4], f32), True, True), {})\n```\n\n----------------------------------------\n\nTITLE: Example of Weight Tensor Before and After Pruning\nDESCRIPTION: Demonstrates how a weight tensor looks before and after applying unstructured pruning by zeroing out the lowest absolute value elements.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nW = [[1 2 3]\n     [4 5 6]\n     [7 1 9]]\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Android Dependencies in Gradle (Release)\nDESCRIPTION: This snippet shows how to configure Gradle dependencies for PyTorch Android, including both lite interpreter and full JIT builds. It specifies the repository and implementation details for release versions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Groovy\nCODE:\n```\nrepositories {\n    jcenter()\n}\n\n# lite interpreter build\ndependencies {\n    implementation 'org.pytorch:pytorch_android_lite:1.10.0'\n    implementation 'org.pytorch:pytorch_android_torchvision_lite:1.10.0'\n}\n\n# full jit build\ndependencies {\n    implementation 'org.pytorch:pytorch_android:1.10.0'\n    implementation 'org.pytorch:pytorch_android_torchvision:1.10.0'\n}\n```\n\n----------------------------------------\n\nTITLE: ONNX-Script Custom SELU Implementation\nDESCRIPTION: Example of implementing a custom SELU operator using ONNX-script and registering it for ONNX export.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n@onnxscript.script(custom_opset)\ndef Selu(X):\n    alpha = 1.67326  # auto wrapped as Constants\n    gamma = 1.0507\n    alphaX = op.CastLike(alpha, X)\n    gammaX = op.CastLike(gamma, X)\n    neg = gammaX * (alphaX * op.Exp(X) - alphaX)\n    pos = gammaX * X\n    zero = op.CastLike(0, X)\n    return op.Where(X <= zero, neg, pos)\n```\n\n----------------------------------------\n\nTITLE: Creating Benchmarking Function\nDESCRIPTION: Implements a benchmarking utility to measure forward and backward pass latencies\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport statistics\n\ndef bench(fn, args, prefix):\n    warmup = 10\n    iterations = 100\n\n    for _ in range(warmup):\n        ref = fn(*args)\n        ref.sum().backward()\n    \n    fw_latencies = []\n    bw_latencies = []\n    for _ in range(iterations):\n        for arg in args:\n            arg.grad = None\n\n        fw_begin = time.perf_counter()\n        ref = fn(*args)\n        fw_end = time.perf_counter()\n\n        loss = ref.sum() \n\n        bw_begin = time.perf_counter()\n        loss.backward()\n        bw_end = time.perf_counter()\n\n        fw_latencies.append(fw_end - fw_begin)\n        bw_latencies.append(bw_end - bw_begin)\n    \n    avg_fw_latency = statistics.mean(fw_latencies) * 10**6\n    avg_bw_latency = statistics.mean(bw_latencies) * 10**6\n    print(prefix, \"Fwd = \" + str(avg_fw_latency) + \" us\", \"Bwd = \" + str(avg_bw_latency) + \" us\", sep=', ')\n```\n\n----------------------------------------\n\nTITLE: Debugging Module Transformation Functions\nDESCRIPTION: A skeleton code example showing how to create a transformation function for PyTorch modules using FX symbolic tracing. This is a template for implementing module transformations with placeholders for actual transformation logic.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Backend with Decorator\nDESCRIPTION: Demonstrates how to register a custom backend using the register_backend decorator, allowing the backend to be referenced by name in torch.compile.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._dynamo import register_backend\n\n@register_backend\ndef my_compiler(gm, example_inputs):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Extended TensorOptions Configuration\nDESCRIPTION: Shows how to specify multiple tensor properties using chained method calls.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::ones(10, torch::TensorOptions().dtype(torch::kFloat32).layout(torch::kStrided))\n```\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::ones(10, torch::dtype(torch::kFloat32).layout(torch::kStrided))\n```\n\n----------------------------------------\n\nTITLE: Braced Set Initialization in Python\nDESCRIPTION: This snippet shows how to initialize sets using braces, including single-element sets, multi-element sets, and set comprehensions. It also demonstrates a dictionary containing sets and a set containing a complex object.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/set_linter_testdata/python_code.py.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nset1 = {1}\nset2 = {1, 2}\n\niterator_set = {i for i in range(10)}\n\n# A dict with two sets.\ndict_set = {\"a\": {2, 3}, \"b\": {i for i in range(3)}}\n\n# A set containing an object constructed with a dict and a set\nsos_set = {Something({i: i + 1 for i in range(3)}, {i + 1 for i in range(3)})}\n```\n\n----------------------------------------\n\nTITLE: Applying Sigmoid Activation in PyTorch\nDESCRIPTION: The sigmoid operator applies the sigmoid activation function on a tensor, mapping input values into the (0, 1) interval. This function is commonly used in classification tasks to form probability distributions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([32, 256, 1, 1], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 8, ((T([32, 512, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations with Layer Normalization\nDESCRIPTION: Collection of tensor operations showing various configurations of layer normalization with different tensor shapes and strides. Operations include forward and backward passes with half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 56, 56, 128], f16, stride=(401408, 56, 1, 3136)), [128], T([128], f16), T([128], f16), 1e-06), {})\ncnt: 3, ((T([32, 56, 56, 128], f16), [128], T([128], f16), T([128], f16), 1e-06), {})\ncnt: 3, ((T([32, 28, 28, 256], f16), [256], T([256], f16), T([256], f16), 1e-06), {})\n```\n\n----------------------------------------\n\nTITLE: Running Specific PyTorch C++ Test\nDESCRIPTION: Executes a single C++ test within a specific test suite using the Google Test framework.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/test_jit --gtest_filter=ContainerAliasingTest.MayContainAlias\n```\n\n----------------------------------------\n\nTITLE: Debugging FX Generated Code with PDB\nDESCRIPTION: Shows how to use Python debugger (pdb) to debug FX-generated forward functions in transformed modules.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)\n```\n\n----------------------------------------\n\nTITLE: Embedding Lookups with embedding in PyTorch (Python)\nDESCRIPTION: Performs aten.embedding, useful for embedding lookup tables used in natural language processing, where it maps indices to vector representations, facilitating the input of categorical data into a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\naten.embedding.default\ncnt: 1, ((T([30522, 768], f16), T([16, 512], i64), 0), {})\ncnt: 1, ((T([512, 768], f16), T([1, 512], i64)), {})\ncnt: 4, ((T([1024, 768], f16), T([16, 512], i64, stride=(2048, 4))), {})\ncnt: 2, ((T([1024, 768], f16), T([16, 512], i64)), {})\ncnt: 1, ((T([2, 768], f16), T([16, 512], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Python Class 'LongWithShortDocstring' With Insufficient Docstring\nDESCRIPTION: This snippet defines a Python class named 'LongWithShortDocstring' with a docstring that is too short (only 10 characters). The docstring contains only a TODO placeholder.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt.before.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass LongWithShortDocstring:\n    \"\"\"TODO\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Optimizations Documentation in RestructuredText\nDESCRIPTION: Documentation for PyTorch's optimization functions, specifically the 'compile' function, with a link to more detailed documentation, formatted in RestructuredText.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_10\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    compile\n\n`torch.compile documentation <https://pytorch.org/docs/main/torch.compiler.html>`__\n```\n\n----------------------------------------\n\nTITLE: Setting Up Conda Environment for PyTorch Source Build (Linux)\nDESCRIPTION: This Bash script demonstrates how to set up a dedicated Conda environment on a Linux system before building PyTorch from source. It involves activating the base Conda installation, creating a new named environment, and activating it. This isolates the PyTorch build dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ source <CONDA_INSTALL_DIR>/bin/activate\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n```\n\n----------------------------------------\n\nTITLE: Using Distributed Autograd Context in PyTorch\nDESCRIPTION: Shows how to use the distributed autograd context manager to properly setup and execute distributed backward pass. The context ensures that all send and recv functions are properly stored.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch.distributed.autograd as dist_autograd\nwith dist_autograd.context() as context_id:\n  loss = model.forward()\n  dist_autograd.backward(context_id, loss)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Ensemble Prediction Performance with Timer - PyTorch - Python\nDESCRIPTION: Benchmarks the execution time of prediction generation with and without vmap for 100 iterations, leveraging torch.utils.benchmark.Timer. Outputs comparative timing for the loop-based and vectorized approaches. Useful for demonstrating practical performance gains from adopting vmap for ensembling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.benchmark import Timer\nwithout_vmap = Timer(\n    stmt=\"[model(minibatch) for model, minibatch in zip(models, minibatches)]\",\n    globals=globals())\nwith_vmap = Timer(\n    stmt=\"vmap(fmodel)(params, buffers, minibatches)\",\n    globals=globals())\nprint(f'Predictions without vmap {without_vmap.timeit(100)}')\nprint(f'Predictions with vmap {with_vmap.timeit(100)}')\n```\n\n----------------------------------------\n\nTITLE: Implementing Self-Deleting Temporary File with Pack/Unpack Hooks in PyTorch\nDESCRIPTION: Example implementation of pack_hook and unpack_hook for tensor serialization with automatic temporary file cleanup. The hooks allow saving tensors to disk and loading them back during backward pass computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass SelfDeletingTempFile():\n    def __init__(self):\n        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n\n    def __del__(self):\n        os.remove(self.name)\n\ndef pack_hook(tensor):\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(temp_file):\n    return torch.load(temp_file.name)\n```\n\n----------------------------------------\n\nTITLE: Using ATen _softmax_backward_data Operator in PyTorch\nDESCRIPTION: This snippet covers the `aten._softmax_backward_data` operator, critical for gradient calculation in neural networks when softmax is applied. Input tensors include shapes like [256, 64, 64] and [256, 256, 256] with f16 data type, dimension -1. Ensures compatibility with tensor operations where differentiability is required.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 1, ((T([256, 64, 64], f16), T([256, 64, 64], f16), -1, f16), {})\ncnt: 2, ((T([256, 256, 256], f16), T([256, 256, 256], f16), -1, f16), {})\ncnt: 1, ((T([256, 1024, 1024], f16), T([256, 1024, 1024], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Embedding Layer Lookups and Backward Passes in PyTorch (Python)\nDESCRIPTION: These snippets detail usage of torch.embedding and embedding_dense_backward, for forward and backward passes in embedding/table lookup layers. Arguments include weight matrices, index tensors, and scale/offset values. The backward variant computes gradients; input tensors must match expected shapes for correct mapping between ids and embeddings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.embedding.default\ncnt: 1, ((T([30522, 768], f16), T([4, 128], i64), 0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.embedding.default\ncnt: 1, ((T([2, 768], f16), T([4, 128], i64, stride=(0, 1))), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.embedding.default\ncnt: 1, ((T([512, 768], f16), T([4, 128], i64), 0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([4, 128, 768], f16), T([4, 128], i64), 512, 0, False), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([4, 128, 768], f16), T([4, 128], i64, stride=(0, 1)), 2, -1, False), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([4, 128, 768], f16), T([4, 128], i64), 30522, 0, False), {})\n```\n\n----------------------------------------\n\nTITLE: Disabling Reduced Precision Reduction for FP16 GEMMs in PyTorch (C++)\nDESCRIPTION: Provides the C++ method to disable reduced precision reductions in FP16 GEMMs performed via CuBLAS within PyTorch. It uses `at::globalContext().setAllowFP16ReductionCuBLAS(false)` to enforce full precision reductions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nat::globalContext().setAllowFP16ReductionCuBLAS(false);\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Debug Flags in CMake\nDESCRIPTION: If the DEBUG_CUDA variable is set, this snippet appends CUDA compiler flags useful for debugging. It always adds `-lineinfo` to debug and release-with-debinfo builds. Additionally, if the CUDA compiler version is less than 12.1, it appends `--source-in-ptx` to potentially aid debugging, avoiding a known crash with CUDA 12.1.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_34\n\nLANGUAGE: cmake\nCODE:\n```\nif(DEBUG_CUDA)\n  string(APPEND CMAKE_CUDA_FLAGS_DEBUG \" -lineinfo\")\n  string(APPEND CMAKE_CUDA_FLAGS_RELWITHDEBINFO \" -lineinfo\")\n  # CUDA-12.1 crashes when trying to compile with --source-in-ptx See\n  # https://github.com/pytorch/pytorch/issues/102372#issuecomment-1572526893\n  if(CMAKE_CUDA_COMPILER_VERSION VERSION_LESS 12.1)\n    string(APPEND CMAKE_CUDA_FLAGS_DEBUG \" --source-in-ptx\")\n    string(APPEND CMAKE_CUDA_FLAGS_RELWITHDEBINFO \" --source-in-ptx\")\n  endif()\nendif(DEBUG_CUDA)\n```\n\n----------------------------------------\n\nTITLE: Square Root Operation in PyTorch\nDESCRIPTION: The example for aten.sqrt.default shows computing the square root of a scalar tensor in float32, a basic mathematical operation common in data normalization processes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sqrt.default\ncnt: 24, ((T([], f32),), {})\n```\n\n----------------------------------------\n\nTITLE: Running Tests and Generating Coverage Report Separately\nDESCRIPTION: This snippet demonstrates how to run tests separately and then generate a coverage report without re-running the tests, useful for accelerating development.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# run tests when you are developing a new feature, assume the test is `test_nn.py`\npython oss_coverage.py --run-only=test_nn.py\n# or you can run it yourself\ncd test/ && python test_nn.py\n# then you want to learn about code coverage, you can just run:\npython oss_coverage.py --run-only=test_nn.py --export --summary\n```\n\n----------------------------------------\n\nTITLE: Configuring QNNPACK Test Executables in CMake\nDESCRIPTION: Sets up multiple test executables for QNNPACK, including compiler flags, include directories, and linking libraries. Each test is added as a CMake test target.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(u8lut32norm-test test/u8lut32norm.cc)\nset_target_properties(u8lut32norm-test PROPERTIES\n  CXX_STANDARD 14\n  CXX_STANDARD_REQUIRED YES\n  CXX_EXTENSIONS NO)\ntarget_include_directories(u8lut32norm-test PRIVATE src test)\ntarget_link_libraries(u8lut32norm-test PRIVATE pytorch_qnnpack cpuinfo fp16 gtest gtest_main)\nadd_test(u8lut32norm-test u8lut32norm-test)\n```\n\n----------------------------------------\n\nTITLE: CLOG Library Target Configuration\nDESCRIPTION: Configures the main CLOG static library target with compiler settings, include directories, and platform-specific definitions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(clog STATIC src/clog.c)\nset_target_properties(clog PROPERTIES\n  C_STANDARD 99\n  C_EXTENSIONS NO)\nCLOG_TARGET_RUNTIME_LIBRARY(clog)\nset_target_properties(clog PROPERTIES PUBLIC_HEADER include/clog.h)\ntarget_include_directories(clog PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include> $<INSTALL_INTERFACE:${CMAKE_INSTALL_INCLUDEDIR}>)\n```\n\n----------------------------------------\n\nTITLE: Unbinding Nested Jagged Tensors in PyTorch\nDESCRIPTION: Demonstrates creating a nested jagged tensor and using the unbind() method to retrieve the constituent tensors. Shows that unbind() returns slices of the underlying memory rather than copies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt.unbind()\n(tensor([[-0.9916, -0.3363, -0.2799],\n        [-2.3520, -0.5896, -0.4374]]), tensor([[-2.0969, -1.0104,  1.4841],\n        [ 2.0952,  0.2973,  0.2516],\n        [ 0.9035,  1.3623,  0.2026]]))\n>>> nt.unbind()[0] is not a\nTrue\n>>> nt.unbind()[0].mul_(3)\ntensor([[ 3.6858, -3.7030, -4.4525],\n        [-2.3481,  2.0236,  0.1975]])\n>>> nt.unbind()\n(tensor([[-2.9747, -1.0089, -0.8396],\n        [-7.0561, -1.7688, -1.3122]]), tensor([[-2.0969, -1.0104,  1.4841],\n        [ 2.0952,  0.2973,  0.2516],\n        [ 0.9035,  1.3623,  0.2026]]))\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Model with ONNX Runtime\nDESCRIPTION: Example showing how to execute an exported ONNX model using ONNX Runtime, including input preparation and inference.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport onnxruntime as ort\nimport numpy as np\n\nort_session = ort.InferenceSession(\"alexnet.onnx\")\n\noutputs = ort_session.run(\n    None,\n    {\"actual_input_1\": np.random.randn(10, 3, 224, 224).astype(np.float32)},\n)\nprint(outputs[0])\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations in PyTorch\nDESCRIPTION: Performs 2D convolution operations with various input, kernel, and output sizes. Includes parameters for stride, padding, and dilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\naten.convolution.default((T([64, 3, 224, 224], f16), T([64, 3, 3, 3], f16), T([64], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.convolution.default((T([64, 64, 224, 224], f16), T([64, 64, 3, 3], f16), T([64], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations in PyTorch Neural Network\nDESCRIPTION: Summary of convolution operations (aten.convolution.default) in the model, showing counts, tensor shapes, strides, padding, and other parameters. These represent the various convolutional layers throughout the neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([224, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 224, 112, 112], f16), T([224, 112, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 2), {})\ncnt: 1, ((T([32, 224, 1, 1], f16), T([8, 224, 1, 1], f16), T([8], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 8, 1, 1], f16), T([224, 8, 1, 1], f16), T([224], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 224, 56, 56], f16), T([224, 224, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([224, 32, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 224, 56, 56], f16), T([224, 112, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})\ncnt: 1, ((T([32, 224, 1, 1], f16), T([56, 224, 1, 1], f16), T([56], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 56, 1, 1], f16), T([224, 56, 1, 1], f16), T([224], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 224, 56, 56], f16), T([448, 224, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 448, 56, 56], f16), T([448, 112, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 4), {})\ncnt: 1, ((T([32, 448, 1, 1], f16), T([56, 448, 1, 1], f16), T([56], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 56, 1, 1], f16), T([448, 56, 1, 1], f16), T([448], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 9, ((T([32, 448, 28, 28], f16), T([448, 448, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 224, 56, 56], f16), T([448, 224, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([32, 448, 28, 28], f16), T([448, 112, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 4), {})\ncnt: 4, ((T([32, 448, 1, 1], f16), T([112, 448, 1, 1], f16), T([112], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([32, 112, 1, 1], f16), T([448, 112, 1, 1], f16), T([448], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 448, 28, 28], f16), T([896, 448, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 896, 28, 28], f16), T([896, 112, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 8), {})\ncnt: 1, ((T([32, 896, 1, 1], f16), T([112, 896, 1, 1], f16), T([112], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 112, 1, 1], f16), T([896, 112, 1, 1], f16), T([896], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 21, ((T([32, 896, 14, 14], f16), T([896, 896, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 448, 28, 28], f16), T([896, 448, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 10, ((T([32, 896, 14, 14], f16), T([896, 112, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 8), {})\ncnt: 10, ((T([32, 896, 1, 1], f16), T([224, 896, 1, 1], f16), T([224], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 10, ((T([32, 224, 1, 1], f16), T([896, 224, 1, 1], f16), T([896], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 896, 14, 14], f16), T([2240, 896, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 2240, 14, 14], f16), T([2240, 112, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 20), {})\ncnt: 1, ((T([32, 2240, 1, 1], f16), T([224, 2240, 1, 1], f16), T([224], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 224, 1, 1], f16), T([2240, 224, 1, 1], f16), T([2240], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 2240, 7, 7], f16), T([2240, 2240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 896, 14, 14], f16), T([2240, 896, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Batched Matrix Multiplication in PyTorch\nDESCRIPTION: This snippet illustrates the application of the ATen bmm (batched matrix multiplication) operator, reflecting its extensive use with batches of matrices having diverse configurations. It is pivotal in batch processing in machine learning, enabling parallel, efficient calculations typically used in sequence operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 6, ((T([192, 128, 64], f16), T([192, 64, 128], f16)), {})\ncnt: 6, ((T([192, 128, 128], f16), T([192, 128, 64], f16)), {})\ncnt: 6, ((T([192, 128, 128], f16, stride=(16384, 1, 128)), T([192, 128, 64], f16)), {})\ncnt: 6, ((T([192, 128, 64], f16), T([192, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 6, ((T([192, 64, 128], f16, stride=(8192, 1, 64)), T([192, 128, 128], f16)), {})\ncnt: 6, ((T([192, 128, 128], f16), T([192, 128, 64], f16, stride=(8192, 1, 128))), {})\n```\n\n----------------------------------------\n\nTITLE: Managing CUDA Streams on Multiple Devices in PyTorch C++\nDESCRIPTION: This example demonstrates how to acquire and set CUDA streams on multiple devices, using CUDAGuard and CUDAStreamGuard to manage device and stream contexts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// acquire new CUDA streams from CUDA stream pool on device 0 and device 1\nat::cuda::CUDAStream myStream0 = at::cuda::getStreamFromPool(false, 0);\nat::cuda::CUDAStream myStream1 = at::cuda::getStreamFromPool(false, 1);\n\n// set current CUDA stream to `myStream0` on device 0\nat::cuda::setCurrentCUDAStream(myStream0);\n// set current CUDA stream to `myStream1` on device 1\nat::cuda::setCurrentCUDAStream(myStream1);\n\n// create a tensor on device 0, no need to specify device index since\n// current device index is 0\ntorch::Tensor tensor0 = torch::ones({2, 2}, torch::device(at::kCUDA));\n// sum() on tensor0 use `myStream0` as current CUDA stream on device 0\ntensor0.sum();\n\n// change the current device index to 1 by using CUDA device guard within a bracket scope\n{\n  at::cuda::CUDAGuard device_guard{1};\n  // create a tensor on device 1\n  torch::Tensor tensor1 = torch::ones({2, 2}, torch::device(at::kCUDA));\n  // sum() on tensor 1 uses `myStream1` as current CUDA stream on device 1\n  tensor1.sum();\n}\n\n// current device is reset to device 0 after device_guard is destroyed\n\n// acquire a new CUDA stream on device 1\nat::cuda::CUDAStream myStream1_1 = at::cuda::getStreamFromPool(false, 1);\n// create a new tensor on device 1\ntorch::Tensor tensor1 = torch::ones({2, 2}, torch::device({torch::kCUDA, 1}));\n\n// change the current device index to 1 and current CUDA stream on device 1\n// to `myStream1_1` using CUDA stream guard within a scope\n{\n  at::cuda::CUDAStreamGuard stream_guard(myStream1_1);\n  // sum() on tensor1 use `myStream1_1` as current CUDA stream on device 1\n  tensor1.sum();\n}\n\n// current device is reset to device 0 and current CUDA stream on device 1 is\n// reset to `myStream1`\n\n// sum() on tensor1 uses `myStream1` as current CUDA stream on device 1\ntensor1.sum();\n```\n\n----------------------------------------\n\nTITLE: Computing Angle and Absolute Values\nDESCRIPTION: Shows how to compute the angle and absolute values of complex tensors using torch.angle and torch.abs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nx1=torch.tensor([3j, 4+4j])\nx1.abs()\nx1.angle()\n```\n\n----------------------------------------\n\nTITLE: Accessing Named Tensor Properties in PyTorch\nDESCRIPTION: This snippet demonstrates how to access the names and shape properties of a named tensor in PyTorch. The example uses a flattened images tensor that has named dimensions 'N' for batch and 'features' for the flattened feature vector, with a shape of 32 samples and 49152 features.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> flat_imgs.names, flat_imgs.shape\n(('N', 'features'), torch.Size([32, 49152]))\n```\n\n----------------------------------------\n\nTITLE: Replacing Add with Mul in PyTorch FX Graph\nDESCRIPTION: This snippet demonstrates how to replace torch.ops.aten.add.Tensor calls with torch.ops.aten.mul.Tensor calls in a PyTorch FX graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef replace_add_with_mul(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n    for node in gm.graph.nodes:\n        if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor:\n            node.target = torch.ops.aten.mul.Tensor\n```\n\n----------------------------------------\n\nTITLE: Initializing Convolution Operation in PyTorch\nDESCRIPTION: This snippet represents the configuration setup for a convolution operation in PyTorch using half-precision (f16) tensors. It details shape, kernel size, stride, padding, dilation, and group count parameters necessary for the convolution operation. Expected outputs include convolved tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pnasnet5large_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([16, 1080, 42, 42], f16), T([432, 1080, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 5, ((T([16, 2160, 21, 21], f16), T([432, 2160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Debugging PyTorch Setup Issues\nDESCRIPTION: A series of commands to debug and resolve issues when running 'python setup.py develop'. This includes compiling a simple C program, cleaning the build directory, and resetting the Git repository.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nprintf '#include <stdio.h>\\nint main() { printf(\"Hello World\");}'|clang -x c -; ./a.out\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit submodule deinit -f .\ngit clean -xdf\npython setup.py clean\ngit submodule update --init --recursive\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Parameters in Optimizer State Dict\nDESCRIPTION: This example demonstrates how to handle missing parameters when loading an optimizer state dict for a model with a modified structure. It uses a custom hook to adapt the state dict, mapping existing parameters and ignoring new ones.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef adapt_state_dict_missing_param(optimizer, state_dict):\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n    for k, v in state_dict['param_groups'][0].items():\n        if k not in ['params', 'param_names']:\n            adapted_state_dict['param_groups'][0][k] = v\n\n    lookup_dict = {\n        'fc.weight': 'fc.weight',\n        'fc.bias': 'fc.bias',\n        'bypass.weight': None,\n    }\n\n    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}\n    for param_id, param_name in zip(\n            optimizer.state_dict()['param_groups'][0]['params'],\n            optimizer.state_dict()['param_groups'][0]['param_names']):\n        name_in_loaded = lookup_dict[param_name]\n        if name_in_loaded in state_dict['param_groups'][0]['param_names']:\n            index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)\n            id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]\n            # Copy the state of the corresponding parameter\n            if id_in_loaded in state_dict['state']:\n                adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\noptimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)\noptimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Seed in PyTorch\nDESCRIPTION: Imports essential PyTorch modules (`torch`, `nn`, `F`), `partial` from `functools`, and sets the random seed for reproducibility using `torch.manual_seed(0)`. These imports are prerequisites for subsequent model definition, data generation, and gradient computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\n\ntorch.manual_seed(0);\n```\n\n----------------------------------------\n\nTITLE: Built-in TorchScript Classes Example\nDESCRIPTION: Example showing usage of built-in TorchScript classes including torch.device and tensor operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.jit.script\nclass A:\n    def __init__(self):\n        self.x = torch.rand(3)\n\n    def f(self, y: torch.device):\n        return self.x.to(device=y)\n\ndef g():\n    a = A()\n    return a.f(torch.device(\"cpu\"))\n\nscript_g = torch.jit.script(g)\nprint(script_g.graph)\n```\n\n----------------------------------------\n\nTITLE: Creating ThreadPoolExecutors for Asynchronous Data Transfer in PyTorch\nDESCRIPTION: Implements two ThreadPoolExecutors with one worker each for device-to-host (D2H) and host-to-device (H2D) data transfers. Uses CUDA streams and semaphores with CUDA events for synchronization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/CHANGELOG.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nd2h_executor = ThreadPoolExecutor(max_workers=1)\nh2d_executor = ThreadPoolExecutor(max_workers=1)\n\n# In each executor\ncuda_stream = torch.cuda.Stream()\nwith torch.cuda.stream(cuda_stream):\n    # Perform data transfer\n```\n\n----------------------------------------\n\nTITLE: Name Unification in Binary Operations\nDESCRIPTION: Shows how binary arithmetic operations unify names from input tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n>>> tensor = torch.randn(3, 3, names=('N', None))\n>>> other = torch.randn(3, 3, names=(None, 'C'))\n>>> (tensor + other).names\n('N', 'C')\n```\n\n----------------------------------------\n\nTITLE: ReLU Activation in PyTorch\nDESCRIPTION: This snippet shows the ReLU (Rectified Linear Unit) activation operation. It is applied in-place on a 4D tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 3, ((T([64, 64, 112, 112], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization Operations\nDESCRIPTION: Native batch normalization forward and backward operation calls with tensor shapes, running statistics, and gradient computation parameters. Shows the usage pattern in a neural network with varying channel dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 32, 56, 56], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Tensor Operations in PyTorch - Python\nDESCRIPTION: This snippet specifies tensor operations including shape dimensions, data types, and configurations like stride and pooling settings. These tuples are likely used for simulating or testing tensor operations in neural networks. Dependencies include a working installation of PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([32, 512, 32, 32], f16), T([32, 256, 32, 32], f16), T([512, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([32, 3, 256, 256], f16), T([32, 3, 256, 256], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([32, 2048, 8, 8], f16, stride=(2048, 1, 0, 0)), 64), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([32], i64),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([32, 128, 128, 128], f16), [3, 3], [2, 2], [1, 1]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 3, ((T([32, 2, 64, 64, 64], f16), T([32, 2, 64, 1, 1], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 4, ((T([32, 2048, 8, 8], f16), T([32, 2048, 8, 8], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Sum Operations with SymInt in PyTorch\nDESCRIPTION: A single record of a symbolic integer summation operation (aten.sum.SymInt) applied to a tensor with shape [8, 1000] and float16 data type. The operation reduces along dimension 0 with keep_dim=True.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([8, 1000], f16, stride=(0, 0)), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Backward Pass using aten._softmax_backward_data in PyTorch\nDESCRIPTION: Employs aten._softmax_backward_data.default for calculating the derivatives of the softmax layer output based on inputs in a four-dimensional format in f16. Requires PyTorch and CUDA for execution with large tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\naten._softmax_backward_data.default, ((T([16, 4, 128, 128], f16), T([16, 4, 128, 128], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating BC-safe function signature change in Python\nDESCRIPTION: Example of a backwards-compatible change to a function signature where a new parameter with a default value is added to an existing function. This type of change preserves compatibility with existing code as it doesn't require callers to change their invocation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# before\ndef foo(x, y):\n    return x, y\n```\n\nLANGUAGE: python\nCODE:\n```\n# after\ndef foo(x, y, z=100):\n    return x, y, z\n```\n\n----------------------------------------\n\nTITLE: Collating Data with PyTorch DataPipes\nDESCRIPTION: Demonstrates the use of the collate() method to process batched data in DataPipes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3).collate()\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Python Implementation of LSTM Cell Using TorchScript\nDESCRIPTION: Python code for an LSTM cell that can be JIT-compiled using PyTorch's TorchScript system. This is the high-level representation that will be converted to the internal graph format and then optimized before execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\ndef LSTMCellS(x, hx, cx, w_ih, w_hh, b_ih, b_hh):\n    gates = x.mm(w_ih.t()) + hx.mm(w_hh.t()) + b_ih + b_hh\n    ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n    ingate = torch.sigmoid(ingate)\n    forgetgate = torch.sigmoid(forgetgate)\n    cellgate = torch.tanh(cellgate)\n    outgate = torch.sigmoid(outgate)\n    cy = (forgetgate * cx) + (ingate * cellgate)\n    hy = outgate * torch.tanh(cy)\n    return hy, cy\n```\n\n----------------------------------------\n\nTITLE: View Operations for Attention Mechanism Reshaping\nDESCRIPTION: Lists the usage of unsafe view operations to reshape tensors for attention heads and sequences. These operations change tensor dimensions without copying data, enabling efficient reshaping for multi-head attention calculations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 108, ((T([2, 16, 576, 48], f16), [32, 576, 48]), {})\ncnt: 36, ((T([2, 16, 48, 576], f16), [32, 48, 576]), {})\ncnt: 36, ((T([32, 576, 576], f16), [2, 16, 576, 576]), {})\ncnt: 144, ((T([2, 576, 576, 16], f16), [663552, 16]), {})\ncnt: 72, ((T([663552, 16], f16), [2, 576, 576, 16]), {})\ncnt: 72, ((T([2, 16, 576, 576], f16), [32, 576, 576]), {})\ncnt: 36, ((T([32, 576, 48], f16), [2, 16, 576, 48]), {})\ncnt: 36, ((T([2, 576, 16, 48], f16), [2, 576, 768]), {})\ncnt: 2, ((T([2, 16, 48, 577], f16), [32, 48, 577]), {})\ncnt: 2, ((T([32, 1, 577], f16), [2, 16, 1, 577]), {})\ncnt: 2, ((T([2, 16, 577, 48], f16), [32, 577, 48]), {})\ncnt: 2, ((T([32, 1, 48], f16), [2, 16, 1, 48]), {})\ncnt: 2, ((T([2, 577, 16, 48], f16), [2, 577, 768]), {})\ncnt: 2, ((T([2, 577, 768], f16), [1154, 768]), {})\ncnt: 36, ((T([2, 576, 3, 16, 48], f16), [2, 576, 2304]), {})\n```\n\n----------------------------------------\n\nTITLE: QAT Module Swap Result for Linear-ReLU\nDESCRIPTION: This code snippet shows the result of QAT module swapping for the Linear-ReLU module. It demonstrates how the fused module is replaced with a QAT-ready version that includes weight fake quantization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nGraphModule(\n  (linear): LinearReLU(\n    in_features=5, out_features=10, bias=True\n    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n  )\n)\n\ndef forward(self, x):\n    linear = self.linear(x);  x = None\n    return linear\n```\n\n----------------------------------------\n\nTITLE: Sigmoid Backward Operations in PyTorch\nDESCRIPTION: This snippet shows the backward pass for sigmoid activation, used during gradient computation. This operation computes gradients through the sigmoid function based on the derivative of sigmoid with respect to its input.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sigmoid_backward.default\ncnt: 1, ((T([96, 65], f16, stride=(0, 0)), T([96, 65], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Shuffling Data with PyTorch DataPipes\nDESCRIPTION: Shows how to use the shuffle() method to randomize the order of data in DataPipes. Demonstrates shuffling at different levels and across batches.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).shuffle()\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3).shuffle()\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3).shuffle(unbatch_level = -1).batch(3)\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-threaded Static Runtime with Thread Pool\nDESCRIPTION: Demonstrates how to implement data parallel execution using Static Runtime with a thread pool. Uses boost::lockfree::stack to cache runtime instances for efficient multi-threaded inference.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/runtime/static/README.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n  // initialization\n  auto mod = PrepareForStaticRuntime(m);\n  // 128 is good for most cases. Pick a number that works for you\n  boost::lockfree::stack<std::shared_ptr<StaticRuntime>,\n    boost::lockfree::fixed_sized<true>> pool(128);\n\n  // inference\n  std::shared_ptr<StaticRuntime> runtime = nullptr;\n  pool.pop(runtime);\n  if (!runtime) {\n    runtime = std::make_shared<StaticRuntime>(mod, opts);\n  }\n  auto output = runtime->run(args, kwargs);\n  pool.push(runtime);\n```\n\n----------------------------------------\n\nTITLE: Custom NumPy Sort Function with Manual vmap Implementation\nDESCRIPTION: Implementation of a custom sorting function using NumPy with manual vmap support. Includes forward, backward, setup_context, and vmap methods with proper tensor-numpy conversion handling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        return (\n            torch.tensor(result, device=device),\n            torch.tensor(ind, device=device),\n            torch.tensor(ind_inv, device=device),\n        )\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        ind, ind_inv = ctx.saved_tensors\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\n    @staticmethod\n    def vmap(info, in_dims, x, dim):\n        x_bdim, _ = in_dims\n        x = x.movedim(x_bdim, 0)\n        dim = dim if dim >= 0 else dim + x.dim() - 1\n        result = NumpySort.apply(x, dim + 1)\n        return NumpySort.apply(x, dim + 1), (0, 0, 0)\n```\n\n----------------------------------------\n\nTITLE: C++ Inference Implementation\nDESCRIPTION: Example C++ code demonstrating how to load and use the compiled model for inference, including support for dynamic batch sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n#include <iostream>\n#include <vector>\n\n#include <torch/torch.h>\n#include <torch/csrc/inductor/aoti_package/model_package_loader.h>\n\nint main() {\n    c10::InferenceMode mode;\n\n    torch::inductor::AOTIModelPackageLoader loader(\"model.pt2\");\n    std::vector<torch::Tensor> inputs = {torch::randn({8, 10}, at::kCUDA)};\n    std::vector<torch::Tensor> outputs = loader.run(inputs);\n    std::cout << \"Result from the first inference:\"<< std::endl;\n    std::cout << outputs[0] << std::endl;\n\n    std::cout << \"Result from the second inference:\"<< std::endl;\n    std::cout << loader.run({torch::randn({1, 10}, at::kCUDA)})[0] << std::endl;\n\n    return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Automatic Model Splitting for Pipeline Parallelism in Python\nDESCRIPTION: This snippet demonstrates how to use the pipeline API to automatically split a model for pipeline parallelism. It specifies a split point before a specific layer and creates a pipeline object.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.pipelining.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.pipelining import pipeline, SplitPoint\n\n# An example micro-batch input\nx = torch.LongTensor([1, 2, 4, 5])\n\npipe = pipeline(\n    module=mod,\n    mb_args=(x,),\n    split_spec={\n        \"layers.1\": SplitPoint.BEGINNING,\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Building the PyTorch Docker Image with CUDA Support - bash\nDESCRIPTION: This snippet builds the official PyTorch Docker image using a provided Dockerfile with CUDA 11.1 and cuDNN v8 support. Docker version 18.06 or higher is required. You may specify the Python version with the PYTHON_VERSION make variable. Execute this in the PyTorch source directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nmake -f docker.Makefile\n```\n\n----------------------------------------\n\nTITLE: Using torch._dynamo.disallow_in_graph for Operators\nDESCRIPTION: Example demonstrating how to disallow specific operators from being included in the TorchDynamo graph, causing a graph break and fallback to eager mode execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fine_grain_apis.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntorch._dynamo.disallow_in_graph(torch.ops.custom_op)\n```\n\n----------------------------------------\n\nTITLE: Defining Fusion Pattern Structure in PyTorch\nDESCRIPTION: This snippet defines the structure of fusion patterns used for matching operations in PyTorch. It explains that patterns consist of operators (which can be module types, functional operators, or PyTorch operators) followed by nested patterns for the operator's arguments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/pattern.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\noperator = module_type | functional | torch op | native op | MatchAllNode\nPattern = (operator, Pattern, Pattern, ...) | operator\n```\n\n----------------------------------------\n\nTITLE: Creating a Tensor with Custom Options in PyTorch C++\nDESCRIPTION: This snippet demonstrates how to create a tensor using the full() factory function with custom TensorOptions. It also shows how to verify the tensor's properties.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor tensor = torch::full({3, 4}, /*value=*/123, options);\n\nassert(tensor.dtype() == torch::kFloat32);\nassert(tensor.layout() == torch::kStrided);\nassert(tensor.device().type() == torch::kCUDA); // or device().is_cuda()\nassert(tensor.device().index() == 1);\nassert(tensor.requires_grad());\n```\n\n----------------------------------------\n\nTITLE: Demonstrating If-statement Structure in PyTorch IR\nDESCRIPTION: This code snippet shows the general structure of an if-statement (prim::If) in PyTorch's IR. It demonstrates how the true and false branches are represented as separate blocks, and how the outputs are handled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n%y_1, ..., %y_r = prim::If(%condition)\n  block0():  # TRUE BRANCH, never takes arguments, has to return r outputs\n    %t_1, ..., %t_k = some::node(%a_value_from_outer_block)\n    -> (%t_1, ..., %t_r)\n  block1():  # FALSE BRANCH, never takes arguments, has to return r outputs\n    %f_1, ..., %f_m = some::node(%a_value_from_outer_block)\n    -> (%f_1, ..., %f_r)\n```\n\n----------------------------------------\n\nTITLE: Constructing Semi-Structured Sparse Tensor in PyTorch\nDESCRIPTION: Creates a dense tensor adhering to 2:4 sparse format, then compresses it to a semi-structured sparse tensor using to_sparse_semi_structured function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.sparse import to_sparse_semi_structured\nA = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda()\nA_sparse = to_sparse_semi_structured(A)\n```\n\n----------------------------------------\n\nTITLE: Tensor Property Conversion in C++\nDESCRIPTION: Shows how to convert tensors between different data types and devices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor source_tensor = torch::randn({2, 3}, torch::kInt64);\n```\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor float_tensor = source_tensor.to(torch::kFloat32);\n```\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor gpu_tensor = float_tensor.to(torch::kCUDA);\n```\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor gpu_two_tensor = float_tensor.to(torch::Device(torch::kCUDA, 1));\n```\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor async_cpu_tensor = gpu_tensor.to(torch::kCPU, /*non_blocking=*/true);\n```\n\n----------------------------------------\n\nTITLE: Tuple Type Import Example in TorchScript\nDESCRIPTION: Demonstrates the correct way to use tuple types in TorchScript by importing from typing module. Shows error case and solution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n# ERROR: Tuple not recognized because not imported from typing\n@torch.jit.export\ndef inc(x: Tuple[int, int]):\n    return (x[0]+1, x[1]+1)\n\nm = torch.jit.script(inc)\nprint(m((1,2)))\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with mm in PyTorch (Python)\nDESCRIPTION: Utilizes aten.mm for matrix-matrix product of two tensors. It is one of the core operations that occur within neural networks to process forward and backward propagation of data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\naten.mm.default\ncnt: 1, ((T([16, 2], f16), T([2, 768], f16)), {})\ncnt: 1, ((T([2, 16], f16, stride=(1, 2)), T([16, 768], f16)), {})\ncnt: 1, ((T([16, 768], f16), T([768, 768], f16)), {})\ncnt: 1, ((T([768, 16], f16, stride=(1, 768)), T([16, 768], f16, stride=(393216, 1))), {})\ncnt: 12, ((T([8192, 768], f16), T([768, 3072], f16)), {})\ncnt: 12, ((T([768, 8192], f16, stride=(1, 768)), T([8192, 3072], f16)), {})\ncnt: 12, ((T([8192, 3072], f16), T([3072, 768], f16)), {})\ncnt: 12, ((T([3072, 8192], f16, stride=(1, 3072)), T([8192, 768], f16)), {})\ncnt: 48, ((T([8192, 768], f16), T([768, 768], f16)), {})\ncnt: 48, ((T([768, 8192], f16, stride=(1, 768)), T([8192, 768], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Element-wise Operations in PyTorch\nDESCRIPTION: This snippet shows element-wise operations like division and activation functions (hardsigmoid, hardswish) applied to tensors of various shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 1344, 7, 7], f16, stride=(1344, 1, 0, 0)), 49), {})\ncnt: 1, ((T([128, 1104, 7, 7], f16, stride=(1104, 1, 0, 0)), 49), {})\n\nOperator: aten.hardsigmoid.default\ncnt: 5, ((T([128, 120, 1, 1], f16),), {})\ncnt: 6, ((T([128, 360, 1, 1], f16),), {})\n\nOperator: aten.hardswish_.default\ncnt: 3, ((T([128, 16, 112, 112], f16),), {})\ncnt: 1, ((T([128, 64, 112, 112], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Add Operator Benchmark\nDESCRIPTION: Example code showing how to support torch.add with different input configurations in the benchmark suite. It defines short configurations through cross-product of parameters and creates a benchmark class.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nadd_short_configs = op_bench.cross_product_configs(\n    M=[8, 64, 128],\n    N=range(2, 10, 3),\n    K=[2 ** x for x in range(0, 3)],\n    tags=[\"short\"]\n)\n\nclass AddBenchmark(op_bench.TorchBenchmarkBase):\n    def init(self, M, N, K, device):\n        self.inputs = {\n            \"input_one\": torch.rand(M, N, K, device=device, requires_grad=self.auto_set()),\n            \"input_two\": torch.rand(M, N, K, device=device, requires_grad=self.auto_set())\n        }\n        self.set_module_name(\"add\")\n\n    def forward(self, input_one, input_two):\n        return torch.add(input_one, input_two)\n\nop_bench.generate_pt_test(add_short_configs, AddBenchmark)\n```\n\n----------------------------------------\n\nTITLE: Supported Non-data-dependent Control Flow with vmap\nDESCRIPTION: This example demonstrates that vmap works correctly with control flow that doesn't depend on the tensor values being vmapped over, such as checking tensor dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef custom_dot(x):\n  if x.dim() == 1:\n    return torch.dot(x, x)\n  return (x * x).sum()\n\nx = torch.randn(3)\nvmap(custom_dot)(x)\n```\n\n----------------------------------------\n\nTITLE: Using ATen Softmax Backward Data Operator in PyTorch\nDESCRIPTION: This operator computes the gradient of the softmax operation using the output tensor from a forward pass. Dependencies are PyTorch and ATen extensions. Parameters include input tensor shapes, the specific dimension, and data type specifications. It outputs the gradient tensor, computed based on the forward softmax output. Ensuring correct dimensions and data types is critical for gradient accuracy.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/ElectraForCausalLM_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([1, 4, 512, 512], f16), T([1, 4, 512, 512], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Upper Triangular Matrix with First-Class Dimensions in Python\nDESCRIPTION: This snippet demonstrates how to extract the upper triangular part of a matrix using first-class dimensions in PyTorch. It uses dimensions as loop indices to compute masks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import where\ndef triu(A):\n    i,j = dims()\n    a = A[i, j]\n    return where(i <= j, a, 0).order(i, j)\ntriu(torch.rand(3, 4))\n```\n\n----------------------------------------\n\nTITLE: Configuring Operator Dispatch in PyTorch native_functions.yaml\nDESCRIPTION: Shows the syntax in `native_functions.yaml` for defining the dispatch target for a specific function overload (`func.out_overload(...)`) using the `CompositeImplicitAutograd` backend, mapping it to the `func_out` implementation. This backend allows automatic differentiation if the kernel's constituent operations support it.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\n# did, you could call them here and autograd would be inferred)\nfunc: func.out_overload(...) -> ...\ndispatch:\n    CompositeImplicitAutograd: func_out\n```\n\n----------------------------------------\n\nTITLE: Python Timer Creation for Eager Mode\nDESCRIPTION: Example of creating a Timer instance for the eager forward mode in Python. It uses the benchmark's Python forward statement and setup code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nTimer(\n    stmt=benchmark.py_fwd_stmt,\n    setup=benchmark.setup.py_setup,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up CMake for PyTorch JIT Hooks\nDESCRIPTION: This CMake snippet configures the build environment for a project using PyTorch's JIT hooks. It sets the minimum required version of CMake, checks for ROCM availability, and includes relevant modules for ROCM support. The snippet sets the C++ standard to 17 and links the target with Torch libraries. Dependencies include a minimum CMake version of 3.15 and the Torch library. The main input is the test_jit_hooks.cpp file, and the output is an executable linked with Torch libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/jit_hooks/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Basic CMake setup\ncmake_minimum_required(VERSION 3.15 FATAL_ERROR)\nproject(jit_hooks)\n\nif(USE_ROCM)\ninclude(utils)\ninclude(LoadHIP)\nendif()\nfind_package(Torch REQUIRED)\n\nadd_executable(test_jit_hooks test_jit_hooks.cpp)\nset_property(TARGET test_jit_hooks PROPERTY CXX_STANDARD 17)\ntarget_link_libraries(test_jit_hooks \"${TORCH_LIBRARIES}\")\n```\n\n----------------------------------------\n\nTITLE: PyTorch CUDA Tensor Operations\nDESCRIPTION: Collection of CUDA kernel wrappers and implementations for basic tensor operations like add, multiply, baddbmm, softmax, and tanh.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_26\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at12_GLOBAL__N_117wrapper_CUDA_tanhERKNS_6TensorE\n_ZN2at12_GLOBAL__N_120wrapper_CUDA_baddbmmERKNS_6TensorES3_S3_RKN3c106ScalarES7_\n_ZN2at12_GLOBAL__N_121wrapper_CUDA__softmaxERKNS_6TensorElb\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication for Final Classification in PyTorch MobileNetV3\nDESCRIPTION: The addmm operation used in the final fully connected layer for classification. It takes the flattened features ([128, 1984]) and multiplies them with weights ([1984, 1000]) to produce logits for 1000 classes, adding a bias term ([1000]).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 1984], f16), T([1984, 1000], f16, stride=(1, 1984))), {})\n```\n\n----------------------------------------\n\nTITLE: Referencing checkpoint functions in PyTorch documentation\nDESCRIPTION: These lines indicate the documentation for specific functions and classes in the checkpoint module. They include checkpoint, checkpoint_sequential, set_checkpoint_debug_enabled, CheckpointPolicy, SelectiveCheckpointContext, and create_selective_checkpoint_contexts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/checkpoint.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autofunction:: checkpoint\n.. autofunction:: checkpoint_sequential\n.. autofunction:: set_checkpoint_debug_enabled\n.. autoclass:: CheckpointPolicy\n.. autoclass:: SelectiveCheckpointContext\n.. autofunction:: create_selective_checkpoint_contexts\n```\n\n----------------------------------------\n\nTITLE: Concatenating Tensors with aten.cat.default\nDESCRIPTION: Records tensor concatenation operations performed by the cat.default operator. The operations primarily combine feature maps at different stages of a neural network, with most concatenations occurring along the channel dimension (dim=1).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.cat.default\ncnt: 1, (([T([8, 512, 12, 16], f16), T([8, 512, 12, 16], f16), T([8, 512, 12, 16], f16), T([8, 512, 12, 16], f16)], 1), {})\ncnt: 1, (([T([8, 256, 24, 32], f16), T([8, 512, 24, 32], f16)], 1), {})\ncnt: 1, (([T([8, 128, 48, 64], f16), T([8, 256, 48, 64], f16)], 1), {})\ncnt: 1, (([T([8, 576, 85], f16), T([8, 2304, 85], f16), T([8, 9216, 85], f16)], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Type Checking in TorchScript with torch.jit.isinstance()\nDESCRIPTION: Returns a boolean indicating whether a variable is of the specified type. Used for type refinement in TorchScript.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ntorch.jit.isinstance()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for Test Executable in CMake\nDESCRIPTION: Adds necessary include directories privately to the `test_lite_interpreter_runtime` target. Specifically, it includes the directory specified by the `ATen_CPU_INCLUDE` variable, which is needed to find ATen CPU headers during compilation. Requires the `test_lite_interpreter_runtime` target and `ATen_CPU_INCLUDE` variable to exist.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lite_interpreter_runtime/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(\n  test_lite_interpreter_runtime PRIVATE\n  ${ATen_CPU_INCLUDE}\n)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Statistics\nDESCRIPTION: This snippet provides a comprehensive list of PyTorch operators used in a neural network model, along with their call counts and tensor shapes. It includes operations like convolutions, activations, and tensor manipulations, which are typical for deep learning models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ese_vovnet19b_dw_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 23, ((T([], i64), 1), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16)), {})\n# ... (truncated for brevity)\nOperator: aten.hardsigmoid_backward.default\ncnt: 1, ((T([128, 1024, 1, 1], f16), T([128, 1024, 1, 1], f16)), {})\ncnt: 1, ((T([128, 768, 1, 1], f16), T([128, 768, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Group Normalization Kernel in CUDA\nDESCRIPTION: This CUDA kernel implements the group normalization operation for PyTorch tensors. It uses template metaprogramming and CUDA-specific optimizations for efficient execution on GPUs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_31\n\nLANGUAGE: CUDA\nCODE:\n```\n_ZN2at6native59_GLOBAL__N__d4303601_20_group_norm_kernel_cu_28d559ca_3597327GroupNormKernelImplInternalIN3c104HalfEEEvRKNS_6TensorES7_S7_llllT_RS5_S9_S9_\n```\n\n----------------------------------------\n\nTITLE: Acquiring CUDA Streams in PyTorch C++\nDESCRIPTION: This snippet demonstrates three methods to acquire CUDA streams: from a pool, getting the default stream, and getting the current stream. It includes options for specifying priority and device index.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nCUDAStream getStreamFromPool(const bool isHighPriority = false, DeviceIndex device = -1);\n```\n\nLANGUAGE: cpp\nCODE:\n```\nCUDAStream getDefaultCUDAStream(DeviceIndex device_index = -1);\n```\n\nLANGUAGE: cpp\nCODE:\n```\nCUDAStream getCurrentCUDAStream(DeviceIndex device_index = -1);\n```\n\n----------------------------------------\n\nTITLE: Computing Log Softmax Backward Data in PyTorch\nDESCRIPTION: Calculates the gradient of the log softmax operation, critical for backpropagation in neural networks. This operation requires the input tensor, the output gradient tensor, and the dimension to perform the operation, all in FP16 format. It outputs a gradient tensor necessary for optimizing models via methods like SGD.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\naten._log_softmax_backward_data.default(T([8192, 10000], f16), T([8192, 10000], f16), 1, f16)\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Tensor Addition Operations in PyTorch\nDESCRIPTION: This code snippet describes the use of the ATen add tensor operation, which sums two tensors element-wise. It indicates different scenarios with varying dimensions and shapes, showcasing its frequent invocation in handling tensor data, especially in computation involving neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([16, 128, 768], f16), T([1, 128, 768], f16)), {})\ncnt: 36, ((T([16, 128, 768], f16), T([16, 128, 768], f16)), {})\ncnt: 1, ((T([30522, 768], f16), T([30522, 768], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Refactoring a Function to Return Auxiliary Outputs (Preferred with torch.func) - PyTorch - Python\nDESCRIPTION: This example shows the suggested approach for writing functions to be used with torch.func transforms in Python. Instead of relying on global state, all necessary outputs are returned from the function. The function 'f' returns both the main value and any intermediate results, making it compatible with torch.func (including auxiliary outputs via has_aux=True). Dependencies are PyTorch and torch.func. Inputs and outputs are tensors, and this pattern avoids mutation or use of global state.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z, intermediate\n\ngrad_x, intermediate = grad(f, has_aux=True)(x)\n```\n\n----------------------------------------\n\nTITLE: Creating Batched CSR Sparse Tensor in PyTorch\nDESCRIPTION: Shows creation of a 3D (batched) CSR (Compressed Sparse Row) tensor from a 3D dense tensor, demonstrating batch dimension support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nt = torch.tensor([[[1., 0], [2., 3.]], [[4., 0], [5., 6.]]])\nt.dim()\nt.to_sparse_csr()\n```\n\n----------------------------------------\n\nTITLE: Updating Version Numbers in PyTorch\nDESCRIPTION: Shows how to update the kMaxSupportedFileFormatVersion and kProducedFileFormatVersion in the versions.h file, including comments explaining the version bumps.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nconstexpr uint64_t kMaxSupportedFileFormatVersion = 0x9L;\n\n...\n// We describe new operator version bump reasons here:\n// 1) [01/24/2022]\n//     We bump the version number to 8 to update aten::linspace\n//     and aten::linspace.out to error out when steps is not\n//     provided. (see: https://github.com/pytorch/pytorch/issues/55951)\n// 2) [01/30/2022]\n//     Bump the version number to 9 to update aten::logspace and\n//     and aten::logspace.out to error out when steps is not\n//     provided. (see: https://github.com/pytorch/pytorch/issues/55951)\nconstexpr uint64_t kProducedFileFormatVersion = 0x9L;\n```\n\n----------------------------------------\n\nTITLE: Optimizing ONNX Model in Python\nDESCRIPTION: This Python snippet applies optimizations to an ONNX model graph such as constant folding and removal of redundant operations. The onnx_program.optimize() function modifies the model in-place.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nonnx_program.optimize()\n```\n\n----------------------------------------\n\nTITLE: Analyzing Convolution Operations in PyTorch\nDESCRIPTION: This snippet shows the configuration of various convolution operations used in the model. It includes input and output tensor shapes, kernel sizes, strides, padding, and other parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([128, 36, 56, 56], f16), T([128, 36, 56, 56], f16), T([36, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 36, [True, True, False]), {})\ncnt: 2, ((T([128, 36, 56, 56], f16), T([128, 24, 56, 56], f16), T([36, 24, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 12, 56, 56], f16), T([128, 12, 56, 56], f16), T([12, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 12, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Invalid DataPipe Classes in Python\nDESCRIPTION: Demonstrates invalid DataPipe class definitions with mismatched return type hints for the __iter__ method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass InvalidDP1(IterDataPipe[int]):\n    def __iter__(self) -> str:\n        pass\n```\n\nLANGUAGE: python\nCODE:\n```\nclass InvalidDP2(IterDataPipe[int]):\n    def __iter__(self) -> Iterator[str]:\n        pass\n```\n\n----------------------------------------\n\nTITLE: Example Device Handling with PyTorch HIP Integration\nDESCRIPTION: Demonstrates how to define and use HIP devices in PyTorch by reusing the existing CUDA interfaces. This example shows device assignment and tensor operations targeted for AMD GPUs while maintaining CUDA compatibility. No extra dependencies are required beyond PyTorch itself. This snippet is used to show tensor creation and device context switching in HIP supported environments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/hip.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncuda = torch.device('cuda')     # Default HIP device\ncuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Build and Feature Options with CMake - CMake\nDESCRIPTION: This code defines and manages a variety of build options and conditional settings for the PyTorch library using CMake syntax. It enables selective inclusion of features such as CUDA, ROCm, distributed backends, and various acceleration libraries based on user preference and platform detection. Dependencies and prerequisites are enforced through cmake_dependent_option and conditional blocks, with safeguards and fallbacks provided for platform-specific nuances (such as Windows vs. Linux behavior). Key parameters include feature toggles (e.g., USE_CUDA, USE_DISTRIBUTED), dependency toggles (e.g., USE_SYSTEM_LIBS), and program/library detection, with outputs affecting compilation flags and linked libraries throughout the build process. Prerequisites include a working CMake installation and the corresponding libraries or tools for each enabled option. Limitations include platform-specific constraints enforced via conditionals, with some features available only for certain operating systems or CPU architectures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# USE_ROCM is guarded against in Dependencies.cmake because USE_ROCM is not properly defined here\\ncmake_dependent_option(USE_CUFILE \\\"Use cuFile\\\" ON \\\"USE_CUDA AND NOT WIN32\\\" OFF)\\noption(USE_FBGEMM \\\"Use FBGEMM (quantized 8-bit server operators)\\\" ON)\\noption(USE_KINETO \\\"Use Kineto profiling library\\\" ON)\\noption(USE_CUPTI_SO \\\"Use CUPTI as a shared library\\\" ON)\\noption(USE_FAKELOWP \\\"Use FakeLowp operators\\\" OFF)\\noption(USE_GFLAGS \\\"Use GFLAGS\\\" OFF)\\noption(USE_GLOG \\\"Use GLOG\\\" OFF)\\noption(USE_LITE_PROTO \\\"Use lite protobuf instead of full.\\\" OFF)\\noption(USE_MAGMA \\\"Use MAGMA\\\" ON)\\noption(USE_PYTORCH_METAL \\\"Use Metal for PyTorch iOS build\\\" OFF)\\noption(USE_PYTORCH_METAL_EXPORT \\\"Export Metal models on MacOSX desktop\\\" OFF)\\noption(USE_NATIVE_ARCH \\\"Use -march=native\\\" OFF)\\ncmake_dependent_option(USE_MPS \\\"Use MPS for macOS build\\\" ON \\\"MPS_FOUND\\\" OFF)\\ncmake_dependent_option(USE_NCCL \\\"Use NCCL\\\" ON\\n                       \\\"USE_CUDA OR USE_ROCM;UNIX;NOT APPLE\\\" OFF)\\ncmake_dependent_option(USE_RCCL \\\"Use RCCL\\\" ON USE_NCCL OFF)\\ncmake_dependent_option(USE_STATIC_NCCL \\\"Use static NCCL\\\" OFF \\\"USE_NCCL\\\" OFF)\\ncmake_dependent_option(USE_SYSTEM_NCCL \\\"Use system-wide NCCL\\\" OFF \\\"USE_NCCL\\\"\\n                       OFF)\\noption(USE_NNAPI \\\"Use NNAPI\\\" OFF)\\noption(USE_NNPACK \\\"Use NNPACK\\\" ON)\\ncmake_dependent_option(USE_NUMA \\\"Use NUMA. Only available on Linux.\\\" ON \\\"LINUX\\\"\\n                       OFF)\\ncmake_dependent_option(USE_NVRTC \\\"Use NVRTC. Only available if USE_CUDA is on.\\\"\\n                       OFF \\\"USE_CUDA\\\" OFF)\\noption(USE_NUMPY \\\"Use NumPy\\\" ON)\\noption(USE_OBSERVERS \\\"Use observers module.\\\" OFF)\\noption(USE_OPENCL \\\"Use OpenCL\\\" OFF)\\noption(USE_OPENMP \\\"Use OpenMP for parallel code\\\" ON)\\noption(USE_PRECOMPILED_HEADERS \\\"Use pre-compiled headers to accelerate build.\\\"\\n       OFF)\\n\\noption(USE_PROF \\\"Use profiling\\\" OFF)\\noption(USE_PYTORCH_QNNPACK \\\"Use ATen/QNNPACK (quantized 8-bit operators)\\\" ON)\\noption(USE_SNPE \\\"Use Qualcomm's SNPE library\\\" OFF)\\noption(USE_SYSTEM_EIGEN_INSTALL\\n    \\\"Use system Eigen instead of the one under third_party\\\" OFF)\\ncmake_dependent_option(\\n    USE_VALGRIND \\\"Use Valgrind. Only available on Linux.\\\" ON\\n    \\\"LINUX\\\" OFF)\\n\\nif(NOT DEFINED USE_VULKAN)\\n  cmake_dependent_option(USE_VULKAN \\\"Use Vulkan GPU backend\\\" ON \\\"ANDROID\\\" OFF)\\nendif()\\n\\noption(USE_SOURCE_DEBUG_ON_MOBILE \\\"Enable\\\" ON)\\noption(USE_LITE_INTERPRETER_PROFILER \\\"Enable\\\" ON)\\ncmake_dependent_option(\\n  USE_LITE_AOTI \\\"Include AOTI sources\\\" OFF\\n  \\\"BUILD_LITE_INTERPRETER\\\" OFF)\\noption(USE_VULKAN_FP16_INFERENCE \\\"Vulkan - Use fp16 inference\\\" OFF)\\noption(USE_VULKAN_RELAXED_PRECISION\\n       \\\"Vulkan - Use relaxed precision math in the kernels (mediump)\\\" OFF)\\n# option USE_XNNPACK: try to enable xnnpack by default.\\noption(USE_XNNPACK \\\"Use XNNPACK\\\" ON)\\noption(USE_ROCM_KERNEL_ASSERT \\\"Use Kernel Assert for ROCm\\\" OFF)\\n# Ensure that an ITT build is the default for x86 CPUs\\ncmake_dependent_option(USE_ITT \\\"Use Intel(R) VTune Profiler ITT functionality\\\"\\n                       ON \\\"CPU_INTEL\\\" OFF)\\n# Ensure that an MKLDNN build is the default for x86 CPUs but optional for\\n# AArch64 (dependent on -DUSE_MKLDNN).\\ncmake_dependent_option(\\n  USE_MKLDNN \\\"Use MKLDNN. Only available on x86, x86_64, AArch64, and ppc64le.\\\"\\n  \\\"${CPU_INTEL}\\\" \\\"CPU_INTEL OR CPU_AARCH64 OR CPU_POWER\\\" OFF)\\ncmake_dependent_option(\\n  USE_MKLDNN_ACL \\\"Use Compute Library for the Arm architecture.\\\" OFF\\n  \\\"USE_MKLDNN AND CPU_AARCH64\\\" OFF)\\nset(MKLDNN_ENABLE_CONCURRENT_EXEC ${USE_MKLDNN})\\ncmake_dependent_option(USE_MKLDNN_CBLAS \\\"Use CBLAS in MKLDNN\\\" OFF \\\"USE_MKLDNN\\\"\\n                       OFF)\\noption(USE_STATIC_MKL \\\"Prefer to link with MKL statically (Unix only)\\\" OFF)\\noption(USE_DISTRIBUTED \\\"Use distributed\\\" ON)\\ncmake_dependent_option(\\n  USE_MPI \\\"Use MPI for Caffe2. Only available if USE_DISTRIBUTED is on.\\\" ON\\n  \\\"USE_DISTRIBUTED\\\" OFF)\\ncmake_dependent_option(\\n  USE_UCC \\\"Use UCC. Only available if USE_DISTRIBUTED is on.\\\" OFF\\n  \\\"USE_DISTRIBUTED\\\" OFF)\\ncmake_dependent_option(USE_SYSTEM_UCC \\\"Use system-wide UCC\\\" OFF \\\"USE_UCC\\\" OFF)\\ncmake_dependent_option(USE_C10D_UCC \\\"USE C10D UCC\\\" ON \\\"USE_DISTRIBUTED;USE_UCC\\\"\\n                       OFF)\\ncmake_dependent_option(\\n    USE_GLOO \\\"Use Gloo. Only available if USE_DISTRIBUTED is on.\\\" ON\\n    \\\"USE_DISTRIBUTED\\\" OFF)\\ncmake_dependent_option(\\n  USE_GLOO_WITH_OPENSSL \\\"Use Gloo with OpenSSL. Only available if USE_GLOO is on.\\\" OFF\\n    \\\"USE_GLOO AND LINUX AND NOT INTERN_BUILD_MOBILE\\\" OFF)\\ncmake_dependent_option(\\n    USE_C10D_GLOO \\\"USE C10D GLOO\\\" ON \\\"USE_DISTRIBUTED;USE_GLOO\\\" OFF)\\ncmake_dependent_option(\\n    USE_C10D_NCCL \\\"USE C10D NCCL\\\" ON \\\"USE_DISTRIBUTED;USE_NCCL\\\" OFF)\\ncmake_dependent_option(\\n    USE_C10D_MPI \\\"USE C10D MPI\\\" ON \\\"USE_DISTRIBUTED;USE_MPI\\\" OFF)\\ncmake_dependent_option(\\n    USE_TENSORPIPE \\\"Use TensorPipe. Only available if USE_DISTRIBUTED is on.\\\" ON\\n    \\\"USE_DISTRIBUTED AND NOT WIN32\\\" OFF)\\noption(ONNX_ML \\\"Enable traditional ONNX ML API.\\\" ON)\\noption(HAVE_SOVERSION \\\"Whether to add SOVERSION to the shared objects\\\" OFF)\\noption(BUILD_LIBTORCH_CPU_WITH_DEBUG\\n       \\\"Enable RelWithDebInfo for libtorch_cpu target only\\\" OFF)\\ncmake_dependent_option(\\n  USE_CCACHE \\\"Attempt using CCache to wrap the compilation\\\" ON \\\"UNIX\\\" OFF)\\noption(WERROR \\\"Build with -Werror supported by the compiler\\\" OFF)\\noption(\\n  DEBUG_CUDA\\n  \\\"When compiling DEBUG, also attempt to compile CUDA with debug flags (may cause nvcc to OOM)\\\"\\n  OFF)\\noption(USE_COREML_DELEGATE \\\"Use the CoreML backend through delegate APIs\\\" OFF)\\noption(USE_PER_OPERATOR_HEADERS\\n       \\\"Whether ATen should generate separate headers for each operator\\\" ON)\\ncmake_dependent_option(\\n  BUILD_LAZY_TS_BACKEND\\n  \\\"Build the lazy Torchscript backend, not compatible with mobile builds\\\" ON\\n  \\\"NOT INTERN_BUILD_MOBILE\\\" OFF)\\ncmake_dependent_option(BUILD_FUNCTORCH \\\"Build Functorch\\\" ON \\\"BUILD_PYTHON\\\" OFF)\\ncmake_dependent_option(BUILD_BUNDLE_PTXAS \\\"Bundle PTX into torch/bin fodler\\\"\\n                       OFF \\\"USE_CUDA\\\" OFF)\\ncmake_dependent_option(USE_KLEIDIAI \\\"Use KleidiAI for the ARM CPU & AARCH64 architecture.\\\" ON\\n                        \\\"CPU_AARCH64\\\" OFF)\\n\\noption(USE_MIMALLOC \\\"Use mimalloc\\\" OFF)\\n# Enable third party mimalloc library to improve memory allocation performance\\n# on Windows.\\noption(USE_MIMALLOC_ON_MKL \\\"Use mimalloc on MKL\\\" OFF)\\nif(WIN32)\\n  set(USE_MIMALLOC ON)\\n\\n  # Not enable USE_MIMALLOC_ON_MKL due to it caused issue:\\n  # https://github.com/pytorch/pytorch/issues/138994\\n  # Will turn on when we can fix USE_STATIC_MKL lost functionality:\\n  # https://github.com/pytorch/pytorch/pull/138996\\n  # set(USE_MIMALLOC_ON_MKL ON)\\nendif()\\n\\nif(USE_CCACHE)\\n  find_program(CCACHE_PROGRAM ccache)\\n  if(CCACHE_PROGRAM)\\n    set(CMAKE_C_COMPILER_LAUNCHER\\n        \\\"${CCACHE_PROGRAM}\\\"\\n        CACHE STRING \\\"C compiler launcher\\\")\\n    set(CMAKE_CXX_COMPILER_LAUNCHER\\n        \\\"${CCACHE_PROGRAM}\\\"\\n        CACHE STRING \\\"CXX compiler launcher\\\")\\n    set(CMAKE_CUDA_COMPILER_LAUNCHER\\n        \\\"${CCACHE_PROGRAM}\\\"\\n        CACHE STRING \\\"CUDA compiler launcher\\\")\\n  else()\\n    message(\\n      STATUS\\n        \\\"Could not find ccache. Consider installing ccache to speed up compilation.\\\"\\n    )\\n  endif()\\nendif()\\n\\n# Since TensorPipe does not support Windows, set it to OFF when WIN32 detected\\n# On Windows platform, if user does not install libuv in build conda env and\\n# does not set libuv_ROOT environment variable. Set USE_DISTRIBUTED to OFF.\\nif(WIN32)\\n  set(USE_TENSORPIPE OFF)\\n  message(WARNING \\\"TensorPipe cannot be used on Windows. Set it to OFF\\\")\\n  set(USE_KLEIDIAI OFF)\\n  message(WARNING \\\"KleidiAI cannot be used on Windows. Set it to OFF\\\")\\n\\n  if(USE_DISTRIBUTED AND NOT DEFINED ENV{libuv_ROOT})\\n    find_library(\\n      libuv_tmp_LIBRARY\\n      NAMES uv libuv\\n      HINTS $ENV{CONDA_PREFIX}\\\\Library $ENV{PREFIX}\\\\Library\\n      PATH_SUFFIXES lib\\n      NO_DEFAULT_PATH)\\n    if(NOT libuv_tmp_LIBRARY)\\n      set(USE_DISTRIBUTED OFF)\\n      set(USE_GLOO OFF)\\n      message(\\n        WARNING\\n          \\\"Libuv is not installed in current conda env. Set USE_DISTRIBUTED to OFF. \\\"\\n          \\\"Please run command 'conda install -c conda-forge libuv=1.39' to install libuv.\\\"\\n      )\\n    else()\\n      set(ENV{libuv_ROOT} ${libuv_tmp_LIBRARY}/../../)\\n    endif()\\n  endif()\\nendif()\\n\\nif(USE_GLOO_WITH_OPENSSL)\\n  set(USE_TCP_OPENSSL_LOAD\\n      ON\\n      CACHE STRING \\\"\\\")\\nendif()\\n\\n# Linux distributions do not want too many embedded sources, in that sense we\\n# need to be able to build pytorch with an (almost) empty third_party directory.\\n# USE_SYSTEM_LIBS is a shortcut variable to toggle all the # USE_SYSTEM_*\\n# variables on. Individual USE_SYSTEM_* variables can be toggled with\\n# USE_SYSTEM_LIBS being \\\"OFF\\\".\\noption(USE_SYSTEM_LIBS \\\"Use all available system-provided libraries.\\\" OFF)\\noption(USE_SYSTEM_CPUINFO \\\"Use system-provided cpuinfo.\\\" OFF)\\noption(USE_SYSTEM_SLEEF \\\"Use system-provided sleef.\\\" OFF)\\noption(USE_SYSTEM_GLOO \\\"Use system-provided gloo.\\\" OFF)\\noption(USE_SYSTEM_FP16 \\\"Use system-provided fp16.\\\" OFF)\\noption(USE_SYSTEM_PYBIND11 \\\"Use system-provided PyBind11.\\\" OFF)\\noption(USE_SYSTEM_PTHREADPOOL \\\"Use system-provided pthreadpool.\\\" OFF)\\noption(USE_SYSTEM_PSIMD \\\"Use system-provided psimd.\\\" OFF)\\noption(USE_SYSTEM_FXDIV \\\"Use system-provided fxdiv.\\\" OFF)\\noption(USE_SYSTEM_BENCHMARK \\\"Use system-provided google benchmark.\\\" OFF)\\noption(USE_SYSTEM_ONNX \\\"Use system-provided onnx.\\\" OFF)\\noption(USE_SYSTEM_XNNPACK \\\"Use system-provided xnnpack.\\\" OFF)\\noption(USE_SYSTEM_NVTX \\\"Use system-provided nvtx.\\\" OFF)\\noption(USE_GOLD_LINKER \\\"Use ld.gold to link\\\" OFF)\\nif(USE_SYSTEM_LIBS)\\n  set(USE_SYSTEM_CPUINFO ON)\\n  set(USE_SYSTEM_SLEEF ON)\\n  set(USE_SYSTEM_GLOO ON)\\n  set(BUILD_CUSTOM_PROTOBUF OFF)\\n  set(USE_SYSTEM_EIGEN_INSTALL ON)\\n  set(USE_SYSTEM_FP16 ON)\\n  set(USE_SYSTEM_PTHREADPOOL ON)\\n  set(USE_SYSTEM_PSIMD ON)\\n  set(USE_SYSTEM_FXDIV ON)\\n  set(USE_SYSTEM_BENCHMARK ON)\\n  set(USE_SYSTEM_ONNX ON)\\n  set(USE_SYSTEM_XNNPACK ON)\\n  set(USE_SYSTEM_PYBIND11 ON)\\n  if(USE_NCCL)\\n    set(USE_SYSTEM_NCCL ON)\\n  endif()\\n  set(USE_SYSTEM_NVTX ON)\\nendif()\\n\\n# /Z7 override option When generating debug symbols, CMake default to use the\\n# flag /Zi. However, it is not compatible with sccache. So we rewrite it off.\\n\n```\n\n----------------------------------------\n\nTITLE: Lowered PyTorch Native Quantized Model Example\nDESCRIPTION: Example of a quantized model after being lowered to PyTorch's native backend. Shows the GraphModule representation with a QuantizedLinearReLU module and its forward method implementation demonstrating the quantization flow.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nGraphModule(\n  (linear): QuantizedLinearReLU(in_features=5, out_features=10, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n)\n\ndef forward(self, x):\n    linear_input_scale_0 = self.linear_input_scale_0\n    linear_input_zero_point_0 = self.linear_input_zero_point_0\n    quantize_per_tensor = torch.quantize_per_tensor(x, linear_input_scale_0, linear_input_zero_point_0, torch.quint8);  x = linear_input_scale_0 = linear_input_zero_point_0 = None\n    linear = self.linear(quantize_per_tensor);  quantize_per_tensor = None\n    dequantize_1 = linear.dequantize();  linear = None\n    return dequantize_1\n```\n\n----------------------------------------\n\nTITLE: Control Flow Debugging Backend Example\nDESCRIPTION: Demonstrates a debugging backend handling code with control flow structures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Windows Components\nDESCRIPTION: This batch script downloads and extracts MKL and MAGMA components necessary for building PyTorch on Windows. Dependencies include 7z and curl utilities. Key variables such as CUDA_PREFIX and CONFIG need to be set before execution. The script adjusts environment variables crucial for CMake.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#2025-04-22_snippet_0\n\nLANGUAGE: bat\nCODE:\n```\nREM Make sure you have 7z and curl installed.\n\nREM Download MKL files\ncurl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O\n7z x -aoa mkl_2020.2.254.7z -omkl\n\nREM Download MAGMA files\nREM version available:\nREM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release)\nREM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release)\nREM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\nREM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\nset CUDA_PREFIX=cuda102\nset CONFIG=release\ncurl -k https://s3.amazonaws.com/ossci-windows/magma_2.5.4_%CUDA_PREFIX%_%CONFIG%.7z -o magma.7z\n7z x -aoa magma.7z -omagma\n\nREM Setting essential environment variables\nset \"CMAKE_INCLUDE_PATH=%cd%\\mkl\\include\"\nset \"LIB=%cd%\\mkl\\lib;%LIB%\"\nset \"MAGMA_HOME=%cd%\\magma\"\n```\n\n----------------------------------------\n\nTITLE: Constructing and Recording a Rendezvous Event in PyTorch\nDESCRIPTION: This function constructs and records a rendezvous event in the PyTorch distributed elastic system. It's specifically designed for handling rendezvous-related events.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.elastic.events.construct_and_record_rdzv_event\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of batch normalization operations with different tensor shapes and data types. It includes input tensors, running mean and variance, and normalization parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/shufflenet_v2_x1_0_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 24, 28, 28], f16), T([128, 24, 28, 28], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 24, 112, 112], f16), T([128, 24, 112, 112], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Over Dimensions in PyTorch\nDESCRIPTION: The mean operator calculates the arithmetic mean of tensor elements across specified dimensions, here indicated by lists [2, 3]. This operation is similar to global averaging and is often used in feature map reduction.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([32, 256, 56, 56], f16), [2, 3], True), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 8, ((T([32, 512, 28, 28], f16), [2, 3], True), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 36, ((T([32, 1024, 14, 14], f16), [2, 3], True), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([32, 2048, 7, 7], f16), [2, 3], True), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([32, 2048, 7, 7], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in PyTorch\nDESCRIPTION: Profiling data for batch normalization operations showing count, tensor shapes, and parameters. These operations normalize activations across batches with learnable parameters, using a momentum of 0.1 and epsilon of 1e-05.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})\ncnt: 2, ((T([32, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([32, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 1e-05), {})\ncnt: 2, ((T([32, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([32, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([32, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f16), False, 0.1, 1e-05), {})\ncnt: 6, ((T([32, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([32, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([32, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), False, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Addition Operations in PyTorch\nDESCRIPTION: Addition operations between tensors with various shapes and strides, using half-precision (f16) format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([1024, 249, 192], f16), T([1024, 249, 192], f16, stride=(47808, 1, 249))), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.threshold_backward.default\nDESCRIPTION: Logs the backward pass for thresholding operations (`aten.threshold_backward.default`), commonly used in ReLU backward. Takes the output gradient and the input tensor from the forward pass (both typically float16) and a threshold value (0 in these examples). Applied to various 4D tensor shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 20, ((T([128, 368, 7, 7], f16), T([128, 368, 7, 7], f16), 0), {})\ncnt: 6, ((T([128, 92, 1, 1], f16), T([128, 92, 1, 1], f16), 0), {})\ncnt: 4, ((T([128, 38, 1, 1], f16), T([128, 38, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 368, 14, 14], f16), T([128, 368, 14, 14], f16), 0), {})\ncnt: 11, ((T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 14, 1, 1], f16), T([128, 14, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 152, 28, 28], f16), T([128, 152, 28, 28], f16), 0), {})\ncnt: 2, ((T([128, 56, 28, 28], f16), T([128, 56, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 6, 1, 1], f16), T([128, 6, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 56, 56, 56], f16), T([128, 56, 56, 56], f16), 0), {})\ncnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 8, 1, 1], f16), T([128, 8, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 24, 112, 112], f16), T([128, 24, 112, 112], f16), 0), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Deep Learning Model\nDESCRIPTION: This code snippet provides a comprehensive overview of PyTorch operators used in a deep learning model. It includes operator names, call counts, input tensor shapes, and parameters for each operation. The analysis covers various operations like softmax, convolutions, pooling, and matrix multiplications, which are crucial for understanding the model's architecture and computation flow.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 2, ((T([512, 256, 256], f16), -1, False), {})\ncnt: 1, ((T([512, 64, 64], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 1, ((T([512, 64, 64], f16), T([512, 64, 64], f16), -1, f16), {})\ncnt: 2, ((T([512, 256, 256], f16), T([512, 256, 256], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 4, ((T([128, 64, 16, 16], f16), [512, 16, 256]), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), [512, 64, 256]), {})\ncnt: 2, ((T([512, 256, 256], f16), [512, 256, 256]), {})\ncnt: 4, ((T([512, 16, 16, 16], f16), [131072, 16]), {})\ncnt: 4, ((T([131072, 31], f16), [512, 16, 16, 31]), {})\ncnt: 2, ((T([512, 16, 16, 16, 16], f16), [512, 256, 256]), {})\ncnt: 1, ((T([512, 256, 64], f16), [512, 256, 64]), {})\ncnt: 2, ((T([512, 64, 256], f16), [128, 256, 16, 16]), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), [512, 128, 256]), {})\ncnt: 1, ((T([512, 256, 128], f16), [512, 256, 128]), {})\ncnt: 2, ((T([512, 128, 256], f16), [128, 512, 16, 16]), {})\ncnt: 2, ((T([128, 64, 8, 8], f16), [512, 16, 64]), {})\ncnt: 1, ((T([128, 512, 8, 8], f16), [512, 128, 64]), {})\ncnt: 1, ((T([512, 64, 64], f16), [512, 64, 64]), {})\ncnt: 2, ((T([512, 8, 8, 16], f16), [32768, 16]), {})\ncnt: 2, ((T([32768, 15], f16), [512, 8, 8, 15]), {})\ncnt: 1, ((T([512, 8, 8, 8, 8], f16), [512, 64, 64]), {})\ncnt: 1, ((T([512, 64, 128], f16), [512, 64, 128]), {})\ncnt: 2, ((T([512, 128, 64], f16), [128, 512, 8, 8]), {})\ncnt: 1, ((T([512, 8, 8, 16], f16), [512, 64, 16]), {})\ncnt: 1, ((T([512, 16, 64], f16), [128, 64, 8, 8]), {})\ncnt: 2, ((T([512, 16, 16, 16], f16), [512, 256, 16]), {})\ncnt: 2, ((T([512, 16, 256], f16), [128, 64, 16, 16]), {})\nOperator: aten.add.Tensor\ncnt: 31, ((T([], i64), 1), {})\ncnt: 4, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16)), {})\ncnt: 4, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16)), {})\ncnt: 4, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16)), {})\ncnt: 2, ((T([512, 16, 16, 16, 16], f16, stride=(8432, 31, 527, 1, 0)), T([512, 16, 16, 16, 16], f16, stride=(8432, 527, 31, 0, 1))), {})\ncnt: 2, ((T([512, 256, 256], f16), T([512, 256, 256], f16)), {})\ncnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16)), {})\ncnt: 1, ((T([512, 8, 8, 8, 8], f16, stride=(1080, 15, 135, 1, 0)), T([512, 8, 8, 8, 8], f16, stride=(1080, 135, 15, 0, 1))), {})\ncnt: 1, ((T([512, 64, 64], f16), T([512, 64, 64], f16)), {})\ncnt: 1, ((T([512, 8, 8, 16], f16, stride=(1024, 16, 128, 1)), T([512, 8, 8, 16], f16)), {})\ncnt: 1, ((T([512, 64, 16], f16), T([512, 64, 16], f16)), {})\ncnt: 2, ((T([512, 16, 16, 16], f16, stride=(4096, 16, 256, 1)), T([512, 16, 16, 16], f16)), {})\ncnt: 2, ((T([512, 256, 16], f16), T([512, 256, 16], f16)), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16)), {})\ncnt: 2, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16)), {})\ncnt: 3, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([128, 512, 16, 16], f16), [2, 2], [2, 2]), {})\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([128, 512, 8, 8], f16), T([128, 512, 16, 16], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})\nOperator: aten.bmm.default\ncnt: 2, ((T([512, 256, 16], f16, stride=(4096, 1, 256)), T([512, 16, 256], f16)), {})\ncnt: 1, ((T([512, 256, 256], f16), T([512, 256, 64], f16, stride=(16384, 1, 256))), {})\ncnt: 1, ((T([512, 256, 256], f16), T([512, 256, 128], f16, stride=(32768, 1, 256))), {})\ncnt: 1, ((T([512, 64, 16], f16, stride=(1024, 1, 64)), T([512, 16, 64], f16)), {})\ncnt: 1, ((T([512, 64, 64], f16), T([512, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 1, ((T([512, 64, 64], f16, stride=(4096, 1, 64)), T([512, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 1, ((T([512, 64, 128], f16, stride=(8192, 1, 64)), T([512, 128, 64], f16)), {})\ncnt: 1, ((T([512, 16, 64], f16), T([512, 64, 64], f16)), {})\ncnt: 1, ((T([512, 64, 64], f16), T([512, 64, 16], f16, stride=(1024, 1, 64))), {})\ncnt: 1, ((T([512, 256, 256], f16, stride=(65536, 1, 256)), T([512, 256, 128], f16, stride=(32768, 1, 256))), {})\ncnt: 1, ((T([512, 256, 128], f16, stride=(32768, 1, 256)), T([512, 128, 256], f16)), {})\ncnt: 2, ((T([512, 16, 256], f16), T([512, 256, 256], f16)), {})\ncnt: 2, ((T([512, 256, 256], f16), T([512, 256, 16], f16, stride=(4096, 1, 256))), {})\ncnt: 1, ((T([512, 256, 256], f16, stride=(65536, 1, 256)), T([512, 256, 64], f16, stride=(16384, 1, 256))), {})\ncnt: 1, ((T([512, 256, 64], f16, stride=(16384, 1, 256)), T([512, 64, 256], f16)), {})\nOperator: aten.cat.default\ncnt: 1, (([T([128, 64, 8, 8], f16), T([128, 64, 8, 8], f16), T([128, 512, 8, 8], f16)], 1), {})\ncnt: 1, (([T([128, 64, 16, 16], f16), T([128, 64, 16, 16], f16), T([128, 512, 16, 16], f16)], 1), {})\ncnt: 1, (([T([128, 64, 16, 16], f16), T([128, 64, 16, 16], f16), T([128, 256, 16, 16], f16)], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 256, 256], f16),), {})\ncnt: 1, ((T([128, 24, 128, 128], f16),), {})\ncnt: 1, ((T([128, 32, 128, 128], f16),), {})\ncnt: 1, ((T([128, 64, 128, 128], f16),), {})\ncnt: 4, ((T([128, 64, 64, 64], f16),), {})\ncnt: 2, ((T([128, 256, 64, 64], f16),), {})\ncnt: 1, ((T([128, 128, 64, 64], f16),), {})\ncnt: 3, ((T([128, 128, 32, 32], f16),), {})\ncnt: 2, ((T([128, 512, 32, 32], f16),), {})\ncnt: 1, ((T([128, 256, 32, 32], f16),), {})\ncnt: 3, ((T([128, 256, 16, 16], f16),), {})\ncnt: 2, ((T([128, 1024, 16, 16], f16),), {})\ncnt: 1, ((T([128, 512, 16, 16], f16),), {})\ncnt: 3, ((T([128, 512, 8, 8], f16),), {})\ncnt: 2, ((T([128, 2048, 8, 8], f16),), {})\nOperator: aten.constant_pad_nd.default\ncnt: 4, ((T([8192, 16, 31], f16), [0, 1], 0.0), {})\ncnt: 4, ((T([8192, 512], f16), [0, 15], 0.0), {})\ncnt: 2, ((T([4096, 8, 15], f16), [0, 1], 0.0), {})\ncnt: 2, ((T([4096, 128], f16), [0, 7], 0.0), {})\ncnt: 2, ((T([4096, 135], f16), [0, -7]), {})\ncnt: 2, ((T([4096, 8, 16], f16), [0, -1]), {})\ncnt: 4, ((T([8192, 527], f16), [0, -15]), {})\ncnt: 4, ((T([8192, 16, 32], f16), [0, -1]), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 256, 256], f16), T([24, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 24, 128, 128], f16), T([32, 24, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 64, 64], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 64, 64, 64], f16), T([64, 16, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 4), {})\ncnt: 2, ((T([128, 1, 64], f16), T([1, 1, 3], f16), None, [1], [1], [1], False, [0], 1), {})\ncnt: 3, ((T([128, 64, 64, 64], f16), T([256, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 64, 64], f16), T([64, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 64, 64], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 16, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 8), {})\ncnt: 2, ((T([128, 1, 128], f16), T([1, 1, 5], f16), None, [1], [2], [1], False, [0], 1), {})\ncnt: 2, ((T([128, 128, 32, 32], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 64, 64], f16), T([512, 256, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 32, 32], f16), T([128, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 32, 32], f16), T([128, 16, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 8), {})\ncnt: 1, ((T([128, 512, 32, 32], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([256, 16, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 16), {})\ncnt: 1, ((T([128, 1, 256], f16), T([1, 1, 5], f16), None, [1], [2], [1], False, [0], 1), {})\ncnt: 2, ((T([128, 256, 16, 16], f16), T([1024, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 32, 32], f16), T([1024, 512, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1024, 16, 16], f16), T([256, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), T([384, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1024, 16, 16], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([640, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 512, 8, 8], f16), T([2048, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1024, 16, 16], f16), T([2048, 1024, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([512, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 8, 8], f16), T([640, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 2, ((T([128, 2048, 8, 8], f16), T([128, 512, 8, 8], f16), T([2048, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 640, 8, 8], f16), T([128, 512, 8, 8], f16), T([640, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Basic Set Operations and Declarations in Python\nDESCRIPTION: This snippet shows various ways of creating and using sets in Python, including empty sets, set variables, and set functions. It also demonstrates how to use comments to ignore specific linter warnings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/set_linter_testdata/python_code.py.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nignored = set()  # noqa: set_linter\na = set()\nb = \"set()\"\nc = set\nd = c.set\nf = (\n   set(\n   )\n)\nignored = (\n   set(  # noqa: set_linter\n   )\n)\n```\n\n----------------------------------------\n\nTITLE: Customizing Torchvision Model Normalization with GroupNorm - PyTorch (Python)\nDESCRIPTION: Demonstrates how to initialize a torchvision model (e.g., ResNet18) with a custom normalization layer using GroupNorm. The 'norm_layer' parameter is set to a lambda function returning GroupNorm, complying with the requirement that the channel count 'c' is divisible by 'g.' This approach requires torchvision and functools, and is useful for swapping BatchNorm2d for GroupNorm in pretrained models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision\nfrom functools import partial\ntorchvision.models.resnet18(norm_layer=lambda c: GroupNorm(num_groups=g, c))\n```\n\n----------------------------------------\n\nTITLE: Clamping Values in PyTorch\nDESCRIPTION: The aten.clamp.default example demonstrates clamping of tensor values for ranges defined by [0, 512] on integers of shape [1]. Clamping is essential to prevent the introduction of out-of-bound values which could hinder model stability.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clamp.default\ncnt: 2, ((T([1], i64), 0, 512), {})\n```\n\n----------------------------------------\n\nTITLE: Defining CUDA Function in c10/cuda\nDESCRIPTION: Example of defining a CUDA function in c10/cuda namespace, which will be transpiled to c10/hip for AMD GPU builds. This demonstrates the namespace and function declaration pattern.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/cuda/README.md#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n// c10/cuda/CUDAFoo.h\nnamespace c10 { namespace cuda {\n\nvoid my_func();\n\n}}\n```\n\n----------------------------------------\n\nTITLE: Softmax Operations in PyTorch\nDESCRIPTION: Implementation of forward and backward softmax operations with 16-bit floating point tensors of shape [8, 12, 512, 512].\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_DistilBert_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._softmax.default(T([8, 12, 512, 512], f16), -1, False)\naten._softmax_backward_data.default(T([8, 12, 512, 512], f16), T([8, 12, 512, 512], f16), -1, f16)\n```\n\n----------------------------------------\n\nTITLE: Building libtorch Using setup.py\nDESCRIPTION: This snippet shows an alternative method to build libtorch using setup.py. It builds the C++ libraries and indicates where to find the output, including libtorch.so.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/libtorch.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd <pytorch_root>\npython setup.py build\n\nls torch/lib/tmp_install # output is produced here\nls torch/lib/tmp_install/lib/libtorch.so # of particular interest\n```\n\n----------------------------------------\n\nTITLE: Example Function for Suffix Truncation\nDESCRIPTION: Sample function demonstrating how suffix truncation works by removing operations from the end of the graph while preserving the failure condition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef f(a):\n    b = x * 2\n    c = b + 3\n    d = c / 4\n    return d\n```\n\nLANGUAGE: python\nCODE:\n```\ndef f(a):\n    b = x * 2\n    c = b + 3\n    return c\n```\n\n----------------------------------------\n\nTITLE: Setting MPS Allocator Low Watermark Ratio (Environment Variable)\nDESCRIPTION: Configures the low watermark ratio for the MPS allocator using `PYTORCH_MPS_LOW_WATERMARK_RATIO`. This is a soft limit used to trigger garbage collection or more frequent command buffer commits (adaptive commit). Default is 1.4 (unified memory) or 1.0 (discrete memory). Set to 0.0 to disable adaptive commit and garbage collection.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nPYTORCH_MPS_LOW_WATERMARK_RATIO\n```\n\n----------------------------------------\n\nTITLE: Sphinx RST API Documentation Structure for PyTorch Compiler\nDESCRIPTION: Sphinx documentation structure defining the torch.compiler module API reference, including autosummary directives for compiler functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_api.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: torch.compiler\n\n.. automodule:: torch.compiler\n\n.. _torch.compiler_api:\n\ntorch.compiler API reference\n============================\n\nFor a quick overview of ``torch.compiler``, see :ref:`torch.compiler_overview`.\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n     compile\n     reset\n     allow_in_graph\n     substitute_in_graph\n     assume_constant_result\n     list_backends\n     disable\n     set_stance\n     cudagraph_mark_step_begin\n     is_compiling\n     is_dynamo_compiling\n     is_exporting\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Support in ATen CMake\nDESCRIPTION: Sets up CUDA-related source files, include directories, and dependencies for ATen. It handles different CUDA components like cuDNN and includes various CUDA source files for different functionalities.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDA)\n  list(APPEND ATen_CUDA_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/cuda)\n  # Next two lines are needed because TunableOp uses third-party/fmt\n  list(APPEND ATen_CUDA_INCLUDE $<TARGET_PROPERTY:fmt::fmt-header-only,INTERFACE_INCLUDE_DIRECTORIES>)\n  list(APPEND ATen_CUDA_DEPENDENCY_LIBS fmt::fmt-header-only)\n  list(APPEND ATen_CUDA_CU_SRCS\n    ${cuda_cu}\n    ${native_cuda_cu}\n    ${native_nested_cuda_cu}\n    ${native_sparse_cuda_cu}\n    ${native_quantized_cuda_cu}\n    ${native_transformers_cuda_cu}\n    ${cuda_generated_sources}\n  )\n  list(APPEND ATen_CUDA_CPP_SRCS\n    ${cuda_cpp}\n    ${native_cuda_cpp}\n    ${native_cudnn_cpp}\n    ${native_miopen_cpp}\n    ${native_nested_cuda_cpp}\n    ${native_quantized_cuda_cpp}\n    ${native_quantized_cudnn_cpp}\n    ${native_sparse_cuda_cpp}\n    ${native_transformers_cuda_cpp}\n  )\n  set(ATen_CUDA_LINALG_SRCS ${native_cuda_linalg_cpp})\n  if(NOT BUILD_LAZY_CUDA_LINALG)\n    list(APPEND ATen_CUDA_CU_SRCS ${native_cuda_linalg_cpp})\n  endif()\n  if(CAFFE2_USE_CUDNN)\n    list(APPEND ATen_CUDA_CPP_SRCS ${cudnn_cpp})\n  endif()\n\n  append_filelist(\"aten_cuda_cu_source_list\" ATen_CUDA_CU_SRCS)\n  append_filelist(\"aten_cuda_with_sort_by_key_source_list\" ATen_CUDA_SRCS_W_SORT_BY_KEY)\n  append_filelist(\"aten_cuda_cu_with_sort_by_key_source_list\" ATen_CUDA_CU_SRCS_W_SORT_BY_KEY)\n\n  exclude(ATen_CUDA_CPP_SRCS \"${ATen_CUDA_CPP_SRCS}\"\n      ${ATen_CUDA_CU_SRCS}\n      ${ATen_CUDA_SRCS_W_SORT_BY_KEY} ${ATen_CUDA_CU_SRCS_W_SORT_BY_KEY})\n  exclude(ATen_CUDA_CU_SRCS \"${ATen_CUDA_CU_SRCS}\"\n      ${ATen_CUDA_SRCS_W_SORT_BY_KEY} ${ATen_CUDA_CU_SRCS_W_SORT_BY_KEY})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking Tracing Status with torch.jit.is_tracing() in TorchScript\nDESCRIPTION: Returns a boolean value indicating whether the current program is traced by torch.jit.trace / torch.jit.trace_module or not.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ntorch.jit.is_tracing()\n```\n\n----------------------------------------\n\nTITLE: Linearized Instructions for LSTM Cell Execution in PyTorch Interpreter\nDESCRIPTION: The linearized instruction sequence that PyTorch's interpreter executes for an LSTM cell. Each instruction defines register movements, operations, and explicit management of value lifetimes to ensure efficient memory usage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\n0, 1, 2, 3, 4, 5, 6 = Load\n7 = Constant\n8 = t move(3)\n9 = mm move(0), move(8)\n10 = t move(4)\n11 = mm move(1), move(10)\n12 = add move(9), move(11), 7\n13 = add move(12), move(5), 7\n14 = add move(13), move(6), 7\n15, 16, 17, 18 = ConstantChunk move(14)\n19 = sigmoid move(15)\n20 = sigmoid move(16)\n21 = tanh move(17)\n22 = sigmoid move(18)\n23 = mul move(20), move(2)\n24 = mul move(19), move(21)\n25 = add move(23), move(24), move(7)\n26 = tanh 25\n27 = mul move(22), move(26)\n28 = TupleConstruct move(27), move(25)\n = Store move(28)\n```\n\n----------------------------------------\n\nTITLE: Setting Up PyTorch Linting Environment\nDESCRIPTION: Installs prerequisites for running PyTorch linting checks locally.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nmake setup-lint\n```\n\n----------------------------------------\n\nTITLE: Defining Kernel Dispatch in C++ Implementation File\nDESCRIPTION: This code snippet demonstrates how to define a kernel dispatch in a separate C++ implementation file using the DEFINE_DISPATCH macro. The dispatch is defined once, outside of the CPU directory, and links to the corresponding header declaration. It is crucial for managing dispatches across different compilation conditions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cpu/README.md#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nDEFINE_DISPATCH(fnNameImpl)\n```\n\n----------------------------------------\n\nTITLE: Defining Tensor and Operator Argument Tuples for Neural Network Layers in PyTorch - Python\nDESCRIPTION: This snippet encodes multiple example operator invocations in PyTorch, describing the input and output tensor shapes, data types (primarily float16 and int64), parameters (e.g., kernel size, stride, padding), and boolean flags for each call. These tuples are used for operator test coverage, shape tracing, or benchmarking. Required dependencies are PyTorch and a framework for interpreting these tuple notations; inputs are tensor configurations and associated parameters, and outputs are none directly unless used to instantiate operators.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncnt: 23, ((T([64, 32, 14, 14], f16), T([64, 128, 14, 14], f16), T([32, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 14, 14], f16), T([64, 960, 14, 14], f16), T([128, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 14, 14], f16), T([64, 928, 14, 14], f16), T([128, 928, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 14, 14], f16), T([64, 896, 14, 14], f16), T([128, 896, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 14, 14], f16), T([64, 864, 14, 14], f16), T([128, 864, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 14, 14], f16), T([64, 832, 14, 14], f16), T([128, 832, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 14, 14], f16), T([64, 800, 14, 14], f16), T([128, 800, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 14, 14], f16), T([64, 768, 14, 14], f16), T([128, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 14, 14], f16), T([64, 736, 14, 14], f16), T([128, 736, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 14, 14], f16), T([64, 704, 14, 14], f16), T([128, 704, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n...\n```\n\n----------------------------------------\n\nTITLE: Tensor Multiplication Operations in PyTorch\nDESCRIPTION: Element-wise multiplication between tensors of matching shapes, using half-precision format. Operations include both spatial and channel-wise multiplications.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\naten.mul.Tensor\ncnt: 10, ((T([128, 120, 28, 28], f16), T([128, 120, 1, 1], f16)), {})\ncnt: 12, ((T([128, 360, 14, 14], f16), T([128, 360, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Forking DataPipes in PyTorch\nDESCRIPTION: Demonstrates the use of the fork() method to create multiple copies of a DataPipe that can be consumed independently.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(2)\ndp1, dp2, dp3 = dp.fork(3)\nfor i in dp1 + dp2 + dp3:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Checking Compilation Status with torch.jit.is_scripting() in TorchScript\nDESCRIPTION: Returns a boolean value indicating whether the current program is compiled by torch.jit.script or not. Used in conditionals to control compilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ntorch.jit.is_scripting()\n```\n\n----------------------------------------\n\nTITLE: Logging Debug Info for Expired Timers in PyTorch Distributed Elastic\nDESCRIPTION: Function to log debug information for expired timers. It is part of the debug info logging functionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ntorch.distributed.elastic.timer.debug_info_logging.log_debug_info_for_expired_timers\n```\n\n----------------------------------------\n\nTITLE: TorchScript Function Definition Example\nDESCRIPTION: Example showing how to define a simple TorchScript function using the @torch.jit.script decorator that performs basic tensor operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\ndef f(a, b):\n  c = a + b\n  d = c * c\n  e = torch.tanh(d * c)\n  return d + (e + e)\n```\n\n----------------------------------------\n\nTITLE: Accessing Real and Imaginary Components\nDESCRIPTION: Demonstrates accessing and modifying real and imaginary parts of complex tensors using real and imag attributes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ny.real\ny.imag\ny.real.mul_(2)\ny\ny.real.stride()\n```\n\n----------------------------------------\n\nTITLE: Defining and Analyzing ATen Operator Call Patterns in PyTorch (Python)\nDESCRIPTION: This snippet represents structured documentation and statistics on calls to PyTorch's ATen operators in Python projects, focusing on input/output tensor shapes, types, and invocation details. It is used for profiling code, optimizing tensor operations, or for backend dispatching and kernel generation. Inputs are described with rich shape/type/stride semantics, outputs and key parameters (e.g., device, dtype, layout) are annotated, and the information assumes knowledge of PyTorch's computational graph and operator schema.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MBartForCausalLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([2048, 50265], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([2048, 50265], f16), T([2048, 50265], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 12, ((T([256, 128, 128], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([256, 128, 128], f16), T([256, 128, 128], f16), -1, f16), {})\nOperator: aten._to_copy.default\ncnt: 1, ((T([128, 128], f32),), {'dtype': f16})\ncnt: 1, ((T([16, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([16, 128, 16, 64], f16), [16, 128, 1024]), {})\ncnt: 1, ((T([2048, 50265], f16), [16, 128, 50265]), {})\ncnt: 12, ((T([16, 16, 128, 64], f16), [256, 128, 64]), {})\ncnt: 12, ((T([16, 128, 1024], f16), [2048, 1024]), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([128], i64), 1), {})\ncnt: 1, ((T([16, 128], i64, stride=(0, 1)), 2), {})\ncnt: 73, ((T([16, 128, 1024], f16), T([16, 128, 1024], f16)), {})\ncnt: 12, ((T([16, 16, 128, 128], f16), T([16, 1, 128, 128], f16)), {})\ncnt: 1, ((T([50265, 1024], f16), T([50265, 1024], f16)), {})\nOperator: aten.addmm.default\ncnt: 48, ((T([1024], f16), T([2048, 1024], f16), T([1024, 1024], f16, stride=(1, 1024))), {})\ncnt: 12, ((T([4096], f16), T([2048, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})\ncnt: 12, ((T([1024], f16), T([2048, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})\nOperator: aten.bmm.default\ncnt: 24, ((T([256, 128, 64], f16), T([256, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 24, ((T([256, 128, 128], f16), T([256, 128, 64], f16)), {})\ncnt: 12, ((T([256, 128, 128], f16, stride=(16384, 1, 128)), T([256, 128, 64], f16)), {})\ncnt: 12, ((T([256, 64, 128], f16, stride=(8192, 1, 64)), T([256, 128, 128], f16)), {})\nOperator: aten.clone.default\ncnt: 2, ((T([16, 128], i64),), {})\nOperator: aten.copy_.default\ncnt: 2, ((T([16, 128], i64), T([16, 128], i64)), {})\nOperator: aten.embedding.default\ncnt: 1, ((T([50265, 1024], f16), T([16, 128], i64), 1), {})\ncnt: 1, ((T([1026, 1024], f16), T([16, 128], i64)), {})\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([16, 128, 1024], f16), T([16, 128], i64), 1026, -1, False), {})\ncnt: 1, ((T([16, 128, 1024], f16), T([16, 128], i64), 50265, 1, False), {})\nOperator: aten.gelu.default\ncnt: 12, ((T([16, 128, 4096], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([16, 128, 4096], f16), T([16, 128, 4096], f16)), {})\nOperator: aten.lt.Tensor\ncnt: 1, ((T([128], i64), T([128, 1], i64)), {})\nOperator: aten.masked_fill_.Scalar\ncnt: 1, ((T([128, 128], f32), T([128, 128], b8), 0), {})\nOperator: aten.mm.default\ncnt: 1, ((T([2048, 1024], f16), T([1024, 50265], f16, stride=(1, 1024))), {})\ncnt: 1, ((T([50265, 2048], f16, stride=(1, 50265)), T([2048, 1024], f16)), {})\ncnt: 1, ((T([2048, 50265], f16), T([50265, 1024], f16)), {})\ncnt: 12, ((T([2048, 1024], f16), T([1024, 4096], f16)), {})\ncnt: 12, ((T([1024, 2048], f16, stride=(1, 1024)), T([2048, 4096], f16)), {})\ncnt: 12, ((T([2048, 4096], f16), T([4096, 1024], f16)), {})\ncnt: 12, ((T([4096, 2048], f16, stride=(1, 4096)), T([2048, 1024], f16)), {})\ncnt: 48, ((T([2048, 1024], f16), T([1024, 1024], f16)), {})\ncnt: 48, ((T([1024, 2048], f16, stride=(1, 1024)), T([2048, 1024], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 2, ((T([16, 128, 1024], f16), 1.0), {})\ncnt: 24, ((T([16, 128, 1024], f16), 0.125), {})\nOperator: aten.native_layer_norm.default\ncnt: 26, ((T([16, 128, 1024], f16), [1024], T([1024], f16), T([1024], f16), 1e-05), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 26, ((T([16, 128, 1024], f16), T([16, 128, 1024], f16), [1024], T([16, 128, 1], f32), T([16, 128, 1], f32), T([1024], f16), T([1024], f16), [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([2048, 50265], f16), T([2048], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([2048, 50265], f16), T([2048], i64), None, 1, -100), {})\nOperator: aten.sum.SymInt\ncnt: 60, ((T([2048, 1024], f16), [0], True), {})\ncnt: 12, ((T([2048, 4096], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Using CUDAGuard to Change Device Index in PyTorch (C++)\nDESCRIPTION: This example demonstrates the use of at::cuda::CUDAGuard to temporarily change the current CUDA device index within a scope. It changes the device index to 1 while maintaining the current CUDA stream.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n{\n  at::cuda::CUDAGuard device_guard(1);\n\n  // current device index is changed to 1 within scope\n  // current CUDA stream is still `streams1[0]` on device 1, no change\n}\n// current device index is reset to 0 after `device_guard` is destroyed\n```\n\n----------------------------------------\n\nTITLE: Setting API Usage Logger in PyTorch C++\nDESCRIPTION: This C++ snippet sets up an API usage logger for PyTorch, allowing developers to track which APIs are invoked. By calling c10::SetAPIUsageHandler, developers can register a logging callback function that outputs API use events to standard error output. This is particularly useful for monitoring and auditing API usage in managed environments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/large_scale_deployments.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nSetAPIUsageLogger([](const std::string& event_name) {\n    std::cerr << \"API was used: \" << event_name << std::endl;\n});\n```\n\n----------------------------------------\n\nTITLE: Performing Matrix Multiplication in PyTorch\nDESCRIPTION: Executes matrix multiplication (mm) operations on half-precision tensors. Used for linear transformations in the neural network, typically between reshaped input/output tensors and weight matrices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\naten.mm.default(T([3200, 768], f16), T([768, 256], f16, stride=(1, 768)))\n```\n\n----------------------------------------\n\nTITLE: Tracking Sigmoid Operations in PyTorch\nDESCRIPTION: Records of sigmoid activation function forward and backward operations with various tensor shapes. Shows input tensor dimensions commonly used in attention mechanisms or feature activations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sigmoid.default\ncnt: 1, ((T([128, 32, 1, 1], f16),), {})\ncnt: 1, ((T([128, 96, 1, 1], f16),), {})\ncnt: 2, ((T([128, 144, 1, 1], f16),), {})\ncnt: 2, ((T([128, 240, 1, 1], f16),), {})\ncnt: 4, ((T([128, 480, 1, 1], f16),), {})\ncnt: 4, ((T([128, 672, 1, 1], f16),), {})\ncnt: 5, ((T([128, 1152, 1, 1], f16),), {})\nOperator: aten.sigmoid_backward.default\ncnt: 5, ((T([128, 1152, 1, 1], f16), T([128, 1152, 1, 1], f16)), {})\ncnt: 4, ((T([128, 672, 1, 1], f16), T([128, 672, 1, 1], f16)), {})\ncnt: 4, ((T([128, 480, 1, 1], f16), T([128, 480, 1, 1], f16)), {})\ncnt: 2, ((T([128, 240, 1, 1], f16), T([128, 240, 1, 1], f16)), {})\ncnt: 2, ((T([128, 144, 1, 1], f16), T([128, 144, 1, 1], f16)), {})\ncnt: 1, ((T([128, 96, 1, 1], f16), T([128, 96, 1, 1], f16)), {})\ncnt: 1, ((T([128, 32, 1, 1], f16), T([128, 32, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: ASAN Environment Setup for PyTorch\nDESCRIPTION: Bash script for setting up Address Sanitizer (ASAN) environment variables and build configuration for PyTorch debugging.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_40\n\nLANGUAGE: bash\nCODE:\n```\nLLVM_ROOT=<wherever your llvm install is>\nPYTORCH_ROOT=<wherever your pytorch checkout is>\n\nLIBASAN_RT=\"$LLVM_ROOT/lib/clang/8.0.0/lib/linux/libclang_rt.asan-x86_64.so\"\nbuild_with_asan()\n{\n  LD_PRELOAD=${LIBASAN_RT} \\\n  CC=\"$LLVM_ROOT/bin/clang\" \\\n  CXX=\"$LLVM_ROOT/bin/clang++\" \\\n  LDSHARED=\"clang --shared\" \\\n  LDFLAGS=\"-stdlib=libstdc++\" \\\n  CFLAGS=\"-fsanitize=address -fno-sanitize-recover=all -shared-libasan -pthread\" \\\n  CXX_FLAGS=\"-pthread\" \\\n  USE_CUDA=0 USE_OPENMP=0 USE_DISTRIBUTED=0 DEBUG=1 \\\n  python setup.py develop\n}\n\nrun_with_asan()\n{\n  LD_PRELOAD=${LIBASAN_RT} $@\n}\n\nexport ASAN_OPTIONS=detect_leaks=0:symbolize=1:strict_init_order=true\nexport UBSAN_OPTIONS=print_stacktrace=1:suppressions=$PYTORCH_ROOT/ubsan.supp\nexport ASAN_SYMBOLIZER_PATH=$LLVM_ROOT/bin/llvm-symbolizer\n```\n\n----------------------------------------\n\nTITLE: Enabling SLEEF Vectorization on macOS Apple Silicon in CMake\nDESCRIPTION: Checks if the build is targeting macOS (CMAKE_SYSTEM_NAME is Darwin) and the processor is Apple Silicon (CMAKE_SYSTEM_PROCESSOR is arm64). If both conditions are true, it appends the `-DAT_BUILD_ARM_VEC256_WITH_SLEEF` preprocessor definition to CMAKE_CXX_FLAGS and also adds it using `add_definitions`. This enables the use of the SLEEF library for vector math functions on this platform.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_37\n\nLANGUAGE: cmake\nCODE:\n```\n# Enable sleef on macOS with Apple silicon by default\nif((${CMAKE_SYSTEM_NAME} STREQUAL \"Darwin\") AND (\"${CMAKE_SYSTEM_PROCESSOR}\" STREQUAL \"arm64\"))\n  message(STATUS \"Running on macOS with Apple silicon\")\n  string(APPEND CMAKE_CXX_FLAGS \" -DAT_BUILD_ARM_VEC256_WITH_SLEEF\")\n  add_definitions(-DAT_BUILD_ARM_VEC256_WITH_SLEEF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up Google Test for QNNPACK Tests in CMake\nDESCRIPTION: Conditionally sets up Google Test as a dependency for QNNPACK unit tests if it's not already available as a target. Forces shared CRT and adds the GoogleTest source directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT TARGET gtest)\n  set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE)\n  add_subdirectory(\n    \"${GOOGLETEST_SOURCE_DIR}\"\n    \"${CONFU_DEPENDENCIES_BINARY_DIR}/googletest\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Custom EMA Implementation using avg_fn\nDESCRIPTION: Example demonstrating how to implement a custom exponential moving average using the avg_fn parameter.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n        0.9 * averaged_model_parameter + 0.1 * model_parameter\nema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)\n```\n\n----------------------------------------\n\nTITLE: Measuring CUDA Kernel Performance with Effective Memory Bandwidth\nDESCRIPTION: Python script to measure the effective memory bandwidth of a CUDA kernel (uniform_ in this example), which is a good performance metric for CUDA optimization. The script calculates bandwidth based on bytes read/written.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.utils.benchmark import Timer\nsize = 128*512\nnrep = 100\nnbytes_read_write = 4 # this is number of bytes read + written by a kernel. Change this to fit your kernel.\n\nfor i in range(10):\n    a=torch.empty(size).cuda().uniform_()\n    torch.cuda.synchronize()\n    out = a.uniform_()\n    torch.cuda.synchronize()\n    t = Timer(stmt=\"a.uniform_()\", globals=globals())\n    res = t.blocked_autorange()\n    timec = res.median\n    print(\"uniform, size, elements\", size, \"forward\", timec, \"bandwidth (GB/s)\", size*(nbytes_read_write)*1e-9/timec)\n    size *=2\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication in PyTorch\nDESCRIPTION: Matrix multiplication operations, likely used in fully connected layers or attention mechanisms. These operations involve reshaping tensors and performing dot products.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([262144, 16], f16), T([16, 23], f16, stride=(1, 16))), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing Custom Observer for PyTorch ModelReport API\nDESCRIPTION: Example of initializing a custom observer with a unique PRE_OBSERVER_NAME for use in a detector. This code demonstrates how to create a fully qualified name (fqn) for each observer that acts as a key in the returned dictionary.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/_model_report/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nPRE_OBSERVER_NAME = \"custom_observer_pre\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Sign Operation in C++\nDESCRIPTION: Example of implementing the sign operation in C++ using unary operation helpers for out, functional, and in-place variants.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nTensor& sign_out(Tensor& result, const Tensor& self) { return unary_op_impl_out(result, self, sign_stub); }\nTensor sign(const Tensor& self) { return unary_op_impl(self, at::sign_out); }\nTensor& sign_(Tensor& self) { return unary_op_impl_(self, at::sign_out); }\n```\n\n----------------------------------------\n\nTITLE: Python Printed Representation of TorchScript IR\nDESCRIPTION: Shows how the TorchScript graph is converted back to readable Python code. This is accessible through the code property of a ScriptModule and represents the same computation as the graph but in Python syntax.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self,\n    x: Tensor,\n    y: int,\n    z: float) -> Tensor:\n  if torch.gt(y, 2):\n    x0 = torch.add(x, z, 1)\n  else:\n    x0 = torch.add(x, y, 1)\n  return x0\n```\n\n----------------------------------------\n\nTITLE: Incorrectly Delaying Shared CUDA Tensor Memory Release (Python)\nDESCRIPTION: Illustrates a bad practice where a shared CUDA tensor `x` received from a queue is not deleted promptly after use. Keeping the reference `x` alive while performing other tasks forces the producer process to maintain the original tensor in memory unnecessarily long, potentially leading to higher memory consumption.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/multiprocessing.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n## Bad\nx = queue.get()\n# do somethings with x\n# do everything else (producer have to keep x in memory)\n```\n\n----------------------------------------\n\nTITLE: Managing CUDA Streams on Same Device in PyTorch C++\nDESCRIPTION: This example demonstrates how to acquire and set CUDA streams on the same device, using setCurrentCUDAStream to switch between a custom stream and the default stream.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n// create a tensor on device 0\ntorch::Tensor tensor0 = torch::ones({2, 2}, torch::device(torch::kCUDA));\n// get a new CUDA stream from CUDA stream pool on device 0\nat::cuda::CUDAStream myStream = at::cuda::getStreamFromPool();\n// set current CUDA stream from default stream to `myStream` on device 0\nat::cuda::setCurrentCUDAStream(myStream);\n// sum() on tensor0 uses `myStream` as current CUDA stream\ntensor0.sum();\n\n// get the default CUDA stream on device 0\nat::cuda::CUDAStream defaultStream = at::cuda::getDefaultCUDAStream();\n// set current CUDA stream back to default CUDA stream on device 0\nat::cuda::setCurrentCUDAStream(defaultStream);\n// sum() on tensor0 uses `defaultStream` as current CUDA stream\ntensor0.sum();\n```\n\n----------------------------------------\n\nTITLE: Adding a New Quantized Operator Entry in native_functions.yaml\nDESCRIPTION: Provides an example YAML configuration for adding a completely new quantized operator (`quantized_xand`) to `ATen/native/native_functions.yaml`. This defines the function signature (`func`) and maps the `QuantizedCPU` dispatch key to the corresponding implementation function (`quantized_xand`). This method is an alternative to `TORCH_LIBRARY_IMPL` in some cases, typically when the quantized and non-quantized signatures match.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- func: quantized_xand(Tensor qa, Tensor qb) -> Tensor\n  dispatch:\n    QuantizedCPU: quantized_xand\n```\n\n----------------------------------------\n\nTITLE: Generating Dummy Data and Targets for CNN in PyTorch\nDESCRIPTION: Sets the computation device to 'cuda' if available (otherwise defaults to CPU implicitly later). Initializes parameters like `num_models` (unused in the snippet) and `batch_size`. Creates a batch of random tensor data (`data`) simulating 28x28 images and corresponding random integer targets (`targets`) for a batch size of 64, moving both tensors to the specified GPU device.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndevice = 'cuda'\n\nnum_models = 10\nbatch_size = 64\ndata = torch.randn(batch_size, 1, 28, 28, device=device)\n\ntargets = torch.randint(10, (64,), device=device)\n```\n\n----------------------------------------\n\nTITLE: Executing aten._softmax in PyTorch\nDESCRIPTION: This code applies the aten._softmax.default operator over a four-dimensional tensor to compute the softmax along the last axis. It extensively uses f16 data type and irregular dimensions. Dependencies include the PyTorch package and CUDA for speed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\naten._softmax.default, ((T([16, 4, 128, 128], f16), -1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating DataPipe Type Attribute in Python\nDESCRIPTION: Shows how to access and print the 'type' attribute of DataPipe classes and instances.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef print_helper(cls, obj):\n    print(f\"DataPipe[{cls.type}]\\nInstance type: {obj.type}\")\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe[List[int]]):\n    def __iter__(self) -> Iterator[List[int]]:\n        pass\nprint_helper(DP, DP())\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe[Any]):\n    def __iter__(self) -> Iterator[Any]:\n        pass\nprint_helper(DP, DP())\n```\n\nLANGUAGE: python\nCODE:\n```\nclass DP(IterDataPipe[tuple]):\n    def __iter__(self) -> Iterator[tuple]:\n        pass\nprint_helper(DP, DP())\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Benchmark Class for Multiple Operators\nDESCRIPTION: This code defines a TorchBenchmarkBase subclass for benchmarking multiple unary operators. It includes methods for initialization and forward computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass UnaryOpBenchmark(op_bench.TorchBenchmarkBase):\n    def init(self, M, N, device, op_func):\n        self.inputs = {\n            \"input\": torch.rand(M, N, device=device)\n        }\n        self.op_func = op_func\n\n    def forward(self, input):\n        return self.op_func(input)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Android Dependencies in Gradle (Nightly)\nDESCRIPTION: This snippet demonstrates how to set up Gradle dependencies for nightly (snapshot) builds of PyTorch Android. It includes repository configuration and implementation details for both lite interpreter and full JIT builds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Groovy\nCODE:\n```\nrepositories {\n    maven {\n        url \"https://oss.sonatype.org/content/repositories/snapshots\"\n    }\n}\n\n# lite interpreter build\ndependencies {\n    ...\n    implementation 'org.pytorch:pytorch_android_lite:1.12.0-SNAPSHOT'\n    implementation 'org.pytorch:pytorch_android_torchvision_lite:1.12.0-SNAPSHOT'\n    ...\n}\n\n# full jit build\ndependencies {\n    ...\n    implementation 'org.pytorch:pytorch_android:1.12.0-SNAPSHOT'\n    implementation 'org.pytorch:pytorch_android_torchvision:1.12.0-SNAPSHOT'\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Recurrent Network Data Parallelism in PyTorch\nDESCRIPTION: This example demonstrates handling data parallelism issues when using packed and unpacked sequence operations in recurrent networks on PyTorch. It highlights using the `total_length` argument in `pad_packed_sequence` to ensure consistent output sequence lengths across devices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/faq.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n    class MyModule(nn.Module):\n        # ... __init__, other methods, etc.\n\n        # padded_input is of shape [B x T x *] (batch_first mode) and contains\n        # the sequences sorted by lengths\n        #   B is the batch size\n        #   T is max sequence length\n        def forward(self, padded_input, input_lengths):\n            total_length = padded_input.size(1)  # get the max sequence length\n            packed_input = pack_padded_sequence(padded_input, input_lengths,\n                                                batch_first=True)\n            packed_output, _ = self.my_lstm(packed_input)\n            output, _ = pad_packed_sequence(packed_output, batch_first=True,\n                                            total_length=total_length)\n            return output\n\n    \n    m = MyModule().cuda()\n    dp_m = nn.DataParallel(m)\n```\n\n----------------------------------------\n\nTITLE: Adding Tensors in PyTorch\nDESCRIPTION: This snippet represents the 'aten.add.Tensor' operation, which handles the addition of tensors with identical dimensions using the PyTorch framework. Dependencies include PyTorch with support for half-precision floating point computations. It processes tensor pairings, where inputs and outputs are tensors of the same dimensions having performed element-wise addition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 2, ((T([96, 24, 56, 56], f16), T([96, 24, 56, 56], f16)), {})\ncnt: 4, ((T([96, 32, 28, 28], f16), T([96, 32, 28, 28], f16)), {})\ncnt: 6, ((T([96, 64, 14, 14], f16), T([96, 64, 14, 14], f16)), {})\ncnt: 4, ((T([96, 96, 14, 14], f16), T([96, 96, 14, 14], f16)), {})\ncnt: 4, ((T([96, 160, 7, 7], f16), T([96, 160, 7, 7], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Summing Tensors using aten.sum.SymInt and aten.sum.default - Python\nDESCRIPTION: Includes usages of aten.sum.SymInt and aten.sum.default to compute the sum across one or multiple dimensions of tensors. SymInt allows for symbolic/int indexing, and default does full-tensor reduction. Inputs are f16 tensors with various shape/stride settings, outputs are reduced tensors or scalars. Prerequisites: PyTorch and compatible tensor shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([2048, 50358], f16, stride=(0, 0)), [0], True), {})\ncnt: 61, ((T([2048, 768], f16), [0], True), {})\ncnt: 12, ((T([2048, 3072], f16), [0], True), {})\ncnt: 1, ((T([2, 1024, 768], f16), [0], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([2, 1024, 50358], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: In-Place Tensor Operations in PyTorch\nDESCRIPTION: Shows different ways to perform in-place tensor operations using both method syntax with trailing underscore and out parameter variant.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\na = torch.zeros(2, 3)\nb = torch.ones(2, 3)\na.add_(b)  # in-place add, so `a` is modified.\ntorch.add(a, b, out=a) # another way to express the same thing\n```\n\n----------------------------------------\n\nTITLE: Starting Multiple Workers in PyTorch Distributed Processing\nDESCRIPTION: This function is used to start multiple worker processes in PyTorch's distributed processing framework. It's part of the elastic multiprocessing module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/multiprocessing.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ntorch.distributed.elastic.multiprocessing.start_processes\n```\n\n----------------------------------------\n\nTITLE: PyTorch ReLU Operation Pattern\nDESCRIPTION: Shows usage patterns of the ReLU activation function (aten.relu_.default) across various tensor shapes, primarily working with half-precision (f16) tensors of different dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 64, 112, 112], f16),), {})\ncnt: 3, ((T([128, 112, 56, 56], f16),), {})\ncnt: 21, ((T([128, 14, 56, 56], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Computing NLL Loss Backward in PyTorch\nDESCRIPTION: The nll_loss_backward operator calculates the negative log likelihood loss gradient, essential for propagating errors back through a network during training. It is applied to loss terms derived from cross-entropy errors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([], f16), T([32, 1000], f16), T([32], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Quantized Operator Schema using TORCH_LIBRARY in C++\nDESCRIPTION: Demonstrates how to define the schema for a new quantized operator (`quantized::xand`) within the `quantized` library namespace using `TORCH_LIBRARY` in `aten/src/ATen/native/quantized/library.cpp`. This schema string `\"quantized::xand(Tensor qa, Tensor qb) -> Tensor\"` specifies the operator's name, arguments, and return type, enabling its registration and subsequent use via `torch._ops`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_1\n\nLANGUAGE: c++\nCODE:\n```\nTORCH_LIBRARY(quantized, m) {\n  // ... the existing definitions ...\n  m.def(\"quantized::xand(Tensor qa, Tensor qb) -> Tensor\");\n}\n```\n\n----------------------------------------\n\nTITLE: Tensor Mean Operation\nDESCRIPTION: Calculation of mean values across specific dimensions of tensors with various shapes. Operations are performed on half-precision (f16) tensors with dimension parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([64, 240, 28, 28], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.relu_.default\nDESCRIPTION: Logs in-place Rectified Linear Unit (ReLU) activation function calls (`aten.relu_.default`). These operations modify the input tensor directly. Applied to float16 tensors, mostly 4D feature maps but also some 4D tensors with singleton spatial dimensions (e.g., [128, C, 1, 1]).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 32, 112, 112], f16),), {})\ncnt: 1, ((T([128, 24, 112, 112], f16),), {})\ncnt: 1, ((T([128, 24, 56, 56], f16),), {})\ncnt: 1, ((T([128, 8, 1, 1], f16),), {})\ncnt: 1, ((T([128, 56, 56, 56], f16),), {})\ncnt: 1, ((T([128, 56, 28, 28], f16),), {})\ncnt: 1, ((T([128, 6, 1, 1], f16),), {})\ncnt: 1, ((T([128, 152, 28, 28], f16),), {})\ncnt: 7, ((T([128, 152, 14, 14], f16),), {})\ncnt: 1, ((T([128, 14, 1, 1], f16),), {})\ncnt: 4, ((T([128, 38, 1, 1], f16),), {})\ncnt: 1, ((T([128, 368, 14, 14], f16),), {})\ncnt: 13, ((T([128, 368, 7, 7], f16),), {})\ncnt: 6, ((T([128, 92, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Using Global Interpreter Lock (GIL) in C++\nDESCRIPTION: This snippet demonstrates how to use the pybind11::gil_scoped_acquire RAII struct to handle taking and releasing the Python Global Interpreter Lock (GIL) when making calls to the Python API.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/README.md#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nvoid iWantToUsePython() {\n  pybind11::gil_scoped_acquire gil;\n  ...\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Custom Backend with CMake\nDESCRIPTION: This CMake script sets up a custom backend for a PyTorch project. It checks for and includes ROCm support, finds the required Torch package, and configures the build settings for the custom backend library and an associated test executable. The script specifies the C++ standard to be used and links the necessary Torch libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/custom_backend/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Basic CMake setup\ncmake_minimum_required(VERSION 3.15 FATAL_ERROR)\nproject(custom_backend)\n\nif(USE_ROCM)\ninclude(utils)\ninclude(LoadHIP)\nendif()\nfind_package(Torch REQUIRED)\n\nadd_library(custom_backend SHARED custom_backend.cpp)\nset_property(TARGET custom_backend PROPERTY CXX_STANDARD 17)\ntarget_link_libraries(custom_backend \"${TORCH_LIBRARIES}\")\n\nadd_executable(test_custom_backend test_custom_backend.cpp)\nset_property(TARGET test_custom_backend PROPERTY CXX_STANDARD 17)\ntarget_link_libraries(test_custom_backend custom_backend)\n```\n\n----------------------------------------\n\nTITLE: ReLU Implementation using Expression Conditional\nDESCRIPTION: Example showing how expression conditionals are used to implement the ReLU function, demonstrating a functional approach with no side effects.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/tensorexpr/ConditionalsInTE.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstore (((load A) >= 0.0) ? (load A) : 0.0), B\n```\n\n----------------------------------------\n\nTITLE: Invoking Loss Functions with aten.nll_loss_forward and aten.nll_loss_backward - PyTorch - Python\nDESCRIPTION: These snippets describe logging of forward and backward passes for negative log-likelihood loss computation, using inputs such as prediction tensor ([4, 2], f16) and target indexes ([4], i64), ignoring class -100. Outputs: loss value and gradients. The backward variant operates similarly with additional input tensors for autograd. Requires: torch, correct input shape matching.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([4, 2], f16), T([4], i64), None, 1, -100, T([], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([4, 2], f16), T([4], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Logging with TORCH_LOGS Environment Variable Examples\nDESCRIPTION: These examples demonstrate setting the `TORCH_LOGS` environment variable in a shell to configure PyTorch's logging behavior. The `+` prefix decreases the log level (more verbose, e.g., DEBUG), while the `-` prefix increases it (less verbose, e.g., ERROR). Specifying an artifact name enables its output. Settings are comma-separated and apply to components like `dynamo`, `aot`, `inductor`, or custom modules.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/logging.rst#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nTORCH_LOGS=\"+dynamo,aot\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nTORCH_LOGS=\"-dynamo,+inductor\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nTORCH_LOGS=\"aot_graphs\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nTORCH_LOGS=\"+dynamo,schedule\"\n```\n\nLANGUAGE: shell\nCODE:\n```\nTORCH_LOGS=\"+some.random.module,schedule\"\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Backward Operations in PyTorch\nDESCRIPTION: This snippet shows backward pass operations for batch normalization, used during gradient computation. These operations compute gradients with respect to input, scale and shift parameters through the normalization layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 5, ((T([96, 512, 4, 4], f16), T([96, 512, 4, 4], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([96, 256, 8, 8], f16), T([96, 256, 8, 8], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([96, 128, 16, 16], f16), T([96, 128, 16, 16], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([96, 64, 32, 32], f16), T([96, 64, 32, 32], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([96, 64, 64, 64], f16), T([96, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Feature Concatenation Operations in DenseNet Dense Blocks\nDESCRIPTION: This snippet shows the tensor concatenation operations used in DenseNet's dense blocks. DenseNet concatenates feature maps from previous layers along the channel dimension (dimension 1) to create dense connections.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.cat.default\ncnt: 1, (([T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16)], 1), {})\ncnt: 1, (([T([32, 256, 28, 28], f16), T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16)], 1), {})\ncnt: 1, (([T([32, 512, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16)], 1), {})\ncnt: 1, (([T([32, 768, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16)], 1), {})\ncnt: 1, (([T([32, 768, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16)], 1), {})\ncnt: 1, (([T([32, 1024, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16)], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Rebuilding PyTorch Files with Debug Information\nDESCRIPTION: Command to rebuild specific PyTorch source files with debug information using the build_with_debinfo.py tool, which allows targeted debugging without maintaining a separate debug build.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_34\n\nLANGUAGE: bash\nCODE:\n```\n% ./tools/build_with_debinfo.py torch/csrc/autograd/python_variable_indexing.cpp\n[1 / 2] Building caffe2/torch/CMakeFiles/torch_python.dir/csrc/autograd/python_variable_indexing.cpp.o\n[2 / 2] Building lib/libtorch_python.dylib\n```\n\n----------------------------------------\n\nTITLE: Using torch.cond for Data-Dependent Operations\nDESCRIPTION: Demonstrates how to use torch.cond as a replacement for data-dependent if statements to maintain graph compilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# old\n@torch.compile\ndef fn(x):\n    if x.sum() > 0:\n        return x + 1\n    return x - 1\n\n# new\n@torch.compile\ndef fn(x):\n    return torch.cond(\n        x.sum() > 0,\n        lambda x: x + 1,\n        lambda x: x - 1,\n        (x,),\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring QNNPACK Default Quantization Settings\nDESCRIPTION: Shows how to configure PyTorch quantization settings for QNNPACK (mobile/ARM) backends using either post-training quantization (PTQ) or quantization-aware training (QAT) configs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# set the qconfig for PTQ\nqconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n# or, set the qconfig for QAT\nqconfig = torch.ao.quantization.get_default_qat_qconfig('qnnpack')\n# set the qengine to control weight packing\ntorch.backends.quantized.engine = 'qnnpack'\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Function for Performance Comparison\nDESCRIPTION: Defines a utility function `get_perf` to compare the performance of two methods benchmarked using `torch.utils.benchmark.Timer`. It takes two `Measurement` objects (returned by `Timer.timeit`) and descriptive strings as input. It calculates the percentage improvement of the first method over the second based on their median execution times (`.times[0]`) and prints the result.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef get_perf(first, first_descriptor, second, second_descriptor):\n  \"\"\"  takes torch.benchmark objects and compares delta of second vs first. \"\"\"\n  second_res = second.times[0]\n  first_res = first.times[0]\n\n  gain = (first_res-second_res)/first_res\n  if gain < 0: gain *=-1 \n  final_gain = gain*100\n\n  print(f\" Performance delta: {final_gain:.4f} percent improvement with {first_descriptor} \")\n```\n\n----------------------------------------\n\nTITLE: Speeding Up CUDA Build with Ninja on Windows\nDESCRIPTION: This script details the installation of Ninja and its configuration as a CMake generator to expedite CUDA builds on Windows. It requires the pip package manager to install Ninja. The script modifies the CMAKE_GENERATOR environment variable to utilize Ninja.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#2025-04-22_snippet_1\n\nLANGUAGE: bat\nCODE:\n```\nREM Let's install ninja first.\npip install ninja\n\nREM Set it as the cmake generator\nset CMAKE_GENERATOR=Ninja\n```\n\n----------------------------------------\n\nTITLE: Adding Global Callback for Operator Profiling in PyTorch C++\nDESCRIPTION: This C++ snippet demonstrates how to initialize and add global callbacks to profile PyTorch operator invocations. It uses the RecordFunctionCallback interface to track function entries and exits, capturing input and execution details for profiling. The snippet requires the PyTorch C++ API, particularly the torch::addGlobalCallback function. The initialization ensures callbacks are triggered during operator execution, and random sampling can be configured to reduce overhead.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/large_scale_deployments.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n// Called somewhere in the program beginning\nvoid init() {\n    // Sample one in a hundred operator runs randomly\n    addGlobalCallback(\n      RecordFunctionCallback(\n        &onFunctionEnter,\n        &onFunctionExit)\n      .needsInputs(true)\n      .samplingProb(0.01)\n    );\n    // Note, to enable observers in the model calling thread,\n    // call enableRecordFunction() in the thread before running a model\n}\n\nvoid onFunctionEnter(const RecordFunction& fn) {\n    std::cerr << \"Before function \" << fn.name()\n              << \" with \" << fn.inputs().size() << \" inputs\" << std::endl;\n}\n\nvoid onFunctionExit(const RecordFunction& fn) {\n    std::cerr << \"After function \" << fn.name();\n}\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication and Addition with addmm in PyTorch (Python)\nDESCRIPTION: The aten.addmm function combines matrix multiplication with addition in a single operation, typically optimizing performance for layers in neural networks such as dense layers or transformations involving bias addition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\naten.addmm.default\ncnt: 48, ((T([768], f16), T([8192, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 12, ((T([3072], f16), T([8192, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 12, ((T([768], f16), T([8192, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 1, ((T([768], f16), T([16, 768], f16, stride=(393216, 1)), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 1, ((T([2], f16), T([16, 768], f16), T([768, 2], f16, stride=(1, 768))), {})\n```\n\n----------------------------------------\n\nTITLE: Calling a Registered Quantized Operator from Python\nDESCRIPTION: Provides a Python function `quantized_xand` that wraps the call to the registered C++ quantized operator. It imports `ops` from `torch._ops` and calls the kernel using the schema name (`ops.quantized.xand`), passing the input tensors `qa` and `qb`. The recommended location for such functions is `torch/ao/nn/quantized/functional.py`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._ops import ops\n\ndef quantized_xand(qa, qb):\n#Notice the schema changed from `quantized::xand` to `quantized.xand`\n  return ops.quantized.xand(qa, qb)\n```\n\n----------------------------------------\n\nTITLE: Using torch.distributed.rpc.remote() in TorchScript\nDESCRIPTION: Executes a remote call on a worker and gets a Remote Reference RRef as the return value.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.rpc.remote()\n```\n\n----------------------------------------\n\nTITLE: Sharing Memory Across CUDA Graph Captures\nDESCRIPTION: This snippet shows how to share memory across different CUDA Graph captures using the 'pool' argument. This is useful for optimizing memory usage when multiple graphs are known to run in a specific order and not concurrently.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\ng1 = torch.cuda.CUDAGraph()\ng2 = torch.cuda.CUDAGraph()\n\n# (create static inputs for g1 and g2, run warmups of their workloads...)\n\n# Captures g1\nwith torch.cuda.graph(g1):\n    static_out_1 = g1_workload(static_in_1)\n\n# Captures g2, hinting that g2 may share a memory pool with g1\nwith torch.cuda.graph(g2, pool=g1.pool()):\n    static_out_2 = g2_workload(static_in_2)\n\nstatic_in_1.copy_(real_data_1)\nstatic_in_2.copy_(real_data_2)\ng1.replay()\ng2.replay()\n```\n\n----------------------------------------\n\nTITLE: Annotating Instance Data Attributes in TorchScript\nDESCRIPTION: Example showing how to annotate instance data attributes in a Module class. Instance attributes can be optionally marked as Final to indicate they cannot be reassigned outside __init__.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass MyModule(torch.nn.Module):\n    offset_: int\n\ndef __init__(self, offset):\n    self.offset_ = offset\n\n...\n```\n\n----------------------------------------\n\nTITLE: Cloning and Copying Tensors - PyTorch - Python\nDESCRIPTION: Documents the explicit creation of tensor clones and copy operations, typically used for data duplication, preventing in-place modification, or checkpointing tensors at certain stages. Input tensors maintain their shapes and data types and both clone and copy_ preserve values; intended for safe tensor manipulations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 224, 224], f16),), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Installing py-spy using pip\nDESCRIPTION: This bash snippet provides the command necessary to install py-spy, a sampling profiler, via the Python package manager pip. No dependencies are needed beyond pip itself.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\npip install py-spy\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch for Android with Custom Flags\nDESCRIPTION: Example of building PyTorch for Android with additional CMAKE flags to enable binary compilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/build_android.sh -DBUILD_BINARY=ON\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Shape Patterns in PyTorch Operations\nDESCRIPTION: Logs showing tensor shapes appearing in PyTorch operations. Each line shows call count, tensor shapes, and data type information. These represent input tensor specifications across different network layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 570, 14, 14], f16),), {})\ncnt: 1, ((T([128, 636, 14, 14], f16),), {})\ncnt: 1, ((T([128, 702, 14, 14], f16),), {})\ncnt: 1, ((T([128, 768, 14, 14], f16),), {})\ncnt: 1, ((T([128, 840, 7, 7], f16),), {})\ncnt: 1, ((T([128, 906, 7, 7], f16),), {})\ncnt: 1, ((T([128, 972, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1044, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1280, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Slice Backward Operation in PyTorch\nDESCRIPTION: This snippet computes the gradient for a slicing operation during the backward pass. It's used to propagate gradients through tensor slicing operations in the computational graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 3, 48, 64, 2], f16), [8, 3, 48, 64, 85], 4, 2, 4, 1), {})\ncnt: 1, ((T([8, 3, 48, 64, 2], f16), [8, 3, 48, 64, 85], 4, 0, 2, 1), {})\ncnt: 1, ((T([8, 3, 24, 32, 2], f16), [8, 3, 24, 32, 85], 4, 2, 4, 1), {})\ncnt: 1, ((T([8, 3, 24, 32, 2], f16), [8, 3, 24, 32, 85], 4, 0, 2, 1), {})\ncnt: 1, ((T([8, 3, 12, 16, 2], f16), [8, 3, 12, 16, 85], 4, 2, 4, 1), {})\ncnt: 1, ((T([8, 3, 12, 16, 2], f16), [8, 3, 12, 16, 85], 4, 0, 2, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Custom MyRelu Autograd Function Implementation\nDESCRIPTION: Implementation of a custom ReLU function using torch.autograd.Function with forward and symbolic methods for ONNX export.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass MyRelu(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        return input.clamp(min=0)\n```\n\n----------------------------------------\n\nTITLE: Executing Batch Norm Backward Pass (aten.native_batch_norm_backward.default) in PyTorch (Python)\nDESCRIPTION: This operator snippet covers backward propagation through a batch normalization layer, as used for gradient calculation during training. It includes input, grad_output, parameter tensors (gamma, beta), running stats, Booleans to indicate flags, and reduction axes. Usage presumes prior batch norm context and uses both float16 and float32 types, requiring PyTorch's autograd engine.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([32, 1280, 7, 7], f16), T([32, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 320, 7, 7], f16), T([32, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), False, 1e-05, [True, True, True]), {})\ncnt: 8, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f32), T([1152], f32), False, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([32, 192, 7, 7], f16), T([32, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 576, 7, 7], f16), T([32, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 576, 14, 14], f16), T([32, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), False, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([32, 96, 14, 14], f16), T([32, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 80, 14, 14], f16), T([32, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), T([32, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([32, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 40, 28, 28], f16), T([32, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), False, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([32, 120, 28, 28], f16), T([32, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f32), T([120], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 72, 28, 28], f16), T([32, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([32, 72, 56, 56], f16), T([32, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 24, 56, 56], f16), T([32, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 48, 56, 56], f16), T([32, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 48, 112, 112], f16), T([32, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 16, 112, 112], f16), T([32, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), False, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Multiplexing DataPipes in PyTorch\nDESCRIPTION: Shows how to use the mux() method to combine multiple DataPipes into a single DataPipe, interleaving their elements.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndp1 = ExampleIterPipe(3)\ndp2 = ExampleIterPipe(3).map(lambda x: x * 10)\ndp3 = ExampleIterPipe(3).map(lambda x: x * 100)\n\ndp = dp1.mux(dp2, dp3)\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Defining Lazy Test Source Files in CMake\nDESCRIPTION: Creates a CMake list variable `LAZY_TEST_SRCS` containing the paths to the core C++ source files for the lazy tensor tests. It uses the previously defined `LAZY_TEST_ROOT` variable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# Build the cpp gtest binary containing the cpp-only tests.\nset(LAZY_TEST_SRCS\n  ${LAZY_TEST_ROOT}/test_backend_device.cpp\n  ${LAZY_TEST_ROOT}/test_cache.cpp\n  ${LAZY_TEST_ROOT}/test_ir.cpp\n  ${LAZY_TEST_ROOT}/test_ir_util.cpp\n  ${LAZY_TEST_ROOT}/test_misc.cpp\n  ${LAZY_TEST_ROOT}/test_permutation_util.cpp\n  ${LAZY_TEST_ROOT}/test_shape.cpp\n  ${LAZY_TEST_ROOT}/test_trie_cache.cpp\n  ${LAZY_TEST_ROOT}/test_util.cpp\n  ${LAZY_TEST_ROOT}/test_lazy_graph_executor.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Sample Inputs Function for OpInfo\nDESCRIPTION: Example of implementing a sample_inputs_func for OpInfo, which generates test cases with different input combinations for the operator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nyield opinfo_core.SampleInput(input, args=(...), kwargs={...})\n```\n\n----------------------------------------\n\nTITLE: Running Full PyTorch Python Test Suite\nDESCRIPTION: Executes the entire PyTorch Python test suite using the run_test.py script.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython test/run_test.py\n```\n\n----------------------------------------\n\nTITLE: Configuring QNNPACK Build Targets with CMake - CMake\nDESCRIPTION: This CMake script sets up the QNNPACK library build process, organizing source files into groups by functionality and architecture, setting compile flags according to target processor, and specifying how to build (static, shared, or default) the library. It ensures correct linking with required dependencies such as clog, cpuinfo, pthreadpool, and fxdiv, and adds appropriate directories for header inclusion. The script uses CMake conditionals to adjust build configurations per platform and manages dependency subdirectory integration or system library detection.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ QNNPACK library\nset(PYTORCH_QNNPACK_INIT_SRCS\n  src/init.c\n  src/add.c\n  src/average-pooling.c\n  src/channel-shuffle.c\n  src/clamp.c\n  src/conv-prepack.cc\n  src/convolution.c\n  src/deconvolution.c\n  src/fc-prepack.cc\n  src/fully-connected.c\n  src/fully-connected-sparse.c\n  src/global-average-pooling.c\n  src/hardsigmoid.c\n  src/hardswish.c\n  src/leaky-relu.c\n  src/max-pooling.c\n  src/sigmoid.c\n  src/softargmax.c\n  src/tanh.c\n  src/operator-delete.c)\n\nset(PYTORCH_QNNPACK_EXEC_SRCS\n  src/conv-run.cc\n  src/deconv-run.cc\n  src/fc-run.cc\n  src/fc-unpack.cc\n  src/fc-dynamic-run.cc\n  src/indirection.c\n  src/operator-run.c)\n\nset(PYTORCH_QNNPACK_SCALAR_UKERNELS\n  src/u8lut32norm/scalar.c\n  src/x8lut/scalar.c)\n\nset(PYTORCH_QNNPACK_PSIMD_UKERNELS\n  src/sgemm/6x8-psimd.c)\n\nset(PYTORCH_QNNPACK_ARM_NEON_UKERNELS\n  src/q8avgpool/mp8x9p8q-neon.c\n  src/q8avgpool/up8x9-neon.c\n  src/q8avgpool/up8xm-neon.c\n  src/q8conv/4x8-neon.c\n  src/q8conv/8x8-neon.c\n  src/q8dwconv/mp8x25-neon.c\n  src/q8dwconv/mp8x25-neon-per-channel.c\n  src/q8dwconv/mp8x27-neon.c\n  src/q8dwconv/up8x9-neon.c\n  src/q8dwconv/up8x9-neon-per-channel.c\n  src/q8gavgpool/mp8x7p7q-neon.c\n  src/q8gavgpool/up8x7-neon.c\n  src/q8gavgpool/up8xm-neon.c\n  src/q8gemm/4x-sumrows-neon.c\n  src/q8gemm/4x8-neon.c\n  src/q8gemm/4x8-dq-neon.c\n  src/q8gemm/4x8c2-xzp-neon.c\n  src/q8gemm/6x4-neon.c\n  src/q8gemm/8x8-neon.c\n  src/q8vadd/neon.c\n  src/sgemm/5x8-neon.c\n  src/sgemm/6x8-neon.c\n  src/u8clamp/neon.c\n  src/u8maxpool/16x9p8q-neon.c\n  src/u8maxpool/sub16-neon.c\n  src/u8rmax/neon.c\n  src/x8zip/x2-neon.c\n  src/x8zip/x3-neon.c\n  src/x8zip/x4-neon.c\n  src/x8zip/xm-neon.c)\n\nset(PYTORCH_QNNPACK_AARCH32_ASM_UKERNELS\n  src/hgemm/8x8-aarch32-neonfp16arith.S\n  src/q8conv/4x8-aarch32-neon.S\n  src/q8dwconv/up8x9-aarch32-neon.S\n  src/q8dwconv/up8x9-aarch32-neon-per-channel.S\n  src/q8gemm/4x8-aarch32-neon.S\n  src/q8gemm/4x8-dq-aarch32-neon.S\n  src/q8gemm/4x8c2-xzp-aarch32-neon.S\n  src/q8gemm_sparse/4x4-packA-aarch32-neon.S\n  src/q8gemm_sparse/4x8c1x4-dq-packedA-aarch32-neon.S\n  src/q8gemm_sparse/4x8c8x1-dq-packedA-aarch32-neon.S)\n\nset(PYTORCH_QNNPACK_AARCH64_ASM_UKERNELS\n  src/q8conv/8x8-aarch64-neon.S\n  src/q8gemm/8x8-aarch64-neon.S\n  src/q8gemm/8x8-dq-aarch64-neon.S\n  src/q8gemm_sparse/8x4-packA-aarch64-neon.S\n  src/q8gemm_sparse/8x8c1x4-dq-packedA-aarch64-neon.S\n  src/q8gemm_sparse/8x8c8x1-dq-packedA-aarch64-neon.S)\n\nset(PYTORCH_QNNPACK_X86_SSE2_UKERNELS\n  src/q8avgpool/mp8x9p8q-sse2.c\n  src/q8avgpool/up8x9-sse2.c\n  src/q8avgpool/up8xm-sse2.c\n  src/q8conv/4x4c2-sse2.c\n  src/q8dwconv/mp8x25-sse2.c\n  src/q8dwconv/mp8x25-sse2-per-channel.c\n  src/q8dwconv/mp8x27-sse2.c\n  src/q8dwconv/up8x9-sse2.c\n  src/q8dwconv/up8x9-sse2-per-channel.c\n  src/q8gavgpool/mp8x7p7q-sse2.c\n  src/q8gavgpool/up8x7-sse2.c\n  src/q8gavgpool/up8xm-sse2.c\n  src/q8gemm/2x4c8-sse2.c\n  src/q8gemm/4x4c2-dq-sse2.c\n  src/q8gemm/4x4c2-sse2.c\n  src/q8gemm_sparse/8x4c1x4-dq-packedA-sse2.c\n  src/q8gemm_sparse/8x4-packA-sse2.c\n  src/q8vadd/sse2.c\n  src/u8clamp/sse2.c\n  src/u8maxpool/16x9p8q-sse2.c\n  src/u8maxpool/sub16-sse2.c\n  src/u8rmax/sse2.c\n  src/x8zip/x2-sse2.c\n  src/x8zip/x3-sse2.c\n  src/x8zip/x4-sse2.c\n  src/x8zip/xm-sse2.c)\n\nset(PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_SCALAR_UKERNELS} ${PYTORCH_QNNPACK_PSIMD_UKERNELS})\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"^armv[5-8]\" OR IOS_ARCH MATCHES \"^armv7\")\n  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_ARM_NEON_UKERNELS})\n  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_AARCH32_ASM_UKERNELS})\nendif()\nif(PYTORCH_QNNPACK_TARGET_PROCESSOR MATCHES \"^(aarch64|arm64)$\" OR IOS_ARCH MATCHES \"^arm64.*\")\n  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_ARM_NEON_UKERNELS})\n  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_AARCH64_ASM_UKERNELS})\nendif()\nif(PYTORCH_QNNPACK_TARGET_PROCESSOR MATCHES \"^(i[3-6]86|x86_64)$\" OR IOS_ARCH MATCHES \"^(i386|x86_64)$\")\n  list(APPEND PYTORCH_QNNPACK_UKERNELS ${PYTORCH_QNNPACK_X86_SSE2_UKERNELS})\nendif()\n\nif(PYTORCH_QNNPACK_LIBRARY_TYPE STREQUAL \"default\")\n  add_library(pytorch_qnnpack ${PYTORCH_QNNPACK_INIT_SRCS} ${PYTORCH_QNNPACK_EXEC_SRCS} ${PYTORCH_QNNPACK_UKERNELS})\nelseif(PYTORCH_QNNPACK_LIBRARY_TYPE STREQUAL \"shared\")\n  add_library(pytorch_qnnpack SHARED ${PYTORCH_QNNPACK_INIT_SRCS} ${PYTORCH_QNNPACK_EXEC_SRCS} ${PYTORCH_QNNPACK_UKERNELS})\nelseif(PYTORCH_QNNPACK_LIBRARY_TYPE STREQUAL \"static\")\n  add_library(pytorch_qnnpack STATIC ${PYTORCH_QNNPACK_INIT_SRCS} ${PYTORCH_QNNPACK_EXEC_SRCS} ${PYTORCH_QNNPACK_UKERNELS})\nelse()\n  message(FATAL_ERROR \"Unsupported QNNPACK library type \\\"${PYTORCH_QNNPACK_LIBRARY_TYPE}\\\". Must be \\\"static\\\", \\\"shared\\\", or \\\"default\\\"\")\nendif()\nset_target_properties(pytorch_qnnpack PROPERTIES\n  CXX_STANDARD 14\n  C_STANDARD 11\n  C_EXTENSIONS YES)\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"^armv[5-8]\" OR IOS_ARCH MATCHES \"^armv7\")\n  set_property(SOURCE ${PYTORCH_QNNPACK_ARM_NEON_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -O2 -marm -mfpu=neon \")\n  if(IOS)\n    set_property(SOURCE ${PYTORCH_QNNPACK_AARCH32_ASM_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -arch ${IOS_ARCH} \")\n  endif()\nendif()\nif(PYTORCH_QNNPACK_TARGET_PROCESSOR MATCHES \"^(aarch64|arm64)$\" OR IOS_ARCH MATCHES \"^arm64.*\")\n  set_property(SOURCE ${PYTORCH_QNNPACK_ARM_NEON_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -O2 \")\n  if(IOS)\n    set_property(SOURCE ${PYTORCH_QNNPACK_AARCH64_ASM_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -arch ${IOS_ARCH} \")\n  endif()\nendif()\nif(PYTORCH_QNNPACK_TARGET_PROCESSOR MATCHES \"^(i[3-6]86|x86_64)$\" OR IOS_ARCH MATCHES \"^(i386|x86_64)$\")\n  set_property(SOURCE ${PYTORCH_QNNPACK_X86_SSE2_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -O2 -msse2 \")\nendif()\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"^armv[5-8]\" OR IOS_ARCH MATCHES \"^armv7\")\n  set_property(SOURCE ${PYTORCH_QNNPACK_PSIMD_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -O2 -marm -mfpu=neon \")\n  set_property(SOURCE ${PYTORCH_QNNPACK_SCALAR_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -O2 -marm \")\nelse()\n  set_property(SOURCE ${PYTORCH_QNNPACK_PSIMD_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -O2 \")\n  set_property(SOURCE ${PYTORCH_QNNPACK_SCALAR_UKERNELS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -O2 \")\nendif()\nset_property(SOURCE ${PYTORCH_QNNPACK_INIT_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -Os \")\nif(NOT CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n  set_property(SOURCE ${PYTORCH_QNNPACK_OPERATOR_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -O2 \")\nendif()\ntarget_include_directories(pytorch_qnnpack PUBLIC include)\ntarget_include_directories(pytorch_qnnpack PUBLIC src)\nset_target_properties(pytorch_qnnpack PROPERTIES PUBLIC_HEADER include/pytorch_qnnpack.h)\nset_target_properties(pytorch_qnnpack PROPERTIES PUBLIC_HEADER include/qnnpack_func.h)\n\n# ---[ Configure clog\nif(NOT TARGET clog)\n  set(CLOG_BUILD_TESTS OFF CACHE BOOL \"\")\n  set(CLOG_RUNTIME_TYPE \"${CPUINFO_RUNTIME_TYPE}\" CACHE STRING \"\")\n  add_subdirectory(\n    \"${CLOG_SOURCE_DIR}\"\n    \"${CONFU_DEPENDENCIES_BINARY_DIR}/clog\")\n  # We build static version of clog but a dynamic library may indirectly depend on it\n  set_property(TARGET clog PROPERTY POSITION_INDEPENDENT_CODE ON)\nendif()\ntarget_link_libraries(pytorch_qnnpack PUBLIC clog)\n\n# ---[ Configure cpuinfo\nif(NOT TARGET cpuinfo AND USE_SYSTEM_CPUINFO)\n  add_library(cpuinfo SHARED IMPORTED)\n  find_library(CPUINFO_LIBRARY cpuinfo)\n  if(NOT CPUINFO_LIBRARY)\n    message(FATAL_ERROR \"Cannot find cpuinfo\")\n  endif()\n  message(\"Found cpuinfo: ${CPUINFO_LIBRARY}\")\n  set_target_properties(cpuinfo PROPERTIES IMPORTED_LOCATION \"${CPUINFO_LIBRARY}\")\nelseif(NOT TARGET cpuinfo)\n  set(CPUINFO_BUILD_TOOLS OFF CACHE BOOL \"\")\n  set(CPUINFO_BUILD_UNIT_TESTS OFF CACHE BOOL \"\")\n  set(CPUINFO_BUILD_MOCK_TESTS OFF CACHE BOOL \"\")\n  set(CPUINFO_BUILD_BENCHMARKS OFF CACHE BOOL \"\")\n  add_subdirectory(\n    \"${CPUINFO_SOURCE_DIR}\"\n    \"${CONFU_DEPENDENCIES_BINARY_DIR}/cpuinfo\")\nendif()\ntarget_link_libraries(pytorch_qnnpack PRIVATE cpuinfo)\n\n# ---[ Configure pthreadpool\nif(NOT TARGET pthreadpool AND NOT USE_SYSTEM_PTHREADPOOL)\n  set(PTHREADPOOL_BUILD_TESTS OFF CACHE BOOL \"\")\n  set(PTHREADPOOL_BUILD_BENCHMARKS OFF CACHE BOOL \"\")\n  add_subdirectory(\n    \"${PTHREADPOOL_SOURCE_DIR}\"\n    \"${CONFU_DEPENDENCIES_BINARY_DIR}/pthreadpool\")\nelseif(NOT TARGET pthreadpool AND USE_SYSTEM_PTHREADPOOL)\n  add_library(pthreadpool SHARED IMPORTED)\n  find_library(PTHREADPOOL_LIBRARY pthreadpool)\n  if(NOT PTHREADPOOL_LIBRARY)\n    message(FATAL_ERROR \"Cannot find pthreadpool\")\n  endif()\n  message(\"-- Found pthreadpool: ${PTHREADPOOL_LIBRARY}\")\n  set_target_properties(pthreadpool PROPERTIES\n    IMPORTED_LOCATION \"${PTHREADPOOL_LIBRARY}\")\n  add_library(pthreadpool_interface INTERFACE)\nendif()\ntarget_link_libraries(pytorch_qnnpack PUBLIC pthreadpool)\n\n# ---[ Configure FXdiv\nif(NOT TARGET fxdiv AND NOT USE_SYSTEM_FXDIV)\n  set(FXDIV_BUILD_TESTS OFF CACHE BOOL \"\")\n  set(FXDIV_BUILD_BENCHMARKS OFF CACHE BOOL \"\")\n  add_subdirectory(\n    \"${FXDIV_SOURCE_DIR}\"\n    \"${CONFU_DEPENDENCIES_BINARY_DIR}/fxdiv\")\nelseif(NOT TARGET fxdiv AND USE_SYSTEM_FXDIV)\n  find_file(FXDIV_HDR fxdiv.h PATH_SUFFIXES include)\n  if(NOT FXDIV_HDR)\n    message(FATAL_ERROR \"Cannot find fxdiv\")\n  endif()\n  add_library(fxdiv STATIC \"${FXDIV_HDR}\")\n  set_property(TARGET fxdiv PROPERTY LINKER_LANGUAGE C)\nendif()\ntarget_link_libraries(pytorch_qnnpack PRIVATE fxdiv)\n\n# -- [ CMake-4 compat mode\nif(CMAKE_VERSION VERSION_GREATER_EQUAL \"4.0.0\" AND NOT (USE_SYSTEM_PSIMD OR USE_SYSTEM_FP16))\n  message(WARNING \"Ancient psimd/FP16 forces CMake compatibility\")\n  set(CMAKE_POLICY_VERSION_MINIMUM 3.5)\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage for MobileNetV2\nDESCRIPTION: This code snippet shows the usage statistics of various PyTorch operators in a MobileNetV2 implementation. It includes tensor shapes, data types, and occurrence counts for operations like hardtanh, batch normalization, and matrix multiplication.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.hardtanh.default\ncnt: 5, ((T([96, 192, 28, 28], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 192, 14, 14], f16), 0.0, 6.0), {})\ncnt: 8, ((T([96, 384, 14, 14], f16), 0.0, 6.0), {})\ncnt: 5, ((T([96, 576, 14, 14], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 576, 7, 7], f16), 0.0, 6.0), {})\ncnt: 6, ((T([96, 960, 7, 7], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 1280, 7, 7], f16), 0.0, 6.0), {})\n\nOperator: aten.hardtanh_backward.default\ncnt: 1, ((T([96, 1280, 7, 7], f16), T([96, 1280, 7, 7], f16), 0.0, 6.0), {})\ncnt: 6, ((T([96, 960, 7, 7], f16), T([96, 960, 7, 7], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 576, 7, 7], f16), T([96, 576, 7, 7], f16), 0.0, 6.0), {})\ncnt: 5, ((T([96, 576, 14, 14], f16), T([96, 576, 14, 14], f16), 0.0, 6.0), {})\ncnt: 8, ((T([96, 384, 14, 14], f16), T([96, 384, 14, 14], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 192, 14, 14], f16), T([96, 192, 14, 14], f16), 0.0, 6.0), {})\ncnt: 5, ((T([96, 192, 28, 28], f16), T([96, 192, 28, 28], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 144, 28, 28], f16), T([96, 144, 28, 28], f16), 0.0, 6.0), {})\ncnt: 3, ((T([96, 144, 56, 56], f16), T([96, 144, 56, 56], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 96, 56, 56], f16), T([96, 96, 56, 56], f16), 0.0, 6.0), {})\ncnt: 1, ((T([96, 96, 112, 112], f16), T([96, 96, 112, 112], f16), 0.0, 6.0), {})\ncnt: 2, ((T([96, 32, 112, 112], f16), T([96, 32, 112, 112], f16), 0.0, 6.0), {})\n\nOperator: aten.mean.dim\ncnt: 1, ((T([96, 1280, 7, 7], f16), [-1, -2], True), {})\n\nOperator: aten.mm.default\ncnt: 1, ((T([96, 1000], f16, stride=(0, 0)), T([1000, 1280], f16)), {})\ncnt: 1, ((T([1000, 96], f16, stride=(0, 0)), T([96, 1280], f16)), {})\n\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([96, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([96, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([96, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([96, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})\ncnt: 2, ((T([96, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([96, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([96, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([96, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([96, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([96, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), False, 0.1, 1e-05), {})\ncnt: 4, ((T([96, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 8, ((T([96, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([96, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([96, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([96, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([96, 160, 7, 7], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), False, 0.1, 1e-05), {})\ncnt: 6, ((T([96, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([96, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([96, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), False, 0.1, 1e-05), {})\n\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([96, 1280, 7, 7], f16), T([96, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([96, 320, 7, 7], f16), T([96, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), False, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([96, 960, 7, 7], f16), T([96, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f32), T([960], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([96, 160, 7, 7], f16), T([96, 160, 7, 7], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([96, 576, 7, 7], f16), T([96, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([96, 576, 14, 14], f16), T([96, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([96, 96, 14, 14], f16), T([96, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 1e-05, [True, True, True]), {})\ncnt: 8, ((T([96, 384, 14, 14], f16), T([96, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f32), T([384], f32), False, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([96, 64, 14, 14], f16), T([96, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([96, 192, 14, 14], f16), T([96, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([96, 192, 28, 28], f16), T([96, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([96, 32, 28, 28], f16), T([96, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([96, 144, 28, 28], f16), T([96, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([96, 144, 56, 56], f16), T([96, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), False, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([96, 24, 56, 56], f16), T([96, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([96, 96, 56, 56], f16), T([96, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([96, 96, 112, 112], f16), T([96, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([96, 16, 112, 112], f16), T([96, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), False, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([96, 32, 112, 112], f16), T([96, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})\n\nOperator: aten.sum.SymInt\ncnt: 1, ((T([96, 1000], f16, stride=(0, 0)), [0], True), {})\n\nOperator: aten.sum.default\ncnt: 1, ((T([96, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Transforming Loop with Continue Statement in Python\nDESCRIPTION: Demonstrates how a Python loop with a continue statement is transformed into a more explicit control flow structure during compilation. This transformation helps in converting the code to SSA form.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nwhile i < 5:\n  if i == 3:\n    i += 1\n    continue\n  i += 2\n```\n\nLANGUAGE: Python\nCODE:\n```\ncontinue_loop = i < 5\nwhile continue_loop:\n  if i == 3:\n    i = i + 1\n    continue_loop = i < 5\n    did_exit = True\n  if did_exit:\n    pass\n  else:\n    i = i + 2\n    continue_loop = i < 5\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Count for aten.addmm.default Operations\nDESCRIPTION: Shows matrix multiplication with addition (addmm) used for the final classification layer, with weights of shape [1984, 1000] operating on feature tensor [128, 1984] and adding a bias of shape [1000].\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 1984], f16), T([1984, 1000], f16, stride=(1, 1984))), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Python Code with py-spy\nDESCRIPTION: This Python snippet demonstrates creating a simple test script to profile the torch.add operation using py-spy, highlighting the need to repeat operations to gather relevant data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nt1 = torch.tensor([[1, 1], [1, 1.]])\nt2 = torch.tensor([[0, 0], [0, 0.]])\n\nfor _ in range(1000000):\n    torch.add(t1, t2)\n```\n\n----------------------------------------\n\nTITLE: Using const Tensor& for Mutable Tensors in PyTorch Kernel Signatures (native_functions.yaml)\nDESCRIPTION: Demonstrates the `use_const_ref_for_mutable_tensors: True` setting in `native_functions.yaml`. When enabled, this flag instructs the code generator to use `const Tensor&` for tensor arguments in C++ function signatures, even if the underlying tensor data might be modified by the kernel. This replaces the older `Tensor&` style, providing better C++ const correctness.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nuse_const_ref_for_mutable_tensors: True\n```\n\n----------------------------------------\n\nTITLE: Creating Symbolic Link for Faster Rebuilds\nDESCRIPTION: A bash command to create a symbolic link from the build folder to torch/lib, allowing for faster rebuilds without reinstalling after C++/CUDA/ObjectiveC file changes on Linux/Mac.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npushd torch/lib; sh -c \"ln -sf ../../build/lib/libtorch_cpu.* .\"; popd\n```\n\n----------------------------------------\n\nTITLE: Running Multithreading in PyTorch with Autograd\nDESCRIPTION: Demonstrates using Python threads to run training function in parallel leveraging PyTorch's autograd. The train_fn defines forward and backward operations, and threading is used to manage multiple executions. Key considerations include avoiding shared state issues and understanding concurrency constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Define a train function to be used in different threads\ndef train_fn():\n    x = torch.ones(5, 5, requires_grad=True)\n    # forward\n    y = (x + 3) * (x + 4) * 0.5\n    # backward\n    y.sum().backward()\n    # potential optimizer update\n\n\n# User write their own threading code to drive the train_fn\nthreads = []\nfor _ in range(10):\n    p = threading.Thread(target=train_fn, args=())\n    p.start()\n    threads.append(p)\n\nfor p in threads:\n    p.join()\n```\n\n----------------------------------------\n\nTITLE: Mocking Modules with Exclusions\nDESCRIPTION: Example of using the mock action with exclusion patterns to mock all modules except those matching the exclusion patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nexporter.mock(\"**\", exclude=[\"torchvision.**\"])\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Multiplication Operations\nDESCRIPTION: Shows matrix multiplication operations (addmm and bmm) with various tensor dimensions and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\naten.addmm.default((T([64], f16), T([100352, 64], f16), T([64, 64], f16, stride=(1, 64))))\naten.bmm.default((T([32, 3136, 64], f16), T([32, 64, 49], f16, stride=(6272, 1, 128))))\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.relu_.default with Tensor Arguments (Text)\nDESCRIPTION: This section logs calls to the `aten.relu_.default` operator, which performs an in-place Rectified Linear Unit (ReLU) activation. The examples show the operator being called on tensors with various shapes (e.g., [128, 128, 1, 1], [128, 768, 1, 1]) and float16 (f16) data type. The input is a tuple containing a single tensor description.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.relu_.default\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([128, 128, 1, 1], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([128, 256, 1, 1], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 9, ((T([128, 768, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Registering Catch-All Custom PyTorch Operator Kernel (C++)\nDESCRIPTION: Illustrates registering a catch-all kernel function (`my_kernel_fallback`) for a custom operator (`my_namespace::my_op`). This kernel will be used for all backends, disabling dispatch for this operator. Registration uses `torch::RegisterOperators::options().catchAllKernel()` with `decltype` and a function pointer.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace { Tensor my_kernel_fallback(Tensor a, Tensor b) {...} }\n\nstatic auto registry = torch::RegisterOperators()\n   .op(\"my_namespace::my_op\", torch::RegisterOperators::options()\n       .catchAllKernel<decltype(my_kernel_fallback), &my_kernel_fallback>());\n```\n\n----------------------------------------\n\nTITLE: Explicitly Defining PyTorch Operator Schema (C++)\nDESCRIPTION: Demonstrates how to explicitly define the operator schema as a string literal passed to the `.op()` method, instead of relying on automatic inference from the kernel signature. This allows for documentation and ensures the schema matches expectations. The schema string follows TorchScript syntax.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace { Tensor my_kernel_cpu(const Tensor& a, const Tensor& b) {...} }\n\nstatic auto registry = torch::RegisterOperators()\n   .op(\"my_namespace::my_op(Tensor a, Tensor b) -> Tensor\",\n       torch::RegisterOperators::options()\n         .kernel<decltype(my_kernel_cpu), &my_kernel_cpu>(CPU()));\n```\n\n----------------------------------------\n\nTITLE: Enabling Manual C++ Kernel Registration in PyTorch (native_functions.yaml)\nDESCRIPTION: Illustrates the `manual_kernel_registration: True` flag in `native_functions.yaml`. Setting this flag prevents the PyTorch code generation system from automatically registering the C++ operator implementation with the dispatcher via the `TypeDefault` (catch-all) key. Manual registration is required elsewhere, typically in `torch/csrc/autograd/VariableTypeManual.cpp`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmanual_kernel_registration: True\n```\n\n----------------------------------------\n\nTITLE: Iterating Through Named Parameters of a Composite Module\nDESCRIPTION: Demonstrates iterating through all named parameters of the `DynamicNet` instance using `named_parameters()`. This shows that the method recursively collects parameters from the module itself (`final`) and all its submodules, including those within `ModuleList` (`linears.0`, `linears.1`, etc.) and `ModuleDict` (if they had parameters).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/modules.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor parameter in dynamic_net.named_parameters():\n  print(parameter)\n: ('linears.0.weight', Parameter containing:\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\n        [-0.4446,  0.4628,  0.8774,  1.6848],\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n        [-0.0550,  1.5317,  1.1064, -0.5562],\n        [-0.4028, -0.6942,  1.5793, -1.0140],\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n        [-0.3526,  0.8756, -1.5847, -0.6016],\n        [-0.3269, -0.1608,  0.2897, -2.0829],\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n('final.weight', Parameter containing:\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.mm.default Calls - PyTorch - Python\nDESCRIPTION: Captures operations using the matrix multiplication (aten.mm.default) operator with inputs as float16 tensors of specified shapes and strides. Dependencies are two-dimensional or broadcast-compatible tensors of compatible dimensions. Inputs are the input tensors; outputs are result tensors of the appropriate shape, with operator count and stride variations noted.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 1024], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1024], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations\nDESCRIPTION: Convolution operations with different kernel sizes, strides and padding configurations\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\naten.convolution.default(T([64, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1)\n```\n\n----------------------------------------\n\nTITLE: Documenting Timer Class in PyTorch Benchmark Utils\nDESCRIPTION: This snippet documents the Timer class from the torch.utils.benchmark module, including all its members.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: Timer\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for PyTorch Android Native Build\nDESCRIPTION: This CMake configuration demonstrates how to set up include directories and link libraries for a PyTorch Android native build. It locates the necessary headers and libraries extracted from the AAR.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n# Relative path of gradle build directory to CMakeLists.txt\nset(build_DIR ${CMAKE_SOURCE_DIR}/build)\n\nfile(GLOB PYTORCH_INCLUDE_DIRS \"${build_DIR}/pytorch_android*.aar/headers\")\nfile(GLOB PYTORCH_LINK_DIRS \"${build_DIR}/pytorch_android*.aar/jni/${ANDROID_ABI}\")\n\nset(BUILD_SUBDIR ${ANDROID_ABI})\ntarget_include_directories(${PROJECT_NAME} PRIVATE\n  ${PYTORCH_INCLUDE_DIRS}\n)\n\nfind_library(PYTORCH_LIBRARY pytorch_jni\n  PATHS ${PYTORCH_LINK_DIRS}\n  NO_CMAKE_FIND_ROOT_PATH)\n\nfind_library(FBJNI_LIBRARY fbjni\n  PATHS ${PYTORCH_LINK_DIRS}\n  NO_CMAKE_FIND_ROOT_PATH)\n\ntarget_link_libraries(${PROJECT_NAME}\n  ${PYTORCH_LIBRARY})\n  ${FBJNI_LIBRARY})\n```\n\n----------------------------------------\n\nTITLE: Applying Native Batch Normalization (aten.native_batch_norm.default) in PyTorch (Python)\nDESCRIPTION: Represents multiple invocations of the native batch normalization operator, each on float16 tensors of NCHW layout and multiple channel sizes. Requires input, weight (gamma), bias (beta), running mean/var, a boolean for training mode, epsilon, and momentum. Used for layer normalization in deep neural networks in float16, dependent on PyTorch batch norm APIs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 1, ((T([32, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 1, ((T([32, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 1, ((T([32, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 3, ((T([32, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 5, ((T([32, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 1, ((T([32, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 3, ((T([32, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 4, ((T([32, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 3, ((T([32, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 6, ((T([32, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 2, ((T([32, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 3, ((T([32, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 1, ((T([32, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 4, ((T([32, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 8, ((T([32, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 1, ((T([32, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), False, 0.00029999999999996696, 1e-05), {})\ncnt: 1, ((T([32, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), False, 0.00029999999999996696, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Convolution Operations\nDESCRIPTION: Series of convolution operations using PyTorch tensors with half precision (f16). Operations include forward and backward passes with varying input shapes, kernel sizes, strides, and padding configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2048, 14, 14], f16), T([800, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 2112, 14, 14], f16), T([800, 2112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication in PyTorch\nDESCRIPTION: This snippet shows matrix multiplication operations in PyTorch using the aten.mm.default operator. It demonstrates multiplications with different tensor shapes and strides, likely part of fully connected layers in a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 2048], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 2048], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Installing Core Generated Headers in CMake\nDESCRIPTION: Prints the target installation directory for core headers for informational purposes. Then, it iterates through the list of core generated headers (`core_generated_headers`), prints a message for each header being installed, and installs it into the `${AT_INSTALL_INCLUDE_DIR}/ATen/core` directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_28\n\nLANGUAGE: cmake\nCODE:\n```\nmessage(\"AT_INSTALL_INCLUDE_DIR ${AT_INSTALL_INCLUDE_DIR}/ATen/core\")\nforeach(HEADER ${core_generated_headers})\n  message(\"core header install: ${HEADER}\")\n  install(FILES ${HEADER} DESTINATION ${AT_INSTALL_INCLUDE_DIR}/ATen/core)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Defining Expression Lists in TorchScript\nDESCRIPTION: Specifies the syntax for expression lists and starred items in TorchScript. Starred items can only appear on the left side of assignments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nexpression_list ::=  expression (',' expression)* [',']\nstarred_item    ::=  '*' primary\n```\n\n----------------------------------------\n\nTITLE: Analyzing Convolution Operations in PyTorch\nDESCRIPTION: This snippet shows various convolution operations with different input and output tensor shapes, kernel sizes, strides, and padding. The operations are represented in a compact format, likely for performance analysis or optimization purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hrnet_w18_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 128, 56, 56], f16), T([128, 18, 56, 56], f16), T([128, 18, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Concatenating Tensors with ATen Cat Operator\nDESCRIPTION: Illustrates `aten.cat`, used to concatenate tensors along a specified dimension, typically for merging feature maps. Examples use tensors like [64, 512, 8, 8], demonstrating its utility in increasing feature space dimensionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.cat.default\ncnt: 1, (([T([64, 512, 8, 8], f16), T([64, 512, 8, 8], f16), T([64, 512, 8, 8], f16)], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Unbinding Tensors using ATen in Python\nDESCRIPTION: Demonstrates the ATen \\\"aten.unbind.int\\\" operator to unbind tensors along a specified dimension, effectively splitting a tensor into smaller ones. Focused on f16 tensors with varied strides. PyTorch with ATen support is required to execute these operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 4, ((T([3, 128, 6, 196, 64], f16, stride=(75264, 225792, 12544, 1, 196)),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 4, ((T([3, 128, 6, 49, 128], f16, stride=(37632, 112896, 6272, 1, 49)),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Sigmoid Operations\nDESCRIPTION: Sigmoid activation and its backward pass on tensors with varying channel dimensions (64, 128, 256)\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 1, 64], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Activation Function Operations (ReLU, SiLU, Sigmoid)\nDESCRIPTION: Implementation of various activation functions including ReLU, SiLU (Swish), and Sigmoid, with their corresponding backward passes. Operations use half precision tensors with different shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 8, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Threshold Backward Operations in PyTorch with Float16 Tensors\nDESCRIPTION: Records of threshold backward operations used in the backward pass of ReLU activations with threshold value 0. Each entry shows the gradient input tensor, output tensor (both with the same shape), and the threshold value.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 3, ((T([8, 2048, 7, 7], f16), T([8, 2048, 7, 7], f16), 0), {})\ncnt: 5, ((T([8, 1024, 7, 7], f16), T([8, 1024, 7, 7], f16), 0), {})\ncnt: 7, ((T([8, 1024, 14, 14], f16), T([8, 1024, 14, 14], f16), 0), {})\ncnt: 11, ((T([8, 512, 14, 14], f16), T([8, 512, 14, 14], f16), 0), {})\ncnt: 5, ((T([8, 512, 28, 28], f16), T([8, 512, 28, 28], f16), 0), {})\ncnt: 7, ((T([8, 256, 28, 28], f16), T([8, 256, 28, 28], f16), 0), {})\ncnt: 4, ((T([8, 256, 56, 56], f16), T([8, 256, 56, 56], f16), 0), {})\ncnt: 6, ((T([8, 128, 56, 56], f16), T([8, 128, 56, 56], f16), 0), {})\ncnt: 1, ((T([8, 64, 112, 112], f16), T([8, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Backend Configuration Setup\nDESCRIPTION: Configuration setup for backend including observation type and dtype configurations for LinearReLU module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nBackendConfig(nniqat.LinearReLU)\n    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)\n    .set_dtype_configs([\n        DTypeConfig(input_dtype=torch.quint8, output_dtype = torch.quint8, weight_dtype = torch.qint8, bias_dtype = torch.float32)]\n    )\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Named Tensor Operations in PyTorch\nDESCRIPTION: Shows how pointwise unary functions preserve tensor names from input to output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.randn(3, 3, names=('N', 'C'))\n>>> x.abs().names\n('N', 'C')\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Flag for Compressed Fatbinary in PyTorch\nDESCRIPTION: Adds a CUDA flag to compress all fatbinary files, which can reduce the size of the compiled CUDA code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nstring(APPEND CMAKE_CUDA_FLAGS \" -Xfatbin -compress-all\")\n```\n\n----------------------------------------\n\nTITLE: PyTorch Accelerator Module Functions Reference\nDESCRIPTION: This code snippet defines the autosummary section for torch.accelerator module functions. It lists all available functions for managing accelerator devices, including device querying, device selection, stream management, and synchronization capabilities.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/accelerator.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    device_count\n    is_available\n    current_accelerator\n    set_device_index\n    set_device_idx\n    current_device_index\n    current_device_idx\n    set_stream\n    current_stream\n    synchronize\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations\nDESCRIPTION: Collection of tensor operations including convolutions with different input/output channels and spatial dimensions. Operations use float16 (f16) data type and include parameters for stride, padding, and groups.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nT([128, 672, 1, 1], f16), T([128, 28, 1, 1], f16), T([672, 28, 1, 1], f16)\n```\n\n----------------------------------------\n\nTITLE: Executing PyTorch CI Workflow with Python\nDESCRIPTION: This Python snippet demonstrates how to execute a PyTorch CI workflow using specific job parameters, such as `binary_linux_manywheel_3_6m_cpu_devtoolset7_nightly_test`, and filters. It includes the option to commit changes with an explanatory message. Dependencies include Python and the proper permissions to execute the script. Inputs include job ID and filter patterns, with commits made as part of the CI process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\npython tools/testing/explicit_ci_jobs.py --job binary_linux_manywheel_3_6m_cpu_devtoolset7_nightly_test --filter-gha '*generated*gcc5.4*' --make-commit\n```\n\n----------------------------------------\n\nTITLE: Serialization of Complex Tensors\nDESCRIPTION: Demonstrates how to save and load complex tensors using torch.save and torch.load.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/complex_numbers.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntorch.save(y, 'complex_tensor.pt')\ntorch.load('complex_tensor.pt')\n```\n\n----------------------------------------\n\nTITLE: Basic CUDA Graph Capture and Replay in PyTorch\nDESCRIPTION: Demonstrates basic graph capture and replay using torch.cuda.graph context manager. Shows how to warm up the workload, capture operations, and replay the graph with new input data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\ng = torch.cuda.CUDAGraph()\n\n# Placeholder input used for capture\nstatic_input = torch.empty((5,), device=\"cuda\")\n\n# Warmup before capture\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for _ in range(3):\n        static_output = static_input * 2\ntorch.cuda.current_stream().wait_stream(s)\n\n# Captures the graph\nwith torch.cuda.graph(g):\n    static_output = static_input * 2\n\n# Fills the graph's input memory with new data to compute on\nstatic_input.copy_(torch.full((5,), 3, device=\"cuda\"))\ng.replay()\nprint(static_output)  # full of 3 * 2 = 6\n\nstatic_input.copy_(torch.full((5,), 4, device=\"cuda\"))\ng.replay()\nprint(static_output)  # full of 4 * 2 = 8\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Data Types with DTypeConfig\nDESCRIPTION: Demonstrates two approaches to specify data types using DTypeConfig: simple dtype configuration and configuration with constraints. Shows how to set quantization parameters and constraints for input, output, weight, and bias types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/backend_config/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.ao.quantization.backend import DTypeConfig, DTypeWithConstraints\n\ndtype_config = DTypeConfig(\n    input_dtype=torch.quint8,\n    output_dtype=torch.quint8,\n    weight_dtype=torch.qint8,\n    bias_dtype=torch.float)\n\ndtype_config_with_constraints = DTypeConfig(\n    input_dtype=DTypeWithConstraints(\n        dtype=torch.quint8,\n        quant_min_lower_bound=0,\n        quant_max_upper_bound=255,\n        scale_min_lower_bound=2 ** -12,\n    ),\n    output_dtype=DTypeWithConstraints(\n        dtype=torch.quint8,\n        quant_min_lower_bound=0,\n        quant_max_upper_bound=255,\n        scale_min_lower_bound=2 ** -12,\n    ),\n    weight_dtype=DTypeWithConstraints(\n        dtype=torch.qint8,\n        quant_min_lower_bound=-128,\n        quant_max_upper_bound=127,\n        scale_min_lower_bound=2 ** -12,\n    ),\n    bias_dtype=torch.float)\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations\nDESCRIPTION: Forward and backward batch normalization operations with running statistics and gradient computation for different tensor shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/maml_omniglot_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.native_batch_norm.default((T([5, 64, 26, 26], f16, stride=(43264, 1, 1664, 64)), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 1.0, 1e-05))\naten.native_batch_norm_backward.default((T([5, 64, 26, 26], f16, stride=(43264, 1, 1664, 64)), T([5, 64, 26, 26], f16, stride=(43264, 1, 1664, 64)), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]))\n```\n\n----------------------------------------\n\nTITLE: Querying Package Directory Structure\nDESCRIPTION: Shows how to use the has_file() method to check for the existence of a specific file within a torch package. This is useful for programmatically verifying package contents.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimporter_file_structure = importer.file_structure()\nfound: bool = importer_file_structure.has_file(\"package_a/subpackage.py\")\n```\n\n----------------------------------------\n\nTITLE: Handling Non-Valid Python Identifiers in Module Attributes\nDESCRIPTION: This code demonstrates the technique used to serialize module attributes with names that aren't valid Python identifiers (like numeric indices). It uses direct assignments to the __annotations__ dictionary rather than standard attribute syntax.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyModule(Module):\n    __annotations__ = []\n    __annotations__[\"0\"] = ASubmodule\n    __annotations__[\"1\"] = ASubmodule\n```\n\n----------------------------------------\n\nTITLE: Computing NLL Loss Backward Pass in PyTorch\nDESCRIPTION: This snippet shows the tensor shapes and parameters for the backward pass of the negative log-likelihood loss in a PyTorch model. It includes input gradients, predictions, and target labels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Installing Nightly PyTorch with Custom Conda Environment\nDESCRIPTION: Uses the tools/nightly.py script to check out a nightly PyTorch branch and install it into an existing conda environment specified with the --prefix argument.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./tools/nightly.py checkout -b my-nightly-branch -p my-env\nsource my-env/bin/activate  # or `& .\\my-env\\Scripts\\Activate.ps1` on Windows\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch Split With Sizes Operator - Python\nDESCRIPTION: This snippet lists input argument tuples for ATen split_with_sizes.default, testing splits of high-dimensional tensors with specific shapes, strides, and split sizes along the last dimension. Used for validating tensor partitioning logic in forward/backward computation graphs for large models. Inputs are tensors and split size lists. Requires only PyTorch as dependency.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([1024, 4, 144, 48], f16, stride=(27648, 144, 1, 576)), [16, 32], -1), {})\ncnt: 1, ((T([1024, 4, 144, 80], f16, stride=(46080, 144, 1, 576)), [16, 64], -1), {})\ncnt: 1, ((T([1024, 1, 144, 80], f16, stride=(11520, 144, 1, 144)), [16, 64], -1), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows batch normalization operations and their backward passes. It includes operations on 4D tensors with different channel sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 3, ((T([64, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\n\nOperator: aten.native_batch_norm_backward.default\ncnt: 3, ((T([64, 64, 112, 112], f16), T([64, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Checking Supported ISA Instructions using collect_env Script\nDESCRIPTION: Command to check supported Instruction Set Architecture (ISA) features on the machine by using PyTorch's collect_env script and filtering for AVX/AMX instructions using grep.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_best_practices_for_backends.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython collect_env.py | grep \"a[(v|m)]x\"\n```\n\n----------------------------------------\n\nTITLE: Print Compile Example\nDESCRIPTION: Demonstration of using print_compile to obtain and display the FX graph for debugging purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch.compile import aot_function\n\nfrom functorch.compile import print_compile\n# Or...\ndef print_compile(fx_g, _):\n    print(fx_g.code)\n    return fx_g\n\ndef foo(x):\n    return x.cos().cos()\ninp = torch.randn(3, requires_grad=True)\naot_function(foo, print_compile)(inp)\n```\n\n----------------------------------------\n\nTITLE: Element-wise Operations in PyTorch\nDESCRIPTION: Element-wise multiplication and division operations on tensors. These are common in various parts of neural networks, including activation functions and normalization layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 4, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16, stride=(64, 1, 0, 0))), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 2048, 8, 8], f16, stride=(2048, 1, 0, 0)), 64), {})\n```\n\n----------------------------------------\n\nTITLE: Importing and Using torch.__config__ Module in Python\nDESCRIPTION: This snippet demonstrates how to import and use the torch.__config__ module. It includes two main functions: 'show' for displaying PyTorch configuration and 'parallel_info' for obtaining parallel processing information.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/config_mod.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\n\n# Show PyTorch configuration\ntorch.__config__.show()\n\n# Get parallel processing information\nparallel_info = torch.__config__.parallel_info()\n```\n\n----------------------------------------\n\nTITLE: Comparing Relative Performance Using get_perf - Python\nDESCRIPTION: Applies the previously defined 'get_perf' function to compare performance of 'no_vmap_timer' vs 'with_vmap_timer' for Jacobian computation. Prints improvement percentage. Depends on prior Timer and get_perf definitions. No direct output values, just print side-effect.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nget_perf(no_vmap_timer, \"without vmap\",  with_vmap_timer, \"vmap\");\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Matrix Multiplication Additions in PyTorch\nDESCRIPTION: Summarizes the use of \\\"aten.addmm.default\\\" operator which performs matrix multiplication followed by addition, with details on tensor dimensions and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 48, ((T([1024], f16), T([1024, 1024], f16), T([1024, 1024], f16, stride=(1, 1024))), {})\ncnt: 12, ((T([4096], f16), T([1024, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})\ncnt: 12, ((T([1024], f16), T([1024, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Convolution Operations\nDESCRIPTION: This snippet demonstrates the usage of the aten.convolution.default operator for performing convolutions with various input shapes, kernel sizes, and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 2, ((T([3, 3, 518, 518], f16), T([64, 3, 7, 7], f16), T([64], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([3, 64, 512, 512], f16), T([128, 64, 3, 3], f16), T([128], f16), [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([3, 128, 256, 256], f16), T([256, 128, 3, 3], f16), T([256], f16), [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Clone Operation\nDESCRIPTION: Basic tensor clone operation for input data with shape [32, 3, 224, 224] in half precision format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([32, 3, 224, 224], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Up Conda and Build Tools Environment for PyTorch Source Build (Windows)\nDESCRIPTION: This Batch script outlines the steps to prepare the environment on Windows for building PyTorch from source. It includes creating and activating a Conda environment, and crucially, running the `vcvarsall.bat` script to configure the necessary environment variables for the Microsoft Visual C++ compiler, which is a prerequisite for the build process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ source <CONDA_INSTALL_DIR>\\Scripts\\activate.bat\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n$ call \"C:\\Program Files\\Microsoft Visual Studio\\<VERSION>\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64\n```\n\n----------------------------------------\n\nTITLE: Conditional Installation of Test Executable in CMake\nDESCRIPTION: Conditionally installs the `test_lite_interpreter_runtime` executable target to the `bin` destination directory if the `INSTALL_TEST` CMake option is enabled. Additionally, for MSVC builds using shared libraries (`MSVC` and `BUILD_SHARED_LIBS` are true), it installs the corresponding PDB debug symbol file. Requires the `test_lite_interpreter_runtime` target to exist.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lite_interpreter_runtime/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(INSTALL_TEST)\n  install(TARGETS test_lite_interpreter_runtime DESTINATION bin)\n  # Install PDB files for MSVC builds\n  if(MSVC AND BUILD_SHARED_LIBS)\n    install(\n      FILES $<TARGET_PDB_FILE:test_lite_interpreter_runtime>\n      DESTINATION bin OPTIONAL)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Project and Output Directories with CMake (CMake)\nDESCRIPTION: Initializes the CMake build system, setting project languages to C and C++, and customizes the output directories for static, shared libraries, and executables. These variables point to subfolders in the CMake binary directory, organizing build artifacts for later integration and easy cleanup. Requires CMake and a compatible toolchain. No external dependencies are necessary beyond the build essentials.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/inductor/cpp/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nproject(my-project LANGUAGES C CXX)\n\n# Build output setup\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/test/lib)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/test/lib)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/test/bin)\n```\n\n----------------------------------------\n\nTITLE: Enabling Fast Math for MPS Metal Kernels (Environment Variable)\nDESCRIPTION: Set `PYTORCH_MPS_FAST_MATH` to `1` to enable fast math optimizations for MPS metal kernels. This may affect numerical precision; consult section 1.6.3 of the Metal Shading Language Specification for details.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nPYTORCH_MPS_FAST_MATH\n```\n\n----------------------------------------\n\nTITLE: Defining a Function with Global State in torch.func (Discouraged Practice) - PyTorch - Python\nDESCRIPTION: This Python snippet demonstrates an incorrect pattern for users of torch.func transforms: the function 'f' assigns intermediate results to a global variable, which is not compatible with torch.func's execution model. The pattern showcases the risks of using mutation or side effects inside functions being transformed. It requires PyTorch and torch.func. The function takes a tensor 'x' as input and relies on global state for intermediate values, which can lead to unexpected results or errors under function transformations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.func import grad\n\n# Don't do this\nintermediate = None\n\ndef f(x):\n  global intermediate\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z\n\nx = torch.randn([])\ngrad_x = grad(f)(x)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with Dimensions in PyTorch\nDESCRIPTION: Implements matrix multiplication using dimension objects and tensor operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef mm(A, B):\n    i, j, k = dims(3)\n    r = (A[i, k] * B[k, j]).sum(k)\n    return r.order(i, j)\nmm(torch.rand(3, 4), torch.rand(4, 5)).shape\n```\n\n----------------------------------------\n\nTITLE: Performing Matrix-Vector Multiplication with CSR Tensor in PyTorch\nDESCRIPTION: Demonstrates matrix-vector multiplication operation with a CSR tensor using the matmul method. The example multiplies a previously defined CSR tensor with a random vector.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nvec = torch.randn(4, 1, dtype=torch.float64)\nsp.matmul(vec)\n```\n\n----------------------------------------\n\nTITLE: Documenting Sample Arguments for aten.upsample_bilinear2d_backward.vec Operator - PyTorch - Python\nDESCRIPTION: This snippet captures input and output tensor shapes for aten.upsample_bilinear2d_backward.vec, the backward pass for bilinear upsampling in PyTorch used for gradient computation. Each sample includes the grad_output tensor, optional placeholder (None), the input spatial size, the align_corners argument (True), and the scale factors. These samples are ideal for test coverage and behavioral checks during gradient calculations in f16 precision workflows.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_unet_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.upsample_bilinear2d_backward.vec\ncnt: 1, ((T([1, 64, 640, 958], f16), None, [1, 64, 320, 479], True, [2.0, 2.0]), {})\ncnt: 1, ((T([1, 128, 320, 478], f16), None, [1, 128, 160, 239], True, [2.0, 2.0]), {})\ncnt: 1, ((T([1, 256, 160, 238], f16), None, [1, 256, 80, 119], True, [2.0, 2.0]), {})\ncnt: 1, ((T([1, 512, 80, 118], f16), None, [1, 512, 40, 59], True, [2.0, 2.0]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Deep Learning Model\nDESCRIPTION: This code snippet shows the usage counts and input tensor shapes for various PyTorch operators in a deep learning model. It includes operations like convolutions, pooling, matrix multiplications, and element-wise operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.max.default\ncnt: 8, ((T([0, 182], b8), T([0, 182], b8)), {})\ncnt: 4, ((T([2], i64),), {})\n\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([4, 64, 592, 608], f16), [3, 3], [2, 2], [1, 1]), {})\ncnt: 1, ((T([4, 256, 37, 38], f16), [1, 1], [2, 2]), {})\n\nOperator: aten.min.default\ncnt: 4, ((T([2], i64),), {})\n\nOperator: aten.minimum.default\ncnt: 4, ((T([], f32), T([], f32)), {})\n\nOperator: aten.mm.default\ncnt: 1, ((T([0, 364], f16), T([364, 1024], f16)), {})\ncnt: 1, ((T([364, 0], f16), T([0, 1024], f16)), {})\ncnt: 1, ((T([0, 91], f16), T([91, 1024], f16)), {})\ncnt: 1, ((T([91, 0], f16), T([0, 1024], f16)), {})\ncnt: 1, ((T([0, 1024], f16), T([1024, 1024], f16)), {})\ncnt: 1, ((T([1024, 0], f16), T([0, 1024], f16)), {})\ncnt: 1, ((T([0, 1024], f16), T([1024, 12544], f16)), {})\ncnt: 1, ((T([1024, 0], f16), T([0, 12544], f16)), {})\n\nOperator: aten.mul.Tensor\ncnt: 4, ((T([], f32), 800.0), {})\ncnt: 4, ((T([], f32), 1333.0), {})\ncnt: 14, ((T([1, 64, 1, 1], f16), T([1, 64, 1, 1], f16)), {})\ncnt: 1, ((T([4, 64, 592, 608], f16), T([1, 64, 1, 1], f16)), {})\ncnt: 6, ((T([4, 64, 296, 304], f16), T([1, 64, 1, 1], f16)), {})\ncnt: 32, ((T([1, 256, 1, 1], f16), T([1, 256, 1, 1], f16)), {})\ncnt: 4, ((T([4, 256, 296, 304], f16), T([1, 256, 1, 1], f16)), {})\ncnt: 16, ((T([1, 128, 1, 1], f16), T([1, 128, 1, 1], f16)), {})\ncnt: 2, ((T([4, 128, 296, 304], f16), T([1, 128, 1, 1], f16)), {})\ncnt: 14, ((T([4, 128, 148, 152], f16), T([1, 128, 1, 1], f16)), {})\ncnt: 22, ((T([1, 512, 1, 1], f16), T([1, 512, 1, 1], f16)), {})\ncnt: 10, ((T([4, 512, 148, 152], f16), T([1, 512, 1, 1], f16)), {})\ncnt: 2, ((T([4, 256, 148, 152], f16), T([1, 256, 1, 1], f16)), {})\ncnt: 22, ((T([4, 256, 74, 76], f16), T([1, 256, 1, 1], f16)), {})\ncnt: 14, ((T([1, 1024, 1, 1], f16), T([1, 1024, 1, 1], f16)), {})\ncnt: 14, ((T([4, 1024, 74, 76], f16), T([1, 1024, 1, 1], f16)), {})\ncnt: 2, ((T([4, 512, 74, 76], f16), T([1, 512, 1, 1], f16)), {})\ncnt: 10, ((T([4, 512, 37, 38], f16), T([1, 512, 1, 1], f16)), {})\ncnt: 8, ((T([1, 2048, 1, 1], f16), T([1, 2048, 1, 1], f16)), {})\ncnt: 8, ((T([4, 2048, 37, 38], f16), T([1, 2048, 1, 1], f16)), {})\ncnt: 1, ((T([304], i32), T([], i64)), {})\ncnt: 1, ((T([296], i32), T([], i64)), {})\ncnt: 1, ((T([152], i32), T([], i64)), {})\ncnt: 1, ((T([148], i32), T([], i64)), {})\ncnt: 1, ((T([76], i32), T([], i64)), {})\ncnt: 1, ((T([74], i32), T([], i64)), {})\ncnt: 1, ((T([38], i32), T([], i64)), {})\ncnt: 1, ((T([37], i32), T([], i64)), {})\ncnt: 2, ((T([19], i32), T([], i64)), {})\ncnt: 2, ((T([1438452], f16), 0.5), {})\ncnt: 4, ((T([1438452, 1], f16), T([1438452, 1], f16)), {})\ncnt: 2, ((T([], f16), T([1438452, 1], f16)), {})\ncnt: 8, ((T([0], f32), T([0], f32)), {})\ncnt: 18, ((T([0], f16), 0.5), {})\ncnt: 8, ((T([0, 91], f16), T([0, 1], f16)), {})\ncnt: 2, ((T([], f16), T([0, 91], f16)), {})\ncnt: 32, ((T([0], f16), T([], f32)), {})\ncnt: 2, ((T([0, 91], f16), T([], f16)), {})\ncnt: 2, ((T([0, 91], f16), T([0, 91], f16)), {})\n\nOperator: aten.mul_.Tensor\ncnt: 8, ((T([0], f16), 1.0714285714285714), {})\n\nOperator: aten.neg.default\ncnt: 2, ((T([0, 91], f16),), {})\n\nOperator: aten.new_empty.default\ncnt: 1, ((T([0, 1, 30, 30], f16), [0, 1, 427, 640]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\ncnt: 1, ((T([0, 1, 30, 30], f16), [0, 1, 612, 612]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\ncnt: 1, ((T([0, 1, 30, 30], f16), [0, 1, 640, 443]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\ncnt: 1, ((T([0, 1, 30, 30], f16), [0, 1, 459, 640]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\n\nOperator: aten.new_full.default\ncnt: 1, ((T([3, 799, 1199], f16), [4, 3, 1184, 1216], 0), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\n\nOperator: aten.new_zeros.default\ncnt: 12, ((T([0], f16), [0]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 12, ((T([0, 4], f16), [0, 4]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\n\nOperator: aten.nonzero.default\ncnt: 4, ((T([5000], b8),), {})\ncnt: 20, ((T([0], b8),), {})\n\nOperator: aten.reciprocal.default\ncnt: 8, ((T([], f32),), {})\n\nOperator: aten.relu.default\ncnt: 2, ((T([0, 1024], f16),), {})\n\nOperator: aten.relu_.default\ncnt: 1, ((T([4, 64, 592, 608], f16),), {})\ncnt: 6, ((T([4, 64, 296, 304], f16),), {})\ncnt: 4, ((T([4, 256, 296, 304], f16),), {})\ncnt: 1, ((T([4, 128, 296, 304], f16),), {})\ncnt: 7, ((T([4, 128, 148, 152], f16),), {})\ncnt: 4, ((T([4, 512, 148, 152], f16),), {})\ncnt: 2, ((T([4, 256, 148, 152], f16),), {})\ncnt: 12, ((T([4, 256, 74, 76], f16),), {})\ncnt: 6, ((T([4, 1024, 74, 76], f16),), {})\ncnt: 1, ((T([4, 512, 74, 76], f16),), {})\ncnt: 5, ((T([4, 512, 37, 38], f16),), {})\ncnt: 3, ((T([4, 2048, 37, 38], f16),), {})\ncnt: 1, ((T([4, 256, 37, 38], f16),), {})\ncnt: 1, ((T([4, 256, 19, 19], f16),), {})\ncnt: 4, ((T([0, 256, 14, 14], f16),), {})\ncnt: 1, ((T([0, 256, 28, 28], f16),), {})\n\nOperator: aten.round.default\ncnt: 16, ((T([], f32),), {})\n\nOperator: aten.rsqrt.default\ncnt: 7, ((T([1, 64, 1, 1], f16),), {})\ncnt: 16, ((T([1, 256, 1, 1], f16),), {})\ncnt: 8, ((T([1, 128, 1, 1], f16),), {})\ncnt: 11, ((T([1, 512, 1, 1], f16),), {})\ncnt: 7, ((T([1, 1024, 1, 1], f16),), {})\ncnt: 4, ((T([1, 2048, 1, 1], f16),), {})\n\nOperator: aten.sigmoid.default\ncnt: 1, ((T([4, 5000], f16),), {})\ncnt: 1, ((T([0, 91, 28, 28], f16),), {})\n\nOperator: aten.slice_backward.default\ncnt: 4, ((T([0, 90], f16), [0, 91], 1, 1, 9223372036854775807, 1), {})\ncnt: 4, ((T([0, 91], f16), [0, 91], 0, 0, 9223372036854775807, 1), {})\ncnt: 4, ((T([0, 363], f16), [0, 364], 1, 1, 9223372036854775807, 1), {})\ncnt: 8, ((T([0, 364], f16), [0, 364], 0, 0, 9223372036854775807, 1), {})\ncnt: 4, ((T([0, 182], f16), [0, 364], 1, 1, 9223372036854775807, 2), {})\ncnt: 4, ((T([0, 182], f16), [0, 364], 1, 0, 9223372036854775807, 2), {})\ncnt: 1, ((T([0, 91], f16), [0, 364], 1, 3, 9223372036854775807, 4), {})\ncnt: 1, ((T([0, 91], f16), [0, 364], 1, 2, 9223372036854775807, 4), {})\ncnt: 1, ((T([0, 91], f16), [0, 364], 1, 1, 9223372036854775807, 4), {})\ncnt: 1, ((T([0, 91], f16), [0, 364], 1, 0, 9223372036854775807, 4), {})\n\nOperator: aten.split_with_sizes.default\ncnt: 1, ((T([4, 359613], f16), [269952, 67488, 16872, 4218, 1083], 1), {})\ncnt: 1, ((T([0, 364], f16), [0, 0, 0, 0]), {})\ncnt: 1, ((T([0, 91], f16), [0, 0, 0, 0]), {})\ncnt: 1, ((T([0, 1, 28, 28], f16), [0, 0, 0, 0]), {})\n\nOperator: aten.sqrt.default\ncnt: 2, ((T([0], f32),), {})\n\nOperator: aten.stack.default\ncnt: 1, (([T([89984], i32), T([89984], i32), T([89984], i32), T([89984], i32)], 1), {})\ncnt: 1, (([T([22496], i32), T([22496], i32), T([22496], i32), T([22496], i32)], 1), {})\ncnt: 1, (([T([5624], i32), T([5624], i32), T([5624], i32), T([5624], i32)], 1), {})\ncnt: 1, (([T([1406], i32), T([1406], i32), T([1406], i32), T([1406], i32)], 1), {})\ncnt: 1, (([T([361], i32), T([361], i32), T([361], i32), T([361], i32)], 1), {})\ncnt: 1, (([T([1438452, 1], f16), T([1438452, 1], f16), T([1438452, 1], f16), T([1438452, 1], f16)], 2), {})\ncnt: 4, (([T([5000, 2], f16), T([5000, 2], f16)], 2), {})\ncnt: 1, (([T([0, 91], f16), T([0, 91], f16), T([0, 91], f16), T([0, 91], f16)], 2), {})\ncnt: 4, (([T([0, 182], f16), T([0, 182], f16)], 2), {})\ncnt: 8, (([T([0], f16), T([0], f16), T([0], f16), T([0], f16)], 1), {})\n\nOperator: aten.sub.Tensor\ncnt: 1, ((T([3, 427, 640], f16, stride=(1, 1920, 3)), T([3, 1, 1], f16)), {})\ncnt: 1, ((T([3, 612, 612], f16, stride=(1, 1836, 3)), T([3, 1, 1], f16)), {})\ncnt: 1, ((T([3, 640, 443], f16, stride=(1, 1329, 3)), T([3, 1, 1], f16)), {})\ncnt: 1, ((T([3, 459, 640], f16, stride=(1, 1920, 3)), T([3, 1, 1], f16)), {})\ncnt: 7, ((T([1, 64, 1, 1], f16), T([1, 64, 1, 1], f16)), {})\ncnt: 16, ((T([1, 256, 1, 1], f16), T([1, 256, 1, 1], f16)), {})\ncnt: 8, ((T([1, 128, 1, 1], f16), T([1, 128, 1, 1], f16)), {})\ncnt: 11, ((T([1, 512, 1, 1], f16), T([1, 512, 1, 1], f16)), {})\ncnt: 7, ((T([1, 1024, 1, 1], f16), T([1, 1024, 1, 1], f16)), {})\ncnt: 4, ((T([1, 2048, 1, 1], f16), T([1, 2048, 1, 1], f16)), {})\ncnt: 2, ((T([1438452], f16, stride=(4,)), T([1438452], f16, stride=(4,))), {})\ncnt: 2, ((T([1438452, 1], f16), T([1438452, 1], f16)), {})\ncnt: 8, ((T([5000], f16, stride=(4,)), T([5000], f16, stride=(4,))), {})\ncnt: 16, ((T([0], f32), T([0], f32)), {})\ncnt: 2, ((T([0], i64), 2), {})\ncnt: 26, ((T([0], f16), T([0], f16)), {})\ncnt: 2, ((T([0, 91], f16), T([0, 91], f16)), {})\n\nOperator: aten.sum.SymInt\ncnt: 1, ((T([0, 364], f16), [0], True), {})\ncnt: 1, ((T([0, 91], f16), [0], True), {})\ncnt: 2, ((T([0, 1024], f16), [0], True), {})\n\nOperator: aten.sum.default\ncnt: 4, ((T([0, 4], f16),), {})\ncnt: 4, ((T([0], i64),), {})\ncnt: 4, ((T([0], f16),), {})\ncnt: 1, ((T([0, 1, 427, 640], f16),), {})\ncnt: 1, ((T([0, 1, 612, 612], f16),), {})\ncnt: 1, ((T([0, 1, 640, 443], f16),), {})\ncnt: 1, ((T([0, 1, 459, 640], f16),), {})\n\nOperator: aten.threshold_backward.default\ncnt: 2, ((T([0, 1024], f16), T([0, 1024], f16), 0), {})\ncnt: 3, ((T([4, 2048, 37, 38], f16), T([4, 2048, 37, 38], f16), 0), {})\ncnt: 5, ((T([4, 512, 37, 38], f16), T([4, 512, 37, 38], f16), 0), {})\ncnt: 1, ((T([4, 512, 74, 76], f16), T([4, 512, 74, 76], f16), 0), {})\ncnt: 6, ((T([4, 1024, 74, 76], f16), T([4, 1024, 74, 76], f16), 0), {})\ncnt: 11, ((T([4, 256, 74, 76], f16), T([4, 256, 74, 76], f16), 0), {})\ncnt: 1, ((T([4, 256, 148, 152], f16), T([4, 256, 148, 152], f16), 0), {})\ncnt: 4, ((T([4, 512, 148, 152], f16), T([4, 512, 148, 152], f16), 0), {})\ncnt: 7, ((T([4, 128, 148, 152], f16), T([4, 128, 148, 152], f16), 0), {})\ncnt: 1, ((T([4, 128, 296, 304], f16), T([4, 128, 296, 304], f16), 0), {})\n\nOperator: aten.topk.default\ncnt: 1, ((T([4, 269952], f16, stride=(359613, 1)), 1000, 1), {})\ncnt: 1, ((T([4, 67488], f16, stride=(359613, 1)), 1000, 1), {})\ncnt: 1, ((T([4, 16872], f16, stride=(359613, 1)), 1000, 1), {})\ncnt: 1, ((T([4, 4218], f16, stride=(359613, 1)), 1000, 1), {})\ncnt: 1, ((T([4, 1083], f16, stride=(359613, 1)), 1000, 1), {})\n\nOperator: aten.unbind.int\n```\n\n----------------------------------------\n\nTITLE: Apply aten.nll_loss_forward in Python\nDESCRIPTION: Utilizes the negative log-likelihood loss function in PyTorch for forward operations, often applicable in classification tasks. Essential dependencies include the PyTorch framework. It handles a softmax probability output against labels, providing a single loss scalar value. This operation is typical in training neural networks using cross-entropy loss.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Patching Code into a Package\nDESCRIPTION: Demonstrates how to use PackageExporter's save_source_string() method to save arbitrary Python source code to a module in the package, allowing for code patching and overriding.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nwith PackageExporter(f) as exporter:\n    # Save the my_module.foo available in your current Python environment.\n    exporter.save_module(\"my_module.foo\")\n\n    # This saves the provided string to my_module/foo.py in the package archive.\n    # It will override the my_module.foo that was previously saved.\n    exporter.save_source_string(\"my_module.foo\", textwrap.dedent(\n        \"\"\"\n        def my_function():\n            print('hello world')\n        \"\"\"\n    ))\n\n    # If you want to treat my_module.bar as a package\n    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)\n    # pass is_package=True,\n    exporter.save_source_string(\"my_module.bar\",\n                                \"def foo(): print('hello')\\n\",\n                                is_package=True)\n\nimporter = PackageImporter(f)\nimporter.import_module(\"my_module.foo\").my_function()  # prints 'hello world'\n```\n\n----------------------------------------\n\nTITLE: FileCheck Test Example for CSE Optimization\nDESCRIPTION: Demonstrates a test case using FileCheck to verify that Common Subexpression Elimination (CSE) removes redundant aten::mul operations. Shows how to write test assertions using CHECK pragmas.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/HowToWriteTestsUsingFileCheck.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef test_cse():\n    input_str = \"\"\"graph(%a : Tensor, %b : Tensor):\n      # CHECK: aten::mul\n      %x : Tensor = aten::mul(%a, %b)\n      # Check that the second aten::mul is removed by CSE.\n      # CHECK-NOT: aten::mul\n      %y : Tensor = aten::mul(%a, %b)\n      # CHECK: return\n      return (%x, %y)\n      \"\"\"\n    parsed = parse_ir(input_str)\n    optimized = run_cse(parsed)\n    FileCheck().run(input_str, optimized)\n```\n\n----------------------------------------\n\nTITLE: Using comptime breakpoint in Python with torch.compile\nDESCRIPTION: This snippet demonstrates how to use a comptime breakpoint within a function decorated with torch.compile. It allows for inspecting Dynamo state at a specific location in the user code being traced.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._dynamo.comptime import comptime\n\n@torch.compile\ndef f(...):\n    ...\n    comptime.breakpoint()\n    ...\n```\n\n----------------------------------------\n\nTITLE: Comparing HVP Implementations in PyTorch\nDESCRIPTION: Verifies that both HVP implementations (forward-reverse and reverse-reverse) produce the same result by comparing their outputs on the same inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nresult_hvp_revrev = hvp_revrev(f, (x,), (tangent,))\nassert torch.allclose(result, result_hvp_revrev[0])\n```\n\n----------------------------------------\n\nTITLE: Fused Linear-ReLU Module After Fusion Step\nDESCRIPTION: This code snippet shows the result of the fusion step, where the Linear and ReLU modules are combined into a single LinearReLU module. It demonstrates how the graph structure is simplified through fusion.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfused: GraphModule(\n  (linear): LinearReLU(\n    (0): Linear(in_features=5, out_features=10, bias=True)\n    (1): ReLU()\n  )\n)\n\ndef forward(self, x):\n    linear = self.linear(x);  x = None\n    return linear\n```\n\n----------------------------------------\n\nTITLE: Performing Matrix Multiplications using addmm\nDESCRIPTION: The operator aten.addmm.default is utilized for efficient matrix multiplications followed by addition, relying on proper tensor dimensions and PyTorch. It allows for matrix operations on tensors in common deep learning architectures generating output matrices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 72, ((T([768], f16), T([1024, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 12, ((T([3072], f16), T([1024, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 12, ((T([768], f16), T([1024, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization in Transformer Blocks\nDESCRIPTION: Shows layer normalization operations in transformer blocks for stabilizing network activations. These normalize tensors along the embedding dimension (768) with learned scale and bias parameters, using epsilon 1e-06 for numerical stability.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_layer_norm.default\ncnt: 72, ((T([2, 576, 768], f16, stride=(442368, 1, 576)), [768], T([768], f16), T([768], f16), 1e-06), {})\ncnt: 3, ((T([2, 577, 768], f16), [768], T([768], f16), T([768], f16), 1e-06), {})\ncnt: 2, ((T([2, 1, 768], f16), [768], T([768], f16), T([768], f16), 1e-06), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 3, ((T([2, 577, 768], f16), T([2, 577, 768], f16), [768], T([2, 577, 1], f32), T([2, 577, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\ncnt: 2, ((T([2, 1, 768], f16), T([2, 1, 768], f16), [768], T([2, 1, 1], f32), T([2, 1, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\ncnt: 72, ((T([2, 576, 768], f16), T([2, 576, 768], f16, stride=(442368, 1, 576)), [768], T([2, 576, 1], f32), T([2, 576, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Custom EMA Implementation using multi_avg_fn\nDESCRIPTION: Example showing how to implement EMA using the more efficient multi_avg_fn parameter.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9))\n```\n\n----------------------------------------\n\nTITLE: Creating EMA Model in PyTorch\nDESCRIPTION: Example showing how to create an Exponential Moving Average (EMA) model with a specified decay rate.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndecay = 0.999\naveraged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen AddMM Operations in PyTorch\nDESCRIPTION: This piece enumerates the use of the ATen addmm operation, which combines matrix multiplication with matrix addition in PyTorch. It involves performing operations between batched matrices, reflecting common practices in neural network layers, particularly in manipulating weights and activations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 25, ((T([768], f16), T([2048, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 6, ((T([3072], f16), T([2048, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 6, ((T([768], f16), T([2048, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 1, ((T([30522], f16), T([2048, 768], f16), T([768, 30522], f16, stride=(1, 768))), {})\n```\n\n----------------------------------------\n\nTITLE: Node Target Dtype Configuration Example\nDESCRIPTION: Example configuration showing target dtype mapping for nodes in the quantization graph, including input, qat_linear_relu, and output nodes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# node_name_to_target_dtype_info =\n# {\n#     # this is placeholder node in FX Graph\n#     \"input\" : {\"input_activation\": torch.float32, \"output_activation\": torch.float32},\n#     \"qat_linear_relu\": {\"input_activation\": torch.quint8, \"output_activation\": torch.quint8, \"weight\": ...}\n#     # this is the return node in FX Graph\n#     \"output\": {\"input_activation\": torch.float32, \"output_activation\": torch.float32}\n# }\n```\n\n----------------------------------------\n\nTITLE: Implementing NCCL Memory Allocator in C++\nDESCRIPTION: C++ source code for implementing the NCCL memory allocator functions that will be loaded as a shared library for use with CUDAPluggableAllocator in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_24\n\nLANGUAGE: cpp\nCODE:\n```\n#include <nccl.h>\n#include <iostream>\nextern \"C\" {\n\nvoid* nccl_alloc_plug(size_t size, int device, void* stream) {\n  std::cout << \"Using ncclMemAlloc\" << std::endl;\n  void* ptr;\n  ncclResult_t err = ncclMemAlloc(&ptr, size);\n  return ptr;\n\n}\n\nvoid nccl_free_plug(void* ptr, size_t size, int device, void* stream) {\n  std::cout << \"Using ncclMemFree\" << std::endl;\n  ncclResult_t err = ncclMemFree(ptr);\n}\n\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA and ROCm Settings in CMake\nDESCRIPTION: This snippet preserves CUDA and ROCm architecture flags for compilation. It sets source file properties with the appropriate architecture flags for CUDA and ROCm builds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDA)\n  torch_cuda_get_nvcc_gencode_flag(_ARCH_FLAGS)\n  set_source_files_properties(${TORCH_SRC_DIR}/csrc/cuda/Module.cpp PROPERTIES COMPILE_FLAGS \"-DCUDA_ARCH_FLAGS=\\\"${_ARCH_FLAGS_readable}\\\"\")\nendif()\n\nif(USE_ROCM)\n  string(REPLACE \";\" \" \" PYTORCH_ROCM_ARCH_readable \"${PYTORCH_ROCM_ARCH}\")\n  set_source_files_properties(${TORCH_SRC_DIR}/csrc/cuda/Module.cpp PROPERTIES COMPILE_FLAGS \"-DCUDA_ARCH_FLAGS=\\\"${PYTORCH_ROCM_ARCH_readable}\\\"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Importing torch.signal module in reStructuredText documentation\nDESCRIPTION: Documentation directives to import and set the torch.signal module as the current module for the documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/signal.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: torch.signal\n.. currentmodule:: torch.signal\n```\n\n----------------------------------------\n\nTITLE: Average Pooling Operations in PyTorch\nDESCRIPTION: This snippet shows average pooling operation used to downsample feature maps. The operation is performed on 4x4 spatial dimensions with a kernel size of 4x4, effectively reducing spatial dimensions to 1x1.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([96, 512, 4, 4], f16), [4, 4]), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Count for aten.convolution_backward.default Operations\nDESCRIPTION: Lists convolution gradient operations used during backpropagation, with various tensor shapes and parameters. These operations calculate gradients for input tensors, weight tensors, and sometimes bias tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 1984, 1, 1], f16), T([128, 1344, 1, 1], f16), T([1984, 1344, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1344, 7, 7], f16), T([128, 224, 7, 7], f16), T([1344, 224, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 224, 7, 7], f16), T([128, 1104, 7, 7], f16), T([224, 1104, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1104, 1, 1], f16), T([128, 48, 1, 1], f16), T([1104, 48, 1, 1], f16), [1104], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 48, 1, 1], f16), T([128, 1104, 1, 1], f16), T([48, 1104, 1, 1], f16), [48], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16), T([1104, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 1104, [True, True, False]), {})\ncnt: 1, ((T([128, 1104, 7, 7], f16), T([128, 184, 7, 7], f16), T([1104, 184, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 5, ((T([128, 184, 7, 7], f16), T([128, 736, 7, 7], f16), T([184, 736, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 5, ((T([128, 736, 1, 1], f16), T([128, 48, 1, 1], f16), T([736, 48, 1, 1], f16), [736], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 5, ((T([128, 48, 1, 1], f16), T([128, 736, 1, 1], f16), T([48, 736, 1, 1], f16), [48], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 5, ((T([128, 736, 7, 7], f16), T([128, 736, 7, 7], f16), T([736, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 736, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Backward Pass Operations in PyTorch Neural Network\nDESCRIPTION: Summary of convolution backward operations (aten.convolution_backward.default) in the model, showing counts, tensor shapes, and gradient flow parameters. These represent the backward pass computations during model training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([32, 2240, 7, 7], f16), T([32, 896, 14, 14], f16), T([2240, 896, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 2240, 7, 7], f16), T([32, 2240, 7, 7], f16), T([2240, 2240, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 2240, 1, 1], f16), T([32, 224, 1, 1], f16), T([2240, 224, 1, 1], f16), [2240], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 224, 1, 1], f16), T([32, 2240, 1, 1], f16), T([224, 2240, 1, 1], f16), [224], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 2240, 7, 7], f16), T([32, 2240, 14, 14], f16), T([2240, 112, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 20, [True, True, False]), {})\ncnt: 1, ((T([32, 2240, 14, 14], f16), T([32, 896, 14, 14], f16), T([2240, 896, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 21, ((T([32, 896, 14, 14], f16), T([32, 896, 14, 14], f16), T([896, 896, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 10, ((T([32, 896, 1, 1], f16), T([32, 224, 1, 1], f16), T([896, 224, 1, 1], f16), [896], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 10, ((T([32, 224, 1, 1], f16), T([32, 896, 1, 1], f16), T([224, 896, 1, 1], f16), [224], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 10, ((T([32, 896, 14, 14], f16), T([32, 896, 14, 14], f16), T([896, 112, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 8, [True, True, False]), {})\ncnt: 1, ((T([32, 896, 14, 14], f16), T([32, 448, 28, 28], f16), T([896, 448, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 896, 1, 1], f16), T([32, 112, 1, 1], f16), T([896, 112, 1, 1], f16), [896], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 112, 1, 1], f16), T([32, 896, 1, 1], f16), T([112, 896, 1, 1], f16), [112], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 896, 14, 14], f16), T([32, 896, 28, 28], f16), T([896, 112, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 8, [True, True, False]), {})\ncnt: 1, ((T([32, 896, 28, 28], f16), T([32, 448, 28, 28], f16), T([896, 448, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 9, ((T([32, 448, 28, 28], f16), T([32, 448, 28, 28], f16), T([448, 448, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([32, 448, 1, 1], f16), T([32, 112, 1, 1], f16), T([448, 112, 1, 1], f16), [448], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 4, ((T([32, 112, 1, 1], f16), T([32, 448, 1, 1], f16), T([112, 448, 1, 1], f16), [112], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 4, ((T([32, 448, 28, 28], f16), T([32, 448, 28, 28], f16), T([448, 112, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 4, [True, True, False]), {})\ncnt: 1, ((T([32, 448, 28, 28], f16), T([32, 224, 56, 56], f16), T([448, 224, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 448, 1, 1], f16), T([32, 56, 1, 1], f16), T([448, 56, 1, 1], f16), [448], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 56, 1, 1], f16), T([32, 448, 1, 1], f16), T([56, 448, 1, 1], f16), [56], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 448, 28, 28], f16), T([32, 448, 56, 56], f16), T([448, 112, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 4, [True, True, False]), {})\ncnt: 1, ((T([32, 448, 56, 56], f16), T([32, 224, 56, 56], f16), T([448, 224, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([32, 224, 56, 56], f16), T([32, 224, 56, 56], f16), T([224, 224, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 224, 1, 1], f16), T([32, 56, 1, 1], f16), T([224, 56, 1, 1], f16), [224], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 56, 1, 1], f16), T([32, 224, 1, 1], f16), T([56, 224, 1, 1], f16), [56], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 224, 56, 56], f16), T([32, 224, 56, 56], f16), T([224, 112, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})\ncnt: 1, ((T([32, 224, 56, 56], f16), T([32, 32, 112, 112], f16), T([224, 32, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 224, 1, 1], f16), T([32, 8, 1, 1], f16), T([224, 8, 1, 1], f16), [224], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 8, 1, 1], f16), T([32, 224, 1, 1], f16), T([8, 224, 1, 1], f16), [8], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 224, 56, 56], f16), T([32, 224, 112, 112], f16), T([224, 112, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Nearest Neighbor Upsampling Backward Pass in PyTorch\nDESCRIPTION: This snippet computes the gradient for nearest neighbor upsampling during the backward pass. It's essential for training neural networks that use upsampling operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 128, 48, 64], f16, stride=(1179648, 3072, 64, 1)), None, [8, 128, 24, 32], [2.0, 2.0]), {})\ncnt: 1, ((T([8, 256, 24, 32], f16, stride=(589824, 768, 32, 1)), None, [8, 256, 12, 16], [2.0, 2.0]), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Convolution Operations in PyTorch\nDESCRIPTION: This snippet shows the tensor shapes and parameters for various convolution operations in the model. It includes different stride and padding configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 2048, 7, 7], f16), T([128, 1024, 14, 14], f16), T([2048, 1024, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 7, ((T([128, 112, 7, 7], f16), T([128, 112, 14, 14], f16, stride=(175616, 196, 14, 1)), T([112, 112, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 896, 14, 14], f16), T([128, 1024, 14, 14], f16), T([896, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Creating Hybrid COO Sparse Tensor in PyTorch\nDESCRIPTION: Creates a 3D Hybrid COO tensor with 2 sparse and 1 dense dimension, showing how rows with any non-zero elements are stored entirely.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nt = torch.tensor([[[0., 0], [1., 2.]], [[0., 0], [3., 4.]]])\nt.to_sparse(sparse_dim=2)\n```\n\n----------------------------------------\n\nTITLE: Using GLOBAL Opcode in Pickle Format\nDESCRIPTION: Example of a GLOBAL opcode in pickle format that is used to identify the implementation of an object's type during the dependency resolution process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nGLOBAL 'torchvision.models.resnet Resnet`\n```\n\n----------------------------------------\n\nTITLE: Migrating to PyTorch 1.2 Recursive Scripting API Example\nDESCRIPTION: Demonstrates how to convert a standard nn.Module to a ScriptModule using the PyTorch 1.2 recursive scripting API with torch.jit.script().\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nmy_model = Model()\nmy_scripted_model = torch.jit.script(my_model)\n```\n\n----------------------------------------\n\nTITLE: Reading PyTorch Version from File\nDESCRIPTION: Reads the default PyTorch version from a version.txt file for use in generated libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\n# ---[ Version numbers for generated libraries\nfile(READ version.txt TORCH_DEFAULT_VERSION)\n```\n\n----------------------------------------\n\nTITLE: Listing MPS Event API Entries - reStructuredText\nDESCRIPTION: This snippet defines the summary and documentation generation for the MPS Event API under torch.mps. It uses Sphinx to structure the event utilities for Metal and GPU synchronization in PyTorch. Requires Sphinx configuration and an importable torch.mps.event module. Outputs Sphinx-generated event API documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\nMPS Event\n------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    event.Event\n```\n\n----------------------------------------\n\nTITLE: Initializing PostTrainingDataSparsity Callback in Python\nDESCRIPTION: Example of creating a PostTrainingDataSparsity callback that sparsifies model parameters after training. It configures a DataNormSparsifier with specific sparsity levels and block parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/lightning/callbacks/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom data_sparsity import PostTrainingDataSparsity\nsparsifier_args = {\n    'sparsity_level': 0.5,\n    'sparse_block_shape': (1, 4),\n    'zeros_per_block': 4\n}\npt_callback = PostTrainingDataSparsity(data_sparsifier_class=DataNormSparsifier, data_sparsifier_args=sparsifier_args)\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in a Vision Transformer Model\nDESCRIPTION: This snippet provides a detailed breakdown of PyTorch operator usage, showing the count (cnt) of each operator call along with the tensor shapes, data types (primarily f16), and parameters used. The data appears to be from a vision transformer architecture with attention mechanisms, convolutions, and other common deep learning operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 3, ((T([64, 4, 401, 401], f16), -1, False), {})\ncnt: 9, ((T([64, 4, 197, 197], f16), -1, False), {})\ncnt: 3, ((T([64, 4, 1, 197], f16), -1, False), {})\ncnt: 3, ((T([64, 4, 1, 401], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 3, ((T([64, 4, 1, 401], f16), T([64, 4, 1, 401], f16), -1, f16), {})\ncnt: 3, ((T([64, 4, 1, 197], f16), T([64, 4, 1, 197], f16), -1, f16), {})\ncnt: 9, ((T([64, 4, 197, 197], f16), T([64, 4, 197, 197], f16), -1, f16), {})\ncnt: 3, ((T([64, 4, 401, 401], f16), T([64, 4, 401, 401], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([64, 4, 401, 32], f16), [256, 401, 32]), {})\ncnt: 6, ((T([64, 4, 32, 401], f16), [256, 32, 401]), {})\ncnt: 3, ((T([256, 401, 401], f16), [64, 4, 401, 401]), {})\ncnt: 3, ((T([256, 401, 32], f16), [64, 4, 401, 32]), {})\ncnt: 6, ((T([64, 401, 4, 32], f16), [64, 401, 128]), {})\ncnt: 30, ((T([64, 4, 197, 64], f16), [256, 197, 64]), {})\ncnt: 12, ((T([64, 4, 64, 197], f16), [256, 64, 197]), {})\ncnt: 9, ((T([256, 197, 197], f16), [64, 4, 197, 197]), {})\ncnt: 9, ((T([256, 197, 64], f16), [64, 4, 197, 64]), {})\ncnt: 12, ((T([64, 197, 4, 64], f16), [64, 197, 256]), {})\ncnt: 3, ((T([64, 256], f16), [64, 1, 256]), {})\ncnt: 3, ((T([256, 1, 197], f16), [64, 4, 1, 197]), {})\ncnt: 3, ((T([256, 1, 64], f16), [64, 4, 1, 64]), {})\ncnt: 3, ((T([64, 128], f16), [64, 1, 128]), {})\ncnt: 3, ((T([256, 1, 401], f16), [64, 4, 1, 401]), {})\ncnt: 3, ((T([256, 1, 32], f16), [64, 4, 1, 32]), {})\ncnt: 3, ((T([64, 401, 128], f16), [25664, 128]), {})\ncnt: 3, ((T([64, 197, 256], f16), [12608, 256]), {})\ncnt: 9, ((T([64, 197, 3, 4, 64], f16), [64, 197, 768]), {})\ncnt: 3, ((T([64, 401, 3, 4, 32], f16), [64, 401, 384]), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([64, 401, 128], f16), T([1, 401, 128], f16)), {})\ncnt: 1, ((T([64, 197, 256], f16), T([1, 197, 256], f16)), {})\ncnt: 27, ((T([64, 401, 128], f16), T([64, 401, 128], f16)), {})\ncnt: 51, ((T([64, 197, 256], f16), T([64, 197, 256], f16)), {})\ncnt: 3, ((T([64, 1, 256], f16), T([256], f16)), {})\ncnt: 3, ((T([64, 1, 256], f16, stride=(50432, 256, 1)), T([64, 1, 256], f16)), {})\ncnt: 3, ((T([64, 1, 128], f16), T([128], f16)), {})\ncnt: 3, ((T([64, 1, 128], f16, stride=(51328, 128, 1)), T([64, 1, 128], f16)), {})\nOperator: aten.addmm.default\ncnt: 6, ((T([384], f16), T([25664, 128], f16), T([128, 384], f16, stride=(1, 128))), {})\ncnt: 9, ((T([128], f16), T([25664, 128], f16), T([128, 128], f16, stride=(1, 128))), {})\ncnt: 3, ((T([128], f16), T([25664, 384], f16), T([384, 128], f16, stride=(1, 384))), {})\ncnt: 18, ((T([768], f16), T([12608, 256], f16), T([256, 768], f16, stride=(1, 256))), {})\ncnt: 15, ((T([256], f16), T([12608, 256], f16), T([256, 256], f16, stride=(1, 256))), {})\ncnt: 9, ((T([256], f16), T([12608, 768], f16), T([768, 256], f16, stride=(1, 768))), {})\ncnt: 6, ((T([256], f16), T([64, 128], f16), T([128, 256], f16, stride=(1, 128))), {})\ncnt: 6, ((T([128], f16), T([64, 256], f16), T([256, 128], f16, stride=(1, 256))), {})\ncnt: 3, ((T([256], f16), T([64, 256], f16), T([256, 256], f16, stride=(1, 256))), {})\ncnt: 3, ((T([128], f16), T([64, 128], f16), T([128, 128], f16, stride=(1, 128))), {})\ncnt: 1, ((T([1000], f16), T([64, 128], f16, stride=(51328, 1)), T([128, 1000], f16, stride=(1, 128))), {})\ncnt: 1, ((T([1000], f16), T([64, 256], f16, stride=(50432, 1)), T([256, 1000], f16, stride=(1, 256))), {})\nOperator: aten.bmm.default\ncnt: 3, ((T([256, 401, 32], f16), T([256, 32, 401], f16)), {})\ncnt: 3, ((T([256, 401, 401], f16), T([256, 401, 32], f16)), {})\ncnt: 9, ((T([256, 197, 64], f16), T([256, 64, 197], f16)), {})\ncnt: 9, ((T([256, 197, 197], f16), T([256, 197, 64], f16)), {})\ncnt: 3, ((T([256, 1, 64], f16), T([256, 64, 197], f16)), {})\ncnt: 3, ((T([256, 1, 197], f16), T([256, 197, 64], f16)), {})\ncnt: 3, ((T([256, 1, 32], f16), T([256, 32, 401], f16)), {})\ncnt: 3, ((T([256, 1, 401], f16), T([256, 401, 32], f16)), {})\ncnt: 3, ((T([256, 401, 1], f16), T([256, 1, 32], f16)), {})\ncnt: 3, ((T([256, 1, 32], f16), T([256, 32, 401], f16, stride=(12832, 1, 32))), {})\ncnt: 3, ((T([256, 32, 1], f16), T([256, 1, 401], f16)), {})\ncnt: 3, ((T([256, 1, 401], f16), T([256, 401, 32], f16, stride=(12832, 1, 401))), {})\ncnt: 3, ((T([256, 197, 1], f16), T([256, 1, 64], f16)), {})\ncnt: 3, ((T([256, 1, 64], f16), T([256, 64, 197], f16, stride=(12608, 1, 64))), {})\ncnt: 3, ((T([256, 64, 1], f16), T([256, 1, 197], f16)), {})\ncnt: 3, ((T([256, 1, 197], f16), T([256, 197, 64], f16, stride=(12608, 1, 197))), {})\ncnt: 9, ((T([256, 197, 197], f16, stride=(38809, 1, 197)), T([256, 197, 64], f16)), {})\ncnt: 9, ((T([256, 197, 64], f16), T([256, 64, 197], f16, stride=(12608, 1, 64))), {})\ncnt: 9, ((T([256, 64, 197], f16, stride=(12608, 1, 64)), T([256, 197, 197], f16)), {})\ncnt: 9, ((T([256, 197, 197], f16), T([256, 197, 64], f16, stride=(12608, 1, 197))), {})\ncnt: 3, ((T([256, 401, 401], f16, stride=(160801, 1, 401)), T([256, 401, 32], f16)), {})\ncnt: 3, ((T([256, 401, 32], f16), T([256, 32, 401], f16, stride=(12832, 1, 32))), {})\ncnt: 3, ((T([256, 32, 401], f16, stride=(12832, 1, 32)), T([256, 401, 401], f16)), {})\ncnt: 3, ((T([256, 401, 401], f16), T([256, 401, 32], f16, stride=(12832, 1, 401))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([64, 1, 128], f16, stride=(0, 128, 1)), T([64, 400, 128], f16, stride=(51200, 1, 400))], 1), {})\ncnt: 1, (([T([64, 1, 256], f16, stride=(0, 256, 1)), T([64, 196, 256], f16, stride=(50176, 1, 196))], 1), {})\ncnt: 6, (([T([64, 1, 256], f16), T([64, 196, 256], f16, stride=(50432, 256, 1))], 1), {})\ncnt: 6, (([T([64, 1, 128], f16), T([64, 400, 128], f16, stride=(51328, 128, 1))], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 240, 240], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 240, 240], f16), T([128, 3, 12, 12], f16), T([128], f16), [12, 12], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 3, 224, 224], f16), T([256, 3, 16, 16], f16), T([256], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 256, 14, 14], f16, stride=(50432, 1, 3584, 256)), T([64, 3, 224, 224], f16), T([256, 3, 16, 16], f16), [256], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\ncnt: 1, ((T([64, 128, 20, 20], f16, stride=(51328, 1, 2560, 128)), T([64, 3, 240, 240], f16), T([128, 3, 12, 12], f16), [128], [12, 12], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 240, 240], f16), T([64, 3, 240, 240], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([2, 64, 1000], f16, stride=(0, 1000, 1)), 2), {})\nOperator: aten.gelu.default\ncnt: 3, ((T([64, 401, 384], f16),), {})\ncnt: 9, ((T([64, 197, 768], f16),), {})\ncnt: 6, ((T([64, 1, 128], f16),), {})\ncnt: 6, ((T([64, 1, 256], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 6, ((T([64, 1, 128], f16), T([64, 1, 128], f16)), {})\ncnt: 6, ((T([64, 1, 256], f16), T([64, 1, 256], f16)), {})\ncnt: 9, ((T([64, 197, 768], f16), T([64, 197, 768], f16)), {})\ncnt: 3, ((T([64, 401, 384], f16), T([64, 401, 384], f16)), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([64], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([2, 64, 1000], f16), [0]), {})\nOperator: aten.mm.default\ncnt: 3, ((T([64, 256], f16, stride=(50432, 1)), T([256, 256], f16, stride=(1, 256))), {})\ncnt: 3, ((T([64, 128], f16, stride=(51328, 1)), T([128, 128], f16, stride=(1, 128))), {})\ncnt: 1, ((T([64, 1000], f16), T([1000, 256], f16)), {})\ncnt: 1, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 256], f16, stride=(50432, 1))), {})\ncnt: 1, ((T([64, 1000], f16), T([1000, 128], f16)), {})\ncnt: 1, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 128], f16, stride=(51328, 1))), {})\ncnt: 6, ((T([64, 256], f16, stride=(50432, 1)), T([256, 128], f16)), {})\ncnt: 6, ((T([256, 64], f16, stride=(1, 50432)), T([64, 128], f16)), {})\ncnt: 6, ((T([64, 128], f16), T([128, 128], f16)), {})\ncnt: 3, ((T([128, 64], f16, stride=(1, 128)), T([64, 128], f16)), {})\ncnt: 9, ((T([25664, 128], f16), T([128, 128], f16)), {})\ncnt: 9, ((T([128, 25664], f16, stride=(1, 128)), T([25664, 128], f16)), {})\ncnt: 3, ((T([128, 64], f16, stride=(1, 128)), T([64, 128], f16, stride=(51328, 1))), {})\ncnt: 6, ((T([64, 128], f16, stride=(51328, 1)), T([128, 256], f16)), {})\ncnt: 6, ((T([128, 64], f16, stride=(1, 51328)), T([64, 256], f16)), {})\ncnt: 6, ((T([64, 256], f16), T([256, 256], f16)), {})\ncnt: 3, ((T([256, 64], f16, stride=(1, 256)), T([64, 256], f16)), {})\ncnt: 15, ((T([12608, 256], f16), T([256, 256], f16)), {})\ncnt: 15, ((T([256, 12608], f16, stride=(1, 256)), T([12608, 256], f16)), {})\ncnt: 3, ((T([256, 64], f16, stride=(1, 256)), T([64, 256], f16, stride=(50432, 1))), {})\ncnt: 9, ((T([12608, 256], f16), T([256, 768], f16)), {})\ncnt: 9, ((T([256, 12608], f16, stride=(1, 256)), T([12608, 768], f16)), {})\ncnt: 18, ((T([12608, 768], f16), T([768, 256], f16)), {})\ncnt: 18, ((T([768, 12608], f16, stride=(1, 768)), T([12608, 256], f16)), {})\ncnt: 3, ((T([25664, 128], f16), T([128, 384], f16)), {})\ncnt: 3, ((T([128, 25664], f16, stride=(1, 128)), T([25664, 384], f16)), {})\ncnt: 6, ((T([25664, 384], f16), T([384, 128], f16)), {})\ncnt: 6, ((T([384, 25664], f16, stride=(1, 384)), T([25664, 128], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 6, ((T([64, 4, 401, 401], f16), 0.1767766952966369), {})\ncnt: 18, ((T([64, 4, 197, 197], f16), 0.125), {})\ncnt: 6, ((T([64, 4, 1, 197], f16), 0.125), {})\ncnt: 6, ((T([64, 4, 1, 401], f16), 0.1767766952966369), {})\nOperator: aten.native_layer_norm.default\ncnt: 10, ((T([64, 401, 128], f16), [128], T([128], f16), T([128], f16), 1e-06), {})\ncnt: 22, ((T([64, 197, 256], f16), [256], T([256], f16), T([256], f16), 1e-06), {})\n```\n\n----------------------------------------\n\nTITLE: Importing DLPack Conversion Functions in PyTorch\nDESCRIPTION: This snippet demonstrates how to import the from_dlpack and to_dlpack functions from the torch.utils.dlpack module. These functions are used for converting between PyTorch tensors and DLPack tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/dlpack.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.utils.dlpack import from_dlpack, to_dlpack\n```\n\n----------------------------------------\n\nTITLE: Mathematical Formulation of Scale and Zero Point Calculation\nDESCRIPTION: This snippet provides the mathematical equations for how scale and zero point parameters are computed in PyTorch's quantization, with different formulations for symmetric and asymmetric quantization schemes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/quantization-support.rst#2025-04-22_snippet_1\n\nLANGUAGE: LaTeX\nCODE:\n```\n\\begin{aligned}\n    \\text{if Symmetric:}&\\\\\n    &s = 2 \\max(|x_\\text{min}|, x_\\text{max}) /\n        \\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\\n    &z = \\begin{cases}\n        0 & \\text{if dtype is qint8} \\\\\n        128 & \\text{otherwise}\n    \\end{cases}\\\\\n    \\text{Otherwise:}&\\\\\n        &s = \\left( x_\\text{max} - x_\\text{min}  \\right ) /\n            \\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\\n        &z = Q_\\text{min} - \\text{round}(x_\\text{min} / s)\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: PyTorch JIT Fuser API Examples\nDESCRIPTION: Python APIs for enabling, disabling, and controlling different fusion mechanisms in PyTorch's JIT, including NNC, NVFuser, and oneDNN Graph. These APIs allow fine-grained control over which fusion optimizations are applied to TorchScript code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ntorch._C._jit_set_texpr_fuser_enabled() # NNC enable/disable\ntorch._C._jit_override_can_fuse_on_cpu() # NNC on CPU\ntorch._C._jit_override_can_fuse_on_gpu() # NNC on GPU\nwith torch.jit.fuser(\"fuser1\"): # NNC context manager\n    pass\ntorch._C._jit_set_nvfuser_enabled() # NVFuser enable/disable (deprecated)\nwith torch.jit.fuser(\"fuser2\"): # NVFuser context manager (deprecated)\n    pass\ntorch._C._jit_set_llga_enabled(True) # oneDNN Graph on CPU\nwith torch.jit.fuser(\"fuser3\"): # oneDNN Graph context manager\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing functorch for PyTorch 1.11.x and 1.12.x\nDESCRIPTION: Command to install functorch package for PyTorch versions 1.11.x and 1.12.x. This is needed because functorch was a separate package before being integrated into PyTorch 1.13.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/install.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install functorch\n```\n\n----------------------------------------\n\nTITLE: Stack-based Operation Implementation in TorchScript\nDESCRIPTION: Shows how TorchScript implements builtin operators using a stack machine model. The example demonstrates adding two tensors by popping operands and pushing results to the stack.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_19\n\nLANGUAGE: cpp\nCODE:\n```\nusing Stack = std::vector<IValue>;\nusing Operation = std::function<void(Stack*)>;\n\n// schema: example_add(Tensor a, Tensor b) -> Tensor\nvoid example_add(Stack* stack) {\n    Tensor a, b;\n    // stack before: ? ? ? a b <- back\n    pop(stack, a, b); // Templated helper function\n                      // that pops a, b and converts them to Tensor\n    push(stack, a + b);\n    // stack after:\n    // ? ? ? c <- back\n}\n```\n\n----------------------------------------\n\nTITLE: Applying Tensor Normalization and Clamping in PyTorch\nDESCRIPTION: Performs normalization and clamping operations on tensors. These operations are used for stabilizing neural network training and preventing numerical issues with small values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\naten.norm.ScalarOpt_dim(T([64, 256], f16, stride=(12800, 1)), 2, [1], True)\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.clamp_min.default(T([64, 1], f16), 1e-12)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Threshold Backward Operations\nDESCRIPTION: Log entries for threshold backward operations used in gradient computation during backpropagation. Shows operations on feature maps of various sizes with a threshold value of 0.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet50_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 3, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), 0), {})\ncnt: 5, ((T([32, 512, 7, 7], f16), T([32, 512, 7, 7], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Chaining Multiple Learning Rate Schedulers in PyTorch\nDESCRIPTION: Example demonstrating how to chain multiple schedulers together. The schedulers are applied sequentially, with each one modifying the learning rate produced by the previous scheduler.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/optim.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler1.step()\n    scheduler2.step()\n```\n\n----------------------------------------\n\nTITLE: LSTM Graph After Derivative Preserving Optimization in PyTorch\nDESCRIPTION: The LSTM graph after applying derivative-preserving optimizations. These optimizations include eliminating dead code, common subexpression elimination, and algebraic rewrites that don't break autograd functionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ngraph(%x : Float(*, *),\n      %hx : Float(*, *),\n      %cx : Float(*, *),\n      %w_ih : Float(*, *),\n      %w_hh : Float(*, *),\n      %b_ih : Float(*),\n      %b_hh : Float(*)):\n  %8 : int = prim::Constant[value=1]()\n  %9 : Float(*, *) = aten::t(%w_ih)\n  %10 : Float(*, *) = aten::mm(%x, %9)\n  %11 : Float(*, *) = aten::t(%w_hh)\n  %12 : Float(*, *) = aten::mm(%hx, %11)\n  %13 : Float(*, *) = aten::add(%10, %12, %8)\n  %14 : Float(*, *) = aten::add(%13, %b_ih, %8)\n  %gates : Float(*, *) = aten::add(%14, %b_hh, %8)\n  %31 : Float(*, *), %32 : Float(*, *), %33 : Float(*, *), %34 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%gates)\n  %ingate : Float(*, *) = aten::sigmoid(%31)\n  %forgetgate : Float(*, *) = aten::sigmoid(%32)\n  %cellgate : Float(*, *) = aten::tanh(%33)\n  %outgate : Float(*, *) = aten::sigmoid(%34)\n  %25 : Float(*, *) = aten::mul(%forgetgate, %cx)\n  %26 : Float(*, *) = aten::mul(%ingate, %cellgate)\n  %cy : Float(*, *) = aten::add(%25, %26, %8)\n  %28 : Float(*, *) = aten::tanh(%cy)\n  %hy : Float(*, *) = aten::mul(%outgate, %28)\n  %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)\n  return (%30)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom NCCL Memory Allocator in PyTorch\nDESCRIPTION: Code that demonstrates how to create a custom ncclMemAlloc allocator wrapped in torch.cuda.memory.CUDAPluggableAllocator. This allocator enables NVLink Switch Reductions for distributed training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nimport torch.distributed as dist\nfrom torch.cuda.memory import CUDAPluggableAllocator\nfrom torch.distributed.distributed_c10d import _get_default_group\nfrom torch.utils import cpp_extension\n\n\n# create allocator\nnccl_allocator_source = \"\"\"\n#include <nccl.h>\n#include <iostream>\nextern \"C\" {\n\nvoid* nccl_alloc_plug(size_t size, int device, void* stream) {\n  std::cout << \"Using ncclMemAlloc\" << std::endl;\n  void* ptr;\n  ncclResult_t err = ncclMemAlloc(&ptr, size);\n  return ptr;\n\n}\n\nvoid nccl_free_plug(void* ptr, size_t size, int device, void* stream) {\n  std::cout << \"Using ncclMemFree\" << std::endl;\n  ncclResult_t err = ncclMemFree(ptr);\n}\n\n}\n\"\"\"\nnccl_allocator_libname = \"nccl_allocator\"\nnccl_allocator = torch.utils.cpp_extension.load_inline(\n    name=nccl_allocator_libname,\n    cpp_sources=nccl_allocator_source,\n    with_cuda=True,\n    extra_ldflags=[\"-lnccl\"],\n    verbose=True,\n    is_python_module=False,\n    build_directory=\"./\",\n)\n\nallocator = CUDAPluggableAllocator(\n    f\"./{nccl_allocator_libname}.so\", \"nccl_alloc_plug\", \"nccl_free_plug\"\n).allocator()\n\n# setup distributed\nrank = int(os.getenv(\"RANK\"))\nlocal_rank = int(os.getenv(\"LOCAL_RANK\"))\nworld_size = int(os.getenv(\"WORLD_SIZE\"))\ntorch.cuda.set_device(local_rank)\ndist.init_process_group(backend=\"nccl\")\ndevice = torch.device(f\"cuda:{local_rank}\")\ndefault_pg = _get_default_group()\nbackend = default_pg._get_backend(device)\n\n# Note: for convenience, ProcessGroupNCCL backend provides\n# the ncclMemAlloc allocator as backend.mem_allocator\nallocator = backend.mem_allocator\n```\n\n----------------------------------------\n\nTITLE: Demonstrating TorchScript Variable Scope Restrictions\nDESCRIPTION: Example showcasing TorchScript's variable resolution rules. Variables must have the same type on all paths and must be defined on all branches before use.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n@torch.jit.script\ndef foo(x):\n    if x < 0:\n        y = 4\n    print(y)\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization and Loss Operations in PyTorch Model\nDESCRIPTION: This code shows batch normalization operations and loss computation for the model. It includes forward and backward passes for batch norm at different feature dimensions, as well as the negative log-likelihood loss calculation used for classification tasks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 8, ((T([128, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 9, ((T([128, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f16), True, 0.1, 1e-05), {})\ncnt: 10, ((T([128, 768, 7, 7], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f16), True, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 10, ((T([128, 768, 7, 7], f16), T([128, 768, 7, 7], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f32), T([768], f32), True, 1e-05, [True, True, True]), {})\ncnt: 9, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f32), T([384], f32), True, 1e-05, [True, True, True]), {})\ncnt: 8, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\nOperator: aten.relu_.default\n```\n\n----------------------------------------\n\nTITLE: Profiling Various ATen Operator Invocation Patterns - PyTorch - Python\nDESCRIPTION: This code snippet represents a structured profiling or schema dataset, enumerating how various PyTorch ATen operators are invoked across a spectrum of tensor shapes and argument lists. Dependencies include a working knowledge of tensor shape notation (e.g., T([128,64,64,64],f16)), PyTorch's type conventions, and potentially a runner that interprets this data for tests. Key parameters are tensor shapes, dtypes, and operator-specific arguments. Inputs are operator, argument tuples, and usage counts; outputs would be coverage data or feeding these to profiling/test harnesses. This dataset is not standalone executable code but must be interpreted by a higher-level script for actual invocation or evaluation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/botnet26t_256_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 256, 16, 16], f16), T([128, 256, 32, 32], f16), T([256, 256, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([128, 512, 32, 32], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 512, 32, 32], f16), T([128, 128, 32, 32], f16), T([512, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16), T([128, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 32, 32], f16), T([128, 512, 32, 32], f16), T([128, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 512, 32, 32], f16), T([128, 256, 64, 64], f16), T([512, 256, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 32, 32], f16), T([128, 128, 64, 64], f16), T([128, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 256, 64, 64], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 256, 64, 64], f16), T([128, 64, 64, 64], f16), T([256, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 64, 64], f16), T([128, 256, 64, 64], f16), T([64, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), T([64, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 128, 128], f16), T([128, 32, 128, 128], f16), T([64, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 24, 128, 128], f16), T([32, 24, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 24, 128, 128], f16), T([128, 3, 256, 256], f16), T([24, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 256, 256], f16), T([128, 3, 256, 256], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 2048, 8, 8], f16, stride=(2048, 1, 0, 0)), 64), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([128, 64, 128, 128], f16), [3, 3], [2, 2], [1, 1]), {})\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([128, 64, 64, 64], f16), T([128, 64, 128, 128], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([128, 64, 64, 64], i64)), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 2048, 8, 8], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 2, ((T([131072, 64], f16), T([64, 31], f16, stride=(1, 64))), {})\ncnt: 2, ((T([131072, 128], f16), T([128, 31], f16, stride=(1, 128))), {})\ncnt: 2, ((T([32768, 128], f16), T([128, 15], f16, stride=(1, 128))), {})\ncnt: 1, ((T([128, 1000], f16), T([1000, 2048], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 2048], f16)), {})\ncnt: 2, ((T([15, 32768], f16, stride=(1, 15)), T([32768, 128], f16)), {})\ncnt: 2, ((T([32768, 15], f16), T([15, 128], f16)), {})\ncnt: 2, ((T([31, 131072], f16, stride=(1, 31)), T([131072, 128], f16)), {})\ncnt: 2, ((T([131072, 31], f16), T([31, 128], f16)), {})\ncnt: 2, ((T([31, 131072], f16, stride=(1, 31)), T([131072, 64], f16)), {})\ncnt: 2, ((T([131072, 31], f16), T([31, 64], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 2, ((T([512, 256, 256], f16), 0.125), {})\ncnt: 2, ((T([512, 256, 256], f16), 0.08838834764831845), {})\ncnt: 2, ((T([512, 64, 64], f16), 0.08838834764831845), {})\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 24, 128, 128], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 64, 128, 128], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 256, 64, 64], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 128, 32, 32], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 512, 32, 32], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 256, 16, 16], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 1024, 16, 16], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 512, 8, 8], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 2048, 8, 8], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f16), True, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([128, 256, 32, 32], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 64, 128, 128], f16), T([128, 64, 128, 128], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 24, 128, 128], f16), T([128, 24, 128, 128], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 24, 128, 128], f16),), {})\ncnt: 1, ((T([128, 32, 128, 128], f16),), {})\ncnt: 1, ((T([128, 64, 128, 128], f16),), {})\ncnt: 4, ((T([128, 64, 64, 64], f16),), {})\ncnt: 2, ((T([128, 256, 64, 64], f16),), {})\ncnt: 1, ((T([128, 128, 64, 64], f16),), {})\ncnt: 3, ((T([128, 128, 32, 32], f16),), {})\ncnt: 2, ((T([128, 512, 32, 32], f16),), {})\ncnt: 1, ((T([128, 256, 32, 32], f16),), {})\ncnt: 3, ((T([128, 256, 16, 16], f16),), {})\ncnt: 2, ((T([128, 1024, 16, 16], f16),), {})\ncnt: 1, ((T([128, 512, 16, 16], f16),), {})\ncnt: 3, ((T([128, 512, 8, 8], f16),), {})\ncnt: 2, ((T([128, 2048, 8, 8], f16),), {})\nOperator: aten.slice_backward.default\ncnt: 2, ((T([4096, 8, 8], f16), [4096, 8, 15], 2, 7, 9223372036854775807, 1), {})\ncnt: 2, ((T([4096, 8, 15], f16), [4096, 9, 15], 1, 0, 8, 1), {})\ncnt: 2, ((T([4096, 9, 15], f16), [4096, 9, 15], 0, 0, 9223372036854775807, 1), {})\ncnt: 4, ((T([8192, 16, 16], f16), [8192, 16, 31], 2, 15, 9223372036854775807, 1), {})\ncnt: 4, ((T([8192, 16, 31], f16), [8192, 17, 31], 1, 0, 16, 1), {})\ncnt: 4, ((T([8192, 17, 31], f16), [8192, 17, 31], 0, 0, 9223372036854775807, 1), {})\nOperator: aten.split_with_sizes.default\ncnt: 1, ((T([128, 768, 16, 16], f16), [256, 256, 256], 1), {})\ncnt: 1, ((T([128, 1536, 16, 16], f16), [512, 512, 512], 1), {})\ncnt: 1, ((T([128, 1536, 8, 8], f16), [512, 512, 512], 1), {})\nOperator: aten.sum.SymInt\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Operations in PyTorch\nDESCRIPTION: This code snippet lists various tensor operations, their shapes, data types, and occurrence counts. It includes both regular tensor operations and the aten.threshold_backward.default operator with different tensor dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/botnet26t_256_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 1, ((T([512, 8, 8, 8, 8], f16, stride=(4096, 64, 1, 512, 8)), [2], True), {})\ncnt: 1, ((T([512, 8, 8, 8, 8], f16, stride=(4096, 512, 8, 64, 1)), [2], True), {})\ncnt: 2, ((T([512, 16, 16, 16, 16], f16, stride=(65536, 256, 1, 4096, 16)), [2], True), {})\ncnt: 2, ((T([512, 16, 16, 16, 16], f16, stride=(65536, 4096, 16, 256, 1)), [2], True), {})\nOperator: aten.threshold_backward.default\ncnt: 2, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16), 0), {})\ncnt: 3, ((T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16), 0), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16), 0), {})\ncnt: 2, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16), 0), {})\ncnt: 3, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16), 0), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([128, 256, 32, 32], f16), 0), {})\ncnt: 2, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16), 0), {})\ncnt: 3, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16), 0), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), 0), {})\ncnt: 2, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16), 0), {})\ncnt: 4, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), 0), {})\ncnt: 1, ((T([128, 64, 128, 128], f16), T([128, 64, 128, 128], f16), 0), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16), 0), {})\ncnt: 1, ((T([128, 24, 128, 128], f16), T([128, 24, 128, 128], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Installing Common Dependencies - Conda and Pip - bash\nDESCRIPTION: This snippet details installing essential build dependencies using conda and pip package managers. It covers CMake and Ninja for the build system, and Python-level dependencies via requirements.txt. Run these from the PyTorch directory after cloning. Conda and pip must be installed beforehand. The snippet applies to all platforms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda install cmake ninja\\n# Run this command from the PyTorch directory after cloning the source code using the Get the PyTorch Source section below\\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations in Neural Network\nDESCRIPTION: Shows various convolution operations in a neural network, including the initial layer that processes the input image and subsequent convolutional layers with different kernel sizes, strides, and channel dimensions across network stages.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([128, 3, 4, 4], f16), T([128], f16), [4, 4], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([128, 1, 7, 7], f16), T([128], f16), [1, 1], [3, 3], [1, 1], False, [0, 0], 128), {})\ncnt: 1, ((T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([256, 128, 2, 2], f16), T([256], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 256, 28, 28], f16, stride=(200704, 1, 7168, 256)), T([256, 1, 7, 7], f16), T([256], f16), [1, 1], [3, 3], [1, 1], False, [0, 0], 256), {})\ncnt: 1, ((T([32, 256, 28, 28], f16, stride=(200704, 1, 7168, 256)), T([512, 256, 2, 2], f16), T([512], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 27, ((T([32, 512, 14, 14], f16, stride=(100352, 1, 7168, 512)), T([512, 1, 7, 7], f16), T([512], f16), [1, 1], [3, 3], [1, 1], False, [0, 0], 512), {})\n```\n\n----------------------------------------\n\nTITLE: Export IR Node Structure for torch.fx.Node Representation in Python\nDESCRIPTION: This snippet establishes the Python schema for a Node in Export IR, specifying the required properties such as name, operation type (op_name), target operation/callable, argument list, keyword argument dictionary, and metadata dictionary. It serves as a basis for representing computational operations within the torch.fx framework, as used by PyTorch Export IR. Expected input values align with torch.fx conventions; limitations include that the specific meaning of target/args/kwargs/meta depends on op_name.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Node:\n  name: str # name of node\n  op_name: str  # type of operation\n\n  # interpretation of the fields below depends on op_name\n  target: [str|Callable]\n  args: List[object]\n  kwargs: Dict[str, object]\n  meta: Dict[str, object]\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Map-style Dataset without Batching\nDESCRIPTION: Demonstrates the equivalent operation of loading individual samples from a map-style dataset when automatic batching is disabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfor index in sampler:\n    yield collate_fn(dataset[index])\n```\n\n----------------------------------------\n\nTITLE: Computing Mean of Tensor Dimensions in Python\nDESCRIPTION: Calculates the mean of tensor elements along specified dimensions. It requires a tensor input, with dimensions like [16, 64] or [16, 256], and a list of dimensions for reduction, such as [0]. The output is a tensor with reduced dimensions, providing average values crucial for statistical normalizations in data analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 4, ((T([16, 64], f16), [0]), {})\ncnt: 4, ((T([16, 128], f16), [0]), {})\ncnt: 26, ((T([16, 256], f16), [0]), {})\n```\n\n----------------------------------------\n\nTITLE: Checking Out Nightly PyTorch Branch with Python\nDESCRIPTION: Uses the tools/nightly.py script to check out a new nightly branch of PyTorch and set up a virtual environment. This allows pure Python development without needing to compile C++ code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./tools/nightly.py checkout -b my-nightly-branch\nsource venv/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\n----------------------------------------\n\nTITLE: Tensor Reduction Operations\nDESCRIPTION: Sum operations across specified dimensions with various tensor shapes, mainly used for spatial dimension reduction in convolution networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet demonstrates the use of batch normalization in PyTorch, showing forward and backward passes with various tensor shapes and parameters. It includes operations for different layers in a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\n\nOperator: aten.native_batch_norm_backward.default\ncnt: 4, ((T([128, 2048, 7, 7], f16), T([128, 2048, 7, 7], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), True, 1e-05, [True, True, True]), {})\ncnt: 9, ((T([128, 256, 7, 7], f16), T([128, 256, 7, 7], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Including CSV Table for Prims IR Operations\nDESCRIPTION: This snippet includes a CSV table containing information about Prims IR operations. It uses the csv-table directive in reStructuredText to reference an external CSV file.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_ir.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. csv-table::\n   :file: ../build/ir/prims_ops.csv\n   :widths: auto\n   :header-rows: 1\n```\n\n----------------------------------------\n\nTITLE: For Loop Example with Tuples in TorchScript\nDESCRIPTION: Demonstrates how for loops unroll when iterating over tuples in TorchScript, generating separate bodies for each tuple member.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom typing import Tuple\n\n@torch.jit.script\ndef fn():\n    tup = (3, torch.ones(4))\n    for x in tup:\n        print(x)\n\nfn()\n```\n\n----------------------------------------\n\nTITLE: Unwrapping and Redispatching NJT Operations in PyTorch\nDESCRIPTION: This snippet describes the process of implementing an operation on a Nested Jagged Tensor by unwrapping it, redispatching on the underlying 'values' buffer, and propagating metadata to create a new output NJT. It highlights the need to compute new metadata when the output shape differs from the input.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# Pseudocode for NJT operation implementation\n# 1. Unwrap the NJT\n# 2. Redispatch the op on the 'values' buffer\n# 3. Propagate relevant metadata (including offsets)\n# 4. Compute new metadata if output shape differs\n# 5. Create new output NJT\n```\n\n----------------------------------------\n\nTITLE: PyTorch NLL Loss Forward Operation\nDESCRIPTION: NLL Loss forward pass computation with f16 tensors, processing batches of 128 samples across 1000 classes with ignore index of -100.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Applying Element-wise Operations in PyTorch\nDESCRIPTION: Performs various element-wise operations such as addition, division, multiplication, and comparison on tensors. These operations are essential for implementing neural network computations and gradient calculations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\naten.add.Tensor(T([64, 256], f16), T([64, 256], f16))\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.div.Tensor(T([64, 256], f16, stride=(12800, 1)), T([64, 256], f16, stride=(1, 0)))\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.mul.Tensor(T([64, 256], f16), T([64, 256], f16))\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.eq.Scalar(T([64, 1], f16), 0)\n```\n\n----------------------------------------\n\nTITLE: Defining Dispatch for Native Functions in YAML\nDESCRIPTION: Shows how to specify dispatch information for native functions in native_functions.yaml, including backend-specific implementations and composite kernels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_13\n\nLANGUAGE: YAML\nCODE:\n```\ndispatch:\n  CPU: kernel_cpu\n  CUDA: kernel_cuda\n  QuantizedCPU: kernel_quantized_cpu\n```\n\nLANGUAGE: YAML\nCODE:\n```\ndispatch:\n  CompositeExplicitAutograd: kernel\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Jacobian Computation With and Without vmap - Python\nDESCRIPTION: Uses torch.utils.benchmark.Timer to time both manual (row-by-row) and functorch.jacrev (vectorized) Jacobian computations. Runs each Timer for 500 iterations and prints the timing results. Requires previous 'compute_jac', 'jacrev', and associated variables. Outputs: raw timing statistics for both approaches.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.benchmark import Timer\n\nwithout_vmap = Timer(stmt=\"compute_jac(xp)\", globals=globals())\nwith_vmap = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\nno_vmap_timer = without_vmap.timeit(500)\nwith_vmap_timer = with_vmap.timeit(500)\n\nprint(no_vmap_timer)\nprint(with_vmap_timer)\n```\n\n----------------------------------------\n\nTITLE: Disabling NVFuser Features in Python\nDESCRIPTION: This code snippet shows how to disable specific NVFuser features using environment variables to address unexpected outputs or performance issues.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\nexport PYTORCH_NVFUSER_DISABLE=fma,index_hoist\n```\n\n----------------------------------------\n\nTITLE: Visualizing Pattern Matching in PyTorch Computational Graph\nDESCRIPTION: This code snippet illustrates how a specific fusion pattern (nn.ReLU, (operator.add, MatchAllNode, (nn.BatchNorm2d, nn.Conv2d))) would match a computational graph in PyTorch. It shows the structure of the graph and how different operations are connected.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/pattern.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ntensor_1            tensor_2\n |                    |\n *(MatchAllNode)  nn.Conv2d\n |                    |\n |             nn.BatchNorm2d\n \\                  /\n  -- operator.add --\n         |\n      nn.ReLU\n```\n\n----------------------------------------\n\nTITLE: Aggregate Sum along Dimensional Indices with ATen\nDESCRIPTION: Performs summation of elements over specified dimensions. Requires tensor inputs with dimensional lists, like [0]. Outputs minimized tensors with relevant elements' sum across dimensions, beneficial for dimensionality reduction in pre-processing tasks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.dim_IntList\ncnt: 4, ((T([16, 64], f16), [0]), {})\ncnt: 4, ((T([16, 128], f16), [0]), {})\ncnt: 26, ((T([16, 256], f16), [0]), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations\nDESCRIPTION: Matrix multiplication operations between tensors of compatible shapes, typically used in fully connected layers. Uses float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 1000], f16), T([1000, 1536], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.sum.SymInt in PyTorch ATen\nDESCRIPTION: Logs calls to the `aten.sum.SymInt` operator, which computes the sum of elements across specified dimensions of a tensor. The arguments include the input tensor (f16), a list of dimensions to reduce (`[0]` or `[2, 3]`), and a boolean flag indicating whether to keep the reduced dimensions (`True`). The `cnt` indicates call frequency.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 1, ((T([128, 256, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 128, 7, 7], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Creating Test Module for Versioned Operator in PyTorch\nDESCRIPTION: Example of creating a test module in fixtures_src.py for the linspace operator to test backward compatibility. The module follows the naming convention 'TestVersioned{OperatorName}V{FormatVersion}' and includes sample usage of the operator that will be changed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass TestVersionedLinspaceV7(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, a: Union[int, float, complex], b: Union[int, float, complex]):\n        c = torch.linspace(a, b, steps=5)\n        d = torch.linspace(a, b)\n        return c, d\n```\n\n----------------------------------------\n\nTITLE: Summation Across Dimensions with aten.sum.SymInt in PyTorch (Python)\nDESCRIPTION: Examples of summing tensors over the first axis across several shapes, returning reduced tensors for further processing. Used for pooling, aggregation, or normalization. Accepts input tensor, dimension list, and keepdim flag; operates exclusively on FP16 inputs for all documented usages.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([512, 30522], f16), [0], True), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 61, ((T([512, 768], f16), [0], True), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 12, ((T([512, 3072], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations\nDESCRIPTION: Documents convolution and convolution backward operations with various kernel sizes and channel dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\naten.convolution.default((T([32, 3, 224, 224], f16), T([64, 3, 4, 4], f16), T([64], f16), [4, 4], [0, 0], [1, 1], False, [0, 0], 1))\naten.convolution_backward.default((T([32, 512, 7, 7], f16, stride=(25088, 1, 3584, 512)), T([32, 512, 7, 7], f16, stride=(25088, 1, 3584, 512)), T([512, 1, 3, 3], f16), [512], [1, 1], [1, 1], [1, 1], False, [0, 0], 512, [True, True, True]))\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations\nDESCRIPTION: Series of matrix multiplication operations between tensors of various shapes using float16 precision, showing both forward and transposed operations\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swin_base_patch4_window7_224_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([50176, 512], f16), T([512, 256], f16, stride=(1, 512))), {})\ncnt: 1, ((T([12544, 1024], f16), T([1024, 512], f16, stride=(1, 1024))), {})\n```\n\n----------------------------------------\n\nTITLE: Batch and Summation Reduction (aten.sum.SymInt, aten.sum.default) in PyTorch (Python)\nDESCRIPTION: These snippets describe sum reductions over batch outputs, one with a symbolic int axis and keepdim parameter, the other summing the entire tensor. Intended for reducing over class or batch axes, accepting float16 input and, in the first case, supporting symbolic/static size semantics. Output is a scalar or reduced-dim tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([32, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Call Statistics for Inception-style Neural Network\nDESCRIPTION: This document lists PyTorch operators used in a neural network's forward and backward passes, including their call counts and tensor shapes. The patterns suggest an Inception-style architecture with multiple parallel paths, various convolution sizes, and a mix of operations typical in deep convolutional neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 4, ((T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16)), {})\ncnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16)), {})\ncnt: 3, ((T([128, 1280, 8, 8], f16), T([128, 1280, 8, 8], f16)), {})\ncnt: 14, ((T([128, 768, 17, 17], f16), T([128, 768, 17, 17], f16)), {})\ncnt: 5, ((T([128, 288, 35, 35], f16), T([128, 288, 35, 35], f16)), {})\ncnt: 3, ((T([128, 256, 35, 35], f16), T([128, 256, 35, 35], f16)), {})\ncnt: 3, ((T([128, 192, 35, 35], f16), T([128, 192, 35, 35], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 94, ((T([], i64), 1), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([128, 192, 35, 35], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 1, ((T([128, 256, 35, 35], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 1, ((T([128, 288, 35, 35], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 4, ((T([128, 768, 17, 17], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), [3, 3], [1, 1], [1, 1]), {})\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([128, 1280, 8, 8], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 4, ((T([128, 768, 17, 17], f16), T([128, 768, 17, 17], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 1, ((T([128, 288, 35, 35], f16), T([128, 288, 35, 35], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 1, ((T([128, 256, 35, 35], f16), T([128, 256, 35, 35], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 1, ((T([128, 192, 35, 35], f16), T([128, 192, 35, 35], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\nOperator: aten.cat.default\ncnt: 1, (([T([128, 64, 35, 35], f16), T([128, 64, 35, 35], f16), T([128, 96, 35, 35], f16), T([128, 32, 35, 35], f16)], 1), {})\ncnt: 2, (([T([128, 64, 35, 35], f16), T([128, 64, 35, 35], f16), T([128, 96, 35, 35], f16), T([128, 64, 35, 35], f16)], 1), {})\ncnt: 1, (([T([128, 384, 17, 17], f16), T([128, 96, 17, 17], f16), T([128, 288, 17, 17], f16)], 1), {})\ncnt: 4, (([T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16)], 1), {})\ncnt: 1, (([T([128, 320, 8, 8], f16), T([128, 192, 8, 8], f16), T([128, 768, 8, 8], f16)], 1), {})\ncnt: 4, (([T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16)], 1), {})\ncnt: 2, (([T([128, 320, 8, 8], f16), T([128, 768, 8, 8], f16), T([128, 768, 8, 8], f16), T([128, 192, 8, 8], f16)], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 299, 299], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 299, 299], f16), T([32, 3, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 149, 149], f16), T([32, 32, 3, 3], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 147, 147], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 73, 73], f16), T([80, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 80, 73, 73], f16), T([192, 80, 3, 3], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 192, 35, 35], f16), T([64, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 35, 35], f16), T([48, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 48, 35, 35], f16), T([64, 48, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 64, 35, 35], f16), T([96, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 96, 35, 35], f16), T([96, 96, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 35, 35], f16), T([32, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 256, 35, 35], f16), T([64, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 35, 35], f16), T([48, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 288, 35, 35], f16), T([64, 288, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 288, 35, 35], f16), T([48, 288, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 288, 35, 35], f16), T([384, 288, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 35, 35], f16), T([96, 96, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 12, ((T([128, 768, 17, 17], f16), T([192, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 768, 17, 17], f16), T([128, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 128, 17, 17], f16), T([128, 128, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 17, 17], f16), T([192, 128, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 128, 17, 17], f16), T([128, 128, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 17, 17], f16), T([192, 128, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 768, 17, 17], f16), T([160, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 160, 17, 17], f16), T([160, 160, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 160, 17, 17], f16), T([192, 160, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 160, 17, 17], f16), T([160, 160, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 160, 17, 17], f16), T([192, 160, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 192, 17, 17], f16), T([192, 192, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 192, 17, 17], f16), T([192, 192, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 17, 17], f16), T([320, 192, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 17, 17], f16), T([192, 192, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([320, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([384, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 384, 8, 8], f16), T([384, 384, 1, 3], f16), None, [1, 1], [0, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 384, 8, 8], f16), T([384, 384, 3, 1], f16), None, [1, 1], [1, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([448, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 448, 8, 8], f16), T([384, 448, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([192, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([320, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([384, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([448, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([192, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 192, 8, 8], f16), T([128, 2048, 8, 8], f16), T([192, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16), T([384, 384, 3, 1], f16), [0], [1, 1], [1, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16), T([384, 384, 1, 3], f16), [0], [1, 1], [0, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 384, 8, 8], f16), T([128, 448, 8, 8], f16), T([384, 448, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 448, 8, 8], f16), T([128, 2048, 8, 8], f16), T([448, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 384, 8, 8], f16), T([128, 2048, 8, 8], f16), T([384, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 320, 8, 8], f16), T([128, 2048, 8, 8], f16), T([320, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 8, 8], f16), T([128, 1280, 8, 8], f16), T([192, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 448, 8, 8], f16), T([128, 1280, 8, 8], f16), T([448, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 384, 8, 8], f16), T([128, 1280, 8, 8], f16), T([384, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 320, 8, 8], f16), T([128, 1280, 8, 8], f16), T([320, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 8, 8], f16), T([128, 192, 17, 17], f16), T([192, 192, 3, 3], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Reduction Input Tuples for aten.sum.SymInt - Python\nDESCRIPTION: These tuples configure inputs for the PyTorch 'aten.sum.SymInt' operator, defining tensors, reduction dimensions (axes), and a keepdims option (boolean). Used to test summing across specified axes for f16 tensors, constraining input shapes and reduction dimensions as per the PyTorch API. Overly large reductions may challenge device memory capacity.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 960, 7, 7], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Backward Convolution Operations in PyTorch\nDESCRIPTION: Illustrates the use of 'aten.convolution_backward.default' for calculating gradients during the backward pass in a convolutional network in PyTorch. It involves operations on inputs, weights, and outputs to compute gradients needed for parameter updates. The configuration denotes strides, padding, and dilation used in the convolutions, indicating bidirectional gradient updates.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([96, 1280, 7, 7], f16), T([96, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 320, 7, 7], f16), T([96, 960, 7, 7], f16), T([320, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Post-derivative Optimization LSTM Graph with CUDA Fusion in PyTorch\nDESCRIPTION: The final optimized LSTM graph after applying CUDA kernel fusion. For inference-only graphs, operations are grouped into fusion groups that can be executed as a single CUDA kernel to improve performance.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ngraph(%x : Float(*, *),\n      %hx : Float(*, *),\n      %cx : Float(*, *),\n      %w_ih : Float(*, *),\n      %w_hh : Float(*, *),\n      %b_ih : Float(*),\n      %b_hh : Float(*)):\n  %9 : Float(*, *) = aten::t(%w_ih)\n  %10 : Float(*, *) = aten::mm(%x, %9)\n  %11 : Float(*, *) = aten::t(%w_hh)\n  %12 : Float(*, *) = aten::mm(%hx, %11)\n  %77 : Tensor[] = prim::ListConstruct(%b_hh, %b_ih, %10, %12)\n  %78 : Tensor[] = aten::broadcast_tensors(%77)\n  %79 : Tensor, %80 : Tensor, %81 : Tensor, %82 : Tensor = prim::ListUnpack(%78)\n  %hy : Float(*, *), %cy : Float(*, *) = prim::FusionGroup_0(%cx, %82, %81, %80, %79)\n  %30 : (Float(*, *), Float(*, *)) = prim::TupleConstruct(%hy, %cy)\n  return (%30);\n\nwith prim::FusionGroup_0 = graph(%13 : Float(*, *),\n      %71 : Tensor,\n      %76 : Tensor,\n      %81 : Tensor,\n      %86 : Tensor):\n  %87 : Float(*, *), %88 : Float(*, *), %89 : Float(*, *), %90 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%86)\n  %82 : Float(*, *), %83 : Float(*, *), %84 : Float(*, *), %85 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%81)\n  %77 : Float(*, *), %78 : Float(*, *), %79 : Float(*, *), %80 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%76)\n  %72 : Float(*, *), %73 : Float(*, *), %74 : Float(*, *), %75 : Float(*, *) = prim::ConstantChunk[chunks=4, dim=1](%71)\n  %69 : int = prim::Constant[value=1]()\n  %70 : Float(*, *) = aten::add(%77, %72, %69)\n  %66 : Float(*, *) = aten::add(%78, %73, %69)\n  %62 : Float(*, *) = aten::add(%79, %74, %69)\n  %58 : Float(*, *) = aten::add(%80, %75, %69)\n  %54 : Float(*, *) = aten::add(%70, %82, %69)\n  %50 : Float(*, *) = aten::add(%66, %83, %69)\n  %46 : Float(*, *) = aten::add(%62, %84, %69)\n  %42 : Float(*, *) = aten::add(%58, %85, %69)\n  %38 : Float(*, *) = aten::add(%54, %87, %69)\n  %34 : Float(*, *) = aten::add(%50, %88, %69)\n  %30 : Float(*, *) = aten::add(%46, %89, %69)\n  %26 : Float(*, *) = aten::add(%42, %90, %69)\n  %ingate : Float(*, *) = aten::sigmoid(%38)\n  %forgetgate : Float(*, *) = aten::sigmoid(%34)\n  %cellgate : Float(*, *) = aten::tanh(%30)\n  %outgate : Float(*, *) = aten::sigmoid(%26)\n  %14 : Float(*, *) = aten::mul(%forgetgate, %13)\n  %11 : Float(*, *) = aten::mul(%ingate, %cellgate)\n  %cy : Float(*, *) = aten::add(%14, %11, %69)\n  %4 : Float(*, *) = aten::tanh(%cy)\n  %hy : Float(*, *) = aten::mul(%outgate, %4)\n  return (%hy, %cy)\n```\n\n----------------------------------------\n\nTITLE: Sum Operations in PyTorch\nDESCRIPTION: This snippet shows sum operations on tensors with various shapes and dimensions. It includes operations that reduce tensors along specified dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([64, 196, 1000], f16), [0, 1], True), {})\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\ncnt: 2, ((T([64, 384], f16, stride=(75648, 1)), [0], True), {})\ncnt: 2, ((T([64, 1152], f16), [0], True), {})\ncnt: 2, ((T([64, 384], f16), [0], True), {})\ncnt: 1, ((T([64, 1, 384], f16, stride=(75648, 384, 1)), [0], True), {})\ncnt: 1, ((T([64, 14, 14, 384], f16, stride=(75648, 5376, 384, 1)), [0, 1, 2], True), {})\ncnt: 14, ((T([64, 14, 14, 1152], f16), [0, 1, 2], True), {})\ncnt: 27, ((T([64, 14, 14, 384], f16), [0, 1, 2], True), {})\ncnt: 1, ((T([64, 14, 14, 384], f16), [0], True), {})\ncnt: 8, ((T([64, 28, 28, 192], f16, stride=(150528, 28, 1, 784)), [0, 1, 2], True), {})\ncnt: 4, ((T([64, 28, 28, 576], f16), [0, 1, 2], True), {})\ncnt: 4, ((T([64, 14, 14, 486], f16), [0, 1, 2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Examining Matrix Multiplication and Tensor Clone Operations in PyTorch\nDESCRIPTION: This snippet shows statistics for addmm (matrix multiplication with addition) and clone operations in the model. The addmm operation is used for the final classification layer, while clone operations are used throughout the network on tensors of various shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 192, 192], f16),), {})\ncnt: 2, ((T([128, 32, 96, 96], f16),), {})\ncnt: 1, ((T([128, 8, 1, 1], f16),), {})\ncnt: 1, ((T([128, 96, 96, 96], f16),), {})\ncnt: 1, ((T([128, 96, 48, 48], f16),), {})\ncnt: 1, ((T([128, 4, 1, 1], f16),), {})\ncnt: 3, ((T([128, 144, 48, 48], f16),), {})\ncnt: 2, ((T([128, 6, 1, 1], f16),), {})\ncnt: 1, ((T([128, 144, 24, 24], f16),), {})\ncnt: 3, ((T([128, 240, 24, 24], f16),), {})\ncnt: 2, ((T([128, 10, 1, 1], f16),), {})\ncnt: 1, ((T([128, 240, 12, 12], f16),), {})\ncnt: 8, ((T([128, 480, 12, 12], f16),), {})\ncnt: 4, ((T([128, 20, 1, 1], f16),), {})\ncnt: 7, ((T([128, 672, 12, 12], f16),), {})\ncnt: 4, ((T([128, 28, 1, 1], f16),), {})\ncnt: 1, ((T([128, 672, 6, 6], f16),), {})\ncnt: 10, ((T([128, 1152, 6, 6], f16),), {})\ncnt: 5, ((T([128, 48, 1, 1], f16),), {})\ncnt: 1, ((T([128, 1280, 6, 6], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication using AddMM in PyTorch\nDESCRIPTION: The aten.addmm.default operator performs matrix multiplication and addition with initial tensor [1536], and matrices [512,1536] and [1536,1536] in float16. This is commonly used in neural network layer computations for linear transformations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 96, ((T([1536], f16), T([512, 1536], f16), T([1536, 1536], f16, stride=(1, 1536))), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Local Timer Client in PyTorch Distributed Elastic\nDESCRIPTION: Class for implementing a local timer client using multiprocess.Queue. It is part of the server/client implementations provided by torchelastic.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nLocalTimerClient\n```\n\n----------------------------------------\n\nTITLE: Running Performance Benchmark\nDESCRIPTION: Executes performance comparison between eager mode and AOT compiled functions\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlarge_inputs = [torch.randn(1024, 2048, requires_grad=True) for _ in range(4)]\n\nbench(fn, large_inputs, \"Eager\")\nbench(aot_nnc_fn, large_inputs, \"AOT\")\n```\n\n----------------------------------------\n\nTITLE: Setting Default CMake Build Type in CMake\nDESCRIPTION: Checks if the CMAKE_BUILD_TYPE variable is set. If not, it defaults the build type to 'Release' and caches this value with a description of available types. The FORCE keyword ensures this default overrides any previous non-cache value.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\n# Set default build type\nif(NOT CMAKE_BUILD_TYPE)\n  message(STATUS \"Build type not set - defaulting to Release\")\n  set(CMAKE_BUILD_TYPE\n      \"Release\"\n      CACHE\n        STRING\n        \"Choose the type of build from: Debug Release RelWithDebInfo MinSizeRel Coverage.\"\n        FORCE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Analyzing Softmax Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of aten._softmax.default and aten._softmax_backward_data.default operators with various tensor shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/attention_is_all_you_need_pytorch_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 6, ((T([256, 8, 33, 33], f16), -1, False), {})\ncnt: 6, ((T([256, 8, 31, 31], f16), -1, False), {})\ncnt: 6, ((T([256, 8, 31, 33], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 6, ((T([256, 8, 31, 33], f16), T([256, 8, 31, 33], f16), -1, f16), {})\ncnt: 6, ((T([256, 8, 31, 31], f16), T([256, 8, 31, 31], f16), -1, f16), {})\ncnt: 6, ((T([256, 8, 33, 33], f16), T([256, 8, 33, 33], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Handling Ragged Structure Incompatibility in PyTorch\nDESCRIPTION: Shows how to work around errors when operating on NJTs with incompatible ragged structures by constructing them with the same offsets.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/nested.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt2 = torch.nested.nested_tensor_from_jagged(values=torch.randn(82, 128), offsets=nt1.offsets())\n>>> nt3 = nt1 + nt2\n>>> nt3.shape\ntorch.Size([2, j1, 128])\n```\n\n----------------------------------------\n\nTITLE: Export Path Documentation in RestructuredText\nDESCRIPTION: Documentation for PyTorch's export functionality which is in prototype stage and may have breaking changes in future releases, formatted in RestructuredText.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_8\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n.. warning::\n    This feature is a prototype and may have compatibility breaking changes in the future.\n\n    export\n    generated/exportdb/index\n```\n\n----------------------------------------\n\nTITLE: Running TIMM Models Performance Benchmarks (Python)\nDESCRIPTION: Commands to run TIMM models performance benchmarks for both training and inference using TorchInductor backend. The commands specify device, precision, and output format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n./benchmarks/dynamo/timm_models.py --performance --training --amp --backend=inductor --output=timm_models_training.csv\n./benchmarks/dynamo/timm_models.py --performance --inference --bfloat16 --backend=inductor --output=timm_models_inference.csv\n```\n\n----------------------------------------\n\nTITLE: Translating a Python For Loop to TorchScript IR\nDESCRIPTION: Shows how a simple Python for loop is translated into TorchScript IR using prim::Loop. The example demonstrates multiplication of a tensor by itself for each element in its first dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n    z = x\n    for i in range(x.size(0)):\n        z = z * z\n    return z\n```\n\nLANGUAGE: python\nCODE:\n```\ngraph(%z.1 : Dynamic):\n  %3 : bool = prim::Constant[value=1]()\n  %1 : int = prim::Constant[value=0]()\n  %2 : int = aten::size(%z.1, %1)\n  %z : Dynamic = prim::Loop(%2, %3, %z.1)\n    block0(%i : int, %5 : Dynamic):\n      %z.2 : Dynamic = aten::mul(%5, %5)\n      -> (%3, %z.2)\n  return (%z)\n```\n\n----------------------------------------\n\nTITLE: Graph Transformation Debugging\nDESCRIPTION: Example of how to implement and debug graph transformations using FX.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    g = tracer_class().trace(module)\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n    return fx.GraphModule(module, g)\n\ntransformed = transform_graph(traced)\nprint(transformed)\n```\n\n----------------------------------------\n\nTITLE: Comparison Operations List in RestructuredText\nDESCRIPTION: A list of comparison and sorting operations available in PyTorch, formatted as a RestructuredText autosummary that will generate individual documentation pages for each function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n    :toctree: generated\n    :nosignatures:\n\n    allclose\n    argsort\n    eq\n    equal\n    ge\n    greater_equal\n    gt\n    greater\n    isclose\n    isfinite\n    isin\n    isinf\n    isposinf\n    isneginf\n    isnan\n    isreal\n    kthvalue\n    le\n    less_equal\n    lt\n    less\n    maximum\n    minimum\n    fmax\n    fmin\n    ne\n    not_equal\n    sort\n    topk\n    msort\n```\n\n----------------------------------------\n\nTITLE: Subclassing for Generated Code Debugging\nDESCRIPTION: Demonstrates how to debug generated code by creating a subclass with the generated forward function for comparison.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\npre_trace = M()\npost_trace = SubclassM()\n```\n\n----------------------------------------\n\nTITLE: Generating function documentation for window functions in reStructuredText\nDESCRIPTION: Auto-summary directive to generate documentation for various window functions available in the torch.signal.windows module, creating separate documentation pages for each function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/signal.rst#2025-04-22_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    bartlett\n    blackman\n    cosine\n    exponential\n    gaussian\n    general_cosine\n    general_hamming\n    hamming\n    hann\n    kaiser\n    nuttall\n```\n\n----------------------------------------\n\nTITLE: Defining Upgrader in TorchScript\nDESCRIPTION: Demonstrates how to define the upgrader for the 'linspace' operator using TorchScript in the kUpgradersEntryMap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nstatic std::unordered_map<std::string, std::string> kUpgradersEntryMap(\n    {\n      {\"linspace_0_7\", R\"SCRIPT(\ndef linspace_0_7(start: Union[int, float, complex], end: Union[int, float, complex], steps: Optional[int], *, dtype: Optional[int], layout: Optional[int],\n                  device: Optional[Device], pin_memory: Optional[bool]):\n  if (steps is None):\n    return torch.linspace(start=start, end=end, steps=100, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory)\n  return torch.linspace(start=start, end=end, steps=steps, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory)\n)SCRIPT\"},\n    }\n\n```\n\n----------------------------------------\n\nTITLE: Batch Matrix Multiplication with aten.bmm in PyTorch\nDESCRIPTION: Executes batch matrix-matrix product over tensors of varying shapes characteristic of minibatch operations, emphasizing f16 processing. PyTorch is required, with CUDA facilitating rapid execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\naten.bmm.default, ((T([64, 128, 32], f16), T([64, 32, 128], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Setting MPS Allocator High Watermark Ratio (Environment Variable)\nDESCRIPTION: Configures the high watermark ratio for the MPS allocator using `PYTORCH_MPS_HIGH_WATERMARK_RATIO`. This acts as a hard limit for total allowed allocations relative to the device's recommended maximum working set size. Default is 1.7. A value of 0.0 disables the limit, 1.0 corresponds to the recommended max, and >1.0 allows exceeding it.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nPYTORCH_MPS_HIGH_WATERMARK_RATIO\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables and Building CUDA-Based PyTorch on Windows - cmd\nDESCRIPTION: This Windows batch script sets essential environment variables for CUDA and MKL, configures the CMake toolchain and overriding toolsets, and invokes the build in development mode. Run this after downloading and unpacking MKL and installing CUDA with Nsight Compute and Visual Studio. Adjust paths to match your local installations. The script is needed when building PyTorch with CUDA support for GPU acceleration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_11\n\nLANGUAGE: cmd\nCODE:\n```\ncmd\\n\\n:: Set the environment variables after you have downloaded and unzipped the mkl package,\\n:: else CMake would throw an error as `Could NOT find OpenMP`.\\nset CMAKE_INCLUDE_PATH={Your directory}\\mkl\\include\\nset LIB={Your directory}\\mkl\\lib;%LIB%\\n\\n:: Read the content in the previous section carefully before you proceed.\\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\\n:: \"Visual Studio 2019 Developer Command Prompt\" will be run automatically.\\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\\nset DISTUTILS_USE_SDK=1\\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,17^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\\n\\n:: [Optional] If you want to override the CUDA host compiler\\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exe\\n\\npython setup.py develop\\n\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Patterns in Neural Network Execution\nDESCRIPTION: This data represents a profiling report of PyTorch operators used during model execution, showing tensor shapes, counts, and data types. The statistics include common neural network operations like convolutions, softmax, pooling, and matrix multiplications with their specific tensor dimensions and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/botnet26t_256_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 2, ((T([512, 256, 256], f16), -1, False), {})\ncnt: 1, ((T([512, 64, 64], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 1, ((T([512, 64, 64], f16), T([512, 64, 64], f16), -1, f16), {})\ncnt: 2, ((T([512, 256, 256], f16), T([512, 256, 256], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 3, ((T([128, 256, 16, 16], f16), [512, 64, 256]), {})\ncnt: 2, ((T([512, 256, 256], f16), [512, 256, 256]), {})\ncnt: 2, ((T([512, 16, 16, 64], f16), [131072, 64]), {})\ncnt: 4, ((T([131072, 31], f16), [512, 16, 16, 31]), {})\ncnt: 2, ((T([512, 16, 16, 16, 16], f16), [512, 256, 256]), {})\ncnt: 1, ((T([512, 256, 64], f16), [512, 256, 64]), {})\ncnt: 3, ((T([512, 64, 256], f16), [128, 256, 16, 16]), {})\ncnt: 3, ((T([128, 512, 16, 16], f16), [512, 128, 256]), {})\ncnt: 2, ((T([512, 16, 16, 128], f16), [131072, 128]), {})\ncnt: 1, ((T([512, 256, 128], f16), [512, 256, 128]), {})\ncnt: 3, ((T([512, 128, 256], f16), [128, 512, 16, 16]), {})\ncnt: 3, ((T([128, 512, 8, 8], f16), [512, 128, 64]), {})\ncnt: 1, ((T([512, 64, 64], f16), [512, 64, 64]), {})\ncnt: 2, ((T([512, 8, 8, 128], f16), [32768, 128]), {})\ncnt: 2, ((T([32768, 15], f16), [512, 8, 8, 15]), {})\ncnt: 1, ((T([512, 8, 8, 8, 8], f16), [512, 64, 64]), {})\ncnt: 1, ((T([512, 64, 128], f16), [512, 64, 128]), {})\ncnt: 3, ((T([512, 128, 64], f16), [128, 512, 8, 8]), {})\ncnt: 1, ((T([512, 8, 8, 128], f16), [512, 64, 128]), {})\ncnt: 1, ((T([512, 16, 16, 128], f16), [512, 256, 128]), {})\ncnt: 1, ((T([512, 16, 16, 64], f16), [512, 256, 64]), {})\nOperator: aten.add.Tensor\ncnt: 31, ((T([], i64), 1), {})\ncnt: 4, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16)), {})\ncnt: 4, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16)), {})\ncnt: 4, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16)), {})\ncnt: 2, ((T([512, 16, 16, 16, 16], f16, stride=(8432, 31, 527, 1, 0)), T([512, 16, 16, 16, 16], f16, stride=(8432, 527, 31, 0, 1))), {})\ncnt: 2, ((T([512, 256, 256], f16), T([512, 256, 256], f16)), {})\ncnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16)), {})\ncnt: 1, ((T([512, 8, 8, 8, 8], f16, stride=(1080, 15, 135, 1, 0)), T([512, 8, 8, 8, 8], f16, stride=(1080, 135, 15, 0, 1))), {})\ncnt: 1, ((T([512, 64, 64], f16), T([512, 64, 64], f16)), {})\ncnt: 1, ((T([512, 8, 8, 128], f16, stride=(8192, 128, 1024, 1)), T([512, 8, 8, 128], f16)), {})\ncnt: 1, ((T([512, 64, 128], f16), T([512, 64, 128], f16)), {})\ncnt: 1, ((T([512, 16, 16, 128], f16, stride=(32768, 128, 2048, 1)), T([512, 16, 16, 128], f16)), {})\ncnt: 1, ((T([512, 256, 128], f16), T([512, 256, 128], f16)), {})\ncnt: 1, ((T([512, 16, 16, 64], f16, stride=(16384, 64, 1024, 1)), T([512, 16, 16, 64], f16)), {})\ncnt: 1, ((T([512, 256, 64], f16), T([512, 256, 64], f16)), {})\ncnt: 1, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([128, 512, 16, 16], f16), [2, 2], [2, 2]), {})\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([128, 512, 8, 8], f16), T([128, 512, 16, 16], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})\nOperator: aten.bmm.default\ncnt: 2, ((T([512, 256, 64], f16, stride=(16384, 1, 256)), T([512, 64, 256], f16)), {})\ncnt: 2, ((T([512, 256, 256], f16), T([512, 256, 64], f16, stride=(16384, 1, 256))), {})\ncnt: 2, ((T([512, 256, 128], f16, stride=(32768, 1, 256)), T([512, 128, 256], f16)), {})\ncnt: 2, ((T([512, 256, 256], f16), T([512, 256, 128], f16, stride=(32768, 1, 256))), {})\ncnt: 2, ((T([512, 64, 128], f16, stride=(8192, 1, 64)), T([512, 128, 64], f16)), {})\ncnt: 2, ((T([512, 64, 64], f16), T([512, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 1, ((T([512, 64, 64], f16, stride=(4096, 1, 64)), T([512, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 1, ((T([512, 128, 64], f16), T([512, 64, 64], f16)), {})\ncnt: 1, ((T([512, 256, 256], f16, stride=(65536, 1, 256)), T([512, 256, 128], f16, stride=(32768, 1, 256))), {})\ncnt: 1, ((T([512, 128, 256], f16), T([512, 256, 256], f16)), {})\ncnt: 1, ((T([512, 256, 256], f16, stride=(65536, 1, 256)), T([512, 256, 64], f16, stride=(16384, 1, 256))), {})\ncnt: 1, ((T([512, 64, 256], f16), T([512, 256, 256], f16)), {})\nOperator: aten.cat.default\ncnt: 1, (([T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16)], 1), {})\ncnt: 1, (([T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16)], 1), {})\ncnt: 1, (([T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16)], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 256, 256], f16),), {})\nOperator: aten.constant_pad_nd.default\ncnt: 4, ((T([8192, 16, 31], f16), [0, 1], 0.0), {})\ncnt: 4, ((T([8192, 512], f16), [0, 15], 0.0), {})\ncnt: 2, ((T([4096, 8, 15], f16), [0, 1], 0.0), {})\ncnt: 2, ((T([4096, 128], f16), [0, 7], 0.0), {})\ncnt: 2, ((T([4096, 135], f16), [0, -7]), {})\ncnt: 2, ((T([4096, 8, 16], f16), [0, -1]), {})\ncnt: 4, ((T([8192, 527], f16), [0, -15]), {})\ncnt: 4, ((T([8192, 16, 32], f16), [0, -1]), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 256, 256], f16), T([24, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 24, 128, 128], f16), T([32, 24, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 64, 64], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 64, 64, 64], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 64, 64, 64], f16), T([256, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 64, 64], f16), T([64, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 64, 64], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 128, 32, 32], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 64, 64], f16), T([512, 256, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 32, 32], f16), T([128, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 32, 32], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 32, 32], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([256, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 256, 16, 16], f16), T([1024, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 32, 32], f16), T([1024, 512, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1024, 16, 16], f16), T([256, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), T([768, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1024, 16, 16], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([1536, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 512, 8, 8], f16), T([2048, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1024, 16, 16], f16), T([2048, 1024, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([512, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 8, 8], f16), T([1536, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 2, ((T([128, 2048, 8, 8], f16), T([128, 512, 8, 8], f16), T([2048, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1536, 8, 8], f16), T([128, 512, 8, 8], f16), T([1536, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 512, 8, 8], f16), T([128, 2048, 8, 8], f16), T([512, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([128, 1024, 16, 16], f16), T([2048, 1024, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1536, 16, 16], f16), T([128, 512, 16, 16], f16), T([1536, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([128, 1024, 16, 16], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 1024, 16, 16], f16), T([128, 256, 16, 16], f16), T([1024, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 768, 16, 16], f16), T([128, 256, 16, 16], f16), T([768, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), T([128, 1024, 16, 16], f16), T([256, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1024, 16, 16], f16), T([128, 512, 32, 32], f16), T([1024, 512, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Exporting Test Models for Backward Compatibility Testing in PyTorch\nDESCRIPTION: Command to export the test model to the fixtures directory by running the generate_models.py script. This creates serialized models that will be used to test backward compatibility after operator changes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython test/jit/fixtures_src/generate_models.py\n```\n\n----------------------------------------\n\nTITLE: Miscellaneous Tensor and Operator Utility Functions (PyTorch ATen C++)\nDESCRIPTION: This group includes tensor option merging, memory format suggestions, and miscellaneous kernel and operator meta functions for types, scalar operations, operator redispatch, device queries, and result typing. Dependencies and context are found in the broader ATen/torch core libraries. Inputs and outputs are varied but generally concern core tensor/parameter types. Limitations are imposed by the API stability and internal calling conventions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_21\n\nLANGUAGE: C++\nCODE:\n```\n_ZNK3c1013TensorOptions8merge_inES0_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4meta14structured_cat4metaERKN3c108IListRefINS_6TensorEEEl\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native11result_typeEN3c108IListRefINS_6TensorEEE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native14_reshape_aliasERKNS_6TensorEN3c108ArrayRefIlEES6_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native8_to_copyERKNS_6TensorESt8optionalIN3c1010ScalarTypeEES4_INS5_6LayoutEES4_INS5_6DeviceEES4_IbEbS4_INS5_12MemoryFormatEE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZNK3c108ArrayRefIlE3vecEv.isra.0\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZNK3c108ArrayRefIlE6equalsES1_.isra.0\n```\n\n----------------------------------------\n\nTITLE: Compilation Time Profiling Example\nDESCRIPTION: Demonstrates how to profile the compilation process itself, including a warm-up compilation step and the main model compilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_profiling_torch_compile.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchvision.models import resnet18\n\ndevice = 'cuda'\nmodel = resnet18().to(device)\ninputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]\n\nmodel_c = torch.compile(model)\n\ndef fwd_bwd(inp):\n    out = model_c(inp)\n    out.sum().backward()\n\ndef warmup_compile():\n    def fn(x):\n        return x.sin().relu()\n\n    x = torch.rand((2, 2), device=device, requires_grad=True)\n    fn_c = torch.compile(fn)\n    out = fn_c(x)\n    out.sum().backward()\n\nwith torch.profiler.profile() as prof:\n    with torch.profiler.record_function(\"warmup compile\"):\n        warmup_compile()\n\n    with torch.profiler.record_function(\"resnet18 compile\"):\n        fwd_bwd(inputs[0])\n\nprof.export_chrome_trace(\"trace_compile.json\")\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Backward Operations\nDESCRIPTION: Backward pass for convolution operations including gradient computation for weights and inputs\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\naten.convolution_backward.default(T([128, 1280, 1, 1], f16), T([128, 960, 1, 1], f16), T([1280, 960, 1, 1], f16), [1280], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True])\n```\n\n----------------------------------------\n\nTITLE: Unsupported Data-dependent Control Flow with vmap\nDESCRIPTION: This example shows how vmap does not support functions with data-dependent control flow, where conditional statements depend on tensor values being vmapped over.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef relu(x):\n  if x > 0:\n    return x\n  return 0\n\nx = torch.randn(3)\nvmap(relu)(x)\n```\n\n----------------------------------------\n\nTITLE: Tensor Copy Operations\nDESCRIPTION: Copy operations between tensors with different shapes and strides, primarily working with half-precision (f16) data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n((T([3, 799, 1199], f16, stride=(1439744, 1216, 1)), T([3, 799, 1199], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Generating Tracing Code for TorchScript Operators\nDESCRIPTION: Shows an example of the C++ code generated for tracing TorchScript operators. This code creates nodes in the Graph being traced to record operator execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::jit::Node* node = nullptr;\nstd::shared_ptr<jit::tracer::TracingState> tracer_state;\nif (jit::tracer::isTracing()) {\n        tracer_state = jit::tracer::getTracingState();\n        at::Symbol op_name;\n        op_name = jit::Symbol::fromQualString(\"aten::__ilshift__\");\n        node = tracer_state->graph->create(op_name, /*num_outputs=*/0);\n        jit::tracer::recordSourceLocation(node);\n        jit::tracer::addInputs(node, \"self\", self);\n        jit::tracer::addInputs(node, \"other\", other);\n        tracer_state->graph->insertNode(node);\n\n        jit::tracer::setTracingState(nullptr);\n}\nTypeDefault::__ilshift__(self, other);\nif (tracer_state) {\n        jit::tracer::setTracingState(std::move(tracer_state));\n        jit::tracer::addOutput(node, self);\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Commit Data with Python\nDESCRIPTION: This Python code processes commit data by reading commit information into a DataFrame, filtering it based on the latest known commit hash, and exporting the cherry-pick related data as a CSV. It uses the pandas library and assumes the existence of a commit hash CSV file. The output is saved and then transformed into markdown files categorized by commit type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/README.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport pandas as pd\n\ncommit_list_df = pd.read_csv(\"results/commitlist.csv\")\nlast_known_good_hash = \"<the most recent hash>\"\n\nprevious_index = commit_list_df[commit_list_df.commit_hash == last_known_good_hash].index.values[0]\ncherry_pick_df = commit_list_df.iloc[previous_index+1:]\npath = \"<your_path>/cherry_picks.csv\"\ncherry_pick_df.to_csv(path, index=False)\n\n\nfrom commitlist import CommitList, to_markdown\ncherry_pick_commit_list = CommitList.from_existing(path)\n\nimport os\ncategories = list(cherry_pick_commit_list.stat().keys())\nfor category in categories:\n    print(f\"Exporting {category}...\")\n    lines =to_markdown(cherry_pick_commit_list, category)\n    filename = f'/tmp/cherry_pick/results/result_{category}.md'\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    with open(filename, 'w') as f:\n        f.writelines(lines)\n```\n\n----------------------------------------\n\nTITLE: Handling Unsafe Globals in NumPy < 1.25 for PyTorch Serialization\nDESCRIPTION: Example of an error message when encountering unsafe globals with NumPy < 1.25 during PyTorch serialization, and how to allowlist the problematic type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nWeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,\nbut got <class 'numpy.dtype[float32]'>\n```\n\n----------------------------------------\n\nTITLE: Creating a TensorExpr C++ Test File Template\nDESCRIPTION: Provides a basic template for a new TensorExpr C++ test file (e.g., `test_foo.cpp`). It includes the necessary header `test/cpp/tensorexpr/test_base.h` and demonstrates defining test cases as `void()` functions starting with the prefix `test` within the `torch::jit` namespace.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/tensorexpr/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <test/cpp/tensorexpr/test_base.h>\n\n// Tests go in torch::jit\nnamespace torch {\nnamespace jit {\n\n// 1. Test cases are void() functions.\n// 2. They start with the prefix `test`\nvoid testCaseOne() {\n    // ...\n}\n\nvoid testCaseTwo() {\n    // ...\n}\n}\n}\n```\n\n----------------------------------------\n\nTITLE: PyTorch IR Statement (Stmt) Definition\nDESCRIPTION: Defines the structure of Statement nodes in the PyTorch IR. Statements represent operations like memory allocation, conditionals, loops, and memory operations. Each statement type has specific fields that define its behavior and relation to other IR components.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/tensorexpr/IRSpecification.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nStmt\n= Block(stmts_ = [Stmt])\n| Store(buf_ = Buf, indices = [Expr], value_ = Expr, mask_ = Expr)\n| Allocate(buf_ = Buf)\n| Free(buf_ = Buf)\n| PlacementAllocate(buf_ = Buf, buf_to_reuse_ = Buf)\n| Let(var_ = Var, val_ = Expr)\n| Cond(condition_ = Expr, true_stmt_ = Block, false_stmt_ = Block)\n| For(var_ = Var, start_ = Expr, stop_ = Expr, body_ = Block, loopOptions = LoopOptions)\n| AtomicAdd(buf_ = Buf, indices = [Expr], value_ = Expr)\n| SyncThreads()\n| ExternalCall(buf_ = Buf, buf_args_ = [Buf], args_ = [Expr])\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Tensor Operations with GPU and CPU in PyTorch\nDESCRIPTION: This code snippet shows examples of tensor operations that fail due to automatic transfer limitations between GPU and CPU in PyTorch. It illustrates that scalar tensors are not automatically transferred between devices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.ones(()).cuda() + torch.ones(1)  # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU\n>>> torch.ones(1) + torch.ones(()).cuda()  # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Count Analysis\nDESCRIPTION: Counter for basic tensor operations with various shapes and data types (float16, int64). Shows frequency of operations with specific tensor configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([4, 5000, 4], f16),), {})\ncnt: 1, ((T([4, 5000], f16),), {})\ncnt: 1, ((T([4, 5000], i64),), {})\ncnt: 24, ((T([0, 1], i64), 1), {})\ncnt: 8, ((T([0, 4], f16), 1), {})\ncnt: 4, ((T([0, 182, 2], f16), 2), {})\ncnt: 1, ((T([0, 91, 4], f16), 2), {})\n```\n\n----------------------------------------\n\nTITLE: Unsafe View Reshaping of Tensors in PyTorch (Python)\nDESCRIPTION: Executes aten._unsafe_view for changing the shape of tensors without copying the data, which is preferred when transforming intermediate tensors within a computational graph requiring specific input sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\naten._unsafe_view.default\ncnt: 36, ((T([16, 12, 512, 64], f16), [192, 512, 64]), {})\ncnt: 12, ((T([16, 12, 64, 512], f16), [192, 64, 512]), {})\ncnt: 12, ((T([192, 512, 512], f16), [16, 12, 512, 512]), {})\ncnt: 12, ((T([192, 512, 64], f16), [16, 12, 512, 64]), {})\ncnt: 24, ((T([16, 512, 12, 64], f16), [16, 512, 768]), {})\ncnt: 12, ((T([16, 512, 768], f16), [8192, 768]), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Element-wise Multiplication Operations in PyTorch\nDESCRIPTION: Log of element-wise multiplication operations between tensors. These operations are used for applying masks, attention weights, or implementing gating mechanisms in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 4, ((T([64, 64, 64, 64], f16), T([64, 64, 1, 1], f16)), {})\ncnt: 4, ((T([64, 128, 32, 32], f16), T([64, 128, 1, 1], f16)), {})\ncnt: 2, ((T([256, 1024, 1024], f16), 0.1767766952966369), {})\ncnt: 4, ((T([64, 256, 16, 16], f16), T([64, 256, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Minimal TensorOptions Syntax\nDESCRIPTION: Demonstrates the most concise way to specify tensor options for a single property.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::ones(10, torch::kFloat32)\n```\n\n----------------------------------------\n\nTITLE: Adding Quantized Dispatch to Existing Operator in native_functions.yaml\nDESCRIPTION: Illustrates how to add a quantized CPU implementation (`quantized_xand`) to an existing operator definition (e.g., `xand`) in `ATen/native/native_functions.yaml`. It involves adding a new key-value pair (`QuantizedCPU: quantized_xand`) under the `dispatch` section for the relevant `func` entry.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\n- func: xand(Tensor a, Tensor b) -> Tensor\n  dispatch:\n    CPU: _xand_cpu     # Assume this existed\n    CUDA: _xand_cuda   # Assume this existed\n    QuantizedCPU: quantized_xand\n```\n\n----------------------------------------\n\nTITLE: Running Max Autotune\nDESCRIPTION: Command to run kernel benchmarking with maximum autotuning enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_inductor_profiling.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py\n```\n\n----------------------------------------\n\nTITLE: Adding Multi-Worker Support for Model Predictions in PyTorch\nDESCRIPTION: Introduces a --num_workers option in server.py to allow multiple workers in the ThreadPoolWorker for model predictions. Each worker uses its own CUDA stream created during thread initialization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/CHANGELOG.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nparser.add_argument('--num_workers', type=int, default=1)\n\n# In worker initialization\nworker_stream = torch.cuda.Stream()\nwith torch.cuda.stream(worker_stream):\n    # Perform model prediction\n```\n\n----------------------------------------\n\nTITLE: Initializing Single-threaded Static Runtime in PyTorch\nDESCRIPTION: Shows two ways to initialize Static Runtime in single-threaded mode with intra-op parallelism. The runtime instance executes the TorchScript module with optimized memory management.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/runtime/static/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n  // m is the TorchScript module\n  auto runtime = StaticRuntime(m, opts);\n  auto output = runtime.run(args, kwargs);\n```\n\nLANGUAGE: cpp\nCODE:\n```\n  auto mod = PrepareForStaticRuntime(m);\n  auto runtime = StaticRuntime(mod, opts);\n  auto output = runtime.run(args, kwargs);\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Addition Operations\nDESCRIPTION: Various tensor addition operations with different shapes and dimensions, primarily using float16 dtype\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten.add.Tensor(T([], i64), 1)\naten.add.Tensor(T([64, 32, 112, 112], f16), T([64, 32, 112, 112], f16))\n```\n\n----------------------------------------\n\nTITLE: Profiling specific FastRNNs implementations with nvprof\nDESCRIPTION: Command to profile specific RNN implementations (ATEN and JIT) using NVIDIA's nvprof tool.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/fastrnns/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m fastrnns.profile --rnns aten jit\n```\n\n----------------------------------------\n\nTITLE: Loss Functions and Masking Utilities in PyTorch (Python)\nDESCRIPTION: Usage patterns for negative log-likelihood (nll_loss), scalar comparisons (ne, rsub), and slicing backward are shown, designed for tasks with masking, class indexing, and gradient flow. Inputs are either probability or label tensors; some take ignore-index or reduction arguments. Slicing/cumsum operate on high-dimensional tensors for sequential models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.ne.Scalar\ncnt: 1, ((T([4, 128], i64), 0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([508, 30522], f16), T([508], i64), None, 1, -100, T([], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([508, 30522], f16), T([508], i64), None, 1, -100), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.rsub.Scalar\ncnt: 1, ((T([4, 1, 1, 128], f16), 1.0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.slice_backward.default\ncnt: 1, ((T([4, 127, 30522], f16), [4, 127, 30522], 2, 0, 9223372036854775807, 1), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.slice_backward.default\ncnt: 1, ((T([4, 127, 30522], f16), [4, 128, 30522], 1, 0, -1, 1), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.slice_backward.default\ncnt: 1, ((T([4, 128, 30522], f16), [4, 128, 30522], 0, 0, 9223372036854775807, 1), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.cumsum.default\ncnt: 1, ((T([4, 128], i32), 1), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Operation Parameters\nDESCRIPTION: Multiple convolution operations with varying tensor shapes, strides and parameters for different layers of a neural network. Uses half-precision (f16) tensors with specific stride configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncnt: 6, ((T([64, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 80, 14, 14], f16, stride=(31360, 196, 14, 1)), T([240, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Executing Log Softmax Operation in PyTorch\nDESCRIPTION: Performs the log softmax operation on a tensor of shape [256, 256008] with data type f16 and a specified dimension. No additional configurations are applied. Ensures numerical stability in softmax operations for large values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"Operator: aten._log_softmax.default\\ncnt: 1, ((T([256, 256008], f16), 1, False), {})\"\"\"\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Multiplication Operations\nDESCRIPTION: Matrix multiplication operations between tensors with different shapes and strides using half-precision format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_xception65_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n((T([32, 1000], f16), T([1000, 2048], f16)), {})\n((T([1000, 32], f16, stride=(1, 1000)), T([32, 2048], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Batch Normalization Operations\nDESCRIPTION: This snippet shows the usage patterns of the native_batch_norm_backward operator in PyTorch. It details the tensor shapes, data types, and hyperparameters used across different layers of a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([32, 1280, 7, 7], f16), T([32, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 320, 7, 7], f16), T([32, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), False, 1e-05, [True, True, True]), {})\ncnt: 8, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f32), T([1152], f32), False, 1e-05, [True, True, True]), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Tensor View and Reshape Operations\nDESCRIPTION: Series of tensor view operations for reshaping attention mechanisms, including transformations between different shapes for multi-head attention calculations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_DistilBert_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten._unsafe_view.default(T([8, 12, 512, 64], f16), [96, 512, 64])\naten._unsafe_view.default(T([8, 12, 64, 512], f16), [96, 64, 512])\naten._unsafe_view.default(T([96, 512, 512], f16), [8, 12, 512, 512])\n```\n\n----------------------------------------\n\nTITLE: Logged Tensor Operations (Operator Unspecified)\nDESCRIPTION: Log entries showing tensor arguments and counts for operations where the specific ATen operator name was not specified immediately preceding these lines in the provided text. Each line shows a call count (`cnt`) and a tuple representing arguments, including tensor descriptions (T([...], dtype, stride=(...))) and other parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([64, 1, 128], f16, stride=(51328, 128, 1)), [128], T([128], f16), T([128], f16), 1e-06), {})\ncnt: 3, ((T([64, 1, 256], f16, stride=(50432, 256, 1)), [256], T([256], f16), T([256], f16), 1e-06), {})\ncnt: 3, ((T([64, 1, 256], f16), [256], T([256], f16), T([256], f16), 1e-06), {})\ncnt: 3, ((T([64, 1, 128], f16), [128], T([128], f16), T([128], f16), 1e-06), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing _softmax Operator Calls in PyTorch with Various Tensor Dimensions\nDESCRIPTION: Records of aten._softmax.default operator calls with different tensor shapes. All operations use half-precision (f16) tensors with dimension 1 as the softmax dimension and False for the third parameter.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 1, ((T([32, 2, 1, 64], f16), 1, False), {})\ncnt: 1, ((T([32, 2, 1, 128], f16), 1, False), {})\ncnt: 1, ((T([32, 2, 1, 256], f16), 1, False), {})\ncnt: 1, ((T([32, 2, 1, 512], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Threshold Backward Operation in PyTorch\nDESCRIPTION: This code snippet shows the usage of the threshold_backward operation in PyTorch. It includes the input tensor shapes and the threshold value used for each operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 2, ((T([32, 240, 1, 1], f16), T([32, 240, 1, 1], f16), 0), {})\ncnt: 2, ((T([32, 168, 1, 1], f16), T([32, 168, 1, 1], f16), 0), {})\ncnt: 1, ((T([32, 120, 1, 1], f16), T([32, 120, 1, 1], f16), 0), {})\ncnt: 2, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), 0), {})\ncnt: 4, ((T([32, 120, 28, 28], f16), T([32, 120, 28, 28], f16), 0), {})\ncnt: 1, ((T([32, 24, 1, 1], f16), T([32, 24, 1, 1], f16), 0), {})\ncnt: 1, ((T([32, 72, 28, 28], f16), T([32, 72, 28, 28], f16), 0), {})\ncnt: 3, ((T([32, 72, 56, 56], f16), T([32, 72, 56, 56], f16), 0), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16), 0), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), 0), {})\ncnt: 1, ((T([32, 16, 112, 112], f16), T([32, 16, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Input Tensor Cloning in PyTorch MobileNetV3\nDESCRIPTION: Operation to clone the input tensor. This is typically done to ensure the input isn't modified during processing or to create a separate copy for operations that might modify the tensor in-place.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations\nDESCRIPTION: Matrix multiplication operations between tensors of different shapes, including operations with stride configurations and half-precision (f16) format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([64, 1000], f16), T([1000, 1536], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Scalar Tensor Subtraction with rsub in PyTorch (Python)\nDESCRIPTION: Applies aten.rsub which allows the subtraction of a scalar from each element in a tensor. It's useful in adjusting values throughout the elements of a tensor by a constant amount.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\naten.rsub.Scalar\ncnt: 1, ((T([16, 1, 1, 512], f16), 1.0), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module in PyTorch Sphinx Documentation\nDESCRIPTION: Uses the currentmodule directive to set the context for subsequent documentation. The module name is dynamically inserted using Jinja2 template syntax.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/classtemplate.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: {{ module }}\n```\n\n----------------------------------------\n\nTITLE: Feature-based Mask Computation in PyTorch\nDESCRIPTION: Shows how masks are computed when working with specific features or channels in the activation tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# when features = None, mask is a tensor computed on the entire activation tensor\n# otherwise, mask is a list of tensors of length = len(features), computed on each feature of activations\n\n# On a high level, this is how the mask is computed if features is not None\nfor i in range(len(features)):\n   aggregated_tensor_feature = aggregate_fn([activation[features[i]] for activation in all_activations])\n   mask[i] = mask_fn(reduce_fn(aggregated_tensor_feature))\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Vision JNI Library Build\nDESCRIPTION: Configures CMake build settings for a shared library that provides JNI bindings for PyTorch Vision. Sets C++17 standard, enables verbose build output, configures source files, and sets up compilation options for exception handling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android_torchvision/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.4.1)\nproject(pytorch_vision_jni CXX)\nset(CMAKE_CXX_STANDARD 17 CACHE STRING \"The C++ standard whose features are requested to build this target.\")\nset(CMAKE_VERBOSE_MAKEFILE ON)\n\nset(pytorch_vision_cpp_DIR ${CMAKE_CURRENT_LIST_DIR}/src/main/cpp)\n\nfile(GLOB pytorch_vision_SOURCES\n  ${pytorch_vision_cpp_DIR}/pytorch_vision_jni.cpp\n)\n\nadd_library(pytorch_vision_jni SHARED\n    ${pytorch_vision_SOURCES}\n)\n\ntarget_compile_options(pytorch_vision_jni PRIVATE\n  -fexceptions\n)\n\nset(BUILD_SUBDIR ${ANDROID_ABI})\n\ntarget_link_libraries(pytorch_vision_jni)\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch ReLU Activation Operations\nDESCRIPTION: This snippet shows the usage of the aten.relu_.default operator for applying the ReLU activation function in-place on tensors with various shapes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 5, ((T([3, 64, 512, 512], f16),), {})\ncnt: 5, ((T([3, 128, 256, 256], f16),), {})\ncnt: 17, ((T([3, 256, 128, 128], f16),), {})\ncnt: 1, ((T([3, 64, 512, 512], f16, stride=(16777216, 1, 32768, 64)),), {})\ncnt: 1, ((T([3, 128, 256, 256], f16, stride=(8388608, 1, 32768, 128)),), {})\n```\n\n----------------------------------------\n\nTITLE: Leaf Modules Example in PyTorch FX\nDESCRIPTION: Demonstrates how leaf modules are handled during symbolic tracing with custom submodules.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n```\n\n----------------------------------------\n\nTITLE: Scripting, Serializing, and Loading a PyTorch Module\nDESCRIPTION: Demonstrates how to script a PyTorch module, save it to a file, and then load it back.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n>>> scripted_module = torch.jit.script(MyModule())\n>>> torch.jit.save(scripted_module, 'mymodule.pt')\n>>> torch.jit.load('mymodule.pt')\nRecursiveScriptModule( original_name=MyModule\n                      (l0): RecursiveScriptModule(original_name=Linear)\n                      (l1): RecursiveScriptModule(original_name=Linear) )\n```\n\n----------------------------------------\n\nTITLE: Generating Initial Commit List in Python\nDESCRIPTION: Command to create a new commit list CSV file containing commits between two specified versions\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython commitlist.py --create_new tags/v1.13.1 <commit_hash>\n```\n\n----------------------------------------\n\nTITLE: Using Lazy Tensor for Dynamic Control Flow in PyTorch\nDESCRIPTION: This code demonstrates how to use Lazy Tensor to handle the same function that caused issues with jit.trace. It shows moving tensors to the 'lazy' device and executing the function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndev = \"lazy\"\nt_lazy = torch.ones(1).to(dev)\nmaybe_false_lazy = torch.BoolTensor([0]).to(dev)\nlazy_result = add_two_maybe(t_lazy, maybe_false_lazy)\n```\n\n----------------------------------------\n\nTITLE: Pixel Shuffle with Dimensions in PyTorch\nDESCRIPTION: Implements pixel shuffle operation for image upscaling using dimension objects.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef pixel_shuffle(img, upscale_factor=2):\n    h2, w2, c, b, h, w = dims(6)\n    h2.size = w2.size = upscale_factor\n    return img[b, (c, h2, w2), h, w].order(b, c, (h, h2), (w, w2))\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Commit Data\nDESCRIPTION: Loads commit data from CSV file and analyzes uncategorized commits by author using pandas DataFrame operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/explore.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncommit_list_df = pd.read_csv(\"results/classifier/commitlist.csv\")\nmean_authors=commit_list_df.query(\"category == 'Uncategorized' & topic != 'not user facing'\").author.to_list()\ncounts = Counter(mean_authors)\ncommit_list_df.head()\n```\n\n----------------------------------------\n\nTITLE: Mapping Functions to Data with PyTorch DataPipes\nDESCRIPTION: Illustrates the use of the map() method to apply functions to data in DataPipes. Shows examples of mapping at different nesting levels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).map(lambda x: x * 2)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3).map(lambda x: x * 2)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3).batch(2).map(lambda x: x * 2, nesting_level = 2)\nfor i in dp:\n    print(i)\n```\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10).batch(3).batch(2).batch(2).map(lambda x: x * 2, nesting_level = -1)\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Loop-Level Implementation of Embedding Lookup\nDESCRIPTION: Shows the loop-based equivalent of the dimension-based embedding lookup operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor sequence in range(words.size(0)):\n    for features in range(embeddings.size(1)):\n        state = embeddings[words[sequence], features]\n```\n\n----------------------------------------\n\nTITLE: Dumping Tensor Dispatch Keys with Functorch (Python)\nDESCRIPTION: This Python code snippet refers to the `torch._C._functorch.dump_tensor` function, which is used for debugging within PyTorch's functorch module. It dumps the dispatch keys currently on the stack, aiding in understanding the dispatch mechanism.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n`torch._C._functorch.dump_tensor`\n```\n\n----------------------------------------\n\nTITLE: Manual Synchronization for Shared CUDA Memory in Python\nDESCRIPTION: Demonstrates a manual approach to synchronize shared CUDA memory between producer and consumer processes using a queue and an event. This method requires blocking the producer process and can become complicated with multiple consumers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/multiprocessing/cuda_multiprocessing.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Producer\nqueue.put(tensor)\nevent.wait()\n\n# Consumer\ntensor = queue.get()\nsafe_to_use_tensor = tensor.clone()\nevent.set()\n```\n\n----------------------------------------\n\nTITLE: Tensor Addition and Matrix Multiplication Operations in PyTorch Model\nDESCRIPTION: Code showing tensor addition, matrix multiplication and other basic operations used throughout the model. These operations handle residual connections, multi-headed attention mechanisms, and linear projections across different feature resolutions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([128, 192, 28, 28], f16), T([1, 192, 28, 28], f16)), {})\ncnt: 14, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16)), {})\ncnt: 1, ((T([128, 384, 14, 14], f16), T([1, 384, 14, 14], f16)), {})\ncnt: 16, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16)), {})\ncnt: 1, ((T([128, 768, 7, 7], f16), T([1, 768, 7, 7], f16)), {})\ncnt: 16, ((T([128, 768, 7, 7], f16), T([128, 768, 7, 7], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 28, ((T([], i64), 1), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 768], f16), T([768, 1000], f16, stride=(1, 768))), {})\nOperator: aten.bmm.default\ncnt: 4, ((T([768, 196, 64], f16), T([768, 64, 196], f16)), {})\ncnt: 4, ((T([768, 196, 196], f16), T([768, 196, 64], f16)), {})\ncnt: 4, ((T([768, 49, 128], f16), T([768, 128, 49], f16)), {})\ncnt: 4, ((T([768, 49, 49], f16), T([768, 49, 128], f16)), {})\ncnt: 4, ((T([768, 49, 49], f16, stride=(2401, 1, 49)), T([768, 49, 128], f16, stride=(6272, 1, 49))), {})\ncnt: 4, ((T([768, 49, 128], f16, stride=(6272, 1, 49)), T([768, 128, 49], f16, stride=(6272, 1, 128))), {})\ncnt: 4, ((T([768, 128, 49], f16, stride=(6272, 1, 128)), T([768, 49, 49], f16)), {})\ncnt: 4, ((T([768, 49, 49], f16), T([768, 49, 128], f16, stride=(6272, 1, 49))), {})\ncnt: 4, ((T([768, 196, 196], f16, stride=(38416, 1, 196)), T([768, 196, 64], f16, stride=(12544, 1, 196))), {})\ncnt: 4, ((T([768, 196, 64], f16, stride=(12544, 1, 196)), T([768, 64, 196], f16, stride=(12544, 1, 64))), {})\ncnt: 4, ((T([768, 64, 196], f16, stride=(12544, 1, 64)), T([768, 196, 196], f16)), {})\ncnt: 4, ((T([768, 196, 196], f16), T([768, 196, 64], f16, stride=(12544, 1, 196))), {})\n```\n\n----------------------------------------\n\nTITLE: Wrapping Constants with Tensors Example\nDESCRIPTION: Shows how to avoid recompilations by wrapping float constants with tensors in optimizer configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nmod = torch.nn.Linear(3, 3)\nopt = torch.optim.Adam(mod.parameters(), lr=torch.tensor(0.01))\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, torch.tensor(0.9))\n```\n\n----------------------------------------\n\nTITLE: Evaluating Forward Time for Sparsified DLRM Models in Python\nDESCRIPTION: Python script to evaluate forward pass execution time for sparsified DLRM models, comparing performance with and without torch.sparse tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/benchmarks/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython evaluate_forward_time.py --raw-data-file=<path_to_raw_data_txt_file> --processed-data-file=<path_to_kaggleAdDisplayChallenge_processed.npz> --sparse-model-metadata=<path_to_sparse_model_metadata_csv>\n```\n\n----------------------------------------\n\nTITLE: Running Code Coverage for Python Test\nDESCRIPTION: This snippet shows how to run the code coverage tool for a specific Python test file (test_complex.py).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython oss_coverage.py --run-only=test_complex.py\n```\n\n----------------------------------------\n\nTITLE: Describing Forward and Backward Convolution Operator Usage - PyTorch - Python\nDESCRIPTION: Enumerates varied argument signatures for the convolution and convolution_backward operators in PyTorch. Demonstrates both forward convolutions (kernels, strides, groups, and padding for various feature map sizes) and their paired backward pass operations accounting for gradients and multi-output (input grad, weight grad etc.). Supports variable tensor rank for both 2D and 1D convolutional cases, batching, and multiple input/output channels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 32, 112, 112], f16), T([32, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 32, 112, 112], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([64, 64, 56, 56], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([64, 64, 56, 56], f16), T([256, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([64, 1, 256], f16), T([1, 1, 5], f16), None, [1], [2], [1], False, [0], 1), {})\ncnt: 2, ((T([64, 256, 56, 56], f16), T([64, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 256, 56, 56], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 128, 56, 56], f16), T([128, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([64, 128, 28, 28], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([64, 1, 512], f16), T([1, 1, 5], f16), None, [1], [2], [1], False, [0], 1), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([512, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([64, 512, 28, 28], f16), T([128, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([64, 128, 28, 28], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 512, 28, 28], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([256, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 23, ((T([64, 256, 14, 14], f16), T([1024, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 23, ((T([64, 1, 1024], f16), T([1, 1, 5], f16), None, [1], [2], [1], False, [0], 1), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([1024, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 22, ((T([64, 1024, 14, 14], f16), T([256, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 22, ((T([64, 256, 14, 14], f16), T([256, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 1024, 14, 14], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([512, 512, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([64, 512, 7, 7], f16), T([2048, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([64, 1, 2048], f16), T([1, 1, 7], f16), None, [1], [3], [1], False, [0], 1), {})\ncnt: 1, ((T([64, 1024, 7, 7], f16), T([2048, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([64, 2048, 7, 7], f16), T([512, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([64, 512, 7, 7], f16), T([512, 512, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 3, ((T([64, 1, 2048], f16), T([64, 1, 2048], f16), T([1, 1, 7], f16), [0], [1], [3], [1], False, [0], 1, [True, True, False]), {})\ncnt: 3, ((T([64, 2048, 7, 7], f16), T([64, 512, 7, 7], f16), T([2048, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([64, 512, 7, 7], f16), T([64, 512, 7, 7], f16), T([512, 512, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([64, 512, 7, 7], f16), T([64, 2048, 7, 7], f16), T([512, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 2048, 7, 7], f16), T([64, 1024, 7, 7], f16), T([2048, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 512, 7, 7], f16), T([64, 512, 14, 14], f16), T([512, 512, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([64, 1024, 14, 14], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 23, ((T([64, 1, 1024], f16), T([64, 1, 1024], f16), T([1, 1, 5], f16), [0], [1], [2], [1], False, [0], 1, [True, True, False]), {})\ncnt: 23, ((T([64, 1024, 14, 14], f16), T([64, 256, 14, 14], f16), T([1024, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 22, ((T([64, 256, 14, 14], f16), T([64, 256, 14, 14], f16), T([256, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 22, ((T([64, 256, 14, 14], f16), T([64, 1024, 14, 14], f16), T([256, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 1024, 14, 14], f16), T([64, 512, 14, 14], f16), T([1024, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 256, 14, 14], f16), T([64, 256, 28, 28], f16), T([256, 256, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([64, 512, 28, 28], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([64, 1, 512], f16), T([64, 1, 512], f16), T([1, 1, 5], f16), [0], [1], [2], [1], False, [0], 1, [True, True, False]), {})\ncnt: 4, ((T([64, 512, 28, 28], f16), T([64, 128, 28, 28], f16), T([512, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([64, 128, 28, 28], f16), T([64, 128, 28, 28], f16), T([128, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([64, 128, 28, 28], f16), T([64, 512, 28, 28], f16), T([128, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 512, 28, 28], f16), T([64, 256, 28, 28], f16), T([512, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 28, 28], f16), T([64, 128, 56, 56], f16), T([128, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 128, 56, 56], f16), T([64, 256, 56, 56], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([64, 1, 256], f16), T([64, 1, 256], f16), T([1, 1, 5], f16), [0], [1], [2], [1], False, [0], 1, [True, True, False]), {})\ncnt: 4, ((T([64, 256, 56, 56], f16), T([64, 64, 56, 56], f16), T([256, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([64, 64, 56, 56], f16), T([64, 256, 56, 56], f16), T([64, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16), T([64, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 64, 112, 112], f16), T([64, 32, 112, 112], f16), T([64, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 32, 112, 112], f16), T([64, 32, 112, 112], f16), T([32, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 32, 112, 112], f16), T([64, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch with Docker\nDESCRIPTION: Command to build PyTorch using the docker.Makefile. Images are tagged as docker.io/${your_docker_username}/pytorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nmake -f docker.Makefile\n```\n\n----------------------------------------\n\nTITLE: Defining the Test Executable for Edge Operator Registration in CMake\nDESCRIPTION: Creates an executable target named `test_edge_op_registration`. It compiles the specified C++ source files (`test_operator_registration.cpp`, `test_main.cpp`). Defines the `USE_GTEST` preprocessor macro privately for this target, indicating its use of the Google Test framework.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(test_edge_op_registration\n        ${TEST_ROOT}/test_operator_registration.cpp\n        ${TEST_ROOT}/test_main.cpp\n        )\n\ntarget_compile_definitions(test_edge_op_registration PRIVATE USE_GTEST)\n```\n\n----------------------------------------\n\nTITLE: In-Place Operation Causing vmap Error - PyTorch - Python\nDESCRIPTION: This snippet demonstrates how using in-place PyTorch operations in batch transforms (vmap) can cause errors due to shape mismatch. The function 'f' attempts an in-place addition on tensor 'x' using 'y'. If the tensor shapes are incompatible under vmap, this results in a runtime error. Dependencies are PyTorch and torch.func. Inputs are singleton and batched tensors; output is a tensor (if successful). Limitation: Do not use in-place operations with mismatched shapes in vmap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(1)\ny = torch.randn(3, 1)  # When vmapped over, looks like it has shape [1]\n\n# Raises an error because `x` has fewer elements than `y`.\nvmap(f, in_dims=(None, 0))(x, y)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies with SHA256 Hashes for PyTorch Bazel Build\nDESCRIPTION: This snippet lists Python package dependencies with pinned versions and SHA256 hashes for security verification. The requirements include pyyaml, requests, sympy, typing-extensions, urllib3, and setuptools, all needed for the PyTorch Bazel build system.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/build/bazel/requirements.txt#2025-04-22_snippet_1\n\nLANGUAGE: pip\nCODE:\n```\n--hash=sha256:7ab55401287bfec946ced39700c053796e7cc0e3acbef09993a9ad2adba6ca6e \\\n    --hash=sha256:7e50d0a0cc3189f9cb0aeb3a6a6af18c16f59f004b866cd2be1c14b36134a4a0 \\\n    --hash=sha256:95a7476c59002f2f6c590b9b7b998306fba6a5aa646b1e22ddfeaf8f78c3a29c \\\n    --hash=sha256:96ff0b2ad353d8f990b63294c8986f1ec3cb19d749234014f4e7eb0112ceba5a \\\n    --hash=sha256:9fad7dcb1aac3c7f0584a5a8133e3a43eeb2fe127f47e3632d43d677c66c102b \\\n    --hash=sha256:9ff0f4f29c51e2803569d7a51c2304de5554655a60c5d776e35b4a41413830d0 \\\n    --hash=sha256:a354325ee03388678242a4d7ebcd08b5c727033fcff3b2f536aea978e15ee9e6 \\\n    --hash=sha256:a4abb4f9001ad2858e7ac189089c42178fcce737e4169dc61321660f1a96c7d2 \\\n    --hash=sha256:ab47dbe5cc8210f55aa58e4805fe224dac469cde56b9f731a4c098b91917159a \\\n    --hash=sha256:afedb719a9dcfc7eaf2287b839d8198e06dcd4cb5d276a3df279231138e83d30 \\\n    --hash=sha256:b3ce300f3644fb06443ee2222c2201dd3a89ea6040541412b8fa189341847218 \\\n    --hash=sha256:b97fe8060236edf3662adfc2c633f56a08ae30560c56310562cb4f95500022d5 \\\n    --hash=sha256:bfe25acf8b437eb2a8b2d49d443800a5f18508cd811fea3181723922a8a82b07 \\\n    --hash=sha256:cd25bcecc4974d09257ffcd1f098ee778f7834c3ad767fe5db785be9a4aa9cb2 \\\n    --hash=sha256:d209d8969599b27ad20994c8e41936ee0964e6da07478d6c35016bc386b66ad4 \\\n    --hash=sha256:d5241e0a80d808d70546c697135da2c613f30e28251ff8307eb72ba696945764 \\\n    --hash=sha256:edd8b5fe47dab091176d21bb6de568acdd906d1887a4584a15a9a96a1dca06ef \\\n    --hash=sha256:f870204a840a60da0b12273ef34f7051e98c3b5961b61b0c2c1be6dfd64fbcd3 \\\n    --hash=sha256:ffa75af20b44f8dba823498024771d5ac50620e6915abac414251bd971b4529f\n    # via -r tools/build/bazel/requirements.in\npyyaml==6.0.1 \\\n    --hash=sha256:04ac92ad1925b2cff1db0cfebffb6ffc43457495c9b3c39d3fcae417d7125dc5 \\\n    --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \\\n    --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \\\n    --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \\\n    --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \\\n    --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \\\n    --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \\\n    --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \\\n    --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \\\n    --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \\\n    --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \\\n    --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \\\n    --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \\\n    --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \\\n    --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \\\n    --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \\\n    --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \\\n    --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \\\n    --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \\\n    --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \\\n    --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \\\n    --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \\\n    --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \\\n    --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \\\n    --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \\\n    --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \\\n    --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \\\n    --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \\\n    --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \\\n    --hash=sha256:a08c6f0fe150303c1c6b71ebcd7213c2858041a7e01975da3a99aed1e7a378ef \\\n    --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \\\n    --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \\\n    --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \\\n    --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \\\n    --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \\\n    --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \\\n    --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \\\n    --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \\\n    --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \\\n    --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \\\n    --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \\\n    --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \\\n    --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \\\n    --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \\\n    --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \\\n    --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \\\n    --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \\\n    --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \\\n    --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \\\n    --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \\\n    --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f\n    # via -r tools/build/bazel/requirements.in\nrequests==2.32.2 \\\n    --hash=sha256:dd951ff5ecf3e3b3aa26b40703ba77495dab41da839ae72ef3c8e5d8e2433289 \\\n    --hash=sha256:fc06670dd0ed212426dfeb94fc1b983d917c4f9847c863f313c9dfaaffb7c23c\n    # via -r tools/build/bazel/requirements.in\nsympy==1.12 \\\n    --hash=sha256:c3588cd4295d0c0f603d0f2ae780587e64e2efeedb3521e46b9bb1d08d184fa5 \\\n    --hash=sha256:ebf595c8dac3e0fdc4152c51878b498396ec7f30e7a914d6071e674d49420fb8\n    # via -r tools/build/bazel/requirements.in\ntyping-extensions==4.11.0 \\\n    --hash=sha256:83f085bd5ca59c80295fc2a82ab5dac679cbe02b9f33f7d83af68e241bea51b0 \\\n    --hash=sha256:c1f94d72897edaf4ce775bb7558d5b79d8126906a14ea5ed1635921406c0387a\n    # via -r tools/build/bazel/requirements.in\nurllib3==2.2.2 \\\n    --hash=sha256:a448b2f64d686155468037e1ace9f2d2199776e17f0a46610480d311f73e3472 \\\n    --hash=sha256:dd505485549a7a552833da5e6063639d0d177c04f23bc3864e41e5dc5f612168\n    # via requests\n\n# The following packages are considered to be unsafe in a requirements file:\nsetuptools==70.0.0 \\\n    --hash=sha256:54faa7f2e8d2d11bcd2c07bed282eef1046b5c080d1c32add737d7b5817b1ad4 \\\n    --hash=sha256:f211a66637b8fa059bb28183da127d4e86396c991a942b028c6650d4319c3fd0\n    # via -r tools/build/bazel/requirements.in\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations in CUDA\nDESCRIPTION: Various tensor operation kernel signatures including convolution, attention, normalization, and basic math functions for GPU execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_34\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native28_efficient_attention_forwardERKNS_6TensorES3_S3_RKSt8optionalIS1_ES7_S7_S4_IlES8_dlbS4_IdES7_S7_\n_ZN2at6native44_scaled_dot_product_efficient_attention_cudaERKNS_6TensorES3_S3_RKSt8optionalIS1_EbdbS4_IdE\n_ZN2at6native40_scaled_dot_product_flash_attention_cudaERKNS_6TensorES3_S3_dbbSt8optionalIdE\n```\n\n----------------------------------------\n\nTITLE: Symbolic Numbers Documentation in RestructuredText\nDESCRIPTION: Documentation for PyTorch's symbolic number classes (SymInt, SymFloat, SymBool) used for dynamic shape handling and their related utility functions, formatted in RestructuredText.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_7\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: SymInt\n    :members:\n\n.. autoclass:: SymFloat\n    :members:\n\n.. autoclass:: SymBool\n    :members:\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    sym_float\n    sym_fresh_size\n    sym_int\n    sym_max\n    sym_min\n    sym_not\n    sym_ite\n    sym_sum\n```\n\n----------------------------------------\n\nTITLE: Configuring Tensor Expression Tests - CMake\nDESCRIPTION: The snippet uses CMake to set up testing and tutorial targets for tensor expressions. It conditions the inclusion of different source files and libraries based on available build options like CUDA, LLVM, and pthreadpool. It configures executables like 'test_tensorexpr' and 'tutorial_tensorexpr', linking against necessary libraries and specifying include directories. The configuration supports CUDA and ROCm environments, adjusting compiler definitions accordingly, and includes options to handle dependencies explicitly for some build environments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/tensorexpr/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TENSOREXPR_TEST_ROOT ${TORCH_ROOT}/test/cpp/tensorexpr)\n\nset(TENSOREXPR_TEST_SRCS\n  ${TENSOREXPR_TEST_ROOT}/test_approx.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_aten.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_boundsinference.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_conv.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_cpp_codegen.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_dynamic_shapes.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_expr.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_external_calls.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_graph_opt.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_ir_printer.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_ir_verifier.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_kernel.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_loopnest.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_memdependency.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_ops.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_quantization.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_memplanning.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_reductions.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_registerizer.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_simplify.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_te_fuser_pass.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_type.cpp\n  ${TENSOREXPR_TEST_ROOT}/test_type_specializations.cpp\n)\n\nif(USE_CUDA)\n  list(APPEND TENSOREXPR_TEST_SRCS ${TENSOREXPR_TEST_ROOT}/test_cuda.cpp)\nendif()\n\nif(USE_LLVM AND LLVM_FOUND)\n  list(APPEND TENSOREXPR_TEST_SRCS ${TENSOREXPR_TEST_ROOT}/test_llvm.cpp)\nendif()\n\nadd_executable(test_tensorexpr\n  ${TORCH_ROOT}/test/cpp/common/main.cpp\n  ${TENSOREXPR_TEST_ROOT}/padded_buffer.cpp\n  ${TENSOREXPR_TEST_SRCS})\n\ntarget_link_libraries(test_tensorexpr PRIVATE torch gtest)\ntarget_include_directories(test_tensorexpr PRIVATE ${ATen_CPU_INCLUDE})\ntarget_compile_definitions(test_tensorexpr PRIVATE USE_GTEST)\n\nadd_executable(tutorial_tensorexpr ${TENSOREXPR_TEST_ROOT}/tutorial.cpp)\ntarget_link_libraries(tutorial_tensorexpr PRIVATE torch)\ntarget_include_directories(tutorial_tensorexpr PRIVATE ${ATen_CPU_INCLUDE})\n\n# The test case depends on the xnnpack header which in turn depends on the\n# pthreadpool header. For some build environment we need add the dependency\n# explicitly.\nif(USE_PTHREADPOOL)\n  target_link_libraries(test_tensorexpr PRIVATE pthreadpool_interface)\nendif()\nif(USE_CUDA)\n  target_compile_definitions(test_tensorexpr PRIVATE USE_CUDA)\n  target_compile_definitions(tutorial_tensorexpr PRIVATE USE_CUDA)\nelseif(USE_ROCM)\n  target_link_libraries(test_tensorexpr PRIVATE\n    hiprtc::hiprtc\n    hip::amdhip64\n    ${TORCH_CUDA_LIBRARIES})\n  target_compile_definitions(test_tensorexpr PRIVATE USE_ROCM)\n\n  target_link_libraries(tutorial_tensorexpr PRIVATE\n    hiprtc::hiprtc\n    hip::amdhip64\n    ${TORCH_CUDA_LIBRARIES})\n  target_compile_definitions(tutorial_tensorexpr PRIVATE USE_ROCM)\nendif()\n\nif(INSTALL_TEST)\n  set_target_properties(test_tensorexpr PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n  install(TARGETS test_tensorexpr DESTINATION bin)\n  set_target_properties(tutorial_tensorexpr PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n  install(TARGETS tutorial_tensorexpr DESTINATION bin)\n  # Install PDB files for MSVC builds\n  if(MSVC AND BUILD_SHARED_LIBS)\n    install(FILES $<TARGET_PDB_FILE:test_tensorexpr> DESTINATION bin OPTIONAL)\n    install(FILES $<TARGET_PDB_FILE:tutorial_tensorexpr> DESTINATION bin OPTIONAL)\n  endif()\nendif()\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Matrix Multiplication and Batched Matrix Multiplication with Dimension Objects in Python\nDESCRIPTION: This snippet demonstrates how to implement matrix multiplication (mm) and batched matrix multiplication (bmm) using dimension objects in PyTorch. It shows how dimension objects avoid naming conflicts even when using the same dimension name in nested functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef mm(A, B):\n    i, j, k = dims()\n    r = (A[i, k] * B[k, j]).sum(k)\n    return r.order(i, j)\n\ndef bmm(A, B):\n    i = dims() # note: doesn't matter than mm internally also uses i\n    return mm(A[i], B[i])\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Addition Operations\nDESCRIPTION: Various tensor addition operations with different shapes and dimensions using float16 precision\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten.add.Tensor(T([], i64), 1)\naten.add.Tensor(T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16))\n```\n\n----------------------------------------\n\nTITLE: Specify Linker Type in CMake\nDESCRIPTION: This bash snippet describes how to set a faster linker type, such as mold, when using CMake for building PyTorch, thereby reducing linking time.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\nCMAKE_LINKER_TYPE=MOLD python setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Addition Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of tensor addition operations (aten.add.Tensor) with various tensor shapes. The operations are performed on half-precision (f16) tensors with different spatial dimensions, following a pyramidal structure common in CNNs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([96, 512, 4, 4], f16), T([96, 512, 4, 4], f16)), {})\ncnt: 2, ((T([96, 256, 8, 8], f16), T([96, 256, 8, 8], f16)), {})\ncnt: 2, ((T([96, 128, 16, 16], f16), T([96, 128, 16, 16], f16)), {})\ncnt: 2, ((T([96, 64, 32, 32], f16), T([96, 64, 32, 32], f16)), {})\ncnt: 1, ((T([96, 64, 64, 64], f16), T([96, 64, 64, 64], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Maintainer Approval Process in Markdown\nDESCRIPTION: Outlines the decision criteria and requirements for approving a new maintainer addition PR.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/community/build_ci_governance.rst#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n* Not earlier than two business days passed before merging (ensure the majority of the contributors have seen it)\n* PR has the correct label (`module: ci`)\n* There are no objections from the current maintainers\n* There are at least three net *thumbs up* from current maintainers (or all maintainers vote *thumbs up* when the module has less than 3 maintainers).\n```\n\n----------------------------------------\n\nTITLE: Submitting Changes with ghstack\nDESCRIPTION: This code snippet invokes `ghstack submit` to submit changes made in a GitHub repository. It requires `ghstack` to be installed as a dependency (detailed at https://github.com/ezyang/ghstack). This command generates a large commit intended for contributors who use this workflow. No input parameters are typically required, but the workflow must align with ghstack usage guidelines.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nghstack submit\n```\n\n----------------------------------------\n\nTITLE: Running Code Coverage for Multiple Tests and Folders\nDESCRIPTION: This example demonstrates how to run the code coverage tool for multiple tests and generate reports for specific folders of interest.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython oss_coverage.py --run-only=atest c10_logging_test --interest-only aten/src/Aten c10/core\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This snippet demonstrates the structure of operator usage analysis in PyTorch. It shows various operators like convolution, batch normalization, and pooling with their input tensor shapes and usage counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swsl_resnext101_32x16d_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([32, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([32, 1000], f16), T([32, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 2, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16)), {})\ncnt: 23, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16)), {})\ncnt: 4, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16)), {})\ncnt: 3, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Copying Tensors with aten.copy_ in PyTorch\nDESCRIPTION: The aten.copy_.default operator facilitates in-place copying of tensor data. It is indispensable when one needs to transfer data between tensors of the same shape and data type efficiently, without introducing new allocations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([256, 197951], f16), T([256, 197951], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Scalar Division Operations in PyTorch\nDESCRIPTION: Records of division operations where a tensor is divided by a scalar value. This specific operation divides a tensor with dimensions [128, 1280, 7, 7] by the scalar value 49.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 1280, 7, 7], f16, stride=(1280, 1, 0, 0)), 49), {})\n```\n\n----------------------------------------\n\nTITLE: Basic Tensor Aliasing in PyTorch\nDESCRIPTION: Demonstrates how tensors can share storage through views and references in PyTorch. Shows basic tensor creation and view operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\na = torch.rand(2, 3)\nb = a\n# At this point, `a` and `b` share their storage.\nc = b[0]\n# `c` shares storage with `a` and `b`, but only sees a slice of the allocated memory.\n```\n\n----------------------------------------\n\nTITLE: Dynamic Shape Handling in FakeTensor\nDESCRIPTION: Explains how FakeTensor handles dynamic shapes through ShapeEnv integration. The implementation manages data-dependent meta functions and memoization of unbacked SymInts for consistent symbolic size handling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_8\n\n\n\n----------------------------------------\n\nTITLE: Describing PyTorch Coverage Plugins Purpose in Markdown\nDESCRIPTION: A README file explaining the purpose of the PyTorch coverage plugins package. It highlights that the package currently only contains a JIT plugin for tracking coverage of code processed through PyTorch's JIT scripting functionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/coverage_plugins_package/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# What is this?\n\nThis folder hosts a minimal package for coverage plug-ins. Currently, the only plug-in is a JIT plug-in that helps coverage mark functions and methods passed through `torch.jit.script` and `torch.jit.script_method` as covered code.\n```\n\n----------------------------------------\n\nTITLE: Loading Data from Iterable-style Dataset without Batching\nDESCRIPTION: Shows the equivalent operation of loading individual samples from an iterable-style dataset when automatic batching is disabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfor data in iter(dataset):\n    yield collate_fn(data)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Where Operations\nDESCRIPTION: Conditional tensor operations using aten.where.self operator with boolean masks and float16 tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.where.self\ncnt: 8, ((T([0, 182], b8), T([0, 182], f16), T([], f16)), {})\ncnt: 2, ((T([0, 91], b8), T([0, 91], f16), T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Running FastRNNs benchmarks with default settings\nDESCRIPTION: Command to run the FastRNNs benchmarking suite with default settings, which compares all available RNN implementations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/fastrnns/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m fastrnns.bench\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Softmax Backward Operations in PyTorch\nDESCRIPTION: This snippet provides an example of the ATen softmax backward data operator, showing its application for back-propagating errors in deep learning. It shows tensor specifications and suggests managing derivatives during calculation through certain options such as data type and dimensional settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 6, ((T([16, 12, 128, 128], f16), T([16, 12, 128, 128], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Backward Operations in Neural Network\nDESCRIPTION: Shows the backward pass of convolution operations during neural network training. These operations compute gradients with respect to inputs, weights, and biases for each convolutional layer in the network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 3, ((T([32, 1024, 7, 7], f16, stride=(50176, 1, 7168, 1024)), T([32, 1024, 7, 7], f16, stride=(50176, 1, 7168, 1024)), T([1024, 1, 7, 7], f16), [1024], [1, 1], [3, 3], [1, 1], False, [0, 0], 1024, [True, True, True]), {})\ncnt: 1, ((T([32, 1024, 7, 7], f16), T([32, 512, 14, 14], f16, stride=(100352, 1, 7168, 512)), T([1024, 512, 2, 2], f16), [1024], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 27, ((T([32, 512, 14, 14], f16, stride=(100352, 1, 7168, 512)), T([32, 512, 14, 14], f16, stride=(100352, 1, 7168, 512)), T([512, 1, 7, 7], f16), [512], [1, 1], [3, 3], [1, 1], False, [0, 0], 512, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Matrix Multiplication with bmm in PyTorch (Python)\nDESCRIPTION: The aten.bmm operation conducts a batch matrix-matrix product of matrices stored in one tensor and of matrices stored in another. This is a fundamental operation in handling mini-batches of data in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\naten.bmm.default\ncnt: 12, ((T([192, 512, 64], f16), T([192, 64, 512], f16)), {})\ncnt: 12, ((T([192, 512, 512], f16), T([192, 512, 64], f16)), {})\ncnt: 12, ((T([192, 512, 512], f16, stride=(262144, 1, 512)), T([192, 512, 64], f16)), {})\ncnt: 12, ((T([192, 512, 64], f16), T([192, 64, 512], f16, stride=(32768, 1, 64))), {})\ncnt: 12, ((T([192, 64, 512], f16, stride=(32768, 1, 64)), T([192, 512, 512], f16)), {})\ncnt: 12, ((T([192, 512, 512], f16), T([192, 512, 64], f16, stride=(32768, 1, 512))), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations with Half Precision\nDESCRIPTION: PyTorch batch normalization operations with tensor shapes and half precision (f16) data type. Shows both forward and backward passes with various input dimensions and batch sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 128, 32, 32], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Multi-CUDA Test in PyTorch C++ Frontend (C++)\nDESCRIPTION: Defines a GoogleTest test case intended to run only on systems with at least two CUDA devices. The `_MultiCUDA` suffix appended to the test case name signals the test runner (specifically logic in `main.cpp`) to execute this test only if two or more CUDA devices are detected. This depends on the GoogleTest framework and PyTorch's custom test filtering logic.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/README.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n```cpp\nTEST(MyTestSuite, MyTestCase_MultiCUDA) { }\n```\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Embedding Operations in PyTorch\nDESCRIPTION: Details instances of \\\"aten.embedding.default\\\" summarizing its use in looking up embeddings for indices in a tensor, frequently used in language models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.embedding.default\ncnt: 1, ((T([50265, 1024], f16), T([8, 128], i64), 0), {})\ncnt: 1, ((T([1024, 1024], f16), T([128], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Recording Native Batch Norm Backward Calls (aten.native_batch_norm_backward) - PyTorch - Python\nDESCRIPTION: This snippet encodes input configurations for aten.native_batch_norm_backward, used in gradient computations of batch normalization, including input/output tensors, running mean/var, scale/bias, training flags, epsilon, and mask flags. All shapes and types (including mixed precision) are described, facilitating the study of backward op scheduling and resource usage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([1, 2304, 1536], f16), T([1, 2304, 1536], f16), T([2304], f16), None, None, T([2304], f32), T([2304], f32), True, 1e-05, [True, True, False]), {})\ncnt: 9, ((T([1, 1536, 384], f16), T([1, 1536, 384], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})\ncnt: 18, ((T([1, 384, 576], f16), T([1, 384, 576], f16), T([384], f16), None, None, T([384], f32), T([384], f32), True, 1e-05, [True, True, False]), {})\ncnt: 8, ((T([1, 384, 1536], f16), T([1, 384, 1536], f16), T([384], f16), None, None, T([384], f32), T([384], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 1536, 1536], f16), T([1, 1536, 1536], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 384, 512], f16), T([1, 384, 512], f16), T([384], f16), None, None, T([384], f32), T([384], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 1536, 512], f16), T([1, 1536, 512], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})\ncnt: 2, ((T([1, 512, 128], f16), T([1, 512, 128], f16), T([512], f16), None, None, T([512], f32), T([512], f32), True, 1e-05, [True, True, False]), {})\ncnt: 5, ((T([1, 128, 576], f16), T([1, 128, 576], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 128, 512], f16), T([1, 128, 512], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 128, 256], f16), T([1, 128, 256], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 512, 256], f16), T([1, 512, 256], f16), T([512], f16), None, None, T([512], f32), T([512], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 256, 64], f16), T([1, 256, 64], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\ncnt: 2, ((T([1, 64, 576], f16), T([1, 64, 576], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 64, 128], f16), T([1, 64, 128], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 256, 128], f16), T([1, 256, 128], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 64, 288], f16), T([1, 64, 288], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 32, 144], f16), T([1, 32, 144], f16), T([32], f16), None, None, T([32], f32), T([32], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 16, 27], f16), T([1, 16, 27], f16), T([16], f16), None, None, T([16], f32), T([16], f32), True, 1e-05, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Backward Selection with select in PyTorch (Python)\nDESCRIPTION: Performs aten.select_backward, allowing gradients to be propagated through selections made on tensor slices, essential for backpropagating changes through indices affected by previous selections.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\naten.select_backward.default\ncnt: 1, ((T([16, 768], f16), [16, 512, 768], 1, 0), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.sigmoid_backward.default with Tensor Arguments (Text)\nDESCRIPTION: This section logs calls to the `aten.sigmoid_backward.default` operator, used during backpropagation to compute gradients for the sigmoid function. It takes the gradient of the output and the output of the original sigmoid function as inputs. The examples show calls with tensors of shapes like [128, 1536, 1, 1] and [128, 256, 1, 1], using the float16 (f16) data type for both arguments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_20\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 9, ((T([128, 1536, 1, 1], f16), T([128, 1536, 1, 1], f16)), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([128, 512, 1, 1], f16), T([128, 512, 1, 1], f16)), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking PyTorch Tensor Addition Operations in Neural Network\nDESCRIPTION: Documents the usage of tensor addition operations across various layers of the neural network. Shows multiple different tensor shapes being added, primarily using half precision (f16) tensors with different spatial dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 34, ((T([], i64), 1), {})\ncnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})\ncnt: 2, ((T([128, 40, 28, 28], f16), T([128, 40, 28, 28], f16)), {})\ncnt: 2, ((T([128, 80, 14, 14], f16), T([128, 80, 14, 14], f16)), {})\ncnt: 2, ((T([128, 112, 14, 14], f16), T([128, 112, 14, 14], f16)), {})\ncnt: 2, ((T([128, 192, 7, 7], f16), T([128, 192, 7, 7], f16)), {})\ncnt: 1, ((T([128, 1152, 7, 7], f16), T([128, 1152, 7, 7], f16)), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 7, 7], f16)), {})\ncnt: 1, ((T([128, 672, 14, 14], f16), T([128, 672, 14, 14], f16)), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16)), {})\ncnt: 1, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16)), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([128, 240, 28, 28], f16)), {})\ncnt: 1, ((T([128, 72, 56, 56], f16), T([128, 72, 56, 56], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Multiplication\nDESCRIPTION: These snippets show element-wise multiplication operations between tensors and scalar values, using float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n((T([16, 1, 1, 1], f16), 0.19245008972987526), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([32, 1, 1, 1], f16), 0.08333333333333333), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([64, 1, 1, 1], f16), 0.05892556509887896), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1, 1, 1], f16), 0.041666666666666664), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 128, 48, 48], f16), 1.0), {})\n```\n\n----------------------------------------\n\nTITLE: Applying In-Place ReLU Activation (aten.relu_.default) in PyTorch (Python)\nDESCRIPTION: These are in-place ReLU operator calls for float16 tensors of various shapes. They modify the input tensor directly, with no returned value, for efficiency during forward inference in deep neural networks. PyTorch must be used in-place context, input tensor is overwritten, dtype and shape must match.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 2, ((T([32, 32, 112, 112], f16),), {})\ncnt: 1, ((T([32, 48, 112, 112], f16),), {})\ncnt: 1, ((T([32, 48, 56, 56], f16),), {})\ncnt: 5, ((T([32, 72, 56, 56], f16),), {})\ncnt: 1, ((T([32, 72, 28, 28], f16),), {})\ncnt: 4, ((T([32, 120, 28, 28], f16),), {})\ncnt: 1, ((T([32, 240, 28, 28], f16),), {})\ncnt: 1, ((T([32, 240, 14, 14], f16),), {})\ncnt: 6, ((T([32, 480, 14, 14], f16),), {})\ncnt: 3, ((T([32, 576, 14, 14], f16),), {})\ncnt: 1, ((T([32, 576, 7, 7], f16),), {})\ncnt: 8, ((T([32, 1152, 7, 7], f16),), {})\ncnt: 1, ((T([32, 1280, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Benchmark Output - System Information and Performance Metrics\nDESCRIPTION: Sample benchmark output showing PyTorch version, CUDA configuration, GPU topology matrix, and training performance metrics across 8 trainer nodes. Includes percentile-based timing measurements for each trainer and aggregate statistics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/benchmarks/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n---------- Info ---------\n\n* PyTorch version: 1.7.0\n* CUDA version: 9.2.0\n\n---------- nvidia-smi topo -m ---------\n\n    GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU     Affinity\n    GPU0     X      NV2     NV1     NV2     NV1     NODE    NODE    NODE    0-19,40-59\n    GPU1    NV2      X      NV2     NV1     NODE    NV1     NODE    NODE    0-19,40-59\n    GPU2    NV1     NV2      X      NV1     NODE    NODE    NV2     NODE    0-19,40-59\n    GPU3    NV2     NV1     NV1      X      NODE    NODE    NODE    NV2     0-19,40-59\n    GPU4    NV1     NODE    NODE    NODE     X      NV2     NV1     NV2     0-19,40-59\n    GPU5    NODE    NV1     NODE    NODE    NV2      X      NV2     NV1     0-19,40-59\n    GPU6    NODE    NODE    NV2     NODE    NV1     NV2      X      NV1     0-19,40-59\n    GPU7    NODE    NODE    NODE    NV2     NV2     NV1     NV1      X      0-19,40-59\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing a single PCIe switch\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n------------------  PyTorch Distributed Benchmark (DDP and RPC) ---------------------\n\n                    sec/epoch  epoch/sec    sec/epoch  epoch/sec    sec/epoch  epoch/sec    sec/epoch  epoch/sec\n    Trainer0:  p50:  0.376s     185/s  p75:  0.384s     182/s  p90:  0.390s     179/s  p95:  0.396s     176/s\n    Trainer1:  p50:  0.377s     204/s  p75:  0.384s     200/s  p90:  0.389s     197/s  p95:  0.393s     195/s\n    Trainer2:  p50:  0.377s     175/s  p75:  0.384s     172/s  p90:  0.390s     169/s  p95:  0.395s     166/s\n    Trainer3:  p50:  0.377s     161/s  p75:  0.384s     158/s  p90:  0.390s     156/s  p95:  0.393s     155/s\n    Trainer4:  p50:  0.377s     172/s  p75:  0.383s     169/s  p90:  0.389s     166/s  p95:  0.395s     164/s\n    Trainer5:  p50:  0.377s     180/s  p75:  0.383s     177/s  p90:  0.389s     174/s  p95:  0.395s     172/s\n    Trainer6:  p50:  0.377s     204/s  p75:  0.384s     200/s  p90:  0.390s     197/s  p95:  0.394s     195/s\n    Trainer7:  p50:  0.377s     185/s  p75:  0.384s     182/s  p90:  0.389s     179/s  p95:  0.394s     177/s\n         All:  p50:  0.377s    1470/s  p75:  0.384s    1443/s  p90:  0.390s    1421/s  p95:  0.396s    1398/s\n```\n\n----------------------------------------\n\nTITLE: Example of Unstructured Pruned Weight Tensor\nDESCRIPTION: Shows a weight tensor after unstructured pruning where individual elements with the lowest values have been zeroed out while preserving the tensor's shape.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nW_pruned = [[0 0 3]\n            [4 5 6]\n            [7 0 9]]\n```\n\n----------------------------------------\n\nTITLE: PyTorch HardSwish Activation Operations\nDESCRIPTION: HardSwish activation function operations applied to tensors of different shapes throughout the network layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 32, 112, 112], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization Operations\nDESCRIPTION: Batch normalization operations with various tensor shapes and configurations. Uses momentum of 0.03 and epsilon of 0.0001 across different channel dimensions ranging from 32 to 1024.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n((T([8, 32, 384, 512], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.03, 0.0001), {})\n```\n\n----------------------------------------\n\nTITLE: Creating a One-Dimensional Tensor with PyTorch C++\nDESCRIPTION: This example demonstrates how to create a one-dimensional tensor (vector) with 5 components, all set to 1, using the ones() factory function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor tensor = torch::ones(5);\n```\n\n----------------------------------------\n\nTITLE: Optimizing PyTorch Mobile Modules\nDESCRIPTION: The torch.utils.mobile_optimizer.optimize_for_mobile function optimizes PyTorch models for mobile execution. It requires a torch.jit.ScriptModule object, a blocklist of optimization passes, a list of preserved methods, and a backend type. The method applies various optimizations depending on the backend, such as operation fusion and prepacking for CPU, and GPU data transfer for Vulkan. Ensure you pass the correct method list to preserve any additional methods beyond 'forward'.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mobile_optimizer.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntorch.utils.mobile_optimizer.optimize_for_mobile\n```\n\n----------------------------------------\n\nTITLE: Reference Quantized Model Forward Pass\nDESCRIPTION: Implementation of the forward pass in a reference quantized model, showing quantization and dequantization operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nquantized: GraphModule(\n  (linear): LinearReLU(\n    (0): QuantizedLinear(Reference)(in_features=5, out_features=10, bias=True)\n    (1): ReLU()\n  )\n)\n\ndef forward(self, x):\n    linear_input_scale_0 = self.linear_input_scale_0\n    linear_input_zero_point_0 = self.linear_input_zero_point_0\n    quantize_per_tensor = torch.quantize_per_tensor(x, linear_input_scale_0, linear_input_zero_point_0, torch.quint8);  x = linear_input_scale_0 = linear_input_zero_point_0 = None\n    dequantize = quantize_per_tensor.dequantize();  quantize_per_tensor = None\n    linear = self.linear(dequantize);  dequantize = None\n    linear_scale_0 = self.linear_scale_0\n    linear_zero_point_0 = self.linear_zero_point_0\n    quantize_per_tensor_1 = torch.quantize_per_tensor(linear, linear_scale_0, linear_zero_point_0, torch.quint8);  linear = linear_scale_0 = linear_zero_point_0 = None\n    dequantize_1 = quantize_per_tensor_1.dequantize();  quantize_per_tensor_1 = None\n    return dequantize_1\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations\nDESCRIPTION: Batch normalization operations with momentum 0.1 and epsilon 1e-05 applied to different tensor shapes. Includes running mean and variance tracking.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Registering Schema-Only PyTorch Operators\nDESCRIPTION: This snippet shows how to register an operator without implementing a kernel. This approach is useful for defining the interface of an operator before implementing its functionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nstatic auto registry = torch::RegisterOperators()\n   .op(\"my_namespace::my_op(Tensor a, Tensor b) -> Tensor\");\n```\n\n----------------------------------------\n\nTITLE: Visualizing DTensor sharding\nDESCRIPTION: Uses the visualize_sharding function to display the sharding of a DTensor with up to 3 dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nvisualize_sharding(dtensor)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations in PyTorch\nDESCRIPTION: Profiling data for matrix multiplication operations showing count and tensor shapes. These operations are likely part of the fully connected layers at the end of the network for classification, involving the final 1000-dimensional output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), T([1000, 1280], f16)), {})\ncnt: 1, ((T([1000, 32], f16, stride=(0, 0)), T([32, 1280], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Installing torchvision Dependency for MNIST Dataset\nDESCRIPTION: Installs the torchvision package, which provides the MNIST dataset with images of handwritten digits for training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npip install torchvision\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Python Source Files\nDESCRIPTION: Sets up the source files for the PyTorch Python bindings. Includes conditionally compiled test sources when BUILD_TEST is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(TOOLS_PATH \"${TORCH_ROOT}/tools\")\n\n\nset(TORCH_PYTHON_SRCS\n    ${GENERATED_THNN_CXX}\n    ${GENERATED_CXX_PYTHON}\n    )\nappend_filelist(\"libtorch_python_core_sources\" TORCH_PYTHON_SRCS)\n\n# NB: This has to match the condition under which the JIT test directory\n#     is included (at the time of writing that's in caffe2/CMakeLists.txt).\nif(BUILD_TEST)\n    add_definitions(-DBUILDING_TESTS)\n    list(APPEND TORCH_PYTHON_SRCS\n      ${TORCH_ROOT}/test/cpp/jit/torch_python_test.cpp\n      )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting New Data for Pad_mm Heuristic Generation\nDESCRIPTION: This bash command runs a script to collect new training data for pad_mm heuristic generation. It performs benchmarks on random inputs and may take a day depending on the number of GPUs used.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/pad_mm/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash generate_heuristic_pad_mm.sh collect\n```\n\n----------------------------------------\n\nTITLE: Registering Test Module for Operator Changes in PyTorch\nDESCRIPTION: Example of registering a test module instance as a key with the corresponding changed operator name as value in the ALL_MODULES dictionary. This ensures the test model covers everything needed for testing backward compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# key: test module instance, value: changed operator name\nALL_MODULES = {\n    TestVersionedLinspaceV7(): \"aten::linspace\",\n}\n```\n\n----------------------------------------\n\nTITLE: Using isinstance with Any Type in TorchScript Function (Python)\nDESCRIPTION: This snippet scripts a function taking an argument of type Any, prints it, and returns whether it is a torch.Tensor. It demonstrates use of runtime type checking within a TorchScripted function. Requires torch and typing.Any; expects any input, prints and outputs a boolean. The scripted function can be called with any type and will handle type checks via isinstance.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom typing import Any\n\ndef f(a:Any):\n    print(a)\n    return (isinstance(a, torch.Tensor))\n\nones = torch.ones([2])\nm = torch.jit.script(f)\nprint(m(ones))\n```\n\n----------------------------------------\n\nTITLE: Adjusting Build Options on macOS - CMake Only Build and GUI - bash\nDESCRIPTION: This snippet configures the CMAKE_PREFIX_PATH and environment variables specific to macOS (e.g., deployment target, compiler), then generates CMake build files and invokes ccmake/cmake-gui for customization. Use this for advanced build configuration prior to compilation on macOS. Required tools: CMake, ccmake/cmake-gui, and all build dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only\\nccmake build  # or cmake-gui build\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Build Version in CMake\nDESCRIPTION: This snippet reads the default version from 'version.txt', removes trailing newlines, and sets a default '0.0.0' if reading fails. It caches this as TORCH_BUILD_VERSION, allowing override via the PYTORCH_BUILD_VERSION environment variable. It then parses the version string into major/minor components and sets TORCH_SOVERSION.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\n# Strip trailing newline\nstring(REGEX REPLACE \"\\n$\" \"\" TORCH_DEFAULT_VERSION \"${TORCH_DEFAULT_VERSION}\")\nif(\"${TORCH_DEFAULT_VERSION} \" STREQUAL \" \")\n  message(WARNING \"Could not get version from base 'version.txt'\")\n  # If we can't get the version from the version file we should probably set it\n  # to something non-sensical like 0.0.0\n  set(TORCH_DEFAULT_VERSION, \"0.0.0\")\nendif()\nset(TORCH_BUILD_VERSION\n    \"${TORCH_DEFAULT_VERSION}\"\n    CACHE STRING \"Torch build version\")\nif(DEFINED ENV{PYTORCH_BUILD_VERSION})\n  set(TORCH_BUILD_VERSION\n      \"$ENV{PYTORCH_BUILD_VERSION}\"\n      CACHE STRING \"Torch build version\" FORCE)\nendif()\nif(NOT TORCH_BUILD_VERSION)\n  # An empty string was specified so force version to the default\n  set(TORCH_BUILD_VERSION\n      \"${TORCH_DEFAULT_VERSION}\"\n      CACHE STRING \"Torch build version\" FORCE)\nendif()\ncaffe2_parse_version_str(TORCH ${TORCH_BUILD_VERSION})\nset(TORCH_SOVERSION \"${TORCH_VERSION_MAJOR}.${TORCH_VERSION_MINOR}\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing NLL Loss Operations in PyTorch\nDESCRIPTION: This snippet shows the forward and backward operations for Negative Log Likelihood (NLL) loss, including input tensors, target labels, and other parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Using Deterministic `torch.bmm` Implementation with Sparse Tensors (Python)\nDESCRIPTION: Shows that after enabling deterministic mode with `torch.use_deterministic_algorithms(True)`, calling `torch.bmm` with sparse CUDA tensors automatically uses its available deterministic implementation instead of the default nondeterministic one. This ensures reproducibility for this specific operation and input type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n>>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\ntensor([[[ 1.1900, -2.3409],\n         [ 0.4796,  0.8003]],\n        [[ 0.1509,  1.8027],\n         [ 0.0333, -1.1444]]], device='cuda:0')\n```\n\n----------------------------------------\n\nTITLE: Generating Coverage Summary for Different Folder\nDESCRIPTION: This example shows how to generate a coverage summary for a different folder of interest without re-running the tests.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# after run this command\npython oss_coverage.py --run-only=atest --interest-only=aten\n# you may then want to learn atest's coverage over c10, instead of running the test again, you can:\npython oss_coverage.py --run-only=atest --interest-only=c10 --summary\n```\n\n----------------------------------------\n\nTITLE: Describing PyTorch threshold_backward Operator Arguments - Python\nDESCRIPTION: This snippet lists call patterns for the threshold_backward operator, which is commonly used for implementing backward passes of threshold or ReLU-like operations. Input arguments are pairs of float16 tensors representing inputs and gradients, as well as threshold values as constants. The variety of tensor shapes demonstrates different spatial and channel dimensions for batch-wise processing. Intended for operator kernel and autograd validation within PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 3, ((T([32, 2048, 8, 8], f16), T([32, 2048, 8, 8], f16), 0), {})\ncnt: 3, ((T([32, 256, 1, 1], f16), T([32, 256, 1, 1], f16), 0), {})\ncnt: 2, ((T([32, 1024, 8, 8], f16), T([32, 1024, 8, 8], f16), 0), {})\ncnt: 2, ((T([32, 512, 8, 8], f16), T([32, 512, 8, 8], f16), 0), {})\ncnt: 24, ((T([32, 1024, 16, 16], f16), T([32, 1024, 16, 16], f16), 0), {})\ncnt: 23, ((T([32, 512, 16, 16], f16), T([32, 512, 16, 16], f16), 0), {})\ncnt: 23, ((T([32, 128, 1, 1], f16), T([32, 128, 1, 1], f16), 0), {})\ncnt: 22, ((T([32, 256, 16, 16], f16), T([32, 256, 16, 16], f16), 0), {})\ncnt: 5, ((T([32, 512, 32, 32], f16), T([32, 512, 32, 32], f16), 0), {})\ncnt: 4, ((T([32, 256, 32, 32], f16), T([32, 256, 32, 32], f16), 0), {})\ncnt: 4, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), 0), {})\ncnt: 3, ((T([32, 128, 32, 32], f16), T([32, 128, 32, 32], f16), 0), {})\ncnt: 4, ((T([32, 256, 64, 64], f16), T([32, 256, 64, 64], f16), 0), {})\ncnt: 4, ((T([32, 128, 64, 64], f16), T([32, 128, 64, 64], f16), 0), {})\ncnt: 3, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), 0), {})\ncnt: 3, ((T([32, 64, 64, 64], f16), T([32, 64, 64, 64], f16), 0), {})\ncnt: 1, ((T([32, 128, 128, 128], f16), T([32, 128, 128, 128], f16), 0), {})\ncnt: 2, ((T([32, 64, 128, 128], f16), T([32, 64, 128, 128], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Sum Operation with Symbol Dimensions\nDESCRIPTION: This section documents a symbolic integer sum operation in PyTorch, operating on a tensor with 64 samples and 1000 output features (likely a classification model). The sum is performed along dimension 0 with keepdim=True to preserve dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Scripted Function Calling Traced Function with Disabled Autocast\nDESCRIPTION: Example showing a limitation where disabling autocast in a traced function is ignored when called from a scripted function with autocast enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.cpu.amp import autocast\n\ndef helper(a, b):\n    with autocast(enabled=False):\n        return torch.mm(a, b) * 2.0\n\ntraced = torch.jit.trace(helper, (x, y))\n\n@torch.jit.script\ndef fn(a, b):\n    with autocast(enabled=True):\n        return traced(a, b)\n```\n\n----------------------------------------\n\nTITLE: Dividing Tensors by Scalar - PyTorch - Python\nDESCRIPTION: Shows usages of the element-wise tensor division by a scalar, especially normalizing outputs after pooling via scalar constants (49, 196). Ensures output values are appropriately scaled, as per e.g. average pooling normalization. Inputs are high-dimensional and possibly non-contiguous; output shape is the same as input.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 4, ((T([64, 2048, 7, 7], f16, stride=(2048, 1, 0, 0)), 49), {})\ncnt: 23, ((T([64, 1024, 14, 14], f16, stride=(1024, 1, 0, 0)), 196), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring AVX2 Performance Kernels in CMake\nDESCRIPTION: Creates a static library for AVX2-optimized performance kernels when the compiler supports AVX2 extensions. Sets platform-specific compiler flags for MSVC and other compilers, linking the resulting interface library to the main project.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/perfkernels/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\n# We will only build the perf kernel files if the compiler supports avx2\n# extensions.\nif(CXX_AVX2_FOUND)\n  add_library(Caffe2_perfkernels_avx2 STATIC ${avx2_srcs})\n  target_link_libraries(Caffe2_perfkernels_avx2 PRIVATE c10)\n\n  if(MSVC AND NOT \"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    target_compile_options(Caffe2_perfkernels_avx2\n        PRIVATE \"/arch:AVX2\"\n        PRIVATE \"/D__FMA__\"\n        PRIVATE \"/D__F16C__\")\n  else()\n    target_compile_options(Caffe2_perfkernels_avx2\n        PRIVATE \"-mavx2\"\n        PRIVATE \"-mfma\"\n        PRIVATE \"-mavx\"\n        PRIVATE \"-mf16c\")\n  endif()\n  caffe2_interface_library(\n      Caffe2_perfkernels_avx2 Caffe2_perfkernels_avx2_interface)\n  list(APPEND\n       Caffe2_DEPENDENCY_WHOLE_LINK_LIBS\n       \"Caffe2_perfkernels_avx2_interface\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Illustrating Git Commit Structure for Pull Request CI\nDESCRIPTION: This diagram shows the relationship between the main branch (merge-destination, commit A), the head of a pull request branch (commit B, `refs/pull/42/head`), and the potential merge commit (commit C, `refs/pull/42/merge`). This structure is relevant for understanding which code version is tested during CI runs for PRs, especially when workflow files are modified.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_41\n\nLANGUAGE: text\nCODE:\n```\n       o---o---B (refs/pull/42/head)\n      /         \\\n     /           C (refs/pull/42/merge)\n    /           /\n---o---o---o---A (merge-destination) - usually main\n```\n\n----------------------------------------\n\nTITLE: Repeating Tensor Elements with ATen\nDESCRIPTION: This operator replicates the elements of a tensor along specified dimensions, expanding its size. Requires an input tensor, e.g., [16, 5, 1, 1], and a tuple of repeat counts like [1, 1, 128, 128]. Outputs are expanded tensors used for broadcasting in model operations, relieving manual tensor reshaping constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.repeat.default\ncnt: 1, ((T([16, 5, 1, 1], f16), [1, 1, 128, 128]), {})\ncnt: 8, ((T([64], f16), [16]), {})\ncnt: 8, ((T([128], f16), [16]), {})\ncnt: 52, ((T([256], f16), [16]), {})\n```\n\n----------------------------------------\n\nTITLE: Demultiplexing DataPipes in PyTorch\nDESCRIPTION: Illustrates the use of the demux() method to split a DataPipe into multiple DataPipes based on a routing function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndp = ExampleIterPipe(10)\ndp1, dp2, dp3 = dp.demux(3, lambda x: x % 3)\nfor i in dp2:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Log Softmax Backward Computation in PyTorch (Python)\nDESCRIPTION: Calculates the backward pass of the log softmax operation often used during backpropagation in neural networks. This function aids in gradient computations required for updating the model parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\naten._log_softmax_backward_data.default\ncnt: 1, ((T([16, 2], f16), T([16, 2], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Enabling Input Mutation Support with CUDAGraph in PyTorch\nDESCRIPTION: Configuration setting to enable input mutation support in CUDAGraph when using the \"reduce-overhead\" mode.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntorch._inductor.config.cudagraph_support_input_mutation = True\n```\n\n----------------------------------------\n\nTITLE: Tensor Division Operations in PyTorch\nDESCRIPTION: This snippet shows tensor division operations where a scalar tensor is divided by a constant value. These operations are likely used for scaling or normalization in the model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 6240), {})\n```\n\n----------------------------------------\n\nTITLE: Unsupported Autocast Decorator on JIT Scripted Functions\nDESCRIPTION: Example showing that applying the autocast decorator directly to a JIT scripted function is not supported in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.cpu.amp import autocast\n\n@torch.jit.script\n@autocast() # not supported\ndef foo(a, b, c, d):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Managing Tensor References in PyTorch - C++\nDESCRIPTION: This snippet demonstrates how to obtain and manage a reference to the indices of a sparse tensor using the `newIndices` function. It highlights the essential practice of freeing these resources to prevent memory leaks. Dependencies include THSTensor and THIndexTensor utilities from PyTorch's tensor libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nTHIndexTensor *indices = THSTensor_(newIndices)(state, sparse);\n// ... do some stuff ...\nTHIndexTensor_(free)(state, indices);\n```\n\n----------------------------------------\n\nTITLE: Fast Numerical Evaluation of Complex Gradients in PyTorch\nDESCRIPTION: Mathematical formulation showing how PyTorch computes scalar quantities for fast backward gradcheck with complex inputs, avoiding full Jacobian matrix reconstruction while maintaining correctness.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/gradcheck.rst#2025-04-22_snippet_1\n\nLANGUAGE: math\nCODE:\n```\n\\begin{aligned}\n    s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\\\n      &= 2 * v^T (\\frac{1}{2} * \\frac{\\partial y}{\\partial a} ur + i * \\frac{1}{2} * \\frac{\\partial y}{\\partial b} ui) \\\\\n      &= v^T (\\frac{\\partial y}{\\partial a} ur + i * \\frac{\\partial y}{\\partial b} ui) \\\\\n      &= v^T ((\\frac{\\partial y}{\\partial a} ur) + i * (\\frac{\\partial y}{\\partial b} ui))\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Defining Threading Environment Variables Table in reStructuredText\nDESCRIPTION: This snippet creates a table in reStructuredText format that lists and describes two threading-related environment variables used in PyTorch: OMP_NUM_THREADS and MKL_NUM_THREADS.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/threading_environment_variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. list-table::\n  :header-rows: 1\n\n  * - Variable\n    - Description\n  * - ``OMP_NUM_THREADS``\n    - Sets the maximum number of threads to use for OpenMP parallel regions.\n  * - ``MKL_NUM_THREADS``\n    - Sets the maximum number of threads to use for the Intel MKL library. Note that MKL_NUM_THREADS takes precedence over ``OMP_NUM_THREADS``.\n```\n\n----------------------------------------\n\nTITLE: Class Name Printing in PyTorch Package\nDESCRIPTION: Shows how class names are mangled by the package system, with the original class showing its basic name while the imported class includes a package prefix.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprint(MyClass.__name__)  # prints \"foo.MyClass\"\nprint(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Split Operations\nDESCRIPTION: Split operations on tensors with specified size divisions, operating on 4D tensors with varying channel dimensions and batch sizes of 128.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 196, 4, 64], f16), [16, 16, 32], 3), {})\n((T([128, 196, 8, 80], f16), [16, 64], 3), {})\n```\n\n----------------------------------------\n\nTITLE: Running Specific Benchmark Test with Filtering\nDESCRIPTION: Runs a specific benchmark test by name with single-threaded configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m pt.add_test --test-name add_K32_M8_N1\n--omp-num-threads 1 --mkl-num-threads 1\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.native_batch_norm_backward.default Calls - PyTorch - Python\nDESCRIPTION: Tracks backward batch normalization operations using aten.native_batch_norm_backward.default with various tensor and scalar argument patterns. Inputs include input, grad_output, weight, saved_mean, saved_invstd, running_mean, running_var, as well as options for affine and epsilon. Output consists of gradients for input, weight, and bias, supporting analysis of backpropagation memory patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([128, 1024, 4, 4], f16), T([128, 1024, 4, 4], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 1280, 4, 4], f16), T([128, 1280, 4, 4], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 960, 7, 7], f16), T([128, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f32), T([960], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16), T([152], f16), T([152], f16), T([152], f16), T([152], f32), T([152], f32), True, 1e-05, [True, True, True]), {})\ncnt: 7, ((T([128, 304, 14, 14], f16), T([128, 304, 14, 14], f16), T([304], f16), T([304], f16), T([304], f16), T([304], f32), T([304], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 288, 28, 28], f16), T([128, 288, 28, 28], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f32), T([288], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), True, 1e-05, [True, True, True]), {})\ncnt: 7, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 128, 56, 56], f16), T([128, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\ncnt: 7, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Lexer Token Stream Example for Python-like Indentation\nDESCRIPTION: Demonstration of how the Lexer handles Python-like indentation by injecting TK_INDENT, TK_DEDENT, and TK_NEWLINE tokens into the token stream.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nif\n  .\n  .\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Shapes and Strides in PyTorch\nDESCRIPTION: This code snippet represents a collection of tensor shape analyses in PyTorch. Each line shows the count of occurrences, tensor shapes, data types, and stride information. The tensors vary in dimensions from 4D to 2D, primarily using float16 (f16) data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net101_26w_4s_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 2048, 7, 7], f16), T([64, 2048, 7, 7], f16), 0), {})\ncnt: 5, ((T([64, 208, 7, 7], f16, stride=(40768, 49, 7, 1)), T([64, 208, 7, 7], f16), 0), {})\ncnt: 4, ((T([64, 208, 7, 7], f16), T([64, 208, 7, 7], f16), 0), {})\ncnt: 2, ((T([64, 832, 7, 7], f16), T([64, 832, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 832, 14, 14], f16), T([64, 832, 14, 14], f16), 0), {})\ncnt: 23, ((T([64, 1024, 14, 14], f16), T([64, 1024, 14, 14], f16), 0), {})\ncnt: 25, ((T([64, 104, 14, 14], f16, stride=(81536, 196, 14, 1)), T([64, 104, 14, 14], f16), 0), {})\ncnt: 44, ((T([64, 104, 14, 14], f16), T([64, 104, 14, 14], f16), 0), {})\ncnt: 22, ((T([64, 416, 14, 14], f16), T([64, 416, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 416, 28, 28], f16), T([64, 416, 28, 28], f16), 0), {})\ncnt: 4, ((T([64, 512, 28, 28], f16), T([64, 512, 28, 28], f16), 0), {})\ncnt: 6, ((T([64, 52, 28, 28], f16, stride=(163072, 784, 28, 1)), T([64, 52, 28, 28], f16), 0), {})\ncnt: 6, ((T([64, 52, 28, 28], f16), T([64, 52, 28, 28], f16), 0), {})\ncnt: 3, ((T([64, 208, 28, 28], f16), T([64, 208, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 208, 56, 56], f16), T([64, 208, 56, 56], f16), 0), {})\ncnt: 3, ((T([64, 256, 56, 56], f16), T([64, 256, 56, 56], f16), 0), {})\ncnt: 5, ((T([64, 26, 56, 56], f16, stride=(326144, 3136, 56, 1)), T([64, 26, 56, 56], f16), 0), {})\ncnt: 4, ((T([64, 26, 56, 56], f16), T([64, 26, 56, 56], f16), 0), {})\ncnt: 3, ((T([64, 104, 56, 56], f16), T([64, 104, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 64, 112, 112], f16), T([64, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Analysis of PyTorch Operator Usage in Transformer Architecture\nDESCRIPTION: This snippet shows the frequency and input tensor specifications for various PyTorch operators used in a transformer model implementation. It provides insights into tensor shapes, data types, and operation counts that can be used for performance optimization and understanding model architecture.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/BERT_pytorch_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 12, ((T([16, 12, 128, 128], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([16, 12, 128, 128], f16), T([16, 12, 128, 128], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([16, 12, 128, 64], f16), [192, 128, 64]), {})\ncnt: 12, ((T([16, 12, 64, 128], f16), [192, 64, 128]), {})\ncnt: 12, ((T([192, 128, 128], f16), [16, 12, 128, 128]), {})\ncnt: 12, ((T([192, 128, 64], f16), [16, 12, 128, 64]), {})\ncnt: 24, ((T([16, 128, 12, 64], f16), [16, 128, 768]), {})\ncnt: 12, ((T([16, 128, 768], f16), [2048, 768]), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([16, 128, 768], f16), T([1, 128, 768], f16)), {})\ncnt: 120, ((T([16, 128, 768], f16), T([16, 128, 768], f16)), {})\ncnt: 24, ((T([16, 128, 1], f16), 1e-06), {})\ncnt: 24, ((T([16, 128, 768], f16), T([768], f16)), {})\ncnt: 1, ((T([16, 128, 768], f16, stride=(0, 0, 0)), T([16, 128, 768], f16)), {})\nOperator: aten.addmm.default\ncnt: 48, ((T([768], f16), T([2048, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 12, ((T([3072], f16), T([2048, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 12, ((T([768], f16), T([2048, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\nOperator: aten.bmm.default\ncnt: 12, ((T([192, 128, 64], f16), T([192, 64, 128], f16)), {})\ncnt: 12, ((T([192, 128, 128], f16), T([192, 128, 64], f16)), {})\ncnt: 12, ((T([192, 128, 128], f16, stride=(16384, 1, 128)), T([192, 128, 64], f16)), {})\ncnt: 12, ((T([192, 128, 64], f16), T([192, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 12, ((T([192, 64, 128], f16, stride=(8192, 1, 64)), T([192, 128, 128], f16)), {})\ncnt: 12, ((T([192, 128, 128], f16), T([192, 128, 64], f16, stride=(8192, 1, 128))), {})\nOperator: aten.clone.default\ncnt: 2, ((T([16, 128], i64),), {})\nOperator: aten.copy_.default\ncnt: 2, ((T([16, 128], i64), T([16, 128], i64)), {})\nOperator: aten.div.Scalar\ncnt: 24, ((T([16, 128, 768], f16, stride=(128, 1, 0)), 768), {})\nOperator: aten.div.Tensor\ncnt: 96, ((T([16, 128, 768], f16), T([16, 128, 1], f16)), {})\ncnt: 24, ((T([16, 12, 128, 128], f16), 8.0), {})\ncnt: 2, ((T([], f16), 1572864), {})\ncnt: 24, ((T([16, 128, 1], f16), T([16, 128, 1], f16)), {})\nOperator: aten.embedding.default\ncnt: 1, ((T([20005, 768], f16), T([16, 128], i64), 0), {})\ncnt: 1, ((T([3, 768], f16), T([16, 128], i64), 0), {})\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([16, 128, 768], f16), T([16, 128], i64), 3, 0, False), {})\ncnt: 1, ((T([16, 128, 768], f16), T([16, 128], i64), 20005, 0, False), {})\nOperator: aten.eq.Scalar\ncnt: 12, ((T([16, 1, 128, 128], b8), 0), {})\ncnt: 24, ((T([16, 128, 1], f16), 0), {})\nOperator: aten.gelu.default\ncnt: 12, ((T([16, 128, 3072], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([16, 128, 3072], f16), T([16, 128, 3072], f16)), {})\nOperator: aten.gt.Scalar\ncnt: 1, ((T([16, 128], i64), 0), {})\nOperator: aten.masked_fill.Scalar\ncnt: 12, ((T([16, 12, 128, 128], f16), T([16, 1, 128, 128], b8), -65504.0), {})\ncnt: 12, ((T([16, 12, 128, 128], f16), T([16, 1, 128, 128], b8), 0), {})\nOperator: aten.masked_fill_.Scalar\ncnt: 24, ((T([16, 128, 1], f16), T([16, 128, 1], b8), 0), {})\nOperator: aten.mean.dim\ncnt: 48, ((T([16, 128, 768], f16), [-1], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([2048, 768], f16, stride=(0, 0)), T([768, 3072], f16)), {})\ncnt: 1, ((T([768, 2048], f16, stride=(0, 0)), T([2048, 3072], f16)), {})\ncnt: 12, ((T([2048, 3072], f16), T([3072, 768], f16)), {})\ncnt: 12, ((T([3072, 2048], f16, stride=(1, 3072)), T([2048, 768], f16)), {})\ncnt: 48, ((T([2048, 768], f16), T([768, 768], f16)), {})\ncnt: 48, ((T([768, 2048], f16, stride=(1, 768)), T([2048, 768], f16)), {})\ncnt: 11, ((T([2048, 768], f16), T([768, 3072], f16)), {})\ncnt: 11, ((T([768, 2048], f16, stride=(1, 768)), T([2048, 3072], f16)), {})\nOperator: aten.mul.Scalar\ncnt: 24, ((T([16, 128, 1], f16), 2), {})\ncnt: 24, ((T([16, 128, 1], f16), 0.002607561929595828), {})\nOperator: aten.mul.Tensor\ncnt: 24, ((T([768], f16), T([16, 128, 768], f16)), {})\ncnt: 48, ((T([16, 128, 768], f16), T([16, 128, 768], f16)), {})\ncnt: 24, ((T([16, 128, 768], f16), T([768], f16)), {})\ncnt: 24, ((T([16, 128, 1], f16), T([16, 128, 768], f16)), {})\nOperator: aten.neg.default\ncnt: 48, ((T([16, 128, 768], f16),), {})\nOperator: aten.repeat.default\ncnt: 1, ((T([16, 1, 128], b8), [1, 128, 1]), {})\nOperator: aten.std.correction\ncnt: 24, ((T([16, 128, 768], f16), [-1]), {'correction': 1, 'keepdim': True})\nOperator: aten.sub.Tensor\ncnt: 48, ((T([16, 128, 768], f16), T([16, 128, 1], f16)), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([2048, 768], f16, stride=(0, 0)), [0], True), {})\ncnt: 12, ((T([2048, 3072], f16), [0], True), {})\ncnt: 48, ((T([16, 128, 768], f16), [0, 1], True), {})\ncnt: 48, ((T([16, 128, 768], f16), [2], True), {})\ncnt: 59, ((T([2048, 768], f16), [0], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([16, 128, 768], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Applying Sigmoid Activation in PyTorch\nDESCRIPTION: This snippet applies the sigmoid activation function to tensors. Sigmoid is commonly used in neural networks to squash values between 0 and 1, often in the output layer for binary classification or as gates in LSTM cells.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 3, 12, 16, 2], f16, stride=(48960, 16320, 1360, 85, 1)),), {})\ncnt: 1, ((T([8, 3, 24, 32, 2], f16, stride=(195840, 65280, 2720, 85, 1)),), {})\ncnt: 1, ((T([8, 3, 48, 64, 2], f16, stride=(783360, 261120, 5440, 85, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.sum.SymInt\nDESCRIPTION: Logs tensor summation operations using `aten.sum.SymInt`. Entries show summation over specific dimensions (`[2, 3]` or `[-1, -2]` for spatial dimensions, `[0]` for batch dimension) while keeping the dimensions (`True`). Tensors are primarily 4D with float16 data type. The `cnt` indicates the frequency of each specific call signature.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\ncnt: 1, ((T([128, 24, 56, 56], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 56, 28, 28], f16), [2, 3], True), {})\ncnt: 4, ((T([128, 152, 14, 14], f16), [2, 3], True), {})\ncnt: 7, ((T([128, 368, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 368, 7, 7], f16), [-1, -2], True), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 7, ((T([128, 368, 7, 7], f16), [2, 3], True), {})\ncnt: 4, ((T([128, 152, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 56, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 24, 56, 56], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.convolution.default Calls in PyTorch Text Trace\nDESCRIPTION: Shows invocation signatures for forward convolution operations, with detailed arguments such as input/output tensor shapes, kernel shapes, strides, paddings, groups, and half-precision usage. These traces reflect the model's convolutional layer configuration and parameter counts, allowing for layer-wise architectural analysis or profiling. All data is shown as plain text, requiring reader familiarity with tensor and convolution conventions in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})\ncnt: 1, ((T([32, 32, 1, 1], f16), T([8, 32, 1, 1], f16), T([8], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 8, 1, 1], f16), T([32, 8, 1, 1], f16), T([32], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([16, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 16, 112, 112], f16), T([96, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 96, 112, 112], f16), T([96, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 96), {})\ncnt: 1, ((T([32, 96, 1, 1], f16), T([4, 96, 1, 1], f16), T([4], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 4, 1, 1], f16), T([96, 4, 1, 1], f16), T([96], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 96, 56, 56], f16), T([24, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 24, 56, 56], f16), T([144, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 144, 56, 56], f16), T([144, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 144), {})\ncnt: 2, ((T([32, 144, 1, 1], f16), T([6, 144, 1, 1], f16), T([6], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 6, 1, 1], f16), T([144, 6, 1, 1], f16), T([144], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 144, 56, 56], f16), T([24, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 144, 56, 56], f16), T([144, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 144), {})\ncnt: 1, ((T([32, 144, 28, 28], f16), T([40, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 40, 28, 28], f16), T([240, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([240, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 240), {})\ncnt: 2, ((T([32, 240, 1, 1], f16), T([10, 240, 1, 1], f16), T([10], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 10, 1, 1], f16), T([240, 10, 1, 1], f16), T([240], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([40, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([240, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 240), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), T([80, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 80, 14, 14], f16), T([480, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 480, 14, 14], f16), T([480, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 480), {})\ncnt: 3, ((T([32, 480, 1, 1], f16), T([20, 480, 1, 1], f16), T([20], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 20, 1, 1], f16), T([480, 20, 1, 1], f16), T([480], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 480, 14, 14], f16), T([80, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 480, 14, 14], f16), T([480, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 480), {})\ncnt: 1, ((T([32, 480, 14, 14], f16), T([112, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 112, 14, 14], f16), T([672, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 3, ((T([32, 672, 1, 1], f16), T([28, 672, 1, 1], f16), T([28], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 28, 1, 1], f16), T([672, 28, 1, 1], f16), T([672], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 672, 14, 14], f16), T([112, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([192, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([32, 192, 7, 7], f16), T([1152, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 1152, 7, 7], f16), T([1152, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1152), {})\ncnt: 4, ((T([32, 1152, 1, 1], f16), T([48, 1152, 1, 1], f16), T([48], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([32, 48, 1, 1], f16), T([1152, 48, 1, 1], f16), T([1152], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 1152, 7, 7], f16), T([192, 1152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 1152, 7, 7], f16), T([1152, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1152), {})\ncnt: 1, ((T([32, 1152, 7, 7], f16), T([320, 1152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Multiplication Operations\nDESCRIPTION: Tensor multiplication operations with broadcasting and scaling, including both regular multiplication and in-place operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n((T([8, 3, 12, 16, 2], f16), T([1, 3, 1, 1, 2], f32)), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Cloning Operations in PyTorch\nDESCRIPTION: Usage statistics of the aten.clone.default operator which creates a copy of input tensors. This operation is commonly used to ensure that tensors have their own memory when needed for in-place operations or to preserve original values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 192, 192], f16),), {})\ncnt: 1, ((T([128, 256, 48, 48], f16),), {})\ncnt: 2, ((T([128, 512, 24, 24], f16),), {})\ncnt: 6, ((T([128, 1536, 12, 12], f16),), {})\ncnt: 3, ((T([128, 1536, 6, 6], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Logging ATen Operator Invocation in PyTorch - Text\nDESCRIPTION: This snippet logs the invocation summary of ATen operators used in PyTorch models. It records operator names, argument shapes, operand types (e.g., 'f16'), key parameters, and number of occurrences ('cnt') in a text format. The logging aids in profiling, trace analysis, and optimization by providing per-operator utilization statistics. Inputs include tensor shapes and parameters; the output is a human-readable summary without executable code. There are no dependencies except access to operator invocation events.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swin_base_patch4_window7_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 2, ((T([4096, 4, 49, 49], f16), -1, False), {})\ncnt: 2, ((T([1024, 8, 49, 49], f16), -1, False), {})\ncnt: 18, ((T([256, 16, 49, 49], f16), -1, False), {})\ncnt: 2, ((T([64, 32, 49, 49], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 2, ((T([64, 32, 49, 49], f16), T([64, 32, 49, 49], f16), -1, f16), {})\ncnt: 18, ((T([256, 16, 49, 49], f16), T([256, 16, 49, 49], f16), -1, f16), {})\ncnt: 2, ((T([1024, 8, 49, 49], f16), T([1024, 8, 49, 49], f16), -1, f16), {})\ncnt: 2, ((T([4096, 4, 49, 49], f16), T([4096, 4, 49, 49], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 6, ((T([4096, 4, 49, 32], f16), [16384, 49, 32]), {})\ncnt: 2, ((T([4096, 4, 32, 49], f16), [16384, 32, 49]), {})\ncnt: 2, ((T([16384, 49, 49], f16), [4096, 4, 49, 49]), {})\ncnt: 2, ((T([16384, 49, 32], f16), [4096, 4, 49, 32]), {})\ncnt: 2, ((T([4096, 49, 4, 32], f16), [4096, 49, 128]), {})\ncnt: 1, ((T([50176, 256], f16), [64, 784, 256]), {})\ncnt: 6, ((T([1024, 8, 49, 32], f16), [8192, 49, 32]), {})\ncnt: 2, ((T([1024, 8, 32, 49], f16), [8192, 32, 49]), {})\ncnt: 2, ((T([8192, 49, 49], f16), [1024, 8, 49, 49]), {})\ncnt: 2, ((T([8192, 49, 32], f16), [1024, 8, 49, 32]), {})\ncnt: 2, ((T([1024, 49, 8, 32], f16), [1024, 49, 256]), {})\ncnt: 1, ((T([12544, 512], f16), [64, 196, 512]), {})\ncnt: 54, ((T([256, 16, 49, 32], f16), [4096, 49, 32]), {})\ncnt: 18, ((T([256, 16, 32, 49], f16), [4096, 32, 49]), {})\ncnt: 18, ((T([4096, 49, 49], f16), [256, 16, 49, 49]), {})\ncnt: 18, ((T([4096, 49, 32], f16), [256, 16, 49, 32]), {})\ncnt: 18, ((T([256, 49, 16, 32], f16), [256, 49, 512]), {})\ncnt: 1, ((T([3136, 1024], f16), [64, 49, 1024]), {})\ncnt: 6, ((T([64, 32, 49, 32], f16), [2048, 49, 32]), {})\ncnt: 2, ((T([64, 32, 32, 49], f16), [2048, 32, 49]), {})\ncnt: 2, ((T([2048, 49, 49], f16), [64, 32, 49, 49]), {})\ncnt: 2, ((T([2048, 49, 32], f16), [64, 32, 49, 32]), {})\ncnt: 2, ((T([64, 49, 32, 32], f16), [64, 49, 1024]), {})\ncnt: 2, ((T([64, 49, 3, 32, 32], f16), [64, 49, 3072]), {})\ncnt: 18, ((T([64, 2, 2, 7, 7, 512], f16), [256, 7, 7, 512]), {})\ncnt: 18, ((T([256, 49, 3, 16, 32], f16), [256, 49, 1536]), {})\ncnt: 18, ((T([64, 2, 7, 2, 7, 512], f16), [64, 14, 14, 512]), {})\ncnt: 2, ((T([64, 4, 4, 7, 7, 256], f16), [1024, 7, 7, 256]), {})\ncnt: 2, ((T([1024, 49, 3, 8, 32], f16), [1024, 49, 768]), {})\ncnt: 2, ((T([64, 4, 7, 4, 7, 256], f16), [64, 28, 28, 256]), {})\ncnt: 2, ((T([64, 8, 8, 7, 7, 128], f16), [4096, 7, 7, 128]), {})\ncnt: 2, ((T([4096, 49, 3, 4, 32], f16), [4096, 49, 384]), {})\ncnt: 2, ((T([64, 8, 7, 8, 7, 128], f16), [64, 56, 56, 128]), {})\nOperator: aten.add.Tensor\ncnt: 2, ((T([4096, 4, 49, 49], f16), T([1, 4, 49, 49], f16)), {})\ncnt: 8, ((T([64, 3136, 128], f16), T([64, 3136, 128], f16)), {})\ncnt: 1, ((T([64, 64, 4, 49, 49], f16), T([1, 64, 1, 49, 49], f16)), {})\ncnt: 2, ((T([1024, 8, 49, 49], f16), T([1, 8, 49, 49], f16)), {})\ncnt: 8, ((T([64, 784, 256], f16), T([64, 784, 256], f16)), {})\ncnt: 1, ((T([64, 16, 8, 49, 49], f16), T([1, 16, 1, 49, 49], f16)), {})\ncnt: 18, ((T([256, 16, 49, 49], f16), T([1, 16, 49, 49], f16)), {})\ncnt: 72, ((T([64, 196, 512], f16), T([64, 196, 512], f16)), {})\ncnt: 9, ((T([64, 4, 16, 49, 49], f16), T([1, 4, 1, 49, 49], f16)), {})\ncnt: 2, ((T([64, 32, 49, 49], f16), T([1, 32, 49, 49], f16)), {})\ncnt: 8, ((T([64, 49, 1024], f16), T([64, 49, 1024], f16)), {})\ncnt: 3, ((T([64, 14, 14, 512], f16), T([64, 14, 14, 512], f16)), {})\ncnt: 3, ((T([64, 28, 28, 256], f16), T([64, 28, 28, 256], f16)), {})\ncnt: 3, ((T([64, 56, 56, 128], f16), T([64, 56, 56, 128], f16)), {})\nOperator: aten.addmm.default\ncnt: 2, ((T([384], f16), T([200704, 128], f16), T([128, 384], f16, stride=(1, 128))), {})\ncnt: 2, ((T([128], f16), T([200704, 128], f16), T([128, 128], f16, stride=(1, 128))), {})\ncnt: 2, ((T([512], f16), T([200704, 128], f16), T([128, 512], f16, stride=(1, 128))), {})\ncnt: 2, ((T([128], f16), T([200704, 512], f16), T([512, 128], f16, stride=(1, 512))), {})\ncnt: 2, ((T([768], f16), T([50176, 256], f16), T([256, 768], f16, stride=(1, 256))), {})\ncnt: 2, ((T([256], f16), T([50176, 256], f16), T([256, 256], f16, stride=(1, 256))), {})\ncnt: 2, ((T([1024], f16), T([50176, 256], f16), T([256, 1024], f16, stride=(1, 256))), {})\ncnt: 2, ((T([256], f16), T([50176, 1024], f16), T([1024, 256], f16, stride=(1, 1024))), {})\ncnt: 18, ((T([1536], f16), T([12544, 512], f16), T([512, 1536], f16, stride=(1, 512))), {})\ncnt: 18, ((T([512], f16), T([12544, 512], f16), T([512, 512], f16, stride=(1, 512))), {})\ncnt: 18, ((T([2048], f16), T([12544, 512], f16), T([512, 2048], f16, stride=(1, 512))), {})\ncnt: 18, ((T([512], f16), T([12544, 2048], f16), T([2048, 512], f16, stride=(1, 2048))), {})\ncnt: 2, ((T([3072], f16), T([3136, 1024], f16), T([1024, 3072], f16, stride=(1, 1024))), {})\ncnt: 2, ((T([1024], f16), T([3136, 1024], f16), T([1024, 1024], f16, stride=(1, 1024))), {})\ncnt: 2, ((T([4096], f16), T([3136, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})\ncnt: 2, ((T([1024], f16), T([3136, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})\ncnt: 1, ((T([1000], f16), T([64, 1024], f16), T([1024, 1000], f16, stride=(1, 1024))), {})\nOperator: aten.bernoulli_.float\ncnt: 2, ((T([64, 1, 1], f16), 0.9956521736457944), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9913043472915888), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9869565209373832), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9826086945831776), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9782608672976494), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9739130418747663), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9695652164518833), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9652173891663551), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.960869561880827), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9565217345952988), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9521739110350609), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9478260837495327), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9434782564640045), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9391304329037666), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9347826093435287), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9304347857832909), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9260869547724724), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9217391312122345), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.917391300201416), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9130434766411781), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9086956530809402), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9043478220701218), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.8999999985098839), {})\nOperator: aten.bmm.default\ncnt: 2, ((T([16384, 49, 32], f16), T([16384, 32, 49], f16)), {})\ncnt: 2, ((T([16384, 49, 49], f16), T([16384, 49, 32], f16)), {})\ncnt: 2, ((T([8192, 49, 32], f16), T([8192, 32, 49], f16)), {})\ncnt: 2, ((T([8192, 49, 49], f16), T([8192, 49, 32], f16)), {})\ncnt: 18, ((T([4096, 49, 32], f16), T([4096, 32, 49], f16)), {})\ncnt: 18, ((T([4096, 49, 49], f16), T([4096, 49, 32], f16)), {})\ncnt: 2, ((T([2048, 49, 32], f16), T([2048, 32, 49], f16)), {})\ncnt: 2, ((T([2048, 49, 49], f16), T([2048, 49, 32], f16)), {})\ncnt: 2, ((T([2048, 49, 49], f16, stride=(2401, 1, 49)), T([2048, 49, 32], f16)), {})\ncnt: 2, ((T([2048, 49, 32], f16), T([2048, 32, 49], f16, stride=(1568, 1, 32))), {})\ncnt: 2, ((T([2048, 32, 49], f16, stride=(1568, 1, 32)), T([2048, 49, 49], f16)), {})\ncnt: 2, ((T([2048, 49, 49], f16), T([2048, 49, 32], f16, stride=(1568, 1, 49))), {})\ncnt: 18, ((T([4096, 49, 49], f16, stride=(2401, 1, 49)), T([4096, 49, 32], f16)), {})\ncnt: 18, ((T([4096, 49, 32], f16), T([4096, 32, 49], f16, stride=(1568, 1, 32))), {})\ncnt: 18, ((T([4096, 32, 49], f16, stride=(1568, 1, 32)), T([4096, 49, 49], f16)), {})\ncnt: 18, ((T([4096, 49, 49], f16), T([4096, 49, 32], f16, stride=(1568, 1, 49))), {})\ncnt: 2, ((T([8192, 49, 49], f16, stride=(2401, 1, 49)), T([8192, 49, 32], f16)), {})\ncnt: 2, ((T([8192, 49, 32], f16), T([8192, 32, 49], f16, stride=(1568, 1, 32))), {})\ncnt: 2, ((T([8192, 32, 49], f16, stride=(1568, 1, 32)), T([8192, 49, 49], f16)), {})\ncnt: 2, ((T([8192, 49, 49], f16), T([8192, 49, 32], f16, stride=(1568, 1, 49))), {})\ncnt: 2, ((T([16384, 49, 49], f16, stride=(2401, 1, 49)), T([16384, 49, 32], f16)), {})\ncnt: 2, ((T([16384, 49, 32], f16), T([16384, 32, 49], f16, stride=(1568, 1, 32))), {})\ncnt: 2, ((T([16384, 32, 49], f16, stride=(1568, 1, 32)), T([16384, 49, 49], f16)), {})\ncnt: 2, ((T([16384, 49, 49], f16), T([16384, 49, 32], f16, stride=(1568, 1, 49))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([64, 28, 28, 128], f16, stride=(401408, 14336, 256, 1)), T([64, 28, 28, 128], f16, stride=(401408, 14336, 256, 1)), T([64, 28, 28, 128], f16, stride=(401408, 14336, 256, 1)), T([64, 28, 28, 128], f16, stride=(401408, 14336, 256, 1))], -1), {})\ncnt: 1, (([T([64, 14, 14, 256], f16, stride=(200704, 14336, 512, 1)), T([64, 14, 14, 256], f16, stride=(200704, 14336, 512, 1)), T([64, 14, 14, 256], f16, stride=(200704, 14336, 512, 1)), T([64, 14, 14, 256], f16, stride=(200704, 14336, 512, 1))], -1), {})\ncnt: 1, (([T([64, 7, 7, 512], f16, stride=(100352, 14336, 1024, 1)), T([64, 7, 7, 512], f16, stride=(100352, 14336, 1024, 1)), T([64, 7, 7, 512], f16, stride=(100352, 14336, 1024, 1)), T([64, 7, 7, 512], f16, stride=(100352, 14336, 1024, 1))], -1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([128, 3, 4, 4], f16), T([128], f16), [4, 4], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([64, 3, 224, 224], f16), T([128, 3, 4, 4], f16), [128], [4, 4], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([64, 49, 1024], f16, stride=(1024, 0, 1)), 49), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.sum.default Operator (Text)\nDESCRIPTION: Logs calls to the `aten.sum.default` operator, likely summing all elements of the input tensor. The arguments only show the input tensor shape (f16), indicating a reduction across all dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sum.default\ncnt: 3, ((T([128, 1536, 6, 6], f16),), {})\ncnt: 6, ((T([128, 1536, 12, 12], f16),), {})\ncnt: 2, ((T([128, 512, 24, 24], f16),), {})\ncnt: 1, ((T([128, 256, 48, 48], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Count Analysis\nDESCRIPTION: Logs of tensor operations showing count (cnt), tensor shapes, and data types for various PyTorch operators including hardswish_backward, index operations, batch normalization, and matrix multiplications. Each entry shows the operator name, count of calls, input tensor specifications and any additional parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((T([128, 196, 128], f16),), {})\ncnt: 4, ((T([128, 196, 256], f16),), {})\ncnt: 6, ((T([128, 49, 512], f16),), {})\ncnt: 4, ((T([128, 49, 256], f16),), {})\ncnt: 1, ((T([128, 16, 1024], f16),), {})\ncnt: 5, ((T([128, 16, 768], f16),), {})\ncnt: 4, ((T([128, 16, 384], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Logging Lift Fresh Copy Operations (aten.lift_fresh_copy) - PyTorch - Python\nDESCRIPTION: This snippet registers call patterns for the aten.lift_fresh_copy operator, which creates a fresh copy of PyTorch tensors, typically with altered memory layout or device. Only simple tensor shape and dtype specifications are logged. Useful for tracking data movement and aliasing behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\n```\n\n----------------------------------------\n\nTITLE: Using experimental local_map feature\nDESCRIPTION: Applies a local function to each shard of a DTensor using the experimental local_map function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nlocal_map(dtensor, local_func)\n```\n\n----------------------------------------\n\nTITLE: Specifying Tensor Allocations for aten.new_zeros.default - Python\nDESCRIPTION: These test cases configure 'aten.new_zeros.default' PyTorch operator invocations, specifying input tensors and desired output shapes as flattened element counts. Used for checking the creation of zeroed tensors, support for various shapes, and ensuring the correct data type (usually half-precision). Requires PyTorch and is limited by device memory for large shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.new_zeros.default\ncnt: 5, ((T([128, 160, 7, 7], f16), [1003520]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 112, 14, 14], f16), [2809856]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ReLU Activation Operations in PyTorch\nDESCRIPTION: This snippet shows multiple instances of in-place ReLU activation operations with different tensor shapes and channel sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 32, 112, 112], f16),), {})\ncnt: 1, ((T([128, 48, 112, 112], f16),), {})\ncnt: 1, ((T([128, 48, 56, 56], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Reflection Padding Operations\nDESCRIPTION: This snippet shows the usage of the aten.reflection_pad2d.default operator for applying reflection padding to tensors with various shapes and padding values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.reflection_pad2d.default\ncnt: 2, ((T([3, 3, 512, 512], f16), [3, 3, 3, 3]), {})\ncnt: 1, ((T([3, 1, 512, 512], f16), [3, 3, 3, 3]), {})\ncnt: 1, ((T([3, 4, 512, 512], f16), [3, 3, 3, 3]), {})\ncnt: 26, ((T([3, 256, 128, 128], f16), [1, 1, 1, 1]), {})\ncnt: 2, ((T([3, 64, 512, 512], f16), [3, 3, 3, 3]), {})\n```\n\n----------------------------------------\n\nTITLE: Type Mismatch Error Illustration in TorchScript - PyTorch - Python\nDESCRIPTION: This code illustrates a runtime type mismatch error in TorchScript when a variable is assigned different types in different branches of an if-statement. The snippet, using the @torch.jit.script decorator, defines a function that returns a torch.Tensor or an int depending on a boolean. Since TorchScript enforces static typing, it triggers a compilation error. PyTorch is required as a dependency and the code is used to show TorchScript limitations on variable assignment types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.jit.script\ndef an_error(x):\n    if x:\n        r = torch.rand(1)\n    else:\n        r = 4\n    return r\n```\n\n----------------------------------------\n\nTITLE: Tensor Addition Operations in PyTorch ResNet Model\nDESCRIPTION: This snippet shows various tensor addition operations used in a ResNet model, including skip connections. Operations mainly work with half-precision (f16) tensors of different sizes corresponding to different stages of the network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 7, ((T([1, 64, 1, 1], f16), 0.0), {})\ncnt: 1, ((T([4, 64, 592, 608], f16), T([1, 64, 1, 1], f16)), {})\ncnt: 6, ((T([4, 64, 296, 304], f16), T([1, 64, 1, 1], f16)), {})\ncnt: 16, ((T([1, 256, 1, 1], f16), 0.0), {})\ncnt: 4, ((T([4, 256, 296, 304], f16), T([1, 256, 1, 1], f16)), {})\ncnt: 8, ((T([1, 128, 1, 1], f16), 0.0), {})\ncnt: 1, ((T([4, 128, 296, 304], f16), T([1, 128, 1, 1], f16)), {})\ncnt: 7, ((T([4, 128, 148, 152], f16), T([1, 128, 1, 1], f16)), {})\ncnt: 11, ((T([1, 512, 1, 1], f16), 0.0), {})\ncnt: 5, ((T([4, 512, 148, 152], f16), T([1, 512, 1, 1], f16)), {})\ncnt: 1, ((T([4, 256, 148, 152], f16), T([1, 256, 1, 1], f16)), {})\ncnt: 11, ((T([4, 256, 74, 76], f16), T([1, 256, 1, 1], f16)), {})\ncnt: 7, ((T([1, 1024, 1, 1], f16), 0.0), {})\ncnt: 7, ((T([4, 1024, 74, 76], f16), T([1, 1024, 1, 1], f16)), {})\ncnt: 1, ((T([4, 512, 74, 76], f16), T([1, 512, 1, 1], f16)), {})\ncnt: 5, ((T([4, 512, 37, 38], f16), T([1, 512, 1, 1], f16)), {})\ncnt: 4, ((T([1, 2048, 1, 1], f16), 0.0), {})\ncnt: 4, ((T([4, 2048, 37, 38], f16), T([1, 2048, 1, 1], f16)), {})\ncnt: 2, ((T([4, 256, 74, 76], f16), T([4, 256, 74, 76], f16)), {})\ncnt: 2, ((T([4, 256, 148, 152], f16), T([4, 256, 148, 152], f16)), {})\ncnt: 1, ((T([4, 256, 296, 304], f16), T([4, 256, 296, 304], f16)), {})\ncnt: 1, ((T([89984, 1, 4], i32), T([1, 3, 4], f16)), {})\ncnt: 1, ((T([22496, 1, 4], i32), T([1, 3, 4], f16)), {})\ncnt: 1, ((T([5624, 1, 4], i32), T([1, 3, 4], f16)), {})\ncnt: 1, ((T([1406, 1, 4], i32), T([1, 3, 4], f16)), {})\ncnt: 1, ((T([361, 1, 4], i32), T([1, 3, 4], f16)), {})\ncnt: 2, ((T([1438452], f16, stride=(4,)), T([1438452], f16)), {})\ncnt: 4, ((T([1438452, 1], f16), T([1438452, 1], f16)), {})\ncnt: 1, ((T([4, 1000], i64), 0), {})\ncnt: 1, ((T([4, 1000], i64), 269952), {})\ncnt: 1, ((T([4, 1000], i64), 337440), {})\ncnt: 1, ((T([4, 1000], i64), 354312), {})\ncnt: 1, ((T([4, 1000], i64), 358530), {})\ncnt: 2, ((T([0], f32), 4), {})\ncnt: 2, ((T([0], f32), T([], f32)), {})\ncnt: 18, ((T([0], f16), T([0], f16)), {})\ncnt: 2, ((T([0, 91], f16), T([0, 1], f16)), {})\ncnt: 6, ((T([0, 91], f16), T([0, 91], f16)), {})\ncnt: 4, ((T([], f16), 0), {})\ncnt: 4, ((T([], f16), T([], f32)), {})\ncnt: 8, ((T([], f32), T([], f16)), {})\ncnt: 1, ((T([], f32), 0), {})\ncnt: 3, ((T([], f32), T([], f32)), {})\ncnt: 7, ((T([0, 364], f16), T([0, 364], f16)), {})\ncnt: 1, ((T([0, 1024], f16), T([0, 1024], f16)), {})\ncnt: 1, ((T([4, 256, 37, 38], f16), T([4, 256, 37, 38], f16)), {})\ncnt: 2, ((T([4, 2048, 37, 38], f16), T([4, 2048, 37, 38], f16)), {})\ncnt: 7, ((T([4, 1024, 74, 76], f16), T([4, 1024, 74, 76], f16)), {})\ncnt: 5, ((T([4, 512, 148, 152], f16), T([4, 512, 148, 152], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Running Training Processes Using Bash\nDESCRIPTION: Execute a training script with multiple processes using a Bash command. This snippet illustrates how to initiate the Python script 'main.py' with 4 subprocesses intended for CPU training. This requires a Python environment with PyTorch and expected inputs are command-line arguments for specifying the number of processes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/multiprocessing.rst#2025-04-22_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npython main.py --num-processes 4\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations in PyTorch\nDESCRIPTION: Collection of PyTorch tensor operations including batch normalization, pooling, matrix multiplication, and activation functions. Shows tensor shapes, data types, and call frequencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Sample tensor operations\nT([64, 512, 28, 28], f16, stride=(512, 1, 0, 0))\naten.max_pool2d_with_indices.default((T([64, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]))\naten.native_batch_norm.default((T([64, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05))\naten.mm.default((T([64, 1000], f16), T([1000, 2048], f16)))\naten.relu_.default((T([64, 32, 112, 112], f16),))\naten.sigmoid.default((T([64, 1, 256], f16),))\n```\n\n----------------------------------------\n\nTITLE: Forward Convolution Operations\nDESCRIPTION: Collection of forward convolution operations with varying input/output tensor shapes, kernel sizes, strides and padding configurations. Operations use float16 precision and include parameters for groups and dilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 112, 33, 33], f16), T([112, 1, 7, 7], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 112), {})\n```\n\n----------------------------------------\n\nTITLE: Saving ONNX Model to File in Python\nDESCRIPTION: This Python snippet saves the in-memory ONNX model represented by onnx_program.model_proto to a Protobuf file format using the save method. The file can be used for further analysis or deployment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nonnx_program.save(\"mlp.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Dimension Flattening and Splitting in PyTorch\nDESCRIPTION: Shows how to use tuples of dimensions for splitting and flattening tensor dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ni, j, k = dims(3)\nj.size = 2\nA = torch.rand(6, 4)\na = A[(i, j), k] # split dim 0 into i,j\nprint(i.size, j.size, k.size)\n\nr = a.order(i, (j, k)) # flatten j and k\nprint(r.shape)\n```\n\n----------------------------------------\n\nTITLE: Custom LogExp Autograd Function Example\nDESCRIPTION: Example of an autograd function that will be automatically inlined during ONNX export into basic ONNX operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nclass MyLogExp(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input: torch.Tensor) -> torch.Tensor:\n        ctx.save_for_backward(input)\n        h = input.exp()\n        return h.log().log()\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch SiLU Activation Operations\nDESCRIPTION: This snippet shows the usage of the SiLU (Swish) activation function in PyTorch. It includes both the forward (silu_) and backward (silu_backward) operations, detailing tensor shapes and data types across various layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.silu_.default\ncnt: 2, ((T([32, 32, 112, 112], f16),), {})\ncnt: 1, ((T([32, 8, 1, 1], f16),), {})\ncnt: 1, ((T([32, 96, 112, 112], f16),), {})\n# ... (truncated for brevity)\n\nOperator: aten.silu_backward.default\ncnt: 1, ((T([32, 1280, 7, 7], f16), T([32, 1280, 7, 7], f16)), {})\ncnt: 4, ((T([32, 48, 1, 1], f16), T([32, 48, 1, 1], f16)), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Custom SaliencyPruner Implementation\nDESCRIPTION: Implementation of a custom pruner that extends BaseStructuredSparsifier. This pruner uses row-wise L1 norm saliency to determine which rows to prune, removing those with the smallest total magnitude.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/pruner/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass SaliencyPruner(BaseStructuredSparsifier):\n     \"\"\"\n     Prune filters based on the saliency\n     The saliency for a filter is given by the sum of the L1 norms of all of its weights\n     \"\"\"\n\n     def update_mask(self, module, tensor_name, **kwargs):\n        # tensor_name will give you the FQN, all other keys in pruning config are present in kwargs\n         weights = getattr(module, tensor_name)\n         mask = getattr(module.parametrizations, tensor_name)[0].mask\n\n         # use negative weights so we can use topk (we prune out the smallest)\n         saliency = -weights.norm(dim=tuple(range(1, weights.dim())), p=1)\n         num_to_pick = int(len(mask) * kwargs[\"sparsity_level\"])\n         prune = saliency.topk(num_to_pick).indices\n\n         # Set the mask to be false for the rows we want to prune\n         mask.data[prune] = False\n```\n\n----------------------------------------\n\nTITLE: vmap with Same Randomness Behavior\nDESCRIPTION: This example demonstrates using vmap with 'same' randomness flag, where random values are generated identically for each element in the batch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef add_noise(x):\n  y = torch.randn(())  # y will be the same across the batch\n  return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"same\")(x)  # we get the same value, repeated 3 times\n```\n\n----------------------------------------\n\nTITLE: Registering TensorExpr C++ Test Cases\nDESCRIPTION: Illustrates how to register newly created test cases (like `testCaseOne` and `testCaseTwo`) in the `tests.h` file. Test cases are added to the `TH_FORALL_TESTS` macro (or `TH_FORALL_TESTS_CUDA` for CUDA tests), omitting the `test` prefix from the function name.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/tensorexpr/README.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n// Add to TH_FORALL_TESTS_CUDA instead for CUDA-requiring tests\n#define TH_FORALL_TESTS(_)             \\\n  _(ADFormulas)                        \\\n  _(Attributes)                        \\\n  ...\n  _(CaseOne)  // note that the `test` prefix is omitted.\n  _(CaseTwo)\n```\n\n----------------------------------------\n\nTITLE: Running oneDNN Graph Tests with Verbose Logging\nDESCRIPTION: Command to run tests with detailed logging enabled to observe the fusion pipeline stages including mutation removal, graph fusion, and kernel execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/onednn/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nDNNL_VERBOSE=1 PYTORCH_JIT_LOG_LEVEL=\">>graph_helper:>>graph_fuser:>>kernel:>>interface\" python -u test/test_jit_llga_fuser.py -k test_conv2d_eltwise\n```\n\n----------------------------------------\n\nTITLE: Disabling NumPy Tracing for Debugging\nDESCRIPTION: Shows how to disable tracing through NumPy functions for debugging purposes using torch._dynamo configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom torch._dynamo import config\nconfig.trace_numpy = False\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.relu_.default in PyTorch ATen\nDESCRIPTION: Logs calls to the `aten.relu_.default` operator, which performs an in-place Rectified Linear Unit (ReLU) activation function on the input tensor. The log shows calls with 4D input tensors of data type `f16`. The `cnt` indicates call frequency.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 32, 1, 1], f16),), {})\ncnt: 1, ((T([128, 64, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Using Loss Parallelism Context Manager in Python\nDESCRIPTION: This snippet demonstrates the usage of the loss_parallel context manager for parallelized cross-entropy loss computation in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.parallel.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.distributed.tensor.parallel import loss_parallel\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Tensor from List\nDESCRIPTION: Demonstrates creating a tensor from a Python list using torch.tensor constructor. Shows both float and numpy array inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensors.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n```\n\n----------------------------------------\n\nTITLE: Analyzing Scalar Division Patterns (aten.div) - PyTorch - Python\nDESCRIPTION: This code catalogues how the aten.div operator (elementwise scalar division) is applied across tensors of different shapes, datatypes, and memory layouts (strides); each entry lists arguments and scalar divisors. Dependencies are standard PyTorch tensor broadcasting and dtype rules. Inputs are tensors and scalars as listed, focus is on invocation variety.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 2304, 7, 7], f16, stride=(2304, 1, 0, 0)), 49), {})\ncnt: 3, ((T([128, 1536, 7, 7], f16, stride=(1536, 1, 0, 0)), 49), {})\ncnt: 6, ((T([128, 1536, 14, 14], f16, stride=(1536, 1, 0, 0)), 196), {})\ncnt: 2, ((T([128, 512, 28, 28], f16, stride=(512, 1, 0, 0)), 784), {})\ncnt: 1, ((T([128, 256, 56, 56], f16, stride=(256, 1, 0, 0)), 3136), {})\n```\n\n----------------------------------------\n\nTITLE: Declaring PyTorch Distribution Modules using py:module Directive\nDESCRIPTION: These reStructuredText directives use `py:module` to declare specific Python modules belonging to the `torch.distributions` package. This allows Sphinx, the documentation generator, to recognize these modules, generate appropriate documentation pages, and manage cross-references within the PyTorch documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributions.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.multinomial\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.multivariate_normal\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.negative_binomial\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.normal\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.one_hot_categorical\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.pareto\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.poisson\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.relaxed_bernoulli\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.relaxed_categorical\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.studentT\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.transformed_distribution\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.uniform\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.utils\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.von_mises\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.weibull\n```\n\nLANGUAGE: rst\nCODE:\n```\n.. py:module:: torch.distributions.wishart\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This code snippet demonstrates the usage of various PyTorch operators in a deep learning model. It includes operations like convolutions, matrix multiplications, activation functions, and tensor manipulations. The snippet shows the operator names, usage counts, and input tensor shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixer_b16_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([64, 768, 384], f16), [64, 768, 384]), {})\ncnt: 12, ((T([64, 768, 196], f16), [49152, 196]), {})\nOperator: aten.add.Tensor\ncnt: 12, ((T([64, 768, 384], f16), T([384], f16)), {})\ncnt: 12, ((T([64, 196, 768], f16, stride=(150528, 1, 196)), T([64, 196, 768], f16, stride=(150528, 1, 196))), {})\ncnt: 12, ((T([64, 196, 768], f16, stride=(150528, 1, 196)), T([64, 196, 768], f16)), {})\ncnt: 12, ((T([64, 196, 768], f16), T([64, 196, 768], f16)), {})\ncnt: 12, ((T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(150528, 1, 196))), {})\nOperator: aten.addmm.default\ncnt: 12, ((T([196], f16), T([49152, 384], f16), T([384, 196], f16, stride=(1, 384))), {})\ncnt: 12, ((T([3072], f16), T([12544, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 12, ((T([768], f16), T([12544, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 1, ((T([1000], f16), T([64, 768], f16), T([768, 1000], f16, stride=(1, 768))), {})\nOperator: aten.bmm.default\ncnt: 12, ((T([64, 768, 196], f16, stride=(150528, 1, 768)), T([64, 196, 384], f16, stride=(0, 1, 196))), {})\ncnt: 12, ((T([64, 196, 768], f16), T([64, 768, 384], f16)), {})\ncnt: 12, ((T([64, 768, 384], f16), T([64, 384, 196], f16, stride=(0, 196, 1))), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([768, 3, 16, 16], f16), T([768], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 768, 14, 14], f16, stride=(150528, 1, 10752, 768)), T([64, 3, 224, 224], f16), T([768, 3, 16, 16], f16), [768], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\ncnt: 12, ((T([384, 196], f16), T([384, 196], f16, stride=(1, 384))), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([64, 196, 768], f16, stride=(768, 0, 1)), 196), {})\nOperator: aten.gelu.default\ncnt: 12, ((T([64, 768, 384], f16),), {})\ncnt: 12, ((T([64, 196, 3072], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([64, 196, 3072], f16), T([64, 196, 3072], f16)), {})\ncnt: 12, ((T([64, 768, 384], f16), T([64, 768, 384], f16)), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([64], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([64, 196, 768], f16), [1]), {})\nOperator: aten.mm.default\ncnt: 1, ((T([64, 1000], f16), T([1000, 768], f16)), {})\ncnt: 1, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 768], f16)), {})\ncnt: 12, ((T([12544, 768], f16), T([768, 3072], f16)), {})\ncnt: 12, ((T([768, 12544], f16, stride=(1, 768)), T([12544, 3072], f16)), {})\ncnt: 12, ((T([12544, 3072], f16), T([3072, 768], f16)), {})\ncnt: 12, ((T([3072, 12544], f16, stride=(1, 3072)), T([12544, 768], f16)), {})\ncnt: 12, ((T([49152, 196], f16), T([196, 384], f16)), {})\ncnt: 12, ((T([196, 49152], f16, stride=(1, 196)), T([49152, 384], f16)), {})\nOperator: aten.native_layer_norm.default\ncnt: 25, ((T([64, 196, 768], f16, stride=(150528, 1, 196)), [768], T([768], f16), T([768], f16), 1e-06), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 13, ((T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(150528, 1, 196)), [768], T([64, 196, 1], f32), T([64, 196, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\ncnt: 12, ((T([64, 196, 768], f16, stride=(150528, 1, 196)), T([64, 196, 768], f16, stride=(150528, 1, 196)), [768], T([64, 196, 1], f32), T([64, 196, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\nOperator: aten.new_empty_strided.default\ncnt: 12, ((T([384, 196], f16, stride=(1, 384)), [384, 196], [196, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\ncnt: 12, ((T([12544, 768], f16), [0], True), {})\ncnt: 12, ((T([12544, 3072], f16), [0], True), {})\ncnt: 12, ((T([49152, 196], f16), [0], True), {})\ncnt: 12, ((T([64, 768, 384], f16), [0, 1], True), {})\ncnt: 12, ((T([64, 196, 384], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Leaky ReLU Operations\nDESCRIPTION: Leaky ReLU activations with slope 0.1 applied to tensors of various sizes. Includes both forward and backward operations for gradient computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n((T([8, 32, 384, 512], f16), 0.1), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Example for aten.nll_loss_forward.default\nDESCRIPTION: Logs the forward pass for Negative Log Likelihood loss (`aten.nll_loss_forward.default`). Computes the loss based on input probabilities ([128, 1000] float16), target labels ([128] int64), optional weight (None), reduction type (1 for mean), and ignore index (-100).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Applying CMake Policies Conditionally (CMake)\nDESCRIPTION: Demonstrates how to append policies to a list and then apply each CMake policy, using a foreach loop and conditional statement to check for policy availability before setting it. This snippet ensures that only supported and required policies are adjusted, improving compatibility with different CMake versions. No external dependencies are needed. Requires CMake 3.13+ for reliable policy list handling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/inductor/cpp/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# TODO(voz): Fix hack below\n# Start hack\nlist(APPEND policies_new  CMP0079)\n\nforeach(policy ${policies_new})\n  if(POLICY ${policy})\n    cmake_policy(SET ${policy} NEW)\n  endif()\nendforeach()\n# End hack\n```\n\n----------------------------------------\n\nTITLE: GELU Backward Calculation in PyTorch (Python)\nDESCRIPTION: Calculates the backward pass of the GELU activation, achieved with aten.gelu_backward, necessary when differentiating GELU during backpropagation to adjust neural network parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\naten.gelu_backward.default\ncnt: 12, ((T([16, 512, 3072], f16), T([16, 512, 3072], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific Compilation Settings for PyTorch Python\nDESCRIPTION: Configures platform-specific compiler options, link flags, and libraries for Windows, macOS, and Linux builds of the PyTorch Python bindings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset(TORCH_PYTHON_COMPILE_DEFINITIONS)\n\nset(TORCH_PYTHON_COMPILE_OPTIONS)\n\nset(TORCH_PYTHON_LINK_FLAGS \"\")\n\nif(MSVC)\n    string(APPEND TORCH_PYTHON_LINK_FLAGS \" /NODEFAULTLIB:LIBCMT.LIB\")\n    list(APPEND TORCH_PYTHON_LINK_LIBRARIES ${PYTHON_LIBRARIES} onnx_library)\n    if(NOT CMAKE_BUILD_TYPE MATCHES \"Release\")\n      string(APPEND TORCH_PYTHON_LINK_FLAGS \" /DEBUG:FULL\")\n    endif()\nelseif(APPLE)\n    string(APPEND TORCH_PYTHON_LINK_FLAGS \" -undefined dynamic_lookup\")\nelse()\n    list(APPEND TORCH_PYTHON_COMPILE_OPTIONS\n      -fno-strict-aliasing\n      -Wno-strict-aliasing)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running CSAN Detection via Command Line\nDESCRIPTION: Command to run the CUDA Sanitizer on a Python script to detect synchronization issues.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda._sanitizer.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nTORCH_CUDA_SANITIZER=1 python example_error.py\n```\n\n----------------------------------------\n\nTITLE: Dimension Permutation Operations in PyTorch\nDESCRIPTION: Demonstrates how operations that permute dimensions handle named tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.randn(3, 3, names=('N', 'C'))\n>>> x.transpose('N', 'C').names\n('C', 'N')\n```\n\n----------------------------------------\n\nTITLE: Running All Benchmarks with Thread Control\nDESCRIPTION: Executes all benchmarks with explicit control over OpenMP and MKL threading.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m benchmark_all_test --omp-num-threads 1 --mkl-num-threads 1\n```\n\n----------------------------------------\n\nTITLE: Importing DataPipe and Typing Libraries in Python\nDESCRIPTION: Imports necessary classes and types for DataPipe typing, including IterDataPipe and various typing constructs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import IterDataPipe\nfrom typing import Any, Iterator, List, Tuple, TypeVar, Set, Union\n\nT_co = TypeVar('T_co', covariant=True)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with Bias in PyTorch\nDESCRIPTION: Performs matrix multiplication with added bias for fully connected layers. Different tensor shapes are used for various layers in the network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\naten.addmm.default((T([4096], f16), T([64, 25088], f16), T([25088, 4096], f16, stride=(1, 25088))), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.addmm.default((T([4096], f16), T([64, 4096], f16), T([4096, 4096], f16, stride=(1, 4096))), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.addmm.default((T([1000], f16), T([64, 4096], f16), T([4096, 1000], f16, stride=(1, 4096))), {})\n```\n\n----------------------------------------\n\nTITLE: Documenting Usage Patterns of ATen Operators in PyTorch (Python)\nDESCRIPTION: This snippet records invocation patterns for a wide range of ATen operators, specifying input/output tensor shapes, datatypes (such as 'f16' and 'i64'), strides, scalar arguments, and the frequency of each signature. The pseudo-Python tuple/list/dict format mimics how these operators are invoked in traced models, allowing for systematic analysis or replay. Required dependencies include the PyTorch 'torch' module with CUDA device support, and knowledge of ATen's tensor API. Inputs are operator invocations and tensor descriptors; outputs are structured pattern entries for analysis. Limitations include lack of actual computationthis only documents structures, not executable code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_struct_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([30, 4771], f16, stride=(1, 30)), -1, False), {})\ncnt: 1, ((T([30, 3600], f16), -1, False), {})\ncnt: 1, ((T([30], f16), -1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([30], f16), T([30], f16), -1, f16), {})\ncnt: 1, ((T([30, 3600], f16), T([30, 3600], f16), -1, f16), {})\ncnt: 1, ((T([30, 4771], f16), T([30, 4771], f16), -1, f16), {})\nOperator: aten.add.Tensor\ncnt: 4, ((T([30, 256], f16), T([30, 256], f16)), {})\ncnt: 1, ((T([], f16), 0), {})\ncnt: 2, ((T([], f16), T([], f16)), {})\ncnt: 4, ((T([30, 256], f16, stride=(1, 30)), T([30, 256], f16)), {})\nOperator: aten.addmm.default\ncnt: 10, ((T([256], f16), T([30, 256], f16), T([256, 256], f16, stride=(1, 256))), {})\nOperator: aten.bmm.default\ncnt: 1, ((T([1, 4771, 256], f16), T([1, 256, 30], f16, stride=(256, 1, 256))), {})\ncnt: 1, ((T([1, 30, 256], f16), T([1, 256, 3600], f16, stride=(256, 1, 256))), {})\ncnt: 1, ((T([1, 1, 256], f16), T([1, 256, 30], f16, stride=(256, 1, 256))), {})\ncnt: 1, ((T([1, 256, 1], f16), T([1, 1, 30], f16)), {})\ncnt: 1, ((T([1, 1, 30], f16), T([1, 30, 256], f16)), {})\ncnt: 1, ((T([1, 256, 30], f16, stride=(7680, 1, 256)), T([1, 30, 3600], f16)), {})\ncnt: 1, ((T([1, 30, 3600], f16), T([1, 3600, 256], f16)), {})\ncnt: 1, ((T([1, 256, 4771], f16, stride=(1221376, 1, 256)), T([1, 4771, 30], f16, stride=(4771, 1, 4771))), {})\ncnt: 1, ((T([1, 4771, 30], f16, stride=(4771, 1, 4771)), T([1, 30, 256], f16)), {})\nOperator: aten.clone.default\ncnt: 1, ((T([40, 29], i64, stride=(1, 40)),), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([40, 29], i64, stride=(1, 40)), T([40, 29], i64, stride=(1, 40))), {})\ncnt: 1, ((T([60, 60, 256], f16), T([60, 60, 256], f16, stride=(60, 1, 3600))), {})\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 34800), {})\ncnt: 2, ((T([], f16), 4320000), {})\ncnt: 2, ((T([], f16), 1200), {})\ncnt: 2, ((T([], f16), 3), {})\nOperator: aten.gather.default\ncnt: 1, ((T([40, 29, 30, 4771], f16, stride=(0, 0, 4771, 1)), 3, T([40, 29, 30, 1], i64, stride=(1, 40, 0, 1))), {})\nOperator: aten.mm.default\ncnt: 8, ((T([30, 256], f16), T([256, 256], f16)), {})\ncnt: 8, ((T([256, 30], f16, stride=(1, 256)), T([30, 256], f16)), {})\ncnt: 2, ((T([30, 256], f16, stride=(1, 30)), T([256, 256], f16)), {})\ncnt: 2, ((T([256, 30], f16), T([30, 256], f16)), {})\nOperator: aten.new_empty_strided.default\ncnt: 1, ((T([60, 60, 256], f16, stride=(60, 1, 3600)), [60, 60, 256], [15360, 256, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten.new_zeros.default\ncnt: 1, ((T([40, 29, 30, 1], f16, stride=(0, 0, 0, 1)), [40, 29, 30, 4771]), {})\nOperator: aten.relu.default\ncnt: 8, ((T([30, 256], f16),), {})\nOperator: aten.scatter_add.default\ncnt: 1, ((T([40, 29, 30, 4771], f16), 3, T([40, 29, 30, 1], i64, stride=(1, 40, 0, 1)), T([40, 29, 30, 1], f16, stride=(0, 0, 0, 1))), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([40, 30], f16, stride=(0, 0)), [0], True), {})\ncnt: 8, ((T([30, 256], f16), [0], True), {})\ncnt: 2, ((T([30, 256], f16, stride=(1, 30)), [0], True), {})\ncnt: 1, ((T([40, 30, 60, 60], f16, stride=(0, 0, 0, 0)), [0], True), {})\ncnt: 1, ((T([40, 29, 30, 4771], f16), [0, 1], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([40, 29, 30], f16),), {})\ncnt: 1, ((T([40, 30, 60, 60], f16, stride=(0, 3600, 60, 1)),), {})\ncnt: 1, ((T([40, 30], f16, stride=(0, 1)),), {})\nOperator: aten.threshold_backward.default\ncnt: 4, ((T([30, 256], f16, stride=(1, 30)), T([30, 256], f16), 0), {})\ncnt: 4, ((T([30, 256], f16), T([30, 256], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Cloning Tensors with ATen Clone Operator\nDESCRIPTION: Covers `aten.clone`, enabling tensor copying to prevent in-place modifications. Supports tensors with shapes such as [64, 3, 256, 256]. Essential for operations requiring tensor duplication before transformations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 256, 256], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in PyTorch\nDESCRIPTION: Records of batch normalization operations with various tensor shapes. Each operation includes input tensor, scale, bias, running mean, running variance, training mode flag, momentum, and epsilon parameters for normalization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f16), True, 0.1, 1e-05), {})\ncnt: 6, ((T([128, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([128, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f16), True, 0.1, 1e-05), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})\ncnt: 6, ((T([128, 288, 14, 14], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 8, ((T([128, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.nll_loss_backward.default Operator (Log)\nDESCRIPTION: Log entry detailing a call to the PyTorch `aten.nll_loss_backward.default` operator. It shows the invocation count (`cnt`) and arguments including gradient output tensor, model output tensor, target tensor, weight tensor (None), reduction type, ignore index, and total weight tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Adding Tensors in PyTorch - Python\nDESCRIPTION: This snippet shows the usage of PyTorch's aten.add.Tensor to perform tensor addition on tensors of different dimensions utilizing half precision (f16). Parameters include tensor shapes and data types. The operation is constrained by compatibility in dimensions for addition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 2, ((T([32, 960, 7, 7], f16), T([32, 960, 7, 7], f16)), {})\ncnt: 2, ((T([32, 160, 7, 7], f16), T([32, 160, 7, 7], f16)), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 7, 7], f16)), {})\ncnt: 1, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16)), {})\ncnt: 1, ((T([32, 112, 14, 14], f16), T([32, 112, 14, 14], f16)), {})\ncnt: 1, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16)), {})\ncnt: 3, ((T([32, 80, 14, 14], f16), T([32, 80, 14, 14], f16)), {})\ncnt: 2, ((T([32, 120, 28, 28], f16), T([32, 120, 28, 28], f16)), {})\ncnt: 2, ((T([32, 40, 28, 28], f16), T([32, 40, 28, 28], f16)), {})\ncnt: 1, ((T([32, 72, 28, 28], f16), T([32, 72, 28, 28], f16)), {})\ncnt: 1, ((T([32, 24, 56, 56], f16), T([32, 24, 56, 56], f16)), {})\ncnt: 1, ((T([32, 16, 112, 112], f16), T([32, 16, 112, 112], f16)), {})\n\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operation Statistics\nDESCRIPTION: Logs of batch normalization operations (aten.native_batch_norm.default) showing input tensor shapes, normalization parameters and running statistics\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.mm.default\nDESCRIPTION: Logs matrix multiplication operations (`aten.mm.default`). The examples involve multiplying float16 tensors of shapes [128, 1000] x [1000, 368] and [1000, 128] x [128, 368]. Note the different strides specified in the second example.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 368], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 368], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Convolution Backward Operations in PyTorch\nDESCRIPTION: Records of aten.convolution_backward.default operator calls for gradient computation. These operations compute gradients for convolution operations with various tensor shapes and convolution parameters, with flags indicating which gradients to compute.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([32, 2048, 7, 7], f16), T([32, 1024, 7, 7], f16), T([2048, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 2048, 7, 7], f16), T([32, 512, 7, 7], f16), T([2048, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 1024, 1, 1], f16), T([32, 256, 1, 1], f16), T([1024, 256, 1, 1], f16), [1024], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 256, 1, 1], f16), T([32, 512, 1, 1], f16), T([256, 512, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 1024, 14, 14], f16), T([32, 512, 14, 14], f16), T([1024, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), T([32, 1024, 14, 14], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 1024, 14, 14], f16), T([32, 512, 14, 14], f16), T([1024, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 1024, 14, 14], f16), T([32, 256, 14, 14], f16), T([1024, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 512, 1, 1], f16), T([32, 128, 1, 1], f16), T([512, 128, 1, 1], f16), [512], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 128, 1, 1], f16), T([32, 256, 1, 1], f16), T([128, 256, 1, 1], f16), [128], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([32, 256, 28, 28], f16), T([512, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), T([32, 512, 28, 28], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([32, 256, 28, 28], f16), T([512, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([32, 128, 28, 28], f16), T([512, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 256, 1, 1], f16), T([32, 64, 1, 1], f16), T([256, 64, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 64, 1, 1], f16), T([32, 128, 1, 1], f16), T([64, 128, 1, 1], f16), [64], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 256, 56, 56], f16), T([32, 128, 56, 56], f16), T([256, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})\ncnt: 1, ((T([32, 128, 56, 56], f16), T([32, 256, 56, 56], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([32, 256, 56, 56], f16), T([32, 64, 56, 56], f16), T([256, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 128, 1, 1], f16), T([32, 32, 1, 1], f16), T([128, 32, 1, 1], f16), [128], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 32, 1, 1], f16), T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), [32], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 128, 56, 56], f16), T([32, 64, 56, 56], f16), T([128, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, False]), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16), T([64, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([32, 32, 112, 112], f16), T([64, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.relu_.default Calls - PyTorch - Python\nDESCRIPTION: Displays argument layouts for in-place ReLU (aten.relu_.default) operator calls with a variety of float16 tensors. Relies on CUDA or CPU PyTorch kernels with ability to mutate tensor values in-place. Inputs are float16 tensors, outputs are the modified tensor after ReLU, with repeated batch and spatial dimensions observed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 32, 112, 112], f16),), {})\ncnt: 7, ((T([128, 64, 56, 56], f16),), {})\ncnt: 4, ((T([128, 32, 56, 56], f16),), {})\ncnt: 1, ((T([128, 128, 56, 56], f16),), {})\ncnt: 7, ((T([128, 144, 28, 28], f16),), {})\ncnt: 4, ((T([128, 72, 28, 28], f16),), {})\ncnt: 1, ((T([128, 288, 28, 28], f16),), {})\ncnt: 7, ((T([128, 304, 14, 14], f16),), {})\ncnt: 4, ((T([128, 152, 14, 14], f16),), {})\ncnt: 1, ((T([128, 480, 14, 14], f16),), {})\ncnt: 1, ((T([128, 960, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1280, 4, 4], f16),), {})\ncnt: 1, ((T([128, 1024, 4, 4], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Analysis\nDESCRIPTION: Detailed breakdown of PyTorch operator usage showing tensor shapes, data types, and operation counts. Includes operations like batch normalization, ReLU activation, tensor multiplication, and division across different tensor dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 224, 112, 112], f16), T([32, 32, 112, 112], f16), T([224, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: TorchDynamo Error Example in Python\nDESCRIPTION: Demonstrates code that generates a TorchDynamo assertion error by attempting to use a tensor as a dictionary key, which is not supported by the compiler.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nimport torch._dynamo as dynamo\n\n\ndef test_assertion_error():\n    y = torch.ones(200, 200)\n    z = {y: 5}\n    return z\n\ncompiled_test_assertion_error = torch.compile(test_assertion_error, backend=\"eager\")\n\ncompiled_test_assertion_error()\n```\n\n----------------------------------------\n\nTITLE: Sum Operations with Symbolic Indices in PyTorch\nDESCRIPTION: Records of sum operations performed along specific symbolic dimensions of tensors. This operation computes the sum along dimension 0 of a tensor with shape [128, 1000], keeping the dimension in the result (keepdim=True).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Addition Operations in Neural Network\nDESCRIPTION: Shows various tensor addition operations (aten.add.Tensor) in a neural network. The operations include adding bias terms to feature maps, element-wise addition of feature maps, and adding small epsilon values for normalization operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 3, ((T([32, 56, 56, 512], f16), T([512], f16)), {})\ncnt: 3, ((T([32, 56, 56, 128], f16), T([128], f16)), {})\ncnt: 7, ((T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128))), {})\ncnt: 1, ((T([32, 1, 56, 56], f16), 1e-06), {})\ncnt: 1, ((T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), T([128, 1, 1], f16)), {})\ncnt: 3, ((T([32, 28, 28, 1024], f16), T([1024], f16)), {})\ncnt: 3, ((T([32, 28, 28, 256], f16), T([256], f16)), {})\ncnt: 7, ((T([32, 256, 28, 28], f16, stride=(200704, 1, 7168, 256)), T([32, 256, 28, 28], f16, stride=(200704, 1, 7168, 256))), {})\n```\n\n----------------------------------------\n\nTITLE: Aggregating Tensor Mean Computation Calls (aten.mean) - PyTorch - Python\nDESCRIPTION: This code records parameterizations of mean reduction operations (aten.mean) in PyTorch, documenting dimensions reduced, tensor shapes, datatypes, and keepdim flags. Patterns suggest reduction over spatial dimensions in higher-rank tensors. Inputs are tensors and dimensions to reduce; output is pattern occurrence only.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 256, 56, 56], f16), [2, 3], True), {})\ncnt: 2, ((T([128, 512, 28, 28], f16), [2, 3], True), {})\ncnt: 6, ((T([128, 1536, 14, 14], f16), [2, 3], True), {})\ncnt: 3, ((T([128, 1536, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 2304, 7, 7], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Logging Options Programmatically\nDESCRIPTION: Example showing how to enable specific logging options in torch.compile using torch._logging.set_logs\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\ntorch._logging.set_logs(graph_breaks=True)\n...\n```\n\n----------------------------------------\n\nTITLE: Mean Dimension Operations in PyTorch\nDESCRIPTION: Collection of mean operations across specific dimensions with various tensor shapes, primarily using half-precision (f16) tensors. Operations focus on reducing spatial dimensions (2,3) or (-1,-2) with keepdim=True.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\naten.mean.dim\ncnt: 5, ((T([128, 120, 28, 28], f16), [2, 3], True), {})\ncnt: 6, ((T([128, 360, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 720, 7, 7], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Conduct aten.sigmoid Activation in Python\nDESCRIPTION: Applies the sigmoid function using PyTorch to convert inputs into activation values between 0 and 1. This operation is pivotal in binary classification models. Dependencies include PyTorchs neural network modules. The input tensor is transformed non-linearly to interpret signals in neural networks effectively.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Applying ATen Sum Operator in PyTorch: Python\nDESCRIPTION: The snippet demonstrates the use of ATen's 'sum' operator for reducing tensors of varying configurations in PyTorch. It highlights tensor dimensions, data types (f16), and certain reduction operations without additional parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([3, 256, 128, 128], f16, stride=(4194304, 1, 32768, 256)),), {})\ncnt: 3, ((T([3, 64, 128, 128], f16),), {})\nOperator: aten.sum.default\ncnt: 1, ((T([3, 1, 512, 512], f16),), {})\ncnt: 1, ((T([3, 3, 512, 512], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication for Linear Projections\nDESCRIPTION: Shows the matrix multiplication operations used throughout the network for linear projections. These operations handle various tensor shapes including attention projections, feed-forward networks, and the final classification layer.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 72, ((T([663552, 16], f16), T([16, 16], f16, stride=(1, 16))), {})\ncnt: 1, ((T([2, 1000], f16), T([1000, 768], f16)), {})\ncnt: 1, ((T([1000, 2], f16, stride=(1, 1000)), T([2, 768], f16, stride=(443136, 1))), {})\ncnt: 2, ((T([2, 768], f16), T([768, 3072], f16)), {})\ncnt: 2, ((T([768, 2], f16, stride=(1, 768)), T([2, 3072], f16)), {})\ncnt: 2, ((T([2, 3072], f16), T([3072, 768], f16)), {})\ncnt: 2, ((T([3072, 2], f16, stride=(1, 3072)), T([2, 768], f16)), {})\ncnt: 4, ((T([2, 768], f16), T([768, 768], f16)), {})\ncnt: 2, ((T([768, 2], f16, stride=(1, 768)), T([2, 768], f16)), {})\ncnt: 4, ((T([1154, 768], f16), T([768, 768], f16)), {})\ncnt: 4, ((T([768, 1154], f16, stride=(1, 768)), T([1154, 768], f16)), {})\ncnt: 2, ((T([768, 2], f16, stride=(1, 768)), T([2, 768], f16, stride=(443136, 1))), {})\ncnt: 36, ((T([1152, 768], f16), T([768, 3072], f16)), {})\ncnt: 36, ((T([768, 1152], f16, stride=(1, 768)), T([1152, 3072], f16)), {})\ncnt: 36, ((T([1152, 3072], f16), T([3072, 768], f16)), {})\ncnt: 36, ((T([3072, 1152], f16, stride=(1, 3072)), T([1152, 768], f16)), {})\ncnt: 36, ((T([1152, 768], f16), T([768, 768], f16)), {})\ncnt: 36, ((T([768, 1152], f16, stride=(1, 768)), T([1152, 768], f16)), {})\ncnt: 72, ((T([16, 663552], f16, stride=(1, 16)), T([663552, 16], f16)), {})\ncnt: 72, ((T([663552, 16], f16), T([16, 16], f16)), {})\ncnt: 36, ((T([1152, 2304], f16), T([2304, 768], f16)), {})\ncnt: 36, ((T([2304, 1152], f16, stride=(1, 2304)), T([1152, 768], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Add Operator Benchmark\nDESCRIPTION: Executes the benchmark for the torch.add operator with single-threaded configuration. The flags control OpenMP and MKL thread counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd pytorch/benchmarks/operator_benchmark\npython -m pt.add_test --omp-num-threads 1 --mkl-num-threads 1\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations - Inception Network\nDESCRIPTION: Documents the tensor operations, shapes and parameters used in an Inception neural network implementation. Includes convolutions, pooling, tensor additions and concatenations with their corresponding input/output tensor shapes and configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example operator calls with tensor shapes and parameters:\n\n# Log softmax operation\naten._log_softmax.default(T([128, 1000], f16), 1, False)\n\n# Convolution operations\naten.convolution.default(T([128, 3, 299, 299], f16), T([32, 3, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1)\n\n# Average pooling\naten.avg_pool2d.default(T([128, 192, 35, 35], f16), [3, 3], [1, 1], [1, 1])\n\n# Tensor addition\naten.add.Tensor(T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16))\n\n# Tensor concatenation \naten.cat.default([T([128, 64, 35, 35], f16), T([128, 64, 35, 35], f16), T([128, 96, 35, 35], f16), T([128, 32, 35, 35], f16)], 1)\n```\n\n----------------------------------------\n\nTITLE: PyTorch SiLU Activation Operations\nDESCRIPTION: SiLU (Swish) activation forward and backward passes on tensors with various spatial dimensions and channel sizes\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 24, 128, 128], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Defining an Output Node in FX Graph (Python)\nDESCRIPTION: This snippet outlines how to represent an output (return) node in an FX graph using FX's pseudo-Python format. The output node designates which node result is returned by the graph function, and is always placed last. This construct semantically mirrors a Python function's return statement, encapsulating the FX graph's termination point.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noutput[](args = (%something, ))\n```\n\n----------------------------------------\n\nTITLE: Basic FX Graph Node Check Example\nDESCRIPTION: Demonstrates checking if nodes in a graph module are div operations\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor node in graph_module.graph.nodes:\n    if node.op == \"call_function\" and node.target != torch.div:\n        raise ValueError(\"Target should be div!\")\n\npm.add_checks(check_div_target)\n\npm(graph_module)    # raises ValueError after replace_div_with_mul pass\n```\n\n----------------------------------------\n\nTITLE: Compiled Module Path Output\nDESCRIPTION: Example output showing the path of compiled TorchInductor module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_inductor_profiling.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n**Compiled module path: /tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py**\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.nll_loss_forward.default Operator (Text)\nDESCRIPTION: Logs a call to the `aten.nll_loss_forward.default` operator, used for computing the negative log likelihood loss. The log shows the arguments including the input tensor shape ([128, 1000] f16), target tensor shape ([128] i64), reduction type (1, likely 'mean'), and ignore index (-100).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Merging Multi-GPU Training Data\nDESCRIPTION: Python command to merge training data collected from multiple GPUs into a single file\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mm/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython torchgen/_autuoheuristic/merge_data.py mm_train.txt data_6.txt data_7.txt\n```\n\n----------------------------------------\n\nTITLE: Importing dependencies for BaseListVariable in PyTorch Dynamo\nDESCRIPTION: This snippet imports necessary modules and classes for the BaseListVariable implementation, including collections, types, typing utilities, and core PyTorch packages.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/set_linter_testdata/includes_doesnt_change.py.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# mypy: ignore-errors\n\nimport collections\nimport types\nfrom typing import Any, Dict, List, Optional, TYPE_CHECKING\n\nimport torch\nimport torch.fx\nfrom torch._guards import Source\n\nfrom ..utils import (\n    namedtuple_fields,\n    odict_values,\n    OrderedSet,\n    set_example_value,\n)\nfrom .base import MutableLocal, VariableTracker, VariableTrackerContainer\n\nif TYPE_CHECKING:\n    from torch._dynamo.codegen import PyCodegen\n```\n\n----------------------------------------\n\nTITLE: PyTorch Sum Operations\nDESCRIPTION: Log entries for sum reduction operations, including symbolic and default implementations. Shows operations on classification output tensors with 1000 classes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet50_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})\n\nOperator: aten.sum.default\ncnt: 1, ((T([32, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Example Output from PyTorch DDP Benchmark Script\nDESCRIPTION: This shows sample output generated by the Distributed Data Parallel benchmark script (run on rank 0). It includes system details like PyTorch/CUDA versions, distributed backend, GPU topology (`nvidia-smi topo -m`), and performance results (seconds/iteration, examples/second) for various models (ResNet, ResNeXt) across different numbers of GPUs and configurations. Requires PyTorch and torchvision to run.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/distributed/ddp/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-----------------------------------\nPyTorch distributed benchmark suite\n-----------------------------------\n\n* PyTorch version: 1.4.0a0+05140f0\n* CUDA version: 10.0\n* Distributed backend: nccl\n\n--- nvidia-smi topo -m ---\n\n        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    mlx5_2  mlx5_0  mlx5_3  mlx5_1  CPU Affinity\nGPU0     X      NV1     NV1     NV2     NV2     SYS     SYS     SYS     SYS     PIX     SYS     PHB     0-19,40-59\nGPU1    NV1      X      NV2     NV1     SYS     NV2     SYS     SYS     SYS     PIX     SYS     PHB     0-19,40-59\nGPU2    NV1     NV2      X      NV2     SYS     SYS     NV1     SYS     SYS     PHB     SYS     PIX     0-19,40-59\nGPU3    NV2     NV1     NV2      X      SYS     SYS     SYS     NV1     SYS     PHB     SYS     PIX     0-19,40-59\nGPU4    NV2     SYS     SYS     SYS      X      NV1     NV1     NV2     PIX     SYS     PHB     SYS     0-19,40-59\nGPU5    SYS     NV2     SYS     SYS     NV1      X      NV2     NV1     PIX     SYS     PHB     SYS     0-19,40-59\nGPU6    SYS     SYS     NV1     SYS     NV1     NV2      X      NV2     PHB     SYS     PIX     SYS     0-19,40-59\nGPU7    SYS     SYS     SYS     NV1     NV2     NV1     NV2      X      PHB     SYS     PIX     SYS     0-19,40-59\nmlx5_2  SYS     SYS     SYS     SYS     PIX     PIX     PHB     PHB      X      SYS     PHB     SYS\nmlx5_0  PIX     PIX     PHB     PHB     SYS     SYS     SYS     SYS     SYS      X      SYS     PHB\nmlx5_3  SYS     SYS     SYS     SYS     PHB     PHB     PIX     PIX     PHB     SYS      X      SYS\nmlx5_1  PHB     PHB     PIX     PIX     SYS     SYS     SYS     SYS     SYS     PHB     SYS      X\n\nLegend:\n\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing a single PCIe switch\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n--------------------------\n\n\nBenchmark: resnet50 with batch size 32\n\n                            sec/iter    ex/sec      sec/iter    ex/sec      sec/iter    ex/sec      sec/iter    ex/sec\n   1 GPUs --   no ddp:  p50:  0.097s     329/s  p75:  0.097s     329/s  p90:  0.097s     329/s  p95:  0.097s     329/s\n   1 GPUs --    1M/1G:  p50:  0.100s     319/s  p75:  0.100s     318/s  p90:  0.100s     318/s  p95:  0.100s     318/s\n   2 GPUs --    1M/2G:  p50:  0.103s     310/s  p75:  0.103s     310/s  p90:  0.103s     310/s  p95:  0.103s     309/s\n   4 GPUs --    1M/4G:  p50:  0.103s     310/s  p75:  0.103s     310/s  p90:  0.103s     310/s  p95:  0.103s     310/s\n   8 GPUs --    1M/8G:  p50:  0.104s     307/s  p75:  0.104s     307/s  p90:  0.104s     306/s  p95:  0.104s     306/s\n  16 GPUs --    2M/8G:  p50:  0.104s     306/s  p75:  0.104s     306/s  p90:  0.104s     306/s  p95:  0.104s     306/s\n\nBenchmark: resnet101 with batch size 32\n\n                            sec/iter    ex/sec      sec/iter    ex/sec      sec/iter    ex/sec      sec/iter    ex/sec\n   1 GPUs --   no ddp:  p50:  0.162s     197/s  p75:  0.162s     197/s  p90:  0.162s     197/s  p95:  0.162s     197/s\n   1 GPUs --    1M/1G:  p50:  0.171s     187/s  p75:  0.171s     186/s  p90:  0.171s     186/s  p95:  0.172s     185/s\n   2 GPUs --    1M/2G:  p50:  0.176s     182/s  p75:  0.176s     181/s  p90:  0.176s     181/s  p95:  0.176s     181/s\n   4 GPUs --    1M/4G:  p50:  0.176s     182/s  p75:  0.176s     181/s  p90:  0.176s     181/s  p95:  0.176s     181/s\n   8 GPUs --    1M/8G:  p50:  0.179s     179/s  p75:  0.179s     178/s  p90:  0.180s     178/s  p95:  0.180s     177/s\n  16 GPUs --    2M/8G:  p50:  0.179s     178/s  p75:  0.180s     177/s  p90:  0.183s     174/s  p95:  0.188s     170/s\n\nBenchmark: resnext50_32x4d with batch size 32\n\n                            sec/iter    ex/sec      sec/iter    ex/sec      sec/iter    ex/sec      sec/iter    ex/sec\n   1 GPUs --   no ddp:  p50:  0.145s     220/s  p75:  0.145s     220/s  p90:  0.145s     220/s  p95:  0.145s     220/s\n   1 GPUs --    1M/1G:  p50:  0.147s     217/s  p75:  0.147s     217/s  p90:  0.148s     216/s  p95:  0.148s     216/s\n   2 GPUs --    1M/2G:  p50:  0.153s     209/s  p75:  0.153s     209/s  p90:  0.153s     209/s  p95:  0.153s     209/s\n   4 GPUs --    1M/4G:  p50:  0.153s     208/s  p75:  0.153s     208/s  p90:  0.154s     208/s  p95:  0.154s     208/s\n   8 GPUs --    1M/8G:  p50:  0.157s     204/s  p75:  0.157s     204/s  p90:  0.157s     203/s  p95:  0.157s     203/s\n  16 GPUs --    2M/8G:  p50:  0.157s     203/s  p75:  0.157s     203/s  p90:  0.158s     203/s  p95:  0.158s     202/s\n\nBenchmark: resnext101_32x8d with batch size 32\n\n                            sec/iter    ex/sec      sec/iter    ex/sec      sec/iter    ex/sec      sec/iter    ex/sec\n   1 GPUs --   no ddp:  p50:  0.415s      77/s  p75:  0.415s      77/s  p90:  0.416s      76/s  p95:  0.417s      76/s\n   1 GPUs --    1M/1G:  p50:  0.425s      75/s  p75:  0.426s      75/s  p90:  0.426s      75/s  p95:  0.426s      75/s\n   2 GPUs --    1M/2G:  p50:  0.438s      73/s  p75:  0.439s      72/s  p90:  0.439s      72/s  p95:  0.439s      72/s\n   4 GPUs --    1M/4G:  p50:  0.439s      72/s  p75:  0.439s      72/s  p90:  0.440s      72/s  p95:  0.440s      72/s\n   8 GPUs --    1M/8G:  p50:  0.447s      71/s  p75:  0.447s      71/s  p90:  0.448s      71/s  p95:  0.448s      71/s\n  16 GPUs --    2M/8G:  p50:  0.450s      71/s  p75:  0.451s      70/s  p90:  0.451s      70/s  p95:  0.451s      70/s\n```\n\n----------------------------------------\n\nTITLE: Adding Y-Flipping Support to PNG Writing Function in C\nDESCRIPTION: Declares a new function tdefl_write_image_to_png_file_in_memory_ex() that supports Y-flipping for OpenGL use and allows explicit control over compression level for real-time compression.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/third_party/miniz-3.0.2/ChangeLog.md#2025-04-22_snippet_2\n\nLANGUAGE: C\nCODE:\n```\ntdefl_write_image_to_png_file_in_memory_ex()\n```\n\n----------------------------------------\n\nTITLE: Configuring C++ Standard and Build Verbosity in CMake\nDESCRIPTION: Includes standard installation directories, sets the required C++ standard to C++17, enables verbose output during the make process, and displays the status of the `ANDROID_STL` variable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(GNUInstallDirs)\n\nset(CMAKE_CXX_STANDARD 17 CACHE STRING \"The C++ standard whose features are requested to build this target.\")\nset(CMAKE_VERBOSE_MAKEFILE ON)\nmessage(STATUS \"ANDROID_STL:${ANDROID_STL}\")\n```\n\n----------------------------------------\n\nTITLE: Disabling cuDNN Convolution Benchmarking in Python\nDESCRIPTION: Disables the cuDNN benchmarking feature by setting `torch.backends.cudnn.benchmark = False`. This forces cuDNN to deterministically select a convolution algorithm instead of benchmarking multiple options, thus avoiding variability between runs caused by benchmarking noise or hardware differences. This may potentially reduce performance compared to allowing benchmarking.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cudnn.benchmark = False\n```\n\n----------------------------------------\n\nTITLE: Enabling cuDNN Convolution Benchmarking in Python\nDESCRIPTION: Enables the cuDNN benchmarking feature by setting `torch.backends.cudnn.benchmark = True`. This allows cuDNN to test multiple convolution algorithms and select the fastest one for the given input sizes, potentially improving performance for a single run but introducing nondeterminism across different runs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/randomness.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cudnn.benchmark = True\n```\n\n----------------------------------------\n\nTITLE: Tensor Implementation Functions in C++\nDESCRIPTION: Function declarations for TensorImpl class methods, including constructors, destructors, and utility functions for managing tensor properties and storage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c1010TensorImplC2EONS_7StorageENS_14DispatchKeySetEN6caffe28TypeMetaE\n_ZN3c1010TensorImplD0Ev\n_ZN3c1010TensorImplD1Ev\n_ZNK3c1010TensorImpl11has_storageEv\n_ZNK3c1010TensorImpl13requires_gradEv\n_ZNK3c1010TensorImpl18compute_contiguousENS0_8identityIbEE\n_ZNK3c1010TensorImpl33compute_non_overlapping_and_denseENS0_8identityIbEE\n_ZNK3c1010TensorImpl35compute_channels_last_contiguous_2dENS0_8identityIbEE\n_ZNK3c1010TensorImpl37compute_strides_like_channels_last_2dENS0_8identityIbEE\n_ZNK3c1010TensorImpl7storageEv\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.nll_loss_forward.default Operator (Log)\nDESCRIPTION: Log entry detailing a call to the PyTorch `aten.nll_loss_forward.default` operator. It shows the invocation count (`cnt`) and arguments including the input tensor, target tensor, weight tensor (None), reduction type, and ignore index.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Disabling NVFuser in PyTorch\nDESCRIPTION: These snippets show three methods to disable NVFuser in PyTorch, including environment variables and a Python API call.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\nexport PYTORCH_JIT_USE_NNC_NOT_NVFUSER=1\n```\n\nLANGUAGE: Python\nCODE:\n```\ntorch._C._jit_set_nvfuser_enabled(False)\n```\n\nLANGUAGE: Bash\nCODE:\n```\nexport PYTORCH_JIT_ENABLE_NVFUSER=0\n```\n\n----------------------------------------\n\nTITLE: Profiling Tensor Copy Operations in PyTorch\nDESCRIPTION: Log of operations that create fresh copies of tensors, detached from computational graphs. This single operation creates a fresh copy of a 64-element integer tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([64], i64),), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform and Compiler Settings\nDESCRIPTION: Configures target processor settings and compiler-specific flags. Includes system checks for supported platforms and architectures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nSET(PYTORCH_QNNPACK_TARGET_PROCESSOR \"${CMAKE_SYSTEM_PROCESSOR}\")\nIF(CMAKE_SYSTEM_NAME STREQUAL \"Darwin\" AND CMAKE_OSX_ARCHITECTURES MATCHES \"^(x86_64|arm64)$\")\n  SET(PYTORCH_QNNPACK_TARGET_PROCESSOR \"${CMAKE_OSX_ARCHITECTURES}\")\nENDIF()\n\nif(CMAKE_CXX_COMPILER_ID MATCHES \"Clang\" OR CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n  set_source_files_properties(src/operator-run.c PROPERTIES COMPILE_FLAGS -Wno-deprecated-declarations)\n  set_source_files_properties(src/conv-run.cc PROPERTIES COMPILE_FLAGS -Wno-deprecated-declarations)\n  set_source_files_properties(src/deconv-run.cc PROPERTIES COMPILE_FLAGS -Wno-deprecated-declarations)\n  set_source_files_properties(src/fc-run.cc PROPERTIES COMPILE_FLAGS -Wno-deprecated-declarations)\n  set_source_files_properties(src/fc-dynamic-run.cc PROPERTIES COMPILE_FLAGS -Wno-deprecated-declarations)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Describing aten._log_softmax_backward_data.default Usage - PyTorch - Python\nDESCRIPTION: Documents the backward operation for _log_softmax, taking gradient output and input tensors, dimension argument and data type. It's crucial for propagating gradients through log_softmax layers. Deprecated usage or unsupported types should be verified for compatibility with custom backends.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Ensembling Predictions with Shared Minibatch via Loop - PyTorch - Python\nDESCRIPTION: Feeds the same data minibatch to all models, collecting their predictions in a list. This is a control scenario for ensemble evaluation and can be compared to vectorized predictions later. Assumes models and data are properly initialized.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/ensembling.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nminibatch = data[0]\npredictions2 = [model(minibatch) for model in models]\n```\n\n----------------------------------------\n\nTITLE: Configuring psimd Dependency for PyTorch QNNPACK\nDESCRIPTION: Sets up the psimd library as a dependency for pytorch_qnnpack. It either builds psimd from source or uses a system-provided version depending on the USE_SYSTEM_PSIMD flag. When using the system version, it finds the header file and creates a static library target.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# ---[ Configure psimd\nif(NOT TARGET psimd AND NOT USE_SYSTEM_PSIMD)\n  add_subdirectory(\n    \"${PSIMD_SOURCE_DIR}\"\n    \"${CONFU_DEPENDENCIES_BINARY_DIR}/psimd\")\nelseif(NOT TARGET psimd AND USE_SYSTEM_PSIMD)\n  find_file(PSIMD_HDR psimd.h PATH_SUFFIXES include)\n  if(NOT PSIMD_HDR)\n    message(FATAL_ERROR \"Cannot find psimd\")\n  endif()\n  add_library(psimd STATIC \"${PSIMD_HDR}\")\n  set_property(TARGET psimd PROPERTY LINKER_LANGUAGE C)\nendif()\ntarget_link_libraries(pytorch_qnnpack PRIVATE psimd)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for PyTorch Android Test App\nDESCRIPTION: This CMake script sets up the build environment for a PyTorch Android test application. It defines the project name, sets C++ standard, configures source files, and sets up include and link directories for PyTorch libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/test_app/app/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.4.1)\nset(PROJECT_NAME pytorch_testapp_jni)\nproject(${PROJECT_NAME} CXX)\nset(CMAKE_CXX_STANDARD 17 CACHE STRING \"The C++ standard whose features are requested to build this target.\")\nset(CMAKE_VERBOSE_MAKEFILE ON)\n\nset(build_DIR ${CMAKE_SOURCE_DIR}/build)\n\nset(pytorch_testapp_cpp_DIR ${CMAKE_CURRENT_LIST_DIR}/src/main/cpp)\nmessage(STATUS \"ANDROID_STL:${ANDROID_STL}\")\nfile(GLOB pytorch_testapp_SOURCES\n  ${pytorch_testapp_cpp_DIR}/pytorch_testapp_jni.cpp\n)\n\nadd_library(${PROJECT_NAME} SHARED\n    ${pytorch_testapp_SOURCES}\n)\n\nfile(GLOB PYTORCH_INCLUDE_DIRS \"${build_DIR}/pytorch_android*.aar/headers\")\nfile(GLOB PYTORCH_LINK_DIRS \"${build_DIR}/pytorch_android*.aar/jni/${ANDROID_ABI}\")\n\ntarget_compile_options(${PROJECT_NAME} PRIVATE\n  -fexceptions\n)\n\nset(BUILD_SUBDIR ${ANDROID_ABI})\n\ntarget_include_directories(${PROJECT_NAME} PRIVATE\n  ${PYTORCH_INCLUDE_DIRS}\n)\n\nfind_library(PYTORCH_LIBRARY pytorch_jni\n  PATHS ${PYTORCH_LINK_DIRS}\n  NO_CMAKE_FIND_ROOT_PATH)\n\ntarget_link_libraries(${PROJECT_NAME}\n  ${PYTORCH_LIBRARY}\n  log)\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Patterns\nDESCRIPTION: This code snippet provides a detailed breakdown of PyTorch operator usage, including counts and parameter structures for various operations. It covers tensor shapes, data types, and specific configurations for operations like layer normalization, loss functions, and activations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilevit_s_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 512, 8, 8], f16), T([64, 512, 8, 8], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([64, 512, 16, 16], f16), T([64, 512, 16, 16], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([64, 128, 16, 16], f16), T([64, 128, 16, 16], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\n# ... (truncated for brevity)\n\nOperator: aten.native_layer_norm.default\ncnt: 5, ((T([256, 256, 144], f16), [144], T([144], f16), T([144], f16), 1e-05), {})\ncnt: 9, ((T([256, 64, 192], f16), [192], T([192], f16), T([192], f16), 1e-05), {})\ncnt: 7, ((T([256, 16, 240], f16), [240], T([240], f16), T([240], f16), 1e-05), {})\n\nOperator: aten.native_layer_norm_backward.default\ncnt: 7, ((T([256, 16, 240], f16), T([256, 16, 240], f16), [240], T([256, 16, 1], f32), T([256, 16, 1], f32), T([240], f16), T([240], f16), [True, True, True]), {})\ncnt: 9, ((T([256, 64, 192], f16), T([256, 64, 192], f16), [192], T([256, 64, 1], f32), T([256, 64, 1], f32), T([192], f16), T([192], f16), [True, True, True]), {})\ncnt: 5, ((T([256, 256, 144], f16), T([256, 256, 144], f16), [144], T([256, 256, 1], f32), T([256, 256, 1], f32), T([144], f16), T([144], f16), [True, True, True]), {})\n\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\n\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\n\nOperator: aten.silu.default\ncnt: 2, ((T([256, 256, 288], f16),), {})\ncnt: 4, ((T([256, 64, 384], f16),), {})\ncnt: 3, ((T([256, 16, 480], f16),), {})\n\nOperator: aten.silu_.default\ncnt: 1, ((T([64, 16, 128, 128], f16),), {})\ncnt: 2, ((T([64, 64, 128, 128], f16),), {})\ncnt: 1, ((T([64, 128, 128, 128], f16),), {})\n# ... (truncated for brevity)\n\nOperator: aten.silu_backward.default\ncnt: 1, ((T([64, 640, 8, 8], f16), T([64, 640, 8, 8], f16)), {})\ncnt: 2, ((T([64, 160, 8, 8], f16), T([64, 160, 8, 8], f16)), {})\ncnt: 1, ((T([64, 160, 8, 8], f16, stride=(20480, 64, 8, 1)), T([64, 160, 8, 8], f16)), {})\n# ... (truncated for brevity)\n\nOperator: aten.stack.default\ncnt: 3, (([T([256, 4, 16, 60], f16), T([256, 4, 16, 60], f16, stride=(3840, 960, 1, 16)), T([256, 4, 16, 60], f16)],), {})\ncnt: 4, (([T([256, 4, 64, 48], f16), T([256, 4, 64, 48], f16, stride=(12288, 3072, 1, 64)), T([256, 4, 64, 48], f16)],), {})\ncnt: 2, (([T([256, 4, 256, 36], f16), T([256, 4, 256, 36], f16, stride=(36864, 9216, 1, 256)), T([256, 4, 256, 36], f16)],), {})\n\nOperator: aten.sum.SymInt\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\ncnt: 6, ((T([4096, 240], f16), [0], True), {})\ncnt: 3, ((T([4096, 480], f16), [0], True), {})\n# ... (truncated for brevity)\n\nOperator: aten.unbind.int\ncnt: 2, ((T([3, 256, 4, 256, 36], f16, stride=(144, 110592, 36, 432, 1)),), {})\ncnt: 4, ((T([3, 256, 4, 64, 48], f16, stride=(192, 36864, 48, 576, 1)),), {})\ncnt: 3, ((T([3, 256, 4, 16, 60], f16, stride=(240, 11520, 60, 720, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Tuning NVFuser Features\nDESCRIPTION: These environment variable settings demonstrate how to disable or enable specific NVFuser features for performance tuning and experimental use.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\nexport PYTORCH_NVFUSER_DISABLE=\"fallback,fma\"\n```\n\nLANGUAGE: Bash\nCODE:\n```\nexport PYTORCH_NVFUSER_ENABLE=\"complex,linear_decomposition,conv_decomposition\"\n```\n\n----------------------------------------\n\nTITLE: Comparing PyTorch DDP Benchmark Results using diff.py\nDESCRIPTION: Demonstrates how to use the `diff.py` script to compare two JSON reports generated by the DDP benchmark script (using the `--json` flag). The command takes two file paths (baseline and test) as input. The output shows a side-by-side comparison of configuration parameters (like `bucket_size`, versions) and performance metrics (throughput, percentage difference) for each benchmark scenario, facilitating A/B testing analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/distributed/ddp/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 diff.py PATH_TO_BASELINE_FILE PATH_TO_TEST_FILE\n```\n\nLANGUAGE: text\nCODE:\n```\n                                 baseline                      test\n                     --------------------      --------------------\nbucket_size:                           25  vs                     1\ncuda_version:                        10.0  vs                  10.0\ndistributed_backend:                 nccl  vs                  nccl\npytorch_version:          1.4.0a0+05140f0  vs       1.4.0a0+05140f0\n\nBenchmark: resnet50 with batch size 32\n\n                  sec/iter    ex/sec      diff        sec/iter    ex/sec      diff\n   1 GPUs:  p75:    0.101s     317/s     -0.3%  p95:    0.101s     317/s     -0.4%\n   2 GPUs:  p75:    0.104s     306/s     -1.0%  p95:    0.104s     306/s     -1.0%\n   4 GPUs:  p75:    0.105s     305/s     -1.6%  p95:    0.105s     304/s     -1.8%\n   8 GPUs:  p75:    0.107s     299/s     -2.6%  p95:    0.107s     298/s     -2.7%\n  16 GPUs:  p75:    0.108s     294/s     -3.8%  p95:    0.122s     262/s    -16.4%\n\nBenchmark: resnet101 with batch size 32\n\n                  sec/iter    ex/sec      diff        sec/iter    ex/sec      diff\n   1 GPUs:  p75:    0.172s     185/s     -1.2%  p95:    0.172s     185/s     -1.3%\n   2 GPUs:  p75:    0.179s     178/s     -2.1%  p95:    0.179s     178/s     -2.0%\n   4 GPUs:  p75:    0.180s     177/s     -2.6%  p95:    0.180s     177/s     -2.6%\n   8 GPUs:  p75:    0.184s     173/s     -3.5%  p95:    0.184s     173/s     -3.5%\n  16 GPUs:  p75:    0.187s     170/s     -0.1%  p95:    0.204s     157/s     -7.9%\n\nBenchmark: resnext50_32x4d with batch size 32\n\n                  sec/iter    ex/sec      diff        sec/iter    ex/sec      diff\n   1 GPUs:  p75:    0.149s     214/s     -1.0%  p95:    0.149s     214/s     -0.9%\n   2 GPUs:  p75:    0.156s     205/s     -1.5%  p95:    0.156s     205/s     -1.6%\n   4 GPUs:  p75:    0.156s     204/s     -1.6%  p95:    0.157s     204/s     -1.8%\n   8 GPUs:  p75:    0.159s     200/s     -1.5%  p95:    0.159s     200/s     -1.5%\n  16 GPUs:  p75:    0.161s     198/s     -1.9%  p95:    0.162s     197/s     -2.3%\n\nBenchmark: resnext101_32x8d with batch size 32\n\n                  sec/iter    ex/sec      diff        sec/iter    ex/sec      diff\n   1 GPUs:  p75:    0.427s      74/s     -0.8%  p95:    0.428s      74/s     -0.7%\n   2 GPUs:  p75:    0.444s      72/s     -1.3%  p95:    0.445s      71/s     -0.7%\n   4 GPUs:  p75:    0.444s      72/s     -1.1%  p95:    0.445s      71/s     -0.8%\n   8 GPUs:  p75:    0.452s      70/s     -1.3%  p95:    0.452s      70/s     -1.3%\n  16 GPUs:  p75:    0.455s      70/s     -0.7%  p95:    0.456s      70/s     -0.6%\n```\n\n----------------------------------------\n\nTITLE: Utilizing PyTorch aten.add.Tensor Operator in Python\nDESCRIPTION: This code snippet demonstrates the use of the 'aten.add.Tensor' operator in PyTorch for element-wise tensor addition. It involves several tensor shapes and datatypes (e.g., f16), highlighting the use of broadcasting for different tensor sizes. This operation is a fundamental part of many neural networks for combining inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([8, 2048, 7, 7], f16), T([8, 2048, 7, 7], f16)), {})\ncnt: 6, ((T([8, 1024, 14, 14], f16), T([8, 1024, 14, 14], f16)), {})\ncnt: 4, ((T([8, 512, 28, 28], f16), T([8, 512, 28, 28], f16)), {})\ncnt: 3, ((T([8, 256, 56, 56], f16), T([8, 256, 56, 56], f16)), {})\ncnt: 1, ((T([8, 64, 56, 56], f16), T([8, 64, 56, 56], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Importing torch.overrides Module in Python\nDESCRIPTION: This snippet shows how to import the torch.overrides module in Python. It sets the current module context for the subsequent documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.overrides.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. currentmodule:: torch.overrides\n```\n\n----------------------------------------\n\nTITLE: DataFrame Operations and Shuffling\nDESCRIPTION: Examples of performing operations on DataFrames including column creation, shuffling, and comparing with regular DataPipe shuffling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndp = get_dataframes_pipe(dataframe_size = 3)\ndp['y'] = dp.i * 100 + dp.j - 2.7\ndp = dp.shuffle()\ndp = dp - 17\ndp['y'] = dp.y * 10000\nfor i in dp.raw_iterator():\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Softmax Operations\nDESCRIPTION: Records usage of softmax and log_softmax operators with various tensor shapes and dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._log_softmax.default((T([32, 1000], f16), 1, False))\naten._log_softmax_backward_data.default((T([32, 1000], f16), T([32, 1000], f16), 1, f16))\naten._softmax.default((T([32, 1, 3136, 49], f16), -1, False))\naten._softmax.default((T([32, 2, 784, 49], f16), -1, False))\naten._softmax.default((T([32, 5, 196, 49], f16), -1, False))\n```\n\n----------------------------------------\n\nTITLE: Utilizing _softmax and _softmax_backward_data\nDESCRIPTION: Softmax and its backward function are frequently used for computing normalized exponential functions and their gradients over specified dimensions. The operations rely on PyTorch, manipulating tensors with shapes like (96, 128, 128) in floating-point formats, returning tensors of similar dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 18, ((T([96, 128, 128], f16), -1, False), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 18, ((T([96, 128, 128], f16), T([96, 128, 128], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Softmax Backward Operations in PyTorch\nDESCRIPTION: This snippet analyzes usage of the \\\"aten._log_softmax_backward_data.default\\\" operator, detailing how gradients are computed in backpropagation through a log-softmax layer.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([1024, 50265], f16), T([1024, 50265], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations\nDESCRIPTION: Matrix multiplication operations between tensors of various shapes, including strided tensors and different batch sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\naten.mm.default((T([32, 1000], f16), T([1000, 512], f16)))\n```\n\n----------------------------------------\n\nTITLE: Implementing aten.split_with_sizes in PyTorch\nDESCRIPTION: The aten.split_with_sizes.default is a PyTorch operation utilized for partitioning tensors into smaller parts along a specified axis, based on given chunk sizes in the snippet. Dependencies include PyTorch. Inputs involve tensor and designated sizes list with dimension index. The result includes smaller tensors as per partitions. Ensure inputs comply with total dimension size constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 32, 112, 112], f16), [16, 16], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Testing Backward Pass for ReLU/Threshold with aten.threshold_backward.default - Python\nDESCRIPTION: Snippets provide inputs to the backward pass for thresholded activations (e.g., ReLU), specifying input/output tensors and threshold value for 'aten.threshold_backward.default'. Used for validating gradient computation and operator support for f16 tensors on typical activation shapes. Requires forward pass context and shape/type matching.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 1280, 1, 1], f16), T([128, 1280, 1, 1], f16), 0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 960, 7, 7], f16), T([128, 960, 7, 7], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations with Convolutions\nDESCRIPTION: Tensor operations involving convolutions with different input shapes, strides and parameters. Operations use float16 (f16) data type and include various tensor transformations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nT([32, 320, 14, 14], f16), T([32, 128, 28, 28], f16), T([320, 128, 2, 2], f16)\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.sum.default with Tensor Arguments (Text)\nDESCRIPTION: This section logs calls to the `aten.sum.default` operator, likely performing a reduction sum over all dimensions of the input tensor. The examples show this operation applied to float16 (f16) tensors of various shapes (e.g., [128, 1000], [128, 1536, 6, 6]). The input is a tuple containing only the tensor description, implying default reduction behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_22\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sum.default\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([128, 1536, 6, 6], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 6, ((T([128, 1536, 12, 12], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([128, 512, 24, 24], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([128, 256, 48, 48], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Activation Functions in Neural Network\nDESCRIPTION: Shows the usage of GELU activation functions and their backward passes in the neural network. These activations are applied after various linear transformations throughout the network architecture.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.gelu.default\ncnt: 3, ((T([32, 56, 56, 512], f16),), {})\ncnt: 3, ((T([32, 28, 28, 1024], f16),), {})\ncnt: 27, ((T([32, 14, 14, 2048], f16),), {})\ncnt: 3, ((T([32, 7, 7, 4096], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 3, ((T([32, 7, 7, 4096], f16), T([32, 7, 7, 4096], f16)), {})\ncnt: 27, ((T([32, 14, 14, 2048], f16), T([32, 14, 14, 2048], f16)), {})\ncnt: 3, ((T([32, 28, 28, 1024], f16), T([32, 28, 28, 1024], f16)), {})\ncnt: 3, ((T([32, 56, 56, 512], f16), T([32, 56, 56, 512], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Sum Operation\nDESCRIPTION: Shows the symbolic integer summation operation (aten.sum.SymInt) on a specific tensor shape with dimension preservation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Sigmoid Activation Operations in PyTorch\nDESCRIPTION: This snippet shows the sigmoid activation function applied to a tensor, which squashes values to the range [0,1]. This is typically used for binary classification or as a gate mechanism in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sigmoid.default\ncnt: 1, ((T([96, 65], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations with Various Kernel Sizes\nDESCRIPTION: Series of convolution operations with varying kernel sizes (3x3, 5x5, 7x7, 9x9) and stride configurations. The tensors use float16 (f16) data type with specific memory strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 156, 14, 14], f16, stride=(122304, 196, 14, 1)), T([64, 156, 14, 14], f16, stride=(122304, 196, 14, 1)), T([156, 1, 9, 9], f16), [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 156, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: ReLU Activation Operations in PyTorch\nDESCRIPTION: Records of ReLU activation operations applied in-place to tensors of various shapes. These operations apply element-wise rectified linear unit function, replacing negative values with zeros.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 2, ((T([128, 32, 112, 112], f16),), {})\ncnt: 1, ((T([128, 48, 112, 112], f16),), {})\ncnt: 1, ((T([128, 48, 56, 56], f16),), {})\ncnt: 4, ((T([128, 72, 56, 56], f16),), {})\ncnt: 1, ((T([128, 144, 56, 56], f16),), {})\ncnt: 1, ((T([128, 144, 28, 28], f16),), {})\ncnt: 6, ((T([128, 120, 28, 28], f16),), {})\ncnt: 1, ((T([128, 240, 28, 28], f16),), {})\ncnt: 7, ((T([128, 240, 14, 14], f16),), {})\ncnt: 2, ((T([128, 480, 14, 14], f16),), {})\ncnt: 6, ((T([128, 288, 14, 14], f16),), {})\ncnt: 1, ((T([128, 576, 14, 14], f16),), {})\ncnt: 1, ((T([128, 576, 7, 7], f16),), {})\ncnt: 8, ((T([128, 1152, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1280, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch C++ Build Using CMake - CMake\nDESCRIPTION: This CMake file sets up a C++ project that depends on the PyTorch library and supports C++17 standards. It locates the required Torch and threading packages, establishes include directories, and sets up the build and linking process for an executable named 'Predictor'. Essential parameters include definitions of C++ standards, linkage options, and external dependencies such as system threads and dynamic libraries. The file is intended for users compiling PyTorch C++ projects using CMake 3.15 or above.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/custom_build/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.15)\n\nproject(custom_build_project)\n\nset(CMAKE_CXX_STANDARD 17 CACHE STRING \"The C++ standard whose features are requested to build this target.\")\n\n# Find torch library\nfind_package(Torch REQUIRED)\n\n# Main executable\nadd_executable(Predictor predictor.cpp)\ntarget_include_directories(Predictor PUBLIC ${TORCH_INCLUDE_DIRS})\n\nfind_package(Threads REQUIRED)\n\ntarget_link_libraries(Predictor\n  -Wl,-s\n  -Wl,--gc-sections\n  -Wl,--whole-archive\n  ${TORCH_LIBRARIES}\n  -Wl,--no-whole-archive\n  Threads::Threads\n  ${CMAKE_DL_LIBS}\n)\n```\n\n----------------------------------------\n\nTITLE: Documenting ATen Operator Invocation Patterns - PyTorch - Python\nDESCRIPTION: This pseudo-code enumerates invocation counts and argument structures for ATen operators in PyTorch, especially with float16 tensors. It lists operator signatures, shapes, types, and additional parameters, providing context for profiling kernel or backend usage trends. Inputs generally specify input/output tensor characteristics, operator specifics, and attributes such as strides, convolutional params, and reduction axes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 4, ((T([128, 4, 196, 196], f16), -1, False), {})\ncnt: 1, ((T([128, 8, 49, 196], f16), -1, False), {})\ncnt: 4, ((T([128, 8, 49, 49], f16), -1, False), {})\ncnt: 1, ((T([128, 16, 16, 49], f16), -1, False), {})\ncnt: 4, ((T([128, 12, 16, 16], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 4, ((T([128, 12, 16, 16], f16), T([128, 12, 16, 16], f16), -1, f16), {})\ncnt: 1, ((T([128, 16, 16, 49], f16), T([128, 16, 16, 49], f16), -1, f16), {})\ncnt: 4, ((T([128, 8, 49, 49], f16), T([128, 8, 49, 49], f16), -1, f16), {})\ncnt: 1, ((T([128, 8, 49, 196], f16), T([128, 8, 49, 196], f16), -1, f16), {})\ncnt: 4, ((T([128, 4, 196, 196], f16), T([128, 4, 196, 196], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 8, ((T([128, 196, 256], f16), [128, 196, 256]), {})\ncnt: 4, ((T([128, 4, 196, 16], f16), [512, 196, 16]), {})\ncnt: 4, ((T([128, 4, 16, 196], f16), [512, 16, 196]), {})\ncnt: 4, ((T([512, 196, 196], f16), [128, 4, 196, 196]), {})\ncnt: 8, ((T([128, 4, 196, 32], f16), [512, 196, 32]), {})\ncnt: 4, ((T([512, 196, 32], f16), [128, 4, 196, 32]), {})\ncnt: 4, ((T([128, 196, 4, 32], f16), [128, 196, 128]), {})\ncnt: 8, ((T([25088, 128], f16), [128, 196, 128]), {})\ncnt: 1, ((T([128, 196, 640], f16), [128, 196, 640]), {})\ncnt: 1, ((T([128, 7, 7, 128], f16), [128, 49, 128]), {})\ncnt: 1, ((T([6272, 128], f16), [128, 49, 128]), {})\ncnt: 5, ((T([128, 8, 49, 16], f16), [1024, 49, 16]), {})\ncnt: 1, ((T([128, 8, 16, 196], f16), [1024, 16, 196]), {})\ncnt: 1, ((T([1024, 49, 196], f16), [128, 8, 49, 196]), {})\ncnt: 1, ((T([128, 8, 196, 64], f16), [1024, 196, 64]), {})\ncnt: 1, ((T([1024, 49, 64], f16), [128, 8, 49, 64]), {})\ncnt: 1, ((T([128, 49, 8, 64], f16), [128, 49, 512]), {})\ncnt: 10, ((T([6272, 256], f16), [128, 49, 256]), {})\ncnt: 9, ((T([6272, 512], f16), [128, 49, 512]), {})\ncnt: 4, ((T([128, 8, 16, 49], f16), [1024, 16, 49]), {})\ncnt: 4, ((T([1024, 49, 49], f16), [128, 8, 49, 49]), {})\ncnt: 8, ((T([128, 8, 49, 32], f16), [1024, 49, 32]), {})\ncnt: 4, ((T([1024, 49, 32], f16), [128, 8, 49, 32]), {})\ncnt: 4, ((T([128, 49, 8, 32], f16), [128, 49, 256]), {})\ncnt: 1, ((T([6272, 1280], f16), [128, 49, 1280]), {})\ncnt: 1, ((T([128, 4, 4, 256], f16), [128, 16, 256]), {})\ncnt: 1, ((T([2048, 256], f16), [128, 16, 256]), {})\ncnt: 1, ((T([128, 16, 16, 16], f16), [2048, 16, 16]), {})\ncnt: 1, ((T([128, 16, 16, 49], f16), [2048, 16, 49]), {})\ncnt: 1, ((T([2048, 16, 49], f16), [128, 16, 16, 49]), {})\ncnt: 1, ((T([128, 16, 49, 64], f16), [2048, 49, 64]), {})\ncnt: 1, ((T([2048, 16, 64], f16), [128, 16, 16, 64]), {})\ncnt: 1, ((T([128, 16, 16, 64], f16), [128, 16, 1024]), {})\ncnt: 10, ((T([2048, 384], f16), [128, 16, 384]), {})\ncnt: 9, ((T([2048, 768], f16), [128, 16, 768]), {})\ncnt: 8, ((T([128, 12, 16, 16], f16), [1536, 16, 16]), {})\ncnt: 4, ((T([1536, 16, 16], f16), [128, 12, 16, 16]), {})\ncnt: 8, ((T([128, 12, 16, 32], f16), [1536, 16, 32]), {})\ncnt: 4, ((T([1536, 16, 32], f16), [128, 12, 16, 32]), {})\ncnt: 4, ((T([128, 16, 12, 32], f16), [128, 16, 384]), {})\ncnt: 1, ((T([128, 16, 16, 64], f16), [2048, 16, 64]), {})\ncnt: 1, ((T([128, 16, 16, 16], f16), [128, 16, 256]), {})\ncnt: 1, ((T([128, 8, 49, 64], f16), [1024, 49, 64]), {})\ncnt: 1, ((T([128, 49, 8, 16], f16), [128, 49, 128]), {})\nOperator: aten.add.Tensor\ncnt: 4, ((T([128, 4, 196, 196], f16), T([4, 196, 196], f16)), {})\ncnt: 8, ((T([128, 196, 128], f16, stride=(25088, 1, 196)), T([128, 196, 128], f16)), {})\ncnt: 1, ((T([128, 8, 49, 196], f16), T([8, 49, 196], f16)), {})\ncnt: 19, ((T([128, 49, 256], f16), T([128, 49, 256], f16)), {})\ncnt: 4, ((T([128, 8, 49, 49], f16), T([8, 49, 49], f16)), {})\ncnt: 1, ((T([128, 16, 16, 49], f16), T([16, 16, 49], f16)), {})\ncnt: 18, ((T([128, 16, 384], f16), T([128, 16, 384], f16)), {})\ncnt: 4, ((T([128, 12, 16, 16], f16), T([12, 16, 16], f16)), {})\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16)), {})\ncnt: 1, ((T([128, 384], f16), T([128, 384], f16)), {})\ncnt: 9, ((T([128, 196, 128], f16), T([128, 196, 128], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 64, ((T([], i64), 1), {})\nOperator: aten.addmm.default\ncnt: 2, ((T([1000], f16), T([128, 384], f16), T([384, 1000], f16, stride=(1, 384))), {})\nOperator: aten.bmm.default\ncnt: 8, ((T([128, 196, 128], f16, stride=(25088, 1, 196)), T([128, 128, 256], f16, stride=(0, 1, 128))), {})\ncnt: 4, ((T([512, 196, 16], f16), T([512, 16, 196], f16)), {})\ncnt: 4, ((T([512, 196, 196], f16), T([512, 196, 32], f16)), {})\ncnt: 1, ((T([128, 196, 128], f16, stride=(25088, 1, 196)), T([128, 128, 640], f16, stride=(0, 1, 128))), {})\ncnt: 1, ((T([1024, 49, 16], f16), T([1024, 16, 196], f16)), {})\ncnt: 1, ((T([1024, 49, 196], f16), T([1024, 196, 64], f16)), {})\ncnt: 4, ((T([1024, 49, 16], f16), T([1024, 16, 49], f16)), {})\ncnt: 4, ((T([1024, 49, 49], f16), T([1024, 49, 32], f16)), {})\ncnt: 1, ((T([2048, 16, 16], f16), T([2048, 16, 49], f16)), {})\ncnt: 1, ((T([2048, 16, 49], f16), T([2048, 49, 64], f16)), {})\ncnt: 4, ((T([1536, 16, 16], f16), T([1536, 16, 16], f16)), {})\ncnt: 4, ((T([1536, 16, 16], f16), T([1536, 16, 32], f16)), {})\ncnt: 4, ((T([1536, 16, 16], f16, stride=(256, 1, 16)), T([1536, 16, 32], f16)), {})\ncnt: 4, ((T([1536, 16, 32], f16), T([1536, 32, 16], f16, stride=(512, 1, 32))), {})\ncnt: 4, ((T([1536, 16, 16], f16, stride=(256, 1, 16)), T([1536, 16, 16], f16)), {})\ncnt: 4, ((T([1536, 16, 16], f16), T([1536, 16, 16], f16, stride=(256, 1, 16))), {})\ncnt: 1, ((T([2048, 49, 16], f16, stride=(784, 1, 49)), T([2048, 16, 64], f16)), {})\ncnt: 1, ((T([2048, 16, 64], f16), T([2048, 64, 49], f16, stride=(3136, 1, 64))), {})\ncnt: 1, ((T([2048, 16, 16], f16, stride=(256, 1, 16)), T([2048, 16, 49], f16)), {})\ncnt: 1, ((T([2048, 16, 49], f16), T([2048, 49, 16], f16, stride=(784, 1, 49))), {})\ncnt: 4, ((T([1024, 49, 49], f16, stride=(2401, 1, 49)), T([1024, 49, 32], f16)), {})\ncnt: 4, ((T([1024, 49, 32], f16), T([1024, 32, 49], f16, stride=(1568, 1, 32))), {})\ncnt: 4, ((T([1024, 16, 49], f16, stride=(784, 1, 16)), T([1024, 49, 49], f16)), {})\ncnt: 4, ((T([1024, 49, 49], f16), T([1024, 49, 16], f16, stride=(784, 1, 49))), {})\ncnt: 1, ((T([1024, 196, 49], f16, stride=(9604, 1, 196)), T([1024, 49, 64], f16)), {})\ncnt: 1, ((T([1024, 49, 64], f16), T([1024, 64, 196], f16, stride=(12544, 1, 64))), {})\ncnt: 1, ((T([1024, 16, 49], f16, stride=(784, 1, 16)), T([1024, 49, 196], f16)), {})\ncnt: 1, ((T([1024, 49, 196], f16), T([1024, 196, 16], f16, stride=(3136, 1, 196))), {})\ncnt: 1, ((T([128, 128, 196], f16), T([128, 196, 640], f16)), {})\ncnt: 1, ((T([128, 196, 640], f16), T([128, 640, 128], f16, stride=(0, 128, 1))), {})\ncnt: 8, ((T([128, 128, 196], f16), T([128, 196, 256], f16)), {})\ncnt: 8, ((T([128, 196, 256], f16), T([128, 256, 128], f16, stride=(0, 128, 1))), {})\ncnt: 4, ((T([512, 196, 196], f16, stride=(38416, 1, 196)), T([512, 196, 32], f16)), {})\ncnt: 4, ((T([512, 196, 32], f16), T([512, 32, 196], f16, stride=(6272, 1, 32))), {})\ncnt: 4, ((T([512, 16, 196], f16, stride=(3136, 1, 16)), T([512, 196, 196], f16)), {})\ncnt: 4, ((T([512, 196, 196], f16), T([512, 196, 16], f16, stride=(3136, 1, 196))), {})\nOperator: aten.cat.default\ncnt: 4, (([T([128, 16, 12, 16], f16, stride=(3072, 16, 256, 1)), T([128, 16, 12, 16], f16, stride=(3072, 1, 256, 16)), T([128, 16, 12, 32], f16, stride=(6144, 32, 512, 1))], 3), {})\ncnt: 1, (([T([128, 49, 16, 16], f16, stride=(12544, 1, 784, 49)), T([128, 49, 16, 64], f16, stride=(50176, 64, 3136, 1))], 3), {})\ncnt: 4, (([T([128, 49, 8, 16], f16, stride=(6272, 16, 784, 1)), T([128, 49, 8, 16], f16, stride=(6272, 1, 784, 49)), T([128, 49, 8, 32], f16, stride=(12544, 32, 1568, 1))], 3), {})\ncnt: 1, (([T([128, 196, 8, 16], f16, stride=(25088, 1, 3136, 196)), T([128, 196, 8, 64], f16, stride=(100352, 64, 12544, 1))], 3), {})\ncnt: 4, (([T([128, 196, 4, 16], f16, stride=(12544, 16, 3136, 1)), T([128, 196, 4, 16], f16, stride=(12544, 1, 3136, 196)), T([128, 196, 4, 32], f16, stride=(25088, 32, 6272, 1))], 3), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([32, 16, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 56, 56], f16), T([64, 32, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 28, 28], f16), T([128, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 128, 14, 14], f16, stride=(25088, 1, 1792, 128)), T([128, 64, 28, 28], f16), T([128, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 28, 28], f16), T([128, 32, 56, 56], f16), T([64, 32, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 56, 56], f16), T([128, 16, 112, 112], f16), T([32, 16, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\ncnt: 1, ((T([640, 128], f16), T([640, 128], f16, stride=(1, 640))), {})\ncnt: 8, ((T([256, 128], f16), T([256, 128], f16, stride=(1, 256))), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 16, 384], f16, stride=(384, 0, 1)), 16), {})\nOperator: aten.div.Tensor\ncnt: 2, ((T([128, 1000], f16), 2), {})\nOperator: aten.hardswish.default\ncnt: 1, ((T([128, 16, 112, 112], f16),), {})\ncnt: 1, ((T([128, 32, 56, 56], f16),), {})\ncnt: 1, ((T([128, 64, 28, 28], f16),), {})\n\n```\n\n----------------------------------------\n\nTITLE: Logging Softmax Operation in PyTorch\nDESCRIPTION: Analysis of the aten._log_softmax.default operator which logs the softmax of a tensor. In the provided example, it processes a tensor with shape [1, 512] using float16 precision on dimension 1. It is essential for normalizing log probabilities during model training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 2, ((T([1, 512], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Copy and Unsafe View Operations in PyTorch\nDESCRIPTION: This snippet showcases `aten._to_copy.default` and `aten._unsafe_view.default` operations. `aten._to_copy.default` is used for copying a tensor to different settings like dtype and layout. `aten._unsafe_view.default` reshapes tensors into specified views. It requires specification of data shape and types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForCausalLM_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([128, 128], f32),), {\\\"dtype\\\": f16})\ncnt: 1, ((T([16, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {\\\"dtype\\\": f16, \\\"layout\\\": torch.strided, \\\"device\\\": \\\"cuda\\\"})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 18, ((T([16, 128, 12, 64], f16), [16, 128, 768]), {})\ncnt: 1, ((T([2048, 50005], f16), [16, 128, 50005]), {})\ncnt: 6, ((T([16, 12, 128, 64], f16), [192, 128, 64]), {})\ncnt: 6, ((T([16, 128, 768], f16), [2048, 768]), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.slice_backward.default Operator (Log)\nDESCRIPTION: Log entries detailing calls to the PyTorch `aten.slice_backward.default` operator. Each line shows the invocation count (`cnt`) and arguments: the gradient output tensor, the input size list, the dimension sliced, the start index, the end index (often max int), and the step.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.slice_backward.default\ncnt: 16, ((T([64, 197, 256], f16), [64, 197, 256], 0, 0, 9223372036854775807, 1), {})\ncnt: 16, ((T([64, 401, 128], f16), [64, 401, 128], 0, 0, 9223372036854775807, 1), {})\ncnt: 6, ((T([64, 196, 256], f16, stride=(50432, 256, 1)), [64, 197, 256], 1, 1, 9223372036854775807, 1), {})\ncnt: 3, ((T([64, 1, 128], f16), [64, 1, 128], 0, 0, 9223372036854775807, 1), {})\ncnt: 9, ((T([64, 1, 128], f16), [64, 401, 128], 1, 0, 1, 1), {})\ncnt: 6, ((T([64, 400, 128], f16, stride=(51328, 128, 1)), [64, 401, 128], 1, 1, 9223372036854775807, 1), {})\ncnt: 3, ((T([64, 1, 256], f16), [64, 1, 256], 0, 0, 9223372036854775807, 1), {})\ncnt: 9, ((T([64, 1, 256], f16), [64, 197, 256], 1, 0, 1, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Addition Operation Patterns\nDESCRIPTION: A collection of tensor addition operations showing various patterns of 4D tensor operations with different shapes (4,512,7,7), (4,32,7,7), (4,256,14,14), (4,128,28,28), and (4,64,56,56) using half-precision (f16) format. Each operation includes count (cnt) of occurrences and stride patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 512, 7, 7], f16, stride=(50176, 49, 7, 1)), T([4, 512, 7, 7], f16, stride=(48608, 49, 7, 1))), {}\n```\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 32, 7, 7], f16, stride=(50176, 49, 7, 1)), T([4, 32, 7, 7], f16, stride=(48608, 49, 7, 1))), {}\n```\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 256, 14, 14], f16, stride=(200704, 196, 14, 1)), T([4, 256, 14, 14], f16, stride=(194432, 196, 14, 1))), {}\n```\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 128, 28, 28], f16, stride=(401408, 784, 28, 1)), T([4, 128, 28, 28], f16, stride=(376320, 784, 28, 1))), {}\n```\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 64, 56, 56], f16, stride=(802816, 3136, 56, 1)), T([4, 64, 56, 56], f16, stride=(702464, 3136, 56, 1))), {}\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.addmm.default in PyTorch ATen\nDESCRIPTION: Logs calls to the `aten.addmm.default` operator, which performs an add and matrix multiplication operation (C = beta*M + alpha*(mat1 @ mat2)). The logs show tensor arguments represented as `T([shape], dtype, optional_stride)`, indicating the dimensions, data type (f16), and sometimes memory layout (stride) of the input tensors. The `cnt` field indicates the frequency of each specific call signature.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 1280], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1280], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Convolution Forward Operations\nDESCRIPTION: A collection of convolution operations with varied input tensor shapes, kernel sizes, and stride parameters. The operations use half-precision (f16) format tensors with batch size 4 and different spatial dimensions (28x28, 14x14, 7x7).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([4, 480, 28, 28], f16), T([128, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX Dependencies using pip\nDESCRIPTION: This Bash snippet shows how to install the necessary dependencies for using the ONNX exporter. The dependencies include the onnx and onnxscript Python packages, which can be installed via pip. These packages enable the exporting and running of models in ONNX format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --upgrade onnx onnxscript\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.mean.dim Calls - PyTorch - Python\nDESCRIPTION: Demonstrates invocations of the aten.mean.dim operator in PyTorch with specific tensor shapes, reduction dimensions, and boolean options. Dependencies include PyTorch's dispatcher for operator invocations and specific tensors with required dimensions and dtypes. Inputs are tensor shapes and options; output is reduced tensor statistics, with usage pattern information for further analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 1024, 4, 4], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.silu_.default In-place SiLU Activation in PyTorch ATen\nDESCRIPTION: Documents observed calls to the in-place SiLU (Sigmoid Linear Unit) activation function (`aten.silu_.default`) in PyTorch ATen. It lists the various input tensor shapes (all using f16 data type) encountered during profiling and their respective call counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.silu_.default\ncnt: 1, ((T([128, 16, 112, 112], f16),), {})\ncnt: 1, ((T([128, 32, 112, 112], f16),), {})\ncnt: 1, ((T([128, 64, 112, 112], f16),), {})\ncnt: 2, ((T([128, 64, 56, 56], f16),), {})\ncnt: 1, ((T([128, 128, 56, 56], f16),), {})\ncnt: 3, ((T([128, 128, 28, 28], f16),), {})\ncnt: 1, ((T([128, 384, 28, 28], f16),), {})\ncnt: 12, ((T([128, 384, 14, 14], f16),), {})\ncnt: 5, ((T([128, 384, 7, 7], f16),), {})\ncnt: 1, ((T([128, 2304, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.sigmoid.default with Tensor Arguments (Text)\nDESCRIPTION: This section logs calls to the `aten.sigmoid.default` operator, which applies the sigmoid activation function element-wise. The examples show the operator being called on tensors with shapes like [128, 256, 1, 1] and [128, 1536, 1, 1], all using the float16 (f16) data type. The input is a tuple containing a single tensor description.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([128, 256, 1, 1], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([128, 512, 1, 1], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 9, ((T([128, 1536, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Layer Configurations\nDESCRIPTION: Convolution operations with different kernel sizes, strides, and padding configurations. Shows progression from initial layers to deeper network levels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 3, 224, 224], f16), T([128, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring TunableOp Environment Variables for Untuned GEMM Collection (Shell)\nDESCRIPTION: Demonstrates setting environment variables in a shell to configure TunableOp for the first step of offline tuning. `PYTORCH_TUNABLEOP_ENABLED=1` enables the feature, `PYTORCH_TUNABLEOP_TUNING=0` disables runtime tuning, and `PYTORCH_TUNABLEOP_RECORD_UNTUNED=1` instructs TunableOp to record encountered operations (like GEMMs) without tuning them, typically into a file like `tunableop_untuned0.csv`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nPYTORCH_TUNABLEOP_ENABLED=1\nPYTORCH_TUNABLEOP_TUNING=0\nPYTORCH_TUNABLEOP_RECORD_UNTUNED=1\n...\n```\n\n----------------------------------------\n\nTITLE: Input Tuples for aten.nll_loss and aten.nll_loss_backward.default - Python\nDESCRIPTION: These examples provide test input tuples for PyTorch's negative log-likelihood loss and its backward counterparts. Each contains tensors for predictions, targets (typically integer class labels), optional weights, and parameters for ignore index and reduction settings. Used in validating both loss computation and backpropagation, the cases depend on correct tensor type/rank and match to PyTorchs NLL loss signature.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Testing PyTorch Model Function\nDESCRIPTION: Creates test inputs and performs basic forward and backward pass to verify the model function works\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\na, b, c, d = [torch.randn(2, 4, requires_grad=True) for _ in range(4)]\nref = fn(a, b, c, d)\nloss = ref.sum()\nloss.backward()\n```\n\n----------------------------------------\n\nTITLE: Configuring AOT Inference Test Executable\nDESCRIPTION: Sets up the main test executable with proper dependencies, linking, and platform-specific configurations for CUDA/ROCm support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/aoti_inference/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(test_aoti_inference\n  ${TORCH_ROOT}/test/cpp/common/main.cpp\n  ${INDUCTOR_TEST_SRCS}\n  data.pt\n  script_data.pt\n  script_model_cpu.pt\n  script_model_cuda.pt\n)\nadd_dependencies(test_aoti_inference aoti_custom_class aoti_script_model)\n\ntarget_link_libraries(test_aoti_inference PRIVATE\n  torch\n  gtest\n  -Wl,--no-as-needed aoti_custom_class\n)\n\nif(USE_CUDA)\n  target_include_directories(test_aoti_inference PRIVATE ${ATen_CUDA_INCLUDE})\n  target_compile_definitions(test_aoti_inference PRIVATE USE_CUDA)\nelseif(USE_ROCM)\n    target_include_directories(test_aoti_inference PRIVATE ${ATen_HIP_INCLUDE})\n    target_compile_definitions(test_aoti_inference PRIVATE USE_ROCM)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tensor Concatenation Operations in PyTorch (14x14 Feature Maps)\nDESCRIPTION: PyTorch's tensor concatenation operations for 14x14 feature maps along dimension 1 (channel dimension). These progressively growing concatenation operations show the dense layer patterns where each new 32-channel feature map is added to an increasingly larger collection.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, (([T([64, 256, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 256, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16), T([64, 32, 14, 14], f16)], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Tagging and Pushing a PyTorch Release Candidate with Git\nDESCRIPTION: This command sequence performs force tagging of a release candidate (RC) and pushes it to the origin remote, which will trigger build workflows for PyTorch binaries. Usage: the tag (e.g., 'v1.12.0-rc2') must follow strict naming conventions and be unique for each RC. Inputs: tag name; outputs: a pushed git tag and the triggering of CI/CD pipelines as configured. Dependencies: git, repo write access. Running this initiates official binary build workflows on GitHub.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#2025-04-22_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ngit tag -f  v1.12.0-rc2\ngit push origin  v1.12.0-rc2\n```\n\n----------------------------------------\n\nTITLE: Documenting MTIA Backend Interfaces with reStructuredText\nDESCRIPTION: This reStructuredText snippet structures the interface documentation for torch.mtia, specifying available API objects and their organization in generated docs. Dependencies include Sphinx and autodoc extensions, as well as the relevant PyTorch backend. It relies on directives like .. automodule:: and .. autosummary:: to generate up-to-date API reference documentation. The input consists of the module name and listed objects; no outputs are generated directly from this snippetits effect is seen in rendered documentation. Limitations: no functional code or logic, strictly documentation structure.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mtia.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\ntorch.mtia\n===================================\n\nThe MTIA backend is implemented out of the tree, only interfaces are be defined here.\n\n.. automodule:: torch.mtia\n.. currentmodule:: torch.mtia\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    StreamContext\n    current_device\n    current_stream\n    default_stream\n    device_count\n    init\n    is_available\n    is_initialized\n    memory_stats\n    get_device_capability\n    empty_cache\n    record_memory_history\n    snapshot\n    set_device\n    set_stream\n    stream\n    synchronize\n    device\n    set_rng_state\n    get_rng_state\n    DeferredMtiaCallError\n\nStreams and events\n------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Event\n    Stream\n\n```\n\n----------------------------------------\n\nTITLE: Implementing _log_softmax_backward_data in PyTorch\nDESCRIPTION: This snippet uses the _log_softmax_backward_data operator for computing the gradient of the _log_softmax function. Required dependencies are PyTorch, and it involves tensors reflecting gradients and previous inputs. The output is a gradient tensor shaped similarly as input tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([1024, 50005], f16), T([1024, 50005], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing GroupNorm in Place of BatchNorm2d - PyTorch (Python)\nDESCRIPTION: Replaces a BatchNorm2d layer with a GroupNorm layer to avoid complications with running statistics during batched operations in Functorch. The first argument \"C\" is the number of channels, while \"G\" is the number of groups, which must evenly divide C. Set track_running_stats to False to ensure running means are not updated. No external dependencies are required beyond PyTorch's GroupNorm.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.batch_norm.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nBatchNorm2d(C, G, track_running_stats=False)\n```\n\n----------------------------------------\n\nTITLE: Calling Autotune Select Algorithm in PyTorch\nDESCRIPTION: Function call for autotuning algorithm selection that must follow AutoHeuristicSelectAlgorithm constructor. Requires matching parameters from the constructor call.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nautotune_select_algorithm(name, choices, input_nodes, layout)\n```\n\n----------------------------------------\n\nTITLE: Executing NumPy Function on CUDA using torch.compile\nDESCRIPTION: Example showing how to compile and execute a NumPy function on CUDA by using torch.device context manager. The function performs matrix multiplication and summation operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\n\n@torch.compile\ndef numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = np.random.randn(1024, 64)\nY = np.random.randn(1024, 64)\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\nassert isinstance(Z, np.ndarray)\n```\n\n----------------------------------------\n\nTITLE: Utilities List in RestructuredText\nDESCRIPTION: A list of utility functions in PyTorch for type promotion, deterministic algorithms configuration, and other system-level operations, formatted as a RestructuredText autosummary.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_6\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    compiled_with_cxx11_abi\n    result_type\n    can_cast\n    promote_types\n    use_deterministic_algorithms\n    are_deterministic_algorithms_enabled\n    is_deterministic_algorithms_warn_only_enabled\n    set_deterministic_debug_mode\n    get_deterministic_debug_mode\n    set_float32_matmul_precision\n    get_float32_matmul_precision\n    set_warn_always\n    get_device_module\n    is_warn_always_enabled\n    vmap\n    _assert\n```\n\n----------------------------------------\n\nTITLE: Defining and Using C-style Logging in C\nDESCRIPTION: This C code snippet demonstrates how to set up and use a logging library for logging errors, warnings, and debug information with printf-style formatting. It includes macro definitions for setting log levels and provides logging functions to report various statuses and events in a program. Dependencies include the clog library, and the code is intended for environments with C99 or C++ compatibility. Inputs are status codes and variable values, and outputs are formatted log messages. The snippet shows usage in a hypothetical function some_function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/README.md#2025-04-22_snippet_0\n\nLANGUAGE: C\nCODE:\n```\n#include <clog.h>\\n\\n#ifndef MYMODULE_LOG_LEVEL\\n    #define MYMODULE_LOG_LEVEL CLOG_DEBUG\\n#endif\\n\\nCLOG_DEFINE_LOG_DEBUG(mymodule_, \\\"My Module\\\", MYMODULE_LOG_LEVEL);\\nCLOG_DEFINE_LOG_INFO(mymodule_, \\\"My Module\\\", MYMODULE_LOG_LEVEL);\\nCLOG_DEFINE_LOG_WARNING(mymodule_, \\\"My Module\\\", MYMODULE_LOG_LEVEL);\\nCLOG_DEFINE_LOG_ERROR(mymodule_, \\\"My Module\\\", MYMODULE_LOG_LEVEL);\\n\\n...\\n\\nvoid some_function(...) {\\n    int status = ...\\n    if (status != 0) {\\n        mymodule_log_error(\\n            \\\"something really bad happened: \\\"\\n            \\\"operation failed with status %d\\\", status);\\n    }\\n\\n    uint32_t expected_zero = ...\\n    if (expected_zero != 0) {\\n        mymodule_log_warning(\\n            \\\"something suspicious happened (var = %\\\"PRIu32\\\"), \\\"\\n            \\\"fall back to generic implementation\\\", expected_zero);\\n    }\\n\\n    void* usually_non_null = ...\\n    if (usually_non_null == NULL) {\\n        mymodule_log_info(\\n            \\\"something unusual, but common, happened: \\\"\\n            \\\"enabling work-around\\\");\\n    }\\n\\n    float a = ...\\n    mymodule_log_debug(\\\"computed a = %.7f\\\", a);\\n}\n```\n\n----------------------------------------\n\nTITLE: Building Static Library for Unboxing Kernels and Custom Ops in CMake\nDESCRIPTION: Defines a static library target named `unbox_lib`. It includes the C++ source files generated by the custom command (`GEN_COMMAND_sources`) and manually written source files (`operator_registry.cpp`, `custom_ops.cpp`). Sets public include directories and links the library against `torch_cpu`. Defines `USE_ATEN_LIB` preprocessor macro for this library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(unbox_lib STATIC\n        ${GEN_COMMAND_sources}\n        ${TEST_ROOT}/operator_registry.cpp\n        ${TEST_ROOT}/custom_ops.cpp\n        )\ntarget_include_directories(unbox_lib PUBLIC ${TEST_ROOT} ${ATen_CPU_INCLUDE})\ntarget_link_libraries(unbox_lib PUBLIC torch_cpu)\ntarget_compile_definitions(unbox_lib PUBLIC USE_ATEN_LIB)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Statistics\nDESCRIPTION: Detailed breakdown of PyTorch operator usage including tensor shapes, data types, and call counts. Shows common deep learning operations like softmax, matrix multiplication (mm/bmm), embeddings, and layer normalization with their specific configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Bart_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Softmax operations\naten._softmax.default(T([48, 512, 512], f16), -1, False)\naten._softmax_backward_data.default(T([48, 512, 512], f16), T([48, 512, 512], f16), -1, f16)\n\n# Matrix multiplications\naten.mm.default(T([2048, 768], f16), T([768, 50265], f16, stride=(1, 768)))\naten.bmm.default(T([48, 512, 64], f16), T([48, 64, 512], f16, stride=(32768, 1, 64)))\n\n# Layer normalization\naten.native_layer_norm.default(T([4, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-05)\n\n# Embedding operations\naten.embedding.default(T([50265, 768], f16), T([4, 512], i64), 1)\n\n# Various tensor manipulations\naten._unsafe_view.default(T([4, 512, 12, 64], f16), [4, 512, 768])\naten.add.Tensor(T([4, 512, 768], f16), T([4, 512, 768], f16))\naten.gelu.default(T([4, 512, 3072], f16))\n```\n\n----------------------------------------\n\nTITLE: Enabling Testing with CTest in CMake\nDESCRIPTION: Enables testing capabilities within the CMake build system using the `enable_testing()` command. This integrates the project with CTest for running tests.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_22\n\nLANGUAGE: cmake\nCODE:\n```\nenable_testing()\n```\n\n----------------------------------------\n\nTITLE: Activation Functions - Sigmoid and SiLU\nDESCRIPTION: Sigmoid and SiLU (Swish) activation functions applied to tensors of varying sizes, including their backward passes for gradient computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 32, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: GELU Activation in Transformer FFN Layers\nDESCRIPTION: Records the GELU (Gaussian Error Linear Unit) activation function calls and their backward passes. These are primarily used in the feedforward networks of transformer blocks with intermediate dimension 3072.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.gelu.default\ncnt: 36, ((T([2, 576, 3072], f16),), {})\ncnt: 2, ((T([2, 1, 3072], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 2, ((T([2, 1, 3072], f16), T([2, 1, 3072], f16)), {})\ncnt: 36, ((T([2, 576, 3072], f16), T([2, 576, 3072], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking PyTorch Operator Usage Statistics\nDESCRIPTION: A log of different PyTorch operators with their call counts, input tensor shapes, and data types. This section specifically tracks sigmoid, sigmoid_backward, slice_backward, sum, and threshold_backward operations with details about their tensor dimensions and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sigmoid.default\ncnt: 1, ((T([1024, 1], f16),), {})\nOperator: aten.sigmoid_backward.default\ncnt: 1, ((T([1024, 1], f16, stride=(0, 0)), T([1024, 1], f16)), {})\nOperator: aten.slice_backward.default\ncnt: 1, ((T([1024, 249, 249], f16), [1024, 249, 249], 0, 0, 9223372036854775807, 1), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([1024, 1], f16), [0], True), {})\ncnt: 9, ((T([1024, 4000], f16), [0], True), {})\ncnt: 1, ((T([1024, 192], f16), [0], True), {})\ncnt: 3, ((T([1024, 1500], f16), [0], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([1024, 1], f16),), {})\nOperator: aten.threshold_backward.default\ncnt: 9, ((T([1024, 4000], f16), T([1024, 4000], f16), 0), {})\ncnt: 1, ((T([1024, 192], f16), T([1024, 192], f16), 0), {})\ncnt: 3, ((T([1024, 1500], f16), T([1024, 1500], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Logging Aten Operator: aten.sum.SymInt (Text)\nDESCRIPTION: Log entries showing example invocations of the 'aten.sum.SymInt' operator. Each line ('cnt') represents a call signature, detailing the input tensor (T) with shape, data type (f16), optional stride, the dimensions to sum over (list of integers), and a boolean flag (likely keepdim).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\ncnt: 8, ((T([4160, 1024], f16), [0], True), {})\ncnt: 4, ((T([4160, 4096], f16), [0], True), {})\ncnt: 4, ((T([4160, 3072], f16), [0], True), {})\ncnt: 1, ((T([64, 1, 1024], f16, stride=(66560, 1024, 1)), [0, 1], True), {})\ncnt: 12, ((T([16448, 512], f16), [0], True), {})\ncnt: 6, ((T([16448, 2048], f16), [0], True), {})\ncnt: 6, ((T([16448, 1536], f16), [0], True), {})\ncnt: 1, ((T([64, 1, 512], f16, stride=(131584, 512, 1)), [0, 1], True), {})\ncnt: 6, ((T([61568, 256], f16), [0], True), {})\ncnt: 3, ((T([61568, 1024], f16), [0], True), {})\ncnt: 3, ((T([61568, 768], f16), [0], True), {})\ncnt: 1, ((T([64, 1, 256], f16, stride=(246272, 256, 1)), [0, 1], True), {})\ncnt: 1, ((T([64, 256, 31, 31], f16, stride=(246272, 1, 7936, 256)), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Building Libtorch C++ Library (Python)\nDESCRIPTION: The build_libtorch.py script builds libtorch, a standalone C++ library without Python support. This build script is tested in CI.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/README.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\npython build_libtorch.py\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Analysis\nDESCRIPTION: Comprehensive listing of PyTorch operators with their call counts, tensor shapes, and parameters. Includes operations like softmax, convolutions, matrix multiplications, and layer normalization commonly used in transformer architectures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/vit_base_patch16_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\n\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\n\nOperator: aten._softmax.default\ncnt: 12, ((T([64, 12, 197, 197], f16), -1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch __torch_function__ Benchmarks\nDESCRIPTION: Commands to run the benchmark suite for measuring __torch_function__ overhead. It includes options for benchmarking all cases or generating flame graphs for specific scenarios.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/overrides_benchmark/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Benchmark all the cases\npython bench.py\n\n# Flame graph pertaining to each case.\npy-spy record -o tensor.svg --native -- python pyspybench.py Tensor\npy-spy record -o subtensor.svg --native -- python pyspybench.py SubTensor\npy-spy record -o overridden.svg --native -- python pyspybench.py WithTorchFunction\npy-spy record -o suboverridden.svg --native -- python pyspybench.py SubWithTorchFunction\n```\n\n----------------------------------------\n\nTITLE: Using CUDAStreamGuard for Stream Management in PyTorch C++\nDESCRIPTION: This example shows how to use CUDAStreamGuard to manage CUDA streams within a scope, automatically handling stream switching and restoration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n// create a tensor on device 0\ntorch::Tensor tensor0 = torch::ones({2, 2}, torch::device(torch::kCUDA));\n// get a new stream from CUDA stream pool on device 0\nat::cuda::CUDAStream myStream = at::cuda::getStreamFromPool(false, 0);\n// set the current CUDA stream to `myStream` within the scope using CUDA stream guard\n{\n  at::cuda::CUDAStreamGuard guard(myStream);\n  // current CUDA stream is `myStream` from here till the end of bracket.\n  // sum() on tensor0 uses `myStream` as current CUDA stream\n  tensor0.sum();\n}\n// current CUDA stream is reset to default CUDA stream after CUDA stream guard is destroyed\n// sum() on tensor0 uses default CUDA stream on device 0 as current CUDA stream\ntensor0.sum();\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA Matrix Multiplication in PyTorch\nDESCRIPTION: This function implements batch matrix multiplication (bmm) for CUDA devices in PyTorch. It takes two input tensors and performs the bmm operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at12_GLOBAL__N_116wrapper_CUDA_bmmERKNS_6TensorES3_\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication in DenseNet Final Classification Layer\nDESCRIPTION: This snippet shows the addmm operation which performs matrix multiplication plus addition for the final classification layer of DenseNet. It operates on the feature representation and weight matrix to produce classification logits.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([32, 1024], f16), T([1024, 1000], f16, stride=(1, 1024))), {})\n```\n\n----------------------------------------\n\nTITLE: Performing aten.silu_backward in PyTorch\nDESCRIPTION: The script includes aten.silu_backward.default to compute gradients for the Swish activation function during backpropagation. Dependencies comprise PyTorch tensor operations. It transforms Swish-activated outputs and requires gradient input w.r.t. loss, yielding appropriate gradient data for weight updates during network training. Inputs should match Swish-activated layer outputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 132, 1, 1], f16), T([64, 132, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.nll_loss_forward.default Calls - PyTorch - Python\nDESCRIPTION: Shows argument structure of forward NLL loss calculations (aten.nll_loss_forward.default) with typical classification tensor shapes and ignore_index. Inputs must be float16 or similar logits and integer class tensors, with optional ignore index, reduction type, and output is scalar or reduction tensor. Useful for monitoring loss computation in classification training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Example for aten.nll_loss_backward.default\nDESCRIPTION: Logs the backward pass for Negative Log Likelihood loss (`aten.nll_loss_backward.default`). It uses the output gradient (scalar float16), the forward input probabilities ([128, 1000] float16), target labels ([128] int64), reduction type (1 for mean), ignore index (-100), and total weight (scalar float16).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Applying Average Pooling with ATen AvgPool2D Operator\nDESCRIPTION: Demonstrates `aten.avg_pool2d`, used in downsampling in neural networks. Tensors reduced using pool size [2, 2], with strides [2, 2]. Critical for maintaining feature maps while reducing spatial dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([64, 512, 16, 16], f16), [2, 2], [2, 2]), {})\n```\n\n----------------------------------------\n\nTITLE: In-place Sigmoid Activation in PyTorch\nDESCRIPTION: This snippet applies the sigmoid activation function in-place, modifying the input tensor directly. This can be more memory-efficient as it doesn't create a new tensor for the result.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 3, 12, 16, 81], f16, stride=(48960, 16320, 1360, 85, 1)),), {})\ncnt: 1, ((T([8, 3, 24, 32, 81], f16, stride=(195840, 65280, 2720, 85, 1)),), {})\ncnt: 1, ((T([8, 3, 48, 64, 81], f16, stride=(783360, 261120, 5440, 85, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Allocating New Empty Tensors with Strides using aten.new_empty_strided - Python\nDESCRIPTION: Uses the aten.new_empty_strided.default operator to create an empty tensor of a specified shape and stride, mainly for efficient memory allocations, especially on CUDA devices. The code specifies dimensions and stride arrays for the tensor. Prerequisites: PyTorch with CUDA support and tensor memory layout knowledge.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.new_empty_strided.default\ncnt: 36, ((T([288, 64, 64], f16), [288, 64, 64], [4096, 64, 1]), {})\n```\n\n----------------------------------------\n\nTITLE: Logging Aten Operator: aten.nll_loss_backward.default (Text)\nDESCRIPTION: Log entry for an invocation of the 'aten.nll_loss_backward.default' operator. Arguments include tensors (T) with shapes and data types (f16, i64), None placeholders, and integer parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: GELU Activation in PyTorch\nDESCRIPTION: The operator aten.gelu.default applies the GELU activation on tensor data shaped [1, 512, 6144] in float16, essential for incorporating non-linear activation in models to account for complex features.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.gelu.default\ncnt: 24, ((T([1, 512, 6144], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch ATen Project with CMake\nDESCRIPTION: This CMake configuration sets up a project that uses PyTorch's ATen library. It requires a minimum CMake version of 3.0, finds and includes the ATen package, enables C++17 support (except on MSVC), and configures the main executable to link against ATen libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/test/test_install/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.0)\nfind_package(ATen REQUIRED)\ninclude_directories(${ATEN_INCLUDE_DIR})\n\n# C++17\nif(not MSVC)\n    set(CMAKE_CXX_FLAGS \"--std=c++17 ${CMAKE_CXX_FLAGS}\")\nendif()\nadd_executable(main main.cpp)\ntarget_link_libraries(main ${ATEN_LIBRARIES})\n```\n\n----------------------------------------\n\nTITLE: Recompilation Trigger Example\nDESCRIPTION: Shows how changing tensor shapes triggers recompilation in torch.compile when guards fail.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile\ndef fn(x):\n    return x + 1\n\nfn(torch.ones(3, 3))\nfn(torch.ones(4, 4))\n```\n\n----------------------------------------\n\nTITLE: Element-wise Multiplication in PyTorch\nDESCRIPTION: This snippet demonstrates element-wise multiplication operations using aten.mul_.Tensor. It shows multiple operations on tensors with various shapes, consistently using half-precision (f16) data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([54765, 192], f16), T([54765, 1], f16)), {})\ncnt: 2, ((T([54704, 192], f16), T([54704, 1], f16)), {})\ncnt: 4, ((T([54786, 192], f16), T([54786, 1], f16)), {})\ncnt: 2, ((T([54804, 192], f16), T([54804, 1], f16)), {})\ncnt: 3, ((T([54757, 192], f16), T([54757, 1], f16)), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for LibTorch Project\nDESCRIPTION: Basic CMakeLists.txt configuration for a project using LibTorch. It sets up the project, finds the Torch package, and configures the build settings including C++ standard and linking.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\nproject(example-app)\n\nfind_package(Torch REQUIRED)\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\")\n\nadd_executable(example-app example-app.cpp)\ntarget_link_libraries(example-app \"${TORCH_LIBRARIES}\")\nset_property(TARGET example-app PROPERTY CXX_STANDARD 17)\n\n# The following code block is suggested to be used on Windows.\n# According to https://github.com/pytorch/pytorch/issues/25457,\n# the DLLs need to be copied to avoid memory errors.\nif (MSVC)\n  file(GLOB TORCH_DLLS \"${TORCH_INSTALL_PREFIX}/lib/*.dll\")\n  add_custom_command(TARGET example-app\n                     POST_BUILD\n                     COMMAND ${CMAKE_COMMAND} -E copy_if_different\n                     ${TORCH_DLLS}\n                     $<TARGET_FILE_DIR:example-app>)\nendif (MSVC)\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Patterns in Deep Learning Model\nDESCRIPTION: This snippet presents a statistical analysis of PyTorch operator calls in a neural network, showing call counts and tensor shapes. The data reveals a pattern typical of a convolutional neural network with progressively decreasing spatial dimensions and increasing channel counts, using half-precision (f16) tensors throughout.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 58, ((T([], i64), 1), {})\ncnt: 2, ((T([128, 24, 48, 48], f16), T([128, 24, 48, 48], f16)), {})\ncnt: 2, ((T([128, 40, 24, 24], f16), T([128, 40, 24, 24], f16)), {})\ncnt: 6, ((T([128, 80, 12, 12], f16), T([128, 80, 12, 12], f16)), {})\ncnt: 6, ((T([128, 112, 12, 12], f16), T([128, 112, 12, 12], f16)), {})\ncnt: 8, ((T([128, 192, 6, 6], f16), T([128, 192, 6, 6], f16)), {})\ncnt: 5, ((T([128, 1152, 6, 6], f16), T([128, 1152, 6, 6], f16)), {})\ncnt: 1, ((T([128, 672, 6, 6], f16), T([128, 672, 6, 6], f16)), {})\ncnt: 3, ((T([128, 672, 12, 12], f16), T([128, 672, 12, 12], f16)), {})\ncnt: 4, ((T([128, 480, 12, 12], f16), T([128, 480, 12, 12], f16)), {})\ncnt: 1, ((T([128, 240, 12, 12], f16), T([128, 240, 12, 12], f16)), {})\ncnt: 1, ((T([128, 240, 24, 24], f16), T([128, 240, 24, 24], f16)), {})\ncnt: 1, ((T([128, 144, 24, 24], f16), T([128, 144, 24, 24], f16)), {})\ncnt: 1, ((T([128, 144, 48, 48], f16), T([128, 144, 48, 48], f16)), {})\ncnt: 1, ((T([128, 96, 48, 48], f16), T([128, 96, 48, 48], f16)), {})\ncnt: 1, ((T([128, 32, 96, 96], f16), T([128, 32, 96, 96], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Concatenation for Token Processing\nDESCRIPTION: Lists tensor concatenation operations used to combine the class token with sequence tokens. The operations typically join a [2, 1, 768] tensor (class token) with a [2, 576, 768] tensor (sequence embeddings) along dimension 1.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.cat.default\ncnt: 1, (([T([2, 1, 768], f16, stride=(0, 768, 1)), T([2, 576, 768], f16, stride=(442368, 1, 576))], 1), {})\ncnt: 2, (([T([2, 1, 768], f16), T([2, 576, 768], f16, stride=(442368, 1, 576))], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Addition in PyTorch (Python)\nDESCRIPTION: Involves the aten.add operation which adds corresponding elements of two tensors. This operation is widely utilized in neural network operations, particularly when aggregating weighted inputs to neurons.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\naten.add.Tensor\ncnt: 1, ((T([16, 512, 768], f16), T([1, 512, 768], f16)), {})\ncnt: 79, ((T([16, 512, 768], f16), T([16, 512, 768], f16)), {})\ncnt: 12, ((T([16, 12, 512, 512], f16), T([16, 1, 1, 512], f16)), {})\ncnt: 2, ((T([1024, 768], f16), T([1024, 768], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Applying _log_softmax Function in PyTorch\nDESCRIPTION: The _log_softmax operator is called to compute the logarithm of softmax values along a specified dimension. Dependencies include PyTorch, and inputs are tensors with specified shapes and types. Output is a tensor with the same shape transformed by the log softmax operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([1024, 50005], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Log Entry for Tensor Pair (Shape [128, 256, 56, 56], f16)\nDESCRIPTION: Logs the occurrence (count 4) of a tensor pair, both with shape [128, 256, 56, 56] and dtype f16, using default strides. Likely generated during PyTorch execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\ncnt: 4, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Length Method for DataPipe\nDESCRIPTION: Implementation of the __len__ method that returns the length of the source DataPipe. Shows proper length computation pattern for chained DataPipes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/datapipes/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MapperIterDataPipe(IterDataPipe):\n    ...\n\n    def __len__(self):\n        return len(self.dp)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Environment Setup\nDESCRIPTION: Instructions for setting up PyTorch environment with CUDA, torchvision, and installing latest PyTorch from source. Includes verification of installation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install torchvision. It comes with the pytorch stable release binary\nconda install pytorch torchvision -c pytorch\n\n# Install the latest pytorch master from source.\n# It should supersede the installation from the release binary.\ncd $PYTORCH_HOME\npython setup.py build develop\n\n# Check the pytorch installation version\npython -c \"import torch; print(torch.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.sigmoid.default Sigmoid Activation in PyTorch ATen\nDESCRIPTION: Documents observed calls to the Sigmoid activation function (`aten.sigmoid.default`) in PyTorch ATen. It lists the different input tensor shapes (all using f16 data type) and their respective call counts during profiling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sigmoid.default\ncnt: 1, ((T([128, 256, 1, 1], f16),), {})\ncnt: 2, ((T([128, 512, 1, 1], f16),), {})\ncnt: 9, ((T([128, 1536, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.silu.default SiLU Activation in PyTorch ATen\nDESCRIPTION: Documents observed calls to the SiLU (Sigmoid Linear Unit) activation function (`aten.silu.default`) in PyTorch ATen. It lists the various input tensor shapes (all using f16 data type) encountered during profiling and their respective call counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_20\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.silu.default\ncnt: 1, ((T([128, 128, 56, 56], f16),), {})\ncnt: 1, ((T([128, 64, 56, 56], f16),), {})\ncnt: 1, ((T([128, 256, 56, 56], f16),), {})\ncnt: 2, ((T([128, 128, 28, 28], f16),), {})\ncnt: 2, ((T([128, 512, 28, 28], f16),), {})\ncnt: 6, ((T([128, 384, 14, 14], f16),), {})\ncnt: 6, ((T([128, 1536, 14, 14], f16),), {})\ncnt: 3, ((T([128, 384, 7, 7], f16),), {})\ncnt: 2, ((T([128, 1536, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Debug Info\nDESCRIPTION: Sets up custom debug information for specified source files. Configures compile flags and linker flags to include debug information for selected targets.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_48\n\nLANGUAGE: cmake\nCODE:\n```\nif(DEFINED USE_CUSTOM_DEBINFO)\n  string(REPLACE \";\" \" \" SOURCE_FILES \"${USE_CUSTOM_DEBINFO}\")\n  message(STATUS \"Source files with custom debug infos: ${SOURCE_FILES}\")\n\n  string(REGEX REPLACE \" +\" \";\" SOURCE_FILES_LIST \"${SOURCE_FILES}\")\n\n  foreach(SOURCE_FILE ${SOURCE_FILES_LIST})\n    if(BUILD_LIBTORCHLESS)\n      caffe2_update_option(USE_CUDA OFF)\n      set(ALL_PT_TARGETS \"torch_python;${C10_LIB};${TORCH_CPU_LIB};${TORCH_LIB}\")\n    else()\n      set(ALL_PT_TARGETS \"torch_python;c10;torch_cpu;torch\")\n    endif()\n    set_source_files_properties(\n      ${SOURCE_FILE} DIRECTORY \"caffe2/\" TARGET_DIRECTORY ${ALL_PT_TARGETS}\n      PROPERTIES COMPILE_FLAGS \"-g\")\n  endforeach()\n\n  set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -g\")\n  set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -g\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Log Softmax Operations in PyTorch\nDESCRIPTION: This snippet captures the use of the ATen log softmax operator on a tensor with specified dimensions and data type. The operation is applied with a dimension and a flag indicating certain preferences. The snippet focuses on a function's application rather than its complete definition or algorithmic details.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([2048, 30522], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Downloading and Generating Existing GPU Heuristics\nDESCRIPTION: Basic commands to download A100/H100 datasets and generate their respective heuristics\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mm/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbash get_mm_dataset.sh # Downloads A100 and H100 datasets\nbash gen_heuristic_a100.sh # Generates A100 heuristic\nbash gen_heuristic_h100.sh # Generates H100 heuristic\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors Filled with Ones using aten.new_ones - Python\nDESCRIPTION: Demonstrates usage of the aten.new_ones.default operator to initialize tensors filled with ones, specifying the required shape, dtype, device, and memory layout. Handles multi-dimensional tensors and adapts to CUDA settings. Dependencies: PyTorch, acceptance of 'f16' and 'f32' types, and device configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.new_ones.default\ncnt: 24, ((T([2, 1, 1, 1024], f16), [2, 1, 1, 192]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\ncnt: 24, ((T([2, 12, 14, 64, 192], f32), [2, 12, 64, 256]), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Tensor Addition Operations\nDESCRIPTION: This snippet shows the usage patterns of the aten.add.Tensor operator, including tensor shapes, data types, and call frequencies. It demonstrates various tensor addition scenarios with different shapes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 27, ((T([3, 256, 128, 128], f16), T([3, 256, 128, 128], f16)), {})\ncnt: 1, ((T([], f16), 0), {})\ncnt: 1, ((T([], f16), T([], f16)), {})\ncnt: 1, ((T([3, 256, 128, 128], f16, stride=(7340032, 16384, 128, 1)), T([3, 256, 128, 128], f16, stride=(8388608, 16384, 128, 1))), {})\ncnt: 2, ((T([3, 256, 128, 128], f16), T([3, 256, 128, 128], f16, stride=(8388608, 16384, 128, 1))), {})\ncnt: 1, ((T([3, 256, 128, 128], f16, stride=(8388608, 16384, 128, 1)), T([3, 256, 128, 128], f16, stride=(8388608, 16384, 128, 1))), {})\ncnt: 1, ((T([3, 128, 256, 256], f16, stride=(16777216, 65536, 256, 1)), T([3, 128, 256, 256], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Executing Tensor Addition Using ATen\nDESCRIPTION: Tensor addition operations are carried out using the aten.add.Tensor operator, requiring PyTorch. This operator performs element-wise or scalar addition on tensors, resulting in a tensor with the modified dimensionality or values based on input operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 2, ((T([8, 128], i64, stride=(0, 1)), 2), {})\ncnt: 97, ((T([8, 128, 768], f16), T([8, 128, 768], f16)), {})\ncnt: 1, ((T([128], i64), 1), {})\ncnt: 6, ((T([8, 12, 128, 128], f16), T([8, 1, 128, 128], f16)), {})\ncnt: 1, ((T([8, 128, 50005], f16), T([1, 50005], f16)), {})\ncnt: 2, ((T([50005, 768], f16), T([50005, 768], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Up Shared Memory Library for PyTorch\nDESCRIPTION: Configures the shared memory subsystem for PyTorch with platform-specific considerations. Uses a different implementation for Windows vs other platforms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n    set(LIBSHM_SUBDIR libshm_windows)\nelse()\n    set(LIBSHM_SUBDIR libshm)\nendif()\n\nset(LIBSHM_SRCDIR ${TORCH_SRC_DIR}/lib/${LIBSHM_SUBDIR})\nadd_subdirectory(${LIBSHM_SRCDIR})\n```\n\n----------------------------------------\n\nTITLE: Creating Sparse BSC Tensor in PyTorch\nDESCRIPTION: Shows how to create a Block Sparse Column (BSC) tensor using ccol_indices, row_indices, and values tensors with specified data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nccol_indices = torch.tensor([0, 2, 4])\nrow_indices = torch.tensor([0, 1, 0, 1])\nvalues = torch.tensor([[[0, 1, 2], [6, 7, 8]],\n                      [[3, 4, 5], [9, 10, 11]],\n                      [[12, 13, 14], [18, 19, 20]],\n                      [[15, 16, 17], [21, 22, 23]]])\nbsc = torch.sparse_bsc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)\n```\n\n----------------------------------------\n\nTITLE: Enumerating PyTorch Tensor Patterns for BatchNorm Operators - Python\nDESCRIPTION: These code snippets enumerate argument combinations for PyTorch batch normalization or similar operators using half- and single-precision FloatTensors. Each tuple specifies shapes and types of spatial tensors, bias, running_mean/var, and flags suitable for module testing. Key parameters include lists of tensor shapes and dtype annotations, batch size, and normalization axes. Outputs are primarily for test case or coverage matrix preparation. Depend on PyTorch's tensor construction and operator APIs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resnest101e_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 25, ((T([32, 1024, 16, 16], f16), T([32, 1024, 16, 16], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\ncnt: 23, ((T([32, 512, 16, 16], f16), T([32, 512, 16, 16], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 23, ((T([32, 128, 1, 1], f16), T([32, 128, 1, 1], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 22, ((T([32, 256, 16, 16], f16), T([32, 256, 16, 16], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([32, 512, 32, 32], f16), T([32, 512, 32, 32], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([32, 256, 32, 32], f16), T([32, 256, 32, 32], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 128, 32, 32], f16), T([32, 128, 32, 32], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([32, 256, 64, 64], f16), T([32, 256, 64, 64], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([32, 128, 64, 64], f16), T([32, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 64, 64, 64], f16), T([32, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 128, 128, 128], f16), T([32, 128, 128, 128], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([32, 64, 128, 128], f16), T([32, 64, 128, 128], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Lightweight Dispatch Option for PyTorch\nDESCRIPTION: Creates a CMake option for enabling codegen unboxing for ATen ops, which requires static dispatch to work properly. Adds a check to ensure static dispatch is enabled when lightweight dispatch is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\noption(\n  USE_LIGHTWEIGHT_DISPATCH\n  \"Enable codegen unboxing for ATen ops, need to work with static dispatch in order to work properly.\"\n  OFF)\nif(USE_LIGHTWEIGHT_DISPATCH AND NOT STATIC_DISPATCH_BACKEND)\n  message(\n    FATAL_ERROR\n      \"Need to enable static dispatch after enabling USE_LIGHTWEIGHT_DISPATCH.\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This code snippet represents a breakdown of PyTorch operator usage in a neural network model. It includes operator names, counts, and tensor shapes for various operations such as convolutions, activations, and other common deep learning operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv2_100_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 52, ((T([], i64), 1), {})\ncnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})\ncnt: 4, ((T([128, 32, 28, 28], f16), T([128, 32, 28, 28], f16)), {})\ncnt: 6, ((T([128, 64, 14, 14], f16), T([128, 64, 14, 14], f16)), {})\ncnt: 4, ((T([128, 96, 14, 14], f16), T([128, 96, 14, 14], f16)), {})\ncnt: 4, ((T([128, 160, 7, 7], f16), T([128, 160, 7, 7], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\ncnt: 2, ((T([128, 32, 112, 112], f16),), {})\ncnt: 1, ((T([128, 96, 112, 112], f16),), {})\ncnt: 1, ((T([128, 96, 56, 56], f16),), {})\ncnt: 3, ((T([128, 144, 56, 56], f16),), {})\ncnt: 1, ((T([128, 144, 28, 28], f16),), {})\ncnt: 5, ((T([128, 192, 28, 28], f16),), {})\ncnt: 1, ((T([128, 192, 14, 14], f16),), {})\ncnt: 8, ((T([128, 384, 14, 14], f16),), {})\ncnt: 5, ((T([128, 576, 14, 14], f16),), {})\ncnt: 1, ((T([128, 576, 7, 7], f16),), {})\ncnt: 6, ((T([128, 960, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1280, 7, 7], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([16, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([96, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 112, 112], f16), T([96, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 96), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([24, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 24, 56, 56], f16), T([144, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([144, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 144), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([24, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([144, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 144), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([32, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 32, 28, 28], f16), T([192, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 192), {})\ncnt: 2, ((T([128, 192, 28, 28], f16), T([32, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 192), {})\ncnt: 1, ((T([128, 192, 14, 14], f16), T([64, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 64, 14, 14], f16), T([384, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 384, 14, 14], f16), T([384, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 384), {})\ncnt: 3, ((T([128, 384, 14, 14], f16), T([64, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 384, 14, 14], f16), T([96, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 96, 14, 14], f16), T([576, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 576, 14, 14], f16), T([576, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 576), {})\ncnt: 2, ((T([128, 576, 14, 14], f16), T([96, 576, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 576, 14, 14], f16), T([576, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 576), {})\ncnt: 1, ((T([128, 576, 7, 7], f16), T([160, 576, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 160, 7, 7], f16), T([960, 160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 960, 7, 7], f16), T([960, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 960), {})\ncnt: 2, ((T([128, 960, 7, 7], f16), T([160, 960, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 960, 7, 7], f16), T([320, 960, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 320, 7, 7], f16), T([128, 960, 7, 7], f16), T([320, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 960, 7, 7], f16), T([128, 960, 7, 7], f16), T([960, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 960, [True, True, False]), {})\ncnt: 3, ((T([128, 960, 7, 7], f16), T([128, 160, 7, 7], f16), T([960, 160, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 160, 7, 7], f16), T([128, 960, 7, 7], f16), T([160, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 160, 7, 7], f16), T([128, 576, 7, 7], f16), T([160, 576, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 576, 7, 7], f16), T([128, 576, 14, 14], f16), T([576, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 576, [True, True, False]), {})\ncnt: 3, ((T([128, 576, 14, 14], f16), T([128, 96, 14, 14], f16), T([576, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 96, 14, 14], f16), T([128, 576, 14, 14], f16), T([96, 576, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 576, 14, 14], f16), T([128, 576, 14, 14], f16), T([576, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 576, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 14, 14], f16), T([128, 384, 14, 14], f16), T([96, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 384, [True, True, False]), {})\ncnt: 4, ((T([128, 384, 14, 14], f16), T([128, 64, 14, 14], f16), T([384, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 64, 14, 14], f16), T([128, 384, 14, 14], f16), T([64, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 14, 14], f16), T([128, 192, 14, 14], f16), T([64, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 14, 14], f16), T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 192, [True, True, False]), {})\ncnt: 3, ((T([128, 192, 28, 28], f16), T([128, 32, 28, 28], f16), T([192, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 32, 28, 28], f16), T([128, 192, 28, 28], f16), T([32, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 192, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 28, 28], f16), T([128, 144, 28, 28], f16), T([32, 144, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 56, 56], f16), T([144, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 144, [True, True, False]), {})\ncnt: 2, ((T([128, 144, 56, 56], f16), T([128, 24, 56, 56], f16), T([144, 24, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 24, 56, 56], f16), T([128, 144, 56, 56], f16), T([24, 144, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([128, 144, 56, 56], f16), T([144, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 144, [True, True, False]), {})\ncnt: 1, ((T([128, 24, 56, 56], f16), T([128, 96, 56, 56], f16), T([24, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([128, 96, 112, 112], f16), T([96, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 96, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 112, 112], f16), T([128, 16, 112, 112], f16), T([96, 16, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 32, 112, 112], f16), T([16, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 32, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 1280, 7, 7], f16, stride=(1280, 1, 0, 0)), 49), {})\nOperator: aten.hardtanh_.default\n```\n\n----------------------------------------\n\nTITLE: Implementing Unsafe View in PyTorch\nDESCRIPTION: Facilitates reshaping tensors without copying data, which may lead to unsafe memory access due to unfixed strides. Its use cases are mainly in scenarios where reshaping operations must be fast, with the side effect of possible data corruption. It requires PyTorch, input tensors, and desired shape dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\naten._unsafe_view.default(T([64, 128, 4, 64], f16), [64, 128, 256])\n```\n\nLANGUAGE: Python\nCODE:\n```\naten._unsafe_view.default(T([8192, 10000], f16), [64, 128, 10000])\n```\n\nLANGUAGE: Python\nCODE:\n```\naten._unsafe_view.default(T([64, 4, 128, 64], f16), [256, 128, 64])\n```\n\nLANGUAGE: Python\nCODE:\n```\naten._unsafe_view.default(T([64, 128, 256], f16), [8192, 256])\n```\n\n----------------------------------------\n\nTITLE: Batching Operations on DataFrames\nDESCRIPTION: Implementation of batching operations on DataFrames and comparison with regular DataPipe batching.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndp = get_dataframes_pipe(dataframe_size = 3)\ndp = dp.shuffle()\ndp = dp.batch(2)\nprint(\"Iterate over DataFrame batches\")\nfor i,v in enumerate(dp):\n    print(v)\n```\n\n----------------------------------------\n\nTITLE: Defining Conditional Memory Efficient Attention Build Option in CMake\nDESCRIPTION: Defines a CMake option USE_MEM_EFF_ATTENTION using `cmake_dependent_option`. This option enables the memory-efficient attention implementation. It defaults to ON if USE_CUDA or USE_ROCM is enabled, otherwise it defaults to OFF.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_32\n\nLANGUAGE: cmake\nCODE:\n```\n# CAVEAT: Again, Flash Attention2 will error while building for sm52 while Mem\n# Eff Attention won't\ncmake_dependent_option(\n  USE_MEM_EFF_ATTENTION\n  \"Enable memory-efficient attention for scaled dot product attention.\\\n  Will be disabled if not supported by the platform\" ON\n  \"USE_CUDA OR USE_ROCM\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Waiting for Asynchronous Tasks with torch.jit.wait() in TorchScript\nDESCRIPTION: Forces completion of a torch.jit.Future[T] asynchronous task, returning the result of the task.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ntorch.jit.wait()\n```\n\n----------------------------------------\n\nTITLE: Syntactic Sugar for Catch-All Lambda Kernel Registration (C++)\nDESCRIPTION: Shows a simplified syntax for registering a stateless lambda as a catch-all kernel. The lambda is passed directly as the second argument to `.op()`, implicitly registering it as a catch-all kernel without needing `.options().catchAllKernel()`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\nstatic auto registry = torch::RegisterOperators()\n .op(\"my_namespace::my_op\", [] (Tensor a, Tensor b) {...});\n```\n\n----------------------------------------\n\nTITLE: C++ Tensor Operation Reference\nDESCRIPTION: Example showing basic tensor operations available in the C++ API, such as add, reshape, and clone using torch::Tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cpp_index.rst#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ntorch::Tensor::add\ntorch::Tensor::reshape\ntorch::Tensor::clone\n```\n\n----------------------------------------\n\nTITLE: Running Sparse Matrix-Vector Multiplication Benchmark\nDESCRIPTION: Command line usage for running SPMV benchmarks using matmul_bench.py.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/sparse/dlmc/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmatmul_bench.py --operation sparse@vector\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication in PyTorch\nDESCRIPTION: This snippet shows matrix multiplication operations, likely used in fully connected layers of the neural network. The operations involve 2D tensors with specific shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hrnet_w18_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16), T([1000, 2048], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Inferred Size and Storage Assignment in PyTorch - C++\nDESCRIPTION: Showcases how to infer a tensor's size and set its storage with proper reference decrementation. Key functions include `THByteStorage_newInferSize` for size calculation and `THTensor_setStorage` for storage manipulation. Ensures reference counts are managed using `c10::raw::intrusive_ptr::decref`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/README.md#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n  THByteStorage *inferred_size = THByteStorage_newInferSize(size, numel);\n  THTensor_(setStorage)(self, tensor->storage, tensor->storageOffset, inferred_size, NULL);\n  c10::raw::intrusive_ptr::decref(inferred_size);\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA Tensor Transposition in PyTorch\nDESCRIPTION: This function implements tensor transposition for CUDA devices in PyTorch. It takes a tensor and two dimension indices to swap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4_ops13transpose_int4callERKNS_6TensorEll\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen GELU Operations in PyTorch\nDESCRIPTION: Tracks the invocation of \\\"aten.gelu.default\\\", which applies the Gaussian Error Linear Unit activation function to input tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.gelu.default\ncnt: 12, ((T([8, 128, 4096], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Copy Operations in PyTorch\nDESCRIPTION: Logs the use of \\\"aten.copy_.default\\\" operator to perform element-wise copying between tensors of potentially different shapes or layouts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 2, ((T([8, 128], i64), T([8, 128], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Count for aten.add.Tensor Operations\nDESCRIPTION: Lists various tensor addition operations with different shapes, showing element-wise addition between tensors of matching dimensions, primarily using half precision (f16) tensors across different layers of the network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 87, ((T([], i64), 1), {})\ncnt: 4, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16)), {})\ncnt: 6, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})\ncnt: 8, ((T([128, 40, 28, 28], f16), T([128, 40, 28, 28], f16)), {})\ncnt: 8, ((T([128, 72, 14, 14], f16), T([128, 72, 14, 14], f16)), {})\ncnt: 10, ((T([128, 120, 14, 14], f16), T([128, 120, 14, 14], f16)), {})\ncnt: 10, ((T([128, 184, 7, 7], f16), T([128, 184, 7, 7], f16)), {})\ncnt: 1, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16)), {})\ncnt: 5, ((T([128, 736, 7, 7], f16), T([128, 736, 7, 7], f16)), {})\ncnt: 1, ((T([128, 720, 7, 7], f16), T([128, 720, 7, 7], f16)), {})\ncnt: 6, ((T([128, 360, 14, 14], f16), T([128, 360, 14, 14], f16)), {})\ncnt: 5, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This code snippet demonstrates the usage of various PyTorch operators in a neural network model. It includes operations like log_softmax, tensor addition, matrix multiplication, concatenation, cloning, padding, and convolution. The snippet shows the frequency of each operation and the tensor shapes involved.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 58, ((T([], i64), 1), {})\ncnt: 2, ((T([64, 32, 112, 112], f16), T([64, 32, 112, 112], f16)), {})\n# ... (truncated for brevity)\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 225, 225], f16), T([32, 3, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Negative Log-Likelihood Loss Backward Pass in PyTorch\nDESCRIPTION: The aten.nll_loss_backward.default example captures the backpropagation pass for negative log-likelihood loss operations on models, handling float16 tensor inputs and integer targets to compute gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 2, ((T([], f16), T([1, 512], f16), T([1], i64), None, 1, 512, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization with native_layer_norm in PyTorch (Python)\nDESCRIPTION: Performs aten.native_layer_norm for normalizing activations across feature dimensions, a common normalization method in neural networks to stabilize learning and improve convergence.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\naten.native_layer_norm.default\ncnt: 25, ((T([16, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-12), {})\n```\n\n----------------------------------------\n\nTITLE: Executing aten.convolution operation in PyTorch\nDESCRIPTION: Performs convolution on input tensors using the Aten backend, with specific kernel size, padding, and strides, returning convolved tensors. Ensure input tensors conform to specified shapes. PyTorch should be utilized for tensor manipulations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), T([16], f16), [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Input Tensor Cloning in DenseNet\nDESCRIPTION: This snippet shows the tensor cloning operation used for the input images in DenseNet. It creates a copy of the input tensor with shape [32, 3, 224, 224] and half-precision floating point (f16) data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([32, 3, 224, 224], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling ATen Matrix Multiplication and Aggregation Operators in PyTorch (Python)\nDESCRIPTION: Details usage of core ATen matrix multiplication (mm) and aggregation (sum, mean) operations as recorded during the model run. Dependencies: PyTorch, tensor inputs with compatible shapes/dtypes. Expects 2D or 1D tensors for multiplication/sum/mean; returns aggregated/multiplied tensors. Observes parameterization for summing/mean over dimensions and optional keepdim/keep parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([16, 1000], f16, stride=(0, 0)), T([1000, 512], f16)), {})\ncnt: 1, ((T([1000, 16], f16, stride=(0, 0)), T([16, 512], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([16, 1000], f16, stride=(0, 0)), [0], True), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.default\ncnt: 1, ((T([16, 1000], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([16, 512, 7, 7], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries to the JNI Target in CMake\nDESCRIPTION: Uses the `target_link_libraries` command to link the JNI target (`${PYTORCH_JNI_TARGET}`) against all the libraries collected in the `pytorch_jni_LIBS` variable. This list was populated differently depending on whether it's an Android or host build and which optional features (like Vulkan, NNPACK, XNNPACK) are enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${PYTORCH_JNI_TARGET} ${pytorch_jni_LIBS})\n```\n\n----------------------------------------\n\nTITLE: Max Pooling Operations in PyTorch with Half-Precision Tensors\nDESCRIPTION: Max pooling operations with different input shapes and kernel sizes. Operations use half-precision (f16) tensors and include stride settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 64, 147, 147], f16), [3, 3], [2, 2]), {})\n```\n\n----------------------------------------\n\nTITLE: Using ATen Lift Fresh Copy Operator in PyTorch\nDESCRIPTION: The lift_fresh_copy operator is utilized to create a fresh copy of a tensor. It is often used when a new contiguous tensor is required. The code snippet indicates a usage pattern that ensures memory layout and alignment for efficient computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([32, 256, 56, 56], f16, stride=(256, 1, 0, 0)), 3136), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Analysis\nDESCRIPTION: Detailed analysis of PyTorch operator usage including _to_copy, abs, add.Tensor, avg_pool2d, convolution, and other core operations. Shows input tensor shapes, data types, and operation counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Super_SloMo_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Sample operator patterns:\n\n# Copy operation\naten._to_copy.default(T([6, 352, 352], i64, stride=(0, 352, 1)), dtype=f16)\n\n# Absolute value\naten.abs.default(T([6, 3, 352, 352], f16))\n\n# Tensor addition\naten.add.Tensor(T([6, 2, 352, 352], f16), T([6, 2, 352, 352], f16))\n\n# Average pooling\naten.avg_pool2d.default(T([6, 32, 352, 352], f16), [2, 2])\n\n# Convolution\naten.convolution.default(T([6, 6, 352, 352], f16), T([32, 6, 7, 7], f16), T([32], f16), \n                        [1, 1], [3, 3], [1, 1], False, [0, 0], 1)\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.nll_loss_forward.default in PyTorch ATen\nDESCRIPTION: Logs calls to the `aten.nll_loss_forward.default` operator, calculating the negative log-likelihood loss. Arguments include the model's output predictions (log probabilities, `[128, 1000]`, f16), target labels (`[128]`, i64), an optional weight tensor (`None`), the reduction mode (`1` indicating mean), and the index to ignore (`-100`). The `cnt` indicates call frequency.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Operations - Addition and Multiplication\nDESCRIPTION: Matrix addition and multiplication operations with various tensor shapes and stride configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Bert_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten.add.Tensor((T([4, 512, 768], f16), T([4, 512, 768], f16)), {})\naten.addmm.default((T([768], f16), T([2048, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\naten.bmm.default((T([48, 512, 64], f16), T([48, 64, 512], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Cloning Tensors with clone in PyTorch (Python)\nDESCRIPTION: Executes aten.clone, which creates a copy of a tensor. Cloning is particularly relevant when modifications to the tensor are needed without affecting the original tensor dataa regular requirement in neural network training where weight updates occur.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\naten.clone.default\ncnt: 1, ((T([16, 512], i64),), {})\ncnt: 1, ((T([16], i64),), {})\n```\n\n----------------------------------------\n\nTITLE: Summarizing ATen Operator Usage - PyTorch - Python\nDESCRIPTION: This snippet catalogs calls to different low-level ATen operators used by PyTorch models, describing for each operator the argument structure and how often that signature appears during traced execution. It is assumed to be Python-generated inventory, possibly output of a tracing/profiling tool. Operators involve tensor algebra, data conversion, and neural network primitives, each listed with shapes, types, and optional argument dictionaries, but no code for execution. The list is for diagnostic/reporting purposes and does not execute transformations or model code directly.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForQuestionAnswering_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 2, ((T([64, 128], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 2, ((T([64, 128], f16), T([64, 128], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 12, ((T([64, 12, 128, 128], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([64, 12, 128, 128], f16), T([64, 12, 128, 128], f16), -1, f16), {})\nOperator: aten._to_copy.default\ncnt: 1, ((T([64, 1, 1, 128], f32),), {'dtype': f16})\ncnt: 1, ((T([64, 128], b8),), {'dtype': i32})\ncnt: 1, ((T([64, 128], i64),), {'dtype': i32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 1, ((T([64, 128], i32),), {'dtype': i64})\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([64, 12, 128, 64], f16), [768, 128, 64]), {})\ncnt: 12, ((T([64, 12, 64, 128], f16), [768, 64, 128]), {})\ncnt: 12, ((T([768, 128, 128], f16), [64, 12, 128, 128]), {})\ncnt: 12, ((T([768, 128, 64], f16), [64, 12, 128, 64]), {})\ncnt: 24, ((T([64, 128, 12, 64], f16), [64, 128, 768]), {})\ncnt: 12, ((T([64, 128, 768], f16), [8192, 768]), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([64, 128], i32), 0), {})\ncnt: 1, ((T([64, 128], i64), 0), {})\ncnt: 73, ((T([64, 128, 768], f16), T([64, 128, 768], f16)), {})\ncnt: 12, ((T([64, 12, 128, 128], f16), T([64, 1, 1, 128], f16)), {})\ncnt: 1, ((T([], f16), T([], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 1, ((T([64, 128, 768], f16), T([64, 128, 768], f16)), {})\nOperator: aten.addmm.default\ncnt: 48, ((T([768], f16), T([8192, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 12, ((T([3072], f16), T([8192, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 12, ((T([768], f16), T([8192, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 1, ((T([2], f16), T([8192, 768], f16), T([768, 2], f16, stride=(1, 768))), {})\nOperator: aten.bmm.default\ncnt: 12, ((T([768, 128, 64], f16), T([768, 64, 128], f16)), {})\ncnt: 12, ((T([768, 128, 128], f16), T([768, 128, 64], f16)), {})\ncnt: 12, ((T([768, 128, 128], f16, stride=(16384, 1, 128)), T([768, 128, 64], f16)), {})\ncnt: 12, ((T([768, 128, 64], f16), T([768, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 12, ((T([768, 64, 128], f16, stride=(8192, 1, 64)), T([768, 128, 128], f16)), {})\ncnt: 12, ((T([768, 128, 128], f16), T([768, 128, 64], f16, stride=(8192, 1, 128))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([64, 128, 1], f16), T([64, 128, 1], f16)], 2), {})\nOperator: aten.clamp.default\ncnt: 2, ((T([64], i64), 0, 128), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 128], i64),), {})\ncnt: 2, ((T([64], i64),), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 128], i64), T([64, 128], i64)), {})\ncnt: 2, ((T([64], i64), T([64], i64)), {})\nOperator: aten.cumsum.default\ncnt: 1, ((T([64, 128], i32), 1), {})\nOperator: aten.div.Tensor\ncnt: 24, ((T([64, 12, 128, 128], f16), 8.0), {})\ncnt: 2, ((T([], f16), 2), {})\nOperator: aten.embedding.default\ncnt: 1, ((T([30522, 768], f16), T([64, 128], i64), 0), {})\ncnt: 1, ((T([2, 768], f16), T([64, 128], i64, stride=(0, 1))), {})\ncnt: 1, ((T([512, 768], f16), T([64, 128], i64), 0), {})\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([64, 128, 768], f16), T([64, 128], i64), 512, 0, False), {})\ncnt: 1, ((T([64, 128, 768], f16), T([64, 128], i64, stride=(0, 1)), 2, -1, False), {})\ncnt: 1, ((T([64, 128, 768], f16), T([64, 128], i64), 30522, 0, False), {})\nOperator: aten.gelu.default\ncnt: 12, ((T([64, 128, 3072], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([64, 128, 3072], f16), T([64, 128, 3072], f16)), {})\nOperator: aten.mm.default\ncnt: 1, ((T([8192, 2], f16), T([2, 768], f16)), {})\ncnt: 1, ((T([2, 8192], f16, stride=(1, 2)), T([8192, 768], f16)), {})\ncnt: 12, ((T([8192, 768], f16), T([768, 3072], f16)), {})\ncnt: 12, ((T([768, 8192], f16, stride=(1, 768)), T([8192, 3072], f16)), {})\ncnt: 12, ((T([8192, 3072], f16), T([3072, 768], f16)), {})\ncnt: 12, ((T([3072, 8192], f16, stride=(1, 3072)), T([8192, 768], f16)), {})\ncnt: 48, ((T([8192, 768], f16), T([768, 768], f16)), {})\ncnt: 48, ((T([768, 8192], f16, stride=(1, 768)), T([8192, 768], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 1, ((T([64, 1, 1, 128], f16), -65504.0), {})\ncnt: 1, ((T([64, 128], i32), T([64, 128], i32)), {})\nOperator: aten.native_layer_norm.default\ncnt: 25, ((T([64, 128, 768], f16), [768], T([768], f16), T([768], f16), 1e-12), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 25, ((T([64, 128, 768], f16), T([64, 128, 768], f16), [768], T([64, 128, 1], f32), T([64, 128, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\nOperator: aten.ne.Scalar\ncnt: 1, ((T([64, 128], i64), 0), {})\nOperator: aten.nll_loss_backward.default\ncnt: 2, ((T([], f16), T([64, 128], f16), T([64], i64), None, 1, 128, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 2, ((T([64, 128], f16), T([64], i64), None, 1, 128), {})\nOperator: aten.rsub.Scalar\ncnt: 1, ((T([64, 1, 1, 128], f16), 1.0), {})\nOperator: aten.split.Tensor\ncnt: 1, ((T([64, 128, 2], f16), 1, -1), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([8192, 2], f16), [0], True), {})\ncnt: 60, ((T([8192, 768], f16), [0], True), {})\ncnt: 12, ((T([8192, 3072], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Conditionally Linking test_jit against System ONNX in CMake\nDESCRIPTION: Checks if the `USE_SYSTEM_ONNX` flag is enabled. If true, it links the `test_jit` executable against the system-provided ONNX libraries (`onnx_proto` and `onnx`). This is necessary for tests involving ONNX export or import functionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_SYSTEM_ONNX)\n  target_link_libraries(test_jit PRIVATE onnx_proto onnx)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Importing the NumPy Compatibility Layer for Debugging\nDESCRIPTION: Example of how to use the compatibility layer in eager mode by replacing the standard NumPy import with torch._numpy for debugging purposes. This allows determining if issues are caused by dynamo or the compatibility layer itself.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/_numpy/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch._numpy as np\n```\n\n----------------------------------------\n\nTITLE: FX Graph Node Format for Operator Calls in Export IR (Textual Representation)\nDESCRIPTION: This snippet presents an example of the formatted string output for a call_function node in FX/Export IR, showing node naming, operation typing, ATen operator target, and argument referencing other nodes. It is used for graph text export or debugging in PyTorch-based workflows, not for direct execution. Dependencies include torch.export and torch.fx graph creation, and the node labeling and attributes must be understood in the context of torch.fx node conventions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%add1 = call_function[target = torch.op.aten.add.Tensor](args = (%x, %y), kwargs = {})\n```\n\n----------------------------------------\n\nTITLE: NLL Loss Calculations in PyTorch\nDESCRIPTION: Forward and backward negative log likelihood loss computations with ignore_index=-100, operating on classification outputs in half-precision format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\naten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in ResNet-like Model\nDESCRIPTION: This code snippet represents a summary of PyTorch operator usage in a deep learning model, likely a ResNet variant. It includes convolution, batch normalization, pooling, and activation operations with their respective tensor shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16), T([32, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([32, 2048, 7, 7], f16, stride=(2048, 1, 0, 0)), 49), {})\ncnt: 1, ((T([32, 512, 14, 14], f16, stride=(512, 1, 0, 0)), 196), {})\ncnt: 1, ((T([32, 256, 28, 28], f16, stride=(256, 1, 0, 0)), 784), {})\ncnt: 1, ((T([32, 128, 56, 56], f16, stride=(128, 1, 0, 0)), 3136), {})\ncnt: 1, ((T([32, 64, 56, 56], f16, stride=(64, 1, 0, 0)), 3136), {})\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 32000), {})\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([32, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([32, 64, 56, 56], i64)), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([32, 64, 56, 56], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 128, 56, 56], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 2048, 7, 7], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), T([1000, 2048], f16)), {})\ncnt: 1, ((T([1000, 32], f16, stride=(0, 0)), T([32, 2048], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 1, ((T([32, 2, 64, 56, 56], f16), T([32, 2, 64, 1, 1], f16)), {})\ncnt: 1, ((T([32, 2, 128, 56, 56], f16), T([32, 2, 128, 1, 1], f16)), {})\ncnt: 1, ((T([32, 2, 256, 28, 28], f16), T([32, 2, 256, 1, 1], f16)), {})\ncnt: 1, ((T([32, 2, 512, 14, 14], f16), T([32, 2, 512, 1, 1], f16)), {})\ncnt: 1, ((T([32, 2, 512, 14, 14], f16, stride=(100352, 0, 196, 14, 1)), T([32, 2, 512, 14, 14], f16)), {})\ncnt: 1, ((T([32, 2, 512, 14, 14], f16, stride=(100352, 0, 196, 14, 1)), T([32, 2, 512, 1, 1], f16)), {})\ncnt: 1, ((T([32, 2, 256, 28, 28], f16, stride=(200704, 0, 784, 28, 1)), T([32, 2, 256, 28, 28], f16)), {})\ncnt: 1, ((T([32, 2, 256, 28, 28], f16, stride=(200704, 0, 784, 28, 1)), T([32, 2, 256, 1, 1], f16)), {})\ncnt: 1, ((T([32, 2, 128, 56, 56], f16, stride=(401408, 0, 3136, 56, 1)), T([32, 2, 128, 56, 56], f16)), {})\ncnt: 1, ((T([32, 2, 128, 56, 56], f16, stride=(401408, 0, 3136, 56, 1)), T([32, 2, 128, 1, 1], f16)), {})\ncnt: 1, ((T([32, 2, 64, 56, 56], f16, stride=(200704, 0, 3136, 56, 1)), T([32, 2, 64, 56, 56], f16)), {})\ncnt: 1, ((T([32, 2, 64, 56, 56], f16, stride=(200704, 0, 3136, 56, 1)), T([32, 2, 64, 1, 1], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 2, ((T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 32, 1, 1], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 64, 1, 1], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 128, 1, 1], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})\ncnt: 3, ((T([32, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 256, 1, 1], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})\ncnt: 2, ((T([32, 2048, 7, 7], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f16), False, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 2, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 256, 1, 1], f16), T([32, 256, 1, 1], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), T([32, 512, 14, 14], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 128, 1, 1], f16), T([32, 128, 1, 1], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), T([32, 256, 28, 28], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 1e-05, [True, True, True]), {})\nOperator: aten.relu_.default\ncnt: 2, ((T([32, 32, 112, 112], f16),), {})\ncnt: 1, ((T([32, 64, 112, 112], f16),), {})\ncnt: 1, ((T([32, 64, 56, 56], f16),), {})\ncnt: 2, ((T([32, 128, 56, 56], f16),), {})\ncnt: 1, ((T([32, 32, 1, 1], f16),), {})\ncnt: 2, ((T([32, 256, 56, 56], f16),), {})\ncnt: 1, ((T([32, 64, 1, 1], f16),), {})\ncnt: 2, ((T([32, 512, 28, 28], f16),), {})\ncnt: 1, ((T([32, 256, 28, 28], f16),), {})\ncnt: 1, ((T([32, 128, 1, 1], f16),), {})\ncnt: 2, ((T([32, 1024, 14, 14], f16),), {})\ncnt: 1, ((T([32, 512, 14, 14], f16),), {})\ncnt: 1, ((T([32, 256, 1, 1], f16),), {})\ncnt: 1, ((T([32, 2048, 7, 7], f16),), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})\ncnt: 1, ((T([32, 2, 512, 14, 14], f16), [3, 4], True), {})\ncnt: 1, ((T([32, 2, 256, 28, 28], f16), [3, 4], True), {})\ncnt: 1, ((T([32, 2, 128, 56, 56], f16), [3, 4], True), {})\ncnt: 1, ((T([32, 2, 64, 56, 56], f16), [3, 4], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([32, 1000], f16),), {})\nOperator: aten.sum.dim_IntList\ncnt: 2, ((T([32, 2, 64, 56, 56], f16), [1]), {})\ncnt: 2, ((T([32, 2, 128, 56, 56], f16), [1]), {})\ncnt: 2, ((T([32, 2, 256, 28, 28], f16), [1]), {})\ncnt: 2, ((T([32, 2, 512, 14, 14], f16), [1]), {})\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), 0), {})\ncnt: 1, ((T([32, 256, 1, 1], f16), T([32, 256, 1, 1], f16), 0), {})\ncnt: 2, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16), 0), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), T([32, 512, 14, 14], f16), 0), {})\ncnt: 1, ((T([32, 128, 1, 1], f16), T([32, 128, 1, 1], f16), 0), {})\ncnt: 2, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16), 0), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), T([32, 256, 28, 28], f16), 0), {})\ncnt: 1, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), 0), {})\ncnt: 2, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16), 0), {})\ncnt: 2, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), 0), {})\ncnt: 1, ((T([32, 32, 1, 1], f16), T([32, 32, 1, 1], f16), 0), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16), 0), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), 0), {})\ncnt: 2, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations\nDESCRIPTION: Batch normalization operations on tensors with various channel dimensions (32 to 448), using momentum 0.1 and epsilon 0.001\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 32, 149, 149], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 0.001), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing sqrt Kernel for CUDA in PyTorch\nDESCRIPTION: This function implements the sqrt operation for CUDA tensors in PyTorch. It uses a 2D loop strategy derived from a 1D implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_41\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c1012function_refIFvPPcPKlllEE11callback_fnIZN2at18TensorIteratorBase15loop_2d_from_1dIZZZNS8_6native7DEFAULT11sqrt_kernelERS9_ENKUlvE_clEvENKUlvE0_clEvEUlS2_S4_lE_EEDaRKT_EUlS2_S4_llE_EEvlS2_S4_ll\n```\n\n----------------------------------------\n\nTITLE: Importing and Type Checking Classes in PyTorch Package\nDESCRIPTION: Demonstrates how importing a class through the package importer creates a distinct type from the original class, causing isinstance() checks to fail between the two versions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimported_MyClass = importer.import_module(\"foo\").MyClass\n\nassert isinstance(my_class_instance, MyClass)  # works\nassert isinstance(my_class_instance, imported_MyClass)  # ERROR!\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Copy Operations in PyTorch\nDESCRIPTION: This snippet discusses the ATen copy operation, showcasing its use in transferring the contents of one tensor to another. This operation is vital for effectively managing memory and data states across computations in various tensor dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 2, ((T([16, 128], i64), T([16, 128], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Importing and Using torch.xpu Module in Python\nDESCRIPTION: This snippet demonstrates how to import and use the torch.xpu module in Python. It includes various functions for device management, stream handling, and other XPU-specific operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/xpu.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch.xpu\n\n# Device management\ntorch.xpu.current_device()\ntorch.xpu.device_count()\ntorch.xpu.set_device(0)\n\n# Stream handling\nwith torch.xpu.StreamContext():\n    # XPU operations here\n    pass\n\n# Random number generation\ntorch.xpu.manual_seed(42)\n\n# Memory management\ntorch.xpu.empty_cache()\ntorch.xpu.memory_allocated()\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows layer normalization operations and their backward passes. It includes operations on tensors with different shapes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_layer_norm.default\ncnt: 8, ((T([64, 28, 28, 192], f16, stride=(150528, 28, 1, 784)), [192], T([192], f16), T([192], f16), 1e-05), {})\ncnt: 28, ((T([64, 14, 14, 384], f16, stride=(75264, 14, 1, 196)), [384], T([384], f16), T([384], f16), 1e-05), {})\ncnt: 3, ((T([64, 197, 384], f16), [384], T([384], f16), T([384], f16), 1e-05), {})\ncnt: 2, ((T([64, 1, 384], f16), [384], T([384], f16), T([384], f16), 1e-05), {})\n\nOperator: aten.native_layer_norm_backward.default\ncnt: 3, ((T([64, 197, 384], f16), T([64, 197, 384], f16), [384], T([64, 197, 1], f32), T([64, 197, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})\ncnt: 2, ((T([64, 1, 384], f16), T([64, 1, 384], f16), [384], T([64, 1, 1], f32), T([64, 1, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})\ncnt: 28, ((T([64, 14, 14, 384], f16), T([64, 14, 14, 384], f16, stride=(75264, 14, 1, 196)), [384], T([64, 14, 14, 1], f32), T([64, 14, 14, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})\ncnt: 8, ((T([64, 28, 28, 192], f16), T([64, 28, 28, 192], f16, stride=(150528, 28, 1, 784)), [192], T([64, 28, 28, 1], f32), T([64, 28, 28, 1], f32), T([192], f16), T([192], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Softmax Operations in PyTorch\nDESCRIPTION: This snippet logs usage of the \\\"aten._softmax.default\\\" operator in the PyTorch framework, which applies softmax to the specified tensor dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 12, ((T([128, 128, 128], f16), -1, False), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations with Shapes and Counts\nDESCRIPTION: Lists various PyTorch operations including log_softmax, convolutions, and tensor additions. Each operation shows tensor shapes, data types (mostly f16), and how many times it was called. The operations are primarily working with batches of 128 and varying channel/spatial dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hrnet_w18_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Log Softmax Operations\naten._log_softmax.default((T([128, 1000], f16), 1, False))\naten._log_softmax_backward_data.default((T([128, 1000], f16), T([128, 1000], f16), 1, f16))\n\n# Tensor Addition Operations\naten.add.Tensor((T([128, 18, 56, 56], f16), T([128, 18, 56, 56], f16)))\naten.add_.Tensor((T([], i64), 1))\n\n# Matrix Multiplication\naten.addmm.default((T([1000], f16), T([128, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))))\n\n# Convolution Operations\naten.convolution.default((T([128, 3, 224, 224], f16), T([64, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1))\naten.convolution_backward.default((T([128, 2048, 7, 7], f16), T([128, 1024, 7, 7], f16), T([2048, 1024, 1, 1], f16), [2048], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]))\n```\n\n----------------------------------------\n\nTITLE: Executing Tensor Copy Operations\nDESCRIPTION: The _to_copy operator handles copying tensors to a specified device, using a particular data type. Dependencies include PyTorch with outputs being a tensor on the intended device and format. Special attention is given to tensor layout and device specifics such as CUDA.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([128, 128], f32),), {'dtype': f16})\ncnt: 1, ((T([8, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\n```\n\n----------------------------------------\n\nTITLE: Ignoring Functions in TorchScript with @torch.jit.ignore Decorator\nDESCRIPTION: Decorator that indicates to the compiler that a function or method should be ignored and left as a Python function. Allows non-TorchScript compatible code in models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_41\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.ignore\n```\n\n----------------------------------------\n\nTITLE: Non-torch Function Handling in FX\nDESCRIPTION: Demonstrates how to handle non-torch functions like built-in Python functions using the wrap API.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    return x / sqrt(len(x))\n\ntorch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n```\n\n----------------------------------------\n\nTITLE: Computing and Verifying Per-Sample Gradients using functorch\nDESCRIPTION: Computes the per-sample gradients efficiently by calling the `vmap`-transformed function `ft_compute_sample_grad` with the model's `params`, `buffers`, and the entire batches of `data` and `targets`. It then verifies the correctness of this `functorch` approach by comparing its output (`ft_per_sample_grads`) element-wise with the results from the naive loop-based method (`per_sample_grads`) using `torch.allclose` with specified tolerances (`atol`, `rtol`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nft_per_sample_grads = ft_compute_sample_grad(params, buffers, data, targets)\n\n# we can double check that the results using functorch grad and vmap match the results of hand processing each one individually:\nfor per_sample_grad, ft_per_sample_grad in zip(per_sample_grads, ft_per_sample_grads):\n    assert torch.allclose(per_sample_grad, ft_per_sample_grad, atol=3e-3, rtol=1e-5)\n```\n\n----------------------------------------\n\nTITLE: Documenting Experimental FX Submodule APIs - reStructuredText\nDESCRIPTION: This snippet uses Sphinx directives in reStructuredText to document all available classes and functions in torch.fx.experimental.symbolic_shapes and torch.fx.experimental.proxy_tensor. It leverages autosummary and automodule commands to automatically generate concise API references. Dependencies include Sphinx with autodoc and autosummary extensions enabled. Key parameters control the content tree and summary signature display, and the expected output is navigable HTML API docs. Its limitation is that documentation accuracy depends on the up-to-date code comments and Sphinx configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.experimental.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: torch.fx.experimental\n\ntorch.fx.experimental\n=====================\n\n.. warning::\n   These APIs are experimental and subject to change without notice.\n\ntorch.fx.experimental.symbolic_shapes\n-------------------------------------\n.. currentmodule:: torch.fx.experimental.symbolic_shapes\n.. automodule:: torch.fx.experimental.symbolic_shapes\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    ShapeEnv\n    DimDynamic\n    StrictMinMaxConstraint\n    RelaxedUnspecConstraint\n    EqualityConstraint\n    SymbolicContext\n    StatelessSymbolicContext\n    StatefulSymbolicContext\n    SubclassSymbolicContext\n    DimConstraints\n    ShapeEnvSettings\n    ConvertIntKey\n    CallMethodKey\n    PropagateUnbackedSymInts\n    DivideByKey\n    InnerTensorKey\n\n    hint_int\n    is_concrete_int\n    is_concrete_bool\n    is_concrete_float\n    has_free_symbols\n    has_free_unbacked_symbols\n    definitely_true\n    definitely_false\n    guard_or_true\n    guard_or_false\n    guard_size_oblivious\n    sym_and\n    sym_eq\n    sym_or\n    constrain_range\n    constrain_unify\n    canonicalize_bool_expr\n    statically_known_true\n    lru_cache\n    check_consistent\n    compute_unbacked_bindings\n    rebind_unbacked\n    resolve_unbacked_bindings\n    is_accessor_node\n\ntorch.fx.experimental.proxy_tensor\n-------------------------------------\n\n.. currentmodule:: torch.fx.experimental.proxy_tensor\n.. automodule:: torch.fx.experimental.proxy_tensor\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    make_fx\n    handle_sym_dispatch\n    get_proxy_mode\n    maybe_enable_thunkify\n    maybe_disable_thunkify\n```\n\n----------------------------------------\n\nTITLE: Wrapping Kernels with Functors for Dispatch (PyTorch ATen C++/CUDA)\nDESCRIPTION: The listed symbols follow a common naming pattern indicating template instantiations for wrapping kernel functors with dispatch, supporting both unboxed and boxed modes (e.g., _ZN3c104impl28wrap_kernel_functor_unboxed_...). These wrapper functors facilitate flexible operator invocation through statically generated call signatures based on parameter lists, handling device dispatch (with DispatchKeySet), operator kernel selection, and parameter adaptation. Prerequisites include knowledge of PyTorch's ATen dispatch system and the requirement that wrapped kernels conform to a defined signature for correct registration. Inputs and outputs depend on the wrapped function's specification; the wrappers adapt these for use in the PyTorch operator library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_16\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104impl28wrap_kernel_functor_unboxed_INS0_6detail24WrapFunctionIntoFunctor_INS_26CompileTimeFunctionPointerIFRN2at6TensorES7_RKS6_bEXadL_ZNS5_12_GLOBAL__N_112_GLOBAL__N_140wrapper_CompositeExplicitAutograd__copy_ES7_S9_bEEEES7_NS_4guts8typelist8typelistIJS7_S9_bEEEEESA_E4callEPNS_14OperatorKernelENS_14DispatchKeySetES7_S9_b\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104impl28wrap_kernel_functor_unboxed_INS0_6detail24WrapFunctionIntoFunctor_INS_26CompileTimeFunctionPointerIFN2at6TensorERKS6_lSt8optionalINS_6SymIntEESB_SA_EXadL_ZNS5_12_GLOBAL__N_112_GLOBAL__N_146wrapper_CompositeExplicitAutograd_Tensor_sliceES8_lSB_SB_SA_EEEES6_NS_4guts8typelist8typelistIJS8_lSB_SB_SA_EEEEESC_E4callEPNS_14OperatorKernelENS_14DispatchKeySetES8_lSB_SB_SA_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104impl28wrap_kernel_functor_unboxed_INS0_6detail24WrapFunctionIntoFunctor_INS_26CompileTimeFunctionPointerIFN2at6TensorERKS6_NS_10ScalarTypeEbbSt8optionalINS_12MemoryFormatEEEXadL_ZNS5_12_GLOBAL__N_112_GLOBAL__N_142wrapper_CompositeImplicitAutograd_dtype_toES8_S9_bbSC_EEEES6_NS_4guts8typelist8typelistIJS8_S9_bbSC_EEEEESD_E4callEPNS_14OperatorKernelENS_14DispatchKeySetES8_S9_bbSC_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104impl28wrap_kernel_functor_unboxed_INS0_6detail24WrapFunctionIntoFunctor_INS_26CompileTimeFunctionPointerIFN2at6TensorERKS6_NS_8ArrayRefINS_6SymIntEEEEXadL_ZNS5_12_GLOBAL__N_112_GLOBAL__N_142wrapper_CompositeImplicitAutograd__reshapeES8_SB_EEEES6_NS_4guts8typelist8typelistIJS8_SB_EEEEESC_E4callEPNS_14OperatorKernelENS_14DispatchKeySetES8_SB_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104impl28wrap_kernel_functor_unboxed_INS0_6detail24WrapFunctionIntoFunctor_INS_26CompileTimeFunctionPointerIFN2at6TensorERKS6_S8_RKSt8optionalIS6_EEXadL_ZNS5_12_GLOBAL__N_112_GLOBAL__N_141wrapper_CompositeImplicitAutograd__linearES8_S8_SC_EEEES6_NS_4guts8typelist8typelistIJS8_S8_SC_EEEEESD_E4callEPNS_14OperatorKernelENS_14DispatchKeySetES8_S8_SC_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104impl28wrap_kernel_functor_unboxed_INS0_6detail24WrapFunctionIntoFunctor_INS_26CompileTimeFunctionPointerIFN2at6TensorENS_14DispatchKeySetERKS6_NS_8ArrayRefINS_6SymIntEEEEXadL_ZN5torch15ADInplaceOrView12_GLOBAL__N_14viewES7_S9_SC_EEEES6_NS_4guts8typelist8typelistIJS7_S9_SC_EEEEESD_E4callEPNS_14OperatorKernelES7_S9_SC_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104impl28wrap_kernel_functor_unboxed_INS0_6detail24WrapFunctionIntoFunctor_INS_26CompileTimeFunctionPointerIFN2at6TensorENS_14DispatchKeySetERKS6_NS_8ArrayRefIlEEEXadL_ZN5torch15ADInplaceOrView12_GLOBAL__N_17permuteES7_S9_SB_EEEES6_NS_4guts8typelist8typelistIJS7_S9_SB_EEEEESC_E4callEPNS_14OperatorKernelES7_S9_SB_\n```\n\n----------------------------------------\n\nTITLE: Apple Platform-Specific Configuration\nDESCRIPTION: Configures Apple-specific settings including framework search paths, clang version detection, and MPSGraph framework detection for MacOS builds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(APPLE)\n  set(CMAKE_FIND_FRAMEWORK LAST)\n  set(CMAKE_FIND_APPBUNDLE LAST)\n  set(CMAKE_MACOSX_RPATH ON)\n  execute_process(COMMAND ${CMAKE_CXX_COMPILER} --version\n                  OUTPUT_VARIABLE clang_full_version_string)\n  string(REGEX REPLACE \"Apple (.*) version ([0-9]+\\\\.[0-9]+).*\" \"\\\\2\"\n                       CLANG_VERSION_STRING ${clang_full_version_string})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tracking Batch Normalization Forward Operator Calls in PyTorch\nDESCRIPTION: Records frequency counts of batch normalization forward operations with different tensor shapes and parameters. Each entry shows a half-precision (f16) input tensor followed by parameters for the operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((T([128, 184, 14, 14], f16), T([184], f16), T([184], f16), T([184], f16), T([184], f16), True, 0.1, 1e-05), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), True, 0.1, 1e-05), {})\ncnt: 2, ((T([128, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 160, 7, 7], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), True, 0.1, 1e-05), {})\ncnt: 5, ((T([128, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch Documentation\nDESCRIPTION: Commands to install documentation dependencies and build PyTorch documentation using Sphinx. Includes steps for installing requirements and generating HTML documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncd docs/\npip install -r requirements.txt\nmake html\nmake serve\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.native_layer_norm_backward.default Operator (Log)\nDESCRIPTION: Log entries detailing calls to the PyTorch `aten.native_layer_norm_backward.default` operator. Each line shows the invocation count (`cnt`) and the specific arguments passed, including input tensor shapes, data types (f16, f32), normalized shape dimensions, mean/variance tensors, weight/bias tensors, and boolean flags indicating gradient requirements.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.native_layer_norm_backward.default\ncnt: 22, ((T([64, 197, 256], f16), T([64, 197, 256], f16), [256], T([64, 197, 1], f32), T([64, 197, 1], f32), T([256], f16), T([256], f16), [True, True, True]), {})\ncnt: 10, ((T([64, 401, 128], f16), T([64, 401, 128], f16), [128], T([64, 401, 1], f32), T([64, 401, 1], f32), T([128], f16), T([128], f16), [True, True, True]), {})\ncnt: 3, ((T([64, 1, 128], f16), T([64, 1, 128], f16), [128], T([64, 1, 1], f32), T([64, 1, 1], f32), T([128], f16), T([128], f16), [True, True, True]), {})\ncnt: 3, ((T([64, 1, 256], f16), T([64, 1, 256], f16), [256], T([64, 1, 1], f32), T([64, 1, 1], f32), T([256], f16), T([256], f16), [True, True, True]), {})\ncnt: 3, ((T([64, 1, 256], f16), T([64, 1, 256], f16, stride=(50432, 256, 1)), [256], T([64, 1, 1], f32), T([64, 1, 1], f32), T([256], f16), T([256], f16), [True, True, True]), {})\ncnt: 3, ((T([64, 1, 128], f16), T([64, 1, 128], f16, stride=(51328, 128, 1)), [128], T([64, 1, 1], f32), T([64, 1, 1], f32), T([128], f16), T([128], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Conditionally Installing JIT Test Shared Libraries in CMake\nDESCRIPTION: Installs the `torchbind_test`, `jitbackend_test`, and `backend_with_compiler` shared library targets to the `lib` directory relative to the installation prefix. This installation step is performed only if the `INSTALL_TEST` CMake option is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif(INSTALL_TEST)\n  install(TARGETS torchbind_test DESTINATION lib)\n  install(TARGETS jitbackend_test DESTINATION lib)\n  install(TARGETS backend_with_compiler DESTINATION lib)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tensor Division Operations with Half Precision\nDESCRIPTION: Series of tensor division operations on 64x1x1 tensors using float16 (half) precision with decreasing scale factors\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swin_base_patch4_window7_224_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([64, 1, 1], f16), 0.9956521736457944), {})\ncnt: 2, ((T([64, 1, 1], f16), 0.9913043472915888), {})\n# ... Additional similar operations\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pytest Shell\nDESCRIPTION: The shell command is used to execute all tests in the specified Python test script related to distributed tensors in PyTorch using pytest. The test can be executed on either CPU or GPU. The command requires pytest to be installed and should be run from the project root.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/README.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npytest test/distributed/tensor/test_dtensor.py\n```\n\n----------------------------------------\n\nTITLE: C++ Timer Creation for Eager Mode\nDESCRIPTION: Example of creating a Timer instance for the eager forward mode in C++. It uses the benchmark's C++ forward statement and setup code, with language specified as 'cpp'.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nTimer(\n    stmt=benchmark.cpp_fwd_stmt,\n    setup=benchmark.setup.cpp_setup,\n    language=\"cpp\",\n)\n```\n\n----------------------------------------\n\nTITLE: Tensor Softmax and Backward in PyTorch\nDESCRIPTION: This snippet shows the use of `aten._softmax.default` and `aten._softmax_backward_data.default`, for softmax computation and its gradient computation, applied across various tensor dimensions. Inputs include tensors, dimensions, and a boolean flag for optional functionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForCausalLM_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 6, ((T([192, 128, 128], f16), -1, False), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 6, ((T([192, 128, 128], f16), T([192, 128, 128], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Division in PyTorch\nDESCRIPTION: Performs element-wise division of a tensor by a scalar value.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\naten.div.Tensor((T([], f16), 64000), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorImpl in C++\nDESCRIPTION: This function initializes a TensorImpl object with storage, dispatch key set, and type metadata. It's a core part of PyTorch's tensor implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c1010TensorImplC1ENS0_8ImplTypeEONS_7StorageENS_14DispatchKeySetEN6caffe28TypeMetaE\n```\n\n----------------------------------------\n\nTITLE: Calculating NLL Loss in PyTorch\nDESCRIPTION: This snippet shows the tensor shapes and parameters for calculating Negative Log Likelihood (NLL) loss and its backward pass in a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Embedding Backward Operations in PyTorch\nDESCRIPTION: Documents uses of the \\\"aten.embedding_dense_backward.default\\\" to detail derivative computations with respect to embedding weights during training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([8, 128, 1024], f16), T([8, 128], i64), 50265, 0, False), {})\n```\n\n----------------------------------------\n\nTITLE: MaxPool2D Operations\nDESCRIPTION: Max pooling operations with 3x3 kernel and stride 2, applied to tensors of different spatial dimensions\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 64, 147, 147], f16), [3, 3], [2, 2]), {})\n```\n\n----------------------------------------\n\nTITLE: Average Pooling Backward Pass in PyTorch\nDESCRIPTION: Statistics for the aten.avg_pool2d_backward.default operator used in backpropagation for average pooling operations. These operations compute gradients for the corresponding forward average pooling operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([128, 1536, 6, 6], f16), T([128, 1536, 12, 12], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\ncnt: 1, ((T([128, 512, 12, 12], f16), T([128, 512, 24, 24], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\ncnt: 1, ((T([128, 256, 24, 24], f16), T([128, 256, 48, 48], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\n```\n\n----------------------------------------\n\nTITLE: Listing PyTorch Copy, Div, Pool, BatchNorm Operator Input/Parameters - Python\nDESCRIPTION: This snippet enumerates argument/shape lists for PyTorch operators such as aten.copy_, aten.div, aten.lift_fresh_copy, max pooling, mean, mm, mul, and batch norm. Each entry provides typical input tensor shapes (including strides and types), scalar values, and operator-appropriate parameter lists. Designed for feeding synthetic operator runners, it assumes PyTorch/aten machinery and shape conventions. Output: tuple-structured invocation arguments suitable for test harnessing. Limitation: Not directly runnable without an execution context.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 256, 256], f16), T([128, 3, 256, 256], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 2048, 8, 8], f16, stride=(2048, 1, 0, 0)), 64), {})\ncnt: 1, ((T([128, 256, 16, 16], f16, stride=(256, 1, 0, 0)), 256), {})\ncnt: 2, ((T([128, 128, 32, 32], f16, stride=(128, 1, 0, 0)), 1024), {})\ncnt: 2, ((T([128, 64, 64, 64], f16, stride=(64, 1, 0, 0)), 4096), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([128, 64, 128, 128], f16), [3, 3], [2, 2], [1, 1]), {})\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([128, 64, 64, 64], f16), T([128, 64, 128, 128], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([128, 64, 64, 64], i64)), {})\nOperator: aten.mean.dim\ncnt: 2, ((T([128, 64, 64, 64], f16), [2, 3]), {})\ncnt: 2, ((T([128, 128, 32, 32], f16), [2, 3]), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), [2, 3]), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 4, ((T([131072, 16], f16), T([16, 31], f16, stride=(1, 16))), {})\ncnt: 2, ((T([32768, 16], f16), T([16, 15], f16, stride=(1, 16))), {})\ncnt: 1, ((T([128, 1000], f16), T([1000, 2048], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 2048], f16)), {})\ncnt: 2, ((T([15, 32768], f16, stride=(1, 15)), T([32768, 16], f16)), {})\ncnt: 2, ((T([32768, 15], f16), T([15, 16], f16)), {})\ncnt: 4, ((T([31, 131072], f16, stride=(1, 31)), T([131072, 16], f16)), {})\ncnt: 4, ((T([131072, 31], f16), T([31, 16], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 4, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16, stride=(64, 1, 0, 0))), {})\ncnt: 4, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16, stride=(128, 1, 0, 0))), {})\ncnt: 2, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16, stride=(256, 1, 0, 0))), {})\ncnt: 4, ((T([512, 256, 256], f16), 0.25), {})\ncnt: 2, ((T([512, 64, 64], f16), 0.25), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16)), {})\ncnt: 2, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16)), {})\ncnt: 2, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 24, 128, 128], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 64, 128, 128], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 256, 64, 64], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 128, 32, 32], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 512, 32, 32], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 256, 16, 16], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 1024, 16, 16], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 512, 8, 8], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 2048, 8, 8], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f16), True, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([128, 256, 32, 32], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Performing Backward Pass for Batch Normalization\nDESCRIPTION: Computes the gradients of the inputs for the backward pass of batch normalization, essential for training neural networks. It requires input and gradient tensors with matching dimensions and appropriate momentum and epsilon settings. Outputs are gradient tensors used for updating weights based on the backward propagation algorithm.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 2, ((T([1, 1024, 128, 128], f16), T([1, 1024, 128, 128], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), False, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([1, 2048, 64, 64], f16), T([1, 2048, 64, 64], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), False, 1e-05, [True, True, True]), {})\ncnt: 13, ((T([1, 4096, 32, 32], f16), T([1, 4096, 32, 32], f16), T([4096], f16), T([4096], f16), T([4096], f16), T([4096], f32), T([4096], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.relu.default\nDESCRIPTION: Logs Rectified Linear Unit (ReLU) activation function calls (`aten.relu.default`). These are out-of-place operations applied to 4D float16 tensors of varying shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.relu.default\ncnt: 1, ((T([128, 24, 56, 56], f16),), {})\ncnt: 1, ((T([128, 56, 28, 28], f16),), {})\ncnt: 4, ((T([128, 152, 14, 14], f16),), {})\ncnt: 7, ((T([128, 368, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Element-wise Addition Operations with aten.add.Tensor\nDESCRIPTION: Documents usage of the add.Tensor operator which performs element-wise addition between tensors. Various tensor shapes and patterns are shown, including both scalar and tensor additions, with most operations using half-precision (float16) data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 2, ((T([8, 64, 192, 256], f16), T([8, 64, 192, 256], f16)), {})\ncnt: 4, ((T([8, 128, 96, 128], f16), T([8, 128, 96, 128], f16)), {})\ncnt: 16, ((T([8, 256, 48, 64], f16), T([8, 256, 48, 64], f16)), {})\ncnt: 16, ((T([8, 512, 24, 32], f16), T([8, 512, 24, 32], f16)), {})\ncnt: 8, ((T([8, 1024, 12, 16], f16), T([8, 1024, 12, 16], f16)), {})\ncnt: 1, ((T([8, 3, 12, 16, 2], f16), T([1, 1, 12, 16, 2], f32)), {})\ncnt: 1, ((T([8, 3, 24, 32, 2], f16), T([1, 1, 24, 32, 2], f32)), {})\ncnt: 1, ((T([8, 3, 48, 64, 2], f16), T([1, 1, 48, 64, 2], f32)), {})\ncnt: 2, ((T([], f16), 0), {})\ncnt: 3, ((T([], f16), T([], f16)), {})\ncnt: 3, ((T([8, 3, 48, 64, 85], f16), T([8, 3, 48, 64, 85], f16)), {})\ncnt: 1, ((T([8, 3, 48, 64, 85], f16, stride=(0, 0, 0, 0, 0)), T([8, 3, 48, 64, 85], f16)), {})\ncnt: 3, ((T([8, 3, 24, 32, 85], f16), T([8, 3, 24, 32, 85], f16)), {})\ncnt: 1, ((T([8, 3, 24, 32, 85], f16, stride=(0, 0, 0, 0, 0)), T([8, 3, 24, 32, 85], f16)), {})\ncnt: 1, ((T([8, 256, 24, 32], f16), T([8, 256, 24, 32], f16)), {})\ncnt: 3, ((T([8, 3, 12, 16, 85], f16), T([8, 3, 12, 16, 85], f16)), {})\ncnt: 1, ((T([8, 3, 12, 16, 85], f16, stride=(0, 0, 0, 0, 0)), T([8, 3, 12, 16, 85], f16)), {})\ncnt: 3, ((T([8, 512, 12, 16], f16), T([8, 512, 12, 16], f16)), {})\ncnt: 1, ((T([8, 512, 12, 16], f16, stride=(393216, 192, 16, 1)), T([8, 512, 12, 16], f16)), {})\ncnt: 1, ((T([8, 512, 24, 32], f16, stride=(589824, 768, 32, 1)), T([8, 512, 24, 32], f16)), {})\ncnt: 1, ((T([8, 256, 48, 64], f16, stride=(1179648, 3072, 64, 1)), T([8, 256, 48, 64], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing Zero Tensors in PyTorch\nDESCRIPTION: This snippet shows the creation of zero-initialized tensors using aten.new_zeros.default. It demonstrates various tensor shapes and configurations, primarily using half-precision (f16) data type on CUDA devices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([1024, 30876], f16, stride=(31068, 1)), [1024, 249, 249]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 1, ((T([54765], f16), [965]), {})\ncnt: 2, ((T([54704], f16), [965]), {})\ncnt: 4, ((T([54786], f16), [965]), {})\ncnt: 2, ((T([54804], f16), [965]), {})\ncnt: 3, ((T([54757], f16), [965]), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch Unfold Backward Operator - Python\nDESCRIPTION: This snippet defines batched test cases for the ATen unfold_backward.default operator, focusing on unfolded views on higher-dimensional tensors (common in convolutional backprop). Each case specifies input/output tensor shapes, dimensions to unfold, size, and step, generally targeting gradient recovery for im2col-like operations. Inputs are tuples of shapes/parameters. Requires PyTorch ATen backend.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 640, 1, 1, 12, 12], f16), [128, 640, 1, 12, 12], 3, 12, 8), {})\ncnt: 1, ((T([128, 640, 1, 12, 12], f16), [128, 640, 12, 12], 2, 12, 8), {})\ncnt: 1, ((T([128, 640, 2, 2, 12, 12], f16), [128, 640, 2, 20, 12], 3, 12, 8), {})\ncnt: 1, ((T([128, 640, 2, 20, 12], f16), [128, 640, 20, 20], 2, 12, 8), {})\ncnt: 1, ((T([128, 384, 2, 2, 12, 12], f16), [128, 384, 2, 20, 12], 3, 12, 8), {})\ncnt: 1, ((T([128, 384, 2, 20, 12], f16), [128, 384, 20, 20], 2, 12, 8), {})\n```\n\n----------------------------------------\n\nTITLE: Running HuggingFace Performance Benchmarks (Python)\nDESCRIPTION: Commands to run HuggingFace performance benchmarks for both training and inference using TorchInductor backend. The commands specify device, precision, and output format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n./benchmarks/dynamo/huggingface.py --performance --training --amp --backend=inductor --output=huggingface_training.csv\n./benchmarks/dynamo/huggingface.py --performance --inference --bfloat16 --backend=inductor --output=huggingface_inference.csv\n```\n\n----------------------------------------\n\nTITLE: PyTorch Bilinear Upsampling Operations\nDESCRIPTION: Usage of aten.upsample_bilinear2d.vec operator with various input tensor shapes and stride configurations. Includes upsampling to different target sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.upsample_bilinear2d.vec\ncnt: 1, ((T([1, 3, 427, 640], f16, stride=(3, 1, 1920, 3)), [799, 1199], False, None), {})\ncnt: 1, ((T([1, 3, 612, 612], f16, stride=(3, 1, 1836, 3)), [800, 800], False, None), {})\ncnt: 1, ((T([1, 3, 640, 443], f16, stride=(3, 1, 1329, 3)), [1155, 800], False, None), {})\ncnt: 1, ((T([1, 3, 459, 640], f16, stride=(3, 1, 1920, 3)), [799, 1115], False, None), {})\n```\n\n----------------------------------------\n\nTITLE: Integer Division with torch.div in PyTorch 1.5 and Earlier\nDESCRIPTION: Demonstrates how torch.div performed integer division in PyTorch 1.5 and earlier versions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# PyTorch 1.5 (and earlier)\n>>> a = torch.tensor(5)\n>>> b = torch.tensor(3)\n>>> a / b\ntensor(1)\n```\n\n----------------------------------------\n\nTITLE: Dimension Unbinding and Transposition in PyTorch\nDESCRIPTION: Examples showing how to unbind dimensions and perform transpositions using the order method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ni, j = dims(2)\nA = torch.rand(3, 4)\nA_T = A[i, j].order(j, i)\nassert torch.allclose(A.T, A_T)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Slice Operations\nDESCRIPTION: Tensor slicing operations with different dimension specifications and stride patterns\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n((T([4096, 8, 8], f16), [4096, 8, 15], 2, 7, 9223372036854775807, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Tensor Multiplication in PyTorch\nDESCRIPTION: Documents the use of the \\\"aten.mul.Tensor\\\", mapping its use for scaling tensors by scalar values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 2, ((T([8, 128, 1024], f16), 1.0), {})\ncnt: 24, ((T([8, 128, 1024], f16), 0.125), {})\n```\n\n----------------------------------------\n\nTITLE: Applying aten._to_copy with dtype Conversion in PyTorch\nDESCRIPTION: The aten._to_copy.default operator example demonstrates converting a tensor of shape [16, 1, 1, 128] from f32 to f16, leveraging PyTorch functionalities. Prerequisites include correct installation of PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\naten._to_copy.default, ((T([16, 1, 1, 128], f32),), {'dtype': f16})\n```\n\n----------------------------------------\n\nTITLE: Convolution Operation Parameters - PyTorch\nDESCRIPTION: Collection of convolution operation specifications with tensor shapes, strides, padding and other parameters in PyTorch's internal format. Each entry shows input tensor, weight tensor, bias tensor (if any), stride, padding, dilation, groups and other configuration parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n((T([1, 80, 40, 40], f16), T([480, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Debugging PyTorch with LLDB Before Debug Info\nDESCRIPTION: Example of using LLDB to debug a PyTorch tensor indexing operation, showing the limited information available without debug symbols. The breakpoint is set at the applySelect function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\n% lldb -o \"b applySelect\" -o \"process launch\" -- python3 -c \"import torch;print(torch.rand(5)[3])\"\n(lldb) target create \"python\"\nCurrent executable set to '/usr/bin/python3' (arm64).\n(lldb) settings set -- target.run-args  \"-c\" \"import torch;print(torch.rand(5)[3])\"\n(lldb) b applySelect\nBreakpoint 1: no locations (pending).\nWARNING:  Unable to resolve breakpoint to any actual locations.\n(lldb) process launch\n2 locations added to breakpoint 1\nProcess 87729 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x00000001023d55a8 libtorch_python.dylib`at::indexing::impl::applySelect(at::Tensor const&, long long, c10::SymInt, long long, c10::Device const&, std::__1::optional<c10::ArrayRef<c10::SymInt>> const&)\nlibtorch_python.dylib`at::indexing::impl::applySelect:\n->  0x1023d55a8 <+0>:  sub    sp, sp, #0xd0\n    0x1023d55ac <+4>:  stp    x24, x23, [sp, #0x90]\n    0x1023d55b0 <+8>:  stp    x22, x21, [sp, #0xa0]\n    0x1023d55b4 <+12>: stp    x20, x19, [sp, #0xb0]\nTarget 0: (python) stopped.\nProcess 87729 launched: '/usr/bin/python' (arm64)\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch on macOS - Development Mode - bash\nDESCRIPTION: This command performs a development-mode build of PyTorch on macOS. It requires all prerequisites and dependencies to be installed and for the working directory to be set to the cloned PyTorch repository. Use Python 3 for maximum compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython3 setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Convolution Backward Pass Parameters - PyTorch\nDESCRIPTION: Backward pass specifications for convolution operations, including gradient tensors, input tensors, weight tensors and configuration parameters for gradient computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n((T([1, 36, 5, 5], f16, stride=(900, 1, 180, 36)), T([1, 88, 5, 5], f16), T([36, 88, 1, 1], f16), [36], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Statistics for Deep Neural Network\nDESCRIPTION: This data represents usage statistics of PyTorch operators in what appears to be an Inception-style neural network implementation. It captures tensor dimensions, data types (mostly float16/f16), and call frequency for various operators such as convolution, pooling, tensor addition, and softmax operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 4, ((T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16)), {})\ncnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16)), {})\ncnt: 3, ((T([128, 1280, 8, 8], f16), T([128, 1280, 8, 8], f16)), {})\ncnt: 14, ((T([128, 768, 17, 17], f16), T([128, 768, 17, 17], f16)), {})\ncnt: 5, ((T([128, 288, 35, 35], f16), T([128, 288, 35, 35], f16)), {})\ncnt: 3, ((T([128, 256, 35, 35], f16), T([128, 256, 35, 35], f16)), {})\ncnt: 3, ((T([128, 192, 35, 35], f16), T([128, 192, 35, 35], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 94, ((T([], i64), 1), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([128, 192, 35, 35], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 1, ((T([128, 256, 35, 35], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 1, ((T([128, 288, 35, 35], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 4, ((T([128, 768, 17, 17], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), [3, 3], [1, 1], [1, 1]), {})\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([128, 1280, 8, 8], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 4, ((T([128, 768, 17, 17], f16), T([128, 768, 17, 17], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 1, ((T([128, 288, 35, 35], f16), T([128, 288, 35, 35], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 1, ((T([128, 256, 35, 35], f16), T([128, 256, 35, 35], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\ncnt: 1, ((T([128, 192, 35, 35], f16), T([128, 192, 35, 35], f16), [3, 3], [1, 1], [1, 1], False, True, None), {})\nOperator: aten.cat.default\ncnt: 1, (([T([128, 64, 35, 35], f16), T([128, 64, 35, 35], f16), T([128, 96, 35, 35], f16), T([128, 32, 35, 35], f16)], 1), {})\ncnt: 2, (([T([128, 64, 35, 35], f16), T([128, 64, 35, 35], f16), T([128, 96, 35, 35], f16), T([128, 64, 35, 35], f16)], 1), {})\ncnt: 1, (([T([128, 384, 17, 17], f16), T([128, 96, 17, 17], f16), T([128, 288, 17, 17], f16)], 1), {})\ncnt: 4, (([T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16), T([128, 192, 17, 17], f16)], 1), {})\ncnt: 1, (([T([128, 320, 8, 8], f16), T([128, 192, 8, 8], f16), T([128, 768, 8, 8], f16)], 1), {})\ncnt: 4, (([T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16)], 1), {})\ncnt: 2, (([T([128, 320, 8, 8], f16), T([128, 768, 8, 8], f16), T([128, 768, 8, 8], f16), T([128, 192, 8, 8], f16)], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 299, 299], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 299, 299], f16), T([32, 3, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 149, 149], f16), T([32, 32, 3, 3], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 147, 147], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 73, 73], f16), T([80, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 80, 73, 73], f16), T([192, 80, 3, 3], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 192, 35, 35], f16), T([64, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 35, 35], f16), T([48, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 48, 35, 35], f16), T([64, 48, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 64, 35, 35], f16), T([96, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 96, 35, 35], f16), T([96, 96, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 35, 35], f16), T([32, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 256, 35, 35], f16), T([64, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 35, 35], f16), T([48, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 288, 35, 35], f16), T([64, 288, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 288, 35, 35], f16), T([48, 288, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 288, 35, 35], f16), T([384, 288, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 35, 35], f16), T([96, 96, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 12, ((T([128, 768, 17, 17], f16), T([192, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 768, 17, 17], f16), T([128, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 128, 17, 17], f16), T([128, 128, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 17, 17], f16), T([192, 128, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 128, 17, 17], f16), T([128, 128, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 17, 17], f16), T([192, 128, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 768, 17, 17], f16), T([160, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 160, 17, 17], f16), T([160, 160, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 160, 17, 17], f16), T([192, 160, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 160, 17, 17], f16), T([160, 160, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 160, 17, 17], f16), T([192, 160, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 192, 17, 17], f16), T([192, 192, 1, 7], f16), None, [1, 1], [0, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 192, 17, 17], f16), T([192, 192, 7, 1], f16), None, [1, 1], [3, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 17, 17], f16), T([320, 192, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 17, 17], f16), T([192, 192, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([320, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([384, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 384, 8, 8], f16), T([384, 384, 1, 3], f16), None, [1, 1], [0, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 384, 8, 8], f16), T([384, 384, 3, 1], f16), None, [1, 1], [1, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([448, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 448, 8, 8], f16), T([384, 448, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1280, 8, 8], f16), T([192, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([320, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([384, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([448, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([192, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 192, 8, 8], f16), T([128, 2048, 8, 8], f16), T([192, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16), T([384, 384, 3, 1], f16), [0], [1, 1], [1, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 384, 8, 8], f16), T([128, 384, 8, 8], f16), T([384, 384, 1, 3], f16), [0], [1, 1], [0, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 384, 8, 8], f16), T([128, 448, 8, 8], f16), T([384, 448, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 448, 8, 8], f16), T([128, 2048, 8, 8], f16), T([448, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 384, 8, 8], f16), T([128, 2048, 8, 8], f16), T([384, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 320, 8, 8], f16), T([128, 2048, 8, 8], f16), T([320, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 8, 8], f16), T([128, 1280, 8, 8], f16), T([192, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 448, 8, 8], f16), T([128, 1280, 8, 8], f16), T([448, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 384, 8, 8], f16), T([128, 1280, 8, 8], f16), T([384, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 320, 8, 8], f16), T([128, 1280, 8, 8], f16), T([320, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 8, 8], f16), T([128, 192, 17, 17], f16), T([192, 192, 3, 3], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking Log Softmax Operator in PyTorch\nDESCRIPTION: Demonstrates the use of the aten._log_softmax.default operator for performing log-softmax operations along a specified dimension on a float16 tensor. This operator is essential in scenarios requiring the computation of log probabilities where numerical stability is crucial.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForQuestionAnswering_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 2, ((T([4, 512], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Importing IterDataPipe from PyTorch\nDESCRIPTION: Imports the IterDataPipe class from torch.utils.data module, which is used as a base class for creating custom DataPipes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/standard_pipes.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import IterDataPipe\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch Operators in ConvNeXt Model\nDESCRIPTION: This snippet shows the operator usage statistics of a ConvNeXt model implementation in PyTorch, displaying the call counts and tensor dimensions for forward and backward operations. It provides insight into the model architecture with downsampling pattern and increasing channel dimensions from 32 to 2560.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gernet_l_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 57, ((T([], i64), 1), {})\ncnt: 2, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16)), {})\ncnt: 4, ((T([128, 192, 32, 32], f16), T([128, 192, 32, 32], f16)), {})\ncnt: 12, ((T([128, 640, 16, 16], f16), T([128, 640, 16, 16], f16)), {})\ncnt: 17, ((T([128, 640, 8, 8], f16), T([128, 640, 8, 8], f16)), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 2560], f16), T([2560, 1000], f16, stride=(1, 2560))), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 256, 256], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 256, 256], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([192, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 192, 32, 32], f16), T([192, 192, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([192, 128, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 32, 32], f16), T([160, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 160, 32, 32], f16), T([160, 160, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 6, ((T([128, 160, 16, 16], f16), T([640, 160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 32, 32], f16), T([640, 192, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 640, 16, 16], f16), T([160, 640, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 160, 16, 16], f16), T([160, 160, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 640, 16, 16], f16), T([1920, 640, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1920, 16, 16], f16), T([1920, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1920), {})\ncnt: 9, ((T([128, 1920, 8, 8], f16), T([640, 1920, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 640, 16, 16], f16), T([640, 640, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 8, ((T([128, 640, 8, 8], f16), T([1920, 640, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 8, ((T([128, 1920, 8, 8], f16), T([1920, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1920), {})\ncnt: 1, ((T([128, 640, 8, 8], f16), T([2560, 640, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 2560, 8, 8], f16), T([128, 640, 8, 8], f16), T([2560, 640, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 9, ((T([128, 640, 8, 8], f16), T([128, 1920, 8, 8], f16), T([640, 1920, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 8, ((T([128, 1920, 8, 8], f16), T([128, 1920, 8, 8], f16), T([1920, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1920, [True, True, False]), {})\ncnt: 8, ((T([128, 1920, 8, 8], f16), T([128, 640, 8, 8], f16), T([1920, 640, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 640, 8, 8], f16), T([128, 640, 16, 16], f16), T([640, 640, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1920, 8, 8], f16), T([128, 1920, 16, 16], f16), T([1920, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1920, [True, True, False]), {})\ncnt: 1, ((T([128, 1920, 16, 16], f16), T([128, 640, 16, 16], f16), T([1920, 640, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 6, ((T([128, 640, 16, 16], f16), T([128, 160, 16, 16], f16), T([640, 160, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 5, ((T([128, 160, 16, 16], f16), T([128, 160, 16, 16], f16), T([160, 160, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 5, ((T([128, 160, 16, 16], f16), T([128, 640, 16, 16], f16), T([160, 640, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 640, 16, 16], f16), T([128, 192, 32, 32], f16), T([640, 192, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 160, 16, 16], f16), T([128, 160, 32, 32], f16), T([160, 160, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 160, 32, 32], f16), T([128, 192, 32, 32], f16), T([160, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 192, 32, 32], f16), T([128, 192, 32, 32], f16), T([192, 192, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 32, 32], f16), T([128, 128, 64, 64], f16), T([192, 128, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 32, 32], f16), T([128, 128, 64, 64], f16), T([192, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 32, 128, 128], f16), T([128, 32, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), T([128, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 32, 128, 128], f16), T([128, 32, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 3, 256, 256], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 256, 256], f16), T([128, 3, 256, 256], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 2560, 8, 8], f16, stride=(2560, 1, 0, 0)), 64), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 2560, 8, 8], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 2560], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 2560], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 32, 128, 128], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 5, ((T([128, 192, 32, 32], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 160, 32, 32], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), True, 0.1, 1e-05), {})\ncnt: 11, ((T([128, 160, 16, 16], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([128, 640, 16, 16], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 1920, 16, 16], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f16), True, 0.1, 1e-05), {})\ncnt: 17, ((T([128, 1920, 8, 8], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f16), True, 0.1, 1e-05), {})\ncnt: 10, ((T([128, 640, 8, 8], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 2560, 8, 8], f16), T([2560], f16), T([2560], f16), T([2560], f16), T([2560], f16), True, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([128, 2560, 8, 8], f16), T([128, 2560, 8, 8], f16), T([2560], f16), T([2560], f16), T([2560], f16), T([2560], f32), T([2560], f32), True, 1e-05, [True, True, True]), {})\ncnt: 10, ((T([128, 640, 8, 8], f16), T([128, 640, 8, 8], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f32), T([640], f32), True, 1e-05, [True, True, True]), {})\ncnt: 17, ((T([128, 1920, 8, 8], f16), T([128, 1920, 8, 8], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f32), T([1920], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 1920, 16, 16], f16), T([128, 1920, 16, 16], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f32), T([1920], f32), True, 1e-05, [True, True, True]), {})\ncnt: 7, ((T([128, 640, 16, 16], f16), T([128, 640, 16, 16], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f32), T([640], f32), True, 1e-05, [True, True, True]), {})\ncnt: 11, ((T([128, 160, 16, 16], f16), T([128, 160, 16, 16], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 160, 32, 32], f16), T([128, 160, 32, 32], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), True, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([128, 192, 32, 32], f16), T([128, 192, 32, 32], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring XPU Macro File\nDESCRIPTION: Sets up build configuration variables and generates the header file containing XPU-specific CMake macros.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/xpu/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(C10_XPU_BUILD_SHARED_LIBS ${BUILD_SHARED_LIBS}) # used in xpu_cmake_macros.h.in\nconfigure_file(\n    ${CMAKE_CURRENT_LIST_DIR}/impl/xpu_cmake_macros.h.in\n    ${CMAKE_BINARY_DIR}/c10/xpu/impl/xpu_cmake_macros.h)\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.sum.SymInt Operator (Text)\nDESCRIPTION: Logs calls to the `aten.sum.SymInt` operator, which performs summation over specified dimensions. Arguments show the input tensor shape (f16), the dimensions to sum over (e.g., [2, 3]), and a boolean flag (True, likely keepdim).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 3, ((T([128, 1536, 6, 6], f16), [2, 3], True), {})\ncnt: 6, ((T([128, 1536, 12, 12], f16), [2, 3], True), {})\ncnt: 2, ((T([128, 512, 24, 24], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 256, 48, 48], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Generating Class Documentation Header in PyTorch Sphinx Docs\nDESCRIPTION: Creates a header for the class documentation using the class name. The underline filter is applied to create a section heading.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/classtemplate.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n{{ name | underline}}\n```\n\n----------------------------------------\n\nTITLE: Stacking Tensors using ATen in Python\nDESCRIPTION: Demonstrates the usage of the ATen \\\"aten.stack.default\\\" operator to stack multiple tensors along a new dimension. The example works with f16 tensors of shape [128, 32, 112, 112]. It requires ATen to be available within the PyTorch environment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 32, 112, 112], f16),), {})\nOperator: aten.stack.default\n```\n\n----------------------------------------\n\nTITLE: Configuring Backend-Specific Tests\nDESCRIPTION: Configures tests for specific backends (Gloo, NCCL, UCC, MPI) based on build flags. Includes special handling for CUDA-enabled tests and private dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/c10d/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_CUDA)\n  if(USE_GLOO AND USE_C10D_GLOO)\n    c10d_add_test(ProcessGroupGlooTest.cpp LINK_LIBRARIES torch_cpu c10d_cuda_test gtest_main INSTALL_TEST ${INSTALL_TEST})\n    c10d_add_test(ProcessGroupGlooAsyncTest.cpp LINK_LIBRARIES torch_cpu c10d_cuda_test gtest_main INSTALL_TEST ${INSTALL_TEST})\n  endif()\n  if(USE_NCCL AND USE_C10D_NCCL)\n    c10d_add_test(\n      ProcessGroupNCCLTest.cpp\n      LINK_LIBRARIES torch_cpu c10d_cuda_test gtest_main __caffe2_nccl INSTALL_TEST ${INSTALL_TEST})\n    c10d_add_test(\n      ProcessGroupNCCLErrorsTest.cpp\n      LINK_LIBRARIES torch_cpu c10d_cuda_test gtest_main __caffe2_nccl INSTALL_TEST ${INSTALL_TEST})\n    if(INSTALL_TEST)\n      install(TARGETS c10d_cuda_test DESTINATION lib)\n    endif()\n  endif()\n  if(USE_UCC AND USE_C10D_UCC)\n    c10d_add_test(\n      ProcessGroupUCCTest.cpp\n      LINK_LIBRARIES torch_cpu c10d_cuda_test gtest_main __caffe2_ucc INSTALL_TEST ${INSTALL_TEST})\n    if(INSTALL_TEST)\n      install(TARGETS c10d_cuda_test DESTINATION lib)\n    endif()\n  endif()\nelse()\n  if(USE_GLOO AND USE_C10D_GLOO)\n    c10d_add_test(ProcessGroupGlooTest.cpp LINK_LIBRARIES torch_cpu gtest_main INSTALL_TEST OFF)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Profiling Convolution Operations in PyTorch\nDESCRIPTION: A detailed log of convolution operations in a neural network, showing tensor shapes, kernel sizes, strides, padding, and other parameters. The format shows operation count, input tensors, and optional parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([64, 256, 32, 32], f16), T([256, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([64, 256, 1, 1], f16), T([16, 256, 1, 1], f16), T([16], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([64, 16, 1, 1], f16), T([256, 16, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([64, 256, 16, 16], f16), T([1024, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 512, 32, 32], f16), T([1024, 512, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Inference Benchmark with Command-line Arguments\nDESCRIPTION: Example command for running the inference benchmark with specific parameters. The command ignores warnings, sets 1000 iterations and a batch size of a 32.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -W ignore server.py --num_iters 1000 --batch_size 32\n```\n\n----------------------------------------\n\nTITLE: Implementing numpy_sort Function with PyTorch vmap\nDESCRIPTION: This snippet defines a numpy_sort function that uses a custom NumpySort operation and demonstrates its usage with torch.vmap for vectorized operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.func.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result\n\nx = torch.randn(2, 3)\nresult = torch.vmap(numpy_sort)(x)\nassert torch.allclose(result, numpy_sort(result, 1))\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Lazy TS Backend Test Sources in CMake\nDESCRIPTION: Checks if the `BUILD_LAZY_TS_BACKEND` CMake option is enabled. If true, it appends additional source files related to lazy operations (`test_lazy_ops.cpp`, `test_lazy_ops_util.cpp`) to the `LAZY_TEST_SRCS` list.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_LAZY_TS_BACKEND)\n    list(APPEND LAZY_TEST_SRCS\n      ${LAZY_TEST_ROOT}/test_lazy_ops.cpp\n      ${LAZY_TEST_ROOT}/test_lazy_ops_util.cpp\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up C10 CUDA Tests and Installation\nDESCRIPTION: Configures test directory and installation rules for the C10 CUDA headers and library. This ensures proper installation of all header files and the generated CMake macros header.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/cuda/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(test)\n\nforeach(file ${C10_CUDA_HEADERS})\n  get_filename_component( dir ${file} DIRECTORY )\n  install( FILES ${file} DESTINATION include/c10/cuda/${dir} )\nendforeach()\ninstall(FILES ${CMAKE_BINARY_DIR}/c10/cuda/impl/cuda_cmake_macros.h\n  DESTINATION include/c10/cuda/impl)\n\nif(MSVC AND C10_CUDA_BUILD_SHARED_LIBS)\n  install(FILES $<TARGET_PDB_FILE:c10_cuda> DESTINATION lib OPTIONAL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Executing Specific Test Cases with Pytest Shell\nDESCRIPTION: This shell command specifies the execution of particular test cases within the Python test script using pytest. The '-s' flag outputs stdout/stderr directly to the console, and the '-k' flag allows for filtering to only run the matched test case. Pytest must be installed to execute the command.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/distributed/tensor/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npytest test/distributed/tensor/test_dtensor.py -s -k test_from_local\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.sigmoid_backward.default Sigmoid Backward Pass in PyTorch ATen\nDESCRIPTION: Documents observed calls to the backward pass for the Sigmoid activation function (`aten.sigmoid_backward.default`) in PyTorch ATen. It lists the different gradient output and original output tensor shapes (all f16) and their respective call counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_19\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sigmoid_backward.default\ncnt: 9, ((T([128, 1536, 1, 1], f16), T([128, 1536, 1, 1], f16)), {})\ncnt: 2, ((T([128, 512, 1, 1], f16), T([128, 512, 1, 1], f16)), {})\ncnt: 1, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Loss Calculation Operations\nDESCRIPTION: NLL (Negative Log Likelihood) loss forward and backward calculations, typically used with classification tasks. Includes operations on tensors with shape [32, 1000].\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convmixer_768_32_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\naten.nll_loss_forward.default((T([32, 1000], f16), T([32], i64), None, 1, -100), {})\naten.nll_loss_backward.default((T([], f16), T([32, 1000], f16), T([32], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet demonstrates batch normalization operations with various input tensor shapes, running mean and variance, and learning rate parameters. These operations are crucial for normalizing the activations in deep neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hrnet_w18_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Executing Log Softmax in PyTorch\nDESCRIPTION: Performs the log of the softmax function on a given tensor with specified dimensions. This function requires the PyTorch library and operates on tensor inputs of FP16 precision. The main parameters include the tensor to operate on and the dimension over which the softmax is computed. The output is a log-softmaxed tensor, suitable for applications such as neural network output transformation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\naten._log_softmax.default(T([8192, 10000], f16), 1, False)\n```\n\n----------------------------------------\n\nTITLE: Build Options Configuration\nDESCRIPTION: Defines various build options and flags for PyTorch compilation, including Python bindings, test builds, and hardware-specific optimizations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\noption(ATEN_NO_TEST \"Do not build ATen test binaries\" OFF)\noption(BUILD_BINARY \"Build C++ binaries\" OFF)\noption(BUILD_CUSTOM_PROTOBUF \"Build and use Caffe2's own protobuf under third_party\" ON)\noption(BUILD_PYTHON \"Build Python binaries\" ON)\noption(BUILD_SHARED_LIBS \"Build libcaffe2.so\" ON)\n```\n\n----------------------------------------\n\nTITLE: Tensor Multiplication Operations in PyTorch\nDESCRIPTION: Profiling data for tensor multiplication operations showing count and tensor shapes. These elementwise multiplications are used throughout the network, including operations for squeeze-and-excitation blocks where channel attention is applied.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 2, ((T([32, 32, 112, 112], f16), T([32, 32, 1, 1], f16)), {})\ncnt: 2, ((T([32, 96, 56, 56], f16), T([32, 96, 1, 1], f16)), {})\ncnt: 2, ((T([32, 144, 56, 56], f16), T([32, 144, 1, 1], f16)), {})\ncnt: 2, ((T([32, 144, 28, 28], f16), T([32, 144, 1, 1], f16)), {})\ncnt: 2, ((T([32, 240, 28, 28], f16), T([32, 240, 1, 1], f16)), {})\ncnt: 2, ((T([32, 240, 14, 14], f16), T([32, 240, 1, 1], f16)), {})\ncnt: 6, ((T([32, 480, 14, 14], f16), T([32, 480, 1, 1], f16)), {})\ncnt: 4, ((T([32, 672, 14, 14], f16), T([32, 672, 1, 1], f16)), {})\ncnt: 2, ((T([32, 672, 7, 7], f16), T([32, 672, 1, 1], f16)), {})\ncnt: 8, ((T([32, 1152, 7, 7], f16), T([32, 1152, 1, 1], f16)), {})\ncnt: 4, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16)), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 7, 7], f16)), {})\ncnt: 2, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16)), {})\ncnt: 3, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16)), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), T([32, 240, 14, 14], f16)), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([32, 240, 28, 28], f16)), {})\ncnt: 1, ((T([32, 144, 28, 28], f16), T([32, 144, 28, 28], f16)), {})\ncnt: 1, ((T([32, 144, 56, 56], f16), T([32, 144, 56, 56], f16)), {})\ncnt: 1, ((T([32, 96, 56, 56], f16), T([32, 96, 56, 56], f16)), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Handling ATen Threshold Backward in PyTorch: Python\nDESCRIPTION: The snippet provides insights into using ATen's 'threshold_backward' operator in PyTorch, showcasing tensor threshold functions in backward computations. It denotes how tensors with certain configurations and strides are processed to achieve desired thresholding behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 4, ((T([3, 64, 512, 512], f16), T([3, 64, 512, 512], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Performing Threshold Backward Operations in ATen\nDESCRIPTION: Computes gradients of inputs from forward threshold functions by leveraging a set threshold value. The operation requires input tensors and associated gradients, with a threshold value of zero. Outputs are gradient tensors indicating where backpropagation would affect lowering computation paths.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 2, ((T([16, 64, 128, 128], f16), T([16, 64, 128, 128], f16), 0), {})\ncnt: 2, ((T([16, 128, 64, 64], f16), T([16, 128, 64, 64], f16), 0), {})\ncnt: 7, ((T([16, 256, 32, 32], f16), T([16, 256, 32, 32], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Nearest Neighbor Upsampling in PyTorch\nDESCRIPTION: This snippet performs nearest neighbor upsampling on 2D tensors. It's commonly used in computer vision tasks to increase the spatial dimensions of feature maps or images.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 256, 12, 16], f16), None, [2.0, 2.0]), {})\ncnt: 1, ((T([8, 128, 24, 32], f16), None, [2.0, 2.0]), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization Backward Operations\nDESCRIPTION: This snippet shows backward operations for batch normalization on tensors with various shapes in f16 precision. Each operation includes input, gradient, scale, bias, running statistics, and flags for computing gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([128, 1984, 7, 7], f16), T([128, 1984, 7, 7], f16), T([1984], f16), T([1984], f16), T([1984], f16), T([1984], f32), T([1984], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 352, 7, 7], f16), T([128, 352, 7, 7], f16), T([352], f16), T([352], f16), T([352], f16), T([352], f32), T([352], f32), True, 1e-05, [True, True, True]), {})\ncnt: 8, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16), T([1104], f16), T([1104], f16), T([1104], f16), T([1104], f32), T([1104], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 184, 7, 7], f16), T([128, 184, 7, 7], f16), T([184], f16), T([184], f16), T([184], f16), T([184], f32), T([184], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([128, 672, 14, 14], f16), T([128, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 112, 14, 14], f16), T([128, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f32), T([112], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 336, 14, 14], f16), T([128, 336, 14, 14], f16), T([336], f16), T([336], f16), T([336], f16), T([336], f32), T([336], f32), True, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f32), T([384], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 64, 14, 14], f16), T([128, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 192, 14, 14], f16), T([128, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 32, 28, 28], f16), T([128, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 96, 28, 28], f16), T([128, 96, 28, 28], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([128, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})\ncnt: 7, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([128, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 96, 112, 112], f16), T([128, 96, 112, 112], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.nll_loss_backward.default Calls - PyTorch - Python\nDESCRIPTION: Provides argument patterns for the backward negative log-likelihood loss operator (aten.nll_loss_backward.default), including tensors and scalar arguments. Dependencies are correct tensor types and shapes for loss gradients, logits, targets, and ignore index. Inputs must match PyTorch NLL loss specifications; the output is the gradient tensor, aiding in profiling training steps.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.native_batch_norm.default in PyTorch ATen\nDESCRIPTION: Logs calls to the `aten.native_batch_norm.default` operator, which performs the forward pass of batch normalization. The arguments include the input tensor (4D, f16), weight (1D, f16), bias (1D, f16), running mean (1D, f16), running variance (1D, f16), a boolean indicating training mode (`True`), momentum factor (`0.1`), and epsilon (`1e-05`). The `cnt` shows how many times each specific combination of tensor shapes and parameters was called.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([128, 8, 112, 112], f16), T([8], f16), T([8], f16), T([8], f16), T([8], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 16, 56, 56], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 32, 56, 56], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 32, 28, 28], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 64, 28, 28], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 64, 14, 14], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 11, ((T([128, 128, 14, 14], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 128, 7, 7], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 256, 7, 7], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Unsafe View Operation in PyTorch\nDESCRIPTION: Utilizing aten._unsafe_view.default to reshape a tensor from [1, 512, 24, 64] to [1, 512, 1536], all in float16 precision. This operator allows for reshaping without enforcing memory safety, often used in performance-critical sections.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 48, ((T([1, 512, 24, 64], f16), [1, 512, 1536]), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten._log_softmax Operator - PyTorch - Python\nDESCRIPTION: This snippet documents the invocation of the aten._log_softmax operator, which computes the logarithm of the softmax over a specified dimension of a floating-point tensor of shape [4, 2] (dtype: f16). There are no additional kwargs. Key parameters include the input tensor, axis, and boolean for contiguous memory. The output is a log-probability tensor of the same shape. Dependencies: torch, with half-precision support. Limitations: f16 precision and specific axis constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([4, 2], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Shape Analysis - Basic Stats\nDESCRIPTION: Documentation of tensor operations showing tensor shapes with f16 precision. Operations mainly work with 4D tensors (batch_size=4) with varying channel dimensions and spatial dimensions of 7x7, 14x14, 28x28, and 56x56.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 768, 14, 14], f16), T([4, 768, 14, 14], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f32), T([768], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: General Schema for Tensor Creation in PyTorch C++\nDESCRIPTION: This snippet shows the general schema for creating tensors using factory functions in PyTorch C++. It includes placeholders for the function name, function-specific options, sizes, and tensor options.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::<function-name>(<function-specific-options>, <sizes>, <tensor-options>)\n```\n\n----------------------------------------\n\nTITLE: Using MaybeOwned<Tensor> in Tensor::expect_contiguous method\nDESCRIPTION: A canonical example of MaybeOwned<Tensor> usage in a Tensor's expect_contiguous method. The method returns a borrowed self-reference when the tensor is already contiguous and an owned reference to a newly created contiguous tensor otherwise.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/maybe_owned.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ninline c10::MaybeOwned<Tensor> Tensor::expect_contiguous(MemoryFormat memory_format) const & {\n  if (is_contiguous(memory_format)) {\n    return c10::MaybeOwned<Tensor>::borrowed(*this);\n  } else {\n    return c10::MaybeOwned<Tensor>::owned(__dispatch_contiguous(memory_format));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization in PyTorch\nDESCRIPTION: Batch normalization operations with input tensors, running mean and variance, and learnable parameters. These are used to normalize activations and improve training stability.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 24, 128, 128], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking Tensor Shapes and Operations in PyTorch Model\nDESCRIPTION: This log captures tensor shapes, counts, and operations during a PyTorch model execution. Each line records the count of occurrences, tensor shapes with data type (f16 for float16), and operation parameters. The log appears to track operations for a convolutional neural network with decreasing spatial dimensions (from 112x112 to 7x7).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([64, 608, 14, 14], f16),), {})\ncnt: 1, ((T([64, 640, 14, 14], f16),), {})\ncnt: 1, ((T([64, 672, 14, 14], f16),), {})\ncnt: 1, ((T([64, 704, 14, 14], f16),), {})\ncnt: 1, ((T([64, 736, 14, 14], f16),), {})\ncnt: 1, ((T([64, 768, 14, 14], f16),), {})\ncnt: 1, ((T([64, 800, 14, 14], f16),), {})\ncnt: 1, ((T([64, 832, 14, 14], f16),), {})\ncnt: 1, ((T([64, 864, 14, 14], f16),), {})\ncnt: 1, ((T([64, 896, 14, 14], f16),), {})\ncnt: 1, ((T([64, 928, 14, 14], f16),), {})\ncnt: 1, ((T([64, 960, 14, 14], f16),), {})\ncnt: 1, ((T([64, 992, 14, 14], f16),), {})\ncnt: 1, ((T([64, 1024, 14, 14], f16),), {})\ncnt: 1, ((T([64, 512, 7, 7], f16),), {})\ncnt: 16, ((T([64, 128, 7, 7], f16),), {})\ncnt: 1, ((T([64, 544, 7, 7], f16),), {})\ncnt: 1, ((T([64, 576, 7, 7], f16),), {})\ncnt: 1, ((T([64, 608, 7, 7], f16),), {})\ncnt: 1, ((T([64, 640, 7, 7], f16),), {})\ncnt: 1, ((T([64, 672, 7, 7], f16),), {})\ncnt: 1, ((T([64, 704, 7, 7], f16),), {})\ncnt: 1, ((T([64, 736, 7, 7], f16),), {})\ncnt: 1, ((T([64, 768, 7, 7], f16),), {})\ncnt: 1, ((T([64, 800, 7, 7], f16),), {})\ncnt: 1, ((T([64, 832, 7, 7], f16),), {})\ncnt: 1, ((T([64, 864, 7, 7], f16),), {})\ncnt: 1, ((T([64, 896, 7, 7], f16),), {})\ncnt: 1, ((T([64, 928, 7, 7], f16),), {})\ncnt: 1, ((T([64, 960, 7, 7], f16),), {})\ncnt: 1, ((T([64, 992, 7, 7], f16),), {})\ncnt: 1, ((T([64, 1024, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Threshold Backward Operations\nDESCRIPTION: Details the backward pass of threshold operations (aten.threshold_backward.default) across various tensor shapes and strides, typically used in gradient computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 3, ((T([128, 2048, 7, 7], f16), T([128, 2048, 7, 7], f16), 0), {})\ncnt: 9, ((T([128, 112, 7, 7], f16, stride=(43904, 49, 7, 1)), T([128, 112, 7, 7], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Compiling MPS Metal Shaders to AIR and Metallib in CMake\nDESCRIPTION: This snippet executes if `USE_MPS` and `CAN_COMPILE_METAL` are both true. It iterates through Metal shader files (`native_mps_metal`), compiles each to two AIR (Apple Intermediate Representation) files (one for Metal 3.0, one for Metal 3.1 with BFloat support), and then links these AIR files into two Metal libraries (`kernels_basic.metallib`, `kernels_bfloat.metallib`). A custom command and target (`metallibs`) are used to manage dependencies and trigger the compilation/linking process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_21\n\nLANGUAGE: cmake\nCODE:\n```\n    if(CAN_COMPILE_METAL)\n        foreach(SHADER ${native_mps_metal})\n            cmake_path(GET SHADER STEM TGT_STEM)\n            string(CONCAT TGT_BASIC ${TGT_STEM} \"_30.air\")\n            string(CONCAT TGT_BFLOAT ${TGT_STEM} \"_31.air\")\n            list(APPEND AIR_BASIC ${TGT_BASIC})\n            list(APPEND AIR_BFLOAT ${TGT_BFLOAT})\n            metal_to_air(${SHADER} ${TGT_BASIC} \"-std=metal3.0\")\n            metal_to_air(${SHADER} ${TGT_BFLOAT} \"-std=metal3.1\")\n        endforeach()\n        air_to_metallib(kernels_basic.metallib ${AIR_BASIC})\n        air_to_metallib(kernels_bfloat.metallib ${AIR_BFLOAT})\n        add_custom_command(\n                          COMMAND echo \"// $$(date)\" > metallib_dummy.cpp\n                          DEPENDS kernels_basic.metallib kernels_bfloat.metallib\n                          OUTPUT metallib_dummy.cpp\n                          COMMENT \"Updating metallibs timestamp\")\n        add_custom_target(metallibs DEPENDS kernels_basic.metallib kernels_bfloat.metallib metallib_dummy.cpp)\n```\n\n----------------------------------------\n\nTITLE: Training DLRM Model with Criteo Dataset in Bash\nDESCRIPTION: Bash script to train the DLRM model using the Criteo Kaggle dataset. It saves the trained model and processes the dataset for further use.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/benchmarks/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./bench/dlrm_s_criteo_kaggle.sh --save-model=./models/criteo_model.ckpt [--use-gpu]\n```\n\n----------------------------------------\n\nTITLE: Threshold Backward Operation in PyTorch\nDESCRIPTION: This snippet shows the backward operation for a threshold function, which is typically used in ReLU activations. It operates on 4D tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 3, ((T([64, 64, 112, 112], f16), T([64, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch CUDA Environment Variables in reStructuredText\nDESCRIPTION: This snippet defines a table of PyTorch-specific CUDA environment variables using reStructuredText syntax. It includes variables for memory caching, allocation configuration, CUDA checks, cuDNN settings, and NCCL behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cuda_environment_variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. list-table::\n  :header-rows: 1\n\n  * - Variable\n    - Description\n  * - ``PYTORCH_NO_CUDA_MEMORY_CACHING``\n    - If set to ``1``, disables caching of memory allocations in CUDA. This can be useful for debugging.\n  * - ``PYTORCH_CUDA_ALLOC_CONF``\n    - For a more in depth explanation of this environment variable, see :ref:`cuda-memory-management`.\n  * - ``PYTORCH_NVML_BASED_CUDA_CHECK``\n    - If set to ``1``, before importing PyTorch modules that check if CUDA is available, PyTorch will use NVML to check if the CUDA driver is functional instead of using the CUDA runtime. This can be helpful if forked processes fail with a CUDA initialization error.\n  * - ``TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT``\n    - The cache limit for the cuDNN v8 API. This is used to limit the memory used by the cuDNN v8 API. The default value is 10000, which roughly corresponds to 2GiB assuming 200KiB per ExecutionPlan. Set to ``0`` for no limit or a negative value for no caching.\n  * - ``TORCH_CUDNN_V8_API_DISABLED``\n    - If set to ``1``, disables the cuDNN v8 API. And will fall back to the cuDNN v7 API.\n  * - ``TORCH_ALLOW_TF32_CUBLAS_OVERRIDE``\n    - If set to ``1``, forces TF32 enablement, overrides ``set_float32_matmul_precision`` setting.\n  * - ``TORCH_NCCL_USE_COMM_NONBLOCKING``\n    - If set to ``1``, enables non-blocking error handling in NCCL.\n  * - ``TORCH_NCCL_AVOID_RECORD_STREAMS``\n    - If set to ``0``, enables fallback to record streams-based synchronization behavior in NCCL.\n  * - ``TORCH_CUDNN_V8_API_DEBUG``\n    - If set to ``1``, sanity check whether cuDNN V8 is being used.\n```\n\n----------------------------------------\n\nTITLE: Generated JIT Graph IR\nDESCRIPTION: The intermediate representation (IR) graph generated from the TorchScript function, showing how the code is transformed into a sequence of primitive operations with explicit value tracking.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ngraph(%0 : Double(2),\n      %1 : Double(2)):\n  %2 : int = prim::Constant[value=1]()\n  %3 : Double(2) = aten::add(%0, %1, %2)\n  %4 : Double(2) = aten::mul(%3, %3)\n  %5 : Double(2) = aten::mul(%4, %3)\n  %6 : Double(2) = aten::tanh(%5)\n  %7 : Double(2) = aten::add(%6, %6, %2)\n  %8 : Double(2) = aten::add(%5, %7, %2)\n  return (%8)\n```\n\n----------------------------------------\n\nTITLE: Listing Example Inputs for aten.upsample_bilinear2d.vec Operator - PyTorch - Python\nDESCRIPTION: This code shows representative input arguments for the aten.upsample_bilinear2d.vec operator, which upsamples spatial tensor data using bilinear interpolation. Each tuple specifies an input tensor (shape and data type), a placeholder (None) for deprecated or optional argument, the boolean align_corners (True), and the upsampling scale factor ([2.0, 2.0]). This setup helps users or test systems validate upsampling operator behavior for typical deep learning feature map sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_unet_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.upsample_bilinear2d.vec\ncnt: 1, ((T([1, 512, 40, 59], f16), None, True, [2.0, 2.0]), {})\ncnt: 1, ((T([1, 256, 80, 119], f16), None, True, [2.0, 2.0]), {})\ncnt: 1, ((T([1, 128, 160, 239], f16), None, True, [2.0, 2.0]), {})\ncnt: 1, ((T([1, 64, 320, 479], f16), None, True, [2.0, 2.0]), {})\n```\n\n----------------------------------------\n\nTITLE: Cloning Tensors in PyTorch - Python\nDESCRIPTION: Demonstrates the usage of aten.clone.default to create copies of existing tensors while preserving the original tensor's state. Useful to prevent unintentional modifications. Requires original tensor to be defined, returns a new tensor with identical content.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([32, 3, 224, 224], f16),), {})\ncnt: 1, ((T([32, 16, 112, 112], f16),), {})\ncnt: 1, ((T([32, 240, 28, 28], f16),), {})\ncnt: 1, ((T([32, 240, 14, 14], f16),), {})\ncnt: 2, ((T([32, 200, 14, 14], f16),), {})\ncnt: 4, ((T([32, 184, 14, 14], f16),), {})\ncnt: 2, ((T([32, 480, 14, 14], f16),), {})\ncnt: 3, ((T([32, 672, 14, 14], f16),), {})\ncnt: 1, ((T([32, 672, 7, 7], f16),), {})\ncnt: 5, ((T([32, 960, 7, 7], f16),), {})\ncnt: 1, ((T([32, 1280], f16),), {})\n\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies for PyTorch\nDESCRIPTION: Requirements file that specifies the exact versions of Python packages needed for building PyTorch documentation. Includes Sphinx documentation generator, custom PyTorch theme, KaTeX for math rendering, visualization libraries like matplotlib and tensorboard, and various Sphinx extensions for enhanced documentation features.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsphinx==5.3.0\n-e git+https://github.com/pytorch/pytorch_sphinx_theme.git@pytorch_sphinx_theme2#egg=pytorch_sphinx_theme2\nsphinxcontrib.katex==0.8.6\nmatplotlib==3.6.0\ntensorboard==2.10.0\npython-etcd==0.4.5\nsphinx-copybutton==0.5.0\nmyst-parser==4.0.1\nsphinx-design==0.6.1\nsphinxcontrib-mermaid==1.0.0\n```\n\n----------------------------------------\n\nTITLE: Checking Timer Expiration in PyTorch Distributed Elastic\nDESCRIPTION: Function to check if a timer has expired. It is part of the client methods for interacting with the timer system.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ntorch.distributed.elastic.timer.expires\n```\n\n----------------------------------------\n\nTITLE: Meta-Fake Implementation of PyTorch Operator: meta_index_select (Python)\nDESCRIPTION: This function provides a meta (fake) implementation for the torch.index_select operator for tracing with FakeTensor. It computes the output tensor's shape using only metadata (not data), adjusting the specified dim to match the number of indices and creates an empty tensor of the resulting size. Dependencies include PyTorch, specifically for new_empty and tensor metadata APIs. Arguments are dim (dimension to select) and index (tensor of indices). The input self must be a compatible FakeTensor. The output is a new empty tensor with the correct meta shape. Intended for internal/tracing use only.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef meta_index_select(self, dim, index):\n    result_size = list(self.size())\n    if self.dim() > 0:\n        result_size[dim] = index.numel()\n    return self.new_empty(result_size)\n```\n\n----------------------------------------\n\nTITLE: Executing CUDNN RNN Backward Pass in PyTorch\nDESCRIPTION: Performs a backward pass of a CUDNN RNN layer, computing gradients with respect to inputs and weights. Uses half-precision (f16) tensors and includes workspace and reserve space parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\naten._cudnn_rnn_backward.default(T([64, 50, 256], f16), [T([3072, 256], f16), T([3072, 768], f16), T([3072], f16), T([3072], f16)], 4, T([3151872], f16), T([1, 64, 768], f16), T([1, 64, 768], f16), T([64, 50, 768], f16, stride=(768, 49152, 1)), T([64, 50, 768], f16), None, None, 2, 768, 0, 1, True, 0.0, True, False, [], None, T([24576016], u8), [True, False, False, True])\n```\n\n----------------------------------------\n\nTITLE: Defining GoogleTest-based JIT C++ Tests in PyTorch - C++\nDESCRIPTION: This snippet demonstrates the setup of unit tests for PyTorch's JIT component using the GoogleTest framework in C++. It shows the inclusion of GoogleTest headers, usage of the 'torch::jit' namespace, and defines test cases using the TEST macro. Variants such as '_CUDA' and '_MultiCUDA' suffixes for test filtering based on GPU hardware are illustrated. Required dependency: GoogleTest development libraries and PyTorch source tree. Input consists of custom test logic and outputs standardized GoogleTest test execution results. Tests are skipped based on system hardware support as indicated by their names.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <gtest/gtest.h>\n\nusing namespace ::torch::jit\n\nTEST(FooTest, BarBaz) {\n   // ...\n}\n\n// Append '_CUDA' to the test case name will automatically filter it out if CUDA\n// is not compiled.\nTEST(FooTest, NeedsAGpu_CUDA) {\n   // ...\n}\n\n// Similarly, if only one GPU is detected, tests with `_MultiCUDA` at the end\n// will not be run.\nTEST(FooTest, NeedsMultipleGpus_MultiCUDA) {\n   // ...\n}\n\n```\n\n----------------------------------------\n\nTITLE: Scalar Division Operation in DenseNet Global Average Pooling\nDESCRIPTION: This snippet shows the scalar division operation used in DenseNet's global average pooling. It divides a tensor of shape [32, 1024, 7, 7] by 49 (the spatial dimensions 77), effectively computing the mean across spatial dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([32, 1024, 7, 7], f16, stride=(1024, 1, 0, 0)), 49), {})\n```\n\n----------------------------------------\n\nTITLE: Traced Function Calling Scripted Function with Autocast\nDESCRIPTION: Example showing that calling a scripted function from a traced function with autocast enabled is not supported in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.cpu.amp import autocast\n\n@torch.jit.script\ndef fn(a, b):\n    return torch.mm(a, b)\n\ndef traced(a, b):\n    with autocast(enabled=True):\n        return fn(a, b)\n\n# running TorchScript with Autocast enabled is not supported\ntorch.jit.trace(traced, (x, y))\n```\n\n----------------------------------------\n\nTITLE: Tensor Multiplication Operations in PyTorch\nDESCRIPTION: This snippet shows tensor multiplication operations with scalar values. It includes operations on tensors of various shapes and dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 4, ((T([64, 6, 196, 9, 9], f16, stride=(95256, 81, 486, 9, 1)), 0.1767766952966369), {})\ncnt: 28, ((T([64, 12, 196, 196], f16), 0.1767766952966369), {})\ncnt: 4, ((T([64, 12, 1, 32], f16), 0.1767766952966369), {})\ncnt: 2, ((T([64, 1000], f16), 0.5), {})\ncnt: 4, ((T([64, 6, 196, 9, 9], f16), 0.1767766952966369), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Copy Operation\nDESCRIPTION: This snippet shows a tensor copy operation, where a tensor of shape [128, 3, 224, 224] with f16 precision is copied to another tensor of the same shape.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Sparsifying Training Data with Data Sparsifier in PyTorch\nDESCRIPTION: Example demonstrating how to sparsify input training data before passing it to a model. This approach can be useful when the input data itself can benefit from sparsification.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = SomeModel()\n\ndata_sparsifier = ImplementedDataSparsifier(threshold=0.2)\n\ndata_name = 'train_data'\n\nfor x, y in train_data_loader:\n    x = data_sparsifier.add_data(name=data_name, data=x)\n    ...\n    y_out = model(x)\n    ...\n    data_sparsifier.step()\n```\n\n----------------------------------------\n\nTITLE: Examining Guard Conditions in Dynamo Compiled Functions\nDESCRIPTION: This snippet shows how to inspect the guarding conditions of a compiled function directly without examining bytecode. It iterates through the code parts of the guard function to display the conditions that must be satisfied for the compiled code to be executed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_dynamo_overview.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor code_part in guard.code_parts:\n    print(code_part)\n```\n\n----------------------------------------\n\nTITLE: Installing functorch Development Dependencies\nDESCRIPTION: Bash commands for installing additional requirements for AOTAutograd and testing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install networkx\n```\n\n----------------------------------------\n\nTITLE: Building with Debug and Custom Flags\nDESCRIPTION: This bash snippet shows how to customize a PyTorch build using environment variables for specific requirements such as enabling debug mode and disabling CUDA support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\nDEBUG=1 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Analyzing Forward Convolution Operations in PyTorch Neural Network\nDESCRIPTION: This snippet details the forward convolution operations used throughout the model, showing kernel sizes, stride values, padding, and tensor shapes. The network uses a mix of 1x1, 3x3, and 5x5 convolutions with various channel configurations, typical of efficient CNN architectures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 192, 192], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 96, 96], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})\ncnt: 1, ((T([128, 32, 1, 1], f16), T([8, 32, 1, 1], f16), T([8], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 8, 1, 1], f16), T([32, 8, 1, 1], f16), T([32], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 96, 96], f16), T([16, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 16, 96, 96], f16), T([96, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 96, 96], f16), T([96, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 96), {})\ncnt: 1, ((T([128, 96, 1, 1], f16), T([4, 96, 1, 1], f16), T([4], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 4, 1, 1], f16), T([96, 4, 1, 1], f16), T([96], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 48, 48], f16), T([24, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 24, 48, 48], f16), T([144, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 144, 48, 48], f16), T([144, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 144), {})\ncnt: 2, ((T([128, 144, 1, 1], f16), T([6, 144, 1, 1], f16), T([6], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 6, 1, 1], f16), T([144, 6, 1, 1], f16), T([144], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 144, 48, 48], f16), T([24, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 144, 48, 48], f16), T([144, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 144), {})\ncnt: 1, ((T([128, 144, 24, 24], f16), T([40, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 40, 24, 24], f16), T([240, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 240, 24, 24], f16), T([240, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 240), {})\ncnt: 2, ((T([128, 240, 1, 1], f16), T([10, 240, 1, 1], f16), T([10], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 10, 1, 1], f16), T([240, 10, 1, 1], f16), T([240], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 240, 24, 24], f16), T([40, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 240, 24, 24], f16), T([240, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 240), {})\ncnt: 1, ((T([128, 240, 12, 12], f16), T([80, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 80, 12, 12], f16), T([480, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 480, 12, 12], f16), T([480, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 480), {})\ncnt: 4, ((T([128, 480, 1, 1], f16), T([20, 480, 1, 1], f16), T([20], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 20, 1, 1], f16), T([480, 20, 1, 1], f16), T([480], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 480, 12, 12], f16), T([80, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 480, 12, 12], f16), T([480, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 480), {})\ncnt: 1, ((T([128, 480, 12, 12], f16), T([112, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 112, 12, 12], f16), T([672, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 672, 12, 12], f16), T([672, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 4, ((T([128, 672, 1, 1], f16), T([28, 672, 1, 1], f16), T([28], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 28, 1, 1], f16), T([672, 28, 1, 1], f16), T([672], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 672, 12, 12], f16), T([112, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 672, 12, 12], f16), T([672, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 1, ((T([128, 672, 6, 6], f16), T([192, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 192, 6, 6], f16), T([1152, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 1152, 6, 6], f16), T([1152, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 1152), {})\ncnt: 5, ((T([128, 1152, 1, 1], f16), T([48, 1152, 1, 1], f16), T([48], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([128, 48, 1, 1], f16), T([1152, 48, 1, 1], f16), T([1152], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 1152, 6, 6], f16), T([192, 1152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1152, 6, 6], f16), T([1152, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1152), {})\ncnt: 1, ((T([128, 1152, 6, 6], f16), T([320, 1152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 320, 6, 6], f16), T([1280, 320, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Conditional Compilation for CUDA/ROCm in CMake\nDESCRIPTION: Configures the `test_lazy` target based on the available GPU backend. If `USE_CUDA` is true, it adds the `USE_CUDA` compile definition. If `USE_ROCM` is true, it links ROCm-specific libraries (`hiprtc`, `amdhip64`, `${TORCH_CUDA_LIBRARIES}`) and adds the `USE_ROCM` compile definition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_CUDA)\n  target_compile_definitions(test_lazy PRIVATE USE_CUDA)\nelseif(USE_ROCM)\n  target_link_libraries(test_lazy PRIVATE\n    hiprtc::hiprtc\n    hip::amdhip64\n    ${TORCH_CUDA_LIBRARIES})\n\n  target_compile_definitions(test_lazy PRIVATE USE_ROCM)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Computing In-Place ReLU Activation in PyTorch\nDESCRIPTION: The relu_ operator in PyTorch applies the ReLU activation function in-place, modifying the input tensor directly to retain positive values and zeroing the negatives. This in-place strategy saves memory but requires returned variables to be non-differentiable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([32, 64, 112, 112], f16),), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([32, 128, 112, 112], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.addmm.default Calls in PyTorch Text Trace\nDESCRIPTION: Presents invocation details for the aten.addmm.default operator, typically used for a fully-connected (linear) layer combining a bias tensor and a matrix-matrix multiplication. The entry encodes the tensor shapes, memory stride info, and datatype (f16). This format is suitable for reviewing memory access patterns and linear layer characteristics in traced deep learning models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([32, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring MPS Profiler Profile/Signpost Options (Environment Variable)\nDESCRIPTION: Sets the profile and signpost bitmasks for the `MPSProfiler` using `PYTORCH_MPS_TRACE_SIGNPOSTS`. Available options can be found in the `ProfileOptions` and `SignpostTypes` enums within the `aten/src/ATen/mps/MPSProfiler.h` header file.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nPYTORCH_MPS_TRACE_SIGNPOSTS\n```\n\n----------------------------------------\n\nTITLE: Creating Standard Benchmarking Binary Targets\nDESCRIPTION: Defines binary targets for speed benchmarking and model comparison tools used in standard (non-mobile) builds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/binaries/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncaffe2_binary_target(\"speed_benchmark_torch.cc\")\ncaffe2_binary_target(\"compare_models_torch.cc\")\n```\n\n----------------------------------------\n\nTITLE: Adding Include Directories for Lazy Tests in CMake\nDESCRIPTION: Specifies the necessary include directories for compiling the `test_lazy` target using `target_include_directories`. It adds the ATen CPU include path (`${ATen_CPU_INCLUDE}`) privately.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(test_lazy PRIVATE ${ATen_CPU_INCLUDE})\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Timer Client in PyTorch Distributed Elastic\nDESCRIPTION: Base class for implementing custom timer clients. Extend this class to create your own timer client implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nTimerClient\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization Operations\nDESCRIPTION: Layer normalization operations with different tensor shapes and epsilon values. Includes both forward and backward passes with corresponding gradient computations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\naten.native_layer_norm.default((T([32, 3136, 64], f16), [64], T([64], f16), T([64], f16), 1e-05))\n```\n\n----------------------------------------\n\nTITLE: PyTorch ReLU Activation Operations\nDESCRIPTION: ReLU activation operations applied to tensors of various sizes, including both in-place and out-of-place operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_xception65_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n((T([32, 256, 38, 38], f16),), {})\n((T([32, 728, 19, 19], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing aten.sum.SymInt Operations in PyTorch\nDESCRIPTION: This code snippet shows various configurations of the aten.sum.SymInt operator in PyTorch. It includes tensor shapes, dimensions for summation, and whether to keep dimensions. The operations are primarily on f16 (float16) tensors with different shapes and summation axes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\ncnt: 3, ((T([64, 1584, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([64, 960, 7, 7], f16), [2, 3], True), {})\ncnt: 3, ((T([64, 480, 14, 14], f16), [2, 3], True), {})\ncnt: 4, ((T([64, 624, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([64, 336, 14, 14], f16), [2, 3], True), {})\ncnt: 3, ((T([64, 336, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([64, 240, 28, 28], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building PyTorch Mobile Model Tracer in CMake\nDESCRIPTION: This CMake snippet configures the build process for the PyTorch mobile model tracer tool. It sets the source directory path, collects source files, defines the executable target, links it with the torch library, and specifies installation destination.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/mobile/model_tracer/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(\n  MODEL_TRACER_DIR\n  \"${TORCH_ROOT}/torch/csrc/jit/mobile/model_tracer\")\n\nlist(APPEND MODEL_TRACER_SOURCES \"\")\n\nappend_filelist(\"torch_mobile_tracer_sources\" MODEL_TRACER_SOURCES)\n\nadd_executable(\n  model_tracer\n  ${MODEL_TRACER_SOURCES})\n\ntarget_link_libraries(model_tracer PRIVATE torch)\n\ninstall(TARGETS model_tracer DESTINATION bin)\n```\n\n----------------------------------------\n\nTITLE: Optional Installation of Lazy Test Executable in CMake\nDESCRIPTION: Conditionally installs the `test_lazy` executable if the `INSTALL_TEST` CMake option is enabled. It sets the `INSTALL_RPATH` property for better library discovery on installation and uses the `install` command to place the executable in the `bin` destination directory. It also handles installing PDB debug files for MSVC shared library builds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(INSTALL_TEST)\n  set_target_properties(test_lazy PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n  install(TARGETS test_lazy DESTINATION bin)\n  # Install PDB files for MSVC builds\n  if(MSVC AND BUILD_SHARED_LIBS)\n    install(FILES $<TARGET_PDB_FILE:test_lazy> DESTINATION bin OPTIONAL)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Executing aten.nll_loss_forward Operator in PyTorch\nDESCRIPTION: In this snippet, Atens nll_loss_forward.default operator is utilized to compute the negative log likelihood loss using input tensor and target indices. Dependencies involve PyTorchs tensor operations, and inputs include a prediction tensor and index tensor of labels. The sponsor will output a loss value for optimization purposes, serving as a loss function for classification tasks. It requires consistent input shapes and indices range matching label count.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Performing Tensor Addition with ATen in Python\nDESCRIPTION: This operator performs element-wise addition on tensors with identical dimensions. Dependencies include two input tensors of shape [16, 256, 32, 32] and data type f16. The outputs are tensors of the same shape and type. Key functionality lies in its ability to efficiently compute the sum of two tensors used in neural network layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 12, ((T([16, 256, 32, 32], f16), T([16, 256, 32, 32], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Threshold Backward Operation using ATen in Python\nDESCRIPTION: Illustrates the \\\"aten.threshold_backward.default\\\" operator in ATen for computing the backward pass of a threshold operation on tensors. The example utilizes f16 tensors of shape [128, 32, 112, 112] with a threshold value of 0. Ensure PyTorch and its ATen component are installed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), 0), {})\nOperator: aten.threshold_backward.default\n```\n\n----------------------------------------\n\nTITLE: Configuring MPS Profiler Log Options (Environment Variable)\nDESCRIPTION: Sets the log options bitmask for the `MPSProfiler` using `PYTORCH_MPS_LOG_PROFILE_INFO`. The available options are defined by the `LogOptions` enum located in the specified header file `aten/src/ATen/mps/MPSProfiler.h`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nPYTORCH_MPS_LOG_PROFILE_INFO\n```\n\n----------------------------------------\n\nTITLE: Addition using aten.add.Tensor in PyTorch\nDESCRIPTION: Illustrates the application of aten.add.Tensor for element-wise addition across different tensor configurations, heavily relying on f16 type. Necessary infrastructures include PyTorch for tensorated computations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\naten.add.Tensor, ((T([16, 128, 512], f16), T([1, 128, 512], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Backward Operations in PyTorch\nDESCRIPTION: This snippet shows backward operations for hardsigmoid and hardswish activations, which are used in the backpropagation process during training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.hardsigmoid_backward.default\ncnt: 1, ((T([128, 1104, 1, 1], f16), T([128, 1104, 1, 1], f16)), {})\ncnt: 5, ((T([128, 736, 1, 1], f16), T([128, 736, 1, 1], f16)), {})\n\nOperator: aten.hardswish_backward.default\ncnt: 1, ((T([128, 1984, 1, 1], f16), T([128, 1984, 1, 1], f16)), {})\ncnt: 1, ((T([128, 1344, 7, 7], f16), T([128, 1344, 7, 7], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Backward Operations in DenseNet Backpropagation\nDESCRIPTION: This snippet shows the backward pass operations for convolutions in DenseNet. It includes gradients flowing back through the network from the final layers to the initial convolution, with appropriate shapes and configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([32, 1024, 7, 7], f16), T([32, 2144, 7, 7], f16), T([1024, 2144, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 8, ((T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([224, 224, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 224, 7, 7], f16), T([32, 1024, 7, 7], f16), T([224, 1024, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 1024, 7, 7], f16), T([32, 1888, 7, 7], f16), T([1024, 1888, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 224, 7, 7], f16), T([32, 768, 7, 7], f16), T([224, 768, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 768, 14, 14], f16), T([32, 1728, 14, 14], f16), T([768, 1728, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 8, ((T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([192, 192, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 192, 14, 14], f16), T([32, 768, 14, 14], f16), T([192, 768, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 768, 14, 14], f16), T([32, 1472, 14, 14], f16), T([768, 1472, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 192, 14, 14], f16), T([32, 512, 14, 14], f16), T([192, 512, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([32, 1056, 28, 28], f16), T([512, 1056, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), T([160, 160, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 160, 28, 28], f16), T([32, 256, 28, 28], f16), T([160, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 256, 56, 56], f16), T([32, 768, 56, 56], f16), T([256, 768, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 5, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([128, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 128, 56, 56], f16), T([32, 64, 112, 112], f16), T([128, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([32, 3, 224, 224], f16), T([64, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Batch Matrix Multiplication in PyTorch\nDESCRIPTION: Logs data on \\\"aten.bmm.default\\\" operations which perform batch matrix multiplication. It captures various tensor shapes and storage strides involved in the process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 24, ((T([128, 128, 64], f16), T([128, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 24, ((T([128, 128, 128], f16), T([128, 128, 64], f16)), {})\ncnt: 12, ((T([128, 128, 128], f16, stride=(16384, 1, 128)), T([128, 128, 64], f16)), {})\ncnt: 12, ((T([128, 64, 128], f16, stride=(8192, 1, 64)), T([128, 128, 128], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten._softmax and aten._softmax_backward_data Operators - PyTorch - Python\nDESCRIPTION: These snippets record calls to softmax and its backward pass for a tensor of shape [4, 12, 1024, 1024] with half-precision. The forward softmax is performed over axis -1 with a contiguous memory flag, and the backward variant backpropagates gradients through the softmax. Expected input/output: high-dimensional activation tensors. Dependencies: torch, GPU hardware for large data and f16. Constraints: input rank and memory requirements.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 12, ((T([4, 12, 1024, 1024], f16), -1, False), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([4, 12, 1024, 1024], f16), T([4, 12, 1024, 1024], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Max Pooling Operations\nDESCRIPTION: Forward and backward max pooling operations with 2x2 kernels and specified strides, including indices for gradient computation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/maml_omniglot_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\naten.max_pool2d_with_indices.default((T([5, 64, 26, 26], f16, stride=(43264, 1, 1664, 64)), [2, 2], [2, 2]))\naten.max_pool2d_with_indices_backward.default((T([5, 64, 1, 1], f16), T([5, 64, 3, 3], f16, stride=(576, 1, 192, 64)), [2, 2], [2, 2], [0, 0], [1, 1], False, T([5, 64, 1, 1], i64)))\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations in PyTorch for Feature Extraction\nDESCRIPTION: This code shows forward convolution operations used in the model for feature extraction at different scales. It includes initial input processing, downsampling operations, and various convolution operations with different kernel sizes, strides, and channel dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([32, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([192, 32, 4, 4], f16), T([192], f16), [4, 4], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 7, ((T([128, 192, 28, 28], f16), T([384, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 7, ((T([128, 384, 28, 28], f16), T([384, 48, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 8), {})\ncnt: 7, ((T([128, 384, 28, 28], f16), T([192, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([384, 192, 2, 2], f16), T([384], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 384, 14, 14], f16), T([1152, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 384, 14, 14], f16), T([384, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 384, 14, 14], f16), T([1536, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 1536, 14, 14], f16), T([384, 1536, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 384, 14, 14], f16), T([768, 384, 2, 2], f16), T([768], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 768, 7, 7], f16), T([2304, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 768, 7, 7], f16), T([768, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 768, 7, 7], f16), T([3072, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 3072, 7, 7], f16), T([768, 3072, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Dimension Removal Operations in PyTorch\nDESCRIPTION: Demonstrates how reduction operations and dimension removal functions handle named tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/name_inference.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n>>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.squeeze('N').names\n('C', 'H', 'W')\n\n>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.sum(['N', 'C']).names\n('H', 'W')\n\n# Reduction ops with keepdim=True don't actually remove dimensions.\n>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.sum(['N', 'C'], keepdim=True).names\n('N', 'C', 'H', 'W')\n```\n\n----------------------------------------\n\nTITLE: Running Single Test in Distributed Spawn Suite\nDESCRIPTION: Command to run a specific test (DDP profiling with torch profiler) in the distributed spawn test suite, setting up environment variables for backend and world size.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntouch /tmp/barrier && TEMP_DIR=\"/tmp\" BACKEND=\"nccl\" WORLD_SIZE=\"2\" python test/distributed/test_distributed_spawn.py -v TestDistBackendWithSpawn.test_ddp_profiling_torch_profiler\n```\n\n----------------------------------------\n\nTITLE: Tracking Tensor Clone Operations in Neural Network\nDESCRIPTION: Records clone operations at different stages of the network, showing tensor shapes from input [128, 3, 224, 224] through various hidden layers. Clone operations create copies of tensors, often necessary for operations that modify tensors in-place.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\ncnt: 1, ((T([128, 32, 112, 112], f16),), {})\ncnt: 1, ((T([128, 240, 28, 28], f16),), {})\ncnt: 1, ((T([128, 240, 14, 14], f16),), {})\ncnt: 4, ((T([128, 480, 14, 14], f16),), {})\ncnt: 3, ((T([128, 672, 14, 14], f16),), {})\ncnt: 1, ((T([128, 672, 7, 7], f16),), {})\ncnt: 2, ((T([128, 1152, 7, 7], f16),), {})\ncnt: 1, ((T([128, 960, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1280, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Describing ATen Operator Usage Patterns in PyTorch (Python)\nDESCRIPTION: This snippet conveys, in structured annotation-like notation, the frequency and argument patterns of various ATen operators in a PyTorch-based model run. Each section includes the operator name, usage count, and the shape, element type, strides, and meta-parameters of its arguments. No explicit dependencies are required, but interpreting the content assumes familiarity with PyTorch, its tensor notation, and ATen operator signatures. Inputs are not actual code, but data-driven summaries of operator calls; outputs help trace activation/data shapes and may guide optimization. All content is to be interpreted as reference/metadata rather than executable code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/resmlp_12_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([128, 196, 1536], f16), [128, 196, 1536]), {})\ncnt: 12, ((T([128, 384, 196], f16), [49152, 196]), {})\nOperator: aten.add.Tensor\ncnt: 12, ((T([128, 196, 384], f16, stride=(75264, 1, 196)), T([128, 196, 384], f16, stride=(75264, 1, 196))), {})\ncnt: 12, ((T([128, 196, 1536], f16), T([1536], f16)), {})\ncnt: 12, ((T([128, 196, 384], f16, stride=(75264, 1, 196)), T([128, 196, 384], f16)), {})\ncnt: 12, ((T([128, 196, 384], f16), T([128, 196, 384], f16)), {})\ncnt: 12, ((T([128, 196, 384], f16), T([128, 196, 384], f16, stride=(75264, 1, 196))), {})\nOperator: aten.addcmul.default\ncnt: 25, ((T([1, 1, 384], f16), T([1, 1, 384], f16), T([128, 196, 384], f16, stride=(75264, 1, 196))), {})\nOperator: aten.addmm.default\ncnt: 12, ((T([196], f16), T([49152, 196], f16), T([196, 196], f16, stride=(1, 196))), {})\ncnt: 12, ((T([384], f16), T([25088, 1536], f16), T([1536, 384], f16, stride=(1, 1536))), {})\ncnt: 1, ((T([1000], f16), T([128, 384], f16), T([384, 1000], f16, stride=(1, 384))), {})\nOperator: aten.bmm.default\ncnt: 12, ((T([128, 196, 384], f16, stride=(75264, 1, 196)), T([128, 384, 1536], f16, stride=(0, 1, 384))), {})\ncnt: 12, ((T([128, 384, 196], f16), T([128, 196, 1536], f16)), {})\ncnt: 12, ((T([128, 196, 1536], f16), T([128, 1536, 384], f16, stride=(0, 384, 1))), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([384, 3, 16, 16], f16), T([384], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 384, 14, 14], f16, stride=(75264, 1, 5376, 384)), T([128, 3, 224, 224], f16), T([384, 3, 16, 16], f16), [384], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\ncnt: 12, ((T([1536, 384], f16), T([1536, 384], f16, stride=(1, 1536))), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 196, 384], f16, stride=(384, 0, 1)), 196), {})\nOperator: aten.gelu.default\ncnt: 12, ((T([128, 196, 1536], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([128, 196, 1536], f16), T([128, 196, 1536], f16)), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 196, 384], f16, stride=(75264, 1, 196)), [1]), {})\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 384], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 384], f16)), {})\ncnt: 12, ((T([25088, 384], f16), T([384, 1536], f16)), {})\ncnt: 12, ((T([384, 25088], f16, stride=(1, 384)), T([25088, 1536], f16)), {})\ncnt: 12, ((T([49152, 196], f16), T([196, 196], f16)), {})\ncnt: 12, ((T([196, 49152], f16, stride=(1, 196)), T([49152, 196], f16)), {})\nOperator: aten.mul.Scalar\ncnt: 25, ((T([128, 196, 384], f16, stride=(75264, 1, 196)), 1), {})\ncnt: 25, ((T([1, 1, 384], f16), 1), {})\nOperator: aten.mul.Tensor\ncnt: 12, ((T([384], f16), T([128, 196, 384], f16, stride=(75264, 1, 196))), {})\ncnt: 12, ((T([384], f16), T([128, 196, 384], f16)), {})\ncnt: 25, ((T([128, 196, 384], f16), T([128, 196, 384], f16, stride=(75264, 1, 196))), {})\ncnt: 13, ((T([128, 196, 384], f16), T([1, 1, 384], f16)), {})\ncnt: 24, ((T([128, 196, 384], f16), T([384], f16)), {})\ncnt: 12, ((T([128, 196, 384], f16), T([128, 196, 384], f16)), {})\ncnt: 12, ((T([128, 196, 384], f16, stride=(75264, 1, 196)), T([128, 196, 384], f16, stride=(75264, 1, 196))), {})\ncnt: 12, ((T([128, 196, 384], f16, stride=(75264, 1, 196)), T([1, 1, 384], f16)), {})\nOperator: aten.new_empty_strided.default\ncnt: 12, ((T([1536, 384], f16, stride=(1, 1536)), [1536, 384], [384, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 50, ((T([128, 196, 384], f16), [0, 1], True), {})\ncnt: 12, ((T([25088, 384], f16), [0], True), {})\ncnt: 12, ((T([128, 196, 1536], f16), [0, 1], True), {})\ncnt: 12, ((T([128, 384, 1536], f16), [0], True), {})\ncnt: 12, ((T([49152, 196], f16), [0], True), {})\ncnt: 24, ((T([128, 196, 384], f16, stride=(75264, 1, 196)), [0, 1], True), {})\n\n```\n\n----------------------------------------\n\nTITLE: Encoding Tensor Comparisons with aten.any\nDESCRIPTION: Utilizing aten.any.default operates on tensors to identify any True elements across dimensions. Dependencies include PyTorch with inputs being boolean tensors, producing outputs indicative of element-wise condition satisfaction.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.any.default\ncnt: 12, ((T([8, 128, 768], b8),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch ReLU Backward Pass Implementation (threshold_backward)\nDESCRIPTION: This snippet shows the backward pass operations for ReLU activations (threshold_backward) with various tensor shapes. Each operation takes an input gradient, output from the forward pass, and threshold value (0). The counts suggest frequency of each operation during backpropagation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([64, 1024, 7, 7], f16), T([64, 1024, 7, 7], f16), 0), {})\ncnt: 16, ((T([64, 128, 7, 7], f16), T([64, 128, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 992, 7, 7], f16), T([64, 992, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 960, 7, 7], f16), T([64, 960, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 928, 7, 7], f16), T([64, 928, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 896, 7, 7], f16), T([64, 896, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 864, 7, 7], f16), T([64, 864, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 832, 7, 7], f16), T([64, 832, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 800, 7, 7], f16), T([64, 800, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 768, 7, 7], f16), T([64, 768, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 736, 7, 7], f16), T([64, 736, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 704, 7, 7], f16), T([64, 704, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 672, 7, 7], f16), T([64, 672, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 640, 7, 7], f16), T([64, 640, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 608, 7, 7], f16), T([64, 608, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 576, 7, 7], f16), T([64, 576, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 544, 7, 7], f16), T([64, 544, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 512, 7, 7], f16), T([64, 512, 7, 7], f16), 0), {})\ncnt: 1, ((T([64, 1024, 14, 14], f16), T([64, 1024, 14, 14], f16), 0), {})\ncnt: 24, ((T([64, 128, 14, 14], f16), T([64, 128, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 992, 14, 14], f16), T([64, 992, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 960, 14, 14], f16), T([64, 960, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 928, 14, 14], f16), T([64, 928, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 896, 14, 14], f16), T([64, 896, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 864, 14, 14], f16), T([64, 864, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 832, 14, 14], f16), T([64, 832, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 800, 14, 14], f16), T([64, 800, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 768, 14, 14], f16), T([64, 768, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 736, 14, 14], f16), T([64, 736, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 704, 14, 14], f16), T([64, 704, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 672, 14, 14], f16), T([64, 672, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 640, 14, 14], f16), T([64, 640, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 608, 14, 14], f16), T([64, 608, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 576, 14, 14], f16), T([64, 576, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 544, 14, 14], f16), T([64, 544, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 480, 14, 14], f16), T([64, 480, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 448, 14, 14], f16), T([64, 448, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 416, 14, 14], f16), T([64, 416, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 384, 14, 14], f16), T([64, 384, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 352, 14, 14], f16), T([64, 352, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 320, 14, 14], f16), T([64, 320, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 288, 14, 14], f16), T([64, 288, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 256, 14, 14], f16), T([64, 256, 14, 14], f16), 0), {})\ncnt: 1, ((T([64, 512, 28, 28], f16), T([64, 512, 28, 28], f16), 0), {})\ncnt: 13, ((T([64, 128, 28, 28], f16), T([64, 128, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 480, 28, 28], f16), T([64, 480, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 448, 28, 28], f16), T([64, 448, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 416, 28, 28], f16), T([64, 416, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 384, 28, 28], f16), T([64, 384, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 352, 28, 28], f16), T([64, 352, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 320, 28, 28], f16), T([64, 320, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 288, 28, 28], f16), T([64, 288, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 224, 28, 28], f16), T([64, 224, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 192, 28, 28], f16), T([64, 192, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 160, 28, 28], f16), T([64, 160, 28, 28], f16), 0), {})\ncnt: 1, ((T([64, 256, 56, 56], f16), T([64, 256, 56, 56], f16), 0), {})\ncnt: 7, ((T([64, 128, 56, 56], f16), T([64, 128, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 224, 56, 56], f16), T([64, 224, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 192, 56, 56], f16), T([64, 192, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 160, 56, 56], f16), T([64, 160, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 96, 56, 56], f16), T([64, 96, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 64, 112, 112], f16), T([64, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Utilizing ATen Upsample Bilinear2d in PyTorch: Python\nDESCRIPTION: This snippet explores the use of the 'upsample_bilinear2d' operator and its backward variant in PyTorch. It describes how bi-linear up-sampling is performed on tensors of various shapes, emphasizing the scaling factors and the optionality in parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.upsample_bilinear2d.vec\ncnt: 2, ((T([3, 256, 128, 128], f16), None, True, [2.0, 2.0]), {})\ncnt: 1, ((T([3, 128, 256, 256], f16), None, True, [2.0, 2.0]), {})\ncnt: 1, ((T([3, 256, 256, 256], f16), None, True, [2.0, 2.0]), {})\nOperator: aten.upsample_bilinear2d_backward.vec\ncnt: 1, ((T([3, 256, 512, 512], f16), None, [3, 256, 256, 256], True, [2.0, 2.0]), {})\n```\n\n----------------------------------------\n\nTITLE: Creating Side-by-Side Code Tables in Markdown\nDESCRIPTION: This Markdown snippet provides a template for displaying side-by-side comparison tables of different code versions. It is specifically formatted to host Python code blocks and manages visual layouts through Markdown's table syntax. This is used to manage code version comparisons effectively in documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/README.md#2025-04-22_snippet_7\n\nLANGUAGE: Markdown\nCODE:\n```\n<table>\n<tr>\n<th>PRIOR RELEASE NUM</th>\n<th>NEW RELEASE NUM</th>\n</tr>\n<tr>\n<td>\n\n```Python\n# Code Snippet 1\n```\n\n</td>\n<td>\n\n```Python\n# Code Snippet 2\n```\n\n</td>\n</tr>\n</table>\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Native Layer Normalization in PyTorch\nDESCRIPTION: Tracks \\\"aten.native_layer_norm.default\\\", detailing its computation and use of additional parameters for tensor normalization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_19\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.native_layer_norm.default\ncnt: 25, ((T([8, 128, 1024], f16), [1024], T([1024], f16), T([1024], f16), 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Downloading MNIST Dataset for Integration Tests (Shell)\nDESCRIPTION: Shell command to download the MNIST dataset, which is a prerequisite for running C++ Frontend integration tests. This command executes a Python script (`tools/download_mnist.py`) specifying the target directory (`test/cpp/api/mnist`) relative to the PyTorch root. This command must be run from the PyTorch root folder before executing the integration tests.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/README.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n```sh\n$ python tools/download_mnist.py -d test/cpp/api/mnist\n```\n```\n\n----------------------------------------\n\nTITLE: Padding Operations for Convolutional Neural Networks\nDESCRIPTION: This snippet shows constant padding operations used to add zero-padding around feature maps in a convolutional neural network. Each operation adds 1 pixel of padding on all sides of half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.constant_pad_nd.default\ncnt: 4, ((T([0, 1, 28, 28], f16), [1, 1, 1, 1], 0.0), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Convolution Backward Test Parameters for aten.convolution_backward (Python)\nDESCRIPTION: This snippet includes several test case definitions for the aten.convolution_backward operator using Python tuple structures. Each case models the backward pass for convolutions, specifying input, grad_output, and weight tensor shapes, types, and auxiliary parameters for stride, padding, dilation, groups, and output masks. This setup enables targeted verification and benchmarking of the backward function for various deep learning convolutional network scenarios and configurations, especially for float16 data and edge cases (varied mask arrays, custom strides).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nasnetalarge_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 70, ((T([16, 672, 11, 11], f16), T([16, 672, 11, 11], f16), T([672, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 38, ((T([16, 672, 11, 11], f16), T([16, 672, 11, 11], f16), T([672, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 26, ((T([16, 672, 11, 11], f16), T([16, 672, 11, 11], f16), T([672, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 9, ((T([16, 672, 11, 11], f16), T([16, 4032, 11, 11], f16), T([672, 4032, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([16, 672, 11, 11], f16), T([16, 2688, 11, 11], f16), T([672, 2688, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([16, 336, 11, 11], f16, stride=(81312, 121, 11, 1)), T([16, 2016, 11, 11], f16), T([336, 2016, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([16, 672, 11, 11], f16), T([16, 672, 25, 25], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 2, ((T([16, 672, 11, 11], f16), T([16, 672, 11, 11], f16), T([672, 1, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 2, ((T([16, 672, 11, 11], f16), T([16, 672, 27, 27], f16), T([672, 1, 7, 7], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 2, ((T([16, 672, 21, 21], f16), T([16, 2016, 21, 21], f16), T([672, 2016, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 70, ((T([16, 336, 21, 21], f16), T([16, 336, 21, 21], f16), T([336, 336, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 38, ((T([16, 336, 21, 21], f16), T([16, 336, 21, 21], f16), T([336, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 336, [True, True, False]), {})\ncnt: 26, ((T([16, 336, 21, 21], f16), T([16, 336, 21, 21], f16), T([336, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 336, [True, True, False]), {})\ncnt: 9, ((T([16, 336, 21, 21], f16), T([16, 2016, 21, 21], f16), T([336, 2016, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([16, 336, 21, 21], f16), T([16, 1344, 21, 21], f16), T([336, 1344, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([16, 168, 21, 21], f16, stride=(148176, 441, 21, 1)), T([16, 1008, 21, 21], f16), T([168, 1008, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([16, 336, 21, 21], f16), T([16, 336, 45, 45], f16), T([336, 1, 5, 5], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 336, [True, True, False]), {})\ncnt: 2, ((T([16, 336, 21, 21], f16), T([16, 336, 21, 21], f16), T([336, 1, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 336, [True, True, False]), {})\ncnt: 2, ((T([16, 336, 21, 21], f16), T([16, 336, 47, 47], f16), T([336, 1, 7, 7], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 336, [True, True, False]), {})\ncnt: 2, ((T([16, 336, 42, 42], f16), T([16, 1008, 42, 42], f16), T([336, 1008, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 60, ((T([16, 168, 42, 42], f16), T([16, 168, 42, 42], f16), T([168, 168, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 36, ((T([16, 168, 42, 42], f16), T([16, 168, 42, 42], f16), T([168, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 168, [True, True, False]), {})\ncnt: 24, ((T([16, 168, 42, 42], f16), T([16, 168, 42, 42], f16), T([168, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 168, [True, True, False]), {})\ncnt: 9, ((T([16, 168, 42, 42], f16), T([16, 1008, 42, 42], f16), T([168, 1008, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([16, 168, 42, 42], f16), T([16, 336, 42, 42], f16), T([168, 336, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([16, 84, 42, 42], f16, stride=(296352, 1764, 42, 1)), T([16, 168, 42, 42], f16), T([84, 168, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 10, ((T([16, 84, 42, 42], f16), T([16, 84, 42, 42], f16), T([84, 84, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([16, 84, 42, 42], f16), T([16, 84, 42, 42], f16), T([84, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 84, [True, True, False]), {})\ncnt: 2, ((T([16, 84, 42, 42], f16), T([16, 84, 42, 42], f16), T([84, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 84, [True, True, False]), {})\ncnt: 2, ((T([16, 84, 42, 42], f16), T([16, 84, 87, 87], f16), T([84, 1, 5, 5], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 84, [True, True, False]), {})\ncnt: 2, ((T([16, 84, 42, 42], f16), T([16, 84, 42, 42], f16), T([84, 1, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 84, [True, True, False]), {})\ncnt: 2, ((T([16, 84, 42, 42], f16), T([16, 84, 89, 89], f16), T([84, 1, 7, 7], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 84, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Summarizing Tensor Multiplication and Elementwise Calls (aten.mul) - PyTorch - Python\nDESCRIPTION: The snippet lists argument patterns for the aten.mul (Tensor) operator, spanning scalar-tensor, tensor-tensor, and broadcasting multiplication combinations across numerous tensor ranks and shapes. Includes half precision types and a variety of scales, as seen in normalization and scale operations. Inputs are tensors and/or scalars, only operator config and usage sampled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 2, ((T([16, 1, 1, 1], f16), 0.34412564994580647), {})\ncnt: 2, ((T([32, 1, 1, 1], f16), 0.1490107774734497), {})\ncnt: 2, ((T([64, 1, 1, 1], f16), 0.10536653122135592), {})\ncnt: 10, ((T([128, 1, 1, 1], f16), 0.07450538873672485), {})\ncnt: 2, ((T([128, 128, 56, 56], f16), 1.0), {})\ncnt: 2, ((T([256, 1, 1, 1], f16), 0.1580497968320339), {})\ncnt: 2, ((T([64, 1, 1, 1], f16), 0.1580497968320339), {})\ncnt: 4, ((T([64, 1, 1, 1], f16), 0.07450538873672485), {})\ncnt: 2, ((T([256, 1, 1, 1], f16), 0.22351616621017456), {})\ncnt: 2, ((T([128, 256, 56, 56], f16), T([128, 256, 1, 1], f16)), {})\ncnt: 2, ((T([128, 256, 56, 56], f16), 2.0), {})\ncnt: 2, ((T([128, 256, 56, 56], f16), 0.2), {})\ncnt: 2, ((T([128, 256, 56, 56], f16), 0.9805806756909201), {})\ncnt: 2, ((T([512, 1, 1, 1], f16), 0.11175808310508728), {})\ncnt: 2, ((T([128, 1, 1, 1], f16), 0.11175808310508728), {})\ncnt: 4, ((T([512, 1, 1, 1], f16), 0.1580497968320339), {})\ncnt: 4, ((T([128, 512, 28, 28], f16), T([128, 512, 1, 1], f16)), {})\ncnt: 4, ((T([128, 512, 28, 28], f16), 2.0), {})\ncnt: 4, ((T([128, 512, 28, 28], f16), 0.2), {})\ncnt: 2, ((T([128, 512, 28, 28], f16), 0.9805806756909201), {})\ncnt: 2, ((T([128, 1, 1, 1], f16), 0.07902489841601695), {})\ncnt: 2, ((T([128, 512, 28, 28], f16), 0.9622504486493761), {})\ncnt: 2, ((T([1536, 1, 1, 1], f16), 0.07902489841601695), {})\ncnt: 2, ((T([384, 1, 1, 1], f16), 0.07902489841601695), {})\ncnt: 36, ((T([384, 1, 1, 1], f16), 0.07450538873672485), {})\ncnt: 18, ((T([1536, 1, 1, 1], f16), 0.09125009274634042), {})\ncnt: 12, ((T([128, 1536, 14, 14], f16), T([128, 1536, 1, 1], f16)), {})\ncnt: 12, ((T([128, 1536, 14, 14], f16), 2.0), {})\ncnt: 12, ((T([128, 1536, 14, 14], f16), 0.2), {})\ncnt: 2, ((T([128, 1536, 14, 14], f16), 0.9805806756909201), {})\ncnt: 16, ((T([384, 1, 1, 1], f16), 0.04562504637317021), {})\ncnt: 2, ((T([128, 1536, 14, 14], f16), 0.9622504486493761), {})\ncnt: 2, ((T([128, 1536, 14, 14], f16), 0.9449111825230679), {})\ncnt: 2, ((T([128, 1536, 14, 14], f16), 0.9284766908852592), {})\ncnt: 2, ((T([128, 1536, 14, 14], f16), 0.9128709291752768), {})\ncnt: 2, ((T([128, 1536, 14, 14], f16), 0.8980265101338745), {})\ncnt: 2, ((T([1536, 1, 1, 1], f16), 0.04562504637317021), {})\ncnt: 6, ((T([128, 1536, 7, 7], f16), T([128, 1536, 1, 1], f16)), {})\ncnt: 6, ((T([128, 1536, 7, 7], f16), 2.0), {})\ncnt: 6, ((T([128, 1536, 7, 7], f16), 0.2), {})\ncnt: 2, ((T([128, 1536, 7, 7], f16), 0.9805806756909201), {})\ncnt: 2, ((T([128, 1536, 7, 7], f16), 0.9622504486493761), {})\ncnt: 2, ((T([2304, 1, 1, 1], f16), 0.04562504637317021), {})\ncnt: 3, ((T([128, 1536, 7, 7], f16), T([128, 1536, 7, 7], f16)), {})\ncnt: 6, ((T([128, 1536, 14, 14], f16), T([128, 1536, 14, 14], f16)), {})\ncnt: 2, ((T([128, 512, 28, 28], f16), T([128, 512, 28, 28], f16)), {})\ncnt: 1, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Documenting torch.mps Module with Sphinx Autosummary - reStructuredText\nDESCRIPTION: This snippet utilizes reStructuredText directives to organize the documentation for the torch.mps module in PyTorch. It lists various MPS device and memory management APIs and Sphinx will autogenerate summary pages for each, provided detailed docstrings/editable documentation exists elsewhere. It requires Sphinx for Python with autodoc and autosummary extensions enabled and expects API items to be importable for documentation build. The input is a set of Python functions; the output is a generated section of API index files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\ntorch.mps\n===================================\n.. automodule:: torch.mps\n.. currentmodule:: torch.mps\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    device_count\n    synchronize\n    get_rng_state\n    set_rng_state\n    manual_seed\n    seed\n    empty_cache\n    set_per_process_memory_fraction\n    current_allocated_memory\n    driver_allocated_memory\n    recommended_max_memory\n    compile_shader\n```\n\n----------------------------------------\n\nTITLE: Implementing tanh Kernel for CUDA in PyTorch\nDESCRIPTION: This CUDA kernel implements the tanh operation for PyTorch tensors on GPU. It's designed to work with the TensorIterator interface.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_39\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native16tanh_kernel_cudaERNS_18TensorIteratorBaseE\n```\n\n----------------------------------------\n\nTITLE: Tensor Summation Operations in PyTorch (SymInt)\nDESCRIPTION: This snippet shows a tensor summation operation along a specific dimension with reduction. This operation is typically used for computing means or sums across batches or features in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([96, 65], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Neural Network\nDESCRIPTION: This code snippet represents a summary of PyTorch operator usage in a neural network. It includes operator names, usage counts, and tensor shapes for various operations such as convolutions, pooling, and tensor manipulations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 3, ((T([128, 256, 48, 48], f16), T([128, 256, 48, 48], f16)), {})\ncnt: 6, ((T([128, 512, 24, 24], f16), T([128, 512, 24, 24], f16)), {})\ncnt: 18, ((T([128, 1536, 12, 12], f16), T([128, 1536, 12, 12], f16)), {})\ncnt: 8, ((T([128, 1536, 6, 6], f16), T([128, 1536, 6, 6], f16)), {})\ncnt: 1, ((T([128, 128, 48, 48], f16), T([128, 128, 48, 48], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 3072], f16), T([3072, 1000], f16, stride=(1, 3072))), {})\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([128, 256, 48, 48], f16), [2, 2], [2, 2], [0, 0], True, False), {})\ncnt: 1, ((T([128, 512, 24, 24], f16), [2, 2], [2, 2], [0, 0], True, False), {})\ncnt: 1, ((T([128, 1536, 12, 12], f16), [2, 2], [2, 2], [0, 0], True, False), {})\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([128, 1536, 6, 6], f16), T([128, 1536, 12, 12], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\ncnt: 1, ((T([128, 512, 12, 12], f16), T([128, 512, 24, 24], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\ncnt: 1, ((T([128, 256, 24, 24], f16), T([128, 256, 48, 48], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 192, 192], f16),), {})\ncnt: 1, ((T([128, 256, 48, 48], f16),), {})\ncnt: 2, ((T([128, 512, 24, 24], f16),), {})\ncnt: 6, ((T([128, 1536, 12, 12], f16),), {})\ncnt: 3, ((T([128, 1536, 6, 6], f16),), {})\nOperator: aten.constant_pad_nd.default\ncnt: 1, ((T([128, 3, 192, 192], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 64, 96, 96], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 256, 48, 48], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 768, 24, 24], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 768, 12, 12], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 768, 13, 13], f16), [0, -1, 0, -1]), {})\ncnt: 1, ((T([128, 768, 25, 25], f16), [0, -1, 0, -1]), {})\ncnt: 1, ((T([128, 256, 49, 49], f16), [0, -1, 0, -1]), {})\ncnt: 1, ((T([128, 64, 97, 97], f16), [0, -1, 0, -1]), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 193, 193], f16), T([16, 3, 3, 3], f16), T([16], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 16, 96, 96], f16), T([32, 16, 3, 3], f16), T([32], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 96, 96], f16), T([64, 32, 3, 3], f16), T([64], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 97, 97], f16), T([128, 64, 3, 3], f16), T([128], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 128, 48, 48], f16), T([256, 128, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 48, 48], f16), T([128, 128, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 128, 48, 48], f16), T([128, 128, 3, 3], f16), T([128], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 1, 1], f16), T([256, 128, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 256, 24, 24], f16), T([512, 256, 1, 1], f16), T([512], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 48, 48], f16), T([256, 256, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 49, 49], f16), T([256, 128, 3, 3], f16), T([256], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 2), {})\ncnt: 3, ((T([128, 256, 24, 24], f16), T([256, 128, 3, 3], f16), T([256], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})\ncnt: 2, ((T([128, 512, 1, 1], f16), T([256, 512, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 256, 1, 1], f16), T([512, 256, 1, 1], f16), T([512], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 24, 24], f16), T([256, 512, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 12, 12], f16), T([1536, 512, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 512, 24, 24], f16), T([768, 512, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 768, 25, 25], f16), T([768, 128, 3, 3], f16), T([768], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 6), {})\ncnt: 11, ((T([128, 768, 12, 12], f16), T([768, 128, 3, 3], f16), T([768], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 6), {})\ncnt: 6, ((T([128, 768, 12, 12], f16), T([1536, 768, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 9, ((T([128, 1536, 1, 1], f16), T([768, 1536, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 9, ((T([128, 768, 1, 1], f16), T([1536, 768, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 6, ((T([128, 1536, 12, 12], f16), T([768, 1536, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1536, 6, 6], f16), T([1536, 1536, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 768, 13, 13], f16), T([768, 128, 3, 3], f16), T([768], f16), [2, 2], [0, 0], [1, 1], False, [0, 0], 6), {})\ncnt: 5, ((T([128, 768, 6, 6], f16), T([768, 128, 3, 3], f16), T([768], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 6), {})\ncnt: 3, ((T([128, 768, 6, 6], f16), T([1536, 768, 1, 1], f16), T([1536], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 1536, 6, 6], f16), T([768, 1536, 1, 1], f16), T([768], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1536, 6, 6], f16), T([3072, 1536, 1, 1], f16), T([3072], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 3072, 6, 6], f16), T([128, 1536, 6, 6], f16), T([3072, 1536, 1, 1], f16), [3072], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 9, ((T([128, 1536, 1, 1], f16), T([128, 768, 1, 1], f16), T([1536, 768, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 9, ((T([128, 768, 1, 1], f16), T([128, 1536, 1, 1], f16), T([768, 1536, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 3, ((T([128, 1536, 6, 6], f16), T([128, 768, 6, 6], f16), T([1536, 768, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 5, ((T([128, 768, 6, 6], f16), T([128, 768, 6, 6], f16), T([768, 128, 3, 3], f16), [768], [1, 1], [1, 1], [1, 1], False, [0, 0], 6, [True, True, True]), {})\ncnt: 2, ((T([128, 768, 6, 6], f16), T([128, 1536, 6, 6], f16), T([768, 1536, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 768, 6, 6], f16), T([128, 768, 13, 13], f16), T([768, 128, 3, 3], f16), [768], [2, 2], [0, 0], [1, 1], False, [0, 0], 6, [True, True, True]), {})\ncnt: 6, ((T([128, 768, 12, 12], f16), T([128, 1536, 12, 12], f16), T([768, 1536, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 1536, 6, 6], f16), T([128, 1536, 6, 6], f16), T([1536, 1536, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 6, ((T([128, 1536, 12, 12], f16), T([128, 768, 12, 12], f16), T([1536, 768, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 11, ((T([128, 768, 12, 12], f16), T([128, 768, 12, 12], f16), T([768, 128, 3, 3], f16), [768], [1, 1], [1, 1], [1, 1], False, [0, 0], 6, [True, True, True]), {})\ncnt: 1, ((T([128, 768, 12, 12], f16), T([128, 768, 25, 25], f16), T([768, 128, 3, 3], f16), [768], [2, 2], [0, 0], [1, 1], False, [0, 0], 6, [True, True, True]), {})\ncnt: 1, ((T([128, 768, 24, 24], f16), T([128, 512, 24, 24], f16), T([768, 512, 1, 1], f16), [768], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 1536, 12, 12], f16), T([128, 512, 12, 12], f16), T([1536, 512, 1, 1], f16), [1536], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 512, 1, 1], f16), T([128, 256, 1, 1], f16), T([512, 256, 1, 1], f16), [512], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 256, 1, 1], f16), T([128, 512, 1, 1], f16), T([256, 512, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 3, ((T([128, 512, 24, 24], f16), T([128, 256, 24, 24], f16), T([512, 256, 1, 1], f16), [512], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 3, ((T([128, 256, 24, 24], f16), T([128, 256, 24, 24], f16), T([256, 128, 3, 3], f16), [256], [1, 1], [1, 1], [1, 1], False, [0, 0], 2, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 24, 24], f16), T([128, 512, 24, 24], f16), T([256, 512, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 24, 24], f16), T([128, 256, 49, 49], f16), T([256, 128, 3, 3], f16), [256], [2, 2], [0, 0], [1, 1], False, [0, 0], 2, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 48, 48], f16), T([128, 256, 48, 48], f16), T([256, 256, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 1, 1], f16), T([128, 128, 1, 1], f16), T([256, 128, 1, 1], f16), [256], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 128, 1, 1], f16), T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), [128], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Fix nvcc Header Dependency Bug\nDESCRIPTION: This bash snippet provides a workaround for a nvcc header dependency bug by using a compiler wrapper, enhancing build reliability for CUDA code in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\nexport CMAKE_CUDA_COMPILER_LAUNCHER=\"python;`pwd`/tools/nvcc_fix_deps.py;ccache\"\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Runtime Test Sources in CMake\nDESCRIPTION: Adds source files for static runtime tests to the STATIC_RUNTIME_TEST_SRCS list and sets it in the parent scope. This includes various test files such as deep_wide_pt.cc, test_utils.cc, and others related to static runtime and module testing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/static_runtime/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND STATIC_RUNTIME_TEST_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/deep_wide_pt.cc)\nlist(APPEND STATIC_RUNTIME_TEST_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/test_utils.cc)\nlist(APPEND STATIC_RUNTIME_TEST_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/test_static_runtime.cc)\nlist(APPEND STATIC_RUNTIME_TEST_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/test_static_module.cc)\nlist(APPEND STATIC_RUNTIME_TEST_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/test_generated_ops.cc)\nset(STATIC_RUNTIME_TEST_SRCS ${STATIC_RUNTIME_TEST_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Testing Tensor In-Place Activation for aten.relu_.default - Python\nDESCRIPTION: These test cases represent in-place application of the ReLU activation on tensors of varying shapes for the 'aten.relu_.default' operator. Each tuple encodes the tensor details only; the operation mutates the tensor in place. ReLU is a common neural network nonlinearity, requiring input of numeric (typically f16/f32) tensors, without additional parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 16, 112, 112], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 8, 112, 112], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Handling Softmax Backward Data in PyTorch\nDESCRIPTION: Calculates gradients for softmax operations, which is vital in neural network training. It accepts input and output gradient tensors, handling softmax across given dimensions and using FP16 precision. This helps efficiently propagate errors backward.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\naten._softmax_backward_data.default(T([256, 128, 128], f16), T([256, 128, 128], f16), -1, f16)\n```\n\n----------------------------------------\n\nTITLE: Linking Dependencies for the Test Executable in CMake\nDESCRIPTION: Sets the dependencies for the `test_edge_op_registration` executable, including `gtest` and the previously defined `unbox_lib`. Links these dependencies privately to the executable. Includes conditional linking options based on the C++ compiler (AppleClang, Clang, GNU) to handle static library linking behavior (`-force_load` or `--whole-archive`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_DEPENDENCIES gtest unbox_lib)\n\ntarget_link_libraries(test_edge_op_registration PRIVATE\n  ${TEST_DEPENDENCIES}\n)\nif((CMAKE_CXX_COMPILER_ID MATCHES \"AppleClang\") OR (APPLE AND CMAKE_CXX_COMPILER_ID MATCHES \"Clang\"))\n  target_link_options(test_edge_op_registration PRIVATE\n          \"-Wl,-force_load,$<TARGET_FILE:unbox_lib>\"\n          )\nelseif(CMAKE_CXX_COMPILER_ID MATCHES \"Clang|GNU\")\n  target_link_options(test_edge_op_registration PRIVATE\n          \"-Wl,--whole-archive,$<TARGET_FILE:unbox_lib>,--no-whole-archive\"\n          )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Listing Supported Operators in Benchmark Suite\nDESCRIPTION: Displays all operators available in the benchmark suite.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m benchmark_all_test --list-ops\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Deterministic RNN Behavior in CUDA\nDESCRIPTION: Instructions for setting environment variables to enforce deterministic behavior in RNN functions. For CUDA 10.1, set CUDA_LAUNCH_BLOCKING=1. For CUDA 10.2 or later, set CUBLAS_WORKSPACE_CONFIG to either :16:8 or :4096:2.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cudnn_rnn_determinism.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nCUDA_LAUNCH_BLOCKING=1\n```\n\nLANGUAGE: bash\nCODE:\n```\nCUBLAS_WORKSPACE_CONFIG=:16:8\n```\n\nLANGUAGE: bash\nCODE:\n```\nCUBLAS_WORKSPACE_CONFIG=:4096:2\n```\n\n----------------------------------------\n\nTITLE: Defining torch.overrides Module in Python\nDESCRIPTION: This snippet defines the torch.overrides module in Python documentation. It sets up the module for further documentation of its contents.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.overrides.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. py:module:: torch.overrides\n```\n\n----------------------------------------\n\nTITLE: Setting C10 Include Directories\nDESCRIPTION: Configures the include directories for the C10 library, ensuring proper access to header files during both build and installation. Sets up both build-interface and install-interface include paths.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n  target_include_directories(\n      c10 PUBLIC\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/../>\n      $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}>\n      $<INSTALL_INTERFACE:include>)\n```\n\n----------------------------------------\n\nTITLE: Handling Type Refinement in Loop Breaks in Python\nDESCRIPTION: Illustrates a complex case where type refinement occurs after a continue statement in a loop. This example demonstrates why the Transform pass is performed after removing Loads and Stores from the graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n...\nif cond:\n  if i < 3:\n      x = torch.jit.annotate(Optional[int], None)\n      continue\n  x = 1\nelse:\n  x = 2\nprint(x)\n```\n\nLANGUAGE: Python\nCODE:\n```\nif cond:\n  if i < 3:\n    x = torch.jit.annotate(Optional[int], None)\n    did_continue = True\n    continue\n  else:\n    did_continue = False\n  if not did_continue:\n    x = 1\nelse:\n  x = 2\nif not did_continue:\n  print(x)\n```\n\n----------------------------------------\n\nTITLE: Executing aten.addmm operation in PyTorch\nDESCRIPTION: Utilizes the Aten backend to perform matrix multiplication of two matrices with an added bias matrix in PyTorch, leveraging half-precision floating point tensors. It requires predefined tensor shapes. Outputs a resultant tensor from the matrix-multiplication operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 2304], f16), T([2304, 1000], f16, stride=(1, 2304))), {})\n```\n\n----------------------------------------\n\nTITLE: Executing Matrix Multiplication with aten.addmm in PyTorch\nDESCRIPTION: Performs a linear transformation with equation: 1.0 * self + beta * (mat1 @ mat2). It is crucial in transforming linear models or within layers of neural networks. The function demands tensors subject to correct dimensionality for valid matrix multiplication, capable of precise operations with FP16.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\naten.addmm.default(T([256], f16), T([8192, 256], f16), T([256, 256], f16, stride=(1, 256)))\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.addmm.default(T([2048], f16), T([8192, 256], f16), T([256, 2048], f16, stride=(1, 256)))\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.addmm.default(T([256], f16), T([8192, 2048], f16), T([2048, 256], f16, stride=(1, 2048)))\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Log\nDESCRIPTION: Log of PyTorch operator calls showing tensor shapes, strides, and operation counts. Includes common deep learning operations like convolution, addition, concatenation and softmax with float16 precision tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([], i64), 1), {})\n# ... additional operations truncated for brevity\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.silu_backward.default SiLU Backward Pass in PyTorch ATen\nDESCRIPTION: Documents observed calls to the backward pass for the SiLU activation function (`aten.silu_backward.default`) in PyTorch ATen. It lists the different gradient output and original input tensor shapes (all f16) and their respective call counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_22\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.silu_backward.default\ncnt: 1, ((T([128, 2304, 7, 7], f16), T([128, 2304, 7, 7], f16)), {})\ncnt: 8, ((T([128, 384, 7, 7], f16), T([128, 384, 7, 7], f16)), {})\ncnt: 2, ((T([128, 1536, 7, 7], f16), T([128, 1536, 7, 7], f16)), {})\ncnt: 18, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16)), {})\ncnt: 6, ((T([128, 1536, 14, 14], f16), T([128, 1536, 14, 14], f16)), {})\ncnt: 1, ((T([128, 384, 28, 28], f16), T([128, 384, 28, 28], f16)), {})\ncnt: 2, ((T([128, 512, 28, 28], f16), T([128, 512, 28, 28], f16)), {})\ncnt: 5, ((T([128, 128, 28, 28], f16), T([128, 128, 28, 28], f16)), {})\ncnt: 2, ((T([128, 128, 56, 56], f16), T([128, 128, 56, 56], f16)), {})\ncnt: 1, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16)), {})\ncnt: 3, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16)), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16)), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16)), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Sparse Fully Connected Test for QNNPACK in CMake\nDESCRIPTION: Creates and configures the sparse fully connected network test executable with C++14 standard requirements, includes the necessary directories, and links against required libraries like pytorch_qnnpack, clog, cpuinfo, fp16, and gtest.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(fully-connected-sparse-test test/fully-connected-sparse.cc)\nset_target_properties(fully-connected-sparse-test PROPERTIES\n  CXX_STANDARD 14\n  CXX_STANDARD_REQUIRED YES\n  CXX_EXTENSIONS NO)\ntarget_include_directories(fully-connected-sparse-test PRIVATE src test)\ntarget_link_libraries(fully-connected-sparse-test PRIVATE pytorch_qnnpack clog cpuinfo fp16 gtest gtest_main)\nadd_test(fully-connected-sparse-test fully-connected-sparse-test)\n```\n\n----------------------------------------\n\nTITLE: Enabling C++ Exceptions for JNI Target in CMake\nDESCRIPTION: Configures the JNI target (specified by `PYTORCH_JNI_TARGET`) to be compiled with C++ exceptions enabled by adding the `-fexceptions` compile option privately to the target.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_compile_options(${PYTORCH_JNI_TARGET} PRIVATE\n  -fexceptions\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Model Architecture and Batch Size Configurations in PyTorch\nDESCRIPTION: A comprehensive list of machine learning model architectures with their recommended batch sizes. Each entry follows the format 'model_name,batch_size' where batch size values range from very small (2, 4, 5) for large models to larger values (128, 256) for more efficient models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/timm_models_list_cpu.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nadv_inception_v3,128\nbeit_base_patch16_224,64\nbotnet26t_256,128\ncait_m36_384,4\ncoat_lite_mini,32\nconvit_base,64\nconvmixer_768_32,2\nconvnext_base,64\ncrossvit_9_240,32\ncspdarknet53,64\ndeit_base_distilled_patch16_224,64\ndm_nfnet_f0,128\ndpn107,32\neca_botnext26ts_256,128\neca_halonext26ts,128\nese_vovnet19b_dw,128\nfbnetc_100,32\nfbnetv3_b,32\ngernet_l,128\nghostnet_100,128\ngluon_inception_v3,128\ngmixer_24_224,16\ngmlp_s16_224,128\nhrnet_w18,128\ninception_v3,128\njx_nest_base,32\nlcnet_050,64\nmixer_b16_224,128\nmixnet_l,128\nmnasnet_100,32\nmobilenetv2_100,32\nmobilenetv3_large_100,32\nmobilevit_s,256\nnfnet_l0,128\npit_b_224,64\npnasnet5large,16\npoolformer_m36,64\nregnety_002,128\nrepvgg_a2,128\nres2net101_26w_4s,64\nres2net50_14w_8s,128\nres2next50,128\nresmlp_12_224,128\nresnest101e,64\nrexnet_100,128\nsebotnet33ts_256,64\nselecsls42b,128\nspnasnet_100,32\nswin_base_patch4_window7_224,64\nswsl_resnext101_32x16d,32\ntf_efficientnet_b0,128\ntf_mixnet_l,32\ntinynet_a,128\ntnt_s_patch16_224,32\ntwins_pcpvt_base,64\nvisformer_small,128\nvit_base_patch16_224,64\nvolo_d1_224,64\nxcit_large_24_p8_224,5\n```\n\n----------------------------------------\n\nTITLE: Profiling Batch Normalization and Backward Operations with ATen in PyTorch (Python)\nDESCRIPTION: Summarizes all calls to ATen-based native_batch_norm (forward) and native_batch_norm_backward operators, including shapes and dtypes for the main data, running stats, weight, and bias tensors, as well as eps and momentum settings. Dependencies: PyTorch model with batch normalization layers. Inputs include N-d tensors and channel vectors; outputs are normalized tensors and gradients. Applies to both inference and training steps.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([16, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 4, ((T([16, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([16, 128, 28, 28], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([16, 256, 14, 14], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([16, 512, 7, 7], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 5, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([16, 64, 112, 112], f16), T([16, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Linking test_jit against Core Dependencies in CMake\nDESCRIPTION: Links the `test_jit` executable against the list of libraries specified in the `JIT_TEST_DEPENDENCIES` variable. This ensures the test executable has access to the necessary PyTorch core functions, GTest framework, and custom test libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(test_jit PRIVATE ${JIT_TEST_DEPENDENCIES})\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for PyTorch Community Docs in reStructuredText\nDESCRIPTION: This snippet configures the table of contents for PyTorch community documentation. It uses the toctree directive with glob and maxdepth options to include all files in the current directory, creating a flat structure with a maximum depth of 1.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/community/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :glob:\n   :maxdepth: 1\n\n   *\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization Forward Operations\nDESCRIPTION: Batch normalization forward pass operations with half-precision tensors, using momentum 0.1 and epsilon 1e-05. Each operation processes tensors of different shapes with their corresponding running statistics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Trace\nDESCRIPTION: Detailed trace of PyTorch operator calls showing tensor shapes, data types, and execution counts. Includes operations like softmax, matrix multiplication, embedding, and layer normalization with their input parameters and frequency of execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Softmax operations\naten._softmax.default(T([2, 12, 64, 1024], f16), -1, False)\naten._softmax.default(T([2, 12, 64, 448], f16), -1, False)\naten._softmax.default(T([2, 12, 12, 64, 512], f16), -1, False)\n\n# Matrix multiplications\naten.bmm.default(T([24, 64, 64], f16), T([24, 64, 1024], f16))\naten.addmm.default(T([768], f16), T([2048, 768], f16), T([768, 768], f16))\n\n# Layer normalization\naten.native_layer_norm.default(T([2, 1024, 768], f16), [768], T([768], f16), T([768], f16), 1e-12)\n```\n\n----------------------------------------\n\nTITLE: Logging Aten Operator: aten.native_layer_norm_backward.default (Text)\nDESCRIPTION: Log entries showing example invocations of the 'aten.native_layer_norm_backward.default' operator. Each line represents a distinct call signature observed, indicated by 'cnt'. Arguments include tensors (T) defined by shape, data type (f16, f32), optional strides, and other parameters like boolean flags.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.native_layer_norm_backward.default\ncnt: 1, ((T([64, 1, 1024], f16), T([64, 1, 1024], f16, stride=(66560, 1024, 1)), [1024], T([64, 1, 1], f32), T([64, 1, 1], f32), T([1024], f16), T([1024], f16), [True, True, True]), {})\ncnt: 8, ((T([64, 65, 1024], f16), T([64, 65, 1024], f16), [1024], T([64, 65, 1], f32), T([64, 65, 1], f32), T([1024], f16), T([1024], f16), [True, True, True]), {})\ncnt: 12, ((T([64, 257, 512], f16), T([64, 257, 512], f16), [512], T([64, 257, 1], f32), T([64, 257, 1], f32), T([512], f16), T([512], f16), [True, True, True]), {})\ncnt: 6, ((T([64, 962, 256], f16), T([64, 962, 256], f16), [256], T([64, 962, 1], f32), T([64, 962, 1], f32), T([256], f16), T([256], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization Operations\nDESCRIPTION: Batch normalization operations across different channel dimensions using float16 precision, with momentum 0.1 and epsilon 1e-05\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization\nDESCRIPTION: Batch normalization operations on tensors with different channel dimensions. Includes running mean/variance tracking and momentum parameter of 0.1.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnative_batch_norm.default((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 0.001))\n```\n\n----------------------------------------\n\nTITLE: Disabling Reduced Precision Reduction for FP16 GEMMs in PyTorch (Python)\nDESCRIPTION: Shows how to disable the use of intermediate reduced precision reductions (e.g., FP16 instead of FP32) during FP16 General Matrix Multiplications (GEMMs). Setting `torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction` to `False` ensures higher numerical precision at the potential cost of performance.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Linear Module in PyTorch\nDESCRIPTION: Implementation of a custom Linear module that inherits from nn.Module. Includes initialization of weights and bias parameters, forward pass implementation, and optional string representation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass Linear(nn.Module):\n    def __init__(self, input_features, output_features, bias=True):\n        super().__init__()\n        self.input_features = input_features\n        self.output_features = output_features\n\n        # nn.Parameter is a special kind of Tensor, that will get\n        # automatically registered as Module's parameter once it's assigned\n        # as an attribute. Parameters and buffers need to be registered, or\n        # they won't appear in .parameters() (doesn't apply to buffers), and\n        # won't be converted when e.g. .cuda() is called. You can use\n        # .register_buffer() to register buffers.\n        # nn.Parameters require gradients by default.\n        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(output_features))\n        else:\n            # You should always register all possible parameters, but the\n            # optional ones can be None if you want.\n            self.register_parameter('bias', None)\n\n        # Not a very smart way to initialize weights\n        nn.init.uniform_(self.weight, -0.1, 0.1)\n        if self.bias is not None:\n            nn.init.uniform_(self.bias, -0.1, 0.1)\n\n    def forward(self, input):\n        # See the autograd section for explanation of what happens here.\n        return LinearFunction.apply(input, self.weight, self.bias)\n\n    def extra_repr(self):\n        # (Optional)Set the extra information about this module. You can test\n        # it by printing an object of this class.\n        return 'input_features={}, output_features={}, bias={}'.format(\n            self.input_features, self.output_features, self.bias is not None\n        )\n```\n\n----------------------------------------\n\nTITLE: Concat Loop with Conditional Load\nDESCRIPTION: Example demonstrating a potential issue with eager evaluation in concat operations where out-of-bounds access could occur without proper lazy evaluation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/tensorexpr/ConditionalsInTE.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nfor i = 0 ... N\n    store ((i < 16) ? load A[i] : load B[i-16]), C\n```\n\n----------------------------------------\n\nTITLE: Generating Configuration Files\nDESCRIPTION: Configures and generates header files for different build configurations (CUDA, ROCm).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nconfigure_file(Config.h.in \"${CMAKE_CURRENT_SOURCE_DIR}/Config.h\")\nif(USE_CUDA OR USE_ROCM)\n  configure_file(cuda/CUDAConfig.h.in \"${CMAKE_CURRENT_SOURCE_DIR}/cuda/CUDAConfig.h\")\nendif()\nif(USE_ROCM)\n  configure_file(hip/HIPConfig.h.in \"${CMAKE_CURRENT_SOURCE_DIR}/hip/HIPConfig.h\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tracking Convolution Backward Operations in Neural Network\nDESCRIPTION: Lists all convolution backward operations used for gradient calculation during backpropagation. Each entry shows the gradient flow through the network layers from the output back to input, with tensor shapes and parameters matching the forward operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 1280, 1, 1], f16), T([128, 960, 1, 1], f16), T([1280, 960, 1, 1], f16), [1280], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 960, 7, 7], f16), T([128, 192, 7, 7], f16), T([960, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 7, 7], f16), T([128, 1152, 7, 7], f16), T([192, 1152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1152, 1, 1], f16), T([128, 288, 1, 1], f16), T([1152, 288, 1, 1], f16), [1152], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 288, 1, 1], f16), T([128, 1152, 1, 1], f16), T([288, 1152, 1, 1], f16), [288], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 1152, 7, 7], f16), T([128, 1152, 7, 7], f16), T([1152, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 1152, [True, True, False]), {})\ncnt: 1, ((T([128, 1152, 7, 7], f16), T([128, 192, 7, 7], f16), T([1152, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 7, 7], f16), T([128, 672, 7, 7], f16), T([192, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 672, 1, 1], f16), T([128, 168, 1, 1], f16), T([672, 168, 1, 1], f16), [672], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 168, 1, 1], f16), T([128, 672, 1, 1], f16), T([168, 672, 1, 1], f16), [168], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 2, ((T([128, 672, 14, 14], f16), T([128, 112, 14, 14], f16), T([672, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 112, 14, 14], f16), T([128, 672, 14, 14], f16), T([112, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 672, 14, 14], f16), T([128, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 1, ((T([128, 112, 14, 14], f16), T([128, 480, 14, 14], f16), T([112, 480, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 480, 1, 1], f16), T([128, 120, 1, 1], f16), T([480, 120, 1, 1], f16), [480], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 120, 1, 1], f16), T([128, 480, 1, 1], f16), T([120, 480, 1, 1], f16), [120], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), T([480, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 480, [True, True, False]), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([128, 80, 14, 14], f16), T([480, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 80, 14, 14], f16), T([128, 480, 14, 14], f16), T([80, 480, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 80, 14, 14], f16), T([128, 240, 14, 14], f16), T([80, 240, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing File-based Timer Server in PyTorch Distributed Elastic\nDESCRIPTION: Class for implementing a file-based timer server using named pipes. It is part of the server/client implementations provided by torchelastic.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nFileTimerServer\n```\n\n----------------------------------------\n\nTITLE: Embedding Gradients Calculation using aten.embedding_dense_backward\nDESCRIPTION: Computing precise gradients for embedding layers through aten.embedding_dense_backward is essential for backpropagation in neural networks. This operation necessitates PyTorch and works by propagating through the embedding layer to yield gradients for the weights.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.embedding_dense_backward.default\ncnt: 2, ((T([8, 128, 768], f16), T([8, 128], i64), 1026, -1, False), {})\ncnt: 2, ((T([8, 128, 768], f16), T([8, 128], i64), 50005, 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.threshold_backward.default in PyTorch ATen\nDESCRIPTION: Logs calls to the `aten.threshold_backward.default` operator, likely computing gradients for an operation like ReLU (thresholding at 0). Arguments include the gradient w.r.t the output tensor, the original input tensor (both 4D, f16), and the threshold value (`0`). The `cnt` indicates call frequency.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 64, 1, 1], f16), T([128, 64, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 32, 1, 1], f16), T([128, 32, 1, 1], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Matrix Multiplication in PyTorch\nDESCRIPTION: Batch matrix multiplication (bmm) operations between 3D tensors with batch size 1024, using half-precision (f16) format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([1024, 249, 192], f16), T([1024, 192, 249], f16, stride=(47808, 1, 192))), {})\n```\n\n----------------------------------------\n\nTITLE: Visualizing FX Graph Mode Quantization Flow with Mermaid\nDESCRIPTION: This Mermaid diagram illustrates the high-level flow of FX graph mode quantization, showing the steps from float model to quantized model, including prepare_fx and convert_fx stages.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n---\ntitle: High Level FX Graph Mode Quantization Flow\n---\nflowchart TD\n    classDef nofs fill:none,stroke:none\n    classDef sub fill:#D6EAF8,stroke:none\n    float_model:::nofs --> prepare_fx:::sub\n    QConfigMapping:::nofs --> prepare_fx\n    BackendConfig:::nofs --> prepare_fx\n    subgraph prepare_fx[\"_(prepare_fx/prepare_qat_fx)_\"]\n    Fuse:::nofs --> swap[QAT Module Swap]:::nofs --> obs[Insert Observers]:::nofs\n    end\n    prepare_fx --> Calibrate/Train:::nofs --> convert_fx:::sub\n    subgraph convert_fx[\"_(convert_fx)_\"]\n    Convert:::nofs --> Lowering:::nofs\n    end\n    convert_fx --> qm[Quantized Model]:::nofs\n```\n\n----------------------------------------\n\nTITLE: Tracking ReLU Backward Operations in PyTorch\nDESCRIPTION: Records threshold_backward operations used during the backpropagation of ReLU activations. These operations handle gradient flow by zeroing out gradients where the corresponding inputs were negative.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 2, ((T([128, 240, 1, 1], f16), T([128, 240, 1, 1], f16), 0), {})\ncnt: 2, ((T([128, 168, 1, 1], f16), T([128, 168, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 120, 1, 1], f16), T([128, 120, 1, 1], f16), 0), {})\ncnt: 2, ((T([128, 32, 1, 1], f16), T([128, 32, 1, 1], f16), 0), {})\ncnt: 4, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 24, 1, 1], f16), T([128, 24, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), 0), {})\ncnt: 3, ((T([128, 72, 56, 56], f16), T([128, 72, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16), 0), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Backward Operations in PyTorch\nDESCRIPTION: This snippet shows backward pass operations for convolutions, used during gradient computation. These operations propagate gradients from output tensors back to input tensors and weights, following the same pattern as the forward convolutions but in reverse.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 3, ((T([96, 512, 4, 4], f16), T([96, 512, 4, 4], f16), T([512, 512, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 512, 4, 4], f16), T([96, 256, 8, 8], f16), T([512, 256, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 512, 4, 4], f16), T([96, 256, 8, 8], f16), T([512, 256, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([96, 256, 8, 8], f16), T([96, 256, 8, 8], f16), T([256, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 256, 8, 8], f16), T([96, 128, 16, 16], f16), T([256, 128, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 256, 8, 8], f16), T([96, 128, 16, 16], f16), T([256, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([96, 128, 16, 16], f16), T([96, 128, 16, 16], f16), T([128, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 128, 16, 16], f16), T([96, 64, 32, 32], f16), T([128, 64, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 128, 16, 16], f16), T([96, 64, 32, 32], f16), T([128, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([96, 64, 32, 32], f16), T([96, 64, 32, 32], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 64, 32, 32], f16), T([96, 64, 64, 64], f16), T([64, 64, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 64, 32, 32], f16), T([96, 64, 64, 64], f16), T([64, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([96, 64, 64, 64], f16), T([96, 9, 128, 128], f16), T([64, 9, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Cloning Operations in PyTorch\nDESCRIPTION: Records occurrences of the \\\"aten.clone.default\\\" operator which duplicates tensors while keeping their contents. It focuses on tensor shapes and integer data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.clone.default\ncnt: 2, ((T([8, 128], i64),), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for PyTorch Python Binding\nDESCRIPTION: Configures all the necessary include directories for building the PyTorch Python extension. Includes paths to PyTorch core, dependencies, and third-party libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(TORCH_PYTHON_INCLUDE_DIRECTORIES\n    ${PYTHON_INCLUDE_DIR}\n\n    ${TORCH_ROOT}\n    ${TORCH_ROOT}/aten/src\n    ${TORCH_ROOT}/aten/src/TH\n\n    ${CMAKE_BINARY_DIR}\n    ${CMAKE_BINARY_DIR}/aten/src\n    ${CMAKE_BINARY_DIR}/caffe2/aten/src\n    ${CMAKE_BINARY_DIR}/third_party\n    ${CMAKE_BINARY_DIR}/third_party/onnx\n\n    ${TORCH_ROOT}/third_party/valgrind-headers\n\n    ${TORCH_ROOT}/third_party/gloo\n    ${TORCH_ROOT}/third_party/onnx\n    ${TORCH_ROOT}/third_party/flatbuffers/include\n    ${TORCH_ROOT}/third_party/kineto/libkineto/include\n    ${TORCH_ROOT}/third_party/cpp-httplib\n    ${TORCH_ROOT}/third_party/nlohmann/include\n\n    ${TORCH_SRC_DIR}/csrc\n    ${TORCH_SRC_DIR}/csrc/api/include\n    ${TORCH_SRC_DIR}/lib\n    )\n\nlist(APPEND TORCH_PYTHON_INCLUDE_DIRECTORIES ${LIBSHM_SRCDIR})\n```\n\n----------------------------------------\n\nTITLE: Adding Test Directory and Installing Headers\nDESCRIPTION: Adds the test subdirectory and configures the installation of C10 HIP headers and the generated macros file.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/hip/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_subdirectory(test)\n\n# ---[ Installation\ninstall(DIRECTORY ${CMAKE_CURRENT_LIST_DIR}\n        DESTINATION include\n        FILES_MATCHING PATTERN \"*.h\")\ninstall(FILES ${CMAKE_BINARY_DIR}/c10/hip/impl/hip_cmake_macros.h\n  DESTINATION include/c10/hip/impl)\n```\n\n----------------------------------------\n\nTITLE: Implementing AOT Autograd with Debug Compiler\nDESCRIPTION: Demonstrates AOT Autograd usage with a simple compiler function that prints the generated graph\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch.compile import aot_function\n\ndef compiler_fn(fx_module: torch.fx.GraphModule, _):\n    print(fx_module.code)\n    return fx_module\n\naot_print_fn = aot_function(fn, fw_compiler=compiler_fn, bw_compiler=compiler_fn)\n\ncloned_inputs = [x.clone().detach().requires_grad_(True) for x in (a, b, c, d)]\ncloned_a, cloned_b, cloned_c, cloned_d = cloned_inputs\nres = aot_print_fn(cloned_a, cloned_b, cloned_c, cloned_d)\nres.sum().backward()\nassert torch.allclose(ref, res)\n```\n\n----------------------------------------\n\nTITLE: Tracking PyTorch Operator Usage in Vision Transformer\nDESCRIPTION: This snippet shows a statistical breakdown of PyTorch operator calls in a Vision Transformer model implementation. Each line shows an operator name, call count, and the tensor shapes and types being processed. The operations include attention mechanisms, tensor manipulations, and neural network primitives required for transformer architecture processing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/deit_base_distilled_patch16_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 12, ((T([64, 12, 198, 198], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([64, 12, 198, 198], f16), T([64, 12, 198, 198], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([64, 12, 198, 64], f16), [768, 198, 64]), {})\ncnt: 12, ((T([64, 12, 64, 198], f16), [768, 64, 198]), {})\ncnt: 12, ((T([768, 198, 198], f16), [64, 12, 198, 198]), {})\ncnt: 12, ((T([768, 198, 64], f16), [64, 12, 198, 64]), {})\ncnt: 12, ((T([64, 198, 12, 64], f16), [64, 198, 768]), {})\ncnt: 12, ((T([64, 198, 3, 12, 64], f16), [64, 198, 2304]), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([64, 198, 768], f16), T([1, 198, 768], f16)), {})\ncnt: 49, ((T([64, 198, 768], f16), T([64, 198, 768], f16)), {})\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16)), {})\nOperator: aten.addmm.default\ncnt: 12, ((T([2304], f16), T([12672, 768], f16), T([768, 2304], f16, stride=(1, 768))), {})\ncnt: 12, ((T([768], f16), T([12672, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 12, ((T([3072], f16), T([12672, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 12, ((T([768], f16), T([12672, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 2, ((T([1000], f16), T([64, 768], f16, stride=(152064, 1)), T([768, 1000], f16, stride=(1, 768))), {})\nOperator: aten.bmm.default\ncnt: 12, ((T([768, 198, 64], f16), T([768, 64, 198], f16)), {})\ncnt: 12, ((T([768, 198, 198], f16), T([768, 198, 64], f16)), {})\ncnt: 12, ((T([768, 198, 198], f16, stride=(39204, 1, 198)), T([768, 198, 64], f16)), {})\ncnt: 12, ((T([768, 198, 64], f16), T([768, 64, 198], f16, stride=(12672, 1, 64))), {})\ncnt: 12, ((T([768, 64, 198], f16, stride=(12672, 1, 64)), T([768, 198, 198], f16)), {})\ncnt: 12, ((T([768, 198, 198], f16), T([768, 198, 64], f16, stride=(12672, 1, 198))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([64, 1, 768], f16, stride=(0, 768, 1)), T([64, 1, 768], f16, stride=(0, 768, 1)), T([64, 196, 768], f16, stride=(150528, 1, 196))], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([768, 3, 16, 16], f16), T([768], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 768, 14, 14], f16, stride=(152064, 1, 10752, 768)), T([64, 3, 224, 224], f16), T([768, 3, 16, 16], f16), [768], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\nOperator: aten.div.Tensor\ncnt: 2, ((T([64, 1000], f16), 2), {})\nOperator: aten.gelu.default\ncnt: 12, ((T([64, 198, 3072], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([64, 198, 3072], f16), T([64, 198, 3072], f16)), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([64], i64),), {})\nOperator: aten.mm.default\ncnt: 2, ((T([64, 1000], f16), T([1000, 768], f16)), {})\ncnt: 2, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 768], f16, stride=(152064, 1))), {})\ncnt: 12, ((T([12672, 768], f16), T([768, 3072], f16)), {})\ncnt: 12, ((T([768, 12672], f16, stride=(1, 768)), T([12672, 3072], f16)), {})\ncnt: 12, ((T([12672, 3072], f16), T([3072, 768], f16)), {})\ncnt: 12, ((T([3072, 12672], f16, stride=(1, 3072)), T([12672, 768], f16)), {})\ncnt: 12, ((T([12672, 768], f16), T([768, 768], f16)), {})\ncnt: 12, ((T([768, 12672], f16, stride=(1, 768)), T([12672, 768], f16)), {})\ncnt: 12, ((T([12672, 2304], f16), T([2304, 768], f16)), {})\ncnt: 12, ((T([2304, 12672], f16, stride=(1, 2304)), T([12672, 768], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 24, ((T([64, 12, 198, 198], f16), 0.125), {})\nOperator: aten.native_layer_norm.default\ncnt: 25, ((T([64, 198, 768], f16), [768], T([768], f16), T([768], f16), 1e-06), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 25, ((T([64, 198, 768], f16), T([64, 198, 768], f16), [768], T([64, 198, 1], f32), T([64, 198, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\nOperator: aten.select_backward.default\ncnt: 1, ((T([64, 768], f16), [64, 198, 768], 1, 1), {})\ncnt: 1, ((T([64, 768], f16), [64, 198, 768], 1, 0), {})\nOperator: aten.slice_backward.default\ncnt: 2, ((T([64, 198, 768], f16), [64, 198, 768], 0, 0, 9223372036854775807, 1), {})\nOperator: aten.stack.default\ncnt: 12, (([T([64, 12, 198, 64], f16), T([64, 12, 198, 64], f16, stride=(152064, 12672, 1, 198)), T([64, 12, 198, 64], f16)],), {})\nOperator: aten.sum.SymInt\ncnt: 2, ((T([64, 1000], f16), [0], True), {})\ncnt: 24, ((T([12672, 768], f16), [0], True), {})\ncnt: 12, ((T([12672, 3072], f16), [0], True), {})\ncnt: 12, ((T([12672, 2304], f16), [0], True), {})\ncnt: 1, ((T([64, 198, 768], f16), [0], True), {})\ncnt: 2, ((T([64, 1, 768], f16, stride=(152064, 768, 1)), [0], True), {})\nOperator: aten.unbind.int\ncnt: 12, ((T([3, 64, 12, 198, 64], f16, stride=(768, 456192, 64, 2304, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CUDAGraph Tensor Access Limitation in PyTorch\nDESCRIPTION: Example showing how accessing tensor outputs from a previous CUDAGraph invocation can lead to runtime errors due to memory being overwritten by subsequent runs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_cudagraph_trees.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile(mode=\"reduce-overhead\")\ndef my_model(x):\n    y = torch.matmul(x, x)\n    return y\n\nx = torch.randn(10, 10, device=\"cuda\")\ny1 = my_model(x)\ny2 = my_model(x)\nprint(y1)\n# RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.sigmoid.default Operator (Text)\nDESCRIPTION: Logs calls to the sigmoid activation function (`aten.sigmoid.default`). The arguments show the shapes of the input tensors (e.g., [128, 256, 1, 1] f16). Different invocation counts are recorded for various input tensor dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sigmoid.default\ncnt: 1, ((T([128, 256, 1, 1], f16),), {})\ncnt: 2, ((T([128, 512, 1, 1], f16),), {})\ncnt: 9, ((T([128, 1536, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows the tensor shapes and parameters for batch normalization operations at different layers of the model. It includes running mean and variance calculations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 112, 56, 56], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), True, 0.1, 1e-05), {})\ncnt: 21, ((T([128, 14, 56, 56], f16), T([14], f16), T([14], f16), T([14], f16), T([14], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Tensor Creation Operations\nDESCRIPTION: This snippet demonstrates the usage of the aten.new_empty_strided.default operator for creating new tensors with specified shapes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.new_empty_strided.default\ncnt: 1, ((T([256, 128, 3, 3], f16, stride=(1152, 1, 384, 128)), [256, 128, 3, 3], [1152, 9, 3, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 1, ((T([128, 64, 3, 3], f16, stride=(576, 1, 192, 64)), [128, 64, 3, 3], [576, 9, 3, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\n```\n\n----------------------------------------\n\nTITLE: Unbinding and Unsqueezing Tensors using aten.unbind and aten.unsqueeze_ - Python\nDESCRIPTION: Presents examples of aten.unbind.int for splitting tensors along a dimension and aten.unsqueeze_.default for inserting singleton dimensions (in-place). Operates over tensors of various ranks and dtypes, highlighting shape manipulations central to neural network data flows. Requires PyTorch and appropriate tensor shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.unbind.int\ncnt: 12, ((T([2, 16, 64], f32),), {})\ncnt: 12, ((T([2, 12, 14, 3], i64),), {})\nOperator: aten.unsqueeze_.default\ncnt: 1, ((T([2, 12, 64, 192], f32), 1), {})\ncnt: 12, ((T([12, 14, 3], i64), 0), {})\ncnt: 48, ((T([2, 12, 64, 64], f16), 2), {})\n```\n\n----------------------------------------\n\nTITLE: Adding Sparse COO Tensors in PyTorch\nDESCRIPTION: This example shows how addition of sparse COO tensors is implemented by concatenating the indices and values tensors. It demonstrates the efficiency of operations on uncoalesced tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/sparse.rst#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n>>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n>>> a + b\ntensor(indices=tensor([[0, 0, 1, 1]]),\n       values=tensor([7, 8, 5, 6]),\n       size=(2,), nnz=4, layout=torch.sparse_coo)\n```\n\n----------------------------------------\n\nTITLE: Configuring Link Libraries for PyTorch Python Binding\nDESCRIPTION: Sets up the required libraries to link against when building the PyTorch Python extension. Includes Python, pybind11, and other dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(TORCH_PYTHON_LINK_LIBRARIES\n    Python::Module\n    pybind::pybind11\n    opentelemetry::api\n    httplib\n    nlohmann\n    shm\n    fmt::fmt-header-only\n    ATEN_CPU_FILES_GEN_LIB)\n\nif(USE_ASAN AND TARGET Sanitizer::address)\n  list(APPEND TORCH_PYTHON_LINK_LIBRARIES Sanitizer::address)\nendif()\nif(USE_ASAN AND TARGET Sanitizer::undefined)\n  list(APPEND TORCH_PYTHON_LINK_LIBRARIES Sanitizer::undefined)\nendif()\nif(USE_TSAN AND TARGET Sanitizer::thread)\n  list(APPEND TORCH_PYTHON_LINK_LIBRARIES Sanitizer::thread)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Executing Softmax Backward Data Operation in PyTorch\nDESCRIPTION: Performs the backward phase for the softmax operation on tensors of type f16 with dimensions matching the forward operation to compute gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"Operator: aten._softmax_backward_data.default\\ncnt: 24, ((T([32, 128, 128], f16), T([32, 128, 128], f16), -1, f16), {})\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage for EfficientNet Model\nDESCRIPTION: This code snippet represents a log or analysis output of PyTorch operators used in an EfficientNet model. It shows the operator names, their call counts, and the tensor shapes they operate on. This information is crucial for understanding the model's computational graph and optimizing performance.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/repvgg_a2_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 61, ((T([], i64), 1), {})\ncnt: 2, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16)), {})\ncnt: 6, ((T([128, 96, 56, 56], f16), T([128, 96, 56, 56], f16)), {})\ncnt: 14, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16)), {})\ncnt: 54, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16)), {})\ncnt: 1, ((T([128, 1408, 7, 7], f16), T([128, 1408, 7, 7], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 1408], f16), T([1408, 1000], f16, stride=(1, 1408))), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([64, 3, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 3, 224, 224], f16), T([64, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([96, 64, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([96, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([96, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([96, 96, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([192, 96, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([192, 96, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 192, 28, 28], f16), T([192, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([128, 192, 28, 28], f16), T([192, 192, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([384, 192, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([384, 192, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 13, ((T([128, 384, 14, 14], f16), T([384, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 13, ((T([128, 384, 14, 14], f16), T([384, 384, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 384, 14, 14], f16), T([1408, 384, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 384, 14, 14], f16), T([1408, 384, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 1408, 7, 7], f16), T([128, 384, 14, 14], f16), T([1408, 384, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1408, 7, 7], f16), T([128, 384, 14, 14], f16), T([1408, 384, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 13, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384, 384, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 13, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384, 384, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 384, 14, 14], f16), T([128, 192, 28, 28], f16), T([384, 192, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 384, 14, 14], f16), T([128, 192, 28, 28], f16), T([384, 192, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192, 192, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([128, 96, 56, 56], f16), T([192, 96, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 28, 28], f16), T([128, 96, 56, 56], f16), T([192, 96, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([128, 96, 56, 56], f16), T([96, 96, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([128, 96, 56, 56], f16), T([96, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([128, 64, 112, 112], f16), T([96, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([128, 64, 112, 112], f16), T([96, 64, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([128, 3, 224, 224], f16), T([64, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([128, 3, 224, 224], f16), T([64, 3, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 1408, 7, 7], f16, stride=(1408, 1, 0, 0)), 49), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 1408, 7, 7], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 1408], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1408], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 5, ((T([128, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})\ncnt: 11, ((T([128, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 41, ((T([128, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f16), True, 0.1, 1e-05), {})\ncnt: 2, ((T([128, 1408, 7, 7], f16), T([1408], f16), T([1408], f16), T([1408], f16), T([1408], f16), True, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 2, ((T([128, 1408, 7, 7], f16), T([128, 1408, 7, 7], f16), T([1408], f16), T([1408], f16), T([1408], f16), T([1408], f32), T([1408], f32), True, 1e-05, [True, True, True]), {})\ncnt: 41, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f32), T([384], f32), True, 1e-05, [True, True, True]), {})\ncnt: 11, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([128, 96, 56, 56], f16), T([128, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 64, 112, 112], f16),), {})\ncnt: 2, ((T([128, 96, 56, 56], f16),), {})\ncnt: 4, ((T([128, 192, 28, 28], f16),), {})\ncnt: 14, ((T([128, 384, 14, 14], f16),), {})\ncnt: 1, ((T([128, 1408, 7, 7], f16),), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 1408, 7, 7], f16), T([128, 1408, 7, 7], f16), 0), {})\ncnt: 14, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), 0), {})\ncnt: 4, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), 0), {})\ncnt: 2, ((T([128, 96, 56, 56], f16), T([128, 96, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Importing Benchmark Utils Module in PyTorch\nDESCRIPTION: This snippet shows how to import and set the current module to torch.utils.benchmark in Python documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: torch.utils.benchmark\n.. currentmodule:: torch.utils.benchmark\n```\n\n----------------------------------------\n\nTITLE: vmap with Side-Effectful Mutation (Discouraged) - PyTorch - Python\nDESCRIPTION: This Python snippet demonstrates a function with side effects (mutation of a list and printing) wrapped with vmap. The function 'f' pops an element from a list and prints a message, then sums a tensor. vmap is applied with mismatched batching (in_dims), and the example shows that side effects (like mutation and print) only happen once, not per batch. The code requires PyTorch and torch.func. This highlights a pattern to avoidusing mutation or print statements within vmap-mapped functions leads to unexpected or incorrect behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef f(x, list):\n  list.pop()\n  print(\"hello!\")\n  return x.sum(0)\n\nx = torch.randn(3, 1)\nlst = [0, 1, 2, 3]\n\nresult = vmap(f, in_dims=(0, None))(x, lst)\n```\n\n----------------------------------------\n\nTITLE: CUDA Kernel Implementation Functions\nDESCRIPTION: CUDA-specific implementations for tensor operations including parallel cat operations and GPU kernel wrappers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_24\n\nLANGUAGE: CUDA\nCODE:\n```\n_ZN2at6native40_GLOBAL__N__5df19e2c_8_Shape_cu_49f7391c12parallel_catINS1_10OpaqueTypeILj2EEELi64ELi64EEEvRKNS_6TensorERKSt6vectorISt17reference_wrapperIS6_ESaISA_EEliN3c1012MemoryFormatE\n_ZN2at6native15gpu_kernel_implI18__nv_hdl_wrapper_tILb0ELb1ELb0E11__nv_dl_tagIPFvRNS_18TensorIteratorBaseEEXadL_ZNS0_62_GLOBAL__N__82494415_23_ActivationSiluKernel_cu_f9d27b8c_3273411silu_kernelES5_EELj5EEFN3c104HalfESB_EJEEEEvS5_RKT_\n```\n\n----------------------------------------\n\nTITLE: Convolutional Operations in PyTorch ResNet Model\nDESCRIPTION: This snippet shows convolution operations that form the backbone of a ResNet model. It includes initial convolutions, residual block convolutions, and bottleneck convolutions with varying kernel sizes, strides, and channel dimensions using half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([4, 3, 1184, 1216], f16), T([64, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 64, 296, 304], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([4, 64, 296, 304], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([4, 64, 296, 304], f16), T([256, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([4, 256, 296, 304], f16), T([64, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 256, 296, 304], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 128, 296, 304], f16), T([128, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([4, 128, 148, 152], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 256, 296, 304], f16), T([512, 256, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([4, 512, 148, 152], f16), T([128, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([4, 128, 148, 152], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 512, 148, 152], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 256, 148, 152], f16), T([256, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 6, ((T([4, 256, 74, 76], f16), T([1024, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 512, 148, 152], f16), T([1024, 512, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([4, 1024, 74, 76], f16), T([256, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([4, 256, 74, 76], f16), T([256, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 1024, 74, 76], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 512, 74, 76], f16), T([512, 512, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([4, 512, 37, 38], f16), T([2048, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 1024, 74, 76], f16), T([2048, 1024, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([4, 2048, 37, 38], f16), T([512, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([4, 512, 37, 38], f16), T([512, 512, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([4, 2048, 37, 38], f16), T([256, 2048, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([4, 256, 37, 38], f16), T([256, 256, 3, 3], f16), T([256], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Cloning Tensors with aten.clone in Python\nDESCRIPTION: The aten.clone.default operator is frequently used to create a duplicate tensor while preserving its dimensions and data types. This operation allows for separate processing paths without altering the original data, ensuring data integrity across parallel processes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([256, 197951], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Division Operations\nDESCRIPTION: Division operations between tensors and scalar values, handling various tensor shapes and broadcast patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n((T([3, 427, 640], f16, stride=(1, 1920, 3)), T([3, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Applying avg_pool2d and avg_pool2d_backward Operators - PyTorch - Python\nDESCRIPTION: Shows configuration and arguments to forward and backward average pooling layers. Parameters such as kernel size, stride, padding, ceil_mode, and count_include_pad are specified for spatial reduction in neural nets. Actual input and output shapes match the feature map dimensions before and after pooling operations, both in forward and backward passes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ecaresnet101d_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([64, 256, 56, 56], f16), [2, 2], [2, 2], [0, 0], True, False), {})\ncnt: 1, ((T([64, 512, 28, 28], f16), [2, 2], [2, 2], [0, 0], True, False), {})\ncnt: 1, ((T([64, 1024, 14, 14], f16), [2, 2], [2, 2], [0, 0], True, False), {})\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([64, 1024, 7, 7], f16), T([64, 1024, 14, 14], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([64, 512, 28, 28], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([64, 256, 56, 56], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication in PyTorch with Half-Precision Tensors\nDESCRIPTION: Matrix multiplication operations with different input shapes. Operations use half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/inception_v3_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1000], f16), T([1000, 2048], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Multiplication Operations\nDESCRIPTION: This snippet shows matrix multiplication operations between tensors in f16 precision. The first multiplies a [128, 1000] tensor with a [1000, 1984] tensor, and the second multiplies a [1000, 128] tensor with a [128, 1984] tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 1984], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1984], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing aten.threshold_backward.default Operations in PyTorch\nDESCRIPTION: This code snippet demonstrates the usage of the aten.threshold_backward.default operator in PyTorch. It shows various tensor shapes and their occurrence counts, all using f16 (float16) data type and a threshold value of 0.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([64, 1536, 7, 7], f16), T([64, 1536, 7, 7], f16), 0), {})\ncnt: 2, ((T([64, 120, 56, 56], f16), T([64, 120, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 192, 56, 56], f16), T([64, 192, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 192, 112, 112], f16), T([64, 192, 112, 112], f16), 0), {})\ncnt: 2, ((T([64, 32, 112, 112], f16), T([64, 32, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Adding Include Directories for test_jit in CMake\nDESCRIPTION: Adds the ATen CPU include directory (`${ATen_CPU_INCLUDE}`) to the list of include directories for the `test_jit` target. This ensures the compiler can find necessary header files from the ATen library during compilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(test_jit PRIVATE ${ATen_CPU_INCLUDE})\n```\n\n----------------------------------------\n\nTITLE: Appending Lite Interpreter Build Flags in CMake\nDESCRIPTION: Conditionally appends C++ preprocessor definitions based on build options related to the Lite Interpreter. It adds `-DBUILD_LITE_INTERPRETER` if BUILD_LITE_INTERPRETER is set, and `-DEDGE_PROFILER_USE_KINETO` if both BUILD_LITE_INTERPRETER and USE_LITE_INTERPRETER_PROFILER are set.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_41\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_LITE_INTERPRETER)\n  string(APPEND CMAKE_CXX_FLAGS \" -DBUILD_LITE_INTERPRETER\")\nendif()\n\nif(TRACING_BASED)\n  string(APPEND CMAKE_CXX_FLAGS \" -DTRACING_BASED\")\nendif()\n\nif(USE_PYTORCH_METAL)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_PYTORCH_METAL\")\nendif()\n\nif(USE_PYTORCH_METAL_EXPORT)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_PYTORCH_METAL_EXPORT\")\nendif()\n\nif(USE_SOURCE_DEBUG_ON_MOBILE)\n  string(APPEND CMAKE_CXX_FLAGS \" -DSYMBOLICATE_MOBILE_DEBUG_HANDLE\")\nendif()\n\nif(BUILD_LITE_INTERPRETER AND USE_LITE_INTERPRETER_PROFILER)\n  string(APPEND CMAKE_CXX_FLAGS \" -DEDGE_PROFILER_USE_KINETO\")\nendif()\n\nif(USE_COREML_DELEGATE)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_COREML_DELEGATE\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tensor Summation Operations in PyTorch (Default)\nDESCRIPTION: This snippet shows a tensor summation operation that reduces all dimensions to a scalar value. This is typically used for computing total loss or other global statistics from tensor values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.default\ncnt: 1, ((T([96, 65], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Gather Operation on PyTorch Tensors\nDESCRIPTION: This snippet demonstrates the use of the gather operation in PyTorch. It shows multiple instances of gathering elements from a tensor of shape [965] using various index tensors, all with float16 data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nT([965], f16).gather(0, T([54824], i64))  # Gather elements using index tensor of shape [54824]\nT([965], f16).gather(0, T([54763], i64))  # Gather elements using index tensor of shape [54763]\nT([965], f16).gather(0, T([54783], i64))  # Gather elements using index tensor of shape [54783]\n# ... (multiple similar gather operations with different index tensor shapes)\n```\n\n----------------------------------------\n\nTITLE: Loss Function Calculations\nDESCRIPTION: NLL (Negative Log Likelihood) loss calculations for model training, including both forward and backward passes with half precision tensors and integer labels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Automating Class Documentation in PyTorch with Sphinx\nDESCRIPTION: Uses the autoclass directive to automatically generate documentation for a PyTorch class. It includes all members of the class but not inherited members.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/classtemplate.rst#2025-04-22_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: {{ name }}\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Patterns\nDESCRIPTION: This code snippet represents a summary of PyTorch operator usage in a deep learning model. It includes details on operator types, input tensor shapes, data types, and call frequencies. The analysis covers operations like log_softmax, add, and cat across different tensor configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([32, 1000], f16), 1, False), {})\n\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([32, 1000], f16), T([32, 1000], f16), 1, f16), {})\n\nOperator: aten.add.Tensor\ncnt: 111, ((T([], i64), 1), {})\ncnt: 1, ((T([32, 256, 56, 56], f16, stride=(928256, 3136, 56, 1)), T([32, 256, 56, 56], f16, stride=(865536, 3136, 56, 1))), {})\n# ... (more add.Tensor operations)\n\nOperator: aten.cat.default\ncnt: 1, (([T([32, 40, 56, 56], f16, stride=(928256, 3136, 56, 1)), T([32, 20, 56, 56], f16, stride=(865536, 3136, 56, 1))], 1), {})\n# ... (more cat.default operations)\n```\n\n----------------------------------------\n\nTITLE: Tensor Concatenation Operations\nDESCRIPTION: Series of tensor concatenation operations with varying dimensions and strides, primarily working with half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, (([T([32, 512, 28, 28], f16), T([32, 192, 28, 28], f16)], 1), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations for Deep Learning Model\nDESCRIPTION: This code snippet represents a collection of PyTorch tensor operations used in a deep learning model. It includes various operators such as softmax, matrix multiplication, convolution, and layer normalization. The operations are presented with their input tensor shapes, data types (primarily float16), and usage count.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tnt_s_patch16_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\n\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\n\nOperator: aten._softmax.default\ncnt: 12, ((T([12544, 4, 16, 16], f16), -1, False), {})\ncnt: 12, ((T([64, 6, 197, 197], f16), -1, False), {})\n\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([64, 6, 197, 197], f16), T([64, 6, 197, 197], f16), -1, f16), {})\ncnt: 12, ((T([12544, 4, 16, 16], f16), T([12544, 4, 16, 16], f16), -1, f16), {})\n\n# ... (truncated for brevity)\n\nOperator: aten.unbind.int\ncnt: 12, ((T([2, 12544, 4, 16, 6], f16, stride=(24, 768, 6, 48, 1)),), {})\ncnt: 12, ((T([2, 64, 6, 197, 64], f16, stride=(384, 151296, 64, 768, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Threshold Backward Operation Calls\nDESCRIPTION: Logs of aten.threshold_backward.default operations used in ReLU gradient computations. Each line shows tensors with shape [batch_size, channels, 1, 1] and the threshold value (0).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 87, 1, 1], f16), T([128, 87, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 81, 1, 1], f16), T([128, 81, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 75, 1, 1], f16), T([128, 75, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 70, 1, 1], f16), T([128, 70, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 64, 1, 1], f16), T([128, 64, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 58, 1, 1], f16), T([128, 58, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 53, 1, 1], f16), T([128, 53, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 47, 1, 1], f16), T([128, 47, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 42, 1, 1], f16), T([128, 42, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 36, 1, 1], f16), T([128, 36, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 30, 1, 1], f16), T([128, 30, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 25, 1, 1], f16), T([128, 25, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 19, 1, 1], f16), T([128, 19, 1, 1], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch NLL Loss Backward Operation\nDESCRIPTION: NLL Loss backward operation with half-precision (f16) tensors on CUDA device. Takes input tensors of shapes [128, 1000] and [128] for gradients calculation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Handling Unsafe Globals in NumPy >= 1.25 for PyTorch Serialization\nDESCRIPTION: Example of an error message when encountering unsafe globals with NumPy >= 1.25 during PyTorch serialization, and how to allowlist the problematic type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nWeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,\nbut got <class 'numpy.dtypes.Float32DType'>\n```\n\n----------------------------------------\n\nTITLE: Configuring Mobile-Specific Build Options for PyTorch\nDESCRIPTION: Sets up specific build options for mobile targets, disabling features not needed on mobile and configuring options for autograd, Python support, and BLAS implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\n# Build libtorch mobile library, which contains ATen/TH ops and native support\n# for TorchScript model, but doesn't contain not-yet-unified caffe2 ops;\nif(INTERN_BUILD_MOBILE)\n  if(NOT BUILD_SHARED_LIBS AND NOT \"${SELECTED_OP_LIST}\" STREQUAL \"\")\n    string(APPEND CMAKE_CXX_FLAGS \" -DNO_EXPORT\")\n  endif()\n  if(BUILD_MOBILE_AUTOGRAD)\n    set(INTERN_DISABLE_AUTOGRAD OFF)\n  else()\n    set(INTERN_DISABLE_AUTOGRAD ON)\n  endif()\n  set(BUILD_PYTHON OFF)\n  set(BUILD_FUNCTORCH OFF)\n  set(USE_DISTRIBUTED OFF)\n  set(NO_API ON)\n  set(USE_FBGEMM OFF)\n  set(INTERN_DISABLE_ONNX ON)\n  if(USE_BLAS)\n    set(INTERN_USE_EIGEN_BLAS ON)\n  else()\n    set(INTERN_USE_EIGEN_BLAS OFF)\n  endif()\n  # Disable developing mobile interpreter for actual mobile build. Enable it\n  # elsewhere to capture build error.\n  set(INTERN_DISABLE_MOBILE_INTERP ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Default Code Coverage for All Tests\nDESCRIPTION: This snippet shows how to run the code coverage tool with default settings, which will run all tests and collect coverage over the entire PyTorch folder.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython oss_coverage.py\n```\n\n----------------------------------------\n\nTITLE: Conditionally Enabling ASAN/UBSAN for test_jit in CMake\nDESCRIPTION: Checks if the `USE_ASAN` flag is enabled. If true, it adds the `-fsanitize=undefined` flag to both the compile and link steps for the `test_jit` target. This enables Undefined Behavior Sanitizer (UBSAN) checks during the build and execution of the tests.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\n# We also build with UBSAN flag in build_asan.h\nif(USE_ASAN)\n  target_compile_options(test_jit PRIVATE \"-fsanitize=undefined\")\n  target_link_libraries(test_jit PRIVATE \"-fsanitize=undefined\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using aten.split_with_sizes.default Operator in PyTorch\nDESCRIPTION: Examples of the aten.split_with_sizes.default operator being applied to tensors of various shapes. This operator splits a tensor into chunks of specified sizes along a given dimension (dimension 1 in all cases shown).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/coat_lite_mini_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.split_with_sizes.default\ncnt: 2, ((T([128, 64, 56, 56], f16, stride=(602304, 1, 10752, 192)), [16, 24, 24], 1), {})\ncnt: 2, ((T([128, 128, 28, 28], f16, stride=(301440, 1, 10752, 384)), [32, 48, 48], 1), {})\ncnt: 2, ((T([128, 320, 14, 14], f16, stride=(189120, 1, 13440, 960)), [80, 120, 120], 1), {})\ncnt: 2, ((T([128, 512, 7, 7], f16, stride=(76800, 1, 10752, 1536)), [128, 192, 192], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing File-based Timer Client in PyTorch Distributed Elastic\nDESCRIPTION: Class for implementing a file-based timer client using named pipes. It is part of the server/client implementations provided by torchelastic.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nFileTimerClient\n```\n\n----------------------------------------\n\nTITLE: Executing aten.avg_pool2d operation in PyTorch\nDESCRIPTION: Averages two-dimensional pooling on a tensor via the Aten backend using specified pooling window, stride, and padding, with outputs a reduced tensor. Dependencies include PyTorch's tensor framework, input tensor shape, and pooling parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([128, 256, 56, 56], f16), [2, 2], [2, 2], [0, 0], True, False), {})\n```\n\n----------------------------------------\n\nTITLE: Log Entry for Tensor Pair (Shape [128, 64, 28, 28], f16, Stride)\nDESCRIPTION: Logs the occurrence (count 6) of a tensor pair. The first tensor has shape [128, 64, 28, 28], dtype f16, and specific strides (200704, 784, 28, 1). The second tensor has the same shape and dtype but default strides. Likely generated during PyTorch execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: text\nCODE:\n```\ncnt: 6, ((T([128, 64, 28, 28], f16, stride=(200704, 784, 28, 1)), T([128, 64, 28, 28], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Backward Softmax Operation in PyTorch\nDESCRIPTION: This snippet highlights the use of `aten._log_softmax_backward_data.default`, which performs the backward operation of log softmax. It requires tensors for the input and gradient, along with the dimension and data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForCausalLM_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([2048, 50005], f16), T([2048, 50005], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Using CUDAMultiStreamGuard for Multiple Streams on Multiple Devices in PyTorch (C++)\nDESCRIPTION: This example illustrates the use of at::cuda::CUDAMultiStreamGuard to manage multiple CUDA streams across multiple devices simultaneously. It sets streams on device 0 and 1 without changing the current device index.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n{\n  // This is the same as calling `torch::cuda::setCurrentCUDAStream` on both streams\n  at::cuda::CUDAMultiStreamGuard multi_guard({streams0[1], streams1[1]});\n\n  // current device index is not change, still 0\n  // current CUDA stream on device 0 and device 1 are set to `streams0[1]` and `streams1[1]`\n}\n// current CUDA stream on device 0 and device 1 are reset to `streams0[0]` and `streams1[0]`\n// after `multi_guard` is destroyed.\n```\n\n----------------------------------------\n\nTITLE: Tracking Native Batch Normalization Calls (aten.native_batch_norm) - PyTorch - Python\nDESCRIPTION: This snippet enumerates call configurations of aten.native_batch_norm, capturing input and parameter tensor shapes and flags for training mode, epsilon, and momentum. Useful in determining how normalization layers are applied per-channel. Relevant for half precision workflows and input validation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([1, 16, 27], f16), T([16], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 32, 144], f16), T([32], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 64, 288], f16), T([64], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 5, ((T([1, 128, 576], f16), T([128], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 256, 128], f16), T([256], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 64, 128], f16), T([64], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 2, ((T([1, 64, 576], f16), T([64], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 256, 64], f16), T([256], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 512, 256], f16), T([512], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 128, 256], f16), T([128], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 2, ((T([1, 512, 128], f16), T([512], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 128, 512], f16), T([128], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 1536, 512], f16), T([1536], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 384, 512], f16), T([384], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 18, ((T([1, 384, 576], f16), T([384], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 9, ((T([1, 1536, 384], f16), T([1536], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 8, ((T([1, 384, 1536], f16), T([384], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 1536, 1536], f16), T([1536], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 2304, 1536], f16), T([2304], f16), None, None, None, True, 0.0, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: ReLU Activation in PyTorch\nDESCRIPTION: This snippet shows the application of ReLU (Rectified Linear Unit) activation function on tensors of various shapes. It demonstrates in-place operations on 16-bit float tensors across different layers of a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 64, 112, 112], f16),), {})\ncnt: 3, ((T([128, 128, 56, 56], f16),), {})\ncnt: 9, ((T([128, 32, 56, 56], f16),), {})\ncnt: 4, ((T([128, 256, 56, 56], f16),), {})\ncnt: 12, ((T([128, 64, 28, 28], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Headers for Installation Based on Build Configuration in CMake\nDESCRIPTION: Initializes the `INSTALL_HEADERS` list with core ATen headers. It then conditionally appends backend-specific headers (native, CPU, sparse, quantized, CUDA, HIP, XPU, MPS, KleidiAI, MiOPEN, MKL-DNN/oneDNN) based on the `INTERN_BUILD_MOBILE` flag. Further conditional logic within the mobile/non-mobile branches handles specific Metal header installation based on flags like `USE_PYTORCH_METAL_EXPORT`, `USE_PYTORCH_METAL`, `APPLE`, and `IOS`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\nset(INSTALL_HEADERS ${base_h} ${ATen_CORE_HEADERS} ${native_nested_h} ${ATen_TRANSFORMER_HEADERS})\nif(NOT INTERN_BUILD_MOBILE)\n  list(APPEND INSTALL_HEADERS ${native_h} ${native_cpu_h} ${native_ao_sparse_h} ${native_quantized_h} ${cuda_h} ${native_cuda_h} ${native_hip_h} ${cudnn_h} ${hip_h} ${xpu_h} ${mps_h} ${native_kleidiai_h} ${native_mps_h} ${native_utils_h} ${miopen_h} ${mkldnn_xpu_h})\n  # Metal\n  if(USE_PYTORCH_METAL_EXPORT)\n    # Add files needed from exporting metal models(optimized_for_mobile)\n    list(APPEND INSTALL_HEADERS ${metal_h} ${metal_prepack_h})\n  elseif(APPLE AND USE_PYTORCH_METAL)\n    # Needed by Metal kernels\n    list(APPEND INSTALL_HEADERS ${metal_h} ${native_metal_h})\n  else()\n    list(APPEND INSTALL_HEADERS ${metal_h})\n  endif()\nelse()\n  if(IOS AND USE_PYTORCH_METAL)\n      list(APPEND INSTALL_HEADERS ${metal_h} ${native_metal_h})\n  else()\n      list(APPEND INSTALL_HEADERS ${metal_h} ${metal_prepack_h})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Role in reStructuredText\nDESCRIPTION: This snippet defines a custom role 'hidden' in reStructuredText, which can be used to create hidden sections in the documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: PyTorch Activation Functions\nDESCRIPTION: LeakyReLU and ReLU activation operations with different tensor shapes and alpha values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Super_SloMo_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n((T([6, 32, 352, 352], f16), 0.1), {})\n```\n\n----------------------------------------\n\nTITLE: Logging Aten Operator: aten.nll_loss_forward.default (Text)\nDESCRIPTION: Log entry for an invocation of the 'aten.nll_loss_forward.default' operator. Arguments include input tensors (T) with shapes and data types (f16, i64), a None placeholder, and integer parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Shape Analysis\nDESCRIPTION: Log entries showing tensor shape patterns and their occurrence counts in a PyTorch model. Each line shows the count (cnt), followed by a tuple containing tensor dimensions [batch_size, channels, height, width] in float16 (f16) format. The data appears to be tracking identical tensor pairs with matching dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 7, 7], f16), 0), {})\ncnt: 5, ((T([128, 672, 14, 14], f16), T([128, 672, 14, 14], f16), 0), {})\ncnt: 2, ((T([128, 336, 14, 14], f16), T([128, 336, 14, 14], f16), 0), {})\ncnt: 6, ((T([128, 384, 14, 14], f16), T([128, 384, 14, 14], f16), 0), {})\ncnt: 3, ((T([128, 192, 14, 14], f16), T([128, 192, 14, 14], f16), 0), {})\ncnt: 5, ((T([128, 192, 28, 28], f16), T([128, 192, 28, 28], f16), 0), {})\ncnt: 2, ((T([128, 96, 28, 28], f16), T([128, 96, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([128, 144, 56, 56], f16), 0), {})\ncnt: 4, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([128, 96, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 96, 112, 112], f16), T([128, 96, 112, 112], f16), 0), {})\ncnt: 3, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Concatenating Tensors along a Specified Dimension in Python\nDESCRIPTION: The function concatenates a list of tensors along the specified dimension. Dependencies include input tensors of shapes [16, 3, 128, 128] and [16, 5, 128, 128] with data type f16. This is useful for combining feature maps in neural networks. The operation outputs a tensor with increased size along the concatenation dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.cat.default\ncnt: 1, (([T([16, 3, 128, 128], f16), T([16, 5, 128, 128], f16)], 1), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Log\nDESCRIPTION: Log of PyTorch operator calls showing tensor operations like convolutions, softmax, and backward passes. Includes tensor shapes, strides, and parameters for each operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\naten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\naten.add.Tensor\ncnt: 44, ((T([], i64), 1), {})\ncnt: 3, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})\ncnt: 3, ((T([128, 56, 28, 28], f16), T([128, 56, 28, 28], f16)), {})\ncnt: 12, ((T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16)), {})\ncnt: 20, ((T([128, 368, 7, 7], f16), T([128, 368, 7, 7], f16)), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch CPU Module Functions List\nDESCRIPTION: Autogenerated documentation summary listing key CPU-related functions in PyTorch including device management and stream operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cpu.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncurrent_device\ncurrent_stream\nis_available\nsynchronize\nstream\nset_device\ndevice_count\nStreamContext\n```\n\n----------------------------------------\n\nTITLE: Average Pooling Operations in PyTorch\nDESCRIPTION: Statistics for the aten.avg_pool2d.default operator showing forward pass of average pooling. These operations use 2x2 kernels with stride 2 and no padding, typically used for downsampling feature maps in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([128, 256, 48, 48], f16), [2, 2], [2, 2], [0, 0], True, False), {})\ncnt: 1, ((T([128, 512, 24, 24], f16), [2, 2], [2, 2], [0, 0], True, False), {})\ncnt: 1, ((T([128, 1536, 12, 12], f16), [2, 2], [2, 2], [0, 0], True, False), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Convolution Backward Operations\nDESCRIPTION: This snippet shows the usage of the aten.convolution_backward.default operator for computing gradients in convolutional layers with various shapes and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([3, 3, 512, 512], f16, stride=(0, 0, 0, 0)), T([3, 64, 518, 518], f16), T([3, 64, 7, 7], f16), [3], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([3, 64, 512, 512], f16), T([3, 256, 512, 512], f16), T([64, 256, 3, 3], f16), [64], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Incorrect Use of torch.zeros in batched function (vmap-incompatible) - PyTorch - Python\nDESCRIPTION: This Python example shows how using global tensor factory functions (like torch.zeros) inside a function intended for vmap can produce shape mismatches and errors. In diag_embed, result is initialized with torch.zeros, which fails under vmap because the underlying shape may not match batched inputs. Dependencies: PyTorch, torch.func. Limitation: Factory functions should generally be replaced with batched equivalents when using vmap.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/func.ux_limitations.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef diag_embed(vec):\n  assert vec.dim() == 1\n  result = torch.zeros(vec.shape[0], vec.shape[0])\n  result.diagonal().copy_(vec)\n  return result\n\nvecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\n\n# RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ...\nvmap(diag_embed)(vecs)\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.sum.SymInt Operator (Log)\nDESCRIPTION: Log entries detailing calls to the PyTorch `aten.sum.SymInt` operator. Each line shows the invocation count (`cnt`) and arguments: the input tensor (shape, dtype f16, sometimes stride), the dimension(s) to sum over, and a boolean indicating whether to keep the dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 2, ((T([64, 1000], f16), [0], True), {})\ncnt: 6, ((T([64, 256], f16, stride=(50432, 1)), [0], True), {})\ncnt: 3, ((T([64, 128], f16), [0], True), {})\ncnt: 12, ((T([25664, 128], f16), [0], True), {})\ncnt: 3, ((T([64, 1, 128], f16), [0, 1], True), {})\ncnt: 6, ((T([64, 128], f16, stride=(51328, 1)), [0], True), {})\ncnt: 3, ((T([64, 256], f16), [0], True), {})\ncnt: 24, ((T([12608, 256], f16), [0], True), {})\ncnt: 3, ((T([64, 1, 256], f16), [0, 1], True), {})\ncnt: 18, ((T([12608, 768], f16), [0], True), {})\ncnt: 6, ((T([25664, 384], f16), [0], True), {})\ncnt: 1, ((T([64, 197, 256], f16), [0], True), {})\ncnt: 1, ((T([64, 1, 256], f16, stride=(50432, 256, 1)), [0], True), {})\ncnt: 1, ((T([64, 401, 128], f16), [0], True), {})\ncnt: 1, ((T([64, 1, 128], f16, stride=(51328, 128, 1)), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This code snippet shows the usage patterns of various PyTorch operators, including their input tensor shapes, data types, and occurrence counts. It covers operators like threshold_backward, topk, unbind, upsample_nearest2d, and where.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 4, ((T([1, 88, 10, 10, 3], f16), [-1]), {})\ncnt: 4, ((T([1, 88, 5, 5, 2], f16), [-1]), {})\nOperator: aten.threshold_backward.default\ncnt: 20, ((T([2], f16), T([2], f16), 0), {})\ncnt: 12, ((T([3], f16), T([3], f16), 0), {})\nOperator: aten.topk.default\ncnt: 1, ((T([1, 6905250], f16), 5000, 1), {})\nOperator: aten.unbind.int\ncnt: 2, ((T([5000, 4], f32), 1), {})\ncnt: 1, ((T([1, 100, 6], f32, stride=(0, 0, 0)),), {})\ncnt: 4, ((T([1, 88, 5, 5, 2], f16, stride=(2200, 25, 5, 1, 0)), -1), {})\ncnt: 4, ((T([1, 88, 10, 10, 3], f16, stride=(8800, 100, 10, 1, 0)), -1), {})\ncnt: 4, ((T([1, 88, 20, 20, 3], f16, stride=(35200, 400, 20, 1, 0)), -1), {})\ncnt: 4, ((T([1, 88, 40, 40, 3], f16, stride=(140800, 1600, 40, 1, 0)), -1), {})\ncnt: 4, ((T([1, 88, 80, 80, 2], f16, stride=(563200, 6400, 80, 1, 0)), -1), {})\ncnt: 4, ((T([1, 88, 40, 40, 2], f16, stride=(140800, 1600, 40, 1, 0)), -1), {})\ncnt: 4, ((T([1, 88, 20, 20, 2], f16, stride=(35200, 400, 20, 1, 0)), -1), {})\ncnt: 4, ((T([1, 88, 10, 10, 2], f16, stride=(8800, 100, 10, 1, 0)), -1), {})\nOperator: aten.upsample_nearest2d.vec\ncnt: 4, ((T([1, 88, 5, 5], f16), [10, 10], None), {})\ncnt: 4, ((T([1, 88, 10, 10], f16), [20, 20], None), {})\ncnt: 4, ((T([1, 88, 20, 20], f16), [40, 40], None), {})\ncnt: 4, ((T([1, 88, 40, 40], f16), [80, 80], None), {})\nOperator: aten.upsample_nearest2d_backward.vec\ncnt: 4, ((T([1, 88, 80, 80], f16), [80, 80], [1, 88, 40, 40], None), {})\ncnt: 4, ((T([1, 88, 40, 40], f16), [40, 40], [1, 88, 20, 20], None), {})\ncnt: 4, ((T([1, 88, 20, 20], f16), [20, 20], [1, 88, 10, 10], None), {})\ncnt: 4, ((T([1, 88, 10, 10], f16), [10, 10], [1, 88, 5, 5], None), {})\nOperator: aten.where.self\ncnt: 1, ((T([5000, 4], b8), T([5000, 4], f32), T([5000, 4], f32)), {})\ncnt: 1, ((T([5000, 4], b8), T([5000, 4], f32), T([], f32)), {})\n```\n\n----------------------------------------\n\nTITLE: SubprocessHandler Class in PyTorch Distributed Elastic\nDESCRIPTION: Class definition for SubprocessHandler, which is responsible for managing subprocesses in PyTorch's distributed elastic multiprocessing module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/subprocess_handler.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ntorch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler.SubprocessHandler\n```\n\n----------------------------------------\n\nTITLE: Creating AOT Model Compiler with Torch Library Linkage\nDESCRIPTION: Creates the Ahead-of-Time model compiler binary target and explicitly links it with the torch library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/binaries/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\ncaffe2_binary_target(aot_model_compiler \"aot_model_compiler.cc\")\ntarget_link_libraries(aot_model_compiler torch)\n```\n\n----------------------------------------\n\nTITLE: Defining a CUDA-Specific Test in PyTorch C++ Frontend (C++)\nDESCRIPTION: Defines a GoogleTest test case intended to run only on systems with CUDA available. The `_CUDA` suffix appended to the test case name signals the test runner (specifically logic in `main.cpp`) to execute this test only if CUDA devices are detected. This relies on the GoogleTest framework and PyTorch's custom test filtering.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n```cpp\nTEST(MyTestSuite, MyTestCase_CUDA) { }\n```\n```\n\n----------------------------------------\n\nTITLE: NLL Loss Forward Operations in PyTorch\nDESCRIPTION: Records of negative log likelihood loss forward operations for classification tasks. The operation includes input logits, target indices, weight tensor, reduction mode, and ignore index parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Threshold Backward Operations in PyTorch\nDESCRIPTION: This snippet shows threshold backward operations with different tensor shapes and channel sizes, used in the backward pass of ReLU activations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 288, 1, 1], f16), T([128, 288, 1, 1], f16), 0), {})\ncnt: 2, ((T([128, 168, 1, 1], f16), T([128, 168, 1, 1], f16), 0), {})\ncnt: 2, ((T([128, 120, 1, 1], f16), T([128, 120, 1, 1], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Applying ReLU Activation in PyTorch\nDESCRIPTION: This snippet shows the frequency and tensor shapes for applying the ReLU (Rectified Linear Unit) activation function in-place on various layers of a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 32, 149, 149], f16),), {})\ncnt: 1, ((T([128, 32, 147, 147], f16),), {})\ncnt: 1, ((T([128, 64, 147, 147], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Max Pooling Backward Operations in DenseNet\nDESCRIPTION: This snippet shows the backward pass operations for max pooling in DenseNet. It propagates gradients back through the pooling layers using the indices stored during the forward pass to ensure gradients flow to the correct positions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([32, 768, 7, 7], f16), T([32, 768, 14, 14], f16), [3, 3], [2, 2], [0, 0], [1, 1], True, T([32, 768, 7, 7], i64)), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), T([32, 512, 28, 28], f16), [3, 3], [2, 2], [0, 0], [1, 1], True, T([32, 512, 14, 14], i64)), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), T([32, 256, 56, 56], f16), [3, 3], [2, 2], [0, 0], [1, 1], True, T([32, 256, 28, 28], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch for iOS\nDESCRIPTION: Basic command to build PyTorch libraries for iOS platform from the PyTorch root directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n#in your PyTorch root directory\nbash scripts/build_ios.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation Directories for libshm in CMake\nDESCRIPTION: Sets up the installation directories for libshm binaries and libraries if not already defined. This ensures proper placement of compiled artifacts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/lib/libshm_windows/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT LIBSHM_INSTALL_LIB_SUBDIR)\n  set(LIBSHM_INSTALL_BIN_SUBDIR \"bin\" CACHE PATH \"libshm install binary directory\")\n  set(LIBSHM_INSTALL_LIB_SUBDIR \"lib\" CACHE PATH \"libshm install library directory\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initializing New Zero Tensors with ATen\nDESCRIPTION: Creates new tensors initialized to zero, essential for setting up parameters and biases in neural networks. Requires template tensors and size lists, such as [16777216]. Outputs zero-filled tensors ready for computational tasks. It's constrained by the need for specified sizes for each dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.new_zeros.default\ncnt: 2, ((T([16, 64, 128, 128], f16), [16777216]), {})\ncnt: 2, ((T([16, 128, 64, 64], f16), [8388608]), {})\ncnt: 7, ((T([16, 256, 32, 32], f16), [4194304]), {})\n```\n\n----------------------------------------\n\nTITLE: Reorderable Logging in PyTorch Compilation\nDESCRIPTION: Example showing how to configure logging functions to be reordered during compilation to avoid graph breaks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ntorch._dynamo.config.reorderable_logging_functions.add(print)\n\n@torch.compile\ndef fn(x):\n    x += 1\n    print(\"log!\")\n    return torch.sin(x)\n```\n\n----------------------------------------\n\nTITLE: Automating Class Documentation for PyTorch\nDESCRIPTION: Uses the 'autoclass' directive to automatically generate documentation for a PyTorch class. It includes inherited members and all class members in the output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/autosummary/class.rst#2025-04-22_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: {{ name }}\n    :inherited-members:\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Slice Backward Operation Calls\nDESCRIPTION: Logs of aten.slice_backward.default operations showing gradients for sliced tensors. Each call includes the tensor shape, original shape, dimension, start index, end index, and stride information.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.slice_backward.default\ncnt: 1, ((T([128, 11, 7, 7], f16, stride=(9065, 49, 7, 1)), [128, 185, 7, 7], 1, 174, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 185, 7, 7], f16), [128, 185, 7, 7], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 174, 7, 7], f16, stride=(9065, 49, 7, 1)), [128, 185, 7, 7], 1, 0, 174, 1), {})\ncnt: 1, ((T([128, 12, 7, 7], f16, stride=(8526, 49, 7, 1)), [128, 174, 7, 7], 1, 162, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 174, 7, 7], f16), [128, 174, 7, 7], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 162, 7, 7], f16, stride=(8526, 49, 7, 1)), [128, 174, 7, 7], 1, 0, 162, 1), {})\ncnt: 1, ((T([128, 11, 7, 7], f16, stride=(7938, 49, 7, 1)), [128, 162, 7, 7], 1, 151, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 162, 7, 7], f16), [128, 162, 7, 7], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 151, 7, 7], f16, stride=(7938, 49, 7, 1)), [128, 162, 7, 7], 1, 0, 151, 1), {})\ncnt: 1, ((T([128, 11, 7, 7], f16, stride=(7399, 49, 7, 1)), [128, 151, 7, 7], 1, 140, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 151, 7, 7], f16), [128, 151, 7, 7], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 140, 7, 7], f16, stride=(7399, 49, 7, 1)), [128, 151, 7, 7], 1, 0, 140, 1), {})\ncnt: 1, ((T([128, 11, 14, 14], f16, stride=(25088, 196, 14, 1)), [128, 128, 14, 14], 1, 117, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 128, 14, 14], f16), [128, 128, 14, 14], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 117, 14, 14], f16, stride=(25088, 196, 14, 1)), [128, 128, 14, 14], 1, 0, 117, 1), {})\ncnt: 1, ((T([128, 11, 14, 14], f16, stride=(22932, 196, 14, 1)), [128, 117, 14, 14], 1, 106, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 117, 14, 14], f16), [128, 117, 14, 14], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 106, 14, 14], f16, stride=(22932, 196, 14, 1)), [128, 117, 14, 14], 1, 0, 106, 1), {})\ncnt: 1, ((T([128, 11, 14, 14], f16, stride=(20776, 196, 14, 1)), [128, 106, 14, 14], 1, 95, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 106, 14, 14], f16), [128, 106, 14, 14], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 95, 14, 14], f16, stride=(20776, 196, 14, 1)), [128, 106, 14, 14], 1, 0, 95, 1), {})\ncnt: 1, ((T([128, 11, 14, 14], f16, stride=(18620, 196, 14, 1)), [128, 95, 14, 14], 1, 84, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 95, 14, 14], f16), [128, 95, 14, 14], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 84, 14, 14], f16, stride=(18620, 196, 14, 1)), [128, 95, 14, 14], 1, 0, 84, 1), {})\ncnt: 1, ((T([128, 12, 14, 14], f16, stride=(16464, 196, 14, 1)), [128, 84, 14, 14], 1, 72, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 84, 14, 14], f16), [128, 84, 14, 14], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 72, 14, 14], f16, stride=(16464, 196, 14, 1)), [128, 84, 14, 14], 1, 0, 72, 1), {})\ncnt: 1, ((T([128, 11, 28, 28], f16, stride=(47824, 784, 28, 1)), [128, 61, 28, 28], 1, 50, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 61, 28, 28], f16), [128, 61, 28, 28], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 50, 28, 28], f16, stride=(47824, 784, 28, 1)), [128, 61, 28, 28], 1, 0, 50, 1), {})\ncnt: 1, ((T([128, 11, 56, 56], f16, stride=(119168, 3136, 56, 1)), [128, 38, 56, 56], 1, 27, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 38, 56, 56], f16), [128, 38, 56, 56], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([128, 27, 56, 56], f16, stride=(119168, 3136, 56, 1)), [128, 38, 56, 56], 1, 0, 27, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Retrieving Logging Handler for PyTorch Distributed Elastic Events\nDESCRIPTION: This function returns a logging handler for the events system in PyTorch's distributed elastic package. It's used to manage how events are logged.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.elastic.events.get_logging_handler\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.threshold_backward.default with Tensor Arguments (Text)\nDESCRIPTION: This section logs calls to `aten.threshold_backward.default`, which computes gradients for the `threshold` operation (commonly used in ReLU's backward pass). It takes the gradient of the output and the input to the original threshold function. The examples show calls with float16 (f16) tensors of shapes like [128, 768, 1, 1] and [128, 128, 1, 1]. The third argument `0` represents the threshold value used in the forward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_23\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.threshold_backward.default\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 9, ((T([128, 768, 1, 1], f16), T([128, 768, 1, 1], f16), 0), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), 0), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([128, 128, 1, 1], f16), T([128, 128, 1, 1], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Sum Operations with Dimension Reduction\nDESCRIPTION: Logs of aten.sum.SymInt operations showing tensor summation along specified dimensions. Records include tensor shapes, dimensions to sum over, and whether to keep the dimension in the output (True).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 1, ((T([128, 1044, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 972, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 906, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 840, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 768, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 702, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 636, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 570, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 504, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 432, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 366, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 300, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 228, 28, 28], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing _softmax_backward_data Operator Calls in PyTorch\nDESCRIPTION: Records of aten._softmax_backward_data.default operator calls with various tensor shapes. All operations use half-precision (f16) tensors, with dimension 1 as the softmax dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 1, ((T([32, 2, 1, 512], f16), T([32, 2, 1, 512], f16), 1, f16), {})\ncnt: 1, ((T([32, 2, 1, 256], f16), T([32, 2, 1, 256], f16), 1, f16), {})\ncnt: 1, ((T([32, 2, 1, 128], f16), T([32, 2, 1, 128], f16), 1, f16), {})\ncnt: 1, ((T([32, 2, 1, 64], f16), T([32, 2, 1, 64], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Calculating Normalized L2 Error with torch.ao.ns.fx.utils in Python\nDESCRIPTION: This snippet shows the function signature for computing the normalized L2 error between two tensors x and y using the torch.ao.ns.fx.utils module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite_fx.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ntorch.ao.ns.fx.utils.compute_normalized_l2_error(x, y)\n```\n\n----------------------------------------\n\nTITLE: Enabling Broadcasting Backward Compatibility Warning in Python\nDESCRIPTION: Shows how to enable a warning to identify potential backward incompatibilities caused by the introduction of broadcasting. Setting `torch.utils.backcompat.broadcast_warning.enabled` to `True` will generate a `UserWarning` when an operation involves tensors that are not the same shape but are broadcastable and have the same number of elements, indicating a potential behavior change compared to older PyTorch versions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/broadcasting.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.utils.backcompat.broadcast_warning.enabled=True\n>>> torch.add(torch.ones(4,1), torch.ones(4))\n__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.\nChanging behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda Environment for PyTorch\nDESCRIPTION: This bash snippet demonstrates how to create and activate a new conda environment to manage separate builds of PyTorch, ensuring isolation of different development versions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n pytorch-myfeature\nsource activate pytorch-myfeature\n# if you run python now, torch will NOT be installed\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: NVFuser Profiling Command\nDESCRIPTION: Shell command to enable TorchScript profiling graph executor logging for debugging fusion patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPYTORCH_JIT_LOG_LEVEL=\"profiling_graph_executor_impl\" python <your pytorch script>\n```\n\n----------------------------------------\n\nTITLE: Average Pooling Backward Operations in PyTorch\nDESCRIPTION: Backward pass operations for average pooling in PyTorch. These compute gradients for the average pooling layers during backpropagation with 2x2 kernels and stride 2.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([64, 512, 7, 7], f16), T([64, 512, 14, 14], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})\ncnt: 1, ((T([64, 256, 14, 14], f16), T([64, 256, 28, 28], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})\ncnt: 1, ((T([64, 128, 28, 28], f16), T([64, 128, 56, 56], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})\n```\n\n----------------------------------------\n\nTITLE: torch.full Behavior in PyTorch 1.5 and Earlier\nDESCRIPTION: Demonstrates how torch.full always returned a float tensor in PyTorch 1.5 and earlier versions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# PyTorch 1.5 and earlier\n>>> torch.full((3,), 1)  # Note the integer fill value...\ntensor([1., 1., 1.])     # ...but float tensor!\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization Operations\nDESCRIPTION: Batch normalization operations on tensors of different sizes with training mode enabled, momentum 0.1 and epsilon 1e-05.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_xception65_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n((T([32, 32, 150, 150], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Creating a Sphinx Toctree for Developer Notes - reStructuredText\nDESCRIPTION: This snippet uses the Sphinx 'toctree' directive in a reStructuredText (.rst) or MyST Markdown (.md) file to dynamically reference and include all documentation files under the 'notes/' directory. The 'glob' option ensures that all files matching the specified pattern are automatically included in the documentation tree, and 'maxdepth: 1' restricts the visible hierarchy to a single level. This setup is essential for managing multiple note files and generating site navigation within Sphinx-based developer documentation. No explicit code dependencies are required, but Sphinx (and MyST if in a .md file) must be configured for the project.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes.md#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n```{toctree}\n:glob:\n:maxdepth: 1\n\nnotes/*\n```\n```\n\n----------------------------------------\n\nTITLE: Performing Convolutions in PyTorch\nDESCRIPTION: This snippet logs the 'aten.convolution.default' operation, detailing convolutional layer operations in networks using PyTorch. Each entry describes a convolution's input and output tensor shapes, filter dimensions, stride, padding, and other parameters. This highlights the critical role of convolutional operations in deep learning models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([96, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 32, 112, 112], f16), T([32, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})\ncnt: 1, ((T([96, 32, 112, 112], f16), T([16, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 16, 112, 112], f16), T([96, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([96, 96, 112, 112], f16), T([96, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 96), {})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating jit.trace Failure with Changed Inputs in PyTorch\nDESCRIPTION: This snippet shows how jit.trace fails when the input conditions change, highlighting the limitations of jit.trace with dynamic control flow.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmaybe_true = torch.BoolTensor([1])\nassert jit(t, maybe_true) == add_two_maybe(t, maybe_true)\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Backward Operations in PyTorch\nDESCRIPTION: Records of backward passes for batch normalization operations with various tensor shapes. Each operation includes gradients, input, scale, bias, running statistics, and flags for computing different gradients.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 320, 7, 7], f16), T([128, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), True, 1e-05, [True, True, True]), {})\ncnt: 8, ((T([128, 1152, 7, 7], f16), T([128, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f32), T([1152], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 192, 7, 7], f16), T([128, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 576, 7, 7], f16), T([128, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 576, 14, 14], f16), T([128, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 96, 14, 14], f16), T([128, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([128, 288, 14, 14], f16), T([128, 288, 14, 14], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f32), T([288], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 80, 14, 14], f16), T([128, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), True, 1e-05, [True, True, True]), {})\ncnt: 7, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([128, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 40, 28, 28], f16), T([128, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), True, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f32), T([120], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([128, 144, 56, 56], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 72, 56, 56], f16), T([128, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 48, 56, 56], f16), T([128, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 48, 112, 112], f16), T([128, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Manipulating Tensors with ATen _unsafe_view Operator\nDESCRIPTION: Shows the usage of `aten._unsafe_view` operator. It creates views over existing tensor data with new shapes like [256, 32, 1024] or [256, 1024, 1024], given the original tensor size. It is crucial for changing data layout without loading data into memory. Dependency is primarily on matching element sizes between the original and new shape.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 3, ((T([64, 128, 32, 32], f16), [256, 32, 1024]), {})\ncnt: 1, ((T([256, 1024, 1024], f16), [256, 1024, 1024]), {})\ncnt: 2, ((T([256, 32, 32, 32], f16), [262144, 32]), {})\n```\n\n----------------------------------------\n\nTITLE: Specifying Dynamic Linking for Host Builds in CMake\nDESCRIPTION: This is the `else` block corresponding to the `if(ANDROID_ABI)`. It configures linking for non-Android (host) builds, preferring dynamic linking. It sets the `pytorch_jni_LIBS` variable with `fbjni`, core PyTorch dynamic libraries (`torch`, `torch_cpu`, `c10`), and `cpuinfo`. It then conditionally appends other dependency libraries (like `nnpack`, `XNNPACK`, `pthreadpool`, `pytorch_qnnpack`) based on corresponding `USE_*` CMake variables.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nelse()\n  # Prefer dynamic linking on the host\n  set(pytorch_jni_LIBS\n      fbjni\n      torch\n      torch_cpu\n      c10\n      cpuinfo\n  )\n\n  if(USE_NNPACK)\n    list(APPEND pytorch_jni_LIBS nnpack)\n  endif()\n\n  if(USE_XNNPACK)\n    list(APPEND pytorch_jni_LIBS XNNPACK)\n    list(APPEND pytorch_jni_LIBS microkernels-prod)\n  endif()\n\n  if(USE_SYSTEM_PTHREADPOOL)\n    list(APPEND pytorch_jni_LIBS pthreadpool)\n  endif()\n\n  if(USE_PYTORCH_QNNPACK)\n    list(APPEND pytorch_jni_LIBS pytorch_qnnpack)\n    list(APPEND pytorch_jni_LIBS clog)\n  endif()\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Minified Reproduction Script\nDESCRIPTION: The final minified reproduction script containing only the essential components needed to trigger the error, with the graph reduced to just the problematic ReLU operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor_minifier.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nimport torch._inductor.inductor_prims\n\nimport torch._dynamo.config\nimport torch._inductor.config\nimport torch._functorch.config\nimport torch.fx.experimental._config\n\ntorch._inductor.config.generate_intermediate_hooks = True\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error'\ntorch._inductor.config.aot_inductor.dump_aoti_minifier = True\n\nisolate_fails_code_str = None\n\nexported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints/exported_program.pt2')\nconfig_patches={'aot_inductor.package': True}\nif __name__ == '__main__':\n    from torch._dynamo.repro.aoti import run_repro\n    with torch.no_grad():\n        run_repro(exported_program, config_patches=config_patches, accuracy=False, command='run', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints', check_str=None)\n```\n\n----------------------------------------\n\nTITLE: Example of a FakeTensor Instance Representation (Python)\nDESCRIPTION: This one-liner in Python demonstrates how to instantiate and display a FakeTensor object, showing typical initialization parameters such as data type, size, and device. Useful for conveying expected output from metadata propagation in symbolic tracing graphs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.ir_spec.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nFakeTensor(dtype=torch.int, size=[2,], device=CPU)\n```\n\n----------------------------------------\n\nTITLE: Stack Operation in PyTorch\nDESCRIPTION: This snippet shows stack operations on tensors with different shapes and strides. It combines multiple tensors along a new dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.stack.default\ncnt: 2, (([T([64, 12, 197, 32], f16, stride=(75648, 6304, 1, 197)), T([64, 12, 197, 32], f16)],), {})\ncnt: 14, (([T([64, 12, 196, 32], f16), T([64, 12, 196, 32], f16, stride=(75264, 6272, 1, 196)), T([64, 12, 196, 32], f16)],), {})\n```\n\n----------------------------------------\n\nTITLE: Perform aten.native_batch_norm Operation in Python\nDESCRIPTION: This snippet involves the native batch normalization operation using PyTorch, providing input tensors of varying dimensions. Dependencies include the PyTorch library for tensor operations. Inputs are multidimensional arrays shaped for specific use cases, generating normalized outputs. The parameters include flags and coefficients that manage the normalization process' behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 0.001), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Backward Operator Invocation for aten.threshold_backward (Python)\nDESCRIPTION: This snippet represents a profiling record for the \"aten.threshold_backward.default\" operator, showing the invocation count and the float16 input/output tensor shapes used in a backward pass for a threshold operation. The snippet depends on PyTorch's operator dispatch and a profiling mechanism. Inputs are the profiles of gradient and input tensors, shape and dtype information, outputs are used for analyzing backward pass shapes and performance. The structure is formatted for batch profiling, without execution logic but useful for kernel optimization analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 5, ((T([32, 1600, 7, 7], f16), T([32, 1600, 7, 7], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Statistics for Deep Learning Models\nDESCRIPTION: This code snippet represents statistics tracking data for various PyTorch operators used in a deep learning model, likely a ResNet. Each line shows the operator name, call count, tensor shapes, data types, and function arguments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._index_put_impl_.default\ncnt: 12, ((T([0], f16), [T([0], i64)], T([0], f16), True, True), {})\ncnt: 12, ((T([0, 4], f16), [T([0], i64)], T([0, 4], f16), True, True), {})\nOperator: aten._softmax.default\ncnt: 1, ((T([0, 91], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 1, ((T([0, 91], f16), T([0, 91], f16), -1, f16), {})\nOperator: aten._to_copy.default\ncnt: 8, ((T([], i64),), {'dtype': f32})\ncnt: 5, ((T([3, 4], f32),), {'dtype': f16, 'device': 'cuda'})\ncnt: 8, ((T([0, 4], f16),), {'dtype': f32})\ncnt: 2, ((T([0], f32),), {'dtype': i64})\ncnt: 4, ((T([0, 4], f16),), {'dtype': i64})\ncnt: 8, ((T([], f32),), {'dtype': f16})\nOperator: aten._unsafe_view.default\ncnt: 2, ((T([296, 304], i32), [89984]), {})\ncnt: 2, ((T([148, 152], i32), [22496]), {})\ncnt: 2, ((T([74, 76], i32), [5624]), {})\ncnt: 2, ((T([37, 38], i32), [1406]), {})\ncnt: 2, ((T([19, 19], i32), [361]), {})\ncnt: 1, ((T([4, 296, 304, 3, 1], f16), [4, 269952, 1]), {})\ncnt: 1, ((T([4, 296, 304, 3, 4], f16), [4, 269952, 4]), {})\ncnt: 1, ((T([4, 148, 152, 3, 1], f16), [4, 67488, 1]), {})\ncnt: 1, ((T([4, 148, 152, 3, 4], f16), [4, 67488, 4]), {})\ncnt: 1, ((T([4, 74, 76, 3, 1], f16), [4, 16872, 1]), {})\ncnt: 1, ((T([4, 74, 76, 3, 4], f16), [4, 16872, 4]), {})\ncnt: 1, ((T([4, 37, 38, 3, 1], f16), [4, 4218, 1]), {})\ncnt: 1, ((T([4, 37, 38, 3, 4], f16), [4, 4218, 4]), {})\ncnt: 1, ((T([4, 19, 19, 3, 1], f16), [4, 1083, 1]), {})\ncnt: 1, ((T([4, 19, 19, 3, 4], f16), [4, 1083, 4]), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Class with Method and Proper Docstring\nDESCRIPTION: A class with a proper docstring and a simple method definition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass LongWithDocstring:\n    \"\"\"This docstring, while short, is enough\"\"\"\n\n    def short1(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Creating Empty Strided Tensors in PyTorch\nDESCRIPTION: This snippet creates empty strided tensors with specified shapes and strides. It's used for initializing tensors with custom memory layouts, which can be beneficial for certain operations or hardware optimizations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 3, 48, 64, 85], f16, stride=(0, 0, 0, 0, 0)), [8, 3, 48, 64, 85], [783360, 261120, 5440, 85, 1]), {})\ncnt: 4, ((T([8, 3, 48, 64, 85], f16), [8, 3, 48, 64, 85], [783360, 261120, 5440, 85, 1]), {})\ncnt: 1, ((T([8, 3, 24, 32, 85], f16, stride=(0, 0, 0, 0, 0)), [8, 3, 24, 32, 85], [195840, 65280, 2720, 85, 1]), {})\ncnt: 4, ((T([8, 3, 24, 32, 85], f16), [8, 3, 24, 32, 85], [195840, 65280, 2720, 85, 1]), {})\ncnt: 1, ((T([8, 3, 12, 16, 85], f16, stride=(0, 0, 0, 0, 0)), [8, 3, 12, 16, 85], [48960, 16320, 1360, 85, 1]), {})\ncnt: 4, ((T([8, 3, 12, 16, 85], f16), [8, 3, 12, 16, 85], [48960, 16320, 1360, 85, 1]), {})\n```\n\n----------------------------------------\n\nTITLE: Using NVFuser Checkers\nDESCRIPTION: Example showing how to use built-in NVFuser checkers with the minifier for checking errors and correctness.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch.compile import minifier, check_nvfuser_subprocess, check_nvfuser_correctness_subprocess\nminifier(failing_graph, inps, check_nvfuser_subprocess)\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation for the Test Executable in CMake\nDESCRIPTION: Conditionally configures installation rules for the `test_edge_op_registration` executable based on the `INSTALL_TEST` CMake variable. If true, it sets the RPATH property for the target to ensure it can find shared libraries at runtime relative to its location and the `../lib` directory, and installs the executable to the `bin` destination.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nif(INSTALL_TEST)\n  set_target_properties(test_edge_op_registration PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n  install(TARGETS test_edge_op_registration DESTINATION bin)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Documenting Miniz Changelog in Markdown\nDESCRIPTION: This code snippet is a Markdown-formatted changelog for the Miniz compression library. It details changes, bug fixes, and new features across multiple versions, from 3.0.2 down to 2.0.0 beta. The changelog includes information about API changes, performance improvements, and compatibility updates.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/third_party/miniz-3.0.2/ChangeLog.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Changelog\n\n### 3.0.2\n\n - Fix buffer overrun in mz_utf8z_to_widechar on Windows\n\n### 3.0.1\n\n - Fix compilation error with MINIZ_USE_UNALIGNED_LOADS_AND_STORES=1\n\n### 3.0.0\n\n - Reduce memory usage for inflate. This changes `struct tinfl_decompressor_tag` and therefore requires a major version bump (breaks ABI compatibility)\n - Add padding to structures so it continues to work if features differ. This also changes some structures\n - Use _ftelli64, _fseeki64 and stat with MinGW32 and OpenWatcom\n - Fix varios warnings with OpenWatcom compiler\n - Avoid using unaligned memory access in UBSan builds\n - Set MINIZ_LITTLE_ENDIAN only if not set\n - Add MINIZ_NO_DEFLATE_APIS and MINIZ_NO_INFLATE_APIS\n - Fix use of uninitialized memory in tinfl_decompress_mem_to_callback()\n - Use wfopen on windows\n - Use _wstat64 instead _stat64 on windows\n - Use level_and_flags after MZ_DEFAULT_COMPRESSION has been handled\n - Improve endianess detection\n - Don't use unaligned stores and loads per default\n - Fix function declaration if MINIZ_NO_STDIO is used\n - Fix MZ_ZIP_GENERAL_PURPOSE_BIT_FLAG_UTF8 not being set\n - Remove total files check (its 32-bit uint)\n - tinfl_decompress: avoid NULL ptr arithmetic UB\n - miniz_zip: fix mz_zip_reader_extract_to_heap to read correct sizes\n - Eliminate 64-bit operations on 32-bit machines\n - Disable treating warnings as error with MSVC\n - Disable building shared lib via CMake by default\n - Fixed alignment problems on MacOS\n - Fixed get error string for MZ_ZIP_TOTAL_ERRORS\n - Write correct FLEVEL 2-bit value in zlib header\n - miniz.pc.in: fix include path not containing the \"miniz\" suffix\n - Fix compatibility with FreeBSD\n - pkg-config tweaks\n - Fix integer overflow in header corruption check\n - Fix some warnings\n - tdefl_compress_normal: Avoid NULL ptr arithmetic UB\n - replace use of stdint.h types with mz_ variants\n\n\n### 2.2.0\n\n - Fix examples with amalgamation\n - Modified cmake script to support shared library mode and find_package\n - Fix for misleading doc comment on `mz_zip_reader_init_cfile` function\n - Add include location tolerance and stop forcing `_GNU_SOURCE`\n - Fix: mz_zip_reader_locate_file_v2 returns an mz_bool\n - Fix large file system checks\n - Add #elif to enable an external mz_crc32() to be linked in\n - Write with dynamic size (size of file/data to be added not known before adding)\n - Added uncompress2 for zlib compatibility\n - Add support for building as a Meson subproject\n - Added OSSFuzz support; Integrate with CIFuzz\n - Add pkg-config file\n - Fixed use-of-uninitialized value msan error when copying dist bytes with no output bytes written.\n - mz_zip_validate_file(): fix memory leak on errors\n - Fixed MSAN use-of-uninitialized in tinfl_decompress when invalid dist is decoded. In this instance dist was 31 which s_dist_base translates as 0\n - Add flag to set (compressed) size in local file header\n - avoid use of uninitialized value in tdefl_record_literal\n\n### 2.1.0\n\n - More instances of memcpy instead of cast and use memcpy per default\n - Remove inline for c90 support\n - New function to read files via callback functions when adding them\n - Fix out of bounds read while reading Zip64 extended information\n - guard memcpy when n == 0 because buffer may be NULL\n - Implement inflateReset() function\n - Move comp/decomp alloc/free  prototypes under guarding #ifndef MZ_NO_MALLOC\n - Fix large file support under Windows\n - Don't warn if _LARGEFILE64_SOURCE is not defined to 1\n - Fixes for MSVC warnings\n - Remove check that path of file added to archive contains ':' or '\\'\n - Add !defined check on MINIZ_USE_ALIGNED_LOADS_AND_STORES\n\n### 2.0.8\n\n - Remove unimplemented functions (mz_zip_locate_file and mz_zip_locate_file_v2)\n - Add license, changelog, readme and example files to release zip\n - Fix heap overflow to user buffer in tinfl_status tinfl_decompress\n - Fix corrupt archive if uncompressed file smaller than 4 byte and the file is added by mz_zip_writer_add_mem*\n\n### 2.0.7\n\n - Removed need in C++ compiler in cmake build\n - Fixed a lot of uninitialized value errors found with Valgrind by memsetting m_dict to 0 in tdefl_init\n - Fix resource leak in mz_zip_reader_init_file_v2\n - Fix assert with mz_zip_writer_add_mem* w/MZ_DEFAULT_COMPRESSION\n - cmake build: install library and headers\n - Remove _LARGEFILE64_SOURCE requirement from apple defines for large files\n\n### 2.0.6\n\n - Improve MZ_ZIP_FLAG_WRITE_ZIP64 documentation\n - Remove check for cur_archive_file_ofs > UINT_MAX because cur_archive_file_ofs is not used after this point\n - Add cmake debug configuration\n - Fix PNG height when creating png files\n - Add \"iterative\" file extraction method based on mz_zip_reader_extract_to_callback.\n - Option to use memcpy for unaligned data access\n - Define processor/arch macros as zero if not set to one\n\n### 2.0.4/2.0.5\n\n - Fix compilation with the various omission compile definitions\n\n### 2.0.3\n\n- Fix GCC/clang compile warnings\n- Added callback for periodic flushes (for ZIP file streaming)\n- Use UTF-8 for file names in ZIP files per default\n\n### 2.0.2\n\n- Fix source backwards compatibility with 1.x\n- Fix a ZIP bit not being set correctly\n\n### 2.0.1\n\n- Added some tests\n- Added CI\n- Make source code ANSI C compatible\n\n### 2.0.0 beta\n\n- Matthew Sitton merged miniz 1.x to Rich Geldreich's vogl ZIP64 changes. Miniz is now licensed as MIT since the vogl code base is MIT licensed\n- Miniz is now split into several files\n- Miniz does now not seek backwards when creating ZIP files. That is the ZIP files can be streamed\n- Miniz automatically switches to the ZIP64 format when the created ZIP files goes over ZIP file limits\n- Similar to [SQLite](https://www.sqlite.org/amalgamation.html) the Miniz source code is amalgamated into one miniz.c/miniz.h pair in a build step (amalgamate.sh). Please use miniz.c/miniz.h in your projects\n- Miniz 2 is only source back-compatible with miniz 1.x. It breaks binary compatibility because structures changed\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This code snippet demonstrates the usage of various PyTorch operators in a deep learning model. It includes operations like softmax, matrix multiplication, embedding, and layer normalization. The analysis shows the operator name, count of usage, and input tensor shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BlenderbotSmallForCausalLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([8192, 50265], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([8192, 50265], f16), T([8192, 50265], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 8, ((T([1024, 128, 128], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 8, ((T([1024, 128, 128], f16), T([1024, 128, 128], f16), -1, f16), {})\nOperator: aten._to_copy.default\ncnt: 1, ((T([128, 128], f32),), {'dtype': f16})\ncnt: 1, ((T([64, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten._unsafe_view.default\ncnt: 24, ((T([64, 128, 16, 32], f16), [64, 128, 512]), {})\ncnt: 1, ((T([8192, 50265], f16), [64, 128, 50265]), {})\ncnt: 8, ((T([64, 16, 128, 32], f16), [1024, 128, 32]), {})\ncnt: 8, ((T([64, 128, 512], f16), [8192, 512]), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([128], i64), 1), {})\ncnt: 1, ((T([64, 128, 512], f16), T([128, 512], f16)), {})\ncnt: 8, ((T([64, 16, 128, 128], f16), T([64, 1, 128, 128], f16)), {})\ncnt: 48, ((T([64, 128, 512], f16), T([64, 128, 512], f16)), {})\ncnt: 1, ((T([50265, 512], f16), T([50265, 512], f16)), {})\nOperator: aten.addmm.default\ncnt: 32, ((T([512], f16), T([8192, 512], f16), T([512, 512], f16, stride=(1, 512))), {})\ncnt: 8, ((T([2048], f16), T([8192, 512], f16), T([512, 2048], f16, stride=(1, 512))), {})\ncnt: 8, ((T([512], f16), T([8192, 2048], f16), T([2048, 512], f16, stride=(1, 2048))), {})\nOperator: aten.bmm.default\ncnt: 16, ((T([1024, 128, 32], f16), T([1024, 32, 128], f16, stride=(4096, 1, 32))), {})\ncnt: 16, ((T([1024, 128, 128], f16), T([1024, 128, 32], f16)), {})\ncnt: 8, ((T([1024, 128, 128], f16, stride=(16384, 1, 128)), T([1024, 128, 32], f16)), {})\ncnt: 8, ((T([1024, 32, 128], f16, stride=(4096, 1, 32)), T([1024, 128, 128], f16)), {})\nOperator: aten.clone.default\ncnt: 2, ((T([64, 128], i64),), {})\nOperator: aten.copy_.default\ncnt: 2, ((T([64, 128], i64), T([64, 128], i64)), {})\nOperator: aten.embedding.default\ncnt: 1, ((T([50265, 512], f16), T([64, 128], i64), 0), {})\ncnt: 1, ((T([512, 512], f16), T([128], i64)), {})\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([128, 512], f16), T([128], i64), 512, -1, False), {})\ncnt: 1, ((T([64, 128, 512], f16), T([64, 128], i64), 50265, 0, False), {})\nOperator: aten.gelu.default\ncnt: 8, ((T([64, 128, 2048], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 8, ((T([64, 128, 2048], f16), T([64, 128, 2048], f16)), {})\nOperator: aten.lt.Tensor\ncnt: 1, ((T([128], i64), T([128, 1], i64)), {})\nOperator: aten.masked_fill_.Scalar\ncnt: 1, ((T([128, 128], f32), T([128, 128], b8), 0), {})\nOperator: aten.mm.default\ncnt: 1, ((T([8192, 512], f16), T([512, 50265], f16, stride=(1, 512))), {})\ncnt: 1, ((T([50265, 8192], f16, stride=(1, 50265)), T([8192, 512], f16)), {})\ncnt: 1, ((T([8192, 50265], f16), T([50265, 512], f16)), {})\ncnt: 8, ((T([8192, 512], f16), T([512, 2048], f16)), {})\ncnt: 8, ((T([512, 8192], f16, stride=(1, 512)), T([8192, 2048], f16)), {})\ncnt: 8, ((T([8192, 2048], f16), T([2048, 512], f16)), {})\ncnt: 8, ((T([2048, 8192], f16, stride=(1, 2048)), T([8192, 512], f16)), {})\ncnt: 32, ((T([8192, 512], f16), T([512, 512], f16)), {})\ncnt: 32, ((T([512, 8192], f16, stride=(1, 512)), T([8192, 512], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 2, ((T([64, 128, 512], f16), 1.0), {})\ncnt: 16, ((T([64, 128, 512], f16), 0.1767766952966369), {})\nOperator: aten.native_layer_norm.default\ncnt: 17, ((T([64, 128, 512], f16), [512], T([512], f16), T([512], f16), 1e-05), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 17, ((T([64, 128, 512], f16), T([64, 128, 512], f16), [512], T([64, 128, 1], f32), T([64, 128, 1], f32), T([512], f16), T([512], f16), [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([8192, 50265], f16), T([8192], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([8192, 50265], f16), T([8192], i64), None, 1, -100), {})\nOperator: aten.sum.SymInt\ncnt: 40, ((T([8192, 512], f16), [0], True), {})\ncnt: 8, ((T([8192, 2048], f16), [0], True), {})\ncnt: 1, ((T([64, 128, 512], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Describing nll_loss Operators and Tensor Signatures - PyTorch - Python\nDESCRIPTION: These code snippets illustrate invocation examples of the PyTorch nll_loss operators (both backward and forward variants). They show the tuple-encoded representation of argument tensors (sizes, dtypes), optional arguments, reduction modes, ignore indices, targets, and output tensors. Dependencies include PyTorch tensor types and the aten.nll_loss_backward/forward operators. Inputs use tensor argument representations with indicia for batch/class/label shape, expected to be mapped to actual tensors for execution or logging. Output is a tuple/list for statistical or test usage; None denotes the absence of an argument. Limitations: This is not executable code itselfdescriptions are for operator signature capture.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\nOperator: aten.nll_loss_forward.default\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations in PyTorch - Python\nDESCRIPTION: This snippet uses aten.convolution.default to perform convolutions typical in CNN architectures. Parameters like stride, padding, and dilation control the operation. Essential for extracting features from input data in machine learning models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([16, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 16, 112, 112], f16), T([16, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 16), {})\ncnt: 1, ((T([32, 16, 112, 112], f16), T([16, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 16, 112, 112], f16), T([64, 16, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([64, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 64), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([24, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 24, 56, 56], f16), T([72, 24, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 72, 56, 56], f16), T([72, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 72), {})\ncnt: 1, ((T([32, 72, 56, 56], f16), T([24, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 72, 56, 56], f16), T([72, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 72), {})\ncnt: 1, ((T([32, 72, 1, 1], f16), T([24, 72, 1, 1], f16), T([24], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 24, 1, 1], f16), T([72, 24, 1, 1], f16), T([72], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 72, 28, 28], f16), T([40, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 40, 28, 28], f16), T([120, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 120, 28, 28], f16), T([120, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 120), {})\ncnt: 2, ((T([32, 120, 1, 1], f16), T([32, 120, 1, 1], f16), T([32], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 32, 1, 1], f16), T([120, 32, 1, 1], f16), T([120], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 120, 28, 28], f16), T([40, 120, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 40, 28, 28], f16), T([240, 40, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([240, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 240), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), T([80, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 80, 14, 14], f16), T([200, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 200, 14, 14], f16), T([200, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 200), {})\ncnt: 1, ((T([32, 200, 14, 14], f16), T([80, 200, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 80, 14, 14], f16), T([184, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 184, 14, 14], f16), T([184, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 184), {})\ncnt: 2, ((T([32, 184, 14, 14], f16), T([80, 184, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 80, 14, 14], f16), T([480, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 480, 14, 14], f16), T([480, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 480), {})\ncnt: 1, ((T([32, 480, 1, 1], f16), T([120, 480, 1, 1], f16), T([120], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 120, 1, 1], f16), T([480, 120, 1, 1], f16), T([480], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 480, 14, 14], f16), T([112, 480, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 112, 14, 14], f16), T([672, 112, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 672, 14, 14], f16), T([672, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 672), {})\ncnt: 2, ((T([32, 672, 1, 1], f16), T([168, 672, 1, 1], f16), T([168], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 168, 1, 1], f16), T([672, 168, 1, 1], f16), T([672], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 672, 14, 14], f16), T([112, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 672), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([160, 672, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([32, 160, 7, 7], f16), T([960, 160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 960, 7, 7], f16), T([960, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 960), {})\ncnt: 2, ((T([32, 960, 1, 1], f16), T([240, 960, 1, 1], f16), T([240], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 240, 1, 1], f16), T([960, 240, 1, 1], f16), T([960], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 960, 7, 7], f16), T([160, 960, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n\n```\n\n----------------------------------------\n\nTITLE: Warning Message in reStructuredText\nDESCRIPTION: Displays a warning message about distributed optimizer not being supported with CUDA tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.optim.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. warning ::\n    Distributed optimizer is not currently supported when using CUDA tensors\n```\n\n----------------------------------------\n\nTITLE: Configuring Mobile Build Binary Targets in PyTorch\nDESCRIPTION: Conditionally compiles specific binary targets when building for mobile platforms. For mobile builds, it includes speed and load benchmarking tools, with optional model comparison based on the BUILD_LITE_INTERPRETER flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/binaries/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(INTERN_BUILD_MOBILE)\n  caffe2_binary_target(\"speed_benchmark_torch.cc\")\n  caffe2_binary_target(\"load_benchmark_torch.cc\")\n  if(NOT BUILD_LITE_INTERPRETER)\n    caffe2_binary_target(\"compare_models_torch.cc\")\n  endif()\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Validating Milestone PR Inclusion Using CLI Script - Python\nDESCRIPTION: This Python command-line invocation validates that all pull requests associated with a specific milestone are present in a PyTorch release branch by analyzing GitHub history and PR metadata. Dependencies include a cloned Git repository, access to GitHub PR data, and the 'github_analyze.py' script with required permissions. Key parameters: 'repo-path' (local repo), 'remote' (name of the remote), 'branch' (target release branch), 'milestone-id' (numeric GitHub milestone), and 'missing-in-branch' (action filter). The input is the repo and milestone identifiers; the output is a report of PRs missing from the branch. Requires Python and access credentials for the repo.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ngithub_analyze.py --repo-path ~/local/pytorch --remote upstream --branch release/2.2 --milestone-id 40 --missing-in-branch\n```\n\n----------------------------------------\n\nTITLE: Recording Tensor Shape and Stride Metadata (Python)\nDESCRIPTION: This code snippet logs a profiling entry for a PyTorch tensor initialization, capturing its shape, data type (f16 for float16), and custom stride, alongside additional metadata fields such as output shape and operator parameters. The dependencies include the PyTorch Tensors and a profiling or logging interface that interprets the tuple structure; key parameters captured are the tensor's dimensions, data type, striding (contiguous or non-contiguous), and kernel call parameters. Inputs are shape/stride specifications, outputs are profiling log entries. The snippet is not a standalone script but an extract from a profiling or regression testing output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([32, 64, 14, 14], f16, stride=(150528, 196, 14, 1)), [32, 64, 14, 14], 3, 0, 9223372036854775807, 1), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Multiplication Operations\nDESCRIPTION: Batch matrix multiplication operations between tensors of various shapes using half precision floating point.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.bmm.default((T([4096, 64, 16], f16), T([4096, 16, 144], f16)))\naten.bmm.default((T([4096, 64, 144], f16), T([4096, 144, 32], f16)))\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations\nDESCRIPTION: Collection of tensor operations including convolutions, batch normalization, pooling and activation functions. Shows tensor shapes, strides and data types with half-precision (f16) format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nasnetalarge_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Sample tensor operation with strides\nT([16, 42, 83, 83], f16, stride=(578676, 6889, 83, 1))\n\n# Batch normalization operation\naten.native_batch_norm.default(\n    T([16, 96, 165, 165], f16), \n    T([96], f16),\n    T([96], f16),\n    T([96], f16),\n    T([96], f16),\n    True, 0.1, 0.001\n)\n\n# Max pooling operation\naten.max_pool2d_with_indices.default(\n    T([16, 42, 167, 167], f16),\n    [3, 3],\n    [2, 2]\n)\n```\n\n----------------------------------------\n\nTITLE: Logging PyTorch Operator Usage Counts and Tensor Shapes\nDESCRIPTION: This log shows detailed information about PyTorch operator usage in a Vision Transformer model. Each line shows an operator name, the count of calls, and the tensor shapes and types used in those calls. The format 'T([dimensions], dtype)' represents tensors with their shapes and data types (primarily f16 for half-precision).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/beit_base_patch16_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 12, ((T([64, 12, 197, 197], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([64, 12, 197, 197], f16), T([64, 12, 197, 197], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([64, 12, 197, 64], f16), [768, 197, 64]), {})\ncnt: 12, ((T([64, 12, 64, 197], f16), [768, 64, 197]), {})\ncnt: 12, ((T([768, 197, 197], f16), [64, 12, 197, 197]), {})\ncnt: 12, ((T([768, 197, 64], f16), [64, 12, 197, 64]), {})\ncnt: 12, ((T([64, 197, 12, 64], f16), [64, 197, 768]), {})\ncnt: 12, ((T([64, 197, 3, 12, 64], f16), [64, 197, 2304]), {})\nOperator: aten.add.Tensor\ncnt: 12, ((T([64, 12, 197, 197], f16), T([1, 12, 197, 197], f16)), {})\ncnt: 48, ((T([64, 197, 768], f16), T([64, 197, 768], f16)), {})\nOperator: aten.addmm.default\ncnt: 12, ((T([2304], f16), T([12608, 768], f16), T([768, 2304], f16, stride=(1, 768))), {})\ncnt: 12, ((T([768], f16), T([12608, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 12, ((T([3072], f16), T([12608, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 12, ((T([768], f16), T([12608, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 1, ((T([1000], f16), T([64, 768], f16), T([768, 1000], f16, stride=(1, 768))), {})\nOperator: aten.bmm.default\ncnt: 12, ((T([768, 197, 64], f16), T([768, 64, 197], f16)), {})\ncnt: 12, ((T([768, 197, 197], f16), T([768, 197, 64], f16)), {})\ncnt: 12, ((T([768, 197, 197], f16, stride=(38809, 1, 197)), T([768, 197, 64], f16)), {})\ncnt: 12, ((T([768, 197, 64], f16), T([768, 64, 197], f16, stride=(12608, 1, 64))), {})\ncnt: 12, ((T([768, 64, 197], f16, stride=(12608, 1, 64)), T([768, 197, 197], f16)), {})\ncnt: 12, ((T([768, 197, 197], f16), T([768, 197, 64], f16, stride=(12608, 1, 197))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([64, 1, 768], f16, stride=(0, 768, 1)), T([64, 196, 768], f16, stride=(150528, 1, 196))], 1), {})\ncnt: 12, (([T([768], f16), T([768], f16), T([768], f16)],), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([768, 3, 16, 16], f16), T([768], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 768, 14, 14], f16, stride=(151296, 1, 10752, 768)), T([64, 3, 224, 224], f16), T([768, 3, 16, 16], f16), [768], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([64, 196, 768], f16, stride=(768, 0, 1)), 196), {})\nOperator: aten.gelu.default\ncnt: 12, ((T([64, 197, 3072], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([64, 197, 3072], f16), T([64, 197, 3072], f16)), {})\nOperator: aten.index.Tensor\ncnt: 12, ((T([732, 12], f16), [T([38809], i64)]), {})\nOperator: aten.index_put.default\ncnt: 12, ((T([732, 12], f16), [T([38809], i64)], T([38809, 12], f16, stride=(1, 38809)), True), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([64], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([64, 196, 768], f16, stride=(151296, 768, 1)), [1]), {})\nOperator: aten.mm.default\ncnt: 1, ((T([64, 1000], f16), T([1000, 768], f16)), {})\ncnt: 1, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 768], f16)), {})\ncnt: 12, ((T([12608, 768], f16), T([768, 3072], f16)), {})\ncnt: 12, ((T([768, 12608], f16, stride=(1, 768)), T([12608, 3072], f16)), {})\ncnt: 12, ((T([12608, 3072], f16), T([3072, 768], f16)), {})\ncnt: 12, ((T([3072, 12608], f16, stride=(1, 3072)), T([12608, 768], f16)), {})\ncnt: 12, ((T([12608, 768], f16), T([768, 768], f16)), {})\ncnt: 12, ((T([768, 12608], f16, stride=(1, 768)), T([12608, 768], f16)), {})\ncnt: 12, ((T([12608, 2304], f16), T([2304, 768], f16)), {})\ncnt: 12, ((T([2304, 12608], f16, stride=(1, 2304)), T([12608, 768], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 12, ((T([64, 12, 197, 64], f16, stride=(453888, 64, 2304, 1)), 0.125), {})\ncnt: 24, ((T([768], f16), T([64, 197, 768], f16)), {})\ncnt: 24, ((T([64, 197, 768], f16), T([768], f16)), {})\ncnt: 24, ((T([64, 197, 768], f16), T([64, 197, 768], f16)), {})\ncnt: 12, ((T([64, 12, 197, 64], f16), 0.125), {})\nOperator: aten.native_layer_norm.default\ncnt: 24, ((T([64, 197, 768], f16), [768], T([768], f16), T([768], f16), 1e-06), {})\ncnt: 1, ((T([64, 768], f16), [768], T([768], f16), T([768], f16), 1e-06), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 1, ((T([64, 768], f16), T([64, 768], f16), [768], T([64, 1], f32), T([64, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\ncnt: 24, ((T([64, 197, 768], f16), T([64, 197, 768], f16), [768], T([64, 197, 1], f32), T([64, 197, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\nOperator: aten.new_zeros.default\ncnt: 12, ((T([38809, 12], f16, stride=(1, 38809)), [732, 12]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\nOperator: aten.slice_backward.default\ncnt: 1, ((T([64, 196, 768], f16), [64, 197, 768], 1, 1, 9223372036854775807, 1), {})\ncnt: 1, ((T([64, 197, 768], f16), [64, 197, 768], 0, 0, 9223372036854775807, 1), {})\nOperator: aten.stack.default\ncnt: 12, (([T([64, 12, 197, 64], f16), T([64, 12, 197, 64], f16, stride=(151296, 12608, 1, 197)), T([64, 12, 197, 64], f16)],), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\ncnt: 24, ((T([64, 197, 768], f16), [0, 1], True), {})\ncnt: 24, ((T([12608, 768], f16), [0], True), {})\ncnt: 12, ((T([12608, 3072], f16), [0], True), {})\ncnt: 12, ((T([64, 12, 197, 197], f16), [0], True), {})\ncnt: 12, ((T([12608, 2304], f16), [0], True), {})\ncnt: 1, ((T([64, 1, 768], f16, stride=(151296, 768, 1)), [0], True), {})\nOperator: aten.unbind.int\ncnt: 12, ((T([3, 64, 12, 197, 64], f16, stride=(768, 453888, 64, 2304, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing aten.native_batch_norm_backward in PyTorch\nDESCRIPTION: This snippet involves the use of the PyTorch aten.native_batch_norm_backward operator, which is essential for computing gradients during the backward pass of batch normalization. The inputs include tensors of specified dimensions and data types, with True as a boolean flag and a float for epsilon. Dependencies include PyTorch's native functions for tensor operations. The outputs will be gradients essential for backpropagation. The function expects matching dimensions and appropriate data types for its execution. This operation is highly sensitive to input shapes and requires floating-point computations for precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 1536, 7, 7], f16), T([64, 1536, 7, 7], f16), T([1536], f16), T([1536], f16), T([1536], f16), T([1536], f32), T([1536], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 4, ((T([64, 264, 7, 7], f16), T([64, 264, 7, 7], f16), T([264], f16), T([264], f16), T([264], f16), T([264], f32), T([264], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 6, ((T([64, 1584, 7, 7], f16), T([64, 1584, 7, 7], f16), T([1584], f16), T([1584], f16), T([1584], f16), T([1584], f32), T([1584], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 960, 7, 7], f16), T([64, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f32), T([960], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring LibSHM CMake Project\nDESCRIPTION: Initializes the CMake project for libshm and sets basic configuration parameters including minimum CMake version and project name.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/lib/libshm/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nproject(libshm C CXX)\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\n\nset(TORCH_ROOT ${CMAKE_CURRENT_LIST_DIR}/../../..//)\n\nif(NOT LIBSHM_INSTALL_LIB_SUBDIR)\n  set(LIBSHM_INSTALL_LIB_SUBDIR \"lib\" CACHE PATH \"libshm install library directory\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: JIT Optimization Limiter Usage in PyTorch\nDESCRIPTION: Example code showing how to use the JIT optimization limiter which helps with debugging by limiting the number of optimizations applied in a specific pass. This feature is controlled through environment variables.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_32\n\nLANGUAGE: cpp\nCODE:\n```\nif (!JIT_OPT_ALLOWED) {\n    GRAPH_DUMP(...); //supplied from jit_log\n    return;\n}\n```\n\n----------------------------------------\n\nTITLE: Tracking Batch Normalization Backward Operator Calls in PyTorch\nDESCRIPTION: Records frequency counts of batch normalization backward operations with different tensor shapes. Each line shows input gradients, output gradients, weights, biases, and running statistics with their shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 5, ((T([128, 960, 7, 7], f16), T([128, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f32), T([960], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 160, 7, 7], f16), T([128, 160, 7, 7], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 672, 14, 14], f16), T([128, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 112, 14, 14], f16), T([128, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f32), T([112], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 80, 14, 14], f16), T([128, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 184, 14, 14], f16), T([128, 184, 14, 14], f16), T([184], f16), T([184], f16), T([184], f16), T([184], f32), T([184], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 200, 14, 14], f16), T([128, 200, 14, 14], f16), T([200], f16), T([200], f16), T([200], f16), T([200], f32), T([200], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([128, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 40, 28, 28], f16), T([128, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f32), T([120], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 72, 56, 56], f16), T([128, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Logging PyTorch Tensor Dimensions and Counts\nDESCRIPTION: Log entries showing the frequency of tensor shapes in a PyTorch model, with each line containing a count and tensor dimensions. Each tensor pair has identical input/output shapes using float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nasnetalarge_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\ncnt: 2, ((T([16, 2688, 11, 11], f16), T([16, 2688, 11, 11], f16), 0), {})\ncnt: 12, ((T([16, 2016, 21, 21], f16), T([16, 2016, 21, 21], f16), 0), {})\ncnt: 4, ((T([16, 672, 21, 21], f16), T([16, 672, 21, 21], f16), 0), {})\ncnt: 66, ((T([16, 336, 21, 21], f16), T([16, 336, 21, 21], f16), 0), {})\ncnt: 2, ((T([16, 1344, 21, 21], f16), T([16, 1344, 21, 21], f16), 0), {})\ncnt: 12, ((T([16, 1008, 42, 42], f16), T([16, 1008, 42, 42], f16), 0), {})\ncnt: 6, ((T([16, 336, 42, 42], f16), T([16, 336, 42, 42], f16), 0), {})\ncnt: 60, ((T([16, 168, 42, 42], f16), T([16, 168, 42, 42], f16), 0), {})\ncnt: 2, ((T([16, 168, 83, 83], f16), T([16, 168, 83, 83], f16), 0), {})\ncnt: 6, ((T([16, 84, 42, 42], f16), T([16, 84, 42, 42], f16), 0), {})\ncnt: 4, ((T([16, 84, 83, 83], f16), T([16, 84, 83, 83], f16), 0), {})\ncnt: 5, ((T([16, 96, 165, 165], f16), T([16, 96, 165, 165], f16), 0), {})\ncnt: 6, ((T([16, 42, 83, 83], f16), T([16, 42, 83, 83], f16), 0), {})\ncnt: 1, ((T([16, 42, 165, 165], f16), T([16, 42, 165, 165], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Referencing torchrun API Documentation in RST\nDESCRIPTION: This RST directive creates a reference label for the torchrun API documentation, allowing it to be linked from other parts of the documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/run.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _launcher-api:\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Tensor Cloning Operations\nDESCRIPTION: This snippet shows the usage of the aten.clone.default operator for creating copies of tensors with various shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 2, ((T([3, 3, 512, 512], f16),), {})\ncnt: 1, ((T([3, 1, 512, 512], f16),), {})\ncnt: 1, ((T([3, 4, 512, 512], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Test Sources and Building Test API Executable\nDESCRIPTION: Configures test source files and builds the main test_api executable with appropriate include directories and library dependencies. Includes conditional compilation for CUDA/ROCm support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TORCH_API_TEST_DIR \"${TORCH_ROOT}/test/cpp/api\")\nset(TORCH_API_TEST_SOURCES\n  ${TORCH_ROOT}/test/cpp/common/main.cpp\n  ${TORCH_API_TEST_DIR}/autograd.cpp\n  ${TORCH_API_TEST_DIR}/any.cpp\n  # ... additional source files ...\n)\nif(USE_CUDA OR USE_ROCM)\n  list(APPEND TORCH_API_TEST_SOURCES ${TORCH_API_TEST_DIR}/parallel.cpp)\nendif()\n\nadd_executable(test_api ${TORCH_API_TEST_SOURCES})\ntarget_include_directories(test_api PRIVATE ${ATen_CPU_INCLUDE})\ntarget_link_libraries(test_api PRIVATE torch gtest)\n```\n\n----------------------------------------\n\nTITLE: Copying Tensor Content Using aten.copy_\nDESCRIPTION: Using aten.copy_.default facilitates copying data from one tensor to another. It maintains dependencies on PyTorch and executes in-place operations altering target tensors with content parallel to the source tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 2, ((T([8, 128], i64), T([8, 128], i64)), {})\ncnt: 1, ((T([8, 127], i64, stride=(128, 1)), T([8, 127], i64)), {})\ncnt: 1, ((T([8], i64, stride=(128,)), T([8], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Division Operations\nDESCRIPTION: These snippets show tensor division operations with scalar values and other tensors, using float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 3072, 6, 6], f16, stride=(3072, 1, 0, 0)), 36), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1536, 6, 6], f16, stride=(1536, 1, 0, 0)), 36), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1536, 12, 12], f16, stride=(1536, 1, 0, 0)), 144), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 512, 24, 24], f16, stride=(512, 1, 0, 0)), 576), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 256, 48, 48], f16, stride=(256, 1, 0, 0)), 2304), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([], f16), 128000), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking PyTorch Softmax Operations in Deep Learning Model\nDESCRIPTION: Details of log_softmax and its backward operation usage with 128 batch size and 1000 output classes (likely classification model). Shows tensor shapes and data types (f16/half precision).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Minifier Launcher Script\nDESCRIPTION: The generated launcher script that controls the minification process and configures the environment for reproducing the error.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor_minifier.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch._inductor.inductor_prims\n\nimport torch._dynamo.config\nimport torch._inductor.config\nimport torch._functorch.config\nimport torch.fx.experimental._config\n\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error'\ntorch._inductor.config.aot_inductor.dump_aoti_minifier = True\n\nisolate_fails_code_str = None\n\nexported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints/exported_program.pt2')\nconfig_patches={}\nif __name__ == '__main__':\n    from torch._dynamo.repro.aoti import run_repro\n    with torch.no_grad():\n        run_repro(exported_program, config_patches=config_patches, accuracy=False, command='minify', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints', check_str=None)\n```\n\n----------------------------------------\n\nTITLE: Downloading MNIST Dataset (Python)\nDESCRIPTION: The download_mnist.py script downloads the MNIST dataset, which is necessary for running C++ API tests.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/README.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\npython download_mnist.py\n```\n\n----------------------------------------\n\nTITLE: Using Pre-Compiled Headers with CMake\nDESCRIPTION: This bash snippet enables the use of pre-compiled headers to accelerate builds by configuriing CMake for PyTorch, especially when modifying multiple files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\nUSE_PRECOMPILED_HEADERS=1 python setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch SiLU (Swish) and SiLU Backward Operators - Python\nDESCRIPTION: This snippet collects input shapes for the aten.silu_.default and aten.silu_backward.default operators, presenting in-place forward activation and their backward gradient operations. Inputs demonstrate varied shapes and dimensions, using float16 for simulated large activation maps. Useful for activation kernel regression and correctness validation, with dependencies limited to PyTorch and ATen conventions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 24, 128, 128], f16),), {})\ncnt: 1, ((T([128, 32, 128, 128], f16),), {})\ncnt: 1, ((T([128, 64, 128, 128], f16),), {})\ncnt: 4, ((T([128, 64, 64, 64], f16),), {})\ncnt: 2, ((T([128, 256, 64, 64], f16),), {})\ncnt: 1, ((T([128, 128, 64, 64], f16),), {})\ncnt: 3, ((T([128, 128, 32, 32], f16),), {})\ncnt: 2, ((T([128, 512, 32, 32], f16),), {})\ncnt: 1, ((T([128, 256, 32, 32], f16),), {})\ncnt: 3, ((T([128, 256, 16, 16], f16),), {})\ncnt: 2, ((T([128, 1024, 16, 16], f16),), {})\ncnt: 1, ((T([128, 512, 16, 16], f16),), {})\ncnt: 3, ((T([128, 512, 8, 8], f16),), {})\ncnt: 2, ((T([128, 2048, 8, 8], f16),), {})\ncnt: 2, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16)), {})\ncnt: 3, ((T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16)), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16)), {})\ncnt: 2, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16)), {})\ncnt: 3, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16)), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([128, 256, 32, 32], f16)), {})\ncnt: 2, ((T([128, 512, 32, 32], f16), T([128, 512, 32, 32], f16)), {})\ncnt: 3, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16)), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16)), {})\ncnt: 2, ((T([128, 256, 64, 64], f16), T([128, 256, 64, 64], f16)), {})\ncnt: 4, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16)), {})\ncnt: 1, ((T([128, 64, 128, 128], f16), T([128, 64, 128, 128], f16)), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16)), {})\ncnt: 1, ((T([128, 24, 128, 128], f16), T([128, 24, 128, 128], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in DenseNet Forward Pass\nDESCRIPTION: This snippet shows the batch normalization operations used throughout DenseNet. It normalizes activations using running statistics with a momentum of 0.1 and epsilon of 1e-05, applied at different network depths with corresponding channel dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 6, ((T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([32, 160, 28, 28], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})\ncnt: 10, ((T([32, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), False, 0.1, 1e-05), {})\ncnt: 2, ((T([32, 768, 14, 14], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f16), False, 0.1, 1e-05), {})\ncnt: 10, ((T([32, 224, 7, 7], f16), T([224], f16), T([224], f16), T([224], f16), T([224], f16), False, 0.1, 1e-05), {})\ncnt: 2, ((T([32, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), False, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Example Function for Delta Debugging\nDESCRIPTION: Sample function showing how delta debugging works by promoting intermediate nodes to inputs to minimize the graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef f(a):\n    b = x * 2\n    c = b + 3\n    d = c / 4\n    return d\n```\n\nLANGUAGE: python\nCODE:\n```\ndef f(a, c):\n    b = x * 2\n    d = c / 4\n    return d\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Vision Transformer\nDESCRIPTION: A statistical breakdown of PyTorch operators used in a Vision Transformer model, including tensor shapes, data types (primarily float16), and call counts. This represents an execution trace or profiling output showing the internal operations of a ViT model processing batches of 8 images with 197 tokens (1414 + 1 class token) and 6 attention heads.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vision_transformer_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 12, ((T([8, 6, 197, 197], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([8, 6, 197, 197], f16), T([8, 6, 197, 197], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([8, 6, 197, 64], f16), [48, 197, 64]), {})\ncnt: 12, ((T([8, 6, 64, 197], f16), [48, 64, 197]), {})\ncnt: 12, ((T([48, 197, 197], f16), [8, 6, 197, 197]), {})\ncnt: 12, ((T([48, 197, 64], f16), [8, 6, 197, 64]), {})\ncnt: 12, ((T([8, 197, 6, 64], f16), [8, 197, 384]), {})\ncnt: 12, ((T([8, 197, 3, 6, 64], f16), [8, 197, 1152]), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([8, 197, 384], f16), T([1, 197, 384], f16)), {})\ncnt: 48, ((T([8, 197, 384], f16), T([8, 197, 384], f16)), {})\nOperator: aten.addmm.default\ncnt: 12, ((T([1152], f16), T([1576, 384], f16), T([384, 1152], f16, stride=(1, 384))), {})\ncnt: 12, ((T([384], f16), T([1576, 384], f16), T([384, 384], f16, stride=(1, 384))), {})\ncnt: 12, ((T([1536], f16), T([1576, 384], f16), T([384, 1536], f16, stride=(1, 384))), {})\ncnt: 12, ((T([384], f16), T([1576, 1536], f16), T([1536, 384], f16, stride=(1, 1536))), {})\ncnt: 1, ((T([1000], f16), T([8, 384], f16, stride=(75648, 1)), T([384, 1000], f16, stride=(1, 384))), {})\nOperator: aten.bmm.default\ncnt: 12, ((T([48, 197, 64], f16), T([48, 64, 197], f16)), {})\ncnt: 12, ((T([48, 197, 197], f16), T([48, 197, 64], f16)), {})\ncnt: 12, ((T([48, 197, 197], f16, stride=(38809, 1, 197)), T([48, 197, 64], f16)), {})\ncnt: 12, ((T([48, 197, 64], f16), T([48, 64, 197], f16, stride=(12608, 1, 64))), {})\ncnt: 12, ((T([48, 64, 197], f16, stride=(12608, 1, 64)), T([48, 197, 197], f16)), {})\ncnt: 12, ((T([48, 197, 197], f16), T([48, 197, 64], f16, stride=(12608, 1, 197))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([8, 1, 384], f16, stride=(0, 384, 1)), T([8, 196, 384], f16, stride=(75264, 1, 196))], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([8, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([8, 3, 224, 224], f16), T([384, 3, 16, 16], f16), T([384], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([8, 384, 14, 14], f16, stride=(75648, 1, 5376, 384)), T([8, 3, 224, 224], f16), T([384, 3, 16, 16], f16), [384], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([8, 3, 224, 224], f16), T([8, 3, 224, 224], f16)), {})\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 8000), {})\nOperator: aten.gelu.default\ncnt: 12, ((T([8, 197, 1536], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([8, 197, 1536], f16), T([8, 197, 1536], f16)), {})\nOperator: aten.mm.default\ncnt: 1, ((T([8, 1000], f16, stride=(0, 0)), T([1000, 384], f16)), {})\ncnt: 1, ((T([1000, 8], f16, stride=(0, 0)), T([8, 384], f16, stride=(75648, 1))), {})\ncnt: 12, ((T([1576, 384], f16), T([384, 1536], f16)), {})\ncnt: 12, ((T([384, 1576], f16, stride=(1, 384)), T([1576, 1536], f16)), {})\ncnt: 12, ((T([1576, 1536], f16), T([1536, 384], f16)), {})\ncnt: 12, ((T([1536, 1576], f16, stride=(1, 1536)), T([1576, 384], f16)), {})\ncnt: 12, ((T([1576, 384], f16), T([384, 384], f16)), {})\ncnt: 12, ((T([384, 1576], f16, stride=(1, 384)), T([1576, 384], f16)), {})\ncnt: 12, ((T([1576, 1152], f16), T([1152, 384], f16)), {})\ncnt: 12, ((T([1152, 1576], f16, stride=(1, 1152)), T([1576, 384], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 24, ((T([8, 6, 197, 197], f16), 0.125), {})\nOperator: aten.native_layer_norm.default\ncnt: 25, ((T([8, 197, 384], f16), [384], T([384], f16), T([384], f16), 1e-06), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 25, ((T([8, 197, 384], f16), T([8, 197, 384], f16), [384], T([8, 197, 1], f32), T([8, 197, 1], f32), T([384], f16), T([384], f16), [True, True, True]), {})\nOperator: aten.select_backward.default\ncnt: 1, ((T([8, 384], f16), [8, 197, 384], 1, 0), {})\nOperator: aten.slice_backward.default\ncnt: 1, ((T([8, 197, 384], f16), [8, 197, 384], 0, 0, 9223372036854775807, 1), {})\nOperator: aten.stack.default\ncnt: 12, (([T([8, 6, 197, 64], f16), T([8, 6, 197, 64], f16, stride=(75648, 12608, 1, 197)), T([8, 6, 197, 64], f16)],), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([8, 1000], f16, stride=(0, 0)), [0], True), {})\ncnt: 24, ((T([1576, 384], f16), [0], True), {})\ncnt: 12, ((T([1576, 1536], f16), [0], True), {})\ncnt: 12, ((T([1576, 1152], f16), [0], True), {})\ncnt: 1, ((T([8, 197, 384], f16), [0], True), {})\ncnt: 1, ((T([8, 1, 384], f16, stride=(75648, 384, 1)), [0], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([8, 1000], f16),), {})\nOperator: aten.unbind.int\ncnt: 12, ((T([3, 8, 6, 197, 64], f16, stride=(384, 226944, 64, 1152, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Listing PyTorch Models with Batch Sizes\nDESCRIPTION: This snippet provides a comprehensive list of PyTorch models and their corresponding batch sizes. Each line contains a model name followed by its batch size, separated by a comma. The batch sizes vary widely, likely reflecting different model complexities and hardware constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/torchbench_models_list_cpu.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nalexnet,128\nattention_is_all_you_need_pytorch,64\nBERT_pytorch,32\ndcgan,256\ndensenet121,512\ndlrm,2048\nfastNLP_Bert,8\nfunctorch_dp_cifar10,1024\nhf_Albert,8\nhf_Bart,8\nhf_Bert,8\nhf_Bert_large,8\nhf_DistilBert,8\nhf_GPT2,8\nhf_GPT2_large,1\nhf_Longformer,4\nhf_Reformer,8\nhf_T5,4\nhf_T5_base,1\nhf_T5_large,1\nLearningToPaint,96\nlennard_jones,1024\nmnasnet1_0,32\nmobilenet_v2,16\nmobilenet_v3_large,32\nnvidia_deeprecommender,256\nphlippe_densenet,128\nphlippe_resnet,512\npytorch_unet,4\nresnet152,32\nresnet18,256\nresnet50,256\nresnext50_32x4d,256\nshufflenet_v2_x1_0,64\nspeech_transformer,1024\nsqueezenet1_1,16\nSuper_SloMo,1024\ntimm_efficientnet,64\ntimm_nfnet,128\ntimm_regnet,32\ntimm_resnest,32\ntimm_vision_transformer,16\ntimm_vision_transformer_large,8\ntimm_vovnet,32\ntts_angular,1024\nvgg16,64\nvision_maskrcnn,1\nyolov3,32\n```\n\n----------------------------------------\n\nTITLE: Defining Binary Bitwise Operations in TorchScript\nDESCRIPTION: Specifies the syntax for bitwise AND (&), XOR (^), and OR (|) operations in TorchScript. These operate on int or Tensor arguments with specific rules for shape matching and broadcasting.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nand_expr ::=  shift_expr | and_expr '&' shift_expr\nxor_expr ::=  and_expr | xor_expr '^' and_expr\nor_expr  ::=  xor_expr | or_expr '|' xor_expr\n```\n\n----------------------------------------\n\nTITLE: Translating Python Conditional to PyTorch IR\nDESCRIPTION: This example shows a Python function with a conditional statement and its corresponding PyTorch IR representation. It demonstrates how control flow is represented using blocks in the IR.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef f(a, b, c):\n    d = a + b\n    if c:\n        e = d + d\n    else:\n        e = b + d\n    return e\n```\n\nLANGUAGE: text\nCODE:\n```\ngraph(%a : Dynamic,\n      %b : Dynamic,\n      %c : Dynamic):\n  %2 : int = prim::Constant[value=1]()\n  %3 : Dynamic = aten::add(%a, %b, %2)\n  %5 : Dynamic = prim::If(%c)\n    block0():\n      %6 : int = prim::Constant[value=1]()\n      %7 : Dynamic = aten::add(%3, %3, %6)\n      -> (%7)\n    }\n    block1():\n      %8 : int = prim::Constant[value=1]()\n      %9 : Dynamic = aten::add(%b, %3, %8)\n      -> (%9)\n  return (%5)\n```\n\n----------------------------------------\n\nTITLE: Executing aten.add.Tensor operation in PyTorch\nDESCRIPTION: Performs element-wise addition on two input tensors using the Aten backend in PyTorch with half-precision tensors. Key parameters include tensor shapes and data types. The output is a tensor with the same shape as the inputs. No additional dependencies are needed beyond PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 3, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Raw DataFrame Iterator Usage\nDESCRIPTION: Example of using raw_iterator() to iterate over DataFrame contents directly.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndp = get_dataframes_pipe()\nfor i in dp.raw_iterator():\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Defining Distributed Autograd Test Executable in CMake\nDESCRIPTION: This CMake script conditionally defines and configures the 'test_dist_autograd' executable if USE_DISTRIBUTED is enabled and the target platform is not Windows (WIN32). It specifies the source files, includes necessary directories (ATen_CPU_INCLUDE), links against 'torch' and 'gtest' libraries. It also conditionally adds a 'USE_CUDA' compile definition if USE_CUDA is enabled. Finally, if INSTALL_TEST is set, it configures installation rules for the executable and its PDB file (on MSVC).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/dist_autograd/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_DISTRIBUTED AND NOT WIN32)\n  set(DIST_AUTOGRAD_TEST_DIR \"${TORCH_ROOT}/test/cpp/dist_autograd\")\n  set(DIST_AUTOGRAD_TEST_SOURCES\n    ${TORCH_ROOT}/test/cpp/common/main.cpp\n    ${DIST_AUTOGRAD_TEST_DIR}/test_dist_autograd.cpp\n  )\n\n  add_executable(test_dist_autograd ${DIST_AUTOGRAD_TEST_SOURCES})\n  target_include_directories(test_dist_autograd PRIVATE ${ATen_CPU_INCLUDE})\n  target_link_libraries(test_dist_autograd PRIVATE torch gtest)\n\n  if(USE_CUDA)\n    target_compile_definitions(test_dist_autograd PRIVATE \"USE_CUDA\")\n  endif()\n\n  if(INSTALL_TEST)\n    set_target_properties(test_dist_autograd PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n    install(TARGETS test_dist_autograd DESTINATION bin)\n    # Install PDB files for MSVC builds\n    if(MSVC AND BUILD_SHARED_LIBS)\n      install(FILES $<TARGET_PDB_FILE:test_dist_autograd> DESTINATION bin OPTIONAL)\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Distributed Elastic Multiprocessing Module\nDESCRIPTION: This snippet shows how to import the PyTorch distributed elastic multiprocessing module. This module provides functionality for managing distributed processes in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/multiprocessing.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch.distributed.elastic.multiprocessing\n```\n\n----------------------------------------\n\nTITLE: Tensor Shape and Count Analysis in PyTorch\nDESCRIPTION: This snippet shows various tensor shapes and their occurrence counts in the project. It includes matrix multiplications with different dimensions and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 28, ((T([12544, 1152], f16), T([1152, 384], f16)), {})\ncnt: 14, ((T([384, 12544], f16, stride=(1, 384)), T([12544, 384], f16)), {})\ncnt: 14, ((T([12544, 384], f16), T([384, 384], f16)), {})\ncnt: 4, ((T([192, 50176], f16, stride=(1, 192)), T([50176, 576], f16)), {})\ncnt: 4, ((T([50176, 192], f16), T([192, 576], f16)), {})\ncnt: 4, ((T([576, 50176], f16, stride=(1, 576)), T([50176, 192], f16)), {})\ncnt: 4, ((T([50176, 576], f16), T([576, 192], f16)), {})\ncnt: 8, ((T([192, 50176], f16, stride=(1, 192)), T([50176, 192], f16)), {})\ncnt: 8, ((T([50176, 192], f16), T([192, 192], f16)), {})\ncnt: 4, ((T([486, 12544], f16, stride=(1, 486)), T([12544, 192], f16)), {})\ncnt: 4, ((T([12544, 486], f16), T([486, 192], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Addition in PyTorch\nDESCRIPTION: Utilizes aten.add.Tensor and its variants for element-wise addition across similar and broadcastable shapes, highlighting efficient computation pathways through direct tensor operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForQuestionAnswering_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 25, ((T([4, 512, 1], f32), 1e-07), {})\ncnt: 25, ((T([4, 512, 768], f16), T([768], f16)), {})\ncnt: 24, ((T([4, 12, 512, 64], f16, stride=(1179648, 192, 2304, 1)), T([1, 12, 1, 64], f16)), {})\ncnt: 48, ((T([4, 512, 768], f16), T([4, 512, 768], f16)), {})\ncnt: 1, ((T([], f16), T([], f16)), {})\ncnt: 50, ((T([4, 512, 768], f32), T([4, 512, 768], f32)), {})\ncnt: 25, ((T([4, 512, 1], f32), T([4, 512, 1], f32)), {})\n```\n\n----------------------------------------\n\nTITLE: Sigmoid Backward Pass in PyTorch\nDESCRIPTION: This snippet computes the gradient of the sigmoid function during the backward pass of backpropagation. It's crucial for training neural networks that use sigmoid activation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 3, 48, 64, 81], f16), T([8, 3, 48, 64, 81], f16, stride=(783360, 261120, 5440, 85, 1))), {})\ncnt: 1, ((T([8, 3, 48, 64, 2], f16), T([8, 3, 48, 64, 2], f16)), {})\ncnt: 1, ((T([8, 3, 24, 32, 81], f16), T([8, 3, 24, 32, 81], f16, stride=(195840, 65280, 2720, 85, 1))), {})\ncnt: 1, ((T([8, 3, 24, 32, 2], f16), T([8, 3, 24, 32, 2], f16)), {})\ncnt: 1, ((T([8, 3, 12, 16, 81], f16), T([8, 3, 12, 16, 81], f16, stride=(48960, 16320, 1360, 85, 1))), {})\ncnt: 1, ((T([8, 3, 12, 16, 2], f16), T([8, 3, 12, 16, 2], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Batch Normalization Operations in PyTorch\nDESCRIPTION: Log of batch normalization operations with running mean and variance tracking. Each operation includes the input tensor, scale, bias, running mean, running variance, and various configuration parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([64, 24, 128, 128], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 32, 128, 128], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 5, ((T([64, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Creating Zero-Initialized Tensors in PyTorch\nDESCRIPTION: This snippet creates zero-initialized tensors with specified shapes. It's commonly used for initializing tensors that will be filled with computed values later in the neural network process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 3, 48, 64, 4], f16), [6266880]), {})\ncnt: 1, ((T([8, 3, 24, 32, 4], f16), [1566720]), {})\ncnt: 1, ((T([8, 3, 12, 16, 4], f16), [391680]), {})\n```\n\n----------------------------------------\n\nTITLE: Documenting Diffs for CUTLASS Extension Headers (diff, C++)\nDESCRIPTION: This snippet is a unified diff, included in a Markdown code block, showing all changes between original FasterTransformer C++ header files and their PyTorch-adapted versions. It illustrates changes in constructor signatures, added helper functions, comment toggling, include path alterations, and various compatibility edits. It assumes basic familiarity with C++ and CUDA, as well as CUTLASS and PyTorch extension layouts. No build or runtime dependencies are required to interpret the diff, but understanding the context requires knowledge of C++ headers and CUDA programming. Inputs: original and modified file versions; output: a diff code block for documentation and traceability.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cuda/cutlass_extensions/README.md#2025-04-22_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\nOnly in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions: compute_occupancy.h\nOnly in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/epilogue: epilogue_quant_helper.h\nOnly in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/epilogue: threadblock\ndiff -r FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel/fpA_intB_gemm.h pytorch/aten/src/ATen/native/cuda/cutlass_extensions/gemm/kernel/fpA_intB_gemm.h\n157c157,158\n<     struct Params {\n---\n>     struct Params\n>     {\n183d183\n<         CUTLASS_HOST_DEVICE\n186d185\n<         CUTLASS_HOST_DEVICE\n188,190c187,188\n<                cutlass::gemm::GemmCoord const& grid_tiled_shape,\n<                const int                       gemm_k_size,\n<                void*                           workspace = nullptr):\n---\n>                int                             device_sms,\n>                int                             sm_occupancy):\n192d189\n<             grid_tiled_shape(grid_tiled_shape),\n205,206d201\n<             semaphore(static_cast<int*>(workspace)),\n<             gemm_k_size(gemm_k_size),\n210a206,227\n>             ThreadblockSwizzle swizzle;\n>             grid_tiled_shape = swizzle.get_tiled_shape(\n>                 args.problem_size,\n>                 {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},\n>                 args.batch_count);\n>\n>             gemm_k_size = args.problem_size.k();\n>         }\n>\n>         size_t get_workspace_size() const\n>         {\n>             return 0;\n>         }\n>\n>         Status init_workspace(void *workspace,cudaStream_t stream = nullptr)\n>         {\n>             return Status::kSuccess;\n>         }\n>\n>         dim3 get_grid_dims() const\n>         {\n>             return ThreadblockSwizzle().get_grid_shape(grid_tiled_shape);\n278,283d294\n<     static size_t get_extra_workspace_size(Arguments const& args, cutlass::gemm::GemmCoord const& grid_tiled_shape)\n<     {\n<\n<         return 0;\n<     }\n<\n464a476,482\n>     CUTLASS_DEVICE\n>     static void invoke(Params const &params, SharedStorage &shared_storage)\n>     {\n>         GemmFpAIntB op;\n>         op(params, shared_storage);\n>     }\n>\n492c510\n< }  // namespace cutlass\n\\ No newline at end of file\n---\n> }  // namespace cutlass\nOnly in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel: gemm_moe_problem_visitor.h\nOnly in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel: gemm_with_epilogue_visitor.h\nOnly in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel: moe_cutlass_kernel.h\nOnly in FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/kernel: moe_problem_visitor.h\ndiff -r FasterTransformer/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/gemm/warp/mma_tensorop_dequantizer.h pytorch/aten/src/ATen/native/cuda/cutlass_extensions/gemm/warp/mma_tensorop_dequantizer.h\n55c55,58\n< #include <src/fastertransformer/utils/cuda_bf16_wrapper.h>\n---\n> //#include <src/fastertransformer/utils/cuda_bf16_wrapper.h>\n> //#ifdef ENABLE_BF16\n> #include <cuda_bf16.h>\n> //#endif\n155c158,159\n< #if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800) && defined(ENABLE_BF16))\n---\n> //#if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800) && defined(ENABLE_BF16))\n> #if (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800))\n470c474\n< ////////////////////////////////////////////////////////////////////////////////\n\\ No newline at end of file\n---\n> ////////////////////////////////////////////////////////////////////////////////\n\n```\n\n----------------------------------------\n\nTITLE: Tracing a Function with jit.trace in PyTorch\nDESCRIPTION: This code snippet demonstrates how to use torch.jit.trace to create a traced version of the 'add_two_maybe' function. It also shows a case where the trace works correctly for the initial inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nt = torch.ones(1)\nmaybe_false = torch.BoolTensor([0])\ngood_inputs = (t, maybe_false)\njit = torch.jit.trace(add_two_maybe, good_inputs)\n# let's check that the results match with eager\nassert jit(*good_inputs) == add_two_maybe(*good_inputs)\n```\n\n----------------------------------------\n\nTITLE: Loading Batched Data from Iterable-style Dataset\nDESCRIPTION: Demonstrates the equivalent operation of loading batched samples from an iterable-style dataset when automatic batching is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/data.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Role for Hidden Sections in PyTorch Documentation\nDESCRIPTION: Creates a custom 'hidden' role for use in reStructuredText documentation. This role can be applied to sections that should be hidden in the rendered output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/autosummary/class.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up PyTorch Development Environment\nDESCRIPTION: Instructions for cloning the PyTorch repository, setting up the development environment, and activating the virtual environment. This includes steps for different build configurations like CUDA and ROCm.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:<USERNAME>/pytorch.git\ncd pytorch\ngit remote add upstream git@github.com:pytorch/pytorch.git\n\nmake setup-env\n# Or run `make setup-env-cuda` for pre-built CUDA binaries\n# Or run `make setup-env-rocm` for pre-built ROCm binaries\nsource venv/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\n----------------------------------------\n\nTITLE: Enabling CPU Fallback for Unsupported MPS Operations (Environment Variable)\nDESCRIPTION: Set `PYTORCH_ENABLE_MPS_FALLBACK` to `1` to allow PyTorch operations to automatically fall back to the CPU execution if they are not currently supported by the MPS backend on Apple Silicon.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps_environment_variables.rst#2025-04-22_snippet_7\n\nLANGUAGE: plaintext\nCODE:\n```\nPYTORCH_ENABLE_MPS_FALLBACK\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in PyTorch with Float16 Tensors\nDESCRIPTION: Records of batch normalization operations with various tensor shapes and parameters. Each entry shows the count (cnt) of operations with specific tensor configurations, including input tensors, running statistics, and epsilon value of 1e-05.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncnt: 8, ((T([8, 1024, 14, 14], f16), T([8, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), False, 1e-05, [True, True, True]), {})\ncnt: 11, ((T([8, 512, 14, 14], f16), T([8, 512, 14, 14], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([8, 512, 28, 28], f16), T([8, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})\ncnt: 7, ((T([8, 256, 28, 28], f16), T([8, 256, 28, 28], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([8, 256, 56, 56], f16), T([8, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([8, 128, 56, 56], f16), T([8, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([8, 64, 112, 112], f16), T([8, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Transformer Implementation\nDESCRIPTION: A detailed breakdown of PyTorch operator calls showing tensor dimensions, data types, and call frequency in what appears to be a transformer model implementation. This analysis helps understand the computational patterns and memory requirements of the model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/AlbertForQuestionAnswering_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 2, ((T([2, 512], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 2, ((T([2, 512], f16), T([2, 512], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 12, ((T([2, 64, 512, 512], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([2, 64, 512, 512], f16), T([2, 64, 512, 512], f16), -1, f16), {})\nOperator: aten._to_copy.default\ncnt: 1, ((T([2, 1, 1, 512], f32),), {'dtype': f16})\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([2, 64, 512, 64], f16), [128, 512, 64]), {})\ncnt: 12, ((T([2, 64, 64, 512], f16), [128, 64, 512]), {})\ncnt: 12, ((T([128, 512, 512], f16), [2, 64, 512, 512]), {})\ncnt: 12, ((T([128, 512, 64], f16), [2, 64, 512, 64]), {})\ncnt: 36, ((T([2, 512, 64, 64], f16), [2, 512, 4096]), {})\ncnt: 12, ((T([2, 512, 4096], f16), [1024, 4096]), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([2, 512, 128], f16), T([2, 512, 128], f16)), {})\ncnt: 12, ((T([2, 64, 512, 512], f16), T([2, 1, 1, 512], f16)), {})\ncnt: 72, ((T([2, 512, 4096], f16), T([2, 512, 4096], f16)), {})\ncnt: 36, ((T([2, 512, 16384], f16), T([2, 512, 16384], f16)), {})\ncnt: 12, ((T([2, 512, 16384], f16), 1.0), {})\ncnt: 1, ((T([], f16), T([], f16)), {})\ncnt: 99, ((T([4096], f16), T([4096], f16)), {})\ncnt: 11, ((T([4096, 16384], f16), T([4096, 16384], f16)), {})\ncnt: 11, ((T([16384], f16), T([16384], f16)), {})\ncnt: 11, ((T([16384, 4096], f16), T([16384, 4096], f16)), {})\ncnt: 44, ((T([4096, 4096], f16), T([4096, 4096], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 1, ((T([2, 512, 128], f16), T([1, 512, 128], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([4096], f16), T([1024, 128], f16), T([128, 4096], f16, stride=(1, 128))), {})\ncnt: 48, ((T([4096], f16), T([1024, 4096], f16), T([4096, 4096], f16, stride=(1, 4096))), {})\ncnt: 12, ((T([16384], f16), T([1024, 4096], f16), T([4096, 16384], f16, stride=(1, 4096))), {})\ncnt: 12, ((T([4096], f16), T([1024, 16384], f16), T([16384, 4096], f16, stride=(1, 16384))), {})\ncnt: 1, ((T([2], f16), T([1024, 4096], f16), T([4096, 2], f16, stride=(1, 4096))), {})\nOperator: aten.bmm.default\ncnt: 12, ((T([128, 512, 64], f16), T([128, 64, 512], f16)), {})\ncnt: 12, ((T([128, 512, 512], f16), T([128, 512, 64], f16)), {})\ncnt: 12, ((T([128, 512, 512], f16, stride=(262144, 1, 512)), T([128, 512, 64], f16)), {})\ncnt: 12, ((T([128, 512, 64], f16), T([128, 64, 512], f16, stride=(32768, 1, 64))), {})\ncnt: 12, ((T([128, 64, 512], f16, stride=(32768, 1, 64)), T([128, 512, 512], f16)), {})\ncnt: 12, ((T([128, 512, 512], f16), T([128, 512, 64], f16, stride=(32768, 1, 512))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([2, 512, 1], f16), T([2, 512, 1], f16)], 2), {})\nOperator: aten.clamp.default\ncnt: 2, ((T([2], i64), 0, 512), {})\nOperator: aten.clone.default\ncnt: 1, ((T([2, 512], i64),), {})\ncnt: 2, ((T([2], i64),), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([2, 512], i64), T([2, 512], i64)), {})\ncnt: 2, ((T([2], i64), T([2], i64)), {})\nOperator: aten.div.Tensor\ncnt: 24, ((T([2, 64, 512, 512], f16), 8.0), {})\ncnt: 2, ((T([], f16), 2), {})\nOperator: aten.embedding.default\ncnt: 1, ((T([30000, 128], f16), T([2, 512], i64), 0), {})\ncnt: 1, ((T([2, 128], f16), T([2, 512], i64, stride=(0, 1))), {})\ncnt: 1, ((T([512, 128], f16), T([1, 512], i64)), {})\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([1, 512, 128], f16), T([1, 512], i64), 512, -1, False), {})\ncnt: 1, ((T([2, 512, 128], f16), T([2, 512], i64, stride=(0, 1)), 2, -1, False), {})\ncnt: 1, ((T([2, 512, 128], f16), T([2, 512], i64), 30000, 0, False), {})\nOperator: aten.mm.default\ncnt: 1, ((T([1024, 2], f16), T([2, 4096], f16)), {})\ncnt: 1, ((T([2, 1024], f16, stride=(1, 2)), T([1024, 4096], f16)), {})\ncnt: 12, ((T([1024, 4096], f16), T([4096, 16384], f16)), {})\ncnt: 12, ((T([4096, 1024], f16, stride=(1, 4096)), T([1024, 16384], f16)), {})\ncnt: 12, ((T([1024, 16384], f16), T([16384, 4096], f16)), {})\ncnt: 12, ((T([16384, 1024], f16, stride=(1, 16384)), T([1024, 4096], f16)), {})\ncnt: 48, ((T([1024, 4096], f16), T([4096, 4096], f16)), {})\ncnt: 48, ((T([4096, 1024], f16, stride=(1, 4096)), T([1024, 4096], f16)), {})\ncnt: 1, ((T([1024, 4096], f16), T([4096, 128], f16)), {})\ncnt: 1, ((T([4096, 1024], f16, stride=(1, 4096)), T([1024, 128], f16)), {})\nOperator: aten.mul.Scalar\ncnt: 12, ((T([2, 512, 16384], f16), 3.0), {})\nOperator: aten.mul.Tensor\ncnt: 1, ((T([2, 1, 1, 512], f16), -65504.0), {})\ncnt: 24, ((T([2, 512, 16384], f16), 0.5), {})\ncnt: 24, ((T([2, 512, 16384], f16), 0.044715), {})\ncnt: 24, ((T([2, 512, 16384], f16), 0.7978845608028654), {})\ncnt: 48, ((T([2, 512, 16384], f16), T([2, 512, 16384], f16)), {})\nOperator: aten.native_layer_norm.default\ncnt: 1, ((T([2, 512, 128], f16), [128], T([128], f16), T([128], f16), 1e-12), {})\ncnt: 24, ((T([2, 512, 4096], f16), [4096], T([4096], f16), T([4096], f16), 1e-12), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 24, ((T([2, 512, 4096], f16), T([2, 512, 4096], f16), [4096], T([2, 512, 1], f32), T([2, 512, 1], f32), T([4096], f16), T([4096], f16), [True, True, True]), {})\ncnt: 1, ((T([2, 512, 128], f16), T([2, 512, 128], f16), [128], T([2, 512, 1], f32), T([2, 512, 1], f32), T([128], f16), T([128], f16), [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 2, ((T([], f16), T([2, 512], f16), T([2], i64), None, 1, 512, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 2, ((T([2, 512], f16), T([2], i64), None, 1, 512), {})\nOperator: aten.pow.Tensor_Scalar\ncnt: 12, ((T([2, 512, 16384], f16), 3.0), {})\ncnt: 12, ((T([2, 512, 16384], f16), 2.0), {})\nOperator: aten.rsub.Scalar\ncnt: 1, ((T([2, 1, 1, 512], f16), 1.0), {})\nOperator: aten.split.Tensor\ncnt: 1, ((T([2, 512, 2], f16), 1, -1), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([1024, 2], f16), [0], True), {})\ncnt: 61, ((T([1024, 4096], f16), [0], True), {})\ncnt: 12, ((T([1024, 16384], f16), [0], True), {})\ncnt: 1, ((T([2, 512, 128], f16), [0], True), {})\nOperator: aten.tanh.default\ncnt: 12, ((T([2, 512, 16384], f16),), {})\nOperator: aten.tanh_backward.default\ncnt: 12, ((T([2, 512, 16384], f16), T([2, 512, 16384], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Including XPU CMake Configuration\nDESCRIPTION: Includes the main XPU configuration file that contains common settings for XPU builds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/xpu/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(../../cmake/public/xpu.cmake)\n```\n\n----------------------------------------\n\nTITLE: Registering Multiple PyTorch Operators for Benchmarking\nDESCRIPTION: This snippet shows how to register multiple PyTorch operators for benchmarking using the generate_pt_tests_from_op_list function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nop_bench.generate_pt_tests_from_op_list(unary_ops_list, unary_ops_configs, UnaryOpBenchmark)\n```\n\n----------------------------------------\n\nTITLE: Tracking Batch Normalization Operations in PyTorch\nDESCRIPTION: Records of batch normalization forward operations with various tensor shapes and parameters. Each line shows the count (cnt) of operations with specific tensor dimensions (batch_size, channels, height, width), data types (f16), and normalization parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((T([128, 112, 12, 12], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([128, 672, 12, 12], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 672, 6, 6], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})\ncnt: 5, ((T([128, 192, 6, 6], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 10, ((T([128, 1152, 6, 6], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 320, 6, 6], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 1280, 6, 6], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Up C10 HIP Library for Libtorchless Build\nDESCRIPTION: Finds the pre-built C10 HIP library when building in libtorchless mode.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/hip/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_LIBTORCHLESS)\n  find_library(C10_HIP_LIB c10_hip PATHS $ENV{LIBTORCH_LIB_PATH} NO_DEFAULT_PATH)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Timer Request in PyTorch Distributed Elastic\nDESCRIPTION: Class for defining timer requests used to pass messages between the server and client in custom timer implementations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nTimerRequest\n```\n\n----------------------------------------\n\nTITLE: MSVC Static Assert Example\nDESCRIPTION: Example demonstrating MSVC compiler limitations with static assertions and constexpr functions in template code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_39\n\nLANGUAGE: cpp\nCODE:\n```\nclass A {\n  static A singleton_;\n  static constexpr inline A* singleton() {\n    return &singleton_;\n  }\n};\nstatic_assert(std::is_same(A*, decltype(A::singleton()))::value, \"hmm\");\n```\n\n----------------------------------------\n\nTITLE: Including Quantized Source Files in Build System (TARGETS)\nDESCRIPTION: Demonstrates how to modify the build configuration file (`caffe2/aten/TARGETS`) to include new quantized source files, particularly if they are not located in the default `native/quantized/cpu` path. It uses a `glob` pattern (`\"src/ATen/native/quantized/**/*.cpp\"`) within the `ATEN_NATIVE_CPP` definition to ensure `.cpp` files within the specified directory structure are compiled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nATEN_NATIVE_CPP = glob([\n#...\n  \"src/ATen/native/quantized/**/*.cpp\",\n])\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Statistics for Neural Network Execution\nDESCRIPTION: This code represents diagnostics output from PyTorch showing operator calls with their input tensor shapes, strides, and occurrence counts. Each line shows how many times a specific operator was called with particular tensor dimensions, primarily using half-precision (f16) tensors in a convolutional neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dla102_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([64, 1024, 7, 7], f16), T([64, 1024, 7, 7], f16, stride=(125440, 49, 7, 1))), {})\ncnt: 1, ((T([64, 1024, 7, 7], f16, stride=(125440, 49, 7, 1)), T([64, 1024, 7, 7], f16)), {})\ncnt: 1, ((T([64, 1024, 7, 7], f16), T([64, 1024, 7, 7], f16)), {})\ncnt: 1, ((T([64, 512, 7, 7], f16, stride=(125440, 49, 7, 1)), T([64, 512, 7, 7], f16)), {})\ncnt: 16, ((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16)), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16, stride=(551936, 196, 14, 1))), {})\ncnt: 4, ((T([64, 512, 14, 14], f16, stride=(551936, 196, 14, 1)), T([64, 512, 14, 14], f16)), {})\ncnt: 4, ((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16, stride=(200704, 196, 14, 1))), {})\ncnt: 4, ((T([64, 512, 14, 14], f16, stride=(200704, 196, 14, 1)), T([64, 512, 14, 14], f16)), {})\ncnt: 2, ((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16, stride=(301056, 196, 14, 1))), {})\ncnt: 4, ((T([64, 512, 14, 14], f16, stride=(301056, 196, 14, 1)), T([64, 512, 14, 14], f16)), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16, stride=(401408, 196, 14, 1))), {})\ncnt: 3, ((T([64, 512, 14, 14], f16, stride=(401408, 196, 14, 1)), T([64, 512, 14, 14], f16)), {})\ncnt: 9, ((T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16)), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16, stride=(903168, 784, 28, 1))), {})\ncnt: 3, ((T([64, 256, 28, 28], f16, stride=(903168, 784, 28, 1)), T([64, 256, 28, 28], f16)), {})\ncnt: 2, ((T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16, stride=(401408, 784, 28, 1))), {})\ncnt: 2, ((T([64, 256, 28, 28], f16, stride=(401408, 784, 28, 1)), T([64, 256, 28, 28], f16)), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16, stride=(602112, 784, 28, 1))), {})\ncnt: 2, ((T([64, 256, 28, 28], f16, stride=(602112, 784, 28, 1)), T([64, 256, 28, 28], f16)), {})\ncnt: 3, ((T([64, 128, 56, 56], f16), T([64, 128, 56, 56], f16)), {})\ncnt: 1, ((T([64, 128, 56, 56], f16), T([64, 128, 56, 56], f16, stride=(802816, 3136, 56, 1))), {})\ncnt: 1, ((T([64, 128, 56, 56], f16, stride=(802816, 3136, 56, 1)), T([64, 128, 56, 56], f16)), {})\ncnt: 1, ((T([64, 32, 112, 112], f16), T([64, 32, 112, 112], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 105, ((T([], i64), 1), {})\ncnt: 3, ((T([64, 128, 56, 56], f16), T([64, 128, 56, 56], f16)), {})\ncnt: 12, ((T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16)), {})\ncnt: 24, ((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16)), {})\ncnt: 3, ((T([64, 1024, 7, 7], f16), T([64, 1024, 7, 7], f16)), {})\nOperator: aten.cat.default\ncnt: 1, (([T([64, 128, 56, 56], f16), T([64, 128, 56, 56], f16)], 1), {})\ncnt: 2, (([T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16)], 1), {})\ncnt: 1, (([T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16), T([64, 128, 28, 28], f16), T([64, 256, 28, 28], f16), T([64, 256, 28, 28], f16)], 1), {})\ncnt: 4, (([T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16)], 1), {})\ncnt: 2, (([T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), T([64, 256, 14, 14], f16), T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16)], 1), {})\ncnt: 1, (([T([64, 1024, 7, 7], f16), T([64, 1024, 7, 7], f16), T([64, 512, 7, 7], f16)], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([16, 3, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 16, 224, 224], f16), T([16, 16, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 16, 224, 224], f16), T([32, 16, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 32, 56, 56], f16), T([128, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 32, 112, 112], f16), T([64, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 64, 112, 112], f16), T([64, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([64, 64, 56, 56], f16), T([128, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 128, 56, 56], f16), T([64, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 256, 56, 56], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 9, ((T([64, 128, 28, 28], f16), T([256, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 128, 56, 56], f16), T([128, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 128, 56, 56], f16), T([128, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 7, ((T([64, 256, 28, 28], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 7, ((T([64, 128, 28, 28], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([64, 512, 28, 28], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 768, 28, 28], f16), T([256, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 1152, 28, 28], f16), T([256, 1152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 17, ((T([64, 256, 14, 14], f16), T([512, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([256, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([256, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 15, ((T([64, 512, 14, 14], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 15, ((T([64, 256, 14, 14], f16), T([256, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([64, 1024, 14, 14], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([64, 1536, 14, 14], f16), T([512, 1536, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 2048, 14, 14], f16), T([512, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 2816, 14, 14], f16), T([512, 2816, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([64, 512, 7, 7], f16), T([1024, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([512, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([512, 512, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 1024, 7, 7], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 512, 7, 7], f16), T([512, 512, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 2560, 7, 7], f16), T([1024, 2560, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([64, 1024, 1, 1], f16), T([1000, 1024, 1, 1], f16), T([1000], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 1000, 1, 1], f16), T([64, 1024, 1, 1], f16), T([1000, 1024, 1, 1], f16), [1000], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([64, 1024, 7, 7], f16), T([64, 2560, 7, 7], f16), T([1024, 2560, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([64, 1024, 7, 7], f16), T([64, 512, 7, 7], f16), T([1024, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 512, 7, 7], f16), T([64, 512, 7, 7], f16), T([512, 512, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 512, 7, 7], f16), T([64, 1024, 7, 7], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 512, 7, 7], f16), T([64, 512, 14, 14], f16), T([512, 512, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([64, 512, 14, 14], f16), T([512, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([64, 2816, 14, 14], f16), T([512, 2816, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 17, ((T([64, 512, 14, 14], f16), T([64, 256, 14, 14], f16), T([512, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 15, ((T([64, 256, 14, 14], f16), T([64, 256, 14, 14], f16), T([256, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 15, ((T([64, 256, 14, 14], f16), T([64, 512, 14, 14], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch BatchNorm Backward Operator - Python\nDESCRIPTION: This snippet defines input shapes, dtypes, and arguments for profiling the PyTorch ATen native_batch_norm_backward.default operator. It systematically varies input tensors (shapes, strides), running stats, weights, and config flags such as training/evaluation mode, epsilon, and result mask. Dependencies include PyTorch and an understanding of ATen operator testing. Inputs are tuples representing tensor and parameter states, expected to simulate realistic backward passes for batch norm layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncnt: 3, ((T([128, 2048, 8, 8], f16), T([128, 2048, 8, 8], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), True, 1e-05, [True, True, True]), {}\ncnt: 3, ((T([128, 512, 8, 8], f16), T([128, 512, 8, 8], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {}\ncnt: 1, ((T([128, 512, 16, 16], f16), T([128, 512, 16, 16], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {}\ncnt: 3, ((T([128, 1024, 16, 16], f16), T([128, 1024, 16, 16], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {}\ncnt: 3, ((T([128, 256, 16, 16], f16), T([128, 256, 16, 16], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {}\n```\n\n----------------------------------------\n\nTITLE: PyTorch Log Softmax Operations\nDESCRIPTION: Log softmax forward and backward operations on 128x1000 tensors using float16 precision\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._log_softmax.default(T([128, 1000], f16), 1, False)\naten._log_softmax_backward_data.default(T([128, 1000], f16), T([128, 1000], f16), 1, f16)\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in PyTorch\nDESCRIPTION: Forward and backward batch normalization operations with momentum=0.1 and epsilon=1e-05, operating on various tensor shapes in half-precision format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\naten.native_batch_norm.default\ncnt: 5, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.threshold_backward.default Threshold Backward Pass in PyTorch ATen\nDESCRIPTION: Documents observed calls to the backward pass for a threshold operation (`aten.threshold_backward.default`), likely related to ReLU backward pass. It lists the different gradient output and original input tensor shapes (all f16), the threshold value (0), and their respective call counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_24\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 9, ((T([128, 384, 1, 1], f16), T([128, 384, 1, 1], f16), 0), {})\ncnt: 2, ((T([128, 128, 1, 1], f16), T([128, 128, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 64, 1, 1], f16), T([128, 64, 1, 1], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Creating Type Stub Targets for PyTorch Python\nDESCRIPTION: Sets up the targets and commands to generate Python type stubs (.pyi files) for the PyTorch Python API, which enables better IDE support and type checking.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nadd_custom_target(torch_python_stubs DEPENDS\n    \"${TORCH_SRC_DIR}/_C/__init__.pyi\"\n    \"${TORCH_SRC_DIR}/_C/_VariableFunctions.pyi\"\n    \"${TORCH_SRC_DIR}/nn/functional.pyi\"\n    \"${TORCH_SRC_DIR}/utils/data/datapipes/datapipe.pyi\"\n)\n\nfile(GLOB_RECURSE torchgen_python \"${PROJECT_SOURCE_DIR}/torchgen/*.py\")\nfile(GLOB_RECURSE autograd_python \"${TOOLS_PATH}/autograd/*.py\")\nfile(GLOB_RECURSE pyi_python \"${TOOLS_PATH}/pyi/*.py\")\nadd_custom_command(\n    OUTPUT\n    \"${TORCH_SRC_DIR}/_C/__init__.pyi\"\n    \"${TORCH_SRC_DIR}/_C/_VariableFunctions.pyi\"\n    \"${TORCH_SRC_DIR}/nn/functional.pyi\"\n    COMMAND\n    \"${Python_EXECUTABLE}\" -mtools.pyi.gen_pyi\n      --native-functions-path \"aten/src/ATen/native/native_functions.yaml\"\n      --tags-path \"aten/src/ATen/native/tags.yaml\"\n      --deprecated-functions-path \"tools/autograd/deprecated.yaml\"\n    DEPENDS\n      \"${TORCH_SRC_DIR}/_C/__init__.pyi.in\"\n      \"${TORCH_SRC_DIR}/_C/_VariableFunctions.pyi.in\"\n      \"${TORCH_SRC_DIR}/nn/functional.pyi.in\"\n      \"${TORCH_ROOT}/aten/src/ATen/native/native_functions.yaml\"\n      \"${TORCH_ROOT}/aten/src/ATen/native/tags.yaml\"\n      \"${TORCH_ROOT}/tools/autograd/deprecated.yaml\"\n      \"${TORCH_ROOT}/torch/_torch_docs.py\"\n      \"${TORCH_ROOT}/torch/_tensor_docs.py\"\n      ${pyi_python}\n      ${autograd_python}\n      ${torchgen_python}\n    WORKING_DIRECTORY\n    \"${TORCH_ROOT}\"\n)\nfile(GLOB_RECURSE datapipe_files \"${TORCH_SRC_DIR}/utils/data/datapipes/*.py\")\nadd_custom_command(\n    OUTPUT\n    \"${TORCH_SRC_DIR}/utils/data/datapipes/datapipe.pyi\"\n    COMMAND\n    \"${Python_EXECUTABLE}\" ${TORCH_SRC_DIR}/utils/data/datapipes/gen_pyi.py\n    DEPENDS\n    \"${TORCH_SRC_DIR}/utils/data/datapipes/datapipe.pyi.in\"\n    ${datapipe_files}\n    WORKING_DIRECTORY\n    \"${TORCH_ROOT}\"\n)\n```\n\n----------------------------------------\n\nTITLE: CUDA Device Count Query in C++\nDESCRIPTION: This function returns the number of available CUDA devices. It's crucial for managing multi-GPU setups in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104cuda12device_countEv\n```\n\n----------------------------------------\n\nTITLE: Threshold Backward Operations\nDESCRIPTION: Backward pass operations for threshold/ReLU computations, working with various tensor shapes. Each operation takes two tensors of the same shape and a threshold value of 0.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 1024, 7, 7], f16), T([4, 1024, 7, 7], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking PyTorch Tensor Operations with Shape Information\nDESCRIPTION: A log tracking PyTorch tensor operations showing the number of times each operation is called with specific tensor shapes and data types. Each line shows the count, tensor shapes (using T notation), data types (f16, i64), and any additional parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([965], f16), 0, T([54836], i64), T([54836], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54774], i64), T([54774], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54803], i64), T([54803], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54770], i64), T([54770], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54747], i64), T([54747], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54737], i64), T([54737], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54741], i64), T([54741], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54800], i64), T([54800], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54811], i64), T([54811], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54758], i64), T([54758], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54829], i64), T([54829], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54838], i64), T([54838], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54759], i64), T([54759], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54733], i64), T([54733], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54844], i64), T([54844], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54718], i64), T([54718], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54842], i64), T([54842], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54769], i64), T([54769], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54821], i64), T([54821], f16)), {})\ncnt: 3, ((T([965], f16), 0, T([54782], i64), T([54782], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54710], i64), T([54710], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54820], i64), T([54820], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54692], i64), T([54692], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54727], i64), T([54727], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54767], i64), T([54767], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54819], i64), T([54819], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54756], i64), T([54756], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54843], i64), T([54843], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54735], i64), T([54735], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54715], i64), T([54715], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54828], i64), T([54828], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54712], i64), T([54712], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54855], i64), T([54855], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54725], i64), T([54725], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54816], i64), T([54816], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54807], i64), T([54807], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54701], i64), T([54701], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54813], i64), T([54813], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54749], i64), T([54749], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54736], i64), T([54736], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54705], i64), T([54705], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54750], i64), T([54750], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54862], i64), T([54862], f16)), {})\ncnt: 1, ((T([965], f16), 0, T([54762], i64), T([54762], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Referencing PyTorch Pipeline Package\nDESCRIPTION: Shows the package import path for PyTorch's pipeline parallelism functionality\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/pipelining/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\ntorch.distributed.pipelining\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Sparse Tensor Usage Patterns\nDESCRIPTION: Detailed log showing tensor specifications with varying indices but consistent shape patterns. Each entry shows count (cnt), tensor dimensions, and configuration including float16 dtype, sparse_coo layout, and CUDA device placement.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((1, 1, [965, 192], T([1, 54738], i64), T([54738, 192], f16)), {'dtype': f16, 'layout': torch.sparse_coo, 'device': 'cuda', 'pin_memory': None})\n```\n\n----------------------------------------\n\nTITLE: Generating cmake_macros.h Header File in CMake\nDESCRIPTION: Uses the `configure_file` command to generate the `cmake_macros.h` header file from the template file `cmake_macros.h.in`, substituting any CMake variables referenced within the template. The output file is placed in the `pytorch_android_DIR`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nconfigure_file(\n    ${pytorch_android_DIR}/cmake_macros.h.in\n    ${pytorch_android_DIR}/cmake_macros.h)\n```\n\n----------------------------------------\n\nTITLE: Matrix Operations with PyTorch Tensors\nDESCRIPTION: Collection of matrix multiplication and addition operations using half-precision (f16) tensors with specified shapes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/maml_omniglot_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten.addmm.default((T([5], f16), T([5, 64], f16), T([64, 5], f16, stride=(1, 64))))\naten.mm.default((T([5, 5], f16, stride=(0, 0)), T([5, 64], f16)))\n```\n\n----------------------------------------\n\nTITLE: Handling Tensor Gradients in PyTorch\nDESCRIPTION: Performs operations related to gradient computation and backpropagation, including selecting and slicing tensors for gradient calculations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\naten.select_backward.default(T([64, 256], f16), [64, 50, 256], 1, -1)\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.slice_backward.default(T([64, 50, 256], f16), [64, 50, 256], 0, 0, 9223372036854775807, 1)\n```\n\n----------------------------------------\n\nTITLE: Including Quantized Source Files in Build System (CMakeLists.txt)\nDESCRIPTION: Shows how to update the `CMakeLists.txt` file (`caffe2/aten/src/ATen/CMakeLists.txt`) to include new quantized source files using `FILE(GLOB ...)`. This ensures that files in specified paths (like `\"native/quantized/cpu/*.cpp\"`) are included in the CMake build process, necessary if custom file locations are used.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nFILE(GLOB native_quantized_cpp\n          \"native/quantized/*.cpp\"\n          \"native/quantized/cpu/*.cpp\")\n```\n\n----------------------------------------\n\nTITLE: Dynamic Shape Recompilation Example\nDESCRIPTION: Demonstrates how recompilations occur with dynamic shapes and fixed cache size limits.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.compile(dynamic=False)\ndef fn(x):\n    return x + 1\n\nfor i in range(1, 10):\n    fn(torch.ones(i))\n```\n\n----------------------------------------\n\nTITLE: TorchScript Custom Class Error Example\nDESCRIPTION: Shows incorrect usage of instance attributes in TorchScript custom classes when not defined in __init__.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n@torch.jit.script\nclass foo:\n    def __init__(self):\n        self.y = 1\n\n# ERROR: self.x is not defined in __init__\ndef assign_x(self):\n    self.x = torch.rand(2, 3)\n```\n\n----------------------------------------\n\nTITLE: Defining OpInfo with Custom Operator\nDESCRIPTION: Example of defining an OpInfo where the target operator differs from the OpInfo name, using ops.aten.bernoulli.p_deterministic as an example.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nopinfo_core.OpInfo(\n    \"ops.aten.bernoulli.p_deterministic\",\n    op=torch.ops.aten.bernoulli.p,\n```\n\n----------------------------------------\n\nTITLE: Implementing Observer Insertion Points in PyTorch ModelReport Detector\nDESCRIPTION: Example of implementing the determine_observer_insert_points() method in a custom detector. This method initializes the custom Observer and adds it to the returned dictionary.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/_model_report/README.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef determine_observer_insert_points():\n    # Initialize and add custom Observer to the returned dictionary\n    pass\n```\n\n----------------------------------------\n\nTITLE: Tensor Operation with Stride in PyTorch\nDESCRIPTION: This code snippet shows a tensor operation involving multiple tensors with specific shapes, data types, and strides. It uses half-precision (f16) tensors and includes integer tensors for indexing or masking purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n((T([1024, 192], f16, stride=(47808, 1)), T([965, 192], f16), T([54711], i64), T([1024], i64), T([54711], i64), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Referencing torch.onnx.is_onnxrt_backend_supported function in RST documentation\nDESCRIPTION: RST documentation directive that references the torch.onnx.is_onnxrt_backend_supported function which likely checks if the ONNX runtime backend is supported in the current environment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo_onnxruntime_backend.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autofunction:: torch.onnx.is_onnxrt_backend_supported\n```\n\n----------------------------------------\n\nTITLE: Enumerating Specific Operator Uses with Parameters - PyTorch Python\nDESCRIPTION: This part documents assignments of operator names (e.g., aten.copy_.default) to tensor argument tuples, where each tuple provides concrete tensor shapes and relevant operator configurations. Each entry defines the expected PyTorch argument structure for a given operator, helpful for validating operator tracing or backend lowering. Python and PyTorch are prerequisites; parameters include tensor dims, types, and operator extras, outputs depend on how test infra consumes these assignments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([64, 1024, 7, 7], f16, stride=(1024, 1, 0, 0)), 49), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([64], i64),), {})\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([64, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([64, 64, 56, 56], i64)), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([64, 1024, 7, 7], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([64, 1000], f16), T([1000, 1024], f16)), {})\ncnt: 1, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 1024], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([64, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([64, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 96, 56, 56], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 160, 56, 56], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 192, 56, 56], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 224, 56, 56], f16), T([224], f16), T([224], f16), T([224], f16), T([224], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 13, ((T([64, 128, 28, 28], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows batch normalization operations at different stages of the network. These operations normalize activations using running statistics (mean and variance) and learnable parameters (scale and shift), improving training stability.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([96, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([96, 64, 32, 32], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([96, 128, 16, 16], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([96, 256, 8, 8], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([96, 512, 4, 4], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), False, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Enumerating ATen Operator Invocation Patterns - PyTorch Text\nDESCRIPTION: This snippet lists invocation schema patterns for various ATen operators in PyTorch, describing operand shapes, types, strides, and argument sets. It provides counts for how often each pattern is used, facilitating coverage analysis and systematization of computation paths in neural network pipelines. Required dependency is PyTorch (ATen), and this format expects symbolic tensor specifications (e.g., T([128, 1000], f16)) instead of actual data; no computation is performed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([128, 152, 14, 14], f16, stride=(178752, 196, 14, 1)), T([128, 152, 14, 14], f16)), {})\ncnt: 2, ((T([128, 304, 14, 14], f16, stride=(178752, 196, 14, 1)), T([128, 304, 14, 14], f16)), {})\ncnt: 1, ((T([128, 152, 14, 14], f16, stride=(119168, 196, 14, 1)), T([128, 152, 14, 14], f16)), {})\ncnt: 1, ((T([128, 304, 14, 14], f16, stride=(119168, 196, 14, 1)), T([128, 304, 14, 14], f16)), {})\ncnt: 1, ((T([128, 72, 28, 28], f16, stride=(338688, 784, 28, 1)), T([128, 72, 28, 28], f16)), {})\ncnt: 2, ((T([128, 144, 28, 28], f16, stride=(338688, 784, 28, 1)), T([128, 144, 28, 28], f16)), {})\ncnt: 1, ((T([128, 72, 28, 28], f16, stride=(225792, 784, 28, 1)), T([128, 72, 28, 28], f16)), {})\ncnt: 1, ((T([128, 144, 28, 28], f16, stride=(225792, 784, 28, 1)), T([128, 144, 28, 28], f16)), {})\ncnt: 1, ((T([128, 32, 56, 56], f16, stride=(602112, 3136, 56, 1)), T([128, 32, 56, 56], f16)), {})\ncnt: 2, ((T([128, 64, 56, 56], f16, stride=(602112, 3136, 56, 1)), T([128, 64, 56, 56], f16)), {})\ncnt: 1, ((T([128, 32, 56, 56], f16, stride=(401408, 3136, 56, 1)), T([128, 32, 56, 56], f16)), {})\ncnt: 1, ((T([128, 64, 56, 56], f16, stride=(401408, 3136, 56, 1)), T([128, 64, 56, 56], f16)), {})\nOperator: aten.add_.Tensor\ncnt: 41, ((T([], i64), 1), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 1024], f16), T([1024, 1000], f16, stride=(1, 1024))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([128, 64, 56, 56], f16), T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16)], 1), {})\ncnt: 1, (([T([128, 64, 56, 56], f16), T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16), T([128, 64, 56, 56], f16)], 1), {})\ncnt: 1, (([T([128, 144, 28, 28], f16), T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16)], 1), {})\ncnt: 1, (([T([128, 144, 28, 28], f16), T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), T([128, 144, 28, 28], f16)], 1), {})\ncnt: 1, (([T([128, 304, 14, 14], f16), T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16)], 1), {})\ncnt: 1, (([T([128, 304, 14, 14], f16), T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16), T([128, 304, 14, 14], f16)], 1), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([64, 32, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 64, 56, 56], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 64, 56, 56], f16), T([32, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 32, 56, 56], f16), T([64, 32, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 56, 56], f16), T([64, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 56, 56], f16), T([128, 192, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 128, 56, 56], f16), T([144, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 144, 28, 28], f16), T([144, 144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 144, 28, 28], f16), T([72, 144, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 72, 28, 28], f16), T([144, 72, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 288, 28, 28], f16), T([144, 288, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([144, 144, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 432, 28, 28], f16), T([288, 432, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 288, 28, 28], f16), T([304, 288, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 304, 14, 14], f16), T([304, 304, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([128, 304, 14, 14], f16), T([152, 304, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([128, 152, 14, 14], f16), T([304, 152, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 608, 14, 14], f16), T([304, 608, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 304, 14, 14], f16), T([304, 304, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 912, 14, 14], f16), T([480, 912, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 480, 14, 14], f16), T([960, 480, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 960, 7, 7], f16), T([1024, 960, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([1280, 1024, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 1280, 4, 4], f16), T([1024, 1280, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 1024, 4, 4], f16), T([128, 1280, 4, 4], f16), T([1024, 1280, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1280, 4, 4], f16), T([128, 1024, 7, 7], f16), T([1280, 1024, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 960, 7, 7], f16), T([1024, 960, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 960, 7, 7], f16), T([128, 480, 14, 14], f16), T([960, 480, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 480, 14, 14], f16), T([128, 912, 14, 14], f16), T([480, 912, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 152, 14, 14], f16), T([128, 304, 14, 14], f16), T([152, 304, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 304, 14, 14], f16), T([128, 152, 14, 14], f16), T([304, 152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 304, 14, 14], f16), T([128, 304, 14, 14], f16), T([304, 304, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 304, 14, 14], f16), T([128, 304, 14, 14], f16), T([304, 304, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 304, 14, 14], f16), T([128, 608, 14, 14], f16), T([304, 608, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 304, 14, 14], f16), T([128, 288, 28, 28], f16), T([304, 288, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 288, 28, 28], f16), T([128, 432, 28, 28], f16), T([288, 432, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 72, 28, 28], f16), T([128, 144, 28, 28], f16), T([72, 144, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 144, 28, 28], f16), T([128, 72, 28, 28], f16), T([144, 72, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144, 144, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), T([144, 144, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([128, 288, 28, 28], f16), T([144, 288, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([128, 128, 56, 56], f16), T([144, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 56, 56], f16), T([128, 192, 56, 56], f16), T([128, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 32, 56, 56], f16), T([128, 64, 56, 56], f16), T([32, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 64, 56, 56], f16), T([128, 32, 56, 56], f16), T([64, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), T([64, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 128, 56, 56], f16), T([64, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 32, 112, 112], f16), T([64, 32, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 1024, 4, 4], f16, stride=(1024, 1, 0, 0)), 16), {})\nOperator: aten.lift_fresh_copy.default\n\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations - Softmax and Log Softmax\nDESCRIPTION: Collection of softmax and log softmax operations with various tensor shapes and half precision (f16) data type. Includes both forward and backward operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._log_softmax.default((T([128, 1000], f16), 1, False))\naten._log_softmax_backward_data.default((T([128, 1000], f16), T([128, 1000], f16), 1, f16))\naten._softmax.default((T([1024, 4, 64, 144], f16), -1, False))\n```\n\n----------------------------------------\n\nTITLE: C++ Neural Network Components\nDESCRIPTION: Core C++ components for neural network development in PyTorch, including model architecture, functional operations, and optimization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cpp_index.rst#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ntorch::nn\ntorch::nn::functional\ntorch::optim\n```\n\n----------------------------------------\n\nTITLE: Linking test_jit against Flatbuffers Library in CMake\nDESCRIPTION: Links the `test_jit` executable against the `flatbuffers` library. This dependency is required because some JIT tests likely involve serialization or interaction with the FlatBuffers format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(\n  test_jit PRIVATE flatbuffers)\n```\n\n----------------------------------------\n\nTITLE: Defining a Long Python Function\nDESCRIPTION: This snippet defines a function named 'a_very_very_long' with placeholder comments indicating multiple lines of code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef a_very_very_long():\n    # Lots of lines!\n    # Lots of lines!\n    # Lots of lines!\n    # Lots of lines!\n    pass\n```\n\n----------------------------------------\n\nTITLE: Special Compiler Flags for CUDA Libraries\nDESCRIPTION: Sets special compiler flags for CUDA-related source files to suppress specific warnings when not using MSVC.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT MSVC)\n  # cudaProfilerInitialize must go away\n  set_source_files_properties(${TORCH_SRC_DIR}/csrc/cuda/shared/cudart.cpp PROPERTIES COMPILE_FLAGS \"-Wno-deprecated-declarations\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tensor Division Using aten.div in Python\nDESCRIPTION: The aten.div.Tensor operator performs element-wise division between a tensor and a scalar, supporting computation involving broadcasting. It is essential for normalizing tensor values or adjusting scales, requiring f16 tensors and a scalar divisor as input.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 50675456), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Convolution Operations in Python\nDESCRIPTION: This section describes the use of the 'aten.convolution.default' operator to apply convolution operations on tensors with varying strides, paddings, and kernel sizes. It supports neural network layers for feature extraction by altering the spatial dimensions of input tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 3, 224, 224], f16), T([64, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 64, 56, 56], f16), T([128, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([8, 128, 56, 56], f16), T([128, 4, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 32), {})\ncnt: 3, ((T([8, 128, 56, 56], f16), T([256, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization Operations\nDESCRIPTION: Forward and backward passes for layer normalization, operating on tensors of shape [8, 512, 768] with epsilon value of 1e-12.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_DistilBert_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\naten.native_layer_norm.default(T([8, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-12)\naten.native_layer_norm_backward.default(T([8, 512, 768], f16), T([8, 512, 768], f16), [768], T([8, 512, 1], f32), T([8, 512, 1], f32), T([768], f16), T([768], f16), [True, True, True])\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for PyTorch Mobile Lightweight Dispatch Tests\nDESCRIPTION: This CMake script sets up the build environment for PyTorch mobile lightweight dispatch tests. It defines paths, creates an executable, sets include directories, compile definitions, and links necessary libraries. It also includes an optional installation step.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/lightweight_dispatch/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.15)\n\nset(TORCH_ROOT ${CMAKE_CURRENT_LIST_DIR}/../../..)\nset(TEST_ROOT ${TORCH_ROOT}/test/mobile/lightweight_dispatch)\n\nadd_executable(test_codegen_unboxing\n  ${TEST_ROOT}/test_lightweight_dispatch.cpp\n  ${TEST_ROOT}/test_codegen_unboxing.cpp\n)\n\ntarget_include_directories(test_codegen_unboxing PRIVATE ${ATen_CPU_INCLUDE})\n\ntarget_compile_definitions(test_codegen_unboxing PRIVATE USE_GTEST)\n\nset(TEST_UNBOXING_DEPENDENCIES torch gtest)\n\ntarget_link_libraries(test_codegen_unboxing PRIVATE\n  ${TEST_UNBOXING_DEPENDENCIES}\n)\n\nif(INSTALL_TEST)\n  install(TARGETS test_codegen_unboxing DESTINATION bin)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Executing Unsafe View Operation in PyTorch\nDESCRIPTION: Reshapes tensors in an unsafe manner without copying data premises, intending to alter the view of tensors for efficient computations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"Operator: aten._unsafe_view.default\\ncnt: 72, ((T([2, 128, 16, 64], f16), [2, 128, 1024]), {})\\ncnt: 1, ((T([256, 256008], f16), [2, 128, 256008]), {})\\ncnt: 24, ((T([2, 16, 128, 64], f16), [32, 128, 64]), {})\\ncnt: 24, ((T([2, 128, 1024], f16), [256, 1024]), {})\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Registering Quantized Operator Implementation using TORCH_LIBRARY_IMPL in C++\nDESCRIPTION: Shows how to register the C++ function `quantized_xand` as the implementation for the \"xand\" operator specifically for the `QuantizedCPU` dispatch key. This registration is done using `TORCH_LIBRARY_IMPL` and `TORCH_FN`, linking the defined schema to the concrete kernel function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/README.md#2025-04-22_snippet_2\n\nLANGUAGE: c++\nCODE:\n```\nTORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {\n  m.impl(\"xand\", TORCH_FN(quantized_xand));\n}\n```\n\n----------------------------------------\n\nTITLE: Attaching Extra Files to TorchScript Exports in PyTorch C++\nDESCRIPTION: This snippet demonstrates how to attach additional metadata files to TorchScript model archives using C++. The SetExportModuleExtraFilesHook function is used to define a global hook that bundles extra files like metadata with every TorchScript model exported in the current process. It requires access to the PyTorch C++ API and environment variables for customizing metadata.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/large_scale_deployments.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nSetExportModuleExtraFilesHook([](const Module&) {\n    ExtraFilesMap files;\n    files[\"producer_info.json\"] = \"{\\\"user\\\": \\\"\" + getenv(\"USER\") + \"\\\"}\";\n    return files;\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Timer Server in PyTorch Distributed Elastic\nDESCRIPTION: Base class for implementing custom timer servers. Extend this class to create your own timer server implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nTimerServer\n```\n\n----------------------------------------\n\nTITLE: Tensor Manipulation Operations (PyTorch ATen C++)\nDESCRIPTION: These symbols correspond to core tensor manipulation routines in the PyTorch ATen library, including creation (e.g., empty_like), transformation (reshape, permute, slice), property inference (named inference), memory format queries, and low-level tensor operations such as computing strides, resizing, and result type computation. Functions may require specific dependencies such as at::Tensor, at::ArrayRef, at::Scalar, and other PyTorch core types. Inputs typically include Tensors, shape, and type parameters; outputs are new or modified Tensors. Some operations handle device/cuda dispatch or optional memory format/layout, and care must be taken for input parameter validity.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_17\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at10empty_likeERKNS_6TensorEN3c1013TensorOptionsESt8optionalINS3_12MemoryFormatEE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at14namedinference25propagate_names_for_addmmERKNS_6TensorES3_S3_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at16toAccumulateTypeEN3c1010ScalarTypeENS0_10DeviceTypeE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at16toAccumulateTypeEN3c1010ScalarTypeEb\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at18TensorIteratorBase20compute_common_dtypeEv\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6detail13computeStrideEN3c108ArrayRefINS1_6SymIntEEES4_S4_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZNK2at7Context16userEnabledCuDNNEv\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at14namedinference20compute_cat_outnamesERKSt6vectorISt17reference_wrapperIKNS_6TensorEESaIS5_EE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at14namedinference26propagate_names_for_expandERKNS_6TensorES3_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at18TensorIteratorBase14remove_operandEi\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at18get_overlap_statusEPKN3c1010TensorImplES3_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at29inferExpandGeometry_dimvectorEN3c108ArrayRefIlEES2_S2_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6detail9empty_cpuEN3c108ArrayRefIlEESt8optionalINS1_10ScalarTypeEES4_INS1_6LayoutEES4_INS1_6DeviceEES4_IbES4_INS1_12MemoryFormatEE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at9has_namesERKN3c108IListRefINS_6TensorEEE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c1015SmallVectorImplINS_6SymIntEEaSEOS2_.isra.0\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZNSt12_Vector_baseIlSaIlEED2Ev\n```\n\n----------------------------------------\n\nTITLE: TorchDynamo Full Graph Compilation Example\nDESCRIPTION: Demonstrates how to use fullgraph mode in torch.compile to ensure entire program compilation into a single graph and throw errors on graph breaks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting_old.rst#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef toy_example(a, b):\n   ...\n\ncompiled_toy = torch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)\n```\n\n----------------------------------------\n\nTITLE: Scalar and Tensor Addition in PyTorch Python\nDESCRIPTION: The 'aten.add' operator is utilized for element-wise addition either between a tensor and a scalar or two tensors of matching shape. The operands must be compatible in dimensions, and it requires PyTorch. Inputs include the tensors and/or scalar, producing a tensor of the accumulated results. Efficient data handling hinges on appropriate tensor alignment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.add.Scalar\ncnt: 1, ((T([100, 1], i64), 1), {})\nOperator: aten.add.Tensor\ncnt: 3, ((T([1, 16, 320, 320], f16), T([1, 16, 320, 320], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Class with Insufficient Docstring\nDESCRIPTION: A class with a method that has a placeholder docstring labeled as TODO.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LongWithShortDocstring:\n    \"\"\"TODO\"\"\"\n\n    def short1(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Analyzing Log Softmax Operations in PyTorch MobileNetV3\nDESCRIPTION: Statistics for the log softmax operations in forward and backward passes. These operators are used for the classification head of the network, operating on a tensor of shape [128, 1000] which represents logits for 1000 classes across a batch of 128 images.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing fill Kernel for CUDA in PyTorch\nDESCRIPTION: This CUDA kernel implements the fill operation for PyTorch tensors on GPU. It uses a FillFunctor and is specialized for Half precision data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_40\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native16fill_kernel_cudaERNS_14TensorIteratorERKN3c106ScalarE\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication and Adding in PyTorch\nDESCRIPTION: The 'aten.addmm.default' operation in PyTorch combines matrix multiplication with addition, a common neural network operation. Inputs comprise tensors for the bias, input, and weight matrices, where the result is calculated as bias plus the product of the input and weight matrices. It assumes half-precision tensor operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([96, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Existing PyTorch Installations\nDESCRIPTION: Commands to uninstall all existing PyTorch installations using conda and pip. This is necessary before reinstalling PyTorch in development mode.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda uninstall pytorch -y\nyes | pip uninstall torch\n```\n\n----------------------------------------\n\nTITLE: Configuring C10 Test Binaries in CMake for PyTorch\nDESCRIPTION: This CMake script identifies all C10 test files, builds executables for each, links against necessary dependencies (C10 library and testing frameworks), and registers them as tests. It includes conditional compilation options for MSVC and optional installation configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/test/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ Test binaries.\n\nfile(GLOB_RECURSE C10_ALL_TEST_FILES *_test.cpp)\nif(BUILD_TEST)\n  foreach(test_src ${C10_ALL_TEST_FILES})\n    get_filename_component(test_file_name ${test_src} NAME_WE)\n    set(test_name \"c10_${test_file_name}\")\n    add_executable(${test_name} \"${test_src}\")\n    if(NOT MSVC)\n      target_compile_options(${test_name} PRIVATE -Wno-unused-variable)\n    endif()\n    target_link_libraries(${test_name} ${C10_LIB} gmock gtest gtest_main)\n    add_test(NAME ${test_name} COMMAND $<TARGET_FILE:${test_name}>)\n    if(INSTALL_TEST)\n      set_target_properties(${test_name} PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n      install(TARGETS ${test_name} DESTINATION test)\n    endif()\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Convolution Backward Operation in PyTorch\nDESCRIPTION: This snippet shows the setup for a backward convolution operation, which computes gradients efficiently. It involves tensors with specific dimensions, kernel size, stride, padding, dilation, and group count that coordinate with the input and output gradient tensors. Outputs are gradients with respect to input and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pnasnet5large_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 48, ((T([16, 864, 11, 11], f16), T([16, 864, 11, 11], f16), T([864, 864, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 24, ((T([16, 216, 42, 42], f16), T([16, 216, 42, 42], f16), T([216, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Tensor Division Operations\nDESCRIPTION: This snippet shows the usage of the aten.div.Tensor operator for performing element-wise division on tensors with scalar values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 786432), {})\ncnt: 2, ((T([], f16), 2359296), {})\ncnt: 2, ((T([], f16), 2), {})\n```\n\n----------------------------------------\n\nTITLE: Executing Softmax Operation in PyTorch\nDESCRIPTION: Computes the softmax operation on a 3D tensor with dimensions [32, 128, 128] and data type f16, along the last dimension. This operation transforms input logits into probabilities.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"Operator: aten._softmax.default\\ncnt: 24, ((T([32, 128, 128], f16), -1, False), {})\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.relu_.default In-place ReLU Activation in PyTorch ATen\nDESCRIPTION: Documents observed calls to the in-place ReLU (Rectified Linear Unit) activation function (`aten.relu_.default`) in PyTorch ATen. It lists the different input tensor shapes (all using f16 data type) and their respective call counts during profiling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 64, 1, 1], f16),), {})\ncnt: 2, ((T([128, 128, 1, 1], f16),), {})\ncnt: 9, ((T([128, 384, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Conditional Flash Attention Build Option in CMake\nDESCRIPTION: Defines a CMake option USE_FLASH_ATTENTION using `cmake_dependent_option`. This option controls whether to build the flash_attention kernel. It defaults to ON if USE_CUDA or USE_ROCM is enabled and the compiler is not MSVC, otherwise it defaults to OFF.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_31\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_dependent_option(\n  USE_FLASH_ATTENTION\n  \"Whether to build the flash_attention kernel for scaled dot product attention.\\\n  Will be disabled if not supported by the platform\"\n  ON\n  \"USE_CUDA OR USE_ROCM;NOT MSVC\"\n  OFF)\n```\n\n----------------------------------------\n\nTITLE: Linking Dependencies to Lite Interpreter Test Executable in CMake\nDESCRIPTION: Links the `test_lite_interpreter_runtime` executable privately against required libraries: `torch` (PyTorch core), `gtest` (Google Test framework), and `backend_with_compiler_runtime` (the custom shared library defined earlier). All specified targets must exist.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lite_interpreter_runtime/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(test_lite_interpreter_runtime PRIVATE torch gtest backend_with_compiler_runtime)\n```\n\n----------------------------------------\n\nTITLE: Cloning Tensors with aten.clone\nDESCRIPTION: The clone operation duplicates tensors, ensuring they are distinct from the original. Dependent on PyTorch, it manages tensor duplication processes, producing a new tensor with the same shape, type, and data as the original input.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 3, ((T([8, 128], i64),), {})\ncnt: 1, ((T([8, 127], i64, stride=(128, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Max Pooling Backward Pass in PyTorch\nDESCRIPTION: Computes the gradients of max pooling operations, using the stored indices from the forward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\naten.max_pool2d_with_indices_backward.default((T([64, 512, 7, 7], f16), T([64, 512, 14, 14], f16), [2, 2], [2, 2], [0, 0], [1, 1], False, T([64, 512, 7, 7], i64)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.max_pool2d_with_indices_backward.default((T([64, 512, 14, 14], f16), T([64, 512, 28, 28], f16), [2, 2], [2, 2], [0, 0], [1, 1], False, T([64, 512, 14, 14], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Test Addition Function\nDESCRIPTION: Defines a CMake function c10d_add_test that standardizes the process of adding distributed tests. Handles test compilation, linking, and installation configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/c10d/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(c10d_add_test test_src)\n  set(prefix ARG)\n  set(noValues)\n  set(singleValues INSTALL_TEST)\n  set(multiValues LINK_LIBRARIES)\n\n  include(CMakeParseArguments)\n  cmake_parse_arguments(${prefix} \"${noValues}\" \"${singleValues}\" \"${multiValues}\" ${ARGN})\n\n  get_filename_component(test_name ${test_src} NAME_WE)\n  add_executable(${test_name} \"${test_src}\")\n  target_include_directories(${test_name} PRIVATE $<BUILD_INTERFACE:${TORCH_SRC_DIR}/csrc/distributed>)\n  target_link_libraries(${test_name} ${ARG_LINK_LIBRARIES})\n  add_test(NAME ${test_name} COMMAND $<TARGET_FILE:${test_name}>)\n\n  if(ARG_INSTALL_TEST)\n    set_target_properties(${test_name} PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n    install(TARGETS ${test_name} DESTINATION bin)\n  endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Disk Savings for Sparsified DLRM Models in Python\nDESCRIPTION: Python script to evaluate disk savings by sparsifying the DLRM model with various configurations. It generates sparsified model checkpoints and metadata.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/benchmarks/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython evaluate_disk_savings.py --model-path=<path_to_model_checkpoint> --sparsified-model-dump-path=<path_to_dump_sparsified_models>\n```\n\n----------------------------------------\n\nTITLE: Incorrect TorchScript Module Type Usage\nDESCRIPTION: This example shows incorrect usage by attempting to instantiate a TestModule within TorchScript scope, which will result in a RuntimeError since module constructors cannot be invoked within TorchScript.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nclass TestModule(torch.nn.Module):\n    def __init__(self, v):\n        super().__init__()\n        self.x = v\n\n    def forward(self, x: int):\n        return self.x + x\n\nclass MyModel:\n    def __init__(self, v: int):\n        self.val = v\n\n    @torch.jit.export\n    def doSomething(self, val: int) -> int:\n        # error: should not invoke the constructor of module type\n        myModel = TestModule(self.val)\n        return myModel(val)\n\n# m = torch.jit.script(MyModel(2)) # Results in below RuntimeError\n# RuntimeError: Could not get name of python class object\n```\n\n----------------------------------------\n\nTITLE: Backward Operations for Select and Slice in PyTorch\nDESCRIPTION: This snippet shows backward operations for select and slice operations on tensors with various shapes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.select_backward.default\ncnt: 1, ((T([64, 384], f16), [64, 197, 384], 1, 0), {})\n\nOperator: aten.slice_backward.default\ncnt: 1, ((T([64, 196, 384], f16), [64, 197, 384], 1, 1, 9223372036854775807, 1), {})\ncnt: 8, ((T([64, 197, 384], f16), [64, 197, 384], 0, 0, 9223372036854775807, 1), {})\ncnt: 2, ((T([64, 196, 384], f16, stride=(75648, 384, 1)), [64, 197, 384], 1, 1, 9223372036854775807, 1), {})\ncnt: 2, ((T([64, 1, 384], f16), [64, 1, 384], 2, 0, 9223372036854775807, 1), {})\ncnt: 4, ((T([64, 1, 384], f16), [64, 197, 384], 1, 0, 1, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Enabling SLEEF Vectorization on AArch64 (non-Android) in CMake\nDESCRIPTION: Checks if the build is *not* targeting Android and if the processor matches 'aarch64'. If true, it appends the `-DAT_BUILD_ARM_VEC256_WITH_SLEEF` preprocessor definition to CMAKE_CXX_FLAGS and adds the definition using `add_definitions`. This enables SLEEF library usage for vector math functions on generic AArch64 platforms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_38\n\nLANGUAGE: cmake\nCODE:\n```\n# Enable sleef on Arm(R) architecture by default (except Android)\nif((NOT ${CMAKE_SYSTEM_NAME} STREQUAL \"Android\")\n  AND(\"${CMAKE_SYSTEM_PROCESSOR}\" MATCHES \"aarch64\"))\n  string(APPEND CMAKE_CXX_FLAGS \" -DAT_BUILD_ARM_VEC256_WITH_SLEEF\")\n  add_definitions(-DAT_BUILD_ARM_VEC256_WITH_SLEEF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Automating Ecosystem Release Changes with Python Script\nDESCRIPTION: This command runs the release/apply-release-changes.py script to automate CI workflow updates and version setting for ecosystem libraries (like torchaudio and torchvision). The script expects the version as a parameter (e.g., '2.7') and must be run from the root of the corresponding repo. Dependencies: Python, the presence of the required script, and correct permissions. Input: version string; output: version.txt updates and workflow references adjusted for the new release. Limitations: the script must be maintained in sync with branching policies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython release/apply-release-changes.py [version]\n```\n\n----------------------------------------\n\nTITLE: Defining a Hidden Role in Sphinx RST Documentation\nDESCRIPTION: Creates a hidden role that can be used to hide sections in the Sphinx documentation. This allows for selective display of documentation content.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/_templates/classtemplate.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Defining Shifting Operations in TorchScript\nDESCRIPTION: Specifies the syntax for shift expressions in TorchScript, which can use << and >> operators with int or Tensor arguments. Shifting is defined in terms of division or multiplication by powers of 2.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nshift_expr ::=  a_expr | shift_expr ( '<<' | '>>' ) a_expr\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Operators for Benchmarking\nDESCRIPTION: This code shows examples of PyTorch operators (abs and acos) that will be benchmarked separately using the same input configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nC = torch.abs(A) # Shape of A [M, N]\nC = torch.acos(A) # Shape of A [M, N]\n```\n\n----------------------------------------\n\nTITLE: Dumping Fusion Pass Results in Python\nDESCRIPTION: This command runs a Python script with enhanced logging to dump fusion pass results, showing the graph before fusion and before compilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\nPYTORCH_JIT_LOG_LEVEL=graph_fuser python your_script.py &> log\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Convolution Operations\nDESCRIPTION: Various convolution operations with different tensor shapes and parameters. Operations involve tensors with varying dimensions including batch size 6 and different channel configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Super_SloMo_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n((T([6, 128, 88, 88], f16), T([6, 128, 88, 88], f16), T([128, 128, 3, 3], f16), [128], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Importing Python Extension Error Example\nDESCRIPTION: Demonstrates an ImportError that occurs when importing a C++ extension without first importing torch, showing undefined symbol errors from PyTorch/ATen.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/faq.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> import extension\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: /home/user/.pyenv/versions/3.7.1/lib/python3.7/site-packages/extension.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN2at19UndefinedTensorImpl10_singletonE\n```\n\n----------------------------------------\n\nTITLE: Specifying Tensor Inputs for BatchNorm and Related PyTorch Operators - Python\nDESCRIPTION: These snippets define input tuples for testing PyTorch operators (likely normalization, convolution, or batchnorm) using half-precision and single-precision tensor descriptions. Each tuple encodes shapes, dtypes, possible strides, and additional numeric or boolean parameters, targeting comprehensive operator validation. Expected inputs include multiple tensors and optional stride specifications, with outputs driving operator test coverage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 56, 14, 14], f16, stride=(21952, 196, 14, 1)), T([128, 56, 14, 14], f16), T([56], f16), T([56], f16), T([56], f16), T([56], f32), T([56], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 56, 14, 14], f16), T([128, 56, 14, 14], f16), T([56], f16), T([56], f16), T([56], f16), T([56], f32), T([56], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 112, 14, 14], f16), T([128, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f32), T([112], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 80, 14, 14], f16), T([128, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 3, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((T([128, 40, 14, 14], f16, stride=(15680, 196, 14, 1)), T([128, 40, 14, 14], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), True, 1e-05, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 5, ((T([128, 40, 14, 14], f16), T([128, 40, 14, 14], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Applying Threshold Backward Operation in PyTorch\nDESCRIPTION: This snippet shows the tensor shapes and parameters for the backward pass of a threshold operation in PyTorch. It includes input gradients, output, and the threshold value, likely used in ReLU or similar activations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([128, 192, 8, 8], f16, stride=(131072, 64, 8, 1)), T([128, 192, 8, 8], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files for test_jit Executable in CMake\nDESCRIPTION: Sets the `JIT_TEST_SRCS` CMake variable to a list containing numerous C++ source files located in the `JIT_TEST_ROOT` directory. These files implement various C++ tests for the PyTorch JIT compiler functionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\n# Build the cpp gtest binary containing the cpp-only tests.\nset(JIT_TEST_SRCS\n  ${JIT_TEST_ROOT}/test_add_if_then_else.cpp\n  ${JIT_TEST_ROOT}/test_alias_analysis.cpp\n  ${JIT_TEST_ROOT}/test_argument_spec.cpp\n  ${JIT_TEST_ROOT}/test_autodiff.cpp\n  ${JIT_TEST_ROOT}/test_load_upgraders.cpp\n  ${JIT_TEST_ROOT}/test_op_replacement.cpp\n  ${JIT_TEST_ROOT}/test_upgrader_utils.cpp\n  ${JIT_TEST_ROOT}/test_backend.cpp\n  ${JIT_TEST_ROOT}/test_class_import.cpp\n  ${JIT_TEST_ROOT}/test_class_parser.cpp\n  ${JIT_TEST_ROOT}/test_class_type.cpp\n  ${JIT_TEST_ROOT}/test_code_template.cpp\n  ${JIT_TEST_ROOT}/test_concat_opt.cpp\n  ${JIT_TEST_ROOT}/test_constant_pooling.cpp\n  ${JIT_TEST_ROOT}/test_cleanup_passes.cpp\n  ${JIT_TEST_ROOT}/test_create_autodiff_subgraphs.cpp\n  ${JIT_TEST_ROOT}/test_custom_class.cpp\n  ${JIT_TEST_ROOT}/test_custom_class_registrations.h\n  ${JIT_TEST_ROOT}/test_custom_class_registrations.cpp\n  ${JIT_TEST_ROOT}/test_custom_operators.cpp\n  ${JIT_TEST_ROOT}/test_dce.cpp\n  ${JIT_TEST_ROOT}/test_fuser.cpp\n  ${JIT_TEST_ROOT}/test_graph_executor.cpp\n  ${JIT_TEST_ROOT}/test_graph_iterator.cpp\n  ${JIT_TEST_ROOT}/test_cs_debug_info_serialization.cpp\n  ${JIT_TEST_ROOT}/test_inliner.cpp\n  ${JIT_TEST_ROOT}/test_interface.cpp\n  ${JIT_TEST_ROOT}/test_interpreter.cpp\n  ${JIT_TEST_ROOT}/test_ir.cpp\n  ${JIT_TEST_ROOT}/test_irparser.cpp\n  ${JIT_TEST_ROOT}/test_jit_type.cpp\n  ${JIT_TEST_ROOT}/test_lite_interpreter.cpp\n  ${JIT_TEST_ROOT}/test_lite_interpreter_direct.cpp\n  ${JIT_TEST_ROOT}/test_lite_trainer.cpp\n  ${JIT_TEST_ROOT}/test_memory_dag.cpp\n  ${JIT_TEST_ROOT}/test_misc.cpp\n  ${JIT_TEST_ROOT}/test_mobile_type_parser.cpp\n  ${JIT_TEST_ROOT}/test_module_api.cpp\n  ${JIT_TEST_ROOT}/test_peephole_optimize.cpp\n  ${JIT_TEST_ROOT}/test_qualified_name.cpp\n  ${JIT_TEST_ROOT}/test_save_load.cpp\n  ${JIT_TEST_ROOT}/test_schema_info.cpp\n  ${JIT_TEST_ROOT}/test_schema_matching.cpp\n  ${JIT_TEST_ROOT}/test_stack_opt.cpp\n  ${JIT_TEST_ROOT}/test_subgraph_matcher.cpp\n  ${JIT_TEST_ROOT}/test_subgraph_rewriter.cpp\n  ${JIT_TEST_ROOT}/test_subgraph_utils.cpp\n  ${JIT_TEST_ROOT}/test_union.cpp\n  ${JIT_TEST_ROOT}/test_utils.cpp\n  ${JIT_TEST_ROOT}/test_script_profile.cpp\n  ${JIT_TEST_ROOT}/test_shape_analysis.cpp\n  ${JIT_TEST_ROOT}/test_jit_logging_levels.cpp\n  ${JIT_TEST_ROOT}/test_file_format.cpp\n  ${JIT_TEST_ROOT}/test_flatbuffer.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Tensors with aten.add in PyTorch\nDESCRIPTION: Executes element-wise addition of tensors with options for scalar addition. It aids in tensor arithmetic using PyTorch, critical for operations in neural networks and numerical calculations. Requires tensor inputs and optionally a scalar for addition, outputting the resultant tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\naten.add.Tensor(T([128], i64), 1)\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.add.Tensor(T([64, 128], i32), 0)\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.add.Tensor(T([64, 128], i64), 1)\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.add.Tensor(T([64, 128, 256], f16), T([64, 128, 256], f16))\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.add.Tensor(T([64, 4, 128, 128], f16), T([64, 1, 128, 128], f16))\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.add.Tensor(T([10000, 256], f16), T([10000, 256], f16))\n```\n\n----------------------------------------\n\nTITLE: Backward Pass for Tanh Activation in PyTorch\nDESCRIPTION: Computes the gradient for the backward pass of the tanh function, integrating into broader backpropagation processes. Requires input tensors and their associated gradients. Outputs are tensors prepared for further gradient calculations, contributing to efficient backpropagation and training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.tanh_backward.default\ncnt: 1, ((T([16, 3, 128, 128], f16, stride=(0, 0, 0, 0)), T([16, 3, 128, 128], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Documentation Structure in RST\nDESCRIPTION: ReStructuredText format documentation header defining the functorch.experimental module section with autosummary directive for function transforms documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/experimental.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nfunctorch.experimental\n======================\n\n.. currentmodule:: functorch.experimental\n\nExperimental Function Transforms\n--------------------------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n```\n\n----------------------------------------\n\nTITLE: Linking Torch Library to Backend Compiler Runtime in CMake\nDESCRIPTION: Links the `backend_with_compiler_runtime` shared library privately against the `torch` library target. This ensures that symbols required by the backend library from the core Torch library are resolved during linking. Both `backend_with_compiler_runtime` and `torch` targets must be defined.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lite_interpreter_runtime/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(backend_with_compiler_runtime PRIVATE torch)\n```\n\n----------------------------------------\n\nTITLE: Configuring IPython to Hide Error Tracebacks\nDESCRIPTION: Sets up IPython to hide error tracebacks, showing only exception messages. This is used for cleaner error output in the examples.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/typing.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Hide traceback of Error\nimport functools\nipython = get_ipython()\ndef showtraceback(self, exc_tuple=None, filename=None, tb_offset=None,\n                  exception_only=False, running_compiled_code=False):\n    try:\n        try:\n            etype, value, tb = self._get_exc_info(exc_tuple)\n        except ValueError:\n            print('No traceback available to show.', file=sys.stderr)\n            return\n\n        # Hide traceback\n        stb = self.InteractiveTB.get_exception_only(etype, value)\n\n        self._showtraceback(etype, value, stb)\n\n    except KeyboardInterrupt:\n        print('\\n' + self.get_exception_only(), file=sys.stderr)\nipython.showtraceback = functools.partial(showtraceback, ipython)\n```\n\n----------------------------------------\n\nTITLE: Using InferenceMode for Inference Workloads in C++\nDESCRIPTION: Example of using c10::InferenceMode guard to optimize performance for inference workload in C++. The guard wraps the entire workflow from model loading to inference execution and post-processing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/inference_mode.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nc10::InferenceMode guard;\nmodel.load_jit(saved_model);\nauto inputs = preprocess_tensors(data);\nauto out = model.forward(inputs);\nauto outputs = postprocess_tensors(out);\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operation in PyTorch Neural Network\nDESCRIPTION: Summary of matrix multiplication operation (aten.addmm.default) in the model, showing count, tensor shapes and strides. This likely represents a fully connected layer processing the final features into class probabilities.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([32, 2240], f16), T([2240, 1000], f16, stride=(1, 2240))), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Threshold Backward Operation in PyTorch\nDESCRIPTION: This snippet shows the usage of threshold_backward.default operator with various tensor shapes and counts of each unique configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swsl_resnext101_32x16d_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), 0), {})\ncnt: 5, ((T([32, 4096, 7, 7], f16), T([32, 4096, 7, 7], f16), 0), {})\ncnt: 1, ((T([32, 4096, 14, 14], f16), T([32, 4096, 14, 14], f16), 0), {})\ncnt: 23, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Shape Patterns and Batch Normalization\nDESCRIPTION: Log entries showing tensor shape patterns used in batch normalization operations, including input tensors, running statistics, and parameters. Shows repeated patterns with varying channel depths (64 to 2048) and spatial dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet50_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncnt: 5, ((T([32, 512, 7, 7], f16), T([32, 512, 7, 7], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Enable Tunable Operation in C++\nDESCRIPTION: Demonstrates enabling the tunable operation feature in PyTorch using C++ syntax. This requires including the ATen CUDA tunable header and accessing the tuning context.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#2025-04-22_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\n#include <ATen/cuda/tunable/Tunable.h>\n\nat::cuda::tunable::getTuningContext()->EnableTunableOp(true);\n```\n\n----------------------------------------\n\nTITLE: Batch Matrix Multiplication Using BMM in PyTorch\nDESCRIPTION: The aten.bmm.default operator is highlighted, showcasing operations on 3D tensors [24, 512, 64] and [24, 64, 512] with float16 precision. It allows efficient batch-wise matrix multiplications, key for batched data processes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 24, ((T([24, 512, 64], f16), T([24, 64, 512], f16, stride=(32768, 1, 64))), {})\n```\n\n----------------------------------------\n\nTITLE: Collecting New Mixed MM Training Data\nDESCRIPTION: Executes benchmarks to collect new training data for generating mixed matrix multiplication heuristics for custom GPU configurations\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mixed_mm/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbash generate_heuristic.sh collect\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Statistics\nDESCRIPTION: Comprehensive listing of PyTorch operators showing call counts and tensor configurations. Includes operations like softmax, matrix multiplication, embedding, and layer normalization commonly used in transformer architectures. Each entry shows operator name, call count, input tensor shapes, dtypes and additional parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Albert_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 12, ((T([8, 12, 512, 512], f16), -1, False), {})\n\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([8, 12, 512, 512], f16), T([8, 12, 512, 512], f16), -1, f16), {})\n\nOperator: aten._to_copy.default\ncnt: 1, ((T([8, 1, 1, 512], f32),), {'dtype': f16})\n\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([8, 12, 512, 64], f16), [96, 512, 64]), {})\ncnt: 12, ((T([8, 12, 64, 512], f16), [96, 64, 512]), {})\ncnt: 12, ((T([96, 512, 512], f16), [8, 12, 512, 512]), {})\ncnt: 12, ((T([96, 512, 64], f16), [8, 12, 512, 64]), {})\ncnt: 36, ((T([8, 512, 12, 64], f16), [8, 512, 768]), {})\ncnt: 12, ((T([8, 512, 768], f16), [4096, 768]), {})\n```\n\n----------------------------------------\n\nTITLE: Scalar Division Operations in PyTorch\nDESCRIPTION: Profiling data for tensor-scalar division operations showing count, tensor shapes, and scalar divisors. These operations divide tensors by scalar values like 49, 196, 784, etc., likely for normalization or averaging.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([32, 1280, 7, 7], f16, stride=(1280, 1, 0, 0)), 49), {})\ncnt: 4, ((T([32, 1152, 7, 7], f16, stride=(1152, 1, 0, 0)), 49), {})\ncnt: 1, ((T([32, 672, 7, 7], f16, stride=(672, 1, 0, 0)), 49), {})\ncnt: 2, ((T([32, 672, 14, 14], f16, stride=(672, 1, 0, 0)), 196), {})\ncnt: 3, ((T([32, 480, 14, 14], f16, stride=(480, 1, 0, 0)), 196), {})\ncnt: 1, ((T([32, 240, 14, 14], f16, stride=(240, 1, 0, 0)), 196), {})\ncnt: 1, ((T([32, 240, 28, 28], f16, stride=(240, 1, 0, 0)), 784), {})\ncnt: 1, ((T([32, 144, 28, 28], f16, stride=(144, 1, 0, 0)), 784), {})\ncnt: 1, ((T([32, 144, 56, 56], f16, stride=(144, 1, 0, 0)), 3136), {})\ncnt: 1, ((T([32, 96, 56, 56], f16, stride=(96, 1, 0, 0)), 3136), {})\ncnt: 1, ((T([32, 32, 112, 112], f16, stride=(32, 1, 0, 0)), 12544), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA Kernels for PyTorch Operations\nDESCRIPTION: This snippet shows CUDA kernel implementations for PyTorch operations, including scaled dot product attention and efficient attention forward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_30\n\nLANGUAGE: CUDA\nCODE:\n```\n_Z171__device_stub__Z40fmha_cutlassF_f16_aligned_64x128_rf_sm80N22PyTorchMemEffAttention15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb1ELi64ELi128ELi128ELb1ELb1EE6ParamsERN22PyTorchMemEffAttention15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb1ELi64ELi128ELi128ELb1ELb1EE6ParamsE\n_Z40fmha_cutlassF_f16_aligned_64x128_rf_sm80N22PyTorchMemEffAttention15AttentionKernelIN7cutlass6half_tENS1_4arch4Sm80ELb1ELi64ELi128ELi128ELb1ELb1EE6ParamsE\n_ZN2at12_GLOBAL__N_112_GLOBAL__N_142wrapper_CUDA___efficient_attention_forwardERKNS_6TensorES4_S4_RKSt8optionalIS2_ES8_S8_S5_IlES9_dlbS5_IdES8_S8_\n_ZN2at12_GLOBAL__N_112_GLOBAL__N_153wrapper_CUDA___scaled_dot_product_efficient_attentionERKNS_6TensorES4_S4_RKSt8optionalIS2_EbdbS5_IdE\n```\n\n----------------------------------------\n\nTITLE: Analyzing Convolution Operations in PyTorch\nDESCRIPTION: Records of aten.convolution.default operator calls with various tensor shapes and convolution parameters. These operations represent different convolutional layers in a neural network with different filter sizes, strides, and padding configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([64, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([128, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})\ncnt: 1, ((T([32, 64, 1, 1], f16), T([32, 64, 1, 1], f16), T([32], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 32, 1, 1], f16), T([128, 32, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([32, 64, 56, 56], f16), T([256, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 256, 56, 56], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 128, 56, 56], f16), T([256, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})\ncnt: 1, ((T([32, 128, 1, 1], f16), T([64, 128, 1, 1], f16), T([64], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 64, 1, 1], f16), T([256, 64, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 128, 28, 28], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), T([512, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), T([512, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})\ncnt: 1, ((T([32, 256, 1, 1], f16), T([128, 256, 1, 1], f16), T([128], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 128, 1, 1], f16), T([512, 128, 1, 1], f16), T([512], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 256, 14, 14], f16), T([1024, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), T([1024, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 1024, 14, 14], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), T([1024, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 2), {})\ncnt: 1, ((T([32, 512, 1, 1], f16), T([256, 512, 1, 1], f16), T([256], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 256, 1, 1], f16), T([1024, 256, 1, 1], f16), T([1024], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 512, 7, 7], f16), T([2048, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 1024, 7, 7], f16), T([2048, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Exponentiating Tensors with Scalar using aten.pow.Tensor_Scalar - Python\nDESCRIPTION: Shows several uses of the aten.pow.Tensor_Scalar operator to raise tensors to a scalar power, supporting both integer and floating-point exponents over multi-dimensional tensors. Relies on tensor broadcasting and matching shapes, using half-precision (f16) data. Requires PyTorch. Scalar parameter indicates the exponent.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.pow.Tensor_Scalar\ncnt: 12, ((T([2, 1024, 3072], f16), 3.0), {})\ncnt: 1, ((T([2, 1024, 768], f16), 3.0), {})\ncnt: 1, ((T([2, 1024, 768], f16), 2.0), {})\ncnt: 12, ((T([2, 1024, 3072], f16), 2.0), {})\n```\n\n----------------------------------------\n\nTITLE: Running All Supported Benchmarks\nDESCRIPTION: Executes all available benchmark tests in the suite.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m benchmark_all_test\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Test Library\nDESCRIPTION: Configures and builds the CUDA test library when CUDA support is enabled. Links against torch_cuda and includes necessary distributed headers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/c10d/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_CUDA)\n  add_library(c10d_cuda_test CUDATest.cu)\n  target_include_directories(c10d_cuda_test PRIVATE $<BUILD_INTERFACE:${TORCH_SRC_DIR}/csrc/distributed>)\n  target_link_libraries(c10d_cuda_test torch_cuda)\n  add_dependencies(c10d_cuda_test torch_cuda)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations in PyTorch for Feature Processing\nDESCRIPTION: This snippet shows addmm operations that perform matrix multiplications with bias addition in a PyTorch model. These operations are typically used in fully connected layers with half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1024], f16), T([0, 12544], f16), T([12544, 1024], f16, stride=(1, 12544))), {})\ncnt: 1, ((T([1024], f16), T([0, 1024], f16), T([1024, 1024], f16, stride=(1, 1024))), {})\ncnt: 1, ((T([91], f16), T([0, 1024], f16), T([1024, 91], f16, stride=(1, 1024))), {})\ncnt: 1, ((T([364], f16), T([0, 1024], f16), T([1024, 364], f16, stride=(1, 1024))), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations with PyTorch Operators\nDESCRIPTION: Record of PyTorch operator calls including tensor shapes, data types, and call counts. Shows patterns of convolutions, pooling, tensor additions and concatenations with fp16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pnasnet5large_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Log softmax operations\naten._log_softmax.default((T([16, 1000], f16), 1, False))\naten._log_softmax_backward_data.default((T([16, 1000], f16), T([16, 1000], f16), 1, f16))\n\n# Tensor additions with various shapes\naten.add.Tensor((T([], i64), 1))\naten.add.Tensor((T([16, 54, 83, 83], f16), T([16, 54, 83, 83], f16)))\n\n# Convolution operations\naten.convolution.default((T([16, 3, 331, 331], f16), T([96, 3, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1))\n\n# Average pooling operations\naten.avg_pool2d.default((T([16, 96, 165, 165], f16), [1, 1], [2, 2], [0, 0], False, False))\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Tensor Copy Operations\nDESCRIPTION: This snippet demonstrates the usage of the aten.copy_.default operator for copying tensor data between tensors with various shapes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 2, ((T([3, 3, 512, 512], f16), T([3, 3, 512, 512], f16)), {})\ncnt: 1, ((T([3, 1, 512, 512], f16), T([3, 1, 512, 512], f16)), {})\ncnt: 1, ((T([3, 4, 512, 512], f16), T([3, 4, 512, 512], f16)), {})\ncnt: 1, ((T([256, 128, 3, 3], f16), T([256, 128, 3, 3], f16, stride=(1152, 1, 384, 128))), {})\ncnt: 1, ((T([128, 64, 3, 3], f16), T([128, 64, 3, 3], f16, stride=(576, 1, 192, 64))), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Batch Normalization Operations\nDESCRIPTION: This snippet demonstrates the usage of the aten.native_batch_norm.default operator for performing batch normalization on tensors with various shapes and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 5, ((T([3, 64, 512, 512], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([3, 128, 256, 256], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of aten.bmm.default operator for batch matrix multiplication with various tensor shapes and strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/attention_is_all_you_need_pytorch_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.bmm.default\ncnt: 6, ((T([2048, 33, 64], f16), T([2048, 64, 33], f16)), {})\ncnt: 6, ((T([2048, 33, 33], f16), T([2048, 33, 64], f16)), {})\ncnt: 6, ((T([2048, 31, 64], f16), T([2048, 64, 31], f16)), {})\ncnt: 6, ((T([2048, 31, 31], f16), T([2048, 31, 64], f16)), {})\ncnt: 6, ((T([2048, 31, 64], f16), T([2048, 64, 33], f16)), {})\ncnt: 6, ((T([2048, 31, 33], f16), T([2048, 33, 64], f16)), {})\ncnt: 6, ((T([2048, 33, 31], f16, stride=(1023, 1, 33)), T([2048, 31, 64], f16)), {})\ncnt: 6, ((T([2048, 31, 64], f16), T([2048, 64, 33], f16, stride=(2112, 1, 64))), {})\ncnt: 6, ((T([2048, 64, 31], f16, stride=(1984, 1, 64)), T([2048, 31, 33], f16)), {})\ncnt: 6, ((T([2048, 31, 33], f16), T([2048, 33, 64], f16, stride=(2112, 1, 33))), {})\ncnt: 6, ((T([2048, 31, 31], f16, stride=(961, 1, 31)), T([2048, 31, 64], f16)), {})\ncnt: 6, ((T([2048, 31, 64], f16), T([2048, 64, 31], f16, stride=(1984, 1, 64))), {})\ncnt: 6, ((T([2048, 64, 31], f16, stride=(1984, 1, 64)), T([2048, 31, 31], f16)), {})\ncnt: 6, ((T([2048, 31, 31], f16), T([2048, 31, 64], f16, stride=(1984, 1, 31))), {})\ncnt: 6, ((T([2048, 33, 33], f16, stride=(1089, 1, 33)), T([2048, 33, 64], f16)), {})\ncnt: 6, ((T([2048, 33, 64], f16), T([2048, 64, 33], f16, stride=(2112, 1, 64))), {})\ncnt: 6, ((T([2048, 64, 33], f16, stride=(2112, 1, 64)), T([2048, 33, 33], f16)), {})\ncnt: 6, ((T([2048, 33, 33], f16), T([2048, 33, 64], f16, stride=(2112, 1, 33))), {})\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Benchmark C++ Extension\nDESCRIPTION: Sets up the required C++ extension for the PyTorch benchmark suite. This must be run before other benchmarks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd pt_extension\npython setup.py install\n```\n\n----------------------------------------\n\nTITLE: Defining Lite Interpreter Runtime Test Executable in CMake\nDESCRIPTION: Defines an executable target named `test_lite_interpreter_runtime`. The source files for this executable are specified by the `LITE_INTERPRETER_RUNTIME_TEST_DIR` variable, which should contain a list of C++ source files. Requires the `LITE_INTERPRETER_RUNTIME_TEST_DIR` variable to be correctly populated.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lite_interpreter_runtime/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(\n  test_lite_interpreter_runtime\n  ${LITE_INTERPRETER_RUNTIME_TEST_DIR})\n```\n\n----------------------------------------\n\nTITLE: FileCheck Builder API Example\nDESCRIPTION: Shows how to use the FileCheck builder API to programmatically create test assertions instead of using string annotations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/HowToWriteTestsUsingFileCheck.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nFileCheck().check(\"aten::mul\")     \\\n           .check_not(\"aten::mul\") \\\n           .check(\"return\")        \\\n           .run(optimized)\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.native_batch_norm_backward.default\nDESCRIPTION: Logs the backward pass for native batch normalization (`aten.native_batch_norm_backward.default`). Takes the output gradient and input tensor (both 4D float16) along with weight, running mean, running variance (all float16), and saved mean/inverse standard deviation (float32) as inputs. Parameters include training mode (`True`), epsilon (`1e-05`), and flags indicating which gradients to compute (`[True, True, True]`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 21, ((T([128, 368, 7, 7], f16), T([128, 368, 7, 7], f16), T([368], f16), T([368], f16), T([368], f16), T([368], f32), T([368], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 368, 14, 14], f16), T([128, 368, 14, 14], f16), T([368], f16), T([368], f16), T([368], f16), T([368], f32), T([368], f32), True, 1e-05, [True, True, True]), {})\ncnt: 12, ((T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16), T([152], f16), T([152], f16), T([152], f16), T([152], f32), T([152], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 152, 28, 28], f16), T([128, 152, 28, 28], f16), T([152], f16), T([152], f16), T([152], f16), T([152], f32), T([152], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 56, 28, 28], f16), T([128, 56, 28, 28], f16), T([56], f16), T([56], f16), T([56], f16), T([56], f32), T([56], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 56, 56, 56], f16), T([128, 56, 56, 56], f16), T([56], f16), T([56], f16), T([56], f16), T([56], f32), T([56], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 24, 112, 112], f16), T([128, 24, 112, 112], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\n\n```\n\n----------------------------------------\n\nTITLE: Building and Running TensorExpr C++ Tests\nDESCRIPTION: Provides the shell commands to first build the specific TensorExpr test binary using Ninja and then run the compiled tests. The run command includes an example of using `gtest_filter` to execute only tests matching a specific pattern.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/tensorexpr/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# (re)build the test binary\nninja build/bin/test_tensorexpr\n# run\nbuild/bin/test_tensorexpr --gtest_filter='glob_style_filter*'\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Statistics for Neural Network Execution\nDESCRIPTION: This data shows the frequency and parameter shapes for various PyTorch operators in a neural network execution. The statistics include tensor dimensions, data types (primarily float16/f16), and operation counts, revealing the computational graph of what appears to be a ResNet-like architecture.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([32, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([32, 1000], f16), T([32, 1000], f16), 1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 3, ((T([100352, 512], f16), [32, 56, 56, 512]), {})\ncnt: 3, ((T([100352, 128], f16), [32, 56, 56, 128]), {})\ncnt: 3, ((T([25088, 1024], f16), [32, 28, 28, 1024]), {})\ncnt: 3, ((T([25088, 256], f16), [32, 28, 28, 256]), {})\ncnt: 27, ((T([6272, 2048], f16), [32, 14, 14, 2048]), {})\ncnt: 27, ((T([6272, 512], f16), [32, 14, 14, 512]), {})\ncnt: 3, ((T([1568, 4096], f16), [32, 7, 7, 4096]), {})\ncnt: 3, ((T([1568, 1024], f16), [32, 7, 7, 1024]), {})\ncnt: 3, ((T([32, 7, 7, 1024], f16), [1568, 1024]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing aten.threshold_backward.default Operator Usage in PyTorch\nDESCRIPTION: This code snippet represents the usage patterns of the aten.threshold_backward.default operator in a PyTorch project. It shows the tensor shapes, data types, and occurrence counts for different calls to this operator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([64, 1536, 7, 7], f16), T([64, 1536, 7, 7], f16), 0), {})\ncnt: 2, ((T([64, 120, 56, 56], f16), T([64, 120, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 192, 56, 56], f16), T([64, 192, 56, 56], f16), 0), {})\ncnt: 1, ((T([64, 192, 112, 112], f16), T([64, 192, 112, 112], f16), 0), {})\ncnt: 2, ((T([64, 32, 112, 112], f16), T([64, 32, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring MPI Tests and Example\nDESCRIPTION: Sets up MPI-specific tests and configures an allreduce example for Linux systems with Gloo support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/c10d/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_MPI AND USE_C10D_MPI)\n  add_definitions(-DMPIEXEC=${MPIEXEC})\n  c10d_add_test(ProcessGroupMPITest.cpp LINK_LIBRARIES torch_cpu MPI::MPI_CXX INSTALL_TEST ${INSTALL_TEST})\nendif()\n\nif(LINUX AND USE_GLOO AND USE_C10D_GLOO)\n  add_executable(example_allreduce example/allreduce.cpp)\n  target_include_directories(example_allreduce PRIVATE $<BUILD_INTERFACE:${TORCH_SRC_DIR}/csrc/distributed>)\n  target_link_libraries(example_allreduce torch_cpu)\n  if(USE_CUDA)\n    target_link_libraries(example_allreduce torch_cuda)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running oneDNN Graph Tests with PyTest\nDESCRIPTION: Command to run the test suite for the oneDNN Graph JIT fuser implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/onednn/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npytest test/test_jit_llga_fuser.py\n```\n\n----------------------------------------\n\nTITLE: Relative Benchmark for jacfwd vs jacrev (Taller Matrix) - Python\nDESCRIPTION: Runs get_perf to compare jacfwd versus jacrev timings in the tall matrix scenario. Requires previous benchmarks and get_perf. Prints percentage improvement between methods.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nget_perf(jacfwd_timing, \"jacfwd\", jacrev_timing, \"jacrev\", );\n```\n\n----------------------------------------\n\nTITLE: Defining a Protected Class\nDESCRIPTION: A protected class (indicated by the underscore prefix) with a placeholder docstring.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass _Protected:\n    \"\"\"TODO\"\"\"\n\n    def short1(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: SymInt Operations in C++\nDESCRIPTION: Function declarations for SymInt class methods, implementing symbolic integer operations and comparisons.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c106SymIntdVERKS0_\n_ZN3c106SymIntmLERKS0_\n_ZN3c106SymIntpLERKS0_\n_ZNK3c106SymIntdvERKS0_\n_ZNK3c106SymIntmlERKS0_\n_ZNK3c106SymIntplERKS0_\n_ZNK3c106SymIntrmERKS0_\n_ZNK3c106SymInt12maybe_as_intEv.isra.0\n_ZNK3c106SymInt6sym_eqERKS0_\n_ZNK3c106SymInt6sym_geERKS0_\n_ZNK3c106SymInt6sym_ltERKS0_\n_ZNK3c106SymInt6sym_neERKS0_\n_ZNK3c106SymInt9guard_intEPKcl\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Patterns - Tensor Operations\nDESCRIPTION: Collection of PyTorch tensor operations including log_softmax, addition, convolution and their corresponding backward passes. Shows detailed tensor shapes, strides and hyperparameters for each operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example convolution operation\naten.convolution.default((T([32, 3, 224, 224], f16), T([64, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1))\n\n# Example tensor addition\naten.add.Tensor((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)))\n\n# Example backward pass\naten.convolution_backward.default((T([32, 2048, 1, 1], f16), T([32, 128, 1, 1], f16), T([2048, 128, 1, 1], f16), [2048], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]))\n```\n\n----------------------------------------\n\nTITLE: Using ATen _log_softmax_backward_data Operator in PyTorch\nDESCRIPTION: This snippet shows the `aten._log_softmax_backward_data` operator's application, used to compute the gradient of the log-softmax operator. Inputs include two tensors of size [64, 1000] with f16 data type, dimension 1, and output in f16. The main dependencies are input tensors and correct dimensional parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Representing With Statements in TorchScript IR\nDESCRIPTION: Explains how with-statements are represented in TorchScript IR using prim::Enter and prim::Exit nodes. It also shows a temporary block-based representation used during the exit_transform pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nwith c as increment:\n  y = x + increment\n```\n\nLANGUAGE: python\nCODE:\n```\n%2 : int = prim::Constant[value=1]()\n%increment.1 : int = prim::Enter(%c.1)\n%y.1 : Tensor = aten::add(%x.1, %increment.1, %2)\n%11 : Tensor = prim::Exit(%c.1)\n```\n\nLANGUAGE: python\nCODE:\n```\n%increment.1 : int = prim::Enter(%c.1)\n= prim::With()\n  block0():\n    %y.1 : Tensor = aten::add(%x.1, %increment.1, %4)\n    -> ()\n  block1():\n    %11 : Tensor = prim::Exit(%c.1)\n    -> ()\n```\n\n----------------------------------------\n\nTITLE: Creating Sparse COO Tensor in PyTorch\nDESCRIPTION: This code snippet demonstrates the creation of a sparse COO tensor using PyTorch's _sparse_coo_tensor_with_dims_and_tensors function. It specifies the tensor dimensions, indices, and values, along with additional parameters like data type and device.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n((1, 1, [965, 192], T([1, 54765], i64), T([54765, 192], f16)), {'dtype': f16, 'layout': torch.sparse_coo, 'device': 'cuda', 'pin_memory': None})\n```\n\n----------------------------------------\n\nTITLE: Displaying PyTorch Performance Benchmark Results in Markdown Table\nDESCRIPTION: A markdown table displaying performance metrics for different PyTorch configurations with batch size 1 and compilation disabled. Metrics include warmup latency, average latency, throughput, and GPU utilization across various worker configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_1_false.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## Batch Size 1 Compile false\n\n| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |\n| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |\n| original | 5.497 +/- 0.681 | 0.383 +/- 0.007 | 168.818 +/- 2.169 | 12.574 +/- 2.205 |\n| h2d_d2h_threads | 4.039 +/- 1.175 | 0.593 +/- 0.246 | 127.449 +/- 34.358 | 11.445 +/- 1.925 |\n| 2_predict_workers | 3.766 +/- 0.610 | 0.369 +/- 0.010 | 186.485 +/- 2.657 | 11.857 +/- 2.591 |\n| 3_predict_workers | 3.460 +/- 0.063 | 0.407 +/- 0.033 | 168.863 +/- 9.227 | 12.427 +/- 1.430 |\n| 4_predict_workers | 4.184 +/- 0.636 | 0.692 +/- 0.182 | 110.231 +/- 31.420 | 8.490 +/- 1.826 |\n```\n\n----------------------------------------\n\nTITLE: Loop-Level Analogy Example in PyTorch\nDESCRIPTION: Simple loop example showing basic iteration over batch channels with corresponding values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor channel in range(batch.size):\n    result[channel] = channel + 1000\n```\n\n----------------------------------------\n\nTITLE: Log Softmax Backward Data Operation in PyTorch\nDESCRIPTION: The aten._log_softmax_backward_data.default operator is used for gradient computation of the logarithmic softmax function. It processes inputs and gradients, both shaped [1, 512] in float16 precision, and is necessary for backward passes in training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 2, ((T([1, 512], f16), T([1, 512], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Dividing Tensors by Scalars in PyTorch\nDESCRIPTION: Performs element-wise division of a tensor by a scalar value. It requires an input tensor of arbitrary shape and a scalar value, here specified as 786432. The function outputs a tensor of the same shape with each element divided by the scalar. This operation is crucial for normalization and scaling in model data pipelines.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 786432), {})\n```\n\n----------------------------------------\n\nTITLE: Using ATen LogSoftmax Operator in PyTorch\nDESCRIPTION: This operator applies the log softmax function along the specified dimension of the input tensor. Dependencies include PyTorch with ATen support. Key parameters are the input tensor shape, dimension along which softmax is computed, and data type. Output is a tensor with log softmax applied. It requires proper input dimension specification to avoid errors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/ElectraForCausalLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([511, 30522], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Real and Imaginary Component Expressions\nDESCRIPTION: Expressions for calculating real and imaginary components of a complex number in terms of z and its conjugate.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_4\n\nLANGUAGE: math\nCODE:\n```\n\\begin{aligned}\n    \\mathrm{Re}(z) &= \\frac {z + z^*}{2} \\\\\n    \\mathrm{Im}(z) &= \\frac {z - z^*}{2j}\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Configuring FP16 Dependency for PyTorch QNNPACK\nDESCRIPTION: Sets up the fp16 library as a dependency for pytorch_qnnpack. Similar to psimd, it either builds fp16 from source or uses a system-provided version based on the USE_SYSTEM_FP16 flag. For building from source, it disables tests and benchmarks to optimize the build process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n# ---[ Configure FP16\nif(NOT TARGET fp16 AND NOT USE_SYSTEM_FP16)\n  set(FP16_BUILD_TESTS OFF CACHE BOOL \"\")\n  set(FP16_BUILD_BENCHMARKS OFF CACHE BOOL \"\")\n  add_subdirectory(\n    \"${FP16_SOURCE_DIR}\"\n    \"${CONFU_DEPENDENCIES_BINARY_DIR}/fp16\")\nelseif(NOT TARGET fp16 AND USE_SYSTEM_FP16)\n  find_file(FP16_HDR fp16.h PATH_SUFFIXES include)\n  if(NOT FP16_HDR)\n    message(FATAL_ERROR \"Cannot find fp16\")\n  endif()\n  add_library(fp16 STATIC \"${FP16_HDR}\")\n  set_target_properties(fp16 PROPERTIES LINKER_LANGUAGE C)\nendif()\ntarget_link_libraries(pytorch_qnnpack PRIVATE fp16)\n```\n\n----------------------------------------\n\nTITLE: Defining EventMetadataValue Class in PyTorch Distributed Elastic\nDESCRIPTION: This class represents a metadata value associated with an event in the PyTorch distributed elastic system. It's used to store additional information about events.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.elastic.events.api.EventMetadataValue\n```\n\n----------------------------------------\n\nTITLE: Defining BatchNorm Forward Test Cases in PyTorch - Python\nDESCRIPTION: This snippet declares a list of batch normalization forward pass test cases for PyTorch using Python tuple notation. Each entry specifies input tensors with various shapes and floating point precisions (f16), as well as associated parameters (such as running mean and variance, affine weights/biases, flags for training, momentum, and epsilon). The code is intended for automated benchmarking or unit testing of the BatchNorm operator, with each tuple corresponding to a single configuration to be tested. Inputs include (input tensor, weight, bias, running_mean, running_var, training, momentum, eps) and a dictionary for any additional keyword parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 160, 28, 28], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 192, 28, 28], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 224, 28, 28], f16), T([224], f16), T([224], f16), T([224], f16), T([224], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 256, 28, 28], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 288, 28, 28], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 320, 28, 28], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 352, 28, 28], f16), T([352], f16), T([352], f16), T([352], f16), T([352], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 384, 28, 28], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 416, 28, 28], f16), T([416], f16), T([416], f16), T([416], f16), T([416], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 448, 28, 28], f16), T([448], f16), T([448], f16), T([448], f16), T([448], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 480, 28, 28], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 256, 14, 14], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 24, ((T([64, 128, 14, 14], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 288, 14, 14], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 320, 14, 14], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 352, 14, 14], f16), T([352], f16), T([352], f16), T([352], f16), T([352], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 384, 14, 14], f16), T([384], f16), T([384], f16), T([384], f16), T([384], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 416, 14, 14], f16), T([416], f16), T([416], f16), T([416], f16), T([416], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 448, 14, 14], f16), T([448], f16), T([448], f16), T([448], f16), T([448], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 512, 14, 14], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 544, 14, 14], f16), T([544], f16), T([544], f16), T([544], f16), T([544], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 608, 14, 14], f16), T([608], f16), T([608], f16), T([608], f16), T([608], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 640, 14, 14], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 704, 14, 14], f16), T([704], f16), T([704], f16), T([704], f16), T([704], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 736, 14, 14], f16), T([736], f16), T([736], f16), T([736], f16), T([736], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 768, 14, 14], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 800, 14, 14], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 832, 14, 14], f16), T([832], f16), T([832], f16), T([832], f16), T([832], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 864, 14, 14], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 896, 14, 14], f16), T([896], f16), T([896], f16), T([896], f16), T([896], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 928, 14, 14], f16), T([928], f16), T([928], f16), T([928], f16), T([928], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 960, 14, 14], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 992, 14, 14], f16), T([992], f16), T([992], f16), T([992], f16), T([992], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 1024, 14, 14], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 512, 7, 7], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 16, ((T([64, 128, 7, 7], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 544, 7, 7], f16), T([544], f16), T([544], f16), T([544], f16), T([544], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 608, 7, 7], f16), T([608], f16), T([608], f16), T([608], f16), T([608], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 640, 7, 7], f16), T([640], f16), T([640], f16), T([640], f16), T([640], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 704, 7, 7], f16), T([704], f16), T([704], f16), T([704], f16), T([704], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 736, 7, 7], f16), T([736], f16), T([736], f16), T([736], f16), T([736], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 768, 7, 7], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 800, 7, 7], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 832, 7, 7], f16), T([832], f16), T([832], f16), T([832], f16), T([832], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 864, 7, 7], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 896, 7, 7], f16), T([896], f16), T([896], f16), T([896], f16), T([896], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 928, 7, 7], f16), T([928], f16), T([928], f16), T([928], f16), T([928], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 992, 7, 7], f16), T([992], f16), T([992], f16), T([992], f16), T([992], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([64, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Limitation with Disabling Autocast in Scripted Functions\nDESCRIPTION: Example demonstrating that disabling autocast within a scripted function doesn't override an active autocast context from eager mode - the operation will still be performed in mixed precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.cuda.amp import autocast\n\n@torch.jit.script\ndef fn(a, b):\n    with autocast(enabled=False):\n        return torch.mm(a, b)\n\nx = torch.rand((2, 2), device='cuda', dtype=torch.float)\ny = torch.rand((2, 2), device='cuda', dtype=torch.float)\n\n# this will print half-precision dtype\nwith autocast(enabled=True):\n    print(fn(x, y).dtype)\n```\n\n----------------------------------------\n\nTITLE: Execute aten.native_batch_norm_backward in Python\nDESCRIPTION: This code snippet represents the backward pass of the native batch normalization operation in PyTorch, indicating derivative calculations for the backward propagation through neural networks. Requirements include compatible versions of PyTorch. Inputs consist of tensors for data, weights, and additional configuration settings for training purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 1536, 7, 7], f16), T([64, 1536, 7, 7], f16), T([1536], f16), T([1536], f16), T([1536], f16), T([1536], f32), T([1536], f32), True, 0.001, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch NLL Loss Operations\nDESCRIPTION: Negative log likelihood loss calculations and backpropagation with 1000-class output and ignore index of -100\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Splitting Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of tensor splitting operations on 4D tensors. It demonstrates splitting along the channel dimension with different split sizes and tensor shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/shufflenet_v2_x1_0_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.split.Tensor\ncnt: 3, ((T([128, 116, 28, 28], f16), 58, 1), {})\ncnt: 7, ((T([128, 232, 14, 14], f16), 116, 1), {})\ncnt: 3, ((T([128, 464, 7, 7], f16), 232, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Benchmark Suite\nDESCRIPTION: This code snippet demonstrates how to run the registered benchmarks using the main method from the benchmark_runner module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    op_bench.benchmark_runner.main()\n```\n\n----------------------------------------\n\nTITLE: Backend Configuration for LinearReLU in PyTorch Quantization\nDESCRIPTION: Configuration code that sets up the backend for quantizing a LinearReLU module. It defines the root module, reference quantized module, and fused module for the quantization pattern.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/fx/README.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nbackend_config configurations used in this step:\nBackendConfig(nniqat.LinearReLU)\n    .set_root_module(nn.Linear)\n    .set_reference_quantized_module_for_root(nnqr.Linear)\n    .set_fused_module(nni.LinearReLU)\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Log Softmax Operations in PyTorch\nDESCRIPTION: This snippet records the usage patterns of the \\\"aten._log_softmax.default\\\" operator in PyTorch, detailing tensor shapes, dimensions, and parameters used in the operation. This operator computes the log of the softmax values for given inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([1024, 50265], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Scatter Operation in PyTorch\nDESCRIPTION: This snippet shows a scatter operation, which is used to update values in a tensor based on indices and a source tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.scatter.src\ncnt: 1, ((T([64, 196, 1000], f16), 1, T([64, 1, 1000], i64), T([64, 1, 1000], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Manipulation Operations in Neural Network\nDESCRIPTION: Shows various tensor manipulation operations including dimension reduction (mean), scalar operations (div, mul), and tensor copying. These are used for normalization, reshaping features, and preparing tensors for other operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([32, 1024, 7, 7], f16, stride=(50176, 1, 7168, 1024)), [-1, -2], True), {})\ncnt: 1, ((T([32, 512, 14, 14], f16, stride=(100352, 1, 7168, 512)), [1], True), {})\ncnt: 1, ((T([32, 256, 28, 28], f16, stride=(200704, 1, 7168, 256)), [1], True), {})\ncnt: 1, ((T([32, 128, 56, 56], f16, stride=(401408, 1, 7168, 128)), [1], True), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([32, 1024, 7, 7], f16, stride=(1024, 1, 0, 0)), 49), {})\ncnt: 1, ((T([32, 512, 14, 14], f16, stride=(196, 0, 14, 1)), 512), {})\ncnt: 1, ((T([32, 256, 28, 28], f16, stride=(784, 0, 28, 1)), 256), {})\ncnt: 1, ((T([32, 128, 56, 56], f16, stride=(3136, 0, 56, 1)), 128), {})\nOperator: aten.mul.Scalar\ncnt: 1, ((T([32, 1, 14, 14], f16), -0.5), {})\ncnt: 1, ((T([32, 1, 14, 14], f16), 0.00390625), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Multiplication Operations in Neural Network\nDESCRIPTION: Shows matrix multiplication operations (aten.mm.default) used in fully connected layers and projections. These operations transform features between different dimensionalities across various stages of the network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 3, ((T([100352, 128], f16), T([128, 512], f16, stride=(1, 128))), {})\ncnt: 3, ((T([100352, 512], f16), T([512, 128], f16, stride=(1, 512))), {})\ncnt: 3, ((T([25088, 256], f16), T([256, 1024], f16, stride=(1, 256))), {})\ncnt: 3, ((T([25088, 1024], f16), T([1024, 256], f16, stride=(1, 1024))), {})\ncnt: 27, ((T([6272, 512], f16), T([512, 2048], f16, stride=(1, 512))), {})\ncnt: 27, ((T([6272, 2048], f16), T([2048, 512], f16, stride=(1, 2048))), {})\ncnt: 3, ((T([1568, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})\ncnt: 3, ((T([1568, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Release Branch Cut Script in Bash\nDESCRIPTION: This shell snippet demonstrates how to initiate the PyTorch release branch cut using the provided convenience script. It sets important environment variables such as DRY_RUN (to enable or disable a dry run), GIT_BRANCH_TO_CUT_FROM (typically 'main' or another branch name), and RELEASE_VERSION (required if the version.txt file is not present). The script scripts/release/cut-release-branch.sh is executed to automate branching, and must be run from the repo root. Inputs include the branch and version to cut. Dependencies: bash shell, access to the PyTorch repository, and script permissions. Outputs: a new release branch is created as per the configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nDRY_RUN=disabled GIT_BRANCH_TO_CUT_FROM=main RELEASE_VERSION=1.11 scripts/release/cut-release-branch.sh\n```\n\n----------------------------------------\n\nTITLE: Computing Slice Backward Gradients using aten.slice_backward - Python\nDESCRIPTION: Records multiple usages of aten.slice_backward.default for slicing gradients along specified axes in tensors, usually a step in gradient propagation after slicing operations. The operator takes start, end, axis, and step for the slice, often with 5D f16 tensors and various complex strides. Prerequisites are PyTorch and correct computation graph shape alignment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.slice_backward.default\ncnt: 372, ((T([2, 12, 16, 64, 64], f16), [2, 12, 16, 64, 64], 1, 0, 9223372036854775807, 1), {})\ncnt: 372, ((T([2, 12, 16, 64, 64], f16), [2, 12, 16, 64, 64], 0, 0, 9223372036854775807, 1), {})\ncnt: 72, ((T([2, 12, 14, 192, 64], f16), [2, 12, 14, 192, 64], 1, 0, 9223372036854775807, 1), {})\ncnt: 72, ((T([2, 12, 14, 192, 64], f16), [2, 12, 14, 192, 64], 0, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 64], f16), [2, 12, 12, 64, 512], 4, -64, 9223372036854775807, 1), {})\ncnt: 48, ((T([2, 12, 12, 64, 512], f16), [2, 12, 12, 64, 512], 3, 0, 9223372036854775807, 1), {})\ncnt: 48, ((T([2, 12, 12, 64, 512], f16), [2, 12, 12, 64, 512], 2, 0, 9223372036854775807, 1), {})\ncnt: 48, ((T([2, 12, 12, 64, 512], f16), [2, 12, 12, 64, 512], 1, 0, 9223372036854775807, 1), {})\ncnt: 48, ((T([2, 12, 12, 64, 512], f16), [2, 12, 12, 64, 512], 0, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 64], f16), [2, 12, 12, 64, 512], 4, 0, 64, 1), {})\ncnt: 12, ((T([2, 12, 12, 192, 64], f16), [2, 12, 14, 192, 64], 2, 1, -1, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 192], f16), [2, 12, 12, 64, 512], 4, 256, -64, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 192], f16), [2, 12, 12, 64, 512], 4, 64, 256, 1), {})\ncnt: 12, ((T([2, 12, 12, 192, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [2, 12, 14, 192, 64], 2, 1, -1, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 64], f16), [2, 12, 16, 64, 64], 2, 2, -2, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [2, 12, 16, 64, 64], 2, 3, -1, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [2, 12, 16, 64, 64], 2, 2, -2, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 64, 1)), [2, 12, 16, 64, 64], 2, 1, -3, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [2, 12, 16, 64, 64], 2, 3, -1, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [2, 12, 16, 64, 64], 2, 2, -2, 1), {})\ncnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(1769472, 147456, 12288, 1, 192)), [2, 12, 16, 64, 64], 2, 1, -3, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Listing Available Benchmark Tests\nDESCRIPTION: Displays all supported benchmark tests in the PyTorch operator benchmark suite.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m pt.add_test --list-tests\n```\n\n----------------------------------------\n\nTITLE: Defining test_jit Executable in CMake\nDESCRIPTION: Creates an executable target named `test_jit`. This executable includes a common main function (`main.cpp`) and all the C++ JIT test source files listed in the `JIT_TEST_SRCS` variable. It serves as the main GTest runner for C++ JIT tests.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(test_jit\n  ${TORCH_ROOT}/test/cpp/common/main.cpp\n  ${JIT_TEST_SRCS}\n)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Count for aten._log_softmax_backward_data Operations\nDESCRIPTION: Shows the call pattern for the _log_softmax_backward_data.default operator using half precision tensors of shape [128, 1000], with dimension 1 and output dtype f16.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking Sum Operations in PyTorch\nDESCRIPTION: Records of tensor sum operations across different dimensions. Shows reduction operations typically used in pooling or attention mechanisms, with various tensor shapes and dimension parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 5, ((T([128, 1152, 6, 6], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 672, 6, 6], f16), [2, 3], True), {})\ncnt: 3, ((T([128, 672, 12, 12], f16), [2, 3], True), {})\ncnt: 4, ((T([128, 480, 12, 12], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 240, 12, 12], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 240, 24, 24], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 144, 24, 24], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 144, 48, 48], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 96, 48, 48], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 32, 96, 96], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Copying Tensors in PyTorch\nDESCRIPTION: This snippet covers 'aten.copy_.default', which is used to perform in-place copying from a source tensor to a destination tensor in PyTorch. Useful for ensuring data consistency or setting initial conditions, it requires tensors of matching dimensions and half-precision compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([96, 3, 224, 224], f16), T([96, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Average Pooling Operations in PyTorch\nDESCRIPTION: Records of aten.avg_pool2d.default operator calls for various tensor shapes and pooling parameters. These operations perform spatial downsampling using different kernel sizes, strides, and padding configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.avg_pool2d.default\ncnt: 1, ((T([32, 128, 56, 56], f16), [3, 3], [2, 2], [1, 1]), {})\ncnt: 1, ((T([32, 256, 56, 56], f16), [2, 2], [2, 2], [0, 0], True, False), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), [3, 3], [2, 2], [1, 1]), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), [2, 2], [2, 2], [0, 0], True, False), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), [3, 3], [2, 2], [1, 1]), {})\ncnt: 1, ((T([32, 1024, 14, 14], f16), [2, 2], [2, 2], [0, 0], True, False), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing oneDNN Graph Build Option in CMake\nDESCRIPTION: Initializes the CMake variable BUILD_ONEDNN_GRAPH to OFF. This variable likely controls whether the oneDNN Graph API integration is built.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_28\n\nLANGUAGE: cmake\nCODE:\n```\nset(BUILD_ONEDNN_GRAPH OFF)\n```\n\n----------------------------------------\n\nTITLE: Defining and Registering a GTest-based C++ Test Executable (CMake)\nDESCRIPTION: Configures a test C++ executable using an explicit source and header include, links it against GTest and the main GTest runner, then registers it as a discoverable CTest unit test. The snippet uses include_directories for custom headers and add_executable/target_link_libraries for target specification. It assumes source files and header location correctness. Requires preceding GTest integration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/inductor/cpp/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n################################\n# Tests\n################################\n\n# TODO(voz): This is a little assumptive of just this one test, rewrite with real dir includes\ninclude_directories(${ATEN_INCLUDE})\nadd_executable(test_cpp_prefix test_cpp_prefix.cpp ../../torchinductor/codegen/cpp_prefix.h)\ntarget_link_libraries(test_cpp_prefix gtest gtest_main)\nadd_test(NAME test_cpp_prefix COMMAND test_cpp_prefix)\n```\n\n----------------------------------------\n\nTITLE: Setting up torch.random module documentation with reStructuredText\nDESCRIPTION: RST directives to configure the documentation for the torch.random module. It sets the current module context and uses automodule to automatically generate documentation for all members in the torch.random module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/random.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: torch.random\n\n.. automodule:: torch.random\n   :members:\n```\n\n----------------------------------------\n\nTITLE: Configuring FuncTorch CMake Build\nDESCRIPTION: Complete CMake configuration for building FuncTorch as a PyTorch extension. Sets up project requirements, compiles C++ sources, configures linking with PyTorch and Python, and handles platform-specific settings for module extensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\nproject(functorch)\nset(CMAKE_CXX_STANDARD 17)\n\ninclude(GNUInstallDirs)\ninclude(CMakePackageConfigHelpers)\n\nset(FT_DIR csrc)\nfile(GLOB_RECURSE FT_SOURCES ${FT_DIR}/*.cpp ${FT_DIR}/*.c)\n\nadd_library(${PROJECT_NAME} MODULE ${FT_SOURCES})\ntarget_include_directories(${PROJECT_NAME} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\ntarget_compile_definitions(${PROJECT_NAME} PRIVATE FUNCTORCH_BUILD_MAIN_LIB)\ntarget_compile_definitions(${PROJECT_NAME} PRIVATE TORCH_EXTENSION_NAME=_C)\ntarget_compile_definitions(${PROJECT_NAME} PRIVATE TORCH_API_INCLUDE_EXTENSION_H)\ntarget_compile_options(${PROJECT_NAME} PRIVATE ${TORCH_PYTHON_COMPILE_OPTIONS})\ntarget_compile_options_if_supported(${PROJECT_NAME} \"-Wmissing-prototypes\")\ntarget_compile_options_if_supported(${PROJECT_NAME} \"-Werror=missing-prototypes\")\nif(BUILD_LIBTORCHLESS)\n  target_link_libraries(${PROJECT_NAME} PRIVATE ${TORCH_LIB} torch_python)\nelse()\n  # functorch cannot use the alias to build on windows\n  target_link_libraries(${PROJECT_NAME} PRIVATE torch torch_python)\nendif()\ntarget_link_libraries(${PROJECT_NAME} PRIVATE pybind::pybind11)\n\nset_target_properties(${PROJECT_NAME} PROPERTIES LIBRARY_OUTPUT_DIRECTORY\n      ${CMAKE_BINARY_DIR}/functorch)\nset_target_properties(${PROJECT_NAME} PROPERTIES INSTALL_RPATH \"${_rpath_portable_origin}/../torch/lib\")\n\n# Copy-pasted prefix/suffix logic for Python extensions from\n# https://github.com/pytorch/pytorch/blob/33bb8ae350611760139457b85842b1d7edf9aa11/caffe2/CMakeLists.txt#L1975\n# https://github.com/pytorch/pytorch/blob/33bb8ae350611760139457b85842b1d7edf9aa11/caffe2/CMakeLists.txt#L2022\n# TODO: It would be good to be able to use Python3_add_library target, but it does not work in many cases\nset_target_properties(${PROJECT_NAME} PROPERTIES PREFIX \"\" DEBUG_POSTFIX \"\")\nif(WIN32)\n  set_target_properties(${PROJECT_NAME} PROPERTIES SUFFIX \".pyd\")\nelse()\n  set_target_properties(${PROJECT_NAME} PROPERTIES SUFFIX \".so\")\nendif()\n# Needed to link functorch on MacOS\nif(NOT ${TORCH_PYTHON_LINK_FLAGS} STREQUAL \"\")\n  set_target_properties(${PROJECT_NAME} PROPERTIES LINK_FLAGS ${TORCH_PYTHON_LINK_FLAGS})\nendif()\ninstall(TARGETS ${PROJECT_NAME} DESTINATION \"${CMAKE_CURRENT_SOURCE_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Build Options for PyTorch Lite Interpreter and Executorch\nDESCRIPTION: Configures CMake options for building Lite Interpreter with tracing build option (off by default) and Executorch (on by default).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\noption(TRACING_BASED\n       \"Master flag to build Lite Interpreter with tracing build option\" OFF)\noption(BUILD_EXECUTORCH \"Master flag to build Executorch\" ON)\n```\n\n----------------------------------------\n\nTITLE: Running FastRNNs benchmarks with specific implementations and grouping\nDESCRIPTION: Command to run benchmarks for specific RNN implementations (CUDNN, ATEN, JIT) with grouping by RNN types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/fastrnns/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m fastrnns.bench --rnns cudnn aten jit --group rnns\n```\n\n----------------------------------------\n\nTITLE: Using Trained AutoHeuristic\nDESCRIPTION: Command to use the trained heuristic in the application with environment variable configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_AUTOHEURISTIC_USE=\"pad_mm\" python run.py\n```\n\n----------------------------------------\n\nTITLE: Tensor Reduction Operations for Gradient Computation\nDESCRIPTION: Shows tensor reduction operations used to compute gradients for model parameters. These sum operations aggregate gradients across batch and sequence dimensions to update parameters during backpropagation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([2, 1000], f16), [0], True), {})\ncnt: 4, ((T([2, 1, 768], f16), [0, 1], True), {})\ncnt: 6, ((T([2, 768], f16), [0], True), {})\ncnt: 2, ((T([2, 3072], f16), [0], True), {})\ncnt: 4, ((T([1154, 768], f16), [0], True), {})\ncnt: 1, ((T([2, 1, 768], f16), [0], True), {})\ncnt: 72, ((T([2, 576, 768], f16), [0, 1], True), {})\ncnt: 72, ((T([1152, 768], f16), [0], True), {})\ncnt: 36, ((T([1152, 3072], f16), [0], True), {})\ncnt: 72, ((T([2, 576, 576, 16], f16, stride=(5308416, 576, 1, 331776)), [0, 1, 2], True), {})\ncnt: 36, ((T([1152, 2304], f16), [0], True), {})\ncnt: 1, ((T([2, 576, 768], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Test Installation Configuration\nDESCRIPTION: Configures installation rules for the test_api executable, including RPATH settings and PDB files for MSVC builds. Conditional installation based on INSTALL_TEST flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(INSTALL_TEST)\n  set_target_properties(test_api PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n  install(TARGETS test_api DESTINATION bin)\n  if(MSVC AND BUILD_SHARED_LIBS)\n    install(FILES $<TARGET_PDB_FILE:test_api> DESTINATION bin OPTIONAL)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Collecting HuggingFace Models Training Data\nDESCRIPTION: Command to collect matrix multiplication training data from HuggingFace models using TorchInductor\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mm/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_AUTOHEURISTIC_USE=\"\" TORCHINDUCTOR_AUTOHEURISTIC_COLLECT=\"mm\" TORCHINDUCTOR_AUTOHEURISTIC_LOG_PATH=\"hf_train_mm.txt\" TORCHINDUCTOR_MAX_AUTOTUNE=1 time python ../../../benchmarks/dynamo/huggingface.py --ci --performance --timing --explain --inductor --device cuda --train --amp\n```\n\n----------------------------------------\n\nTITLE: Editing Git Submodule Configuration (Bash)\nDESCRIPTION: Opens the `.gitmodules` file in the default text editor using the `git config` command with the `-e` flag. This allows for direct modification of submodule settings, such as changing the remote URL, which might be necessary when updating submodules for a release, potentially pointing them to a fork.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ngit config --file=.gitmodules -e\n```\n\n----------------------------------------\n\nTITLE: Git Release Preparation Workflow in Shell\nDESCRIPTION: This sequence of git commands demonstrates how to prepare the repository for tagging a new release candidate. First, it checks out the release branch, then displays the recent commit history for validation. Inputs: branch name ('release/1.12'); outputs: current HEAD is on the release branch and recent commits are listed. Dependencies: git installed, appropriate repository access rights.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#2025-04-22_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ngit checkout release/1.12\ngit log --oneline\n```\n\n----------------------------------------\n\nTITLE: PyTorch Core C++ Function Signatures\nDESCRIPTION: Collection of mangled C++ function signatures from PyTorch core implementation, including tensor operations, CUDA functions, autograd mechanics, and Python bindings. These represent the low-level implementation functions that power PyTorch's high-level APIs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_25\n\nLANGUAGE: C++\nCODE:\n```\n_ZNK2at8internal23OpaqueOptionalTensorRef9getTensorEv\n_ZNSt20__uninitialized_copyILb0EE13__uninit_copyIPKN3c106SymIntEPS3_EET0_T_S8_S7_\n_ZN2at13checkDimRangeEPKcRKNS_17TensorGeometryArgEll\n...\n```\n\n----------------------------------------\n\nTITLE: Defining C10 CUDA Source and Header Files\nDESCRIPTION: Lists all the source files and headers for the C10 CUDA library. These files define CUDA-related functionality like exception handling, stream management, and memory allocation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/cuda/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(C10_CUDA_SRCS\n    CUDAAllocatorConfig.cpp\n    CUDACachingAllocator.cpp\n    CUDADeviceAssertionHost.cpp\n    CUDAException.cpp\n    CUDAFunctions.cpp\n    CUDAMallocAsyncAllocator.cpp\n    CUDAMiscFunctions.cpp\n    CUDAStream.cpp\n    impl/CUDAGuardImpl.cpp\n    impl/CUDATest.cpp\n    driver_api.cpp\n)\nset(C10_CUDA_HEADERS\n    CUDAAllocatorConfig.h\n    CUDACachingAllocator.h\n    CUDADeviceAssertionHost.h\n    CUDAException.h\n    CUDAFunctions.h\n    CUDAGuard.h\n    CUDAMacros.h\n    CUDAMathCompat.h\n    CUDAMiscFunctions.h\n    CUDAStream.h\n    impl/CUDAGuardImpl.h\n    impl/CUDATest.h\n)\nset(CUDA_LINK_LIBRARIES_KEYWORD PRIVATE)\n```\n\n----------------------------------------\n\nTITLE: Type and Device Conversion with aten._to_copy in PyTorch (Python)\nDESCRIPTION: These snippets illustrate ATen's _to_copy for data type and device transformations of tensors, such as converting float32 to float16 or int64 to int32 on CUDA devices. Requirements include that input tensor dtype and optionally layout or device be specified. Key parameters are the input tensor and desired target dtype, affecting downstream performance and compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([4, 1, 1, 128], f32),), {'dtype': f16})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([4, 128], b8),), {'dtype': i32})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([4, 128], i64),), {'dtype': i32, 'layout': torch.strided, 'device': 'cuda'})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([4, 128], i32),), {'dtype': i64})\n```\n\n----------------------------------------\n\nTITLE: Non-Set Examples in Python\nDESCRIPTION: This snippet demonstrates examples that are not sets but might be confused with set syntax. It includes dictionary initialization, multi-line strings containing the word 'set', and a class with a 'set' method.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/set_linter_testdata/python_code.py.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nd = {}\nlong_string = \"\"\" set()\nset() set x.set set()\n\\\"\"\"\"\n\nclass A:\n    def set(self, x):\n        self.x = x\n\nset = A().set\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Softmax Backward Operations in PyTorch\nDESCRIPTION: Documenting the instances of \\\"aten._softmax_backward_data.default\\\" operator. It is used in the process of backpropagation through a softmax layer.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([128, 128, 128], f16), T([128, 128, 128], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Comparing DataFrame and Regular DataPipe Iteration\nDESCRIPTION: Demonstration of iteration over both DataFrame Pipe and Regular DataPipe to show their similar behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint('DataFrames Pipe')\ndp = get_dataframes_pipe()\nfor i in dp:\n    print(i)\n\nprint('Regular DataPipe')\ndp = get_regular_pipe()\nfor i in dp:\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Defining USE_GTEST Preprocessor Macro for test_jit in CMake\nDESCRIPTION: Adds the `USE_GTEST` preprocessor definition to the `test_jit` target. This macro is likely used within the C++ source code to conditionally compile GTest-specific code or configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\n# TODO temporary until we can delete the old gtest polyfills.\ntarget_compile_definitions(test_jit PRIVATE USE_GTEST)\n```\n\n----------------------------------------\n\nTITLE: Applying Softmax to Tensors in PyTorch (Python)\nDESCRIPTION: Performs aten._softmax, which applies the softmax function across the specified dimension of the tensor. This operation is commonly used to convert network outputs into probabilities.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\naten._softmax.default\ncnt: 12, ((T([16, 12, 512, 512], f16), -1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Reporting aten.copy_ Operator Usage Patterns - PyTorch - Python\nDESCRIPTION: This snippet encodes instance counts and argument configs for the in-place copy operator in PyTorch (aten.copy_), with each entry listing source and destination tensor shapes and datatypes. Requires PyTorch's tensor semantics. Inputs are both source and target tensors; output is not specified as the listing is for pattern analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Intel XPU Support for PyTorch Python\nDESCRIPTION: Sets up Intel XPU (GPU/accelerator) support for PyTorch Python bindings when the USE_XPU option is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_XPU)\n    include(${TORCH_ROOT}/cmake/public/xpu.cmake)\n    append_filelist(\"libtorch_python_xpu_sources\" TORCH_PYTHON_SRCS)\n\n    list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_XPU)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing C10 XPU Headers and Generated Files\nDESCRIPTION: Sets up installation rules for all header files and the generated cmake macros header, preserving directory structure.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/xpu/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nforeach(file ${C10_XPU_HEADERS})\n  get_filename_component(dir ${file} DIRECTORY)\n  install(FILES ${file} DESTINATION include/c10/xpu/${dir})\nendforeach()\ninstall(FILES ${CMAKE_BINARY_DIR}/c10/xpu/impl/xpu_cmake_macros.h\n  DESTINATION include/c10/xpu/impl)\n```\n\n----------------------------------------\n\nTITLE: Displaying Hugging Face Benchmark Help Information\nDESCRIPTION: This command displays the help information for the Hugging Face benchmarking script, providing details on available options and their usage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_performance_dashboard.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython benchmarks/dynamo/huggingface.py -h\n```\n\n----------------------------------------\n\nTITLE: Testing FastRNNs correctness for specific implementations\nDESCRIPTION: Command to test specifically the JIT implementation of RNNs for correctness validation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/fastrnns/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m fastrnns.test --rnns jit\n```\n\n----------------------------------------\n\nTITLE: Tracing ATen Tensor Arithmetic Operators in PyTorch Model (Python)\nDESCRIPTION: Captures all invocations of arithmetic-related ATen operators (add, add_, addmm, div) as used in a typical PyTorch model, logging argument tensor shapes, dtypes, and optional operator options. Dependencies: PyTorch (ATen backend), model code with mixed-precision tensors. Accepts input tensor pairs or tensors with scalars and returns the arithmetic result in specified dtype and shape; scalar variants work elementwise. Limitations: Only includes tensor shapes/types, not the actual computational graph or code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16)), {})\ncnt: 2, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16)), {})\ncnt: 2, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16)), {})\ncnt: 3, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add_.Tensor\ncnt: 2, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16)), {})\ncnt: 2, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16)), {})\ncnt: 2, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16)), {})\ncnt: 2, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([16, 512], f16), T([512, 1000], f16, stride=(1, 512))), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([16, 512, 7, 7], f16, stride=(512, 1, 0, 0)), 49), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 16000), {})\n```\n\n----------------------------------------\n\nTITLE: Enumerating Inputs for aten.threshold_backward.default Operator - PyTorch - Python\nDESCRIPTION: This snippet provides multiple example arguments for the aten.threshold_backward.default operator in PyTorch, wherein each entry details input and output tensor shapes (all using f16), alongside a threshold parameter (set to 0). The patterns illustrate varying spatial dimensions and channel sizes, helpful for validating correct broadcasting and thresholding logic. Each tuple packs inputs in a consistent order for reproducibility in automated test scripts or documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_unet_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((T([1, 64, 640, 959], f16), T([1, 64, 640, 959], f16), 0), {})\ncnt: 1, ((T([1, 64, 320, 479], f16), T([1, 64, 320, 479], f16), 0), {})\ncnt: 3, ((T([1, 128, 320, 479], f16), T([1, 128, 320, 479], f16), 0), {})\ncnt: 1, ((T([1, 128, 160, 239], f16), T([1, 128, 160, 239], f16), 0), {})\ncnt: 3, ((T([1, 256, 160, 239], f16), T([1, 256, 160, 239], f16), 0), {})\ncnt: 1, ((T([1, 256, 80, 119], f16), T([1, 256, 80, 119], f16), 0), {})\ncnt: 3, ((T([1, 512, 80, 119], f16), T([1, 512, 80, 119], f16), 0), {})\ncnt: 2, ((T([1, 512, 40, 59], f16), T([1, 512, 40, 59], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Fixing MKL Symbol Lookup Issue on Linux for PyTorch\nDESCRIPTION: Adds linker flags on Linux to address a symbol lookup error with Intel MKL. Sets the --no-as-needed flag and appends any environment LDFLAGS if they aren't already present.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\n# This is a fix for a rare build issue on Ubuntu: symbol lookup error:\n# miniconda3/envs/pytorch-py3.7/lib/libmkl_intel_lp64.so: undefined symbol:\n# mkl_blas_dsyrk\n# https://software.intel.com/en-us/articles/symbol-lookup-error-when-linking-intel-mkl-with-gcc-on-ubuntu\nif(LINUX)\n  set(CMAKE_SHARED_LINKER_FLAGS\n      \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,--no-as-needed\")\n\n  set(ENV_LDFLAGS \"$ENV{LDFLAGS}\")\n  string(STRIP \"${ENV_LDFLAGS}\" ENV_LDFLAGS)\n  # Do not append linker flags passed via env var if they already there\n  if(NOT ${CMAKE_SHARED_LINKER_FLAGS} MATCHES \"${ENV_LDFLAGS}\")\n     set(CMAKE_SHARED_LINKER_FLAGS\n         \"${CMAKE_SHARED_LINKER_FLAGS} ${ENV_LDFLAGS}\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Functorch Benchmarks\nDESCRIPTION: Commands for installing functorch (either stable or from source) and running benchmarks with functorch integration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/functional_autograd_benchmark/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Install stable functorch:\npip install functorch\n# or install from source:\npip install git+https://github.com/pytorch/functorch\n\n# Run the benchmark for the base\n# This will use the GPU if available.\npushd benchmarks/functional_autograd_benchmark\npython functional_autograd_benchmark.py --output bench-with-functorch.txt\n```\n\n----------------------------------------\n\nTITLE: Appending QNNPACK C++ Compiler Flag in CMake\nDESCRIPTION: If the USE_PYTORCH_QNNPACK option is enabled, this snippet appends the preprocessor definition `-DUSE_PYTORCH_QNNPACK` to the C++ compiler flags (CMAKE_CXX_FLAGS). This enables code related to the QNNPACK backend.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_36\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_PYTORCH_QNNPACK)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_PYTORCH_QNNPACK\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Model-Specific Numerical Parameters (Text/Config)\nDESCRIPTION: A text-based configuration listing various transformer model names (e.g., AlbertForMaskedLM, BartForConditionalGeneration, GPT2ForSequenceClassification) paired with integer values. This likely serves as a lookup table for model-specific settings, potentially batch sizes or other configuration parameters, used within the PyTorch project.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/huggingface_models_list.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nAlbertForMaskedLM,8\nAlbertForQuestionAnswering,8\nAllenaiLongformerBase,8\nBartForCausalLM,8\nBartForConditionalGeneration,4\nBertForMaskedLM,32\nBertForQuestionAnswering,32\nBlenderbotForCausalLM,32\nBlenderbotForConditionalGeneration,16\nBlenderbotSmallForCausalLM,256\nBlenderbotSmallForConditionalGeneration,128\nCamemBert,32\nDebertaForMaskedLM,32\nDebertaForQuestionAnswering,32\nDebertaV2ForMaskedLM,8\nDebertaV2ForQuestionAnswering,8\nDistilBertForMaskedLM,256\nDistilBertForQuestionAnswering,512\nDistillGPT2,32\nElectraForCausalLM,64\nElectraForQuestionAnswering,128\nGPT2ForSequenceClassification,8\nGPTJForCausalLM,1\nGPTJForQuestionAnswering,1\nGPTNeoForCausalLM,32\nGPTNeoForSequenceClassification,32\nGoogleFnet,32\nLayoutLMForMaskedLM,32\nLayoutLMForSequenceClassification,32\nM2M100ForConditionalGeneration,64\nMBartForCausalLM,8\nMBartForConditionalGeneration,4\nMT5ForConditionalGeneration,32\nMegatronBertForCausalLM,16\nMegatronBertForQuestionAnswering,16\nMobileBertForMaskedLM,256\nMobileBertForQuestionAnswering,256\nOPTForCausalLM,4\nPLBartForCausalLM,16\nPLBartForConditionalGeneration,8\nPegasusForCausalLM,128\nPegasusForConditionalGeneration,64\nRobertaForCausalLM,32\nRobertaForQuestionAnswering,32\nSpeech2Text2ForCausalLM,1024\nT5ForConditionalGeneration,8\nT5Small,8\nTrOCRForCausalLM,64\nXGLMForCausalLM,32\nXLNetLMHeadModel,16\nYituTechConvBert,32\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Input Shapes in PyTorch\nDESCRIPTION: This code snippet shows the input tensor shapes and parameters for the native_batch_norm operation in PyTorch. It includes counts of how many times each unique shape combination is used.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([32, 200, 14, 14], f16), T([200], f16), T([200], f16), T([200], f16), T([200], f16), False, 0.01, 0.001), {})\ncnt: 4, ((T([32, 184, 14, 14], f16), T([184], f16), T([184], f16), T([184], f16), T([184], f16), False, 0.01, 0.001), {})\ncnt: 2, ((T([32, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), False, 0.01, 0.001), {})\ncnt: 2, ((T([32, 112, 14, 14], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f16), False, 0.01, 0.001), {})\ncnt: 3, ((T([32, 672, 14, 14], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), False, 0.01, 0.001), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f16), False, 0.01, 0.001), {})\ncnt: 3, ((T([32, 160, 7, 7], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), False, 0.01, 0.001), {})\ncnt: 5, ((T([32, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), False, 0.01, 0.001), {})\n```\n\n----------------------------------------\n\nTITLE: Adding Dynamic CUDA Dependencies in CMake\nDESCRIPTION: This snippet executes when the environment variable `ATEN_STATIC_CUDA` is *not* set. It appends the standard CUDA libraries (`${CUDA_LIBRARIES}`) and dynamic versions of cuSPARSE and cuFFT to the `ATen_CUDA_DEPENDENCY_LIBS` list. If `BUILD_LAZY_CUDA_LINALG` is not enabled, it also appends the dynamic cuSolver library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\n  else()\n    list(APPEND ATen_CUDA_DEPENDENCY_LIBS\n      ${CUDA_LIBRARIES}\n      CUDA::cusparse\n      CUDA::cufft\n    )\n   if(NOT BUILD_LAZY_CUDA_LINALG)\n     list(APPEND ATen_CUDA_DEPENDENCY_LIBS\n       CUDA::cusolver\n     )\n   endif()\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Tracking Batch Normalization Backward Operations in PyTorch\nDESCRIPTION: Records of batch normalization backward operations with various tensor shapes and parameters. Each line shows the count of operations with input and output tensor dimensions, running statistics, and gradient flags.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([128, 1280, 6, 6], f16), T([128, 1280, 6, 6], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 320, 6, 6], f16), T([128, 320, 6, 6], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), True, 1e-05, [True, True, True]), {})\ncnt: 10, ((T([128, 1152, 6, 6], f16), T([128, 1152, 6, 6], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f32), T([1152], f32), True, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([128, 192, 6, 6], f16), T([128, 192, 6, 6], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 672, 6, 6], f16), T([128, 672, 6, 6], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})\ncnt: 7, ((T([128, 672, 12, 12], f16), T([128, 672, 12, 12], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 112, 12, 12], f16), T([128, 112, 12, 12], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f32), T([112], f32), True, 1e-05, [True, True, True]), {})\ncnt: 8, ((T([128, 480, 12, 12], f16), T([128, 480, 12, 12], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 80, 12, 12], f16), T([128, 80, 12, 12], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 240, 12, 12], f16), T([128, 240, 12, 12], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 240, 24, 24], f16), T([128, 240, 24, 24], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 40, 24, 24], f16), T([128, 40, 24, 24], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 144, 24, 24], f16), T([128, 144, 24, 24], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 144, 48, 48], f16), T([128, 144, 48, 48], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 24, 48, 48], f16), T([128, 24, 48, 48], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 96, 48, 48], f16), T([128, 96, 48, 48], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 96, 96, 96], f16), T([128, 96, 96, 96], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 16, 96, 96], f16), T([128, 16, 96, 96], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 32, 96, 96], f16), T([128, 32, 96, 96], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Backward Pass\nDESCRIPTION: Backward propagation for batch normalization with gradient calculations. Uses both f16 and f32 tensors for running statistics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 0.001, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Describing Sparse Matrix Benchmarks in PyTorch\nDESCRIPTION: This markdown snippet outlines the purpose and scope of sparse matrix benchmarks in PyTorch. It explains that these benchmarks are used to compare performance of sparse matrix operations across different formats and frameworks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/sparse/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n#Sparse benchmarks\n\nThese sets of benchmarks are for the sparse matrix functionality. They exist for\ncomparing the performance of sparse matrix routines such as SpMV between various\nsparse matrix formats and with other frameworks such as TensorFlow.\n```\n\n----------------------------------------\n\nTITLE: Defining the PyTorch JNI Shared Library Target in CMake\nDESCRIPTION: Creates the main shared library target using `add_library`. The target name is determined by the `PYTORCH_JNI_TARGET` variable (either `pytorch_jni_lite` or `pytorch_jni`), and it's built from the source files collected in the `pytorch_android_SOURCES` variable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(${PYTORCH_JNI_TARGET} SHARED ${pytorch_android_SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Applying PyTorch aten.add_.Tensor Operator in Python\nDESCRIPTION: This snippet showcases the 'aten.add_.Tensor' operator from PyTorch, which performs in-place element-wise addition of tensors. It processes tensors with the same shape and half-precision float format (f16), reducing memory footprint by modifying the original tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([8, 256, 56, 56], f16), T([8, 256, 56, 56], f16)), {})\ncnt: 4, ((T([8, 512, 28, 28], f16), T([8, 512, 28, 28], f16)), {})\ncnt: 6, ((T([8, 1024, 14, 14], f16), T([8, 1024, 14, 14], f16)), {})\ncnt: 3, ((T([8, 2048, 7, 7], f16), T([8, 2048, 7, 7], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Executing aten.sigmoid_backward in PyTorch\nDESCRIPTION: This snippet illustrates the utilization of the aten.sigmoid_backward.default operator to calculate gradients during the backward pass for the sigmoid function. Dependencies relate to PyTorch's efficient tensor computations. Inputs include the respective output from sigmoid activation and the gradient tensor for backpropagation. The output is a gradient tensor, instrumental for updating neural network weights.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ncount: 3, ((T([64, 1584, 1, 1], f16), T([64, 1584, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Unbind Operation in PyTorch\nDESCRIPTION: This snippet shows unbind operations on 5D tensors. It splits the tensor into a tuple of tensors along the specified dimension.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.unbind.int\ncnt: 14, ((T([3, 64, 12, 196, 32], f16, stride=(384, 225792, 32, 1152, 1)),), {})\ncnt: 2, ((T([2, 64, 12, 197, 32], f16, stride=(384, 151296, 32, 768, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Basic Arithmetic Operations: Addition and Multiplication in PyTorch (Python)\nDESCRIPTION: These represent batched elementwise add and mul operations between PyTorch tensors, as well as in-place variants (add_). Inputs and outputs preserve dtype and shape where relevant; supported dtypes include FP16 and integer. Operations may influence gradient flow if used in model parameter updates.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([4, 128], i32), 0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([4, 128], i64), 0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 73, ((T([4, 128, 768], f16), T([4, 128, 768], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 12, ((T([4, 12, 128, 128], f16), T([4, 1, 1, 128], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([30522, 768], f16), T([30522, 768], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add_.Tensor\ncnt: 1, ((T([4, 128, 768], f16), T([4, 128, 768], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 1, ((T([4, 1, 1, 128], f16), -65504.0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 1, ((T([4, 128], i32), T([4, 128], i32)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Element-wise Addition Operations in PyTorch\nDESCRIPTION: Records of aten.add.Tensor operator calls for element-wise addition of tensors with various shapes. All operations use half-precision (f16) tensors, with some tensors having specialized stride patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([32, 2, 512, 14, 14], f16), T([32, 2, 512, 14, 14], f16, stride=(100352, 0, 196, 14, 1))), {})\ncnt: 1, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16)), {})\ncnt: 1, ((T([32, 2, 256, 28, 28], f16), T([32, 2, 256, 28, 28], f16, stride=(200704, 0, 784, 28, 1))), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16)), {})\ncnt: 1, ((T([32, 2, 128, 56, 56], f16), T([32, 2, 128, 56, 56], f16, stride=(401408, 0, 3136, 56, 1))), {})\ncnt: 1, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)), {})\ncnt: 1, ((T([32, 2, 64, 56, 56], f16), T([32, 2, 64, 56, 56], f16, stride=(200704, 0, 3136, 56, 1))), {})\ncnt: 1, ((T([32, 64, 56, 56], f16), T([32, 64, 56, 56], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Setting ATen CUDA NVRTC Stub Sources and Dependencies in CMake\nDESCRIPTION: Conditionally executes if `USE_CUDA` is true. It sets the `ATen_NVRTC_STUB_SRCS` variable to the list of CUDA NVRTC stub C++ files contained in `cuda_nvrtc_stub_cpp`. It also appends the `ATEN_CUDA_FILES_GEN_LIB` target (likely representing generated CUDA files) to the `ATen_CUDA_DEPENDENCY_LIBS` list.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_CUDA)\n  set(ATen_NVRTC_STUB_SRCS ${cuda_nvrtc_stub_cpp})\n  list(APPEND ATen_CUDA_DEPENDENCY_LIBS ATEN_CUDA_FILES_GEN_LIB)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building and Linking Custom Shared Library for PyTorch (CMake)\nDESCRIPTION: This CMake snippet builds a shared library aoti_custom_ops from custom_ops.cpp, linking it against the torch library as a dependency. It is intended for use with torch.ops.load_library() in Python test contexts, enabling custom operator integration. The optional install command places the built library in the correct directory for use in tests, conditional on the INSTALL_TEST flag being set. Necessary dependencies include PyTorch C++ libraries and a properly configured build environment; inputs are source files and build flags, and outputs are one or more shared library files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/inductor/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Build separate libraries the define custom classes/operators used from our Python tests.\n# These are intended to be used with torch.ops.load_library() in our Python test suite.\nadd_library(aoti_custom_ops SHARED custom_ops.cpp)\ntarget_link_libraries(aoti_custom_ops torch)\n\nif(INSTALL_TEST)\n  install(TARGETS aoti_custom_ops DESTINATION lib)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Including CSV Table for Core Aten IR Operations\nDESCRIPTION: This snippet includes a CSV table containing information about Core Aten IR operations. It uses the csv-table directive in reStructuredText to reference an external CSV file.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_ir.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. csv-table::\n   :file: ../build/ir/aten_ops.csv\n   :widths: auto\n   :header-rows: 1\n```\n\n----------------------------------------\n\nTITLE: Backward Pass for Softmax in PyTorch (Python)\nDESCRIPTION: Computes the derivative for aten._softmax during backpropagation. It's used to compute the gradients of the softmax operation, crucial in training deep learning models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\naten._softmax_backward_data.default\ncnt: 12, ((T([16, 12, 512, 512], f16), T([16, 12, 512, 512], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring MSVC Z7 Override Option for PyTorch Build\nDESCRIPTION: Creates a CMake option to replace /Zi and /ZI with /Z7 when using MSVC to work around sccache bug. This option is enabled by default for MSVC but depends on whether MSVC is being used.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_dependent_option(\n  MSVC_Z7_OVERRIDE\n  \"Work around sccache bug by replacing /Zi and /ZI with /Z7 when using MSVC (if you are not using sccache, you can turn this OFF)\"\n  ON\n  \"MSVC\"\n  OFF)\n```\n\n----------------------------------------\n\nTITLE: Tensor Creation and Manipulation in PyTorch\nDESCRIPTION: This snippet showcases the creation and manipulation of PyTorch tensors with various shapes and data types. It includes operations like clone, copy, and division on tensors of different sizes, primarily using float16 (f16) and int64 (i64) data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nT([1024, 192], f16)  # Create a tensor of shape [1024, 192] with float16 data type\nT([1024, 30876], f16)  # Create a tensor of shape [1024, 30876] with float16 data type\nT([1024, 2000], f16).clone()  # Clone a tensor of shape [1024, 2000] with float16 data type\nT([248, 1024], i64).copy_(T([248, 1024], i64))  # Copy one int64 tensor to another\nT([], f16) / 1024  # Divide a scalar tensor by 1024\n```\n\n----------------------------------------\n\nTITLE: Copying Tensors with ATen\nDESCRIPTION: This function copies data from the source tensor to the destination tensor. Dependencies include two tensors with identical shapes, e.g., [16, 3, 128, 128], and data type f16. The main constraint is that source and destination tensors must have the same shapes and types to execute a successful copy operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([16, 3, 128, 128], f16), T([16, 3, 128, 128], f16)), {})\ncnt: 1, ((T([16, 5], f16), T([16, 5], f16)), {})\ncnt: 4, ((T([64], f16), T([64], f16)), {})\ncnt: 4, ((T([128], f16), T([128], f16)), {})\ncnt: 26, ((T([256], f16), T([256], f16)), {})\ncnt: 4, ((T([16, 64, 128, 128], f16), T([16, 64, 128, 128], f16)), {})\ncnt: 2, ((T([1, 1024, 128, 128], f16), T([1, 1024, 128, 128], f16)), {})\ncnt: 4, ((T([16, 128, 64, 64], f16), T([16, 128, 64, 64], f16)), {})\ncnt: 2, ((T([1, 2048, 64, 64], f16), T([1, 2048, 64, 64], f16)), {})\ncnt: 14, ((T([16, 256, 32, 32], f16), T([16, 256, 32, 32], f16)), {})\ncnt: 7, ((T([1, 4096, 32, 32], f16), T([1, 4096, 32, 32], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Up Parallel Info Binary with Custom Include Directories\nDESCRIPTION: Creates a 'parallel_info' binary target and configures its include directories to access ATen type interfaces needed by the implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/binaries/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncaffe2_binary_target(\"parallel_info.cc\")\ntarget_include_directories(parallel_info PUBLIC\n  ${CMAKE_BINARY_DIR}/aten/src) # provides \"ATen/TypeExtendedInterface.h\" to ATen.h\n```\n\n----------------------------------------\n\nTITLE: Running Add Benchmark with Long Tag Filter\nDESCRIPTION: Runs torch.add benchmarks using only the test cases tagged as 'long'.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython -m pt.add_test --tag-filter long\n```\n\n----------------------------------------\n\nTITLE: Running Code Coverage for Specific Test\nDESCRIPTION: This snippet shows how to run the code coverage tool for a specific test (atest) and generate reports for the entire PyTorch folder.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython oss_coverage.py --run-only=atest\n```\n\n----------------------------------------\n\nTITLE: Including Core Dependencies File in CMake\nDESCRIPTION: Includes the main CMake file responsible for handling dependencies, `cmake/Dependencies.cmake`. This file likely contains logic for finding and configuring various libraries required by PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_30\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(cmake/Dependencies.cmake)\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization Implementation\nDESCRIPTION: Layer normalization kernel implementation for GPU execution\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_36\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native53_GLOBAL__N__e6784c59_20_layer_norm_kernel_cu_9c5ada8a19LayerNormKernelImplERKNS_6TensorES4_S4_lldPS2_S5_S5_\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradle for Local PyTorch Android Build\nDESCRIPTION: This Gradle configuration demonstrates how to use locally built PyTorch Android AAR files in an Android project. It includes repository setup and dependency declarations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_3\n\nLANGUAGE: Groovy\nCODE:\n```\nallprojects {\n    repositories {\n        flatDir {\n            dirs 'libs'\n        }\n    }\n}\n\ndependencies {\n    implementation(name:'pytorch_android', ext:'aar')\n    implementation(name:'pytorch_android_torchvision', ext:'aar')\n    ...\n    implementation 'com.facebook.soloader:nativeloader:0.10.5'\n    implementation 'com.facebook.fbjni:fbjni-java-only:0.2.2'\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Softmax Operations with Backward Pass Support\nDESCRIPTION: This snippet illustrates both the forward (aten._softmax.default) and backward (aten._softmax_backward_data.default) operations performed on a tensor using the softmax function. It supports half-precision float tensors, emphasizing compatibility for optimized compute scenarios within deep learning models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForQuestionAnswering_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 12, ((T([4, 12, 512, 512], f16), -1, False), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax_backward_data.default\ncnt: 12, ((T([4, 12, 512, 512], f16), T([4, 12, 512, 512], f16), -1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Creating ThreadPoolExecutor for Asynchronous Model Forward Pass in PyTorch\nDESCRIPTION: Implements a ThreadPoolExecutor with a single worker to asynchronously run the model's forward pass and response step. This modification aims to prevent blocking while polling the request queue.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nThreadPoolExecutor(max_workers=1)\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Native Layer Norm Backward in PyTorch\nDESCRIPTION: Documents \\\"aten.native_layer_norm_backward.default\\\" operator for backward pass of layer normalization detailing its parameter use.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_20\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.native_layer_norm_backward.default\ncnt: 25, ((T([8, 128, 1024], f16), T([8, 128, 1024], f16), [1024], T([8, 128, 1], f32), T([8, 128, 1], f32), T([1024], f16), T([1024], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch CPU Stream Classes\nDESCRIPTION: Documentation section listing stream-related classes for CPU operations in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cpu.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nStream\n```\n\n----------------------------------------\n\nTITLE: Generating Commit for Specific CI Jobs\nDESCRIPTION: Uses a Python script to generate a commit that limits CI to run only specific jobs and their dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\n# --job: specify one or more times to filter to a specific job + its dependencies\n```\n\n----------------------------------------\n\nTITLE: Complete Minifier Usage Example\nDESCRIPTION: Full example demonstrating how to use the minifier with a serialized graph and custom checker function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/minifier.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.fx as fx\nfrom functorch.compile import minifier, check_nvfuser_subprocess, check_nvfuser_correctness_subprocess\n\ninps = [(torch.Size([3]), torch.float32), (torch.Size([3]), torch.float32)]\ninps = [torch.ones(shape, dtype=dtype) for (shape, dtype) in inps]\nfrom foo import FxModule\nmod = FxModule()\n\nminifier(fx.symbolic_trace(mod), inps, pass_checker)\n```\n\n----------------------------------------\n\nTITLE: Mask Function Implementation in PyTorch\nDESCRIPTION: Example implementation of a mask function that creates a binary mask based on a threshold value.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef mask_fn(tensor, threshold):  # threshold is the sparse config here\n    mask = torch.ones_like(tensor)\n    mask[torch.abs(tensor) < threshold] = 0.0\n    return mask\n```\n\n----------------------------------------\n\nTITLE: Tracking Convolution Backward Operations in PyTorch\nDESCRIPTION: Documentation of backward pass operations for convolutions with their gradient tensors, weights, and configuration parameters. These operations handle both 3x3 and 1x1 convolutions at different spatial resolutions with half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([4, 32, 7, 7], f16, stride=(50176, 49, 7, 1)), T([4, 128, 7, 7], f16), T([32, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding cuDNN Dependency in CMake\nDESCRIPTION: Checks if the `CAFFE2_USE_CUDNN` variable is true. If it is, the cuDNN libraries (specified by the `CUDNN_LIBRARIES` variable) are appended to the `ATen_CUDA_DEPENDENCY_LIBS` list.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\n  if(CAFFE2_USE_CUDNN)\n    list(APPEND ATen_CUDA_DEPENDENCY_LIBS ${CUDNN_LIBRARIES})\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Adding fbjni Subdirectory Dependency in CMake\nDESCRIPTION: Sets the path to the fbjni library source (`fbjni_DIR`) and defines a build directory for it (`fbjni_BUILD_DIR`) based on the main build subdirectory (`BUILD_SUBDIR`). It then includes the fbjni library build process as a subdirectory using `add_subdirectory`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nset(fbjni_DIR ${CMAKE_CURRENT_LIST_DIR}/../libs/fbjni/)\nset(fbjni_BUILD_DIR ${CMAKE_BINARY_DIR}/fbjni/${BUILD_SUBDIR})\n\nadd_subdirectory(${fbjni_DIR} ${fbjni_BUILD_DIR})\n```\n\n----------------------------------------\n\nTITLE: Configuring SVE Performance Kernels in CMake\nDESCRIPTION: Sets up a static library for SVE (Scalable Vector Extension) optimized performance kernels when the compiler supports SVE extensions. Adds necessary compiler flags and links the resulting interface library to the main project.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/perfkernels/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# We will only build the SVE perfkernel files if the compiler supports SVE\n# extensions.\nif(CXX_SVE_FOUND)\n  add_library(Caffe2_perfkernels_sve STATIC ${sve_srcs})\n  target_link_libraries(Caffe2_perfkernels_sve PRIVATE c10)\n  install(TARGETS Caffe2_perfkernels_sve\n      ARCHIVE DESTINATION \"${CMAKE_INSTALL_LIBDIR}\")\n\n  target_compile_options(Caffe2_perfkernels_sve PRIVATE \"-march=armv8-a+sve\")\n\n  caffe2_interface_library(\n      Caffe2_perfkernels_sve Caffe2_perfkernels_sve_interface)\n  list(APPEND\n       Caffe2_DEPENDENCY_WHOLE_LINK_LIBS \"Caffe2_perfkernels_sve_interface\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: PyTorch Neural Network Operations - Embeddings and Activation\nDESCRIPTION: Neural network specific operations including embedding lookups, GELU activation functions, and their backward passes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Bert_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.embedding.default((T([30522, 768], f16), T([4, 512], i64), 0), {})\naten.gelu.default((T([4, 512, 3072], f16),), {})\naten.native_layer_norm.default((T([4, 512, 768], f16), [768], T([768], f16), T([768], f16), 1e-12), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Python Class with a Docstring\nDESCRIPTION: This snippet defines a class named 'LintInitClass' with a docstring and an __init__ method containing placeholder comments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass LintInitClass:\n    \"\"\"This is a very long comment, a very long comment\"\"\"\n\n    def __init__(self) -> None:\n        # Lots of lines!\n        # Lots of lines!\n        pass\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations in PyTorch\nDESCRIPTION: Multiple convolution operations with various input shapes, kernel sizes, and strides. These operations are typical in convolutional neural networks, with some likely being part of residual blocks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([128, 1, 128], f16), T([1, 1, 5], f16), None, [1], [2], [1], False, [0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([128, 128, 32, 32], f16), T([512, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Conversion with Copy in PyTorch (Python)\nDESCRIPTION: Utilizes aten._to_copy to copy tensor data while optionally changing its dtype, noted by the conversion from f32 to f16, a common practice for memory and computation efficiency during training pipeline.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\naten._to_copy.default\ncnt: 1, ((T([16, 1, 1, 512], f32),), {\"dtype\": f16})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization\nDESCRIPTION: These snippets show batch normalization operations applied to tensors of various shapes, using float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n((T([1, 16, 27], f16), T([16], f16), None, None, None, True, 0.0, 1e-05), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([1, 32, 144], f16), T([32], f16), None, None, None, True, 0.0, 1e-05), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([1, 64, 288], f16), T([64], f16), None, None, None, True, 0.0, 1e-05), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([1, 128, 576], f16), T([128], f16), None, None, None, True, 0.0, 1e-05), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([1, 256, 128], f16), T([256], f16), None, None, None, True, 0.0, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Summation in PyTorch\nDESCRIPTION: Computes the sum of all elements in the input tensor or along specified dimensions, with options for keeping the original dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\naten.sum.SymInt((T([64, 1000], f16, stride=(0, 0)), [0], True), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.sum.SymInt((T([64, 4096], f16), [0], True), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.sum.default((T([64, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Adjusting Build Options on Linux - CMake Only Build and GUI - bash\nDESCRIPTION: This snippet shows how to generate build files for CMake without compiling, allowing manual adjustment of CMake settings (e.g., CuDNN or BLAS paths). Then use ccmake or cmake-gui to make changes interactively. Prerequisites: CMake, ccmake/cmake-gui, and all PyTorch dependencies installed. Only run on Linux.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\\npython setup.py build --cmake-only\\nccmake build  # or cmake-gui build\n```\n\n----------------------------------------\n\nTITLE: Using torch.distributed.rpc.rpc_sync() in TorchScript\nDESCRIPTION: Makes a blocking RPC call to run a function on a remote worker. RPC messages are sent and received in parallel to execution of Python code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.rpc.rpc_sync()\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Directive for PyTorch Numeric Suite\nDESCRIPTION: ReStructuredText documentation directives defining the structure and content for the PyTorch numeric suite module documentation. Includes warning about prototype status and automodule configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _torch_ao_ns_numeric_suite:\n\ntorch.ao.ns._numeric_suite\n--------------------------\n\n.. warning ::\n     This module is an early prototype and is subject to change.\n\n.. currentmodule:: torch.ao.ns._numeric_suite\n\n.. automodule:: torch.ao.ns._numeric_suite\n    :members:\n    :member-order: bysource\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operation Statistics for Transformer Model\nDESCRIPTION: A comprehensive list of PyTorch operators used in a transformer model implementation, showing operation counts, tensor shapes, strides, and data types. The operations include softmax, tensor reshaping, matrix multiplication, and other essential computations for transformer attention mechanisms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/coat_lite_mini_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 2, ((T([128, 8, 3137, 8], f16, stride=(602304, 8, 192, 1)), 2, False), {})\ncnt: 2, ((T([128, 8, 785, 16], f16, stride=(301440, 16, 384, 1)), 2, False), {})\ncnt: 2, ((T([128, 8, 197, 40], f16, stride=(189120, 40, 960, 1)), 2, False), {})\ncnt: 2, ((T([128, 8, 50, 64], f16, stride=(76800, 64, 1536, 1)), 2, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 2, ((T([128, 8, 50, 64], f16, stride=(25600, 3200, 1, 50)), T([128, 8, 50, 64], f16), 2, f16), {})\ncnt: 2, ((T([128, 8, 197, 40], f16, stride=(63040, 7880, 1, 197)), T([128, 8, 197, 40], f16), 2, f16), {})\ncnt: 2, ((T([128, 8, 785, 16], f16, stride=(100480, 12560, 1, 785)), T([128, 8, 785, 16], f16), 2, f16), {})\ncnt: 2, ((T([128, 8, 3137, 8], f16, stride=(200768, 25096, 1, 3137)), T([128, 8, 3137, 8], f16), 2, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 6, ((T([128, 8, 3137, 8], f16), [1024, 3137, 8]), {})\ncnt: 2, ((T([1024, 8, 8], f16), [128, 8, 8, 8]), {})\ncnt: 2, ((T([1024, 3137, 8], f16), [128, 8, 3137, 8]), {})\ncnt: 2, ((T([128, 3137, 8, 8], f16), [128, 3137, 64]), {})\ncnt: 6, ((T([128, 8, 785, 16], f16), [1024, 785, 16]), {})\ncnt: 2, ((T([1024, 16, 16], f16), [128, 8, 16, 16]), {})\ncnt: 2, ((T([1024, 785, 16], f16), [128, 8, 785, 16]), {})\ncnt: 2, ((T([128, 785, 8, 16], f16), [128, 785, 128]), {})\ncnt: 6, ((T([128, 8, 197, 40], f16), [1024, 197, 40]), {})\ncnt: 2, ((T([1024, 40, 40], f16), [128, 8, 40, 40]), {})\ncnt: 2, ((T([1024, 197, 40], f16), [128, 8, 197, 40]), {})\ncnt: 2, ((T([128, 197, 8, 40], f16), [128, 197, 320]), {})\ncnt: 6, ((T([128, 8, 50, 64], f16), [1024, 50, 64]), {})\ncnt: 2, ((T([1024, 64, 64], f16), [128, 8, 64, 64]), {})\ncnt: 2, ((T([1024, 50, 64], f16), [128, 8, 50, 64]), {})\ncnt: 2, ((T([128, 50, 8, 64], f16), [128, 50, 512]), {})\ncnt: 2, ((T([128, 50, 3, 8, 64], f16), [128, 50, 1536]), {})\ncnt: 2, ((T([128, 197, 3, 8, 40], f16), [128, 197, 960]), {})\ncnt: 2, ((T([128, 785, 3, 8, 16], f16), [128, 785, 384]), {})\ncnt: 2, ((T([128, 3137, 3, 8, 8], f16), [128, 3137, 192]), {})\nOperator: aten.add.Tensor\ncnt: 2, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16, stride=(200768, 1, 3584, 64))), {})\ncnt: 6, ((T([128, 8, 3137, 8], f16), T([128, 8, 3137, 8], f16)), {})\ncnt: 10, ((T([128, 3137, 64], f16), T([128, 3137, 64], f16)), {})\ncnt: 2, ((T([128, 128, 28, 28], f16), T([128, 128, 28, 28], f16, stride=(100480, 1, 3584, 128))), {})\ncnt: 6, ((T([128, 8, 785, 16], f16), T([128, 8, 785, 16], f16)), {})\ncnt: 10, ((T([128, 785, 128], f16), T([128, 785, 128], f16)), {})\ncnt: 2, ((T([128, 320, 14, 14], f16), T([128, 320, 14, 14], f16, stride=(63040, 1, 4480, 320))), {})\ncnt: 6, ((T([128, 8, 197, 40], f16), T([128, 8, 197, 40], f16)), {})\ncnt: 10, ((T([128, 197, 320], f16), T([128, 197, 320], f16)), {})\ncnt: 2, ((T([128, 512, 7, 7], f16), T([128, 512, 7, 7], f16, stride=(25600, 1, 3584, 512))), {})\ncnt: 6, ((T([128, 8, 50, 64], f16), T([128, 8, 50, 64], f16)), {})\ncnt: 10, ((T([128, 50, 512], f16), T([128, 50, 512], f16)), {})\ncnt: 4, ((T([3, 128, 8, 50, 64], f16), T([3, 128, 8, 50, 64], f16)), {})\ncnt: 2, ((T([128, 512, 7, 7], f16, stride=(25600, 1, 3584, 512)), T([128, 512, 7, 7], f16, stride=(25088, 1, 3584, 512))), {})\ncnt: 1, ((T([192, 1, 7, 7], f16), T([192, 1, 7, 7], f16)), {})\ncnt: 2, ((T([192], f16), T([192], f16)), {})\ncnt: 1, ((T([192, 1, 5, 5], f16), T([192, 1, 5, 5], f16)), {})\ncnt: 2, ((T([128, 1, 3, 3], f16), T([128, 1, 3, 3], f16)), {})\ncnt: 2, ((T([128], f16), T([128], f16)), {})\ncnt: 1, ((T([512, 1, 3, 3], f16), T([512, 1, 3, 3], f16)), {})\ncnt: 1, ((T([512], f16), T([512], f16)), {})\ncnt: 4, ((T([3, 128, 8, 197, 40], f16), T([3, 128, 8, 197, 40], f16)), {})\ncnt: 2, ((T([128, 320, 14, 14], f16, stride=(63040, 1, 4480, 320)), T([128, 320, 14, 14], f16, stride=(62720, 1, 4480, 320))), {})\ncnt: 1, ((T([120, 1, 7, 7], f16), T([120, 1, 7, 7], f16)), {})\ncnt: 2, ((T([120], f16), T([120], f16)), {})\ncnt: 1, ((T([120, 1, 5, 5], f16), T([120, 1, 5, 5], f16)), {})\ncnt: 1, ((T([80, 1, 3, 3], f16), T([80, 1, 3, 3], f16)), {})\ncnt: 1, ((T([80], f16), T([80], f16)), {})\ncnt: 1, ((T([320, 1, 3, 3], f16), T([320, 1, 3, 3], f16)), {})\ncnt: 1, ((T([320], f16), T([320], f16)), {})\ncnt: 4, ((T([3, 128, 8, 785, 16], f16), T([3, 128, 8, 785, 16], f16)), {})\ncnt: 2, ((T([128, 128, 28, 28], f16, stride=(100480, 1, 3584, 128)), T([128, 128, 28, 28], f16, stride=(100352, 1, 3584, 128))), {})\ncnt: 1, ((T([48, 1, 7, 7], f16), T([48, 1, 7, 7], f16)), {})\ncnt: 2, ((T([48], f16), T([48], f16)), {})\ncnt: 1, ((T([48, 1, 5, 5], f16), T([48, 1, 5, 5], f16)), {})\ncnt: 1, ((T([32, 1, 3, 3], f16), T([32, 1, 3, 3], f16)), {})\ncnt: 1, ((T([32], f16), T([32], f16)), {})\ncnt: 4, ((T([3, 128, 8, 3137, 8], f16), T([3, 128, 8, 3137, 8], f16)), {})\ncnt: 2, ((T([128, 64, 56, 56], f16, stride=(200768, 1, 3584, 64)), T([128, 64, 56, 56], f16, stride=(200704, 1, 3584, 64))), {})\ncnt: 1, ((T([24, 1, 7, 7], f16), T([24, 1, 7, 7], f16)), {})\ncnt: 2, ((T([24], f16), T([24], f16)), {})\ncnt: 1, ((T([24, 1, 5, 5], f16), T([24, 1, 5, 5], f16)), {})\ncnt: 1, ((T([16, 1, 3, 3], f16), T([16, 1, 3, 3], f16)), {})\ncnt: 1, ((T([16], f16), T([16], f16)), {})\ncnt: 1, ((T([64, 1, 3, 3], f16), T([64, 1, 3, 3], f16)), {})\ncnt: 1, ((T([64], f16), T([64], f16)), {})\nOperator: aten.addmm.default\ncnt: 2, ((T([192], f16), T([401536, 64], f16), T([64, 192], f16, stride=(1, 64))), {})\ncnt: 2, ((T([64], f16), T([401536, 64], f16), T([64, 64], f16, stride=(1, 64))), {})\ncnt: 2, ((T([512], f16), T([401536, 64], f16), T([64, 512], f16, stride=(1, 64))), {})\ncnt: 2, ((T([64], f16), T([401536, 512], f16), T([512, 64], f16, stride=(1, 512))), {})\ncnt: 2, ((T([384], f16), T([100480, 128], f16), T([128, 384], f16, stride=(1, 128))), {})\ncnt: 2, ((T([128], f16), T([100480, 128], f16), T([128, 128], f16, stride=(1, 128))), {})\ncnt: 2, ((T([1024], f16), T([100480, 128], f16), T([128, 1024], f16, stride=(1, 128))), {})\ncnt: 2, ((T([128], f16), T([100480, 1024], f16), T([1024, 128], f16, stride=(1, 1024))), {})\ncnt: 2, ((T([960], f16), T([25216, 320], f16), T([320, 960], f16, stride=(1, 320))), {})\ncnt: 2, ((T([320], f16), T([25216, 320], f16), T([320, 320], f16, stride=(1, 320))), {})\ncnt: 2, ((T([1280], f16), T([25216, 320], f16), T([320, 1280], f16, stride=(1, 320))), {})\ncnt: 2, ((T([320], f16), T([25216, 1280], f16), T([1280, 320], f16, stride=(1, 1280))), {})\ncnt: 2, ((T([1536], f16), T([6400, 512], f16), T([512, 1536], f16, stride=(1, 512))), {})\ncnt: 2, ((T([512], f16), T([6400, 512], f16), T([512, 512], f16, stride=(1, 512))), {})\ncnt: 2, ((T([2048], f16), T([6400, 512], f16), T([512, 2048], f16, stride=(1, 512))), {})\ncnt: 2, ((T([512], f16), T([6400, 2048], f16), T([2048, 512], f16, stride=(1, 2048))), {})\ncnt: 1, ((T([1000], f16), T([128, 512], f16, stride=(25600, 1)), T([512, 1000], f16, stride=(1, 512))), {})\nOperator: aten.bmm.default\ncnt: 4, ((T([1024, 8, 3137], f16, stride=(25096, 1, 8)), T([1024, 3137, 8], f16)), {})\ncnt: 4, ((T([1024, 3137, 8], f16), T([1024, 8, 8], f16)), {})\ncnt: 4, ((T([1024, 16, 785], f16, stride=(12560, 1, 16)), T([1024, 785, 16], f16)), {})\ncnt: 4, ((T([1024, 785, 16], f16), T([1024, 16, 16], f16)), {})\ncnt: 4, ((T([1024, 40, 197], f16, stride=(7880, 1, 40)), T([1024, 197, 40], f16)), {})\ncnt: 4, ((T([1024, 197, 40], f16), T([1024, 40, 40], f16)), {})\ncnt: 4, ((T([1024, 64, 50], f16, stride=(3200, 1, 64)), T([1024, 50, 64], f16)), {})\ncnt: 4, ((T([1024, 50, 64], f16), T([1024, 64, 64], f16)), {})\ncnt: 2, ((T([1024, 50, 64], f16), T([1024, 64, 64], f16, stride=(4096, 1, 64))), {})\ncnt: 2, ((T([1024, 64, 64], f16), T([1024, 64, 50], f16, stride=(3200, 1, 64))), {})\ncnt: 2, ((T([1024, 197, 40], f16), T([1024, 40, 40], f16, stride=(1600, 1, 40))), {})\ncnt: 2, ((T([1024, 40, 40], f16), T([1024, 40, 197], f16, stride=(7880, 1, 40))), {})\ncnt: 2, ((T([1024, 785, 16], f16), T([1024, 16, 16], f16, stride=(256, 1, 16))), {})\ncnt: 2, ((T([1024, 16, 16], f16), T([1024, 16, 785], f16, stride=(12560, 1, 16))), {})\ncnt: 2, ((T([1024, 3137, 8], f16), T([1024, 8, 8], f16, stride=(64, 1, 8))), {})\ncnt: 2, ((T([1024, 8, 8], f16), T([1024, 8, 3137], f16, stride=(25096, 1, 8))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([128, 1, 64], f16, stride=(0, 64, 1)), T([128, 3136, 64], f16)], 1), {})\ncnt: 2, (([T([128, 1, 64], f16, stride=(200768, 64, 1)), T([128, 3136, 64], f16, stride=(200704, 1, 3136))], 1), {})\ncnt: 2, (([T([128, 16, 56, 56], f16), T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)], 1), {})\ncnt: 1, (([T([128, 1, 128], f16, stride=(0, 128, 1)), T([128, 784, 128], f16)], 1), {})\ncnt: 2, (([T([128, 1, 128], f16, stride=(100480, 128, 1)), T([128, 784, 128], f16, stride=(100352, 1, 784))], 1), {})\ncnt: 2, (([T([128, 32, 28, 28], f16), T([128, 48, 28, 28], f16), T([128, 48, 28, 28], f16)], 1), {})\ncnt: 1, (([T([128, 1, 320], f16, stride=(0, 320, 1)), T([128, 196, 320], f16)], 1), {})\ncnt: 2, (([T([128, 1, 320], f16, stride=(63040, 320, 1)), T([128, 196, 320], f16, stride=(62720, 1, 196))], 1), {})\ncnt: 2, (([T([128, 80, 14, 14], f16), T([128, 120, 14, 14], f16), T([128, 120, 14, 14], f16)], 1), {})\ncnt: 1, (([T([128, 1, 512], f16, stride=(0, 512, 1)), T([128, 49, 512], f16)], 1), {})\ncnt: 2, (([T([128, 1, 512], f16, stride=(25600, 512, 1)), T([128, 49, 512], f16, stride=(25088, 1, 49))], 1), {})\ncnt: 2, (([T([128, 128, 7, 7], f16), T([128, 192, 7, 7], f16), T([128, 192, 7, 7], f16)], 1), {})\ncnt: 2, (([T([128, 128, 7, 7], f16, stride=(6272, 1, 896, 128)), T([128, 192, 7, 7], f16, stride=(9408, 1, 1344, 192)), T([128, 192, 7, 7], f16, stride=(9408, 1, 1344, 192))], 1), {})\ncnt: 2, (([T([128, 80, 14, 14], f16, stride=(15680, 1, 1120, 80)), T([128, 120, 14, 14], f16, stride=(23520, 1, 1680, 120)), T([128, 120, 14, 14], f16, stride=(23520, 1, 1680, 120))], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring C10 Library Basic Properties in CMake\nDESCRIPTION: Sets up the minimum CMake version, project name, and basic build properties for the C10 library. Establishes C++17 as the standard and enables export of compile commands.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\nproject(c10 CXX)\n\nset(CMAKE_CXX_STANDARD 17 CACHE STRING \"The C++ standard whose features are requested to build this target.\")\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n```\n\n----------------------------------------\n\nTITLE: Summing Tensor Elements in PyTorch\nDESCRIPTION: This snippet computes the sum of all elements in a tensor. It's often used in loss calculations or for reducing tensor dimensions in neural network architectures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 12096, 85], f16),), {})\ncnt: 1, ((T([8, 3, 12, 16, 85], f16),), {})\ncnt: 1, ((T([8, 3, 24, 32, 85], f16),), {})\ncnt: 1, ((T([8, 3, 48, 64, 85], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Role in reStructuredText\nDESCRIPTION: Defines a hidden role for use in reStructuredText documentation, which can be used to create hidden sections.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.optim.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom TorchScript Operation Library\nDESCRIPTION: Builds a shared library for custom TorchScript operations with conditional CUDA/ROCm support and links against LibTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/aoti_inference/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(aoti_custom_class SHARED aoti_custom_class.cpp)\nset_target_properties(aoti_custom_class PROPERTIES\n    LIBRARY_OUTPUT_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR})\nif(USE_CUDA)\n  target_compile_definitions(aoti_custom_class PRIVATE USE_CUDA)\nelseif(USE_ROCM)\n    target_compile_definitions(aoti_custom_class PRIVATE USE_ROCM)\nendif()\ntarget_link_libraries(aoti_custom_class torch)\n```\n\n----------------------------------------\n\nTITLE: Using aten.nll_loss_backward Operator in PyTorch\nDESCRIPTION: This snippet focuses on the aten.nll_loss_backward.default operation in PyTorch, which calculates the derivative of the negative log likelihood loss function. It requires a prediction tensor, target indices, and additional scalar values for operations. The expected input is a probability distribution and target indices for the corresponding labels. Outputs include the gradient with respect to the input probabilities. It is crucial for training classifiers and assumes appropriate input sizes and matching shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Applying aten.relu_ Activation in PyTorch\nDESCRIPTION: This function uses aten.relu_.default, a PyTorch operator applying the Rectified Linear Unit (ReLU) activation function on tensors. Key dependencies include PyTorch library for deep learning operations. Inputs are multi-dimensional tensors, and the output is the same tensor with ReLU applied, yielding non-negative values, suitable for introducing non-linearity in neural networks. Input tensors must be of compatible type and dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([64, 32, 112, 112], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 192, 112, 112], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Setting and Propagating ATen Build Variables with CMake - CMake\nDESCRIPTION: This snippet uses the CMake 'set' command to export multiple source, test, include, and library list variables related to the ATen backend in the PyTorch project to the parent scope. This enables modular CMake scripts to define and pass build information up the hierarchy, supporting CPU, CUDA, XPU, HIP, MPS, Vulkan, and mobile backends. Each set statement assigns the variable's current value and appends 'PARENT_SCOPE', ensuring that parent scripts (or higher-level CMakeLists) can access these settings. Dependencies include the CMake build system and previously defined variables, and parameters include specific backend or feature groupings. The input is the CMake environment and variable values, while outputs are the variables being visible in the parent scope; correct definitions upstream are required.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_32\n\nLANGUAGE: CMake\nCODE:\n```\n# Pass source, includes, and libs to parent\nset(ATen_CORE_SRCS ${ATen_CORE_SRCS} PARENT_SCOPE)\nset(ATen_CPU_SRCS ${ATen_CPU_SRCS} PARENT_SCOPE)\nset(ATen_XPU_SRCS ${ATen_XPU_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_CU_SRCS ${ATen_CUDA_CU_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_CPP_SRCS ${ATen_CUDA_CPP_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_LINALG_SRCS ${ATen_CUDA_LINALG_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_SRCS_W_SORT_BY_KEY ${ATen_CUDA_SRCS_W_SORT_BY_KEY} PARENT_SCOPE)\nset(ATen_CUDA_CU_SRCS_W_SORT_BY_KEY ${ATen_CUDA_CU_SRCS_W_SORT_BY_KEY} PARENT_SCOPE)\nset(ATen_NVRTC_STUB_SRCS ${ATen_NVRTC_STUB_SRCS} PARENT_SCOPE)\nset(ATen_HIP_SRCS ${ATen_HIP_SRCS} PARENT_SCOPE)\nset(ATen_MPS_SRCS ${ATen_MPS_SRCS} PARENT_SCOPE)\nset(ATen_XPU_SRCS ${ATen_XPU_SRCS} PARENT_SCOPE)\nset(ATen_QUANTIZED_SRCS ${ATen_QUANTIZED_SRCS} PARENT_SCOPE)\nset(ATen_CPU_TEST_SRCS ${ATen_CPU_TEST_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_TEST_SRCS ${ATen_CUDA_TEST_SRCS} PARENT_SCOPE)\nset(ATen_XPU_TEST_SRCS ${ATen_XPU_TEST_SRCS} PARENT_SCOPE)\nset(ATen_CORE_TEST_SRCS ${ATen_CORE_TEST_SRCS} PARENT_SCOPE)\nset(ATen_HIP_TEST_SRCS ${ATen_HIP_TEST_SRCS} PARENT_SCOPE)\nset(ATen_VULKAN_TEST_SRCS ${ATen_VULKAN_TEST_SRCS} PARENT_SCOPE)\nset(ATen_MOBILE_BENCHMARK_SRCS ${ATen_MOBILE_BENCHMARK_SRCS} PARENT_SCOPE)\nset(ATen_MOBILE_TEST_SRCS ${ATen_MOBILE_TEST_SRCS} ${ATen_VULKAN_TEST_SRCS} PARENT_SCOPE)\nset(ATen_VEC_TEST_SRCS  ${ATen_VEC_TEST_SRCS} PARENT_SCOPE)\nset(ATen_QUANTIZED_TEST_SRCS ${ATen_QUANTIZED_TEST_SRCS} PARENT_SCOPE)\nset(ATen_MPS_TEST_SRCS ${ATen_MPS_TEST_SRCS} PARENT_SCOPE)\nset(ATen_CPU_INCLUDE ${ATen_CPU_INCLUDE} PARENT_SCOPE)\nset(ATen_THIRD_PARTY_INCLUDE ${ATen_THIRD_PARTY_INCLUDE} PARENT_SCOPE)\nset(ATen_CUDA_INCLUDE ${ATen_CUDA_INCLUDE} PARENT_SCOPE)\nset(ATen_HIP_INCLUDE ${ATen_HIP_INCLUDE} PARENT_SCOPE)\nset(ATen_XPU_INCLUDE ${ATen_XPU_INCLUDE} PARENT_SCOPE)\nset(ATen_VULKAN_INCLUDE ${ATen_VULKAN_INCLUDE} PARENT_SCOPE)\nset(ATen_CPU_DEPENDENCY_LIBS ${ATen_CPU_DEPENDENCY_LIBS} PARENT_SCOPE)\nset(ATen_CUDA_DEPENDENCY_LIBS ${ATen_CUDA_DEPENDENCY_LIBS} PARENT_SCOPE)\nset(ATen_XPU_DEPENDENCY_LIBS ${ATen_XPU_DEPENDENCY_LIBS} PARENT_SCOPE)\nset(ATen_HIP_DEPENDENCY_LIBS ${ATen_HIP_DEPENDENCY_LIBS} PARENT_SCOPE)\nset(FLASH_ATTENTION_CUDA_SOURCES ${FLASH_ATTENTION_CUDA_SOURCES} PARENT_SCOPE)\nset(MEM_EFF_ATTENTION_CUDA_SOURCES ${MEM_EFF_ATTENTION_CUDA_SOURCES} PARENT_SCOPE)\nset(ATen_ATTENTION_KERNEL_SRCS ${ATen_ATTENTION_KERNEL_SRCS} PARENT_SCOPE)\n\n```\n\n----------------------------------------\n\nTITLE: Executing CUDNN RNN Forward Pass in PyTorch\nDESCRIPTION: Performs a forward pass of a CUDNN RNN layer with specified input tensors, weights, and configurations. Uses half-precision (f16) tensors for input and weights.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\naten._cudnn_rnn.default(T([64, 50, 40], f16), [T([3072, 40], f16), T([3072, 768], f16), T([3072], f16), T([3072], f16)], 4, None, T([1, 64, 768], f16), T([1, 64, 768], f16), 2, 768, 0, 1, True, 0.0, True, False, [], None)\n```\n\n----------------------------------------\n\nTITLE: Initializing CMake Project for PyTorch JNI\nDESCRIPTION: Sets the minimum required CMake version, defines an option `BUILD_LITE_INTERPRETER` to control which JNI variant is built (defaulting to ON), displays the status of this option, and sets the project name and target variable (`PYTORCH_JNI_TARGET`) based on the option's value.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.4.1)\noption(BUILD_LITE_INTERPRETER \"Master flag to build pytorch_jni_lite\" ON)\nmessage(\n  STATUS\n  \"BUILD_LITE_INTERPRETER (pytorch_jni_lite): ${BUILD_LITE_INTERPRETER}\")\n\nif(BUILD_LITE_INTERPRETER)\n  project(pytorch_jni_lite CXX)\n  set(PYTORCH_JNI_TARGET pytorch_jni_lite)\nelse()\n  project(pytorch_jni CXX)\n  set(PYTORCH_JNI_TARGET pytorch_jni)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Selecting JNI Source Files Based on Build Type in CMake\nDESCRIPTION: Uses `file(GLOB ...)` to populate the `pytorch_android_SOURCES` variable with the appropriate C++ source files. If `BUILD_LITE_INTERPRETER` is true, it includes `pytorch_jni_lite.cpp`; otherwise, it includes `pytorch_jni_jit.cpp`. Both configurations include the common JNI files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_LITE_INTERPRETER)\n  file(GLOB pytorch_android_SOURCES\n    ${pytorch_android_DIR}/pytorch_jni_lite.cpp\n    ${pytorch_android_DIR}/pytorch_jni_common.cpp\n    ${pytorch_android_DIR}/pytorch_jni_common.h\n  )\nelse()\n  file(GLOB pytorch_android_SOURCES\n    ${pytorch_android_DIR}/pytorch_jni_jit.cpp\n    ${pytorch_android_DIR}/pytorch_jni_common.cpp\n    ${pytorch_android_DIR}/pytorch_jni_common.h\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Listing All Benchmark Tests\nDESCRIPTION: Shows all individual tests available across all operators in the benchmark suite.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython -m benchmark_all_test --list-tests\n```\n\n----------------------------------------\n\nTITLE: Profiling Multiple aten Operator Calls - PyTorch - Python\nDESCRIPTION: These snippets list calls to various aten operators in PyTorch with tensor descriptor tuples, parameter details, and invocation counts. Dependencies include having PyTorch installed and a working knowledge of its tensor API. Each call records the tensor shape, data type (e.g., f16, i64), potential stride, and operator-specific parameters; the structure is useful for profiling or reproducing workloads for benchmarking, fuzzing, or test generation. The inputs are tuples containing tensor properties and operator arguments, and the outputs are typically results of those operations or information for test harnesses.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pnasnet5large_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([16, 54, 83, 83], f16), T([16, 54, 167, 167], f16), T([54, 1, 3, 3], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 54, [True, True, False]), {})\ncnt: 2, ((T([16, 54, 83, 83], f16), T([16, 54, 83, 83], f16), T([54, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 54, [True, True, False]), {})\ncnt: 1, ((T([16, 54, 83, 83], f16), T([16, 54, 169, 169], f16), T([54, 1, 5, 5], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 54, [True, True, False]), {})\ncnt: 1, ((T([16, 54, 83, 83], f16), T([16, 54, 83, 83], f16), T([54, 1, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 54, [True, True, False]), {})\ncnt: 1, ((T([16, 54, 83, 83], f16), T([16, 54, 171, 171], f16), T([54, 1, 7, 7], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 54, [True, True, False]), {})\ncnt: 1, ((T([16, 96, 83, 83], f16), T([16, 96, 169, 169], f16), T([96, 1, 5, 5], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 96, [True, True, False]), {})\ncnt: 1, ((T([16, 54, 165, 165], f16), T([16, 96, 165, 165], f16), T([54, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 96, 165, 165], f16), T([16, 3, 331, 331], f16), T([96, 3, 3, 3], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([16, 3, 331, 331], f16), T([16, 3, 331, 331], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([16, 4320, 11, 11], f16, stride=(4320, 1, 0, 0)), 121), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([16], i64),), {})\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([16, 96, 167, 167], f16), [3, 3], [2, 2]), {})\ncnt: 2, ((T([16, 54, 167, 167], f16), [3, 3], [2, 2]), {})\ncnt: 3, ((T([16, 108, 85, 85], f16), [3, 3], [2, 2]), {})\ncnt: 12, ((T([16, 216, 42, 42], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 3, ((T([16, 432, 43, 43], f16), [3, 3], [2, 2]), {})\ncnt: 9, ((T([16, 432, 21, 21], f16), [3, 3], [1, 1], [1, 1]), {})\ncnt: 3, ((T([16, 864, 23, 23], f16), [3, 3], [2, 2]), {})\ncnt: 9, ((T([16, 864, 11, 11], f16), [3, 3], [1, 1], [1, 1]), {})\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 9, ((T([16, 864, 11, 11], f16, stride=(522720, 121, 11, 1)), T([16, 864, 11, 11], f16), [3, 3], [1, 1], [1, 1], [1, 1], False, T([16, 864, 11, 11], i64)), {})\ncnt: 3, ((T([16, 864, 11, 11], f16, stride=(522720, 121, 11, 1)), T([16, 864, 23, 23], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 864, 11, 11], i64)), {})\ncnt: 9, ((T([16, 432, 21, 21], f16, stride=(952560, 441, 21, 1)), T([16, 432, 21, 21], f16), [3, 3], [1, 1], [1, 1], [1, 1], False, T([16, 432, 21, 21], i64)), {})\ncnt: 3, ((T([16, 432, 21, 21], f16, stride=(952560, 441, 21, 1)), T([16, 432, 43, 43], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 432, 21, 21], i64)), {})\ncnt: 12, ((T([16, 216, 42, 42], f16, stride=(1905120, 1764, 42, 1)), T([16, 216, 42, 42], f16), [3, 3], [1, 1], [1, 1], [1, 1], False, T([16, 216, 42, 42], i64)), {})\ncnt: 3, ((T([16, 108, 42, 42], f16, stride=(952560, 1764, 42, 1)), T([16, 108, 85, 85], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 108, 42, 42], i64)), {})\ncnt: 2, ((T([16, 54, 83, 83], f16, stride=(1860030, 6889, 83, 1)), T([16, 54, 167, 167], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 54, 83, 83], i64)), {})\ncnt: 1, ((T([16, 96, 83, 83], f16), T([16, 96, 167, 167], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([16, 96, 83, 83], i64)), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([16, 4320, 11, 11], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([16, 1000], f16), T([1000, 4320], f16)), {})\ncnt: 1, ((T([1000, 16], f16, stride=(1, 1000)), T([16, 4320], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([16, 96, 165, 165], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 0.001), {})\ncnt: 1, ((T([16, 54, 165, 165], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f16), True, 0.1, 0.001), {})\ncnt: 14, ((T([16, 54, 83, 83], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f16), True, 0.1, 0.001), {})\ncnt: 2, ((T([16, 108, 83, 83], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f16), True, 0.1, 0.001), {})\ncnt: 13, ((T([16, 108, 42, 42], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f16), True, 0.1, 0.001), {})\ncnt: 56, ((T([16, 216, 42, 42], f16), T([216], f16), T([216], f16), T([216], f16), T([216], f16), True, 0.1, 0.001), {})\ncnt: 2, ((T([16, 432, 42, 42], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f16), True, 0.1, 0.001), {})\ncnt: 55, ((T([16, 432, 21, 21], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f16), True, 0.1, 0.001), {})\ncnt: 2, ((T([16, 864, 21, 21], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f16), True, 0.1, 0.001), {})\ncnt: 55, ((T([16, 864, 11, 11], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f16), True, 0.1, 0.001), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 17, ((T([16, 864, 11, 11], f16, stride=(522720, 121, 11, 1)), T([16, 864, 11, 11], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f32), T([864], f32), True, 0.001, [True, True, True]), {})\ncnt: 38, ((T([16, 864, 11, 11], f16), T([16, 864, 11, 11], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f32), T([864], f32), True, 0.001, [True, True, True]), {})\ncnt: 2, ((T([16, 864, 21, 21], f16), T([16, 864, 21, 21], f16), T([864], f16), T([864], f16), T([864], f16), T([864], f32), T([864], f32), True, 0.001, [True, True, True]), {})\ncnt: 17, ((T([16, 432, 21, 21], f16, stride=(952560, 441, 21, 1)), T([16, 432, 21, 21], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f32), T([432], f32), True, 0.001, [True, True, True]), {})\ncnt: 38, ((T([16, 432, 21, 21], f16), T([16, 432, 21, 21], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f32), T([432], f32), True, 0.001, [True, True, True]), {})\ncnt: 2, ((T([16, 432, 42, 42], f16), T([16, 432, 42, 42], f16), T([432], f16), T([432], f16), T([432], f16), T([432], f32), T([432], f32), True, 0.001, [True, True, True]), {})\ncnt: 16, ((T([16, 216, 42, 42], f16, stride=(1905120, 1764, 42, 1)), T([16, 216, 42, 42], f16), T([216], f16), T([216], f16), T([216], f16), T([216], f32), T([216], f32), True, 0.001, [True, True, True]), {})\ncnt: 40, ((T([16, 216, 42, 42], f16), T([16, 216, 42, 42], f16), T([216], f16), T([216], f16), T([216], f16), T([216], f32), T([216], f32), True, 0.001, [True, True, True]), {})\ncnt: 5, ((T([16, 108, 42, 42], f16, stride=(952560, 1764, 42, 1)), T([16, 108, 42, 42], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f32), T([108], f32), True, 0.001, [True, True, True]), {})\ncnt: 8, ((T([16, 108, 42, 42], f16), T([16, 108, 42, 42], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f32), T([108], f32), True, 0.001, [True, True, True]), {})\ncnt: 2, ((T([16, 108, 83, 83], f16), T([16, 108, 83, 83], f16), T([108], f16), T([108], f16), T([108], f16), T([108], f32), T([108], f32), True, 0.001, [True, True, True]), {})\ncnt: 6, ((T([16, 54, 83, 83], f16, stride=(1860030, 6889, 83, 1)), T([16, 54, 83, 83], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f32), T([54], f32), True, 0.001, [True, True, True]), {})\ncnt: 8, ((T([16, 54, 83, 83], f16), T([16, 54, 83, 83], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f32), T([54], f32), True, 0.001, [True, True, True]), {})\ncnt: 1, ((T([16, 54, 165, 165], f16), T([16, 54, 165, 165], f16), T([54], f16), T([54], f16), T([54], f16), T([54], f32), T([54], f32), True, 0.001, [True, True, True]), {})\ncnt: 1, ((T([16, 96, 165, 165], f16), T([16, 96, 165, 165], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 0.001, [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([16, 1000], f16), T([16], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([16, 1000], f16), T([16], i64), None, 1, -100), {})\nOperator: aten.relu.default\ncnt: 4, ((T([16, 96, 165, 165], f16),), {})\ncnt: 7, ((T([16, 54, 83, 83], f16),), {})\ncnt: 4, ((T([16, 54, 165, 165], f16),), {})\ncnt: 2, ((T([16, 270, 83, 83], f16),), {})\ncnt: 6, ((T([16, 108, 83, 83], f16),), {})\ncnt: 7, ((T([16, 108, 42, 42], f16),), {})\ncnt: 2, ((T([16, 540, 42, 42], f16),), {})\ncnt: 48, ((T([16, 216, 42, 42], f16),), {})\ncnt: 8, ((T([16, 1080, 42, 42], f16),), {})\ncnt: 6, ((T([16, 432, 42, 42], f16),), {})\ncnt: 43, ((T([16, 432, 21, 21], f16),), {})\ncnt: 8, ((T([16, 2160, 21, 21], f16),), {})\ncnt: 6, ((T([16, 864, 21, 21], f16),), {})\ncnt: 43, ((T([16, 864, 11, 11], f16),), {})\ncnt: 6, ((T([16, 4320, 11, 11], f16),), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([16, 1000], f16), [0], True), {})\nOperator: aten.threshold_backward.default\ncnt: 6, ((T([16, 4320, 11, 11], f16), T([16, 4320, 11, 11], f16), 0), {})\ncnt: 43, ((T([16, 864, 11, 11], f16), T([16, 864, 11, 11], f16), 0), {})\ncnt: 8, ((T([16, 2160, 21, 21], f16), T([16, 2160, 21, 21], f16), 0), {})\ncnt: 6, ((T([16, 864, 21, 21], f16), T([16, 864, 21, 21], f16), 0), {})\ncnt: 43, ((T([16, 432, 21, 21], f16), T([16, 432, 21, 21], f16), 0), {})\ncnt: 8, ((T([16, 1080, 42, 42], f16), T([16, 1080, 42, 42], f16), 0), {})\ncnt: 6, ((T([16, 432, 42, 42], f16), T([16, 432, 42, 42], f16), 0), {})\ncnt: 48, ((T([16, 216, 42, 42], f16), T([16, 216, 42, 42], f16), 0), {})\ncnt: 2, ((T([16, 540, 42, 42], f16), T([16, 540, 42, 42], f16), 0), {})\ncnt: 2, ((T([16, 270, 83, 83], f16), T([16, 270, 83, 83], f16), 0), {})\ncnt: 6, ((T([16, 108, 83, 83], f16), T([16, 108, 83, 83], f16), 0), {})\ncnt: 7, ((T([16, 108, 42, 42], f16), T([16, 108, 42, 42], f16), 0), {})\ncnt: 4, ((T([16, 96, 165, 165], f16), T([16, 96, 165, 165], f16), 0), {})\ncnt: 4, ((T([16, 54, 165, 165], f16), T([16, 54, 165, 165], f16), 0), {})\ncnt: 7, ((T([16, 54, 83, 83], f16), T([16, 54, 83, 83], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Installing Generated CPU and CUDA Headers in CMake\nDESCRIPTION: Iterates through the lists of generated headers (`generated_headers` and `cuda_generated_headers`). It installs each header file directly into the `${AT_INSTALL_INCLUDE_DIR}/ATen` directory, assuming these generated headers do not require subdirectory structures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_27\n\nLANGUAGE: cmake\nCODE:\n```\n# TODO: Install hip_generated_headers when we have it\nforeach(HEADER ${generated_headers} ${cuda_generated_headers})\n  # NB: Assumed to be flat\n  install(FILES ${HEADER} DESTINATION ${AT_INSTALL_INCLUDE_DIR}/ATen)\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Logging Tensor Information (Potentially Scatter Source)\nDESCRIPTION: This log format captures a count (`cnt`) and a tuple containing tensor information. The tensor is represented by `T([shape], dtype)`, here specifically `f16`. The `[965]` likely relates to another dimension or index used in a subsequent operation (like scatter_add), and `{}` indicates no keyword arguments were logged. This pattern appears before the explicit `aten.scatter_add.default` logs and seems related to its inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: plaintext\nCODE:\n```\ncnt: 4, ((T([54787], f16), [965]), {})\ncnt: 3, ((T([54768], f16), [965]), {})\ncnt: 2, ((T([54697], f16), [965]), {})\ncnt: 4, ((T([54833], f16), [965]), {})\ncnt: 2, ((T([54809], f16), [965]), {})\n# ... (many similar lines omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Logging Aten Operator: aten.stack.default (Text)\nDESCRIPTION: Log entries showing example invocations of the 'aten.stack.default' operator. Each line ('cnt') represents a call signature, detailing a list of input tensors (T) being stacked, including their shapes, data types (f16), and potentially strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.stack.default\ncnt: 4, (([T([64, 16, 65, 64], f16), T([64, 16, 65, 64], f16, stride=(66560, 4160, 1, 65)), T([64, 16, 65, 64], f16)],), {})\ncnt: 6, (([T([64, 8, 257, 64], f16), T([64, 8, 257, 64], f16, stride=(131584, 16448, 1, 257)), T([64, 8, 257, 64], f16)],), {})\ncnt: 3, (([T([64, 4, 962, 64], f16), T([64, 4, 962, 64], f16, stride=(246272, 61568, 1, 962)), T([64, 4, 962, 64], f16)],), {})\n```\n\n----------------------------------------\n\nTITLE: Updating Existing Commit List in Python\nDESCRIPTION: Command to update an existing commit list with new commits up to a specified hash\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython commitlist.py --update_to <commit_hash>\n```\n\n----------------------------------------\n\nTITLE: Computing SQNR using torch.ao.ns.fx.utils in Python\nDESCRIPTION: This code snippet demonstrates the function signature for computing Signal-to-Quantization-Noise Ratio (SQNR) between two tensors x and y using the torch.ao.ns.fx.utils module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite_fx.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ntorch.ao.ns.fx.utils.compute_sqnr(x, y)\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen GELU Backward Operations in PyTorch\nDESCRIPTION: Records instances of \\\"aten.gelu_backward.default\\\", detailing how gradients are calculated during backpropagation for GELU activated layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.gelu_backward.default\ncnt: 12, ((T([8, 128, 4096], f16), T([8, 128, 4096], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Registering FakeTensor Kernel in PyTorch\nDESCRIPTION: This code snippet references the documentation for registering a FakeTensor kernel (meta kernel) in PyTorch, which is used for reasoning about input/output shapes of operators.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.rst#2025-04-22_snippet_18\n\nLANGUAGE: reStructuredText\nCODE:\n```\n:func:`torch.library.register_fake`\n```\n\n----------------------------------------\n\nTITLE: CUDA Operations in C++\nDESCRIPTION: Function declarations for CUDA-related operations, including device properties, memory allocation, and CUBLAS functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4cuda14get_p2p_accessEii\n_ZN2at4cuda19getDevicePropertiesEl\n_ZN2at4cuda22getCUDADeviceAllocatorEv\n_ZN2at4cuda26getCurrentDevicePropertiesEv\n_ZN2at4cuda6detail12_GLOBAL__N_118_hasPrimaryContextEa\n_ZN2at4cuda6detail5nvrtcEv\n_ZN2at4cuda9warp_sizeEv\n_ZN2at6detail10empty_cudaEN3c108ArrayRefIlEENS1_10ScalarTypeESt8optionalINS1_6DeviceEES5_INS1_12MemoryFormatEE\n_ZN2at6detail10empty_cudaEN3c108ArrayRefIlEERKNS1_13TensorOptionsE\n_ZN2at6detail10empty_cudaEN3c108ArrayRefIlEESt8optionalINS1_10ScalarTypeEES4_INS1_6LayoutEES4_INS1_6DeviceEES4_IbES4_INS1_12MemoryFormatEE\n```\n\n----------------------------------------\n\nTITLE: Reduction Function Implementation in PyTorch\nDESCRIPTION: Example implementation of a reduction function that computes the mean along dimension 0.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef reduce_fn(agg_tensor):\n    return agg_tensor.mean(dim=0)\n```\n\n----------------------------------------\n\nTITLE: Tensor Memory Operations\nDESCRIPTION: Memory allocation operations for tensors including empty and zero initialization with specific strides and shapes. Uses CUDA device with half-precision (f16) format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([1024, 512, 2, 2], f16, stride=(2048, 1, 1024, 512)), [1024, 512, 2, 2], [2048, 4, 2, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 1, ((T([32, 1024], f16), [32768]), {})\n```\n\n----------------------------------------\n\nTITLE: Example TunableOp Results CSV File Format\nDESCRIPTION: Illustrates the structure of a CSV file generated by TunableOp to store tuning results. It includes 'Validator' lines checking library/PyTorch versions and lines detailing tuned operations: operator name, parameters (e.g., shapes for GEMM), the chosen solution identifier, and optionally, the average execution time. This file is read at startup and updated if new tunings are found.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/cuda/tunable/README.md#2025-04-22_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\nValidator,PT_VERSION,2.2.0\nValidator,ROCM_VERSION,6.0.0.0-12969-1544e39\nValidator,HIPBLASLT_VERSION,0.6.0-a9c5cc7\nValidator,ROCBLAS_VERSION,4.0.0-72e57364-dirty\nGemmTunableOp_float_NT,nt_25088_4096_64,1219,1.262\nGemmTunableOp_float_NT,nt_4096_4096_64,1216,0.033\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Sum Operations in PyTorch\nDESCRIPTION: Tracks \\\"aten.sum.SymInt\\\" operations, detailing summation over unspecified dimensions of tensors in  neural network layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_23\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 60, ((T([1024, 1024], f16), [0], True), {})\ncnt: 12, ((T([1024, 4096], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files for Lite Interpreter Tests in CMake\nDESCRIPTION: This snippet defines the CMake variable `LITE_INTERPRETER_RUNTIME_TEST_DIR`. It is initially set to a directory path and then immediately overwritten with a list of specific C++ source files that constitute the Lite Interpreter runtime tests. Requires the `TORCH_ROOT` variable to be predefined.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lite_interpreter_runtime/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(\n  LITE_INTERPRETER_RUNTIME_TEST_DIR\n  \"${TORCH_ROOT}/test/cpp/lite_interpreter_runtime\")\nset(LITE_INTERPRETER_RUNTIME_TEST_DIR\n  ${TORCH_ROOT}/test/cpp/lite_interpreter_runtime/main.cpp\n  ${TORCH_ROOT}/test/cpp/lite_interpreter_runtime/test_lite_interpreter_runtime.cpp\n  ${TORCH_ROOT}/test/cpp/lite_interpreter_runtime/test_mobile_profiler.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying Benchmark Results for Batch Size 32 in Markdown Table\nDESCRIPTION: A markdown table showing performance metrics for different worker configurations with batch size 32 and compilation disabled. Metrics include warmup latency, average latency, throughput, and GPU utilization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_32_false.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |\n| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |\n| original | 5.680 +/- 0.919 | 4.785 +/- 0.864 | 394.178 +/- 81.705 | 38.515 +/- 11.152 |\n| h2d_d2h_threads | 4.856 +/- 0.142 | 6.694 +/- 0.497 | 287.201 +/- 41.480 | 27.028 +/- 4.773 |\n| 2_predict_workers | 3.465 +/- 0.082 | 5.369 +/- 0.900 | 334.981 +/- 50.292 | 31.635 +/- 4.492 |\n| 3_predict_workers | 3.819 +/- 0.617 | 4.409 +/- 0.149 | 402.236 +/- 22.151 | 35.893 +/- 0.877 |\n| 4_predict_workers | 3.994 +/- 0.509 | 6.007 +/- 0.408 | 296.260 +/- 16.524 | 25.751 +/- 1.810 |\n```\n\n----------------------------------------\n\nTITLE: Initializing Lazy Tensor Backend in PyTorch\nDESCRIPTION: This snippet shows how to initialize the TorchScript-based backend for Lazy Tensor, which is necessary for running traced graphs with Lazy Tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch._lazy\nimport torch._lazy.ts_backend\ntorch._lazy.ts_backend.init()\n```\n\n----------------------------------------\n\nTITLE: Tensor Sum Operations\nDESCRIPTION: Sum operations on tensors, including symbolic integer operations and default sum implementations. Operates on 2D tensors with shape [4, 1000].\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 1000], f16, stride=(0, 0)), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Import Static Libraries on Android in CMake\nDESCRIPTION: Defines a CMake function `import_static_lib` specifically for Android builds (`if(ANDROID_ABI)` block). This function takes a library name as an argument, creates an `IMPORTED STATIC` library target for it, and sets its `IMPORTED_LOCATION` property to the path where the pre-built static library (`.a` file) is expected within the `jniLibs` directory, based on the current `ANDROID_ABI`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nif(ANDROID_ABI)\n  function(import_static_lib name)\n    add_library(${name} STATIC IMPORTED)\n    set_property(\n        TARGET ${name}\n        PROPERTY IMPORTED_LOCATION\n        ${CMAKE_CURRENT_LIST_DIR}/src/main/jniLibs/${ANDROID_ABI}/${name}.a)\n  endfunction(import_static_lib)\n```\n\n----------------------------------------\n\nTITLE: Appending ATen Mobile Benchmark Source Files in CMake\nDESCRIPTION: Appends several C++ source files located in the `benchmarks` subdirectory (`tensor_add.cpp`, `quantize_per_channel.cpp`, `stateful_conv1d.cpp`) to the `ATen_MOBILE_BENCHMARK_SRCS` list variable. This list likely accumulates source files used for building mobile-specific benchmarks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_31\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND ATen_MOBILE_BENCHMARK_SRCS\n  ${CMAKE_CURRENT_SOURCE_DIR}/benchmarks/tensor_add.cpp)\nlist(APPEND ATen_MOBILE_BENCHMARK_SRCS\n  ${CMAKE_CURRENT_SOURCE_DIR}/benchmarks/quantize_per_channel.cpp)\nlist(APPEND ATen_MOBILE_BENCHMARK_SRCS\n  ${CMAKE_CURRENT_SOURCE_DIR}/benchmarks/stateful_conv1d.cpp)\n```\n\n----------------------------------------\n\nTITLE: Configuring Convolution Test for QNNPACK in CMake\nDESCRIPTION: Creates and configures the convolution test executable with C++14 standard requirements, includes the necessary directories, and links against required libraries like pytorch_qnnpack, clog, cpuinfo, fp16, and gtest.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(convolution-test test/convolution.cc)\nset_target_properties(convolution-test PROPERTIES\n  CXX_STANDARD 14\n  CXX_STANDARD_REQUIRED YES\n  CXX_EXTENSIONS NO)\ntarget_include_directories(convolution-test PRIVATE src test)\ntarget_link_libraries(convolution-test PRIVATE pytorch_qnnpack clog cpuinfo fp16 gtest gtest_main)\nadd_test(convolution-test convolution-test)\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Tensor Addition in PyTorch\nDESCRIPTION: Tracks operations involving the \\\"aten.add.Tensor\\\" operator, which performs element-wise addition of tensors, listing various tensor shapes and data types involved.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 1, ((T([128], i64), 1), {})\ncnt: 1, ((T([8, 128, 1024], f16), T([128, 1024], f16)), {})\ncnt: 12, ((T([8, 16, 128, 128], f16), T([8, 1, 128, 128], f16)), {})\ncnt: 72, ((T([8, 128, 1024], f16), T([8, 128, 1024], f16)), {})\ncnt: 1, ((T([50265, 1024], f16), T([50265, 1024], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Grid Sampling Operations\nDESCRIPTION: Grid sampling operations for 2D tensors with specified interpolation modes and padding configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Super_SloMo_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n((T([6, 3, 352, 352], f16), T([6, 352, 352, 2], f16), 0, 0, False), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Pooling Operations\nDESCRIPTION: Max pooling operations with 2x2 kernel size and stride of 2.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Super_SloMo_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n((T([6, 64, 352, 352], f16), [2, 2], [2, 2]), {})\n```\n\n----------------------------------------\n\nTITLE: Generating Class Header for PyTorch Documentation\nDESCRIPTION: Creates a header for the class documentation. The class name is inserted dynamically and underlined using Jinja2 templating.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/autosummary/class.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{{ name | underline}}\n```\n\n----------------------------------------\n\nTITLE: Chaining Multiple PyTorch Operator Registrations (C++)\nDESCRIPTION: Illustrates how to register multiple operators or kernels within a single `torch::RegisterOperators` instance by chaining calls to the `.op()` method. This example registers two different operators, `my_namespace::my_op_1` and `my_namespace::my_op_2`, each with a CPU kernel.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/op_registration/README.md#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\nstatic auto registry = torch::RegisterOperators()\n    .op(\"my_namespace::my_op_1\", torch::RegisterOperators::options()\n        .kernel<MyKernel1>(CPU()))\n    .op(\"my_namespace::my_op_2\", torch::RegisterOperators::options()\n        .kernel<MyKernel2>(CPU()));\n```\n\n----------------------------------------\n\nTITLE: Building and Running JIT C++ Tests in PyTorch - Bash Scripts\nDESCRIPTION: This snippet provides Bash commands for building the PyTorch source code and executing the JIT test binaries. It includes steps for building with setup.py, rebuilding the test binary using Ninja, and running the GoogleTest binary with optional test filtering. Dependencies are a working Bash shell, a Python environment, PyTorch source code, and the Ninja build system. Input commands may be modified to target different source directories or test filters; outputs include compiled binaries and test result output in the shell.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# ... Build PyTorch from source, e.g.\npython setup.py develop\n# (re)build just the binary\nninja -C build bin/test_jit\n# run tests\nbuild/bin/test_jit --gtest_filter='glob_style_filter*'\n\n```\n\n----------------------------------------\n\nTITLE: Extracting a TorchScript Serialized Model Archive\nDESCRIPTION: This snippet demonstrates how to use the unzip command to extract a TorchScript model file (model.pt) and view its internal file structure using the tree command. It shows the standard organization of code, data, and constants in the archive.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/docs/serialization.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ unzip model.pt\nArchive:  model.pt\n  extracting ...\n\n$ tree model/\n code/\n    __torch__.py\n    __torch__.py.debug_pkl\n    foo/\n       bar.py\n       bar.py.debug_pkl\n data.pkl\n constants.pkl\n data/\n     0\n     1\n```\n\n----------------------------------------\n\nTITLE: CUDA-Accelerated Matrix and Tensor Kernel Implementations (PyTorch Native/CUDA C++)\nDESCRIPTION: These function symbols mark CUDA-optimized kernel implementations for key tensor algebra (e.g., batched matmul, addmm, bmm, and related kernels). These functions are primarily invoked when dispatching to CUDA devices, ensuring optimal performance on GPUs. Prerequisites are an active CUDA device and compatible input tensor shapes. Inputs involve tensors and possibly optional scalar/bias/activation parameters; outputs are typically the resultant tensors of the operation. Limitations may include adherence to alignment/contiguity requirements by cuBLAS kernels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_20\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4cuda4blas13gemm_and_biasIN3c104HalfEEEvbblllNS_10OpMathTypeIT_E4typeEPKS6_lSA_lSA_PS6_lNS1_29GEMMAndBiasActivationEpilogueE\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native25structured_addmm_out_cuda4implERKNS_6TensorES4_S4_RKN3c106ScalarES8_S4_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native22structured_mm_out_cuda4implERKNS_6TensorES4_S4_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native23structured_bmm_out_cuda4implERKNS_6TensorES4_S4_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native12_GLOBAL__N_121baddbmm_out_cuda_implERKNS_6TensorES4_S4_S4_RKN3c106ScalarES8_.isra.0\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native31prepare_batch_matrix_for_cublasERKNS_6TensorERbRlbll\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding ATen Test Subdirectory in CMake\nDESCRIPTION: Checks build configuration flags to decide whether to include the ATen tests. If `ATEN_NO_TEST` is set or if `BUILD_LITE_INTERPRETER` is enabled, it prints a message indicating tests are disabled. Otherwise, it uses `add_subdirectory(test)` to include the `test` subdirectory in the build.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_30\n\nLANGUAGE: cmake\nCODE:\n```\nif(ATEN_NO_TEST)\n  message(\"disable test because ATEN_NO_TEST is set\")\nelif(BUILD_LITE_INTERPRETER)\n  message(\"disable aten test when BUILD_LITE_INTERPRETER is enabled\")\nelse()\n  add_subdirectory(test)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Executing aten.convolution_backward operation in PyTorch\nDESCRIPTION: Conducts backward pass for convolutions on the given tensors using Aten in PyTorch, considering kernel sizes, padding, and strides. Outputs gradients for each input. This requires initial setup of forward convolution operations and PyTorch infrastructure.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 2304, 7, 7], f16), T([128, 1536, 7, 7], f16), T([2304, 1536, 1, 1], f16), [2304], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.native_batch_norm_backward.default Operator (Text)\nDESCRIPTION: Logs calls to the `aten.native_batch_norm_backward.default` operator. Each entry displays the count (`cnt`) and the argument tuple, detailing tensor shapes (f16, f32), boolean flags, epsilon value (1e-05), and gradient enablement flags. This indicates usage in computing gradients for batch normalization layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([1, 3072, 1536], f16), T([1, 3072, 1536], f16), T([3072], f16), None, None, T([3072], f32), T([3072], f32), True, 1e-05, [True, True, False]), {})\ncnt: 9, ((T([1, 1536, 768], f16), T([1, 1536, 768], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})\ncnt: 18, ((T([1, 768, 1152], f16), T([1, 768, 1152], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})\ncnt: 8, ((T([1, 768, 1536], f16), T([1, 768, 1536], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 1536, 1536], f16), T([1, 1536, 1536], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 768, 512], f16), T([1, 768, 512], f16), T([768], f16), None, None, T([768], f32), T([768], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 1536, 512], f16), T([1, 1536, 512], f16), T([1536], f16), None, None, T([1536], f32), T([1536], f32), True, 1e-05, [True, True, False]), {})\ncnt: 3, ((T([1, 512, 256], f16), T([1, 512, 256], f16), T([512], f16), None, None, T([512], f32), T([512], f32), True, 1e-05, [True, True, False]), {})\ncnt: 4, ((T([1, 256, 1152], f16), T([1, 256, 1152], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 256, 512], f16), T([1, 256, 512], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 256, 256], f16), T([1, 256, 256], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\ncnt: 2, ((T([1, 256, 128], f16), T([1, 256, 128], f16), T([256], f16), None, None, T([256], f32), T([256], f32), True, 1e-05, [True, True, False]), {})\ncnt: 2, ((T([1, 128, 1152], f16), T([1, 128, 1152], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 128, 128], f16), T([1, 128, 128], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 128, 576], f16), T([1, 128, 576], f16), T([128], f16), None, None, T([128], f32), T([128], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 64, 288], f16), T([1, 64, 288], f16), T([64], f16), None, None, T([64], f32), T([64], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 32, 144], f16), T([1, 32, 144], f16), T([32], f16), None, None, T([32], f32), T([32], f32), True, 1e-05, [True, True, False]), {})\ncnt: 1, ((T([1, 16, 27], f16), T([1, 16, 27], f16), T([16], f16), None, None, T([16], f32), T([16], f32), True, 1e-05, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Profiler Module Structure\nDESCRIPTION: RST documentation structure defining the PyTorch profiler module organization and class references.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/profiler.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: torch.profiler\n\ntorch.profiler\n==============\n\nOverview\n--------\n.. automodule:: torch.profiler\n\n\nAPI Reference\n-------------\n\n.. autoclass:: torch.profiler._KinetoProfile\n  :members:\n\n.. autoclass:: torch.profiler.profile\n  :members:\n\n.. autoclass:: torch.profiler.ProfilerAction\n  :members:\n\n.. autoclass:: torch.profiler.ProfilerActivity\n  :members:\n\n.. autofunction:: torch.profiler.schedule\n\n.. autofunction:: torch.profiler.tensorboard_trace_handler\n```\n\n----------------------------------------\n\nTITLE: Configuring Core Overhead Benchmark with Google Benchmark Library\nDESCRIPTION: Conditionally creates the core_overhead_benchmark target when BUILD_TEST is enabled and links it with the Google benchmark library for performance testing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/binaries/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(BUILD_TEST)\n  # Core overhead benchmark\n  caffe2_binary_target(\"core_overhead_benchmark.cc\")\n  target_link_libraries(core_overhead_benchmark benchmark)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring MIMALLOC Memory Allocator in CMake\nDESCRIPTION: Configures build options for the MIMALLOC memory allocator when enabled. Sets up required definitions and includes the MIMALLOC subdirectory and headers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_45\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_MIMALLOC)\n  set(MI_OVERRIDE OFF)\n  set(MI_BUILD_SHARED OFF)\n  set(MI_BUILD_OBJECT OFF)\n  set(MI_BUILD_TESTS OFF)\n  add_definitions(-DUSE_MIMALLOC)\n  add_subdirectory(third_party/mimalloc)\n  include_directories(third_party/mimalloc/include)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Profiling ATen Convolution and Convolution Backward Operators in PyTorch (Python)\nDESCRIPTION: Tracks calls to ATen convolution and its backward variants, with detailed tensor dimensions and convolution settings (stride, padding, dilation, groups, etc). Dependencies: PyTorch, model containing convolutional layers, mixed-precision support. Inputs include weight, input, and optional bias tensors; outputs correspond to forward or backward pass results. Constraints: Only operator calls are shown, not model source code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([16, 3, 224, 224], f16), T([64, 3, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([16, 64, 56, 56], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 64, 56, 56], f16), T([128, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([16, 128, 28, 28], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 64, 56, 56], f16), T([128, 64, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 128, 28, 28], f16), T([256, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([16, 256, 14, 14], f16), T([256, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 128, 28, 28], f16), T([256, 128, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 256, 14, 14], f16), T([512, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 3, ((T([16, 512, 7, 7], f16), T([512, 512, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 256, 14, 14], f16), T([512, 256, 1, 1], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 3, ((T([16, 512, 7, 7], f16), T([16, 512, 7, 7], f16), T([512, 512, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 512, 7, 7], f16), T([16, 256, 14, 14], f16), T([512, 256, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 512, 7, 7], f16), T([16, 256, 14, 14], f16), T([512, 256, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([16, 256, 14, 14], f16), T([16, 256, 14, 14], f16), T([256, 256, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 256, 14, 14], f16), T([16, 128, 28, 28], f16), T([256, 128, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 256, 14, 14], f16), T([16, 128, 28, 28], f16), T([256, 128, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([16, 128, 28, 28], f16), T([16, 128, 28, 28], f16), T([128, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 128, 28, 28], f16), T([16, 64, 56, 56], f16), T([128, 64, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 128, 28, 28], f16), T([16, 64, 56, 56], f16), T([128, 64, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([16, 64, 56, 56], f16), T([16, 64, 56, 56], f16), T([64, 64, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([16, 64, 112, 112], f16), T([16, 3, 224, 224], f16), T([64, 3, 7, 7], f16), [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.select_backward.default Operator (Log)\nDESCRIPTION: Log entries detailing calls to the PyTorch `aten.select_backward.default` operator. Each line shows the invocation count (`cnt`) and arguments: the gradient output tensor, the input size list, the dimension selected, and the index selected.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.select_backward.default\ncnt: 1, ((T([64, 256], f16), [64, 197, 256], 1, 0), {})\ncnt: 1, ((T([64, 128], f16), [64, 401, 128], 1, 0), {})\n```\n\n----------------------------------------\n\nTITLE: Appending CPU Include Paths to Backend Include Lists in CMake\nDESCRIPTION: Appends the contents of the `ATen_CPU_INCLUDE` list variable to the include lists for CUDA (`ATen_CUDA_INCLUDE`), HIP (`ATen_HIP_INCLUDE`), and Vulkan (`ATen_VULKAN_INCLUDE`). This ensures that backend-specific code can access common CPU headers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\n# Include CPU paths for CUDA/HIP as well\nlist(APPEND ATen_CUDA_INCLUDE ${ATen_CPU_INCLUDE})\nlist(APPEND ATen_HIP_INCLUDE ${ATen_CPU_INCLUDE})\nlist(APPEND ATen_VULKAN_INCLUDE ${ATen_CPU_INCLUDE})\n```\n\n----------------------------------------\n\nTITLE: Summing Tensor Elements in PyTorch\nDESCRIPTION: Computes the sum of tensor elements, providing aggregated output useful for loss calculation and statistics. Without dimensions specified, it sums all elements, but with dimensions, it sums along the specified axes. This flexibility in aggregation facilitates diverse data processing and loss computations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.default\ncnt: 1, ((T([16, 3, 128, 128], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch CPU Test Sources and Includes with CMake\nDESCRIPTION: This CMake snippet glob's test files, appends specific source files to the CPU sources list, adds include directories, and sets variables in the parent scope. It includes third-party miniz library and custom adapter files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/serialize/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB tmp *_test.cc)\n\nset(Caffe2_CPU_TEST_SRCS ${Caffe2_CPU_TEST_SRCS} ${tmp})\nlist(APPEND Caffe2_CPU_SRCS\n  ${PROJECT_SOURCE_DIR}/third_party/miniz-3.0.2/miniz.c\n  ${CMAKE_CURRENT_SOURCE_DIR}/inline_container.cc\n  ${CMAKE_CURRENT_SOURCE_DIR}/istream_adapter.cc\n  ${CMAKE_CURRENT_SOURCE_DIR}/file_adapter.cc\n  ${CMAKE_CURRENT_SOURCE_DIR}/crc.cc\n  ${CMAKE_CURRENT_SOURCE_DIR}/read_adapter_interface.cc)\nlist(APPEND Caffe2_CPU_INCLUDE ${PROJECT_SOURCE_DIR}/third_party/miniz-3.0.2)\n\nset(Caffe2_CPU_TEST_SRCS ${Caffe2_CPU_TEST_SRCS} PARENT_SCOPE)\nset(Caffe2_CPU_SRCS ${Caffe2_CPU_SRCS} PARENT_SCOPE)\nset(Caffe2_CPU_INCLUDE ${Caffe2_CPU_INCLUDE} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Defining Lazy Test Root Directory in CMake\nDESCRIPTION: Sets a CMake variable `LAZY_TEST_ROOT` to the path of the C++ lazy tests directory, relative to the main `TORCH_ROOT` directory. This variable is used later to locate source files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(LAZY_TEST_ROOT ${TORCH_ROOT}/test/cpp/lazy)\n```\n\n----------------------------------------\n\nTITLE: Executing aten._log_softmax in PyTorch\nDESCRIPTION: This snippet captures the application of the aten._log_softmax.default operator on a tensor with dimensions [2048, 30522] and data type f16, focusing on softmax along dimension 1 without keeping dimensions. It may require PyTorch library availability and suitable CUDA environment for efficient execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\naten._log_softmax.default, ((T([2048, 30522], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Boolean Conversion Macro\nDESCRIPTION: Creates a macro to convert CMake boolean values to 0 or 1 for use in configuration files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nmacro(set_bool OUT IN)\n  if(${IN})\n    set(${OUT} 1)\n  else()\n    set(${OUT} 0)\n  endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Defining a Short Class with Proper Docstring\nDESCRIPTION: A minimal class that includes a proper docstring despite having no implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass ShortWithDocstring:\n    \"\"\"This docstring, while short, is enough\"\"\"\n    pass\n```\n\n----------------------------------------\n\nTITLE: Max Pooling Operations in DenseNet Forward Pass\nDESCRIPTION: This snippet shows the max pooling operations used in DenseNet's forward pass. These operations perform downsampling with 33 kernels, stride 2, and track indices for use in the backward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([32, 256, 56, 56], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})\ncnt: 1, ((T([32, 768, 14, 14], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})\n```\n\n----------------------------------------\n\nTITLE: Exporting Categories to Markdown in Python\nDESCRIPTION: Command to export categorized commits to markdown format in separate files\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython commitlist.py --export_markdown\n```\n\n----------------------------------------\n\nTITLE: torch.full Behavior in PyTorch 1.7\nDESCRIPTION: Shows how torch.full infers the returned tensor's dtype from the fill value in PyTorch 1.7.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# PyTorch 1.7\n>>> torch.full((3,), 1)\ntensor([1, 1, 1])\n\n>>> torch.full((3,), True)\ntensor([True, True, True])\n\n>>> torch.full((3,), 1.)\ntensor([1., 1., 1.])\n\n>>> torch.full((3,), 1 + 1j)\ntensor([1.+1.j, 1.+1.j, 1.+1.j])\n```\n\n----------------------------------------\n\nTITLE: Computing NLL Loss Forward Pass in PyTorch\nDESCRIPTION: This snippet shows the tensor shapes and parameters for the forward pass of the negative log-likelihood loss in a PyTorch model. It includes predictions and target labels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Defining BaseListVariable Container Class for PyTorch Dynamo\nDESCRIPTION: Defines the BaseListVariable class which extends VariableTrackerContainer for tracking list-like data structures. The class specifies 'set' as its container type. This is part of PyTorch's Dynamo symbolic execution system for optimizing Python code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/set_linter_testdata/includes.py.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BaseListVariable(VariableTrackerContainer):\n    our_container = set\n```\n\n----------------------------------------\n\nTITLE: Referencing RecordFunction Header File in PyTorch\nDESCRIPTION: This code snippet shows the file path for the RecordFunction header file in the PyTorch codebase. RecordFunction is a key component used by the profiler to instrument CPU-side events.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/profiler/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n[/aten/src/ATen/record_function.h](/aten/src/ATen/record_function.h)\n```\n\n----------------------------------------\n\nTITLE: PyTorch BatchNorm Operations\nDESCRIPTION: Batch normalization operations with half-precision (f16) tensors of varying sizes. Includes parameters for epsilon (1e-05) and running statistics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: In-place Tensor Addition in PyTorch ResNet Skip Connections\nDESCRIPTION: This snippet shows in-place tensor addition operations (add_) used for residual/skip connections in a ResNet model. The operations work with half-precision (f16) tensors of various sizes that correspond to different network stages.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add_.Tensor\ncnt: 3, ((T([4, 256, 296, 304], f16), T([4, 256, 296, 304], f16)), {})\ncnt: 4, ((T([4, 512, 148, 152], f16), T([4, 512, 148, 152], f16)), {})\ncnt: 6, ((T([4, 1024, 74, 76], f16), T([4, 1024, 74, 76], f16)), {})\ncnt: 3, ((T([4, 2048, 37, 38], f16), T([4, 2048, 37, 38], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Using Pytorchbot for Automated Cherry Picking via GitHub Commands\nDESCRIPTION: This code block documents the command-line usage of the @pytorchbot cherry-pick GitHub bot to automate cherry-picking a pull request onto a release branch. The command supports arguments such as --onto (target branch), --fixes (linked issue for tracking), and -c (classification of cherry-pick type: regression, critical, docs, etc.). Dependencies: GitHub permissions, the bot enabled on the repo, and the existence of a valid target branch. Inputs: PR to cherry-pick, tracking issue. Output: a new PR on the specified release branch with the appropriate linkage and labels. Limitations: currently only available in the PyTorch organization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#2025-04-22_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nusage: @pytorchbot cherry-pick --onto ONTO [--fixes FIXES] -c\n                               {regression,critical,fixnewfeature,docs,release}\n\nCherry pick a pull request onto a release branch for inclusion in a release\n\noptional arguments:\n  --onto ONTO           Branch you would like to cherry pick onto (Example: release/2.2)\n  --fixes FIXES         Link to the issue that your PR fixes (i.e. https://github.com/pytorch/pytorch/issues/110666)\n  -c {regression,critical,fixnewfeature,docs,release}\n                        A machine-friendly classification of the cherry-pick reason.\n```\n\n----------------------------------------\n\nTITLE: Tensor Subtraction with sub in PyTorch (Python)\nDESCRIPTION: Utilizes aten.sub for subtracting one tensor from another element-wise, essential during operations involving differential calculations where pairwise differences across tensors are needed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\naten.sub.Tensor\ncnt: 2, ((T([16, 512], i64, stride=(2048, 4)), T([16, 512], i64, stride=(2048, 4))), {})\n```\n\n----------------------------------------\n\nTITLE: Creating Tensor Copies with aten.clone.default\nDESCRIPTION: Shows usage of the clone.default operator which creates deep copies of tensors. This is typically used before modifying tensors to prevent in-place changes to the originals, with operations focusing on feature maps and input images.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([8, 3, 384, 512], f16),), {})\ncnt: 1, ((T([8, 3, 12, 16, 85], f16),), {})\ncnt: 1, ((T([8, 3, 24, 32, 85], f16),), {})\ncnt: 1, ((T([8, 3, 48, 64, 85], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Running a Local HTTP Server Using Python\nDESCRIPTION: This Python command starts a simple HTTP server on port 8000 to preview documentation files locally. Requires Python to be installed and executed within a directory containing HTML files. Outputs an HTTP service on port 8000 accessible via web browsers. It is limited by local network restrictions or firewall settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\npython -m http.server 8000 <path_to_html_output>\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations in DenseNet Classification Layer\nDESCRIPTION: This snippet shows matrix multiplication operations used in DenseNet's final classification layer. It performs multiplication between the pooled features and the weight matrix, both in forward and backward passes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), T([1000, 1024], f16)), {})\ncnt: 1, ((T([1000, 32], f16, stride=(0, 0)), T([32, 1024], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This code snippet represents a structured log of PyTorch operator usage. It includes operator names, call counts, and the shapes of tensors they operate on. This information is useful for performance analysis and optimization of PyTorch models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/alexnet_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._adaptive_avg_pool2d.default\ncnt: 1, ((T([128, 256, 6, 6], f16), [6, 6]), {})\nOperator: aten._adaptive_avg_pool2d_backward.default\ncnt: 1, ((T([128, 256, 6, 6], f16), T([128, 256, 6, 6], f16)), {})\nOperator: aten.addmm.default\ncnt: 1, ((T([4096], f16), T([128, 9216], f16), T([9216, 4096], f16, stride=(1, 9216))), {})\ncnt: 1, ((T([4096], f16), T([128, 4096], f16), T([4096, 4096], f16, stride=(1, 4096))), {})\ncnt: 1, ((T([1000], f16), T([128, 4096], f16), T([4096, 1000], f16, stride=(1, 4096))), {})\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([64, 3, 11, 11], f16), T([64], f16), [4, 4], [2, 2], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 64, 27, 27], f16), T([192, 64, 5, 5], f16), T([192], f16), [1, 1], [2, 2], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 192, 13, 13], f16), T([384, 192, 3, 3], f16), T([384], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 384, 13, 13], f16), T([256, 384, 3, 3], f16), T([256], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([128, 256, 13, 13], f16), T([256, 256, 3, 3], f16), T([256], f16), [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 256, 13, 13], f16), T([128, 256, 13, 13], f16), T([256, 256, 3, 3], f16), [256], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 13, 13], f16), T([128, 384, 13, 13], f16), T([256, 384, 3, 3], f16), [256], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 384, 13, 13], f16), T([128, 192, 13, 13], f16), T([384, 192, 3, 3], f16), [384], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 192, 27, 27], f16), T([128, 64, 27, 27], f16), T([192, 64, 5, 5], f16), [192], [1, 1], [2, 2], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 64, 55, 55], f16), T([128, 3, 224, 224], f16), T([64, 3, 11, 11], f16), [64], [4, 4], [2, 2], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 128000), {})\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([128, 64, 55, 55], f16), [3, 3], [2, 2]), {})\ncnt: 1, ((T([128, 192, 27, 27], f16), [3, 3], [2, 2]), {})\ncnt: 1, ((T([128, 256, 13, 13], f16), [3, 3], [2, 2]), {})\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([128, 256, 6, 6], f16), T([128, 256, 13, 13], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([128, 256, 6, 6], i64)), {})\ncnt: 1, ((T([128, 192, 13, 13], f16), T([128, 192, 27, 27], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([128, 192, 13, 13], i64)), {})\ncnt: 1, ((T([128, 64, 27, 27], f16), T([128, 64, 55, 55], f16), [3, 3], [2, 2], [0, 0], [1, 1], False, T([128, 64, 27, 27], i64)), {})\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16, stride=(0, 0)), T([1000, 4096], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(0, 0)), T([128, 4096], f16)), {})\ncnt: 1, ((T([128, 4096], f16), T([4096, 4096], f16)), {})\ncnt: 1, ((T([4096, 128], f16, stride=(1, 4096)), T([128, 4096], f16)), {})\ncnt: 1, ((T([128, 4096], f16), T([4096, 9216], f16)), {})\ncnt: 1, ((T([4096, 128], f16, stride=(1, 4096)), T([128, 9216], f16)), {})\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 64, 55, 55], f16),), {})\ncnt: 1, ((T([128, 192, 27, 27], f16),), {})\ncnt: 1, ((T([128, 384, 13, 13], f16),), {})\ncnt: 2, ((T([128, 256, 13, 13], f16),), {})\ncnt: 2, ((T([128, 4096], f16),), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16, stride=(0, 0)), [0], True), {})\ncnt: 2, ((T([128, 4096], f16), [0], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([128, 1000], f16),), {})\nOperator: aten.threshold_backward.default\ncnt: 2, ((T([128, 4096], f16), T([128, 4096], f16), 0), {})\ncnt: 2, ((T([128, 256, 13, 13], f16), T([128, 256, 13, 13], f16), 0), {})\ncnt: 1, ((T([128, 384, 13, 13], f16), T([128, 384, 13, 13], f16), 0), {})\ncnt: 1, ((T([128, 192, 27, 27], f16), T([128, 192, 27, 27], f16), 0), {})\ncnt: 1, ((T([128, 64, 55, 55], f16), T([128, 64, 55, 55], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations in PyTorch\nDESCRIPTION: Matrix multiplication operations using half-precision tensors, primarily for final layer computations with specific stride patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\naten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 1984], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1984], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Up C10 Installation Rules\nDESCRIPTION: Configures the installation process for the C10 library, including the target, header files, and optional PDB files for MSVC builds. Defines where and how components will be installed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_LIBTORCHLESS)\n  # ---[ Installation\n  # Note: for now, we will put all export path into one single Caffe2Targets group\n  # to deal with the cmake deployment need. Inside the Caffe2Targets set, the\n  # individual libraries like libc10.so and libcaffe2.so are still self-contained.\n  install(TARGETS c10 EXPORT Caffe2Targets DESTINATION lib)\nendif()\n\ninstall(DIRECTORY ${CMAKE_CURRENT_LIST_DIR}\n        DESTINATION include\n        FILES_MATCHING PATTERN \"*.h\")\ninstall(FILES ${CMAKE_BINARY_DIR}/c10/macros/cmake_macros.h\n        DESTINATION include/c10/macros)\n\nif(MSVC AND C10_BUILD_SHARED_LIBS)\n  install(FILES $<TARGET_PDB_FILE:c10> DESTINATION lib OPTIONAL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Usage Examples for aten.mul.Tensor\nDESCRIPTION: Logs element-wise tensor multiplication (`aten.mul.Tensor`). Most examples show multiplication of a 4D tensor with another 4D tensor where the last two dimensions are singleton (broadcastable), e.g., [128, C, H, W] * [128, C, 1, 1]. Other examples show element-wise multiplication of identically shaped tensors. All tensors are float16.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/regnety_002_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 1, 1], f16)), {})\ncnt: 2, ((T([128, 56, 28, 28], f16), T([128, 56, 1, 1], f16)), {})\ncnt: 8, ((T([128, 152, 14, 14], f16), T([128, 152, 1, 1], f16)), {})\ncnt: 14, ((T([128, 368, 7, 7], f16), T([128, 368, 1, 1], f16)), {})\ncnt: 7, ((T([128, 368, 7, 7], f16), T([128, 368, 7, 7], f16)), {})\ncnt: 4, ((T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16)), {})\ncnt: 1, ((T([128, 56, 28, 28], f16), T([128, 56, 28, 28], f16)), {})\ncnt: 1, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Split Operations\nDESCRIPTION: Tensor splitting operations along channel dimension with specified size chunks\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 384, 16, 16], f16), [64, 64, 256], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing User to User RRef Sharing in PyTorch\nDESCRIPTION: Illustrates the most complex RRef sharing scenario where a caller user shares an RRef with a callee user, involving coordination between three parties: caller, callee, and owner. Shows remote creation and passing of RRef between workers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/rref.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed.rpc as rpc\n\n# on worker A and worker C\ndef func(rref):\n    pass\n\n# on worker A\nrref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n# say the rref has RRefId 100 and ForkId 1\nrpc.rpc_async('C', func, args=(rref, ))\n```\n\n----------------------------------------\n\nTITLE: Negative Log Likelihood Loss Backward Pass in PyTorch (Python)\nDESCRIPTION: Carries out the aten.nll_loss_backward function to compute gradients during backpropagation by recovering derivatives of the negative log likelihood loss, vital in optimizing classification models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\naten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([16, 2], f16), T([16], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Empty Submodule Declarations in RestructuredText\nDESCRIPTION: Declarations of empty submodules in PyTorch that are only used internally or for tracking purposes, formatted in RestructuredText.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_12\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. Empty submodules added only for tracking.\n.. py:module:: torch.contrib\n.. py:module:: torch.utils.backcompat\n\n.. This module is only used internally for ROCm builds.\n.. py:module:: torch.utils.hipify\n\n.. This module needs to be documented. Adding here in the meantime\n.. for tracking purposes\n.. py:module:: torch.utils.model_dump\n.. py:module:: torch.utils.viz\n.. py:module:: torch.functional\n.. py:module:: torch.quasirandom\n.. py:module:: torch.return_types\n.. py:module:: torch.serialization\n.. py:module:: torch.signal.windows.windows\n.. py:module:: torch.sparse.semi_structured\n.. py:module:: torch.storage\n.. py:module:: torch.torch_version\n.. py:module:: torch.types\n.. py:module:: torch.version\n```\n\n----------------------------------------\n\nTITLE: Implementing Feature Augmentation in AutoHeuristic\nDESCRIPTION: Example of creating augmented features using AHOperation objects for combining existing features into new derived features.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef m_times_k(data: Any) -> float:\n    return data['m'] * data['k']\n\nm_times_k_op = AHOperation(\"m*k\", m_times_k)\nah_operations = [m_times_k_op]\n\n# specify augmented features by setting `augment_context` to `ah_operations`\nautoheuristic = AutoHeuristic(..., augment_context=ah_operations, ...)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Multiplication\nDESCRIPTION: Matrix multiplication operations between tensors, including cases with transposed tensors and specific strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmm.default((T([128, 1000], f16), T([1000, 1280], f16)))\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operation Function Signatures\nDESCRIPTION: Collection of mangled C++ function signatures implementing core PyTorch tensor operations including permute, transpose, layer normalization, and various autograd functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_23\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4_ops10layer_norm4callERKNS_6TensorEN3c108ArrayRefINS5_6SymIntEEERKSt8optionalIS2_ESC_db\n_ZN2at6native15layer_norm_cudaERKNS_6TensorEN3c108ArrayRefIlEERKSt8optionalIS1_ESA_d\n_ZN2at6native28scaled_dot_product_attentionERKNS_6TensorES3_S3_RKSt8optionalIS1_EdbS4_IdE\n```\n\n----------------------------------------\n\nTITLE: Autograd Handling of Named Tensors\nDESCRIPTION: Details the limited support for named tensors in PyTorch autograd, noting that while gradient computations remain valid, names are currently ignored in gradient outputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/named_tensor.rst#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nx = torch.randn(3, names=('D',))\nweight = torch.randn(3, names=('D',), requires_grad=True)\nloss = (x - weight).abs()\ngrad_loss = torch.randn(3)\nloss.backward(grad_loss)\nweight.grad  # Unnamed for now. Will be named in the future\n\nweight.grad.zero_()\ngrad_loss = grad_loss.refine_names('C')\nloss = (x - weight).abs()\n# Ideally we'd check that the names of loss and grad_loss match but we don't yet.\nloss.backward(grad_loss)\nweight.grad\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations - Softmax and Views\nDESCRIPTION: Common tensor operations including softmax, backward propagation, and view reshaping with specified tensor shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Bert_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._softmax.default((T([4, 12, 512, 512], f16), -1, False), {})\naten._softmax_backward_data.default((T([4, 12, 512, 512], f16), T([4, 12, 512, 512], f16), -1, f16), {})\naten._unsafe_view.default((T([4, 12, 512, 64], f16), [48, 512, 64]), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Metal Support in ATen CMake\nDESCRIPTION: Sets up Metal-related source files and compilation flags based on build options. It handles different scenarios for Metal support, including export, regular usage on Apple platforms, and fallback.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_PYTORCH_METAL_EXPORT)\n  # Add files needed from exporting metal models(optimized_for_mobile)\n  set(all_cpu_cpp ${all_cpu_cpp} ${metal_cpp} ${metal_prepack_cpp})\nelseif(APPLE AND USE_PYTORCH_METAL)\n  # Compile Metal kernels\n  set(all_cpu_cpp ${all_cpu_cpp} ${metal_cpp} ${native_metal_srcs})\nelse()\n  set(all_cpu_cpp ${all_cpu_cpp} ${metal_cpp})\nendif()\n```\n\n----------------------------------------\n\nTITLE: PyTorch Model Batch Size Configuration List\nDESCRIPTION: A comprehensive listing of deep learning models with their recommended batch sizes. The configuration includes various model architectures such as inception, transformer-based models (ViT, Swin), convolutional networks (ResNet variants), and hybrid models with batch sizes ranging from 4 to 1024 depending on model complexity and memory requirements.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/timm_models_list.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nadv_inception_v3 128\nbeit_base_patch16_224 128\nbotnet26t_256 128\ncait_m36_384 4\ncoat_lite_mini 128\nconvit_base 128\nconvmixer_768_32 64\nconvnext_base 128\ncrossvit_9_240 256\ncspdarknet53 128\ndeit_base_distilled_patch16_224 128\ndla102 128\ndm_nfnet_f0 128\ndpn107 64\neca_botnext26ts_256 128\neca_halonext26ts 128\nese_vovnet19b_dw 256\nfbnetc_100 512\nfbnetv3_b 256\ngernet_l 128\nghostnet_100 512\ngluon_inception_v3 256\ngmixer_24_224 128\ngmlp_s16_224 128\nhrnet_w18 128\ninception_v3 128\njx_nest_base 128\nlcnet_050 256\nlevit_128 1024\nmixer_b16_224 128\nmixnet_l 128\nmnasnet_100 512\nmobilenetv2_100 128\nmobilenetv3_large_100 512\nmobilevit_s 128\nnfnet_l0 128\npit_b_224 64\npnasnet5large 32\npoolformer_m36 128\nregnety_002 1024\nrepvgg_a2 128\nres2net101_26w_4s 128\nres2net50_14w_8s 128\nres2next50 128\nresmlp_12_224 128\nresnest101e 128\nrexnet_100 256\nsebotnet33ts_256 64\nselecsls42b 128\nspnasnet_100 128\nswin_base_patch4_window7_224 128\nswsl_resnext101_32x16d 64\ntf_efficientnet_b0 128\ntf_mixnet_l 128\ntinynet_a 128\ntnt_s_patch16_224 128\ntwins_pcpvt_base 128\nvisformer_small 128\nvit_base_patch16_224 128\nvolo_d1_224 128\nxcit_large_24_p8_224 16\n```\n\n----------------------------------------\n\nTITLE: Mean Dimension Reduction Operations in PyTorch\nDESCRIPTION: Profiling data for mean operations that reduce tensor dimensions, showing count, tensor shapes, dimensions to reduce, and whether to keep dimensions. These operations calculate means across spatial dimensions, likely for global average pooling.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([32, 32, 112, 112], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 96, 56, 56], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 144, 56, 56], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 144, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), [2, 3], True), {})\ncnt: 3, ((T([32, 480, 14, 14], f16), [2, 3], True), {})\ncnt: 2, ((T([32, 672, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), [2, 3], True), {})\ncnt: 4, ((T([32, 1152, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 1280, 7, 7], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Defining, Configuring, and Linking backend_with_compiler Shared Library in CMake\nDESCRIPTION: Creates a shared library named `backend_with_compiler` using the source files defined in `BACKEND_WITH_COMPILER_SRCS`. If `USE_KINETO` is enabled, it adds the `-DUSE_KINETO` compile definition to the library. Finally, it links the library against the main `torch` library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(backend_with_compiler SHARED\n        ${BACKEND_WITH_COMPILER_SRCS}\n        )\nif(USE_KINETO)\n  set_target_properties(backend_with_compiler PROPERTIES COMPILE_FLAGS\n  \"-DUSE_KINETO\")\nendif()\ntarget_link_libraries(backend_with_compiler torch)\n```\n\n----------------------------------------\n\nTITLE: Exporting with Optional Input Provided using torch.export (Python)\nDESCRIPTION: This snippet demonstrates how `torch.export.export` handles optional arguments when they *are* provided during tracing. The module `M` has an optional argument `y`. When exported with both `x` and `y` provided, the resulting graph includes the code path where `y` is used (`return y * x`), and the exported function signature requires both `x` and `y` as inputs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/export.programming_model.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass M(torch.nn.Module):\n    def forward(self, x, y=None):\n        if y is not None:\n            return y * x\n        return x + x\n\n# Optional input is passed in\nep = torch.export.export(M(), (torch.randn(3, 3), torch.randn(3, 3)))\nprint(ep)\n\"\"\"\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 3]\", y: \"f32[3, 3]\"):\n            # File: /data/users/angelayi/pytorch/moo.py:15 in forward, code: return y * x\n            mul: \"f32[3, 3]\" = torch.ops.aten.mul.Tensor(y, x);  y = x = None\n            return (mul,)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.sum.SymInt Sum Reduction in PyTorch ATen\nDESCRIPTION: Documents observed calls to the sum reduction operation (`aten.sum.SymInt`) in PyTorch ATen. It lists different input tensor shapes (f16), the dimensions to sum over (e.g., [0] or [2, 3]), whether to keep the dimensions (True), and the respective call counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_23\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 3, ((T([128, 1536, 7, 7], f16), [2, 3], True), {})\ncnt: 6, ((T([128, 1536, 14, 14], f16), [2, 3], True), {})\ncnt: 2, ((T([128, 512, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 256, 56, 56], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Operator Tags Documentation in RestructuredText\nDESCRIPTION: Documentation for PyTorch's Tag class used for operator tagging, formatted in RestructuredText.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_11\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: Tag\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Defining Command for Generating Unboxing Kernels in CMake\nDESCRIPTION: Constructs the command (`GEN_COMMAND`) to execute the Python module `torchgen.gen_executorch` for generating C++ source files related to unboxing kernels. Specifies source paths, output directory, YAML configuration files, and flags. Also defines the list of expected output source files (`GEN_COMMAND_sources`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# Generate unboxing kernels\nset(GEN_COMMAND\n        \"${Python_EXECUTABLE}\" -m torchgen.gen_executorch\n        --source-path=${TEST_ROOT}\n        --install-dir=${OUTPUT_DIRECTORY}\n        --tags-path=${TORCH_ROOT}/aten/src/ATen/native/tags.yaml\n        --aten-yaml-path=${TORCH_ROOT}/aten/src/ATen/native/native_functions.yaml\n        --use-aten-lib\n        --op-selection-yaml-path=${TEST_ROOT}/selected_operators.yaml\n        --custom-ops-yaml-path=${TEST_ROOT}/custom_ops.yaml\n        )\nset(GEN_COMMAND_sources\n        ${OUTPUT_DIRECTORY}/RegisterCodegenUnboxedKernelsEverything.cpp\n        ${OUTPUT_DIRECTORY}/RegisterCPUCustomOps.cpp\n        ${OUTPUT_DIRECTORY}/Functions.h\n        ${OUTPUT_DIRECTORY}/NativeFunctions.h\n        ${OUTPUT_DIRECTORY}/CustomOpsNativeFunctions.h\n        )\nmessage(STATUS \"Generating sources for unboxing kernels ${GEN_COMMAND}\")\n```\n\n----------------------------------------\n\nTITLE: Type Conversion and Copy in PyTorch Python\nDESCRIPTION: These snippets signify the 'aten._to_copy' operator usage, indicating a tensor's type conversion with possibly specified properties like dtype, layout, device, and memory pinning. The operation converts input tensor types, outputs a copied tensor in the new type, and depends on PyTorch. Precise attributes guide the conversion process, crucial for computational efficiency in varied hardware environments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([5000, 4], f16),), {'dtype': f32})\ncnt: 1, ((T([5000], f16),), {'dtype': f32})\ncnt: 1, ((T([5000], i64),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda', 'pin_memory': False})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Matrix Multiplications in PyTorch\nDESCRIPTION: Logs instances of \\\"aten.mm.default\\\" which perform matrix multiplication on various shaped float16 tensors with possible memory strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([1024, 1024], f16), T([1024, 50265], f16, stride=(1, 1024))), {})\ncnt: 1, ((T([50265, 1024], f16, stride=(1, 50265)), T([1024, 1024], f16)), {})\ncnt: 1, ((T([1024, 50265], f16), T([50265, 1024], f16)), {})\ncnt: 12, ((T([1024, 1024], f16), T([1024, 4096], f16)), {})\ncnt: 12, ((T([1024, 1024], f16, stride=(1, 1024)), T([1024, 4096], f16)), {})\ncnt: 12, ((T([1024, 4096], f16), T([4096, 1024], f16)), {})\ncnt: 12, ((T([4096, 1024], f16, stride=(1, 4096)), T([1024, 1024], f16)), {})\ncnt: 48, ((T([1024, 1024], f16), T([1024, 1024], f16)), {})\ncnt: 48, ((T([1024, 1024], f16, stride=(1, 1024)), T([1024, 1024], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch QNNPACK Components\nDESCRIPTION: Configures the installation of the pytorch_qnnpack target, specifying destinations for libraries, archives, and header files using standard CMAKE installation directories.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS pytorch_qnnpack\n    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_INCLUDEDIR})\n```\n\n----------------------------------------\n\nTITLE: Tensor Copy Operations in PyTorch\nDESCRIPTION: This snippet shows a tensor copy operation that copies data from one tensor to another with the same shape. This is an in-place operation that modifies the destination tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([96, 9, 128, 128], f16), T([96, 9, 128, 128], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Referencing Supported TorchScript Ops in Documentation - reStructuredText\nDESCRIPTION: This snippet uses Sphinx's automodule directive in reStructuredText to automatically generate documentation for the torch.jit.supported_ops module. It makes functions and classes available from the specified module visible within the generated documentation, ensuring up-to-date references for TorchScript builtins. Dependencies include Sphinx and the relevant PyTorch modules, and the directive assumes torch.jit.supported_ops is importable in the documentation environment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_builtin_functions.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: torch.jit.supported_ops\n```\n\n----------------------------------------\n\nTITLE: Enumerating Tensor Signatures for relu_, sigmoid, sigmoid_backward, and silu_ Operators - PyTorch - Python\nDESCRIPTION: This block sequences invocation examples for multiple PyTorch in-place and functional operators (relu_, sigmoid, sigmoid_backward, silu_). Each entry encodes the shapes and dtypes of input (and where applicable, output or gradient) tensors for specific operator variants. Arguments are compactly described as tuples containing tensor specifications, enabling easy tracking of tensor patterns for operator call statistics, testing, or kernel dispatch analysis. There are no external runtime dependencies, but implied use is by tensor-level operator instrumentation. Inputs and outputs are schematic and use symbolic tensor signatures. Limitations: These are data lines or log entries, not executable code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 19, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 25, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 30, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 36, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 42, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 47, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 53, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 58, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 64, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 70, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 75, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 81, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 87, 1, 1], f16),), {})\nOperator: aten.relu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 228, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 300, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 366, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 432, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 504, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 570, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 636, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 702, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 768, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 840, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 906, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 972, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 1044, 1, 1], f16),), {})\nOperator: aten.sigmoid.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 1044, 1, 1], f16), T([128, 1044, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 972, 1, 1], f16), T([128, 972, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 906, 1, 1], f16), T([128, 906, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 840, 1, 1], f16), T([128, 840, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 768, 1, 1], f16), T([128, 768, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 702, 1, 1], f16), T([128, 702, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 636, 1, 1], f16), T([128, 636, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 570, 1, 1], f16), T([128, 570, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 504, 1, 1], f16), T([128, 504, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 432, 1, 1], f16), T([128, 432, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 366, 1, 1], f16), T([128, 366, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 300, 1, 1], f16), T([128, 300, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 228, 1, 1], f16), T([128, 228, 1, 1], f16)), {})\nOperator: aten.sigmoid_backward.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 32, 112, 112], f16),), {})\nOperator: aten.silu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 96, 112, 112], f16),), {})\nOperator: aten.silu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 162, 56, 56], f16),), {})\nOperator: aten.silu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 228, 56, 56], f16),), {})\nOperator: aten.silu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 300, 28, 28], f16),), {})\nOperator: aten.silu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 366, 28, 28], f16),), {})\nOperator: aten.silu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 432, 14, 14], f16),), {})\nOperator: aten.silu_.default\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 504, 14, 14], f16),), {})\nOperator: aten.silu_.default\n```\n\n----------------------------------------\n\nTITLE: Reporting Performance Improvement with vmap\nDESCRIPTION: Calls the previously defined `get_perf` helper function to compare the benchmark results. It passes the timing measurement object for the `vmap`-based method (`with_vmap_timing`) as 'first' and the timing for the naive method (`no_vmap_timing`) as 'second'. This prints the calculated percentage performance improvement achieved by using the `functorch` `vmap` approach compared to the naive loop.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nget_perf(with_vmap_timing, \"vmap\", no_vmap_timing,\"no vmap\" )\n```\n\n----------------------------------------\n\nTITLE: Running Store and Process Group Wrapper Tests in Python\nDESCRIPTION: Commands to run unit tests for the Store component and Process Group Wrapper in PyTorch Distributed using Python.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython test/distributed/test_store.py\npython test/distributed/test_pg_wrapper.py\n```\n\n----------------------------------------\n\nTITLE: Setting Default Installation Directories\nDESCRIPTION: Defines default installation directories for binaries, libraries, includes, and shared files if not already set.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT AT_INSTALL_BIN_DIR OR NOT AT_INSTALL_LIB_DIR OR NOT AT_INSTALL_INCLUDE_DIR OR NOT AT_INSTALL_SHARE_DIR)\n  set(AT_INSTALL_BIN_DIR \"bin\" CACHE PATH \"AT install binary subdirectory\")\n  set(AT_INSTALL_LIB_DIR \"lib\" CACHE PATH \"AT install library subdirectory\")\n  set(AT_INSTALL_INCLUDE_DIR \"include\" CACHE PATH \"AT install include subdirectory\")\n  set(AT_INSTALL_SHARE_DIR \"share\" CACHE PATH \"AT install include subdirectory\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Unsupported Autocast Usage Pattern\nDESCRIPTION: Example showing an uncommon autocast usage pattern that is not supported in JIT scripting, specifically reusing an autocast instance in nested contexts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.cpu.amp import autocast\n\n@torch.jit.script\ndef fn(a, b, c, d):\n    with autocast(enabled=True) as autocast_instance: # not supported\n        ...\n        with autocast_instance:\n            ...\n```\n\n----------------------------------------\n\nTITLE: Generating Pad_mm Heuristic from Collected Data\nDESCRIPTION: This bash command runs a script to generate the pad_mm heuristic using the previously collected training data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/pad_mm/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash generate_heuristic_pad_mm.sh generate\n```\n\n----------------------------------------\n\nTITLE: Logging Aten Operator: aten.unbind.int (Text)\nDESCRIPTION: Log entries showing example invocations of the 'aten.unbind.int' operator. Each line ('cnt') represents a call signature, detailing the input tensor (T) to be unbound, including its shape, data type (f16), and stride information. The dimension to unbind along is implicitly the first one (dim=0) unless otherwise specified in the operator's definition or context not shown here.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.unbind.int\ncnt: 3, ((T([3, 64, 4, 962, 64], f16, stride=(256, 738816, 64, 768, 1)),), {})\ncnt: 6, ((T([3, 64, 8, 257, 64], f16, stride=(512, 394752, 64, 1536, 1)),), {})\ncnt: 4, ((T([3, 64, 16, 65, 64], f16, stride=(1024, 199680, 64, 3072, 1)),), {})\n```\n\n----------------------------------------\n\nTITLE: Python Command for Running MAML Examples\nDESCRIPTION: Basic command line instruction for executing any of the MAML example scripts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/examples/maml_omniglot/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython {filename}\n```\n\n----------------------------------------\n\nTITLE: Installing libshm components in CMake\nDESCRIPTION: Configures the installation of the 'shm' library, its header file, and optionally the PDB file for MSVC builds. This ensures all necessary components are properly installed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/lib/libshm_windows/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(TARGETS shm DESTINATION \"${LIBSHM_INSTALL_LIB_SUBDIR}\")\ninstall(FILES libshm.h DESTINATION \"include\")\n\nif(MSVC AND BUILD_SHARED_LIBS)\n  install(FILES $<TARGET_PDB_FILE:shm> DESTINATION \"${LIBSHM_INSTALL_LIB_SUBDIR}\" OPTIONAL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Examining Backward Convolution Operations in PyTorch Neural Network\nDESCRIPTION: This snippet shows the backward convolution operations used during the backpropagation phase of training. Each operation includes input gradients, output gradients, weight tensors, and configuration parameters, revealing the gradient flow through the network architecture.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 1280, 6, 6], f16), T([128, 320, 6, 6], f16), T([1280, 320, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 320, 6, 6], f16), T([128, 1152, 6, 6], f16), T([320, 1152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 5, ((T([128, 1152, 1, 1], f16), T([128, 48, 1, 1], f16), T([1152, 48, 1, 1], f16), [1152], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 5, ((T([128, 48, 1, 1], f16), T([128, 1152, 1, 1], f16), T([48, 1152, 1, 1], f16), [48], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 1152, 6, 6], f16), T([128, 1152, 6, 6], f16), T([1152, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1152, [True, True, False]), {})\ncnt: 5, ((T([128, 1152, 6, 6], f16), T([128, 192, 6, 6], f16), T([1152, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 192, 6, 6], f16), T([128, 1152, 6, 6], f16), T([192, 1152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 1152, 6, 6], f16), T([128, 1152, 6, 6], f16), T([1152, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 1152, [True, True, False]), {})\ncnt: 1, ((T([128, 192, 6, 6], f16), T([128, 672, 6, 6], f16), T([192, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 672, 1, 1], f16), T([128, 28, 1, 1], f16), T([672, 28, 1, 1], f16), [672], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 4, ((T([128, 28, 1, 1], f16), T([128, 672, 1, 1], f16), T([28, 672, 1, 1], f16), [28], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([128, 672, 6, 6], f16), T([128, 672, 12, 12], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 4, ((T([128, 672, 12, 12], f16), T([128, 112, 12, 12], f16), T([672, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Using Final Variables in TorchScript If Statements\nDESCRIPTION: Demonstrates how using a final-annotated boolean variable in an if statement results in only the true branch being evaluated.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\na : torch.jit.final[Bool] = True\n\nif a:\n    return torch.empty(2,3)\nelse:\n    return []\n```\n\n----------------------------------------\n\nTITLE: Setting JIT Test Root Directory Variable in CMake\nDESCRIPTION: Defines a CMake variable `JIT_TEST_ROOT` pointing to the directory containing the C++ JIT test source files relative to the `TORCH_ROOT` directory. This variable is used later to reference source files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(JIT_TEST_ROOT ${TORCH_ROOT}/test/cpp/jit)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for PyTorch Commit Analysis\nDESCRIPTION: Imports required Python libraries including pandas, numpy, and custom modules for data analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/explore.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nfrom pprint import pprint\nfrom collections import Counter\nimport common\nimport math\n```\n\n----------------------------------------\n\nTITLE: PyTorch CUDA BLAS Operations\nDESCRIPTION: CUDA BLAS implementations for matrix operations like GEMM for different data types including Half and Float.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_28\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4cuda4blas5bgemmIN3c104HalfEEEvcclllNS_10OpMathTypeIT_E4typeEPKS6_llSA_llS8_PS6_lll\n_ZN2at4cuda4blas4gemmIN3c104HalfEEEvcclllNS_10OpMathTypeIT_E4typeEPKS6_lSA_lS8_PS6_l\n```\n\n----------------------------------------\n\nTITLE: Downloading Mixed MM Datasets for A100/H100\nDESCRIPTION: Downloads the pre-existing datasets for A100 and H100 GPUs used in mixed matrix multiplication heuristics\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mixed_mm/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbash get_mixedmm_dataset.sh\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Copy Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of tensor copy operations in the model. It includes the shapes of the tensors being copied and the frequency of these operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\ncnt: 15, ((T([128, 160, 7, 7], f16), T([128, 160, 7, 7], f16)), {})\ncnt: 6, ((T([128, 112, 14, 14], f16), T([128, 112, 14, 14], f16)), {})\ncnt: 12, ((T([128, 80, 14, 14], f16), T([128, 80, 14, 14], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Sum Operation with Symbolic Dimensions\nDESCRIPTION: This snippet shows a sum operation on a tensor of shape [128, 1000] with f16 precision along dimension 0, keeping the dimension in the result (keepdim=True). The operation uses symbolic dimensions (SymInt).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Warning Message for PyTorch Compilation\nDESCRIPTION: A markdown warning block notifying users about the experimental nature of compilation features and their incompatibility with functorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/examples/compilation/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n> **WARNING**: Compilation is currently very experimental and example\\nhere don't work out of the box with functorch\n```\n\n----------------------------------------\n\nTITLE: Analyzing Matrix Multiplication with Bias Addition in PyTorch\nDESCRIPTION: Record of aten.addmm.default operator call for matrix multiplication with bias addition. This operation uses half-precision (f16) tensors and likely represents the final fully connected layer in a neural network with 1000 output classes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([32, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Copy Operation\nDESCRIPTION: This snippet shows a tensor copy operation between two float16 tensors with the same shape.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 3, 192, 192], f16), T([128, 3, 192, 192], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Error Message Example for Multiple OpenMP Runtime Initialization\nDESCRIPTION: An example error message that occurs when multiple copies of the OpenMP runtime have been linked into a program, causing potential performance degradation or incorrect results. This illustrates why the FindOpenMP.cmake module was modified to avoid linking multiple OpenMP libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/Modules/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOMP: Error #15: Initializing libomp.dylib, but found libiomp5.dylib already\nitialized.\n\nOMP: Hint This means that multiple copies of the OpenMP runtime have been\nlinked into the program. That is dangerous, since it can degrade performance\nor cause incorrect results. The best thing to do is to ensure that only a\nsingle OpenMP runtime is linked into the process, e.g. by avoiding static\nlinking of the OpenMP runtime in any library. As an unsafe, unsupported,\nundocumented workaround you can set the environment variable\nKMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but\nthat may cause crashes or silently produce incorrect results. For more\ninformation, please see http://openmp.llvm.org/\n```\n\n----------------------------------------\n\nTITLE: Operator Frequency and Parameter Analysis in PyTorch\nDESCRIPTION: This section presents different PyTorch operators with their execution count and the shape of tensors they operate on. It includes operators like `aten.add`, `aten.mul`, and `aten.mm`, analyzing their frequency with various tensor shapes and data types, such as `f16` for half-precision floating points. This structured information is crucial for performance analysis and optimization in machine learning models using PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GoogleFnet_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: pseudo\nCODE:\n```\nOperator: aten._fft_c2c.default\\ncnt: 12, ((T([1, 512, 768], c32), [1, 2], 0, True), {})\\nOperator: aten._log_softmax.default\\ncnt: 1, ((T([512, 32000], f16), 1, False), {})\\nOperator: aten._log_softmax_backward_data.default\\ncnt: 1, ((T([512, 32000], f16), T([512, 32000], f16), 1, f16), {})\\nOperator: aten._to_copy.default\\ncnt: 12, ((T([1, 512, 768], f16),), {'dtype': c32})\\nOperator: aten.add.Tensor\\ncnt: 28, ((T([1, 512, 768], f16), T([1, 512, 768], f16)), {})\\ncnt: 24, ((T([1, 512, 768], f16), T([1, 512, 768], f16, stride=(786432, 1536, 2))), {})\\ncnt: 36, ((T([1, 512, 3072], f16), T([1, 512, 3072], f16)), {})\\ncnt: 12, ((T([1, 512, 3072], f16), 1.0), {})\\ncnt: 1, ((T([1, 512, 768], f16), 1.0), {})\\ncnt: 1, ((T([32000, 768], f16), T([32000, 768], f16)), {})...\\n\n```\n\n----------------------------------------\n\nTITLE: Custom Side-Effect Op Registration in C++\nDESCRIPTION: Example of registering a custom operator that has side effects using TORCH_LIBRARY macro and AliasAnalysisKind::CONSERVATIVE flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_35\n\nLANGUAGE: cpp\nCODE:\n```\nTORCH_LIBRARY(my_library, m) {\n  m.def(torch::schema(\n    \"my_logging_op(Tensor data) -> ()\",\n    c10::AliasAnalysisKind::CONSERVATIVE\"));\n}\n```\n\n----------------------------------------\n\nTITLE: Slicing Backward Computation with slice in PyTorch (Python)\nDESCRIPTION: Conducts aten.slice_backward necessary when integrating backward passes through slice operations in tensors, ensuring proper derivatives are passed back during model training and updates.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\naten.slice_backward.default\ncnt: 1, ((T([16, 512, 768], f16), [16, 512, 768], 0, 0, 9223372036854775807, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Defining BaseListVariable class with container specification\nDESCRIPTION: This snippet defines the BaseListVariable class that inherits from VariableTrackerContainer and specifies 'set' as its container type via the our_container class variable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/set_linter_testdata/includes_doesnt_change.py.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass BaseListVariable(VariableTrackerContainer):\n    our_container = set\n```\n\n----------------------------------------\n\nTITLE: Employing aten._unsafe_view for Tensor Reshaping in PyTorch\nDESCRIPTION: This code utilizes aten._unsafe_view.default to reshape two tensors of shape [16, 4, 128, 32] and [64, 128, 32] to new shapes without modifying data pointers, indicating high efficiency. PyTorch and CUDA are essential for quick processing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\naten._unsafe_view.default, ((T([16, 4, 128, 32], f16), [64, 128, 32]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten._unsafe_view.default, ((T([64, 128, 32], f16), [16, 4, 128, 32]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Operations in PyTorch\nDESCRIPTION: This code snippet represents a series of tensor operations in PyTorch, including their shapes, data types, and occurrence counts. It showcases various tensor configurations used in a neural network model, primarily focusing on f16 (float16) data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 56, 56], f16), [60, 60, 60, 60], 1), {})\ncnt: 3, ((T([64, 56, 28, 28], f16), [28, 28], 1), {})\ncnt: 6, ((T([64, 336, 28, 28], f16), [168, 168], 1), {})\ncnt: 1, ((T([64, 336, 28, 28], f16), [112, 112, 112], 1), {})\ncnt: 3, ((T([64, 104, 14, 14], f16), [52, 52], 1), {})\ncnt: 3, ((T([64, 624, 14, 14], f16), [156, 156, 156, 156], 1), {})\ncnt: 3, ((T([64, 624, 14, 14], f16), [312, 312], 1), {})\ncnt: 3, ((T([64, 160, 14, 14], f16), [80, 80], 1), {})\ncnt: 3, ((T([64, 480, 14, 14], f16), [120, 120, 120, 120], 1), {})\ncnt: 3, ((T([64, 480, 14, 14], f16), [240, 240], 1), {})\ncnt: 1, ((T([64, 960, 14, 14], f16), [240, 240, 240, 240], 1), {})\ncnt: 3, ((T([64, 1584, 7, 7], f16), [396, 396, 396, 396], 1), {})\ncnt: 3, ((T([64, 1584, 7, 7], f16), [792, 792], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Installing ATen Headers While Preserving Directory Structure in CMake\nDESCRIPTION: Iterates through the list of headers defined in `INSTALL_HEADERS`. For each header file, it calculates the relative path within the source directory (stripping `CMAKE_CURRENT_SOURCE_DIR` and `Torch_SOURCE_DIR` prefixes and adding an `ATen/` prefix) to determine the target installation subdirectory. It then uses the `install(FILES ... DESTINATION ...)` command to install the header into the correct subdirectory under `${AT_INSTALL_INCLUDE_DIR}`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\n# https://stackoverflow.com/questions/11096471/how-can-i-install-a-hierarchy-of-files-using-cmake\nforeach(HEADER  ${INSTALL_HEADERS})\n  string(REPLACE \"${CMAKE_CURRENT_SOURCE_DIR}/\" \"ATen/\" HEADER_SUB ${HEADER})\n  string(REPLACE \"${Torch_SOURCE_DIR}/\" \"\" HEADER_SUB ${HEADER_SUB})\n  get_filename_component(DIR ${HEADER_SUB} DIRECTORY)\n  install(FILES ${HEADER} DESTINATION \"${AT_INSTALL_INCLUDE_DIR}/${DIR}\")\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Backward Pass using aten._log_softmax_backward_data in PyTorch\nDESCRIPTION: Utilizes the aten._log_softmax_backward_data.default operator for computing the gradient of the log softmax operation. It involves tensors with dimensions [2048, 30522] and data type f16, requiring the PyTorch library along with CUDA for accelerated performance.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/MobileBertForMaskedLM_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\naten._log_softmax_backward_data.default, ((T([2048, 30522], f16), T([2048, 30522], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Running Specific PyTorch Python Test File\nDESCRIPTION: Executes a specific PyTorch Python test file, in this case the JIT tests.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython test/test_jit.py\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA/ROCm Support for test_jit in CMake\nDESCRIPTION: Conditionally configures GPU support for the `test_jit` executable. If `USE_CUDA` is enabled, it defines `USE_CUDA` and suppresses sign-compare warnings (except on MSVC). If `USE_ROCM` is enabled, it links against ROCm libraries (hiprtc, amdhip64) and defines `USE_ROCM`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_CUDA)\n  target_compile_definitions(test_jit PRIVATE USE_CUDA)\n  # Suppress sign compare checks for NVFUSER JIT tests\n  if(NOT MSVC)\n    target_compile_options(test_jit PRIVATE $<$<COMPILE_LANGUAGE:CXX>:-Wno-sign-compare>)\n  endif()\nelsif(USE_ROCM)\n  target_link_libraries(test_jit PRIVATE\n    hiprtc::hiprtc\n    hip::amdhip64\n    ${TORCH_CUDA_LIBRARIES})\n\n  target_compile_definitions(test_jit PRIVATE USE_ROCM)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Caffe2 CPU Source Files in CMake\nDESCRIPTION: This CMake code configures the source files to be included in the Caffe2 CPU build. It adds core utilities like string_utils and ThreadPool by default, conditionally adds pthreadpool sources when USE_PTHREADPOOL is enabled, and excludes proto_wrap.cc for mobile builds. The final list is set to the parent scope.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/utils/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND Caffe2_CPU_SRCS\n  utils/string_utils.cc\n  utils/threadpool/ThreadPool.cc\n)\n\nif(USE_PTHREADPOOL)\n  list(APPEND Caffe2_CPU_SRCS\n    utils/threadpool/pthreadpool-cpp.cc\n    utils/threadpool/thread_pool_guard.cpp\n  )\nendif()\n\nif(NOT INTERN_BUILD_MOBILE)\n  list(APPEND Caffe2_CPU_SRCS\n    utils/proto_wrap.cc\n  )\nendif()\nset(Caffe2_CPU_SRCS ${Caffe2_CPU_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization Operations\nDESCRIPTION: Native layer normalization operations on tensors of various shapes, including both forward and backward passes with epsilon value of 1e-05\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swin_base_patch4_window7_224_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([64, 3136, 128], f16, stride=(401408, 1, 3136)), [128], T([128], f16), T([128], f16), 1e-05), {})\ncnt: 4, ((T([64, 3136, 128], f16), [128], T([128], f16), T([128], f16), 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Generating H100 Mixed MM Heuristics\nDESCRIPTION: Generates the mixed matrix multiplication heuristic specifically for NVIDIA H100 GPU using pre-collected data\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mixed_mm/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbash gen_mixedmm_heuristic_h100.sh\n```\n\n----------------------------------------\n\nTITLE: Adding Static CUDA Runtime Dependencies in CMake\nDESCRIPTION: Checks if the environment variable `ATEN_STATIC_CUDA` is set. If true, it appends the static CUDA OS abstraction library (`CUDA::culibos`) and the static CUDA runtime library (`CUDA::cudart_static`) to the `ATen_CUDA_DEPENDENCY_LIBS` list.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\n  if($ENV{ATEN_STATIC_CUDA})\n    list(APPEND ATen_CUDA_DEPENDENCY_LIBS\n      CUDA::culibos\n      CUDA::cudart_static\n    )\n  endif($ENV{ATEN_STATIC_CUDA})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding LibTorch Include Directories in CMake\nDESCRIPTION: Adds the LibTorch include directory, specified by the `libtorch_include_DIR` variable, to the JNI target's public include directories. The `BEFORE` keyword ensures it takes precedence, and `$<BUILD_INTERFACE:...>` makes it available to consuming targets.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(${PYTORCH_JNI_TARGET} BEFORE\nPUBLIC $<BUILD_INTERFACE:${libtorch_include_DIR}>)\n```\n\n----------------------------------------\n\nTITLE: Logging Aten Operator: aten.slice_backward.default (Text)\nDESCRIPTION: Log entries showing example invocations of the 'aten.slice_backward.default' operator. Each line represents a call signature observed ('cnt'). Arguments include an input tensor (T) with shape and data type (f16), the original size list, dimension, start index, end index (potentially max value), and step.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.slice_backward.default\ncnt: 1, ((T([64, 1, 1024], f16), [64, 1, 1024], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([64, 1, 1024], f16), [64, 65, 1024], 1, 0, 1, 1), {})\ncnt: 1, ((T([64, 65, 1024], f16), [64, 65, 1024], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([64, 256, 512], f16), [64, 257, 512], 1, 1, 9223372036854775807, 1), {})\ncnt: 2, ((T([64, 257, 512], f16), [64, 257, 512], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([64, 1, 512], f16), [64, 257, 512], 1, 0, 1, 1), {})\ncnt: 1, ((T([64, 961, 256], f16), [64, 962, 256], 1, 1, 9223372036854775807, 1), {})\ncnt: 2, ((T([64, 962, 256], f16), [64, 962, 256], 0, 0, 9223372036854775807, 1), {})\ncnt: 1, ((T([64, 1, 256], f16), [64, 962, 256], 1, 0, 1, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Disabling Automatic Device Guard in PyTorch Kernels (native_functions.yaml)\nDESCRIPTION: Demonstrates the `device_guard: False` setting in `native_functions.yaml`. This flag prevents the automatic generation of a `DeviceGuard` by ATen's code generation, which is useful for kernels that manage device context manually or do not interact with specific devices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ndevice_guard: False\n```\n\n----------------------------------------\n\nTITLE: Configuring QNNPACK Benchmark Executables in CMake\nDESCRIPTION: Sets up multiple benchmark executables for QNNPACK operations, including compiler flags, include directories, and linking libraries. This configuration is applied when PYTORCH_QNNPACK_BUILD_BENCHMARKS is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(add-bench bench/add.cc)\nset_target_properties(add-bench PROPERTIES\n  CXX_STANDARD 14\n  CXX_STANDARD_REQUIRED YES\n  CXX_EXTENSIONS NO)\ntarget_link_libraries(add-bench PRIVATE pytorch_qnnpack benchmark)\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication in ATen Library\nDESCRIPTION: Matrix multiplication is carried out using the mm operator which multiplies matrices or tensors in PyTorch, commonly part of feedforward operations in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([32, 1000], f16), T([1000, 2048], f16)), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([1000, 32], f16, stride=(1, 1000)), T([32, 2048], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Example Version Output\nDESCRIPTION: Sample output showing the version information format when running the version checking code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/versioning.rst#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nPyTorch version from parts: 1.8.0\nPyTorch version: 1.8.0\n```\n\n----------------------------------------\n\nTITLE: Configuring C10 Benchmark Binaries in CMake\nDESCRIPTION: Configures the build process for C10 benchmark executables. For each CPP benchmark file, it creates an executable target, links required libraries (C10 and benchmark), and sets up installation properties when needed. Includes conditional installation configuration with custom RPATH settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/benchmark/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE C10_ALL_BENCH_FILES *.cpp)\nif(BUILD_TEST)\n  foreach(bench_src ${C10_ALL_BENCH_FILES})\n    get_filename_component(bench_file_name ${bench_src} NAME_WE)\n    set(bench_name \"c10_${bench_file_name}\")\n    add_executable(${bench_name} \"${bench_src}\")\n    target_link_libraries(${bench_name} ${C10_LIB} benchmark)\n    if(INSTALL_TEST)\n      set_target_properties(${bench_name} PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n      install(TARGETS ${bench_name} DESTINATION test)\n    endif()\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing ATen THC CUDA Header Files using CMake\nDESCRIPTION: This CMake snippet uses the `install` command to specify that the CUDA header files `THCAtomics.cuh` and `THCDeviceUtils.cuh` should be installed. The `DESTINATION` parameter defines the installation path relative to the installation prefix, placing the files within a `THC` subdirectory under the path specified by the `ATEN_INSTALL_INCLUDE_SUBDIR` variable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/THC/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES\n          THCAtomics.cuh\n          THCDeviceUtils.cuh\n          DESTINATION \"${ATEN_INSTALL_INCLUDE_SUBDIR}/THC\")\n```\n\n----------------------------------------\n\nTITLE: PyTorch IR Expression (Expr) Definition\nDESCRIPTION: Defines the structure of Expression nodes in the PyTorch IR. Expressions represent computations, variables, buffers, and mathematical operations. They serve as building blocks for more complex statements and can be nested within other expressions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/tensorexpr/IRSpecification.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nExpr\n= Var()\n| Buf(base_handle_ = Var, dims = [Expr], qscale_ = Expr, qzero_ = Expr)\n| Term(variables_ = [Expr], scalar_ = Expr)\n| Polynomial(variables_ = [Term], scalar_ = Expr)\n| MaxTerm(variables_ = [Term], scalar_ = Expr)\n| MinTerm(variables_ = [Term], scalar_ = Expr)\n| Cast(src_value_ = Expr)\n| BitCast(src_value_ = Expr)\n| BinaryOpNode(lhs_ = Expr, rhs_ = Expr)\n| ImmInt/ImmFloat/etc.()\n| Ramp(base_ = Expr, stride_ = Expr)\n| Load(buf_ = Buf, indices = [Expr], mask_ = Expr)\n| Broadcast(value_ = Expr, lanes_ = int)\n| IfThenElse(condition_ = Expr, true_ = Expr, false_ = Expr)\n| Intrinsics(op_type_ = {kSin, kPow, kExp, ...}, params_ = [Expr])\n| CompareSelect(lhs_ = Expr, rhs_ = Expr, ret_val1_ = Expr, ret_val2_ = Expr, compare_op_ = {kEQ, kGT, kGE, ...}, bias_ = {kUnbiased, kLikely, kUnlikely})\n| ReduceOp(body_ = Expr, reduce_args_ = [Var], reducer = Reducer)\n```\n\n----------------------------------------\n\nTITLE: Stacking Tensors in PyTorch\nDESCRIPTION: This snippet stacks tensors along a new dimension. It's commonly used to combine multiple tensors into a single tensor, often for creating coordinate grids or combining features.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, (([T([12, 16], i64, stride=(0, 1)), T([12, 16], i64, stride=(1, 0))], 2), {})\ncnt: 1, (([T([24, 32], i64, stride=(0, 1)), T([24, 32], i64, stride=(1, 0))], 2), {})\ncnt: 1, (([T([48, 64], i64, stride=(0, 1)), T([48, 64], i64, stride=(1, 0))], 2), {})\n```\n\n----------------------------------------\n\nTITLE: C++ Timer Creation for TorchScript Mode\nDESCRIPTION: Example of creating a Timer instance for TorchScript mode in C++. It includes statement code to prepare inputs as IValues and call the model's forward method, with setup code that loads the model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nTimer(\n    stmt=\"\"\"\n        std::vector<torch::jit::IValue> ivalue_inputs(\n            torch::jit::IValue({x}),\n            torch::jit::IValue({w})\n        );\n        auto y = jit_model.forward(ivalue_inputs);\n    \"\"\",\n    setup=\"\"\"\n        # benchmark.setup.cpp_setup\n        # jit_model = torch::jit::load(...)\n        # Warm up jit_model\n    \"\"\",\n)\n```\n\n----------------------------------------\n\nTITLE: Conditionally Adding Linux Linker Flags for test_jit in CMake\nDESCRIPTION: Checks if the operating system is Linux. If true, it adds specific linker flags (`-Wl,--no-as-needed,...--as-needed`) when linking the `test_jit` executable. This ensures that the `jitbackend_test` and `backend_with_compiler` shared libraries are correctly linked, potentially resolving issues with dependencies not being explicitly used but still required.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nif(LINUX)\n  #Update to target_link_options when CMake version can be upgraded\n  target_link_libraries(test_jit PRIVATE \"-Wl,--no-as-needed,$<TARGET_FILE:jitbackend_test>,$<TARGET_FILE:backend_with_compiler>,--as-needed\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining map_nt Function for NestedTensors in C++\nDESCRIPTION: The `map_nt` function applies a given operation to the dense representation of a NestedTensor. It utilizes the NestedTensor's underlying buffer, preserving its structure. Dependencies include getting the NestedTensor implementation and its sizes. Inputs are a NestedTensor and a function `f`, and the output is a new NestedTensor post-application of `f`. This function is designed for operations that can treat the NestedTensor as a dense structure.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/nested/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\ntemplate <typename Func>\nTensor map_nt(const Tensor& nt, Func f) {\n  auto* nt_impl = get_nested_tensor_impl(nt);\n  const auto& sizes = nt_impl->get_nested_sizes();\n  return at::detail::make_tensor<NestedTensorImpl>(f(nt_impl->get_buffer()), sizes);\n}\n```\n\n----------------------------------------\n\nTITLE: Registering custom sharding with experimental register_sharding\nDESCRIPTION: Registers a custom sharding function for a specific operator using the experimental register_sharding decorator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n@register_sharding(torch.nn.Linear)\ndef sharded_linear(types, args, kwargs, default_sharding):\n    # Custom sharding logic here\n```\n\n----------------------------------------\n\nTITLE: Adding Core Distributed Tests\nDESCRIPTION: Adds core distributed tests for Backoff, FileStore, TCPStore, and HashStore components. Links against torch_cpu and gtest.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/c10d/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nc10d_add_test(BackoffTest.cpp LINK_LIBRARIES torch_cpu gtest_main INSTALL_TEST OFF)\nc10d_add_test(FileStoreTest.cpp LINK_LIBRARIES torch_cpu gtest_main INSTALL_TEST ${INSTALL_TEST})\nc10d_add_test(TCPStoreTest.cpp LINK_LIBRARIES torch_cpu gtest_main INSTALL_TEST ${INSTALL_TEST})\nif(NOT WIN32)\n  c10d_add_test(HashStoreTest.cpp LINK_LIBRARIES torch_cpu gtest_main INSTALL_TEST ${INSTALL_TEST})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Recording an Event in PyTorch Distributed Elastic\nDESCRIPTION: This function is used to record an event in the distributed elastic system. It's part of the events API and likely takes parameters to describe the event being recorded.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.elastic.events.record\n```\n\n----------------------------------------\n\nTITLE: Counting Uncategorized Commits\nDESCRIPTION: Counts the number of uncategorized commits that are user-facing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/explore.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# The number un categorized and no topic commits\nno_category = commit_list_df.query(\"category == 'Uncategorized' & topic != 'not user facing'\")\nprint(len(no_category))\n```\n\n----------------------------------------\n\nTITLE: Generating Custom GPU Mixed MM Heuristics\nDESCRIPTION: Processes collected training data to generate new mixed matrix multiplication heuristics for custom GPU configurations\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mixed_mm/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbash generate_heuristic.sh generate\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Shapes and Counts in PyTorch\nDESCRIPTION: This code snippet represents a series of tensor shape analyses, likely for a PyTorch model. Each line shows the count of occurrences, tensor shapes, data types (f16 for float16), and stride information where applicable. The tensors have varying dimensions (1D to 4D) and sizes, suggesting different layers or operations in a neural network architecture.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([128, 240, 1, 1], f16), T([128, 240, 1, 1], f16), 0), {})\ncnt: 4, ((T([128, 480, 7, 7], f16, stride=(47040, 49, 7, 1)), T([128, 480, 7, 7], f16), 0), {})\ncnt: 4, ((T([128, 480, 7, 7], f16), T([128, 480, 7, 7], f16), 0), {})\ncnt: 2, ((T([128, 168, 1, 1], f16), T([128, 168, 1, 1], f16), 0), {})\ncnt: 2, ((T([128, 336, 14, 14], f16, stride=(131712, 196, 14, 1)), T([128, 336, 14, 14], f16), 0), {})\ncnt: 2, ((T([128, 336, 14, 14], f16), T([128, 336, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 120, 1, 1], f16), T([128, 120, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([128, 240, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16), 0), {})\ncnt: 2, ((T([128, 92, 14, 14], f16, stride=(36064, 196, 14, 1)), T([128, 92, 14, 14], f16), 0), {})\ncnt: 2, ((T([128, 92, 14, 14], f16), T([128, 92, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 100, 14, 14], f16, stride=(39200, 196, 14, 1)), T([128, 100, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 100, 14, 14], f16), T([128, 100, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 120, 28, 28], f16, stride=(188160, 784, 28, 1)), T([128, 120, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 32, 1, 1], f16), T([128, 32, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 60, 28, 28], f16, stride=(94080, 784, 28, 1)), T([128, 60, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 60, 28, 28], f16), T([128, 60, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 20, 1, 1], f16), T([128, 20, 1, 1], f16), 0), {})\ncnt: 2, ((T([128, 36, 56, 56], f16, stride=(225792, 3136, 56, 1)), T([128, 36, 56, 56], f16), 0), {})\ncnt: 2, ((T([128, 36, 56, 56], f16), T([128, 36, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 24, 112, 112], f16, stride=(602112, 12544, 112, 1)), T([128, 24, 112, 112], f16), 0), {})\ncnt: 1, ((T([128, 24, 112, 112], f16), T([128, 24, 112, 112], f16), 0), {})\ncnt: 1, ((T([128, 8, 112, 112], f16, stride=(200704, 12544, 112, 1)), T([128, 8, 112, 112], f16), 0), {})\ncnt: 1, ((T([128, 8, 112, 112], f16), T([128, 8, 112, 112], f16), 0), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Updating ATen CUDA Include Path in CMake\nDESCRIPTION: This snippet uses the CMake `set` command to append the current source directory (`CMAKE_CURRENT_SOURCE_DIR`) to the `ATen_CUDA_INCLUDE` variable list. The `PARENT_SCOPE` option makes this updated list available in the CMake scope that included this file, effectively adding the current directory to the include paths for ATen CUDA compilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/THC/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(ATen_CUDA_INCLUDE ${ATen_CUDA_INCLUDE}\n  \"${CMAKE_CURRENT_SOURCE_DIR}\"\nPARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.mul.Tensor in PyTorch ATen\nDESCRIPTION: Logs calls to the `aten.mul.Tensor` operator, performing element-wise multiplication between two tensors. The examples show operations on 4D tensors (common in convolutional layers) with `f16` data type, including cases potentially involving broadcasting (e.g., multiplying a `[128, 128, 7, 7]` tensor by a `[128, 128, 1, 1]` tensor). The `cnt` indicates the frequency of each call pattern.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 2, ((T([128, 128, 7, 7], f16), T([128, 128, 1, 1], f16)), {})\ncnt: 2, ((T([128, 256, 7, 7], f16), T([128, 256, 1, 1], f16)), {})\ncnt: 1, ((T([128, 256, 7, 7], f16), T([128, 256, 7, 7], f16)), {})\ncnt: 1, ((T([128, 128, 7, 7], f16), T([128, 128, 7, 7], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Determining Cosine Similarity using torch.ao.ns.fx.utils in Python\nDESCRIPTION: This code snippet illustrates the function signature for computing the cosine similarity between two tensors x and y using the torch.ao.ns.fx.utils module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite_fx.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ntorch.ao.ns.fx.utils.compute_cosine_similarity(x, y)\n```\n\n----------------------------------------\n\nTITLE: Applying ReLU Activation in PyTorch\nDESCRIPTION: This snippet demonstrates the usage of ReLU (Rectified Linear Unit) activation function on tensors with various shapes. It shows the count of operations for each unique tensor configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 2, ((T([32, 64, 112, 112], f16),), {})\ncnt: 6, ((T([32, 128, 56, 56], f16),), {})\ncnt: 1, ((T([32, 256, 56, 56], f16),), {})\ncnt: 5, ((T([32, 160, 28, 28], f16),), {})\ncnt: 1, ((T([32, 512, 28, 28], f16),), {})\ncnt: 10, ((T([32, 192, 14, 14], f16),), {})\ncnt: 2, ((T([32, 768, 14, 14], f16),), {})\ncnt: 10, ((T([32, 224, 7, 7], f16),), {})\ncnt: 2, ((T([32, 1024, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Handling macOS Cross-Compilation for Protobuf in CMake\nDESCRIPTION: Detects if cross-compiling for x86_64 or arm64 on macOS (but not iOS). If so, it sets CROSS_COMPILING_MACOSX to TRUE. It then builds a universal host 'protoc' executable using a temporary shell script to handle complex command arguments and potential quoting issues with CMake's execute_process. Finally, it sets PROTOBUF_PROTOC_EXECUTABLE and CAFFE2_CUSTOM_PROTOC_EXECUTABLE to use the newly built universal protoc.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\n# The below means we are cross compiling for arm64 or x86_64 on MacOSX\nif(NOT IOS\n   AND CMAKE_SYSTEM_NAME STREQUAL \"Darwin\"\n   AND CMAKE_OSX_ARCHITECTURES MATCHES \"^(x86_64|arm64)$\")\n  set(CROSS_COMPILING_MACOSX TRUE)\n  # We need to compile a universal protoc to not fail protobuf build We set\n  # CMAKE_TRY_COMPILE_TARGET_TYPE to STATIC_LIBRARY (vs executable) to succeed\n  # the cmake compiler check for cross-compiling\n  set(protoc_build_command\n      \"./scripts/build_host_protoc.sh --other-flags -DCMAKE_OSX_ARCHITECTURES=\\\"x86_64;arm64\\\" -DCMAKE_TRY_COMPILE_TARGET_TYPE=STATIC_LIBRARY -DCMAKE_C_COMPILER_WORKS=1 -DCMAKE_CXX_COMPILER_WORKS=1\"\n  )\n  # We write to a temp scriptfile because CMake COMMAND dislikes double quotes\n  # in commands\n  file(WRITE ${PROJECT_SOURCE_DIR}/tmp_protoc_script.sh\n       \"#!/bin/bash\\n${protoc_build_command}\")\n  file(\n    COPY ${PROJECT_SOURCE_DIR}/tmp_protoc_script.sh\n    DESTINATION ${PROJECT_SOURCE_DIR}/scripts/\n    FILE_PERMISSIONS OWNER_EXECUTE OWNER_WRITE OWNER_READ)\n  execute_process(\n    COMMAND ./scripts/tmp_protoc_script.sh\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}\n    RESULT_VARIABLE BUILD_HOST_PROTOC_RESULT)\n  file(REMOVE ${PROJECT_SOURCE_DIR}/tmp_protoc_script.sh\n       ${PROJECT_SOURCE_DIR}/scripts/tmp_protoc_script.sh)\n  if(NOT BUILD_HOST_PROTOC_RESULT EQUAL \"0\")\n    message(FATAL_ERROR \"Could not compile universal protoc.\")\n  endif()\n  set(PROTOBUF_PROTOC_EXECUTABLE\n      \"${PROJECT_SOURCE_DIR}/build_host_protoc/bin/protoc\")\n  set(CAFFE2_CUSTOM_PROTOC_EXECUTABLE\n      \"${PROJECT_SOURCE_DIR}/build_host_protoc/bin/protoc\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Promoting Conda Packages\nDESCRIPTION: Script to promote PyTorch conda packages. Requires access to the PyTorch conda channel and proper git tag checkout.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npromote/conda_to_conda.sh\n```\n\n----------------------------------------\n\nTITLE: Rerunning Categorization Filters in Python\nDESCRIPTION: Command to update commit categorization using new filter rules for uncategorized commits\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython commitlist.py --rerun_with_new_filters\n```\n\n----------------------------------------\n\nTITLE: ModuleList For Loop Example in TorchScript\nDESCRIPTION: Shows how for loops over nn.ModuleList unroll at compile time with each module member.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nclass SubModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(2))\n\n    def forward(self, input):\n        return self.weight + input\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mods = torch.nn.ModuleList([SubModule() for i in range(10)])\n\n    def forward(self, v):\n        for module in self.mods:\n            v = module(v)\n        return v\n\nmodel = torch.jit.script(MyModule())\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Softmax Operations in PyTorch\nDESCRIPTION: The snippet details the use of the ATen softmax operator applied on a tensor, illustrating dimensional and data type configurations. The softmax function is applied with specific parameters indicating dimension and boolean settings, reflecting its utilization across multiple instances.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 6, ((T([16, 12, 128, 128], f16), -1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Declaring an ATen Native Function in YAML\nDESCRIPTION: Defines the structure for registering an ATen native function in `native_functions.yaml`. It specifies the function signature (`func`), how it should be exposed (`variants` as 'function' or 'method'), and maps dispatch keys (e.g., 'CPU', 'CUDA') to the corresponding C++ implementation function names. Default arguments can be specified within the signature.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- func: func_name(ArgType arg0[=default], ArgType arg1[=default], ...) -> Return\n  variants: function, method\n  dispatch:\n    CPU: func_cpu\n    CUDA: func_cuda\n```\n\n----------------------------------------\n\nTITLE: Using ATen Softmax Operator in PyTorch\nDESCRIPTION: This operator applies the softmax function along the specified dimension of the input tensor. Dependencies include PyTorch with ATen support. The main parameters include the input tensor with its shape, the dimension for softmax computation, and a flag for data type switching. The output is a tensor with softmax values. The operation is constrained by the input tensor dimension requirements.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/ElectraForCausalLM_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 12, ((T([1, 4, 512, 512], f16), -1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing a Python Class with Method Docstring\nDESCRIPTION: This snippet defines a class named 'LintInitInit' with an __init__ method that includes a docstring and placeholder comments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass LintInitInit:\n    def __init__(self) -> None:\n        \"\"\"This is a very long comment, a very long comment\"\"\"\n\n        # Lots of lines!\n        # Lots of lines!\n        pass\n```\n\n----------------------------------------\n\nTITLE: Generating Tree AST for Python Expression in C++\nDESCRIPTION: Example of the Tree AST representation for the Python expression 'z.sigmoid() - (x + y)'. This shows how compound expressions are represented as nested Tree nodes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\n (-\n        (+\n          (variable (ident x))\n          (variable (ident y)))\n        (apply\n          (.\n                (variable (ident z))\n                (ident sigmoid))\n          (list)\n          (list))))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Tensor Shapes in PyTorch\nDESCRIPTION: This snippet shows the tensor shapes and parameters for batch normalization operations in a PyTorch model. It includes input tensors, running mean and variance, and other batch norm parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 96, 17, 17], f16), T([128, 96, 17, 17], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 0.001, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Installing Operator Headers and Declarations YAML in CMake\nDESCRIPTION: Installs the main operator header (`ops_h`) and any generated operator headers (`ops_generated_headers`) into the `${AT_INSTALL_INCLUDE_DIR}/ATen/ops` directory. It also installs the `Declarations.yaml` file (likely containing operator definitions) from the build directory into the ATen share directory (`${AT_INSTALL_SHARE_DIR}/ATen`).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_29\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(FILES ${ops_h} ${ops_generated_headers}\n  DESTINATION ${AT_INSTALL_INCLUDE_DIR}/ATen/ops)\ninstall(FILES ${CMAKE_BINARY_DIR}/aten/src/ATen/Declarations.yaml\n  DESTINATION ${AT_INSTALL_SHARE_DIR}/ATen)\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch C++ Extension Module\nDESCRIPTION: This snippet shows the import statement for the torch.utils.cpp_extension module, which provides utilities for working with C++ extensions in PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/cpp_extension.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch.utils.cpp_extension\n```\n\n----------------------------------------\n\nTITLE: Tensor Indexing Operations\nDESCRIPTION: Complex indexing operations on tensors with different shapes and strides, including both reading and writing operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 359613], f16), [T([4, 1], i64), T([4, 5000], i64)]), {})\n```\n\n----------------------------------------\n\nTITLE: Using PyTorch aten.native_batch_norm Operator in Python\nDESCRIPTION: This snippet demonstrates the use of 'aten.native_batch_norm.default', which performs batch normalization on input tensors. It is an essential normalization technique in training deep neural networks for stabilizing and accelerating convergence.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([8, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), False, 0.1, 1e-05), {})\ncnt: 6, ((T([8, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})\ncnt: 5, ((T([8, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), False, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Fully Connected Test for QNNPACK in CMake\nDESCRIPTION: Creates and configures the fully connected network test executable with C++14 standard requirements, includes the necessary directories, and links against required libraries like pytorch_qnnpack, clog, cpuinfo, fp16, and gtest.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(fully-connected-test test/fully-connected.cc)\nset_target_properties(fully-connected-test PROPERTIES\n  CXX_STANDARD 14\n  CXX_STANDARD_REQUIRED YES\n  CXX_EXTENSIONS NO)\ntarget_include_directories(fully-connected-test PRIVATE src test)\ntarget_link_libraries(fully-connected-test PRIVATE pytorch_qnnpack clog cpuinfo fp16 gtest gtest_main)\nadd_test(fully-connected-test fully-connected-test)\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Embedding Bag Operation Parameters\nDESCRIPTION: Records of aten._embedding_bag.default operator calls with input tensors of shape [965, 192] in float16, various index tensors, offset tensor of shape [1024], and associated parameters. Each entry shows the count of occurrences for that specific parameter combination.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._embedding_bag.default\ncnt: 2, ((T([965, 192], f16), T([54824], i64), T([1024], i64), False, 0, True, T([54824], f16)), {})\ncnt: 2, ((T([965, 192], f16), T([54798], i64), T([1024], i64), False, 0, True, T([54798], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch ReLU In-place Operations\nDESCRIPTION: Log entries for in-place ReLU activation operations on tensors of various shapes. Shows the operation being applied to feature maps with different channel depths and spatial dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet50_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([32, 64, 112, 112], f16),), {})\ncnt: 6, ((T([32, 64, 56, 56], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Appending Vulkan Libraries to Link List in CMake\nDESCRIPTION: Checks if the `USE_VULKAN` flag is enabled. If it is, the contents of the `Vulkan_LIBS` variable (populated by `VulkanDependencies.cmake`) are appended to the `pytorch_jni_LIBS` list, ensuring the JNI target links against the required Vulkan libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_VULKAN)\n  list(APPEND pytorch_jni_LIBS ${Vulkan_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Logging Aten Operator: aten.select_backward.default (Text)\nDESCRIPTION: Log entry for an invocation of the 'aten.select_backward.default' operator. Arguments include an input tensor (T) with shape and data type (f16), followed by dimensions and indices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/pit_b_224_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.select_backward.default\ncnt: 1, ((T([64, 1024], f16), [64, 1, 1024], 1, 0), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake with PyTorch from conda or pip\nDESCRIPTION: Command to configure CMake using the PyTorch installation path from conda or pip. This allows seamless integration with existing PyTorch installations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\ncmake -DCMAKE_PREFIX_PATH=`python3 -c 'import torch;print(torch.utils.cmake_prefix_path)'` ..\n```\n\n----------------------------------------\n\nTITLE: Configuring cuDNN Support for PyTorch Python\nDESCRIPTION: Adds cuDNN support to PyTorch Python bindings when either CUDA or ROCm is enabled with cuDNN.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDNN OR USE_ROCM)\n    list(APPEND TORCH_PYTHON_SRCS\n      ${TORCH_SRC_DIR}/csrc/cuda/shared/cudnn.cpp\n      )\n    if(USE_STATIC_CUDNN)\n        set_source_files_properties(\n          ${TORCH_SRC_DIR}/csrc/cuda/shared/cudnn.cpp\n          PROPERTIES COMPILE_FLAGS \"-DUSE_STATIC_CUDNN\"\n        )\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Analyzing Sum Operations in PyTorch\nDESCRIPTION: This snippet shows sum operations performed on tensors with different shapes and dimensions, including reduction along specific axes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 1, ((T([128, 1152, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Installation Commands for torchdim\nDESCRIPTION: Shell commands for setting up conda environment and installing PyTorch nightly build along with torchdim package.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda create --name dim\nconda activate dim\n\n# For CUDA 10.2\nconda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch-nightly\n# For CUDA 11.3\nconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly\n# For CPU-only build\nconda install pytorch torchvision torchaudio cpuonly -c pytorch-nightly\n\npip install ninja  # Makes the build go faster\npip install --user \"git+https://github.com/facebookresearch/torchdim\"\n```\n\n----------------------------------------\n\nTITLE: Restoring Backups from S3\nDESCRIPTION: Script to restore release candidates from S3 backups to test channels. Uses RESTORE_FROM environment variable to specify the tag to restore from.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nRESTORE_FROM=v1.5.0-rc5 ./restore-backup.sh\n```\n\n----------------------------------------\n\nTITLE: Installing iOS Build Dependencies\nDESCRIPTION: Installs required dependencies for building PyTorch on iOS using Homebrew package manager.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nbrew install cmake automake libtool\n```\n\n----------------------------------------\n\nTITLE: Generated Triton Kernel with Injected Error\nDESCRIPTION: The Triton kernel code generated with an intentionally injected error in the ReLU operation, demonstrating the syntax error that will be caught by the minifier.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor_minifier.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@triton.jit\ndef triton_poi_fused_addmm_relu_sigmoid_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 16\n    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = compile error!\n    tmp4 = tl.sigmoid(tmp3)\n    tl.store(in_out_ptr0 + (x2), tmp4, xmask)\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module Context in Sphinx Documentation\nDESCRIPTION: Sets the current module context for the documentation using a template variable that will be replaced during rendering.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/_templates/autosummary/class.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: {{ module }}\n```\n\n----------------------------------------\n\nTITLE: PyTorch Mean Operation Along Dimensions\nDESCRIPTION: This snippet shows a mean operation that reduces a tensor of shape [128, 1984, 7, 7] with f16 precision along its last two dimensions (-1, -2), keeping the dimensions in the result (keepdim=True).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 1984, 7, 7], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Debugging Environment Variables Table in RST\nDESCRIPTION: This RST code snippet creates a table listing debugging environment variables for PyTorch, including their names and descriptions. It covers variables for C++ stack traces, log levels, and logging configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/debugging_environment_variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table::\n  :header-rows: 1\n\n  * - Variable\n    - Description\n  * - ``TORCH_SHOW_CPP_STACKTRACES``\n    - If set to ``1``, makes PyTorch print out a stack trace when it detects a C++ error.\n  * - ``TORCH_CPP_LOG_LEVEL``\n    - Set the log level of c10 logging facility (supports both GLOG and c10 loggers). Valid values are ``INFO``, ``WARNING``, ``ERROR``, and ``FATAL`` or their numerical equivalents ``0``, ``1``, ``2``, and ``3``.\n  * - ``TORCH_LOGS``\n    -  For a more in depth explanation of this environment variable, see :doc:`/logging`.\n```\n\n----------------------------------------\n\nTITLE: Gathering CPU Source Files by Architecture in CMake\nDESCRIPTION: Collects source files and separates them by CPU architecture extensions (common, AVX, AVX2, AVX512, SVE). Uses glob patterns to find files and then excludes architecture-specific files from the common sources list.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/perfkernels/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ CPU files.\nfile(GLOB common_srcs *.cc)\nfile(GLOB avx_srcs *_avx.cc)\nfile(GLOB avx2_srcs *_avx2.cc)\nfile(GLOB avx512_srcs *_avx512.cc)\nfile(GLOB sve_srcs *_sve.cc)\n# exclude avx, avx2, avx512, and sve srcs from common_srcs\nexclude(common_srcs \"${common_srcs}\" ${avx_srcs})\nexclude(common_srcs \"${common_srcs}\" ${avx2_srcs})\nexclude(common_srcs \"${common_srcs}\" ${avx512_srcs})\nexclude(common_srcs \"${common_srcs}\" ${sve_srcs})\n\n# We will always build common srcs.\nset(Caffe2_CPU_SRCS ${Caffe2_CPU_SRCS} ${common_srcs})\n```\n\n----------------------------------------\n\nTITLE: Implementing Tensor Operations in PyTorch Autograd\nDESCRIPTION: This C++ code implements various tensor operations like bmm (batch matrix multiplication), baddbmm (batch add and matrix multiplication), and select for PyTorch's autograd system. It uses C++ templates and function pointers for flexible dispatching.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_32\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autograd12VariableType12_GLOBAL__N_13bmmEN3c1014DispatchKeySetERKN2at6TensorES8_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autograd12VariableType12_GLOBAL__N_17baddbmmEN3c1014DispatchKeySetERKN2at6TensorES8_S8_RKNS3_6ScalarESB_\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autograd12VariableType12_GLOBAL__N_110select_intEN3c1014DispatchKeySetERKN2at6TensorElNS3_6SymIntE\n```\n\n----------------------------------------\n\nTITLE: Unsupported Tensor Indexing for Writes in PyTorch ONNX Export\nDESCRIPTION: Examples of unsupported tensor indexing patterns for writing operations, including multiple tensor indices, non-consecutive indices, negative values, and implicit broadcasting.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data\ndata[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data\ndata[torch.tensor([[0, 2], [1, 1]]), 1:3] = new_data\n```\n\n----------------------------------------\n\nTITLE: Configuring C10 CUDA Build Settings\nDESCRIPTION: Sets up basic configuration for the C10 CUDA build, including macros and library references. It configures the CMake macros header file and sets up the C10_CUDA_LIB variable based on whether building with LIBTORCHLESS.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/cuda/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(../../cmake/public/utils.cmake)\ninclude(../../cmake/public/cuda.cmake)\n\n# ---[ Configure macro file.\nset(C10_CUDA_BUILD_SHARED_LIBS ${BUILD_SHARED_LIBS}) # used in cmake_macros.h.in\n# Probably have to do this :(\nconfigure_file(\n    ${CMAKE_CURRENT_LIST_DIR}/impl/cuda_cmake_macros.h.in\n    ${CMAKE_BINARY_DIR}/c10/cuda/impl/cuda_cmake_macros.h)\n\nif(BUILD_LIBTORCHLESS)\n  find_library(C10_CUDA_LIB c10_cuda PATHS $ENV{LIBTORCH_LIB_PATH} NO_DEFAULT_PATH)\nelse()\n  set(C10_CUDA_LIB c10_cuda)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating TorchScript Model Data\nDESCRIPTION: Creates custom commands to generate TorchScript model data for both CPU and CUDA versions using Python scripts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/aoti_inference/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(\n    OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/script_data.pt\n           ${CMAKE_CURRENT_BINARY_DIR}/script_model_cpu.pt\n           ${CMAKE_CURRENT_BINARY_DIR}/script_model_cuda.pt\n    COMMAND python ${AOT_INDUCTOR_TEST_ROOT}/compile_model.py\n    DEPENDS compile_model.py\n)\nadd_custom_target(aoti_script_model ALL\n    DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/script_data.pt\n    DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/script_model_cpu.pt\n    DEPENDS ${CMAKE_CURRENT_BINARY_DIR}/script_model_cuda.pt\n)\nadd_dependencies(aoti_script_model aoti_custom_class)\n```\n\n----------------------------------------\n\nTITLE: Maintainer Requirements List in Markdown\nDESCRIPTION: Lists the qualification requirements for becoming a PyTorch maintainer, including contribution history and recency criteria.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/community/build_ci_governance.rst#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n* Land at least six commits to the related part of the PyTorch repository\n* At least one of these commits must be submitted in the last six months\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Operations with Shape and Stride Information in PyTorch\nDESCRIPTION: Collection of tensor operation statistics showing tensor shapes, data types, strides, and operation counts. The data appears to be from a profiling or debugging output that tracks how many times specific tensor operations are performed during model execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/coat_lite_mini_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((T([128, 8, 3136, 8], f16), [128, 8, 3137, 8], 2, 1, 9223372036854775807, 1), {})\ncnt: 4, ((T([128, 8, 3137, 8], f16), [128, 8, 3137, 8], 1, 0, 9223372036854775807, 1), {})\ncnt: 4, ((T([128, 8, 3137, 8], f16), [128, 8, 3137, 8], 0, 0, 9223372036854775807, 1), {})\ncnt: 2, ((T([128, 1, 64], f16, stride=(200768, 64, 1)), [128, 3137, 64], 1, 0, 1, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Data Collection Command for AutoHeuristic\nDESCRIPTION: Shell command to collect training data for AutoHeuristic. Specifies log path and collection target using environment variables.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_AUTOHEURISTIC_LOG_PATH=\"train.txt\" \\\n  TORCHINDUCTOR_AUTOHEURISTIC_COLLECT=\"pad_mm\" python run.py\n```\n\n----------------------------------------\n\nTITLE: PyTorch GELU Activation Function\nDESCRIPTION: These snippets show the application of the GELU (Gaussian Error Linear Unit) activation function to tensors of various shapes, using float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 16, 96, 96], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 32, 96, 96], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 64, 96, 96], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 128, 48, 48], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 256, 48, 48], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 256, 24, 24], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 512, 24, 24], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 768, 24, 24], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 768, 12, 12], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1536, 12, 12], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 768, 6, 6], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1536, 6, 6], f16),), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 3072, 6, 6], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Max Pooling with Indices in PyTorch\nDESCRIPTION: Applies 2D max pooling over input tensors, returning both the result and the indices of the max values.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\naten.max_pool2d_with_indices.default((T([64, 64, 224, 224], f16), [2, 2], [2, 2]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.max_pool2d_with_indices.default((T([64, 128, 112, 112], f16), [2, 2], [2, 2]), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Support and Dependencies in ATen CMake\nDESCRIPTION: Sets up CPU-related include directories, libraries, and checks for various system functions. It also configures third-party libraries like BLAS, LAPACK, NNPACK, and SLEEF.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND ATen_CPU_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/..)\n\nif(BLAS_FOUND)\n  list(APPEND ATen_CPU_DEPENDENCY_LIBS ${BLAS_LIBRARIES})\nendif(BLAS_FOUND)\n\nif(LAPACK_FOUND)\n  list(APPEND ATen_CPU_DEPENDENCY_LIBS ${LAPACK_LIBRARIES})\n  if(USE_CUDA AND MSVC)\n    # Although Lapack provides CPU (and thus, one might expect that ATen_cuda\n    # would not need this at all), some of our libraries (magma in particular)\n    # backend to CPU BLAS/LAPACK implementations, and so it is very important\n    # we get the *right* implementation, because even if the symbols are the\n    # same, LAPACK implementions may have different calling conventions.\n    # This caused https://github.com/pytorch/pytorch/issues/7353\n    #\n    # We do NOT do this on Linux, since we just rely on torch_cpu to\n    # provide all of the symbols we need\n    list(APPEND ATen_CUDA_DEPENDENCY_LIBS ${LAPACK_LIBRARIES})\n  endif()\nendif(LAPACK_FOUND)\n\nif(UNIX AND NOT APPLE)\n   include(CheckLibraryExists)\n   # https://github.com/libgit2/libgit2/issues/2128#issuecomment-35649830\n   CHECK_LIBRARY_EXISTS(rt clock_gettime \"time.h\" NEED_LIBRT)\n   if(NEED_LIBRT)\n     list(APPEND ATen_CPU_DEPENDENCY_LIBS rt)\n     set(CMAKE_REQUIRED_LIBRARIES ${CMAKE_REQUIRED_LIBRARIES} rt)\n   endif(NEED_LIBRT)\nendif(UNIX AND NOT APPLE)\n\nif(UNIX)\n  include(CheckFunctionExists)\n  set(CMAKE_EXTRA_INCLUDE_FILES \"sys/mman.h\")\n  CHECK_FUNCTION_EXISTS(mmap HAVE_MMAP)\n  if(HAVE_MMAP)\n    add_definitions(-DHAVE_MMAP=1)\n  endif(HAVE_MMAP)\n  # done for lseek: https://www.gnu.org/software/libc/manual/html_node/File-Position-Primitive.html\n  add_definitions(-D_FILE_OFFSET_BITS=64)\n  CHECK_FUNCTION_EXISTS(shm_open HAVE_SHM_OPEN)\n  if(HAVE_SHM_OPEN)\n    add_definitions(-DHAVE_SHM_OPEN=1)\n  endif(HAVE_SHM_OPEN)\n  CHECK_FUNCTION_EXISTS(shm_unlink HAVE_SHM_UNLINK)\n  if(HAVE_SHM_UNLINK)\n    add_definitions(-DHAVE_SHM_UNLINK=1)\n  endif(HAVE_SHM_UNLINK)\n  CHECK_FUNCTION_EXISTS(malloc_usable_size HAVE_MALLOC_USABLE_SIZE)\n  if(HAVE_MALLOC_USABLE_SIZE)\n    add_definitions(-DHAVE_MALLOC_USABLE_SIZE=1)\n  endif(HAVE_MALLOC_USABLE_SIZE)\nendif(UNIX)\n\nADD_DEFINITIONS(-DUSE_EXTERNAL_MZCRC)\n\nif(NOT MSVC)\n  list(APPEND ATen_CPU_DEPENDENCY_LIBS m)\nendif()\n\nif(AT_NNPACK_ENABLED)\n  include_directories(${NNPACK_INCLUDE_DIRS})\n  list(APPEND ATen_CPU_DEPENDENCY_LIBS nnpack) # cpuinfo is added below\nendif()\n\nif(MKLDNN_FOUND)\n  list(APPEND ATen_CPU_DEPENDENCY_LIBS ${MKLDNN_LIBRARIES})\nendif(MKLDNN_FOUND)\n\nif(USE_MKLDNN_ACL)\n    list(APPEND ATen_CPU_INCLUDE ${ACL_INCLUDE_DIRS})\n    list(APPEND ATen_CPU_DEPENDENCY_LIBS ${ACL_LIBRARIES})\nendif()\n\nif(NOT CMAKE_SYSTEM_PROCESSOR MATCHES \"^(s390x|ppc64le)$\")\n  list(APPEND ATen_CPU_DEPENDENCY_LIBS cpuinfo)\nendif()\n\nif(NOT EMSCRIPTEN AND NOT INTERN_BUILD_MOBILE)\n  if(NOT MSVC)\n    # Bump up optimization level for sleef to -O1, since at -O0 the compiler\n    # excessively spills intermediate vector registers to the stack\n    # and makes things run impossibly slowly\n    set(OLD_CMAKE_C_FLAGS_DEBUG ${CMAKE_C_FLAGS_DEBUG})\n    if(${CMAKE_C_FLAGS_DEBUG} MATCHES \"-O0\")\n      string(REGEX REPLACE \"-O0\" \"-O1\" CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})\n    else()\n      set(CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG} -O1\")\n    endif()\n  elseif(CMAKE_SYSTEM_PROCESSOR STREQUAL \"ARM64\")\n    set(SLEEF_ARCH_AARCH64 ON)\n  endif()\n\n  if(NOT USE_SYSTEM_SLEEF)\n    set(SLEEF_BUILD_SHARED_LIBS OFF CACHE BOOL \"Build sleef static\" FORCE)\n    set(SLEEF_BUILD_DFT OFF CACHE BOOL \"Don't build sleef DFT lib\" FORCE)\n    set(SLEEF_BUILD_GNUABI_LIBS OFF CACHE BOOL \"Don't build sleef gnuabi libs\" FORCE)\n    set(SLEEF_BUILD_TESTS OFF CACHE BOOL \"Don't build sleef tests\" FORCE)\n    set(SLEEF_BUILD_SCALAR_LIB OFF CACHE BOOL \"libsleefscalar will be built.\" FORCE)\n    if(WIN32)\n      set(SLEEF_BUILD_WITH_LIBM OFF CACHE BOOL \"Don't build sleef with libm for Windows.\" FORCE)\n    endif()\n    if(CMAKE_SYSTEM_NAME STREQUAL \"Darwin\")\n      if(CMAKE_SYSTEM_PROCESSOR STREQUAL \"arm64\" OR CMAKE_OSX_ARCHITECTURES MATCHES \"arm64\")\n        set(DISABLE_SVE ON CACHE BOOL \"Xcode's clang-12.5 crashes while trying to compile SVE code\" FORCE)\n      endif()\n    endif()\n    add_subdirectory(\"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\" ${CMAKE_BINARY_DIR}/sleef)\n    set_property(TARGET sleef PROPERTY FOLDER \"dependencies\")\n    list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)\n    link_directories(${CMAKE_BINARY_DIR}/sleef/lib)\n  else()\n    add_library(sleef SHARED IMPORTED)\n    find_library(SLEEF_LIBRARY sleef)\n    if(NOT SLEEF_LIBRARY)\n      message(FATAL_ERROR \"Cannot find sleef\")\n    endif()\n    message(\"Found sleef: ${SLEEF_LIBRARY}\")\n    set_target_properties(sleef PROPERTIES IMPORTED_LOCATION \"${SLEEF_LIBRARY}\")\n  endif()\n  list(APPEND ATen_CPU_DEPENDENCY_LIBS sleef)\n\n  if(NOT MSVC)\n    set(CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Promoting PyTorch Wheels Using S3\nDESCRIPTION: Script to promote PyTorch wheel packages from staging to production S3 buckets. Requires AWS access to PyTorch account and proper git tag checkout.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npromote/s3_to_s3.sh\n```\n\n----------------------------------------\n\nTITLE: Profiling Mean Reduction Operations in PyTorch\nDESCRIPTION: Log of mean operations that reduce tensors along specified dimensions. These are typically used in pooling layers or for calculating statistics in batch normalization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 2, ((T([64, 64, 64, 64], f16), [2, 3], True), {})\ncnt: 2, ((T([64, 128, 32, 32], f16), [2, 3], True), {})\ncnt: 2, ((T([64, 256, 16, 16], f16), [2, 3], True), {})\ncnt: 1, ((T([64, 1280, 8, 8], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.convolution_backward.default Calls in PyTorch Text Trace\nDESCRIPTION: Records argument patterns for the backward pass of convolution operations, referencing input and weight tensor shapes, bias gradients, and various stride, padding, and group values. This data supports gradient computation performance analysis or custom backward pass implementations. All traces follow PyTorch's argument ordering and are relevant for low-level kernel optimization or model introspection.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([32, 1280, 7, 7], f16), T([32, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([32, 320, 7, 7], f16), T([32, 1152, 7, 7], f16), T([320, 1152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([32, 1152, 1, 1], f16), T([32, 48, 1, 1], f16), T([1152, 48, 1, 1], f16), [1152], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 4, ((T([32, 48, 1, 1], f16), T([32, 1152, 1, 1], f16), T([48, 1152, 1, 1], f16), [48], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16), T([1152, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1152, [True, True, False]), {})\ncnt: 4, ((T([32, 1152, 7, 7], f16), T([32, 192, 7, 7], f16), T([1152, 192, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([32, 192, 7, 7], f16), T([32, 1152, 7, 7], f16), T([192, 1152, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16), T([1152, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 1152, [True, True, False]), {})\ncnt: 1, ((T([32, 192, 7, 7], f16), T([32, 672, 7, 7], f16), T([192, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([32, 672, 1, 1], f16), T([32, 28, 1, 1], f16), T([672, 28, 1, 1], f16), [672], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 3, ((T([32, 28, 1, 1], f16), T([32, 672, 1, 1], f16), T([28, 672, 1, 1], f16), [28], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})\ncnt: 3, ((T([32, 672, 14, 14], f16), T([32, 112, 14, 14], f16), T([672, 112, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([32, 112, 14, 14], f16), T([32, 672, 14, 14], f16), T([112, 672, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16), T([672, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 672, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Output Directories in CMake\nDESCRIPTION: Specifies the output directories for build artifacts within the CMake binary directory (CMAKE_BINARY_DIR). It sets separate locations for archive files (static libraries), library files (shared libraries), and runtime executables.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_21\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ CMake build directories\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies\nDESCRIPTION: Command to install required Python packages for building the documentation using pip package manager.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining a Short Function with Comments\nDESCRIPTION: A simple function with no implementation but containing comments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef short():\n    #\n    #\n    #\n    pass\n```\n\n----------------------------------------\n\nTITLE: Importing torch.ao.ns._numeric_suite_fx module in Python\nDESCRIPTION: This snippet demonstrates how to import the torch.ao.ns._numeric_suite_fx module. The module is part of PyTorch's numeric suite and is marked as an early prototype.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite_fx.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport torch.ao.ns._numeric_suite_fx\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: A list of required Python packages needed for the PyTorch project. Includes PyGithub for GitHub API interactions and tqdm for displaying progress bars.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nPyGithub\ntqdm\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Copy Operations in PyTorch\nDESCRIPTION: This snippet shows a tensor copy operation, which is used to create a new tensor with the same data as an existing tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Default BLAS Usage Option for PyTorch\nDESCRIPTION: Sets the default value for the USE_BLAS option to ON if it is not already defined.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\n# INTERN_BUILD_ATEN_OPS is used to control whether to build ATen/TH operators.\nset(INTERN_BUILD_ATEN_OPS ON)\n\nif(NOT DEFINED USE_BLAS)\n  set(USE_BLAS ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Core CMake Configuration and Version Settings\nDESCRIPTION: Initializes CMake with minimum version requirement and sets key policies for compilation behavior. Includes compiler policies for Apple Clang and LTO settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\ncmake_policy(SET CMP0010 NEW)\ncmake_policy(SET CMP0025 NEW)\ncmake_policy(SET CMP0069 NEW)\ncmake_policy(SET CMP0092 NEW)\n```\n\n----------------------------------------\n\nTITLE: Building libtorch Using CMake\nDESCRIPTION: This snippet provides a complete example of building libtorch using CMake. It clones the PyTorch repository, creates a build directory, configures the build with CMake, and installs the library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/libtorch.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone -b main --recurse-submodule https://github.com/pytorch/pytorch.git\nmkdir pytorch-build\ncd pytorch-build\ncmake -DBUILD_SHARED_LIBS:BOOL=ON -DCMAKE_BUILD_TYPE:STRING=Release -DPYTHON_EXECUTABLE:PATH=`which python3` -DCMAKE_INSTALL_PREFIX:PATH=../pytorch-install ../pytorch\ncmake --build . --target install\n```\n\n----------------------------------------\n\nTITLE: Initializing Simple PyTorch Model Function\nDESCRIPTION: Defines a basic PyTorch function that performs element-wise operations on four input tensors\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/aot_autograd_optimizations.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndef fn(a, b, c, d):\n    x = a + b + c + d\n    return x.cos().cos()\n```\n\n----------------------------------------\n\nTITLE: Recording Example Input Tensors for aten.sum.default Operator - PyTorch - Python\nDESCRIPTION: This snippet lists example inputs (tensor shapes and data types) for the aten.sum.default operator, typical in summation or reduction operations in PyTorch. Inputs are specified in tuples, showing explicit batch, channel, spatial dimensions, and the f16 data type, suitable for float16 precision computations. There are no additional parameters for these operator calls; they exemplify expected inputs for testing or documentation purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_unet_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([1, 256, 80, 119], f16),), {})\ncnt: 1, ((T([1, 128, 160, 239], f16),), {})\ncnt: 1, ((T([1, 64, 320, 479], f16),), {})\nOperator: aten.sum.default\ncnt: 1, ((T([1, 2, 640, 959], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling ATen Operator Invocation Arguments - PyTorch - Python\nDESCRIPTION: This data structure captures the invocation count and detailed parameterization of each ATen operator call, including tensor shapes, data types, additional arguments, and options or attributes. Each record maps a profiling sample (e.g., 'cnt: 4, ((T([1, 112, 40, 40], f16), ...), {})') for a particular operator configuration, suitable for further analysis or generating test benches. Dependencies include PyTorch's ATen operator set and tensor representations; expected inputs are numerical arguments and tensor metadata, and no outputs are computed in this static format. No computation is performed; this serves data logging and is constrained to static schema and PyTorch types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((T([1, 112, 40, 40], f16), T([1, 112, 40, 40], f16), T([112], f16), T([112], f16), T([112], f16), T([112], f32), T([112], f32), False, 0.001, [True, True, True]), {})\\ncnt: 8, ((T([1, 480, 40, 40], f16), T([1, 480, 40, 40], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), False, 0.001, [True, True, True]), {})\\ncnt: 4, ((T([1, 80, 40, 40], f16), T([1, 80, 40, 40], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), False, 0.001, [True, True, True]), {})\\ncnt: 1, ((T([1, 240, 40, 40], f16), T([1, 240, 40, 40], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 0.001, [True, True, True]), {})\\ncnt: 5, ((T([1, 240, 80, 80], f16), T([1, 240, 80, 80], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), False, 0.001, [True, True, True]), {})\\ncnt: 3, ((T([1, 40, 80, 80], f16), T([1, 40, 80, 80], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), False, 0.001, [True, True, True]), {})\\ncnt: 1, ((T([1, 144, 80, 80], f16), T([1, 144, 80, 80], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), False, 0.001, [True, True, True]), {})\\ncnt: 5, ((T([1, 144, 160, 160], f16), T([1, 144, 160, 160], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f32), T([144], f32), False, 0.001, [True, True, True]), {})\\ncnt: 3, ((T([1, 24, 160, 160], f16), T([1, 24, 160, 160], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), False, 0.001, [True, True, True]), {})\\ncnt: 1, ((T([1, 96, 160, 160], f16), T([1, 96, 160, 160], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 0.001, [True, True, True]), {})\\ncnt: 1, ((T([1, 96, 320, 320], f16), T([1, 96, 320, 320], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), False, 0.001, [True, True, True]), {})\\ncnt: 3, ((T([1, 16, 320, 320], f16), T([1, 16, 320, 320], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), False, 0.001, [True, True, True]), {})\\ncnt: 2, ((T([1, 32, 320, 320], f16), T([1, 32, 320, 320], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), False, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.neg.default\\ncnt: 2, ((T([5000], f32, stride=(4,)),), {})\\ncnt: 8, ((T([1, 88, 5, 5], f16),), {})\\ncnt: 20, ((T([1, 88, 10, 10], f16),), {})\\ncnt: 20, ((T([1, 88, 20, 20], f16),), {})\\ncnt: 20, ((T([1, 88, 40, 40], f16),), {})\\ncnt: 8, ((T([1, 88, 80, 80], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.new_zeros.default\\ncnt: 1, ((T([100, 1], f32, stride=(0, 0)), [5000, 1]), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\\ncnt: 1, ((T([100, 4], f32), [5000, 4]), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\\ncnt: 1, ((T([1, 5000, 1], f16), [1, 5000, 90]), {})\\ncnt: 1, ((T([1, 5000, 90], f16), [1, 76725, 90]), {})\\ncnt: 1, ((T([1, 5000, 4], f16), [1, 76725, 4]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu.default\\ncnt: 20, ((T([2], f16),), {})\\ncnt: 12, ((T([3], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.remainder.Scalar\\ncnt: 1, ((T([1, 5000], i64), 90), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.scatter_add_.default\\ncnt: 1, ((T([1, 5000, 90], f16), 2, T([1, 5000, 1], i64), T([1, 5000, 1], f16)), {})\\ncnt: 1, ((T([1, 76725, 90], f16), 1, T([1, 5000, 90], i64, stride=(5000, 1, 0)), T([1, 5000, 90], f16)), {})\\ncnt: 1, ((T([1, 76725, 4], f16), 1, T([1, 5000, 4], i64, stride=(5000, 1, 0)), T([1, 5000, 4], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.select_backward.default\\ncnt: 1, ((T([5000, 4], f16), [1, 5000, 4], 0, 0), {})\\ncnt: 1, ((T([5000, 1], f16), [1, 5000, 1], 0, 0), {})\\ncnt: 20, ((T([], f16), [2], 0, 1), {})\\ncnt: 20, ((T([], f16), [2], 0, 0), {})\\ncnt: 12, ((T([], f16), [3], 0, 2), {})\\ncnt: 12, ((T([], f16), [3], 0, 1), {})\\ncnt: 12, ((T([], f16), [3], 0, 0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sigmoid.default\\ncnt: 1, ((T([1, 32, 1, 1], f16),), {})\\ncnt: 1, ((T([1, 16, 1, 1], f16),), {})\\ncnt: 1, ((T([1, 96, 1, 1], f16),), {})\\ncnt: 3, ((T([1, 144, 1, 1], f16),), {})\\ncnt: 3, ((T([1, 240, 1, 1], f16),), {})\\ncnt: 4, ((T([1, 480, 1, 1], f16),), {})\\ncnt: 4, ((T([1, 672, 1, 1], f16),), {})\\ncnt: 5, ((T([1, 1152, 1, 1], f16),), {})\\ncnt: 1, ((T([1, 1920, 1, 1], f16),), {})\\ncnt: 1, ((T([5000, 1], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sigmoid_backward.default\\ncnt: 1, ((T([5000, 1], f16), T([5000, 1], f16)), {})\\ncnt: 1, ((T([1, 1920, 1, 1], f16), T([1, 1920, 1, 1], f16)), {})\\ncnt: 5, ((T([1, 1152, 1, 1], f16), T([1, 1152, 1, 1], f16)), {})\\ncnt: 4, ((T([1, 672, 1, 1], f16), T([1, 672, 1, 1], f16)), {})\\ncnt: 4, ((T([1, 480, 1, 1], f16), T([1, 480, 1, 1], f16)), {})\\ncnt: 3, ((T([1, 240, 1, 1], f16), T([1, 240, 1, 1], f16)), {})\\ncnt: 3, ((T([1, 144, 1, 1], f16), T([1, 144, 1, 1], f16)), {})\\ncnt: 1, ((T([1, 96, 1, 1], f16), T([1, 96, 1, 1], f16)), {})\\ncnt: 1, ((T([1, 16, 1, 1], f16), T([1, 16, 1, 1], f16)), {})\\ncnt: 1, ((T([1, 32, 1, 1], f16), T([1, 32, 1, 1], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.silu_.default\\ncnt: 2, ((T([1, 32, 320, 320], f16),), {})\\ncnt: 1, ((T([1, 8, 1, 1], f16),), {})\\ncnt: 1, ((T([1, 16, 320, 320], f16),), {})\\ncnt: 2, ((T([1, 4, 1, 1], f16),), {})\\ncnt: 1, ((T([1, 96, 320, 320], f16),), {})\\ncnt: 1, ((T([1, 96, 160, 160], f16),), {})\\ncnt: 5, ((T([1, 144, 160, 160], f16),), {})\\ncnt: 3, ((T([1, 6, 1, 1], f16),), {})\\ncnt: 1, ((T([1, 144, 80, 80], f16),), {})\\ncnt: 5, ((T([1, 240, 80, 80], f16),), {})\\ncnt: 3, ((T([1, 10, 1, 1], f16),), {})\\ncnt: 1, ((T([1, 240, 40, 40], f16),), {})\\ncnt: 8, ((T([1, 480, 40, 40], f16),), {})\\ncnt: 4, ((T([1, 20, 1, 1], f16),), {})\\ncnt: 7, ((T([1, 672, 40, 40], f16),), {})\\ncnt: 4, ((T([1, 28, 1, 1], f16),), {})\\ncnt: 1, ((T([1, 672, 20, 20], f16),), {})\\ncnt: 10, ((T([1, 1152, 20, 20], f16),), {})\\ncnt: 5, ((T([1, 48, 1, 1], f16),), {})\\ncnt: 2, ((T([1, 1920, 20, 20], f16),), {})\\ncnt: 1, ((T([1, 80, 1, 1], f16),), {})\\ncnt: 14, ((T([1, 88, 10, 10], f16),), {})\\ncnt: 14, ((T([1, 88, 20, 20], f16),), {})\\ncnt: 14, ((T([1, 88, 40, 40], f16),), {})\\ncnt: 10, ((T([1, 88, 80, 80], f16),), {})\\ncnt: 10, ((T([1, 88, 5, 5], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.silu_backward.default\\ncnt: 10, ((T([1, 88, 5, 5], f16), T([1, 88, 5, 5], f16)), {})\\ncnt: 14, ((T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16)), {})\\ncnt: 14, ((T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16)), {})\\ncnt: 14, ((T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16)), {})\\ncnt: 10, ((T([1, 88, 80, 80], f16), T([1, 88, 80, 80], f16)), {})\\ncnt: 1, ((T([1, 80, 1, 1], f16), T([1, 80, 1, 1], f16)), {})\\ncnt: 2, ((T([1, 1920, 20, 20], f16), T([1, 1920, 20, 20], f16)), {})\\ncnt: 5, ((T([1, 48, 1, 1], f16), T([1, 48, 1, 1], f16)), {})\\ncnt: 10, ((T([1, 1152, 20, 20], f16), T([1, 1152, 20, 20], f16)), {})\\ncnt: 4, ((T([1, 28, 1, 1], f16), T([1, 28, 1, 1], f16)), {})\\ncnt: 1, ((T([1, 672, 20, 20], f16), T([1, 672, 20, 20], f16)), {})\\ncnt: 7, ((T([1, 672, 40, 40], f16), T([1, 672, 40, 40], f16)), {})\\ncnt: 4, ((T([1, 20, 1, 1], f16), T([1, 20, 1, 1], f16)), {})\\ncnt: 8, ((T([1, 480, 40, 40], f16), T([1, 480, 40, 40], f16)), {})\\ncnt: 3, ((T([1, 10, 1, 1], f16), T([1, 10, 1, 1], f16)), {})\\ncnt: 1, ((T([1, 240, 40, 40], f16), T([1, 240, 40, 40], f16)), {})\\ncnt: 5, ((T([1, 240, 80, 80], f16), T([1, 240, 80, 80], f16)), {})\\ncnt: 3, ((T([1, 6, 1, 1], f16), T([1, 6, 1, 1], f16)), {})\\ncnt: 1, ((T([1, 144, 80, 80], f16), T([1, 144, 80, 80], f16)), {})\\ncnt: 5, ((T([1, 144, 160, 160], f16), T([1, 144, 160, 160], f16)), {})\\ncnt: 2, ((T([1, 4, 1, 1], f16), T([1, 4, 1, 1], f16)), {})\\ncnt: 1, ((T([1, 96, 160, 160], f16), T([1, 96, 160, 160], f16)), {})\\ncnt: 1, ((T([1, 96, 320, 320], f16), T([1, 96, 320, 320], f16)), {})\\ncnt: 1, ((T([1, 16, 320, 320], f16), T([1, 16, 320, 320], f16)), {})\\ncnt: 1, ((T([1, 8, 1, 1], f16), T([1, 8, 1, 1], f16)), {})\\ncnt: 2, ((T([1, 32, 320, 320], f16), T([1, 32, 320, 320], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.stack.default\\ncnt: 4, (([T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16)], -1), {})\\ncnt: 4, (([T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16)], -1), {})\\ncnt: 4, (([T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16)], -1), {})\\ncnt: 4, (([T([1, 88, 80, 80], f16), T([1, 88, 80, 80], f16)], -1), {})\\ncnt: 4, (([T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16), T([1, 88, 40, 40], f16)], -1), {})\\ncnt: 4, (([T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16), T([1, 88, 20, 20], f16)], -1), {})\\ncnt: 4, (([T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16), T([1, 88, 10, 10], f16)], -1), {})\\ncnt: 4, (([T([1, 88, 5, 5], f16), T([1, 88, 5, 5], f16)], -1), {})\\ncnt: 2, (([T([5000], f32), T([5000], f32), T([5000], f32), T([5000], f32)], 1), {})\\ncnt: 1, (([T([100, 6], f32)],), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sub.Tensor\\ncnt: 2, ((T([5000], f16, stride=(4,)), T([5000], f16, stride=(4,))), {})\\ncnt: 2, ((T([5000], f32), T([5000], f32)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\\ncnt: 1, ((T([1, 1920, 20, 20], f16), [2, 3], True), {})\\ncnt: 5, ((T([1, 1152, 20, 20], f16), [2, 3], True), {})\\ncnt: 1, ((T([1, 672, 20, 20], f16), [2, 3], True), {})\\ncnt: 3, ((T([1, 672, 40, 40], f16), [2, 3], True), {})\\ncnt: 4, ((T([1, 480, 40, 40], f16), [2, 3], True), {})\\ncnt: 1, ((T([1, 240, 40, 40], f16), [2, 3], True), {})\\ncnt: 2, ((T([1, 240, 80, 80], f16), [2, 3], True), {})\\ncnt: 1, ((T([1, 144, 80, 80], f16), [2, 3], True), {})\\ncnt: 2, ((T([1, 144, 160, 160], f16), [2, 3], True), {})\\ncnt: 1, ((T([1, 96, 160, 160], f16), [2, 3], True), {})\\ncnt: 1, ((T([1, 16, 320, 320], f16), [2, 3], True), {})\\ncnt: 1, ((T([1, 32, 320, 320], f16), [2, 3], True), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.default\\ncnt: 20, ((T([2], f16),), {})\\ncnt: 12, ((T([3], f16),), {})\\ncnt: 1, ((T([1, 100, 6], f32),), {})\\ncnt: 16, ((T([1, 88, 5, 5], f16),), {})\\ncnt: 40, ((T([1, 88, 10, 10], f16),), {})\\ncnt: 40, ((T([1, 88, 20, 20], f16),), {})\\ncnt: 40, ((T([1, 88, 40, 40], f16),), {})\\ncnt: 16, ((T([1, 88, 80, 80], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.dim_IntList\\ncnt: 4, ((T([1, 88, 10, 10, 2], f16), [-1]), {})\\ncnt: 4, ((T([1, 88, 20, 20, 2], f16), [-1]), {})\\ncnt: 4, ((T([1, 88, 40, 40, 2], f16), [-1]), {})\\ncnt: 4, ((T([1, 88, 80, 80, 2], f16), [-1]), {})\\ncnt: 4, ((T([1, 88, 40, 40, 3], f16), [-1]), {})\\ncnt: 4, ((T([1, 88, 20, 20, 3], f16), [-1]), {})\n```\n\n----------------------------------------\n\nTITLE: Running TensorExpr GPU Benchmark\nDESCRIPTION: Example command for running TensorExpr benchmark on GPU with forward mode, trace JIT mode, and TensorExpr CUDA fuser.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/tensorexpr/HowToRun.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npython -m benchmarks.tensorexpr --device gpu --mode fwd --jit-mode trace --cuda-fuser=te\n```\n\n----------------------------------------\n\nTITLE: Documentation Link in Markdown\nDESCRIPTION: Markdown link syntax referencing the documentation writing section in CONTRIBUTING.md\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[Writing documentation section of CONTRIBUTING.md](../CONTRIBUTING.md#writing-documentation)\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX Namespace for PyTorch Build\nDESCRIPTION: Configures the namespace for ONNX based on whether using system ONNX or not. Sets it to 'onnx_torch' when not using system ONNX, and 'onnx' when using system ONNX.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT USE_SYSTEM_ONNX)\n  set(ONNX_NAMESPACE\n      \"onnx_torch\"\n      CACHE\n        STRING\n        \"A namespace for ONNX; needed to build with other frameworks that share ONNX.\"\n  )\nelse()\n  set(ONNX_NAMESPACE\n      \"onnx\"\n      CACHE\n        STRING\n        \"A namespace for ONNX; needed to build with other frameworks that share ONNX.\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA Tensor Multiplication in PyTorch\nDESCRIPTION: This function implements element-wise tensor multiplication for CUDA devices in PyTorch. It takes two input tensors and performs the multiplication.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4_ops10mul_Tensor4callERKNS_6TensorES4_\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.unbind.int Operator (Log)\nDESCRIPTION: Log entries detailing calls to the PyTorch `aten.unbind.int` operator. Each line shows the invocation count (`cnt`) and the input tensor being unbound, including its shape, data type (f16), and stride information. The default dimension (0) is implied.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.unbind.int\ncnt: 3, ((T([3, 64, 4, 401, 32], f16, stride=(128, 153984, 32, 384, 1)),), {})\ncnt: 9, ((T([3, 64, 4, 197, 64], f16, stride=(256, 151296, 64, 768, 1)),), {})\ncnt: 1, ((T([2, 64, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Enabling Health Check Server in LocalElasticAgent\nDESCRIPTION: Sets up an environment variable to enable a health check monitoring server in LocalElasticAgent. The TORCHELASTIC_HEALTH_CHECK_PORT variable must be defined with a port number.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/agent.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport os\n\nos.environ[\"TORCHELASTIC_HEALTH_CHECK_PORT\"] = \"8080\"\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Compilation Flags in ATen CMake\nDESCRIPTION: Sets compilation flags for CUDA, enabling various features like tensor core MMA and extended MMA shapes for SM90 architecture.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDA AND NOT USE_ROCM)\n  add_definitions(-DCUTLASS_ENABLE_TENSOR_CORE_MMA=1)\n  add_definitions(-DCUTLASS_ENABLE_SM90_EXTENDED_MMA_SHAPES=1)\n  add_definitions(-DCUTE_SM90_EXTENDED_MMA_SHAPES_ENABLED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Performing Tensor Addition with ATen Add Operator\nDESCRIPTION: Illustrates the `aten.add.Tensor` operator for tensor addition. Supports various data types like i64 and f16 with shapes like [64, 256, 64, 64]. The operator requires correct dimensional alignment and data type compatibility between input tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 38, ((T([], i64), 1), {})\ncnt: 4, ((T([64, 256, 64, 64], f16), T([64, 256, 64, 64], f16)), {})\ncnt: 6, ((T([64, 512, 32, 32], f16), T([64, 512, 32, 32], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Running commitlist.py with Bash\nDESCRIPTION: This Bash command is used to generate a new commit list for specific commit hashes and update to the latest cherry-picks. It requires Python and the commitlist.py script as dependencies. The inputs include the version tag and the latest commit hash, and it updates the result file with new commits.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/README.md#2025-04-22_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\npython commitlist.py --create_new tags/v1.13.1 <commit-hash>\n```\n\nLANGUAGE: Bash\nCODE:\n```\npython commitlist.py --update_to <latest-cherry-pick-hash>\n```\n\n----------------------------------------\n\nTITLE: Apache 2.0 License Header for PyTorch\nDESCRIPTION: Standard Apache 2.0 license header with Facebook copyright notice used across PyTorch source files. Specifies usage rights, redistribution terms, and liability disclaimers under the Apache 2.0 license.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/apache_python.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Copyright (c) 2016-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows the usage patterns of batch normalization operations with different tensor shapes and configurations. It includes counts for each unique configuration and details on input tensors, running statistics, and hyperparameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([32, 1024, 7, 7], f16), T([32, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), False, 1e-05, [True, True, True]), {})\ncnt: 10, ((T([32, 224, 7, 7], f16), T([32, 224, 7, 7], f16), T([224], f16), T([224], f16), T([224], f16), T([224], f32), T([224], f32), False, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([32, 768, 14, 14], f16), T([32, 768, 14, 14], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f32), T([768], f32), False, 1e-05, [True, True, True]), {})\ncnt: 10, ((T([32, 192, 14, 14], f16), T([32, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), False, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([32, 160, 28, 28], f16), T([32, 160, 28, 28], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), False, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), False, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([32, 128, 56, 56], f16), T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([32, 64, 112, 112], f16), T([32, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Storage Offset for TensorImpl in C++\nDESCRIPTION: This function sets the storage offset for a TensorImpl object. It's used to adjust the starting point of the tensor data within its storage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c1010TensorImpl18set_storage_offsetEl\n```\n\n----------------------------------------\n\nTITLE: Performance Metrics Table in Markdown\nDESCRIPTION: Markdown table displaying performance metrics for different experimental configurations including original setup, h2d_d2h_threads, and multiple predict worker scenarios. Metrics include warmup latency, average latency, throughput, and GPU utilization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_128_false.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |\n| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |\n| original | 5.355 +/- 0.293 | 14.172 +/- 0.267 | 518.884 +/- 7.854 | 56.108 +/- 0.862 |\n| h2d_d2h_threads | 3.810 +/- 0.319 | 14.146 +/- 1.145 | 551.909 +/- 34.079 | 52.057 +/- 4.121 |\n| 2_predict_workers | 3.639 +/- 0.037 | 11.161 +/- 0.160 | 636.701 +/- 14.753 | 53.279 +/- 2.659 |\n| 3_predict_workers | 4.930 +/- 0.060 | 10.532 +/- 0.801 | 677.736 +/- 25.115 | 53.806 +/- 1.324 |\n| 4_predict_workers | 3.819 +/- 0.253 | 11.451 +/- 0.439 | 638.146 +/- 22.611 | 50.129 +/- 1.764 |\n```\n\n----------------------------------------\n\nTITLE: Adding RPATH for Apple Builds in CMake\nDESCRIPTION: For Apple platforms (macOS, iOS), adds a custom post-build command to the `pytorch_jni` target. This command uses `install_name_tool` to add `@loader_path` to the runtime path (rpath) of the built library, helping it find its dependencies relative to its own location at runtime.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nif(APPLE)\n  # Need to add rpath so dlopen can find dependencies.\n  add_custom_command(TARGET pytorch_jni\n      POST_BUILD COMMAND\n      ${CMAKE_INSTALL_NAME_TOOL} -add_rpath \"@loader_path\"\n        $<TARGET_FILE:pytorch_jni>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Traversing Tree AST with Tree Views in C++\nDESCRIPTION: Example of traversing a Tree AST using switch statements and constructing appropriate Tree views for different node types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nswitch (tree.kind()) {\n  case TK_VAR:\n          auto var = Var(tree); // construct tree view\n        return environment_stack->getSugaredVar(var.name());\n  case '.':\n    auto select = Select(tree); // construct tree view\n    auto sv = emitSugaredExpr(select.value(), 1);\n    return sv->attr(select.range(), method, select.selector().name());\n  case TK_APPLY: {\n        auto apply = Apply(tree); // construct tree view\n        return emitApplyExpr(apply, n_binders);\n  } break;\n\n```\n\n----------------------------------------\n\nTITLE: Referencing worker_main Function in Python\nDESCRIPTION: This snippet references the worker_main function from the torch.distributed.elastic.control_plane module. It's used to generate automatic documentation for the function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/control_plane.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: torch.distributed.elastic.control_plane.worker_main\n```\n\n----------------------------------------\n\nTITLE: Training Commit Classifier in Python\nDESCRIPTION: Commands for training and using a machine learning classifier for commit categorization\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython classifier.py --train\npython categorize.py --use_classifier\n```\n\n----------------------------------------\n\nTITLE: CUDA Device Selection in C++\nDESCRIPTION: These functions handle CUDA device selection and querying. They are essential for managing which GPU is active for PyTorch operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104cuda14ExchangeDeviceEi\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104cuda14MaybeSetDeviceEi\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104cuda14current_deviceEv\n```\n\n----------------------------------------\n\nTITLE: Summing Tensor Along Dimension in PyTorch\nDESCRIPTION: This snippet shows a sum operation on a tensor in PyTorch, likely used for reducing the output of the final layer. It sums along the first dimension (batch dimension) and keeps the result's dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Negative Log Likelihood Loss Backward in PyTorch\nDESCRIPTION: Logs occurrences of \\\"aten.nll_loss_backward.default\\\", used during training to compute gradients for NLL Loss functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_21\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([1024, 50265], f16), T([1024], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Class with Method but No Docstring\nDESCRIPTION: A class with a method that uses comments instead of a proper docstring.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass LongWithoutDocstring:\n    # A comment isn't a docstring\n\n    def short1(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Tensor Addition and Convolution Operations\nDESCRIPTION: Multiple tensor addition operations and convolution operations with various configurations, including 7x7 and 1x1 convolutions with different stride and padding settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convmixer_768_32_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten.add.Tensor((T([32, 768, 32, 32], f16), T([32, 768, 32, 32], f16)), {})\naten.convolution.default((T([32, 3, 224, 224], f16), T([768, 3, 7, 7], f16), T([768], f16), [7, 7], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Bitwise Operations on Boolean Tensors in PyTorch\nDESCRIPTION: This snippet shows bitwise AND operations on boolean tensors (b8 data type) in PyTorch. These operations are likely used for mask combination or logical operations on detection results.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.bitwise_and.Tensor\ncnt: 4, ((T([5000], b8), T([5000], b8)), {})\ncnt: 4, ((T([0], b8), T([0], b8)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Installation Rules for JNI Library in CMake\nDESCRIPTION: Specifies installation rules for the built JNI target (`${PYTORCH_JNI_TARGET}`). It installs the archive (static lib, if applicable), library (shared lib/DLL), and runtime (executable, or DLL on Windows) components to standard destinations defined by `CMAKE_INSTALL_LIBDIR` and `CMAKE_INSTALL_BINDIR` from the `GNUInstallDirs` module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS ${PYTORCH_JNI_TARGET}\n  ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}) #For windows\n```\n\n----------------------------------------\n\nTITLE: Log Entry for Tensor Pair (Shape [128, 256, 28, 28], f16)\nDESCRIPTION: Logs the occurrence (count 3) of a tensor pair, both with shape [128, 256, 28, 28] and dtype f16, using default strides. Likely generated during PyTorch execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([128, 256, 28, 28], f16), T([128, 256, 28, 28], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: PEP Method for Performance Benchmarking\nDESCRIPTION: This snippet illustrates using the Performance Evaluation Platform (PEP) to benchmark machine learning models on ARMv7 devices. It includes instructions for setting up the environment and running test commands, highlighting the ability to modify specifications.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Clone PyTorch 1.0 repo\nmkdir ~/Code && cd ~/Code\ngit clone --recursive https://github.com/pytorch/pytorch.git\ncd pytorch\n\n# Optional: update QNNPACK submodule to latest revision\ngit submodule update --remote third_party/QNNPACK\n\n# Clone PEP repo\ncd ~/Code\ngit clone --recursive https://github.com/facebook/FAI-PEP.git aibench\ncd aibench\n\n# Run PEP benchmark with cool specifications. Try changing that cmd with more specifications!\n# First time compile could take 20+ minutes\n./benchmarking/run_bench.py \\\n  --platform android \\\n  -b ~/Code/aibench/specifications/models/caffe2/mobilenet_v2/mobilenet_v2_quant.json \\\n  --platform android --repo_dir ~/Code/pytorch \\\n  --frameworks_dir ~/Code/aibench/specifications/frameworks --framework caffe2\n```\n\n----------------------------------------\n\nTITLE: Tensor Softmax Operation in PyTorch\nDESCRIPTION: This code snippet demonstrates the use of the `aten._log_softmax.default` operator in PyTorch, which computes the log softmax of a tensor along a specified dimension. Required inputs include the tensor and dimension, with an optional boolean for whether to retain dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForCausalLM_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([2048, 50005], f16), 1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.nll_loss_backward.default Operator (Text)\nDESCRIPTION: Logs a call to the `aten.nll_loss_backward.default` operator, used for computing gradients of the negative log likelihood loss. The log shows the arguments including the gradient output (scalar f16), input tensor shape ([128, 1000] f16), target tensor shape ([128] i64), reduction type (1, likely 'mean'), and ignore index (-100).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining and Linking jitbackend_test Shared Library in CMake\nDESCRIPTION: Creates a shared library named `jitbackend_test` from the specified C++ source file. This library is likely related to testing JIT backend functionalities and is intended for use within PyTorch Python tests via `torch.ops.load_library()`. It links against the main `torch` library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(jitbackend_test SHARED ${JIT_TEST_ROOT}/test_backend_lib.cpp)\ntarget_link_libraries(jitbackend_test torch)\n```\n\n----------------------------------------\n\nTITLE: Building Shared Memory Library\nDESCRIPTION: Configures the shared memory library build settings including version properties, include directories, and linking with Torch CPU library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/lib/libshm/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(shm SHARED core.cpp)\nif(HAVE_SOVERSION)\n  set_target_properties(shm PROPERTIES\n      VERSION ${TORCH_VERSION} SOVERSION ${TORCH_SOVERSION})\nendif()\n\ntarget_include_directories(shm PUBLIC\n  ${TORCH_ROOT}/torch/lib # provides \"libshm/libshm.h\"\n)\n\n### Torch packages supposes libraries prefix is \"lib\"\nset_target_properties(shm PROPERTIES\n  PREFIX \"lib\"\n  IMPORT_PREFIX \"lib\"\n  CXX_STANDARD 17)\ntarget_link_libraries(shm PRIVATE ${TORCH_CPU_LIB})\n```\n\n----------------------------------------\n\nTITLE: Building HTML Documentation\nDESCRIPTION: Command to generate HTML documentation using make build system. Output files will be created in the build/html directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Pixel Shuffle with Einops in PyTorch\nDESCRIPTION: Alternative implementation of pixel shuffle using Einops for dimension rearrangement.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/dim/README.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef pixel_shuffle_einops(img, upscale_factor=2):\n    from einops import rearrange\n    return rearrange(img, 'b (c h2 w2) h w -> b c (h h2) (w w2)', h2=upscale_factor, w2=upscale_factor)\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Role in Sphinx Documentation Template\nDESCRIPTION: Creates a custom 'hidden' role for use in Sphinx documentation that applies the 'hidden-section' class to text.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/_templates/autosummary/class.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Including Miscellaneous Checks and ExternalProject in CMake\nDESCRIPTION: Includes two standard CMake modules/scripts: `cmake/MiscCheck.cmake` for various compiler and system checks, and `ExternalProject` to enable functionality for managing third-party dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ Misc checks to cope with various compiler modes\ninclude(cmake/MiscCheck.cmake)\n\n# External projects\ninclude(ExternalProject)\n```\n\n----------------------------------------\n\nTITLE: Compiling QNNPACK on Raspberry Pi\nDESCRIPTION: This snippet outlines the process to build QNNPACK using CMake on a Raspberry Pi. It includes steps for cloning the PyTorch repository, updating submodules, and configuring the build settings to prevent out-of-memory issues by limiting simultaneous jobs to one.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Clone PyTorch 1.0 repo\ngit clone --recursive https://github.com/pytorch/pytorch.git\ncd pytorch\n\n# Optional: update QNNPACK submodule to latest revision\ngit submodule update --remote third_party/QNNPACK\n\n# Build Caffe2 (including binaries) for the host system\n# Use only 1 thread for build to avoid out-of-memory failures\nMAX_JOBS=1 scripts/build_local.sh -DBUILD_BINARY=ON -DBUILD_PYTHON=OFF \\\n    -DUSE_OBSERVERS=OFF -DUSE_DISTRIBUTED=OFF\n\n# Download model weights\nwget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/init_net.pb\n\n# Download model graph\nwget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/predict_net.pb\n\n# Run speed benchmark with 50 warm-up iterations and 10 measurement iterations\nbuild/bin/speed_benchmark --net predict_net.pb --init_net init_net.pb \\\n    --input data --input_dims 1,3,224,224 --input_type float \\\n    --warmup 50 --iter 10\n```\n\n----------------------------------------\n\nTITLE: Apache 2.0 License Header\nDESCRIPTION: Standard Apache 2.0 license header with Facebook copyright notice, defining usage terms and conditions for the PyTorch project.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/apache_header.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n/**\n * Copyright (c) 2016-present, Facebook, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n```\n\n----------------------------------------\n\nTITLE: Embedding Operations - PyTorch\nDESCRIPTION: Token embedding operations for input sequences, including both forward and backward passes. Handles vocabulary size of 50257 and embedding dimension of 768.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_GPT2_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\naten.embedding.default((T([50257, 768], f16), T([4, 512], i64)), {})\naten.embedding_dense_backward.default((T([4, 512, 768], f16), T([4, 512], i64), 50257, -1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Importing and Using tempfile Module in Python\nDESCRIPTION: This snippet imports the tempfile module and prints the path of a temporary file. It demonstrates how to use the tempfile module to get the system's temporary directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/set_linter_testdata/python_code.py.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport tempfile\n\nprint(f\"{tempfile.gettempdir()}/memory_snapshot.pickle\")\n```\n\n----------------------------------------\n\nTITLE: Batch Updating Workflow Branch References with Bash\nDESCRIPTION: This shell one-liner iterates over all YAML files in the .github/workflows directory to update workflow branch references from '@main' to '@release/2.0'. It uses the sed utility for an in-place replacement, automating a critical branch update step after a release is cut. Dependencies: bash shell, GNU sed, and permission to write to the workflow files. Key parameter: the target branch (e.g., 'release/2.0'). Input: YAML workflow files; output: updated branch references for CI consistency.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/RELEASE.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nfor i in .github/workflows/*.yml; do sed -i -e s#@main#@release/2.0# $i; done\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors and Moving Models to a Selected Device in PyTorch\nDESCRIPTION: This Python code demonstrates how to utilize a pre-defined `torch.device` object (stored in `args.device`) to create tensors directly on the target device (CPU or CUDA) using `torch.empty(..., device=args.device)`. It also shows moving a PyTorch model (`net`) to the specified device using the `.to(device=args.device)` method, ensuring operations run on the correct hardware. It requires `torch` and assumes `args.device` (likely defined by the previous snippet) and a `Network` class (presumably a `torch.nn.Module`) are defined.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nx = torch.empty((8, 42), device=args.device)\nnet = Network().to(device=args.device)\n```\n\n----------------------------------------\n\nTITLE: Checking GCC/Clang Version for KleidiAI Support in PyTorch\nDESCRIPTION: Disables KleidiAI if the compiler version is less than 11, as KleidiAI requires at least GCC 11 or Clang 11.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_KLEIDIAI AND CMAKE_C_COMPILER_VERSION)\n    if(CMAKE_C_COMPILER_VERSION VERSION_LESS 11)\n      set(USE_KLEIDIAI OFF)\n      message(WARNING \"Disabling KleidiAI: Requires atleast GCC 11 or Clang 11\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Lifting Operation\nDESCRIPTION: This snippet shows a tensor lifting operation applied to a 1D tensor of shape [128] with int64 data type, likely used for creating a fresh copy of indices or labels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\n```\n\n----------------------------------------\n\nTITLE: Importing Control Plane Module in Python\nDESCRIPTION: This snippet shows how to import the control_plane module from torch.distributed.elastic. It sets up the current module for documentation purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/control_plane.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: torch.distributed.elastic.control_plane\n.. currentmodule:: torch.distributed.elastic.control_plane\n```\n\n----------------------------------------\n\nTITLE: Setting UTF-8 Source Encoding for MSVC in CMake\nDESCRIPTION: Checks if the compiler is MSVC. If it is, it attempts to append the `/utf-8` flag to CMAKE_CXX_FLAGS using `append_cxx_flag_if_supported`. This ensures the compiler treats source files as UTF-8 encoded.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_29\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n  # The source code is in utf-8 encoding\n  append_cxx_flag_if_supported(\"/utf-8\" CMAKE_CXX_FLAGS)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Value Clamping Operations in PyTorch\nDESCRIPTION: This snippet shows tensor value clamping operations used to constrain values within specific ranges. These are typically used for bounding box coordinates in object detection or for numerical stability in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clamp.default\ncnt: 2, ((T([1438452, 1], f16), None, 4.135166556742356), {})\ncnt: 1, ((T([5000, 2], f16, stride=(4, 2)), 0, 1199), {})\ncnt: 2, ((T([5000, 2], f16, stride=(4, 2)), 0, 799), {})\ncnt: 3, ((T([5000, 2], f16, stride=(4, 2)), 0, 800), {})\ncnt: 1, ((T([5000, 2], f16, stride=(4, 2)), 0, 1155), {})\ncnt: 1, ((T([5000, 2], f16, stride=(4, 2)), 0, 1115), {})\ncnt: 2, ((T([0], f32), 2, 5), {})\ncnt: 2, ((T([0, 91], f16), None, 4.135166556742356), {})\ncnt: 1, ((T([0, 182], f16), 0, 1199), {})\ncnt: 2, ((T([0, 182], f16), 0, 799), {})\ncnt: 3, ((T([0, 182], f16), 0, 800), {})\ncnt: 1, ((T([0, 182], f16), 0, 1155), {})\ncnt: 1, ((T([0, 182], f16), 0, 1115), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Autodiff Tests for TorchScript Functions\nDESCRIPTION: Shows how to specify autodiff test configurations for TorchScript functions. These configurations control whether a node should be autodifferentiated and specify which nodes should appear in the differentiated graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n('conv1d', ...), # No symbolic gradient formula\n('avg_pool2d', ..., (True,)), # Has symbolic gradient formula, only has one nonfusible node aten::avg_pool2d\n('nll_loss', ..., (True, 'aten::nll_loss_forward')), # Is replaced by a different node in its symbolic gradient formula\n('dropout', ..., (True, ['prim::is_CUDA', 'aten::bernoulli_'], ['aten::rand_like', ..., 'aten::div'])), # Some ops are fused when fusion is enabled\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Usage Statistics\nDESCRIPTION: This snippet shows the usage statistics of various PyTorch operators in a deep learning model. It includes operator names, usage counts, and input tensor shapes. This information is useful for understanding the model architecture and potential optimization opportunities.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gmlp_s16_224_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 30, ((T([64, 768, 196], f16), [64, 768, 196]), {})\nOperator: aten.add.Tensor\ncnt: 30, ((T([64, 768, 196], f16), T([196], f16)), {})\ncnt: 30, ((T([64, 196, 256], f16, stride=(50176, 1, 196)), T([64, 196, 256], f16)), {})\ncnt: 30, ((T([64, 196, 256], f16), T([64, 196, 256], f16)), {})\nOperator: aten.addmm.default\ncnt: 30, ((T([1536], f16), T([12544, 256], f16), T([256, 1536], f16, stride=(1, 256))), {})\ncnt: 30, ((T([256], f16), T([12544, 768], f16), T([768, 256], f16, stride=(1, 768))), {})\ncnt: 1, ((T([1000], f16), T([64, 256], f16), T([256, 1000], f16, stride=(1, 256))), {})\nOperator: aten.bmm.default\ncnt: 30, ((T([64, 768, 196], f16, stride=(150528, 1, 768)), T([64, 196, 196], f16, stride=(0, 1, 196))), {})\ncnt: 30, ((T([64, 196, 768], f16), T([64, 768, 196], f16, stride=(150528, 1, 768))), {})\ncnt: 30, ((T([64, 768, 196], f16, stride=(150528, 1, 768)), T([64, 196, 196], f16, stride=(0, 196, 1))), {})\nOperator: aten.cat.default\ncnt: 30, (([T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(150528, 1, 196))], 2), {})\nOperator: aten.clone.default\ncnt: 1, ((T([64, 3, 224, 224], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([256, 3, 16, 16], f16), T([256], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 256, 14, 14], f16, stride=(50176, 1, 3584, 256)), T([64, 3, 224, 224], f16), T([256, 3, 16, 16], f16), [256], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 224, 224], f16), T([64, 3, 224, 224], f16)), {})\ncnt: 30, ((T([196, 196], f16), T([196, 196], f16, stride=(1, 196))), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([64, 196, 256], f16, stride=(256, 0, 1)), 196), {})\nOperator: aten.gelu.default\ncnt: 30, ((T([64, 196, 1536], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 30, ((T([64, 196, 1536], f16), T([64, 196, 1536], f16)), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([64], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([64, 196, 256], f16), [1]), {})\nOperator: aten.mm.default\ncnt: 1, ((T([64, 1000], f16), T([1000, 256], f16)), {})\ncnt: 1, ((T([1000, 64], f16, stride=(1, 1000)), T([64, 256], f16)), {})\ncnt: 30, ((T([12544, 256], f16), T([256, 768], f16)), {})\ncnt: 30, ((T([256, 12544], f16, stride=(1, 256)), T([12544, 768], f16)), {})\ncnt: 30, ((T([12544, 1536], f16), T([1536, 256], f16)), {})\ncnt: 30, ((T([1536, 12544], f16, stride=(1, 1536)), T([12544, 256], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 30, ((T([64, 196, 768], f16, stride=(301056, 1536, 1)), T([64, 196, 768], f16, stride=(150528, 1, 196))), {})\ncnt: 30, ((T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(301056, 1536, 1))), {})\ncnt: 30, ((T([64, 196, 768], f16), T([64, 196, 768], f16, stride=(150528, 1, 196))), {})\nOperator: aten.native_layer_norm.default\ncnt: 31, ((T([64, 196, 256], f16, stride=(50176, 1, 196)), [256], T([256], f16), T([256], f16), 1e-06), {})\ncnt: 30, ((T([64, 196, 768], f16, stride=(301056, 1536, 1)), [768], T([768], f16), T([768], f16), 1e-05), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 31, ((T([64, 196, 256], f16), T([64, 196, 256], f16, stride=(50176, 1, 196)), [256], T([64, 196, 1], f32), T([64, 196, 1], f32), T([256], f16), T([256], f16), [True, True, True]), {})\ncnt: 30, ((T([64, 196, 768], f16, stride=(150528, 1, 196)), T([64, 196, 768], f16, stride=(301056, 1536, 1)), [768], T([64, 196, 1], f32), T([64, 196, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\nOperator: aten.new_empty_strided.default\ncnt: 30, ((T([196, 196], f16, stride=(1, 196)), [196, 196], [196, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\nOperator: aten.split.Tensor\ncnt: 30, ((T([64, 196, 1536], f16), 768, -1), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([64, 1000], f16), [0], True), {})\ncnt: 30, ((T([12544, 256], f16), [0], True), {})\ncnt: 30, ((T([64, 768, 196], f16, stride=(150528, 1, 768)), [0, 1], True), {})\ncnt: 30, ((T([64, 196, 196], f16), [0], True), {})\ncnt: 30, ((T([12544, 1536], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Division Operations in PyTorch\nDESCRIPTION: Profiling data for tensor-tensor division operations showing the count and tensor shapes. This operation divides empty tensors by the constant 32000, possibly for learning rate scaling or normalization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 32000), {})\n```\n\n----------------------------------------\n\nTITLE: Applying LayerNorm Backward with aten Operators - Python\nDESCRIPTION: This snippet captures calls to the aten.native_layer_norm_backward.default operator in PyTorch, used for the backward pass of a layer normalization operation. The inputs include input, grad_output, normalized_shape, mean, rstd, and the weight/bias tensors, crucial for computing gradients; outputs track flags for gradients needed. Dependencies include PyTorch and proper tensor geometry. Inputs are typically multiple tensors matching a normalized shape and appropriate data types (f16, f32). The pattern assumes correct dimension alignment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_layer_norm_backward.default\ncnt: 26, ((T([2, 1024, 768], f16), T([2, 1024, 768], f16), [768], T([2, 1024, 1], f32), T([2, 1024, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Python Timer Creation for TorchScript Mode\nDESCRIPTION: Example of creating a Timer instance for TorchScript mode in Python. It includes statement code to call the JIT model and setup code that loads the model and warms it up.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nTimer(\n    stmt=\"\"\"\n        y = jit_model(x, w)\n    \"\"\",\n    setup=\"\"\",\n        # benchmark.setup.py_setup\n        # jit_model = torch.jit.load(...)\n        # Warm up jit_model\n    \"\"\",\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring ITT Instrumentation Support\nDESCRIPTION: Adds Intel ITT (Instrumentation and Tracing Technology) support to PyTorch Python bindings when the USE_ITT option is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_ITT)\n  list(APPEND TORCH_PYTHON_SRCS\n    ${TORCH_SRC_DIR}/csrc/itt.cpp\n  )\n  list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_ITT)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Runtime Benchmark Sources in CMake\nDESCRIPTION: Adds source files for static runtime benchmarks to the STATIC_RUNTIME_BENCHMARK_SRCS list and sets it in the parent scope. This includes deep_wide_pt.cc and deep_wide_pt_bench.cc files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/static_runtime/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND STATIC_RUNTIME_BENCHMARK_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/deep_wide_pt.cc)\nlist(APPEND STATIC_RUNTIME_BENCHMARK_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/deep_wide_pt_bench.cc)\nset(STATIC_RUNTIME_BENCHMARK_SRCS ${STATIC_RUNTIME_BENCHMARK_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Tensor Copy Operations in PyTorch\nDESCRIPTION: Profiling data for tensor copy operations showing the count and tensor shapes involved. This operation copies data from one tensor to another with identical shape.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Restructured Text Documentation Structure\nDESCRIPTION: RST markup defining the documentation structure including table of contents, warnings, and sections for functorch documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/index.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n:github_url: https://github.com/pytorch/functorch\n\nfunctorch\n===================================\n\n.. currentmodule:: functorch\n\n.. warning::\n\n   We've integrated functorch into PyTorch. As the final step of the\n   integration, the functorch APIs are deprecated as of PyTorch 2.0.\n   Please use the torch.func APIs instead and see the\n   `migration guide <https://pytorch.org/docs/main/func.migrating.html>`_\n   and `docs <https://pytorch.org/docs/main/func.html>`_\n   for more details.\n\n.. toctree::\n   :maxdepth: 2\n   :caption: functorch: Getting Started\n\n   install\n   notebooks/whirlwind_tour.ipynb\n   ux_limitations\n\n.. toctree::\n   :maxdepth: 2\n   :caption: functorch API Reference and Notes\n\n   functorch\n   experimental\n   aot_autograd\n\n.. toctree::\n   :maxdepth: 1\n   :caption: functorch Tutorials\n\n   notebooks/jacobians_hessians.ipynb\n   notebooks/ensembling.ipynb\n   notebooks/per_sample_grads.ipynb\n   notebooks/neural_tangent_kernels.ipynb\n   notebooks/aot_autograd_optimizations.ipynb\n   notebooks/minifier.ipynb\n```\n\n----------------------------------------\n\nTITLE: Template Source Comment in Sphinx Documentation\nDESCRIPTION: A comment indicating the source file of this template and noting that it doesn't include inherited members in the documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/_templates/classtemplate.rst#2025-04-22_snippet_3\n\nLANGUAGE: restructuredtext\nCODE:\n```\n..\n  autogenerated from source/_templates/classtemplate.rst\n  note it does not have :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Checking Cherry-picked Commits\nDESCRIPTION: Verifies if a specific commit hash exists in the dataset, useful for identifying cherry-picked commits.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/explore.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# check for cherry-picked commits\nexample_sha = '55c76baf579cb6593f87d1a23e9a49afeb55f15a'\ncommit_hashes = set(commit_list_df.commit_hash.to_list())\n\nexample_sha[:11] in commit_hashes\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Tensor Sum Operations\nDESCRIPTION: This snippet demonstrates the usage of tensor sum operations in PyTorch. It includes both symbolic and default sum operations, showing tensor shapes, dimensions to sum over, and whether to keep dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})\ncnt: 4, ((T([32, 1152, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), [2, 3], True), {})\n# ... (truncated for brevity)\n\nOperator: aten.sum.default\ncnt: 1, ((T([32, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch SiLU Backward Operation Calls\nDESCRIPTION: Logs of aten.silu_backward.default operations with tensor shapes and data types. Each line shows the count, tensor shapes ([batch_size, channels, height, width]), and data type (f16 for float16).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.silu_backward.default\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16)), {})\ncnt: 1, ((T([128, 1044, 7, 7], f16), T([128, 1044, 7, 7], f16)), {})\ncnt: 1, ((T([128, 972, 7, 7], f16), T([128, 972, 7, 7], f16)), {})\ncnt: 1, ((T([128, 906, 7, 7], f16), T([128, 906, 7, 7], f16)), {})\ncnt: 1, ((T([128, 840, 7, 7], f16), T([128, 840, 7, 7], f16)), {})\ncnt: 1, ((T([128, 768, 14, 14], f16), T([128, 768, 14, 14], f16)), {})\ncnt: 1, ((T([128, 702, 14, 14], f16), T([128, 702, 14, 14], f16)), {})\ncnt: 1, ((T([128, 636, 14, 14], f16), T([128, 636, 14, 14], f16)), {})\ncnt: 1, ((T([128, 570, 14, 14], f16), T([128, 570, 14, 14], f16)), {})\ncnt: 1, ((T([128, 504, 14, 14], f16), T([128, 504, 14, 14], f16)), {})\ncnt: 1, ((T([128, 432, 14, 14], f16), T([128, 432, 14, 14], f16)), {})\ncnt: 1, ((T([128, 366, 28, 28], f16), T([128, 366, 28, 28], f16)), {})\ncnt: 1, ((T([128, 300, 28, 28], f16), T([128, 300, 28, 28], f16)), {})\ncnt: 1, ((T([128, 228, 56, 56], f16), T([128, 228, 56, 56], f16)), {})\ncnt: 1, ((T([128, 162, 56, 56], f16), T([128, 162, 56, 56], f16)), {})\ncnt: 1, ((T([128, 96, 112, 112], f16), T([128, 96, 112, 112], f16)), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Debugging and Configuring DDPOptimizer in TorchDynamo\nDESCRIPTION: Instructions for debugging DDPOptimizer by setting environment variables to control log verbosity, and how to disable the optimization if needed. TORCH_LOGS can be set to 'ddp_graphs' for full graph dumps or other values for basic information.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/ddp.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nTORCH_LOGS='ddp_graphs'\n```\n\nLANGUAGE: python\nCODE:\n```\ntorch._dynamo.config.optimize_ddp=False\n```\n\n----------------------------------------\n\nTITLE: Conditionally Linking test_jit against MKL-DNN in CMake\nDESCRIPTION: Checks if the `USE_MKLDNN` flag is enabled. If true, it links the `test_jit` executable against the MKL-DNN library (`caffe2::mkldnn`). This enables testing JIT functionality related to MKL-DNN integration for optimized CPU performance.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_MKLDNN)\n  target_link_libraries(test_jit PRIVATE caffe2::mkldnn)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating New Empty Strided Tensors in ATen\nDESCRIPTION: Generates new tensors with specified strides and dimensions, critical for setting up complex tensor operations. Requires a template tensor and lists of sizes and strides, e.g., [16777216, 16384, 128, 1]. Outputs are empty tensors prepared for subsequent data population, ensuring efficient memory layout and access patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.new_empty_strided.default\ncnt: 2, ((T([1, 1024, 128, 128], f16), [1, 1024, 128, 128], [16777216, 16384, 128, 1]), {})\ncnt: 2, ((T([1, 2048, 64, 64], f16), [1, 2048, 64, 64], [8388608, 4096, 64, 1]), {})\ncnt: 7, ((T([1, 4096, 32, 32], f16), [1, 4096, 32, 32], [4194304, 1024, 32, 1]), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Python Class 'LintInit' Without Docstring\nDESCRIPTION: This snippet defines a Python class named 'LintInit' without a docstring. It includes an __init__ method with a comment indicating multiple lines of code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt.before.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass LintInit:\n    def __init__(self) -> None:\n        # Lots of lines!\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Masked Fill Operations in PyTorch\nDESCRIPTION: Records occurrence of the \\\"aten.masked_fill_.Scalar\\\", used to fill tensors with a scalar value based on a mask.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.masked_fill_.Scalar\ncnt: 1, ((T([128, 128], f32), T([128, 128], b8), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Quality for Sparsified DLRM Models in Python\nDESCRIPTION: Python script to evaluate model quality metrics for sparsified DLRM models. It uses raw and processed data files along with sparse model metadata.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/data_sparsifier/benchmarks/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython evaluate_model_metrics.py --raw-data-file=<path_to_raw_data_txt_file> --processed-data-file=<path_to_kaggleAdDisplayChallenge_processed.npz> --sparse-model-metadata=<path_to_sparse_model_metadata_csv>\n```\n\n----------------------------------------\n\nTITLE: Enabling Tracing via Environment Variable in CMake\nDESCRIPTION: Initializes a `TRACE_ENABLED` flag to OFF. Checks if the environment variable `TRACE_ENABLED` is defined and set to \"1\"; if so, it sets the CMake variable `TRACE_ENABLED` to ON and prints a status message. Otherwise, it confirms tracing is OFF.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(TRACE_ENABLED OFF)\nif(DEFINED ENV{TRACE_ENABLED})\n  if($ENV{TRACE_ENABLED} STREQUAL \"1\")\n    message(STATUS \"TRACE_ENABLED ON\")\n    set(TRACE_ENABLED ON)\n  endif()\nendif()\nif(NOT TRACE_ENABLED)\n  message(STATUS \"TRACE_ENABLED OFF\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations\nDESCRIPTION: Sequence of convolution operations with different input/output channels and spatial dimensions. Operations use float16 precision and include parameters for stride, padding, and dilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 128, 14, 14], f16), T([4, 704, 14, 14], f16), T([128, 704, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Distributed Elastic Events Module\nDESCRIPTION: This snippet shows how to import the events module from PyTorch's distributed elastic package. It's a prerequisite for using the events API.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.distributed.elastic.events\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using AOTInductor Minifier in PyTorch\nDESCRIPTION: Example model implementation demonstrating how to set up AOTInductor minifier and intentionally trigger a compilation error using ReLU operation for testing purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_aot_inductor_minifier.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch._inductor import config as inductor_config\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 16)\n        self.relu = torch.nn.ReLU()\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.sigmoid(x)\n        return x\n\n\ninductor_config.aot_inductor.dump_aoti_minifier = True\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = \"compile_error\"\n\nwith torch.no_grad():\n    model = Model().to(\"cuda\")\n    example_inputs = (torch.randn(8, 10).to(\"cuda\"),)\n    ep = torch.export.export(model, example_inputs)\n    package_path = torch._inductor.aoti_compile_and_package(ep)\n    compiled_model = torch._inductor.aoti_load_package(package_path)\n    result = compiled_model(*example_inputs)\n```\n\n----------------------------------------\n\nTITLE: Appending FBGEMM C++ Compiler Flag in CMake\nDESCRIPTION: If the USE_FBGEMM option is enabled, this snippet appends the preprocessor definition `-DUSE_FBGEMM` to the C++ compiler flags (CMAKE_CXX_FLAGS). This definition makes FBGEMM-related code paths active during compilation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_35\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_FBGEMM)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_FBGEMM\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Documenting FunctionCounts Class in PyTorch Benchmark Utils\nDESCRIPTION: This snippet documents the FunctionCounts class from the torch.utils.benchmark module, including all its members.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_5\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: FunctionCounts\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Resetting Generated Directories in Git (Shell)\nDESCRIPTION: The git_reset_generated_dirs.sh script resets the generated directories in the Git repository, reverting any changes made to generated files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/README.md#2025-04-22_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n./git_reset_generated_dirs.sh\n```\n\n----------------------------------------\n\nTITLE: Profiling Convolution Backward Pass in PyTorch\nDESCRIPTION: Log of backward pass operations for convolutional layers showing gradients flowing through the network. Each entry includes input gradients, output gradients, weights, and configuration parameters for the operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 1280, 8, 8], f16), T([64, 1536, 8, 8], f16), T([1280, 1536, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([64, 1536, 8, 8], f16), T([64, 512, 8, 8], f16), T([1536, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([64, 512, 8, 8], f16), T([64, 1536, 8, 8], f16), T([512, 1536, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Visualizing PyTorch CI Stats Workflow with Mermaid\nDESCRIPTION: A diagram illustrating the flow of CI statistics from jobs through intermediate storage to the final database. Shows how jobs with AWS credentials can write directly to S3, while jobs without credentials use GitHub artifacts, with both paths converging at the upload-test-stats workflow.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/stats/README.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    J1[Job with AWS creds<br>e.g. linux, win] --raw stats--> S3[(AWS S3)]\n    J2[Job w/o AWS creds<br>e.g. mac] --raw stats--> GHA[(GH artifacts)]\n\n    S3 --> uts[upload-test-stats.yml]\n    GHA --> uts\n\n    uts --json--> s3[(s3)]\n    s3 --> DB[(database)]\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler and Linker Options for Non-MSVC Environment\nDESCRIPTION: This CMake script snippet configures various C++ compiler flags to improve the build process, manage warnings, and select the correct linker for non-MSVC environments. It checks the compiler type (e.g., Clang or GCC) to apply suitable flags, manages ARM neon checks for aarch64, and deals with code coverage settings. Dependencies include the presence of certain compilers (GCC, Clang, MSVC) and platforms (Windows, Apple, aarch64). Inputs rely on the compiler identifiers while outputs modify the compiler and linker flags accordingly.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_44\n\nLANGUAGE: CMake\nCODE:\n```\n# third_party/FBGEMM\ninclude(cmake/public/utils.cmake)\nif(NOT MSVC)\n  string(APPEND CMAKE_CXX_FLAGS \" -O2 -fPIC\")\n  # Eigen fails to build with some versions, so convert this to a warning\n  # Details at http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1459\n  string(APPEND CMAKE_CXX_FLAGS \" -Wall\")\n  string(APPEND CMAKE_CXX_FLAGS \" -Wextra\")\n  append_cxx_flag_if_supported(\"-Werror=return-type\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=non-virtual-dtor\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=braced-scalar-init\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=range-loop-construct\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=bool-operation\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wnarrowing\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-missing-field-initializers\"\n                               CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-unknown-pragmas\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-unused-parameter\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-strict-overflow\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-strict-aliasing\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-stringop-overflow\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wvla-extension\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wsuggest-override\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wnewline-eof\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Winconsistent-missing-override\"\n                               CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Winconsistent-missing-destructor-override\"\n                               CMAKE_CXX_FLAGS)\n  if(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n    string(APPEND CMAKE_CXX_FLAGS \" -Wno-pass-failed\")\n  endif()\n  if(CMAKE_COMPILER_IS_GNUCXX)\n    # Suppress \"The ABI for passing parameters with 64-byte alignment has\n    # changed in GCC 4.6\"\n    string(APPEND CMAKE_CXX_FLAGS \" -Wno-psabi\")\n  endif()\n\n  # Use ld.gold if available, fall back to ld.bfd (the default ld) if not\n  if(USE_GOLD_LINKER)\n    if(USE_DISTRIBUTED AND USE_MPI)\n      # Same issue as here with default MPI on Ubuntu\n      # https://bugs.launchpad.net/ubuntu/+source/deal.ii/+bug/1841577\n      message(WARNING \"Refusing to use gold when USE_MPI=1\")\n    else()\n      execute_process(\n        COMMAND \"${CMAKE_C_COMPILER}\" -fuse-ld=gold -Wl,--version\n        ERROR_QUIET\n        OUTPUT_VARIABLE LD_VERSION)\n      if(NOT \"${LD_VERSION}\" MATCHES \"GNU gold\")\n        message(\n          WARNING\n            \"USE_GOLD_LINKER was set but ld.gold isn't available, turning it off\"\n        )\n        set(USE_GOLD_LINKER OFF)\n      else()\n        message(STATUS \"ld.gold is available, using it to link\")\n        set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fuse-ld=gold\")\n        set(CMAKE_SHARED_LINKER_FLAGS\n            \"${CMAKE_SHARED_LINKER_FLAGS} -fuse-ld=gold\")\n        set(CMAKE_MODULE_LINKER_FLAGS\n            \"${CMAKE_MODULE_LINKER_FLAGS} -fuse-ld=gold\")\n      endif()\n    endif()\n  endif()\n\n  append_cxx_flag_if_supported(\"-Wno-error=old-style-cast\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wconstant-conversion\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Wno-aligned-allocation-unavailable\"\n                               CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Qunused-arguments\" CMAKE_CXX_FLAGS)\n\n  if(${USE_COLORIZE_OUTPUT})\n    # Why compiler checks are necessary even when `try_compile` is used Because\n    # of the bug in ccache that can incorrectly identify `-fcolor-diagnostics`\n    # As supported by GCC, see https://github.com/ccache/ccache/issues/740 (for\n    # older ccache) and https://github.com/ccache/ccache/issues/1275 (for newer\n    # ones)\n    if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n      append_cxx_flag_if_supported(\"-fdiagnostics-color=always\" CMAKE_CXX_FLAGS)\n    else()\n      append_cxx_flag_if_supported(\"-fcolor-diagnostics\" CMAKE_CXX_FLAGS)\n    endif()\n  endif()\n\n  append_cxx_flag_if_supported(\"-faligned-new\" CMAKE_CXX_FLAGS)\n\n  if(WERROR)\n    append_cxx_flag_if_supported(\"-Werror\" CMAKE_CXX_FLAGS)\n    if(NOT COMPILER_SUPPORT_WERROR)\n      set(WERROR FALSE)\n    endif()\n  endif()\n  append_cxx_flag_if_supported(\"-Wno-maybe-uninitialized\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-fstandalone-debug\" CMAKE_CXX_FLAGS_DEBUG)\n  if(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\" AND CMAKE_CXX_COMPILER_ID MATCHES \"GNU\")\n    if(CMAKE_BUILD_TYPE MATCHES Debug)\n      message(Warning \"Applying -Og optimization for aarch64 GCC debug build to workaround ICE\")\n    endif()\n    string(APPEND CMAKE_CXX_FLAGS_DEBUG \" -fno-omit-frame-pointer -Og\")\n    string(APPEND CMAKE_LINKER_FLAGS_DEBUG \" -fno-omit-frame-pointer -Og\")\n  else()\n    string(APPEND CMAKE_CXX_FLAGS_DEBUG \" -fno-omit-frame-pointer -O0\")\n    string(APPEND CMAKE_LINKER_FLAGS_DEBUG \" -fno-omit-frame-pointer -O0\")\n  endif()\n  append_cxx_flag_if_supported(\"-fno-math-errno\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-fno-trapping-math\" CMAKE_CXX_FLAGS)\n  append_cxx_flag_if_supported(\"-Werror=format\" CMAKE_CXX_FLAGS)\n  if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 13)\n    append_cxx_flag_if_supported(\"-Wno-dangling-reference\" CMAKE_CXX_FLAGS)\n    append_cxx_flag_if_supported(\"-Wno-error=dangling-reference\" CMAKE_CXX_FLAGS)\n  endif()\nelse()\n  # Define export functions for AOTI.\n  add_compile_definitions(EXPORT_AOTI_FUNCTIONS)\n\n  # skip unwanted includes from windows.h\n  add_compile_definitions(WIN32_LEAN_AND_MEAN)\n  # Windows SDK broke compatibility since version 25131, but introduced this\n  # define for backward compatibility.\n  add_compile_definitions(_UCRT_LEGACY_INFINITY)\n  # disable min/max macros\n  add_compile_definitions(NOMINMAX)\n  # Turn off these warnings on Windows. destructor was implicitly defined as\n  # delete\n  append_cxx_flag_if_supported(\"/wd4624\" CMAKE_CXX_FLAGS)\n  # unknown pragma\n  append_cxx_flag_if_supported(\"/wd4068\" CMAKE_CXX_FLAGS)\n  # unexpected tokens following preprocessor directive - expected a newline\n  append_cxx_flag_if_supported(\"/wd4067\" CMAKE_CXX_FLAGS)\n  # conversion from 'size_t' to 'unsigned int', possible loss of data\n  append_cxx_flag_if_supported(\"/wd4267\" CMAKE_CXX_FLAGS)\n  # no suitable definition provided for explicit template instantiation request\n  append_cxx_flag_if_supported(\"/wd4661\" CMAKE_CXX_FLAGS)\n  # recursive on all control paths, function will cause runtime stack overflow\n  append_cxx_flag_if_supported(\"/wd4717\" CMAKE_CXX_FLAGS)\n  # conversion from '_Ty' to '_Ty', possible loss of data\n  append_cxx_flag_if_supported(\"/wd4244\" CMAKE_CXX_FLAGS)\n  # unsafe use of type 'bool' in operation\n  append_cxx_flag_if_supported(\"/wd4804\" CMAKE_CXX_FLAGS)\n  # inconsistent dll linkage\n  append_cxx_flag_if_supported(\"/wd4273\" CMAKE_CXX_FLAGS)\nendif()\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\")\n  include(CheckCSourceCompiles)\n  check_c_source_compiles(\n    \"#include <arm_neon.h>\nint main() {\n  float a[] = {1.0, 1.0};\n  float32x4x2_t v;\n  v.val[0] = vcombine_f32 (vcreate_f32 (0UL), vcreate_f32 (0UL));\n  v.val[1] = vcombine_f32 (vcreate_f32 (0UL), vcreate_f32 (0UL));\n  vst1q_f32_x2(a, v);\n  return 0;\n}\"\n    HAS_VST1)\n\n  if(NOT HAS_VST1)\n    string(APPEND CMAKE_CXX_FLAGS \" -DMISSING_ARM_VST1\")\n  endif()\nendif()\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\")\n  include(CheckCSourceCompiles)\n  check_c_source_compiles(\n    \"#include <arm_neon.h>\nint main() {\n  float a[] = {1.0, 1.0};\n  vld1q_f32_x2(a);\n  return 0;\n}\"\n    HAS_VLD1)\n\n  if(NOT HAS_VLD1)\n    string(APPEND CMAKE_CXX_FLAGS \" -DMISSING_ARM_VLD1\")\n  endif()\nendif()\n\n# Add code coverage flags to supported compilers\nif(USE_CPP_CODE_COVERAGE)\n  if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    string(APPEND CMAKE_C_FLAGS \" --coverage -fprofile-abs-path\")\n    string(APPEND CMAKE_CXX_FLAGS \" --coverage -fprofile-abs-path\")\n  elseif(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n    string(APPEND CMAKE_C_FLAGS \" -fprofile-instr-generate -fcoverage-mapping\")\n    string(APPEND CMAKE_CXX_FLAGS\n           \" -fprofile-instr-generate -fcoverage-mapping\")\n  else()\n    message(\n      ERROR\n      \"Code coverage for compiler ${CMAKE_CXX_COMPILER_ID} is unsupported\")\n  endif()\n\nendif()\n\nif(APPLE)\n  if(USE_MPS)\n    string(APPEND CMAKE_OBJCXX_FLAGS \" -DUSE_MPS -fno-objc-arc\")\n    string(APPEND CMAKE_CXX_FLAGS \" -DUSE_MPS\")\n    string(\n      APPEND\n      CMAKE_SHARED_LINKER_FLAGS\n      \" -weak_framework Foundation -weak_framework MetalPerformanceShaders -weak_framework MetalPerformanceShadersGraph -weak_framework Metal\"\n    )\n    # To suppress MPSGraph availability warnings\n    append_cxx_flag_if_supported(\"-Wno-unguarded-availability-new\"\n                                 CMAKE_OBJCXX_FLAGS)\n  endif()\n  append_cxx_flag_if_supported(\"-Wno-missing-braces\" CMAKE_CXX_FLAGS)\nendif()\n\nif(USE_XPU)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_XPU\")\nendif()\n\nif(EMSCRIPTEN)\n  string(\n    APPEND\n    CMAKE_CXX_FLAGS\n    \" -Wno-implicit-function-declaration -DEMSCRIPTEN -s DISABLE_EXCEPTION_CATCHING=0\"\n  )\nendif()\n\nappend_cxx_flag_if_supported(\"-Wno-stringop-overflow\" CMAKE_CXX_FLAGS)\n\nif(ANDROID AND (NOT ANDROID_DEBUG_SYMBOLS))\n  if(CMAKE_COMPILER_IS_GNUCXX)\n    string(APPEND CMAKE_CXX_FLAGS \" -s\")\n  elseif(\"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n    string(APPEND CMAKE_CXX_FLAGS \" -g0\")\n  else()\n    string(APPEND CMAKE_EXE_LINKER_FLAGS \" -s\")\n  endif()\nendif()\n\nif(NOT APPLE AND UNIX)\n  list(APPEND Caffe2_DEPENDENCY_LIBS dl)\nendif()\n\n# Prefix path to Caffe2 headers. If a directory containing installed Caffe2\n# headers was inadvertently added to the list of include directories, prefixing\n# PROJECT_SOURCE_DIR means this source tree always takes precedence.\ninclude_directories(BEFORE ${PROJECT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Configuring NNAPI Backend in CMake\nDESCRIPTION: This snippet adds the Android NNAPI delegate library, linking it with the appropriate dependencies. It's skipped for MacOS due to build issues.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT ${CMAKE_SYSTEM_NAME} MATCHES \"Darwin\")\n  add_library(nnapi_backend SHARED\n          ${TORCH_SRC_DIR}/csrc/jit/backends/nnapi/nnapi_backend_lib.cpp\n          ${TORCH_SRC_DIR}/csrc/jit/backends/nnapi/nnapi_backend_preprocess.cpp\n          )\n  if(BUILD_LIBTORCHLESS)\n    target_link_libraries(nnapi_backend PRIVATE ${TORCH_LIB} torch_python pybind::pybind11)\n  else()\n    target_link_libraries(nnapi_backend PRIVATE torch torch_python pybind::pybind11)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining the Documentation Structure with Sphinx Toctree - reStructuredText\nDESCRIPTION: This snippet uses the Sphinx `toctree` directive in reStructuredText to organize navigation within the generated PyTorch documentation. The `:glob:` option allows for wildcard inclusion of files, and `:maxdepth:` sets the depth of the navigation tree. The snippet includes pointers to sub-section files such as 'pytorch-api' and 'notes'. It requires Sphinx with support for the reStructuredText dialect.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/index.md#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n```{toctree}\n:glob:\n:maxdepth: 2\n\npytorch-api\n```\n```\n\nLANGUAGE: reStructuredText\nCODE:\n```\n```{toctree}\n:glob:\n:maxdepth: 2\n\nnotes\n```\n```\n\n----------------------------------------\n\nTITLE: Defining and Linking torchbind_test Shared Library in CMake\nDESCRIPTION: Creates a shared library named `torchbind_test` from the specified C++ source and header files. This library defines custom classes and operators intended for use within PyTorch Python tests via `torch.ops.load_library()`. It links against the main `torch` library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# Build separate libraries the define custom classes/operators used from our Python tests.\n# These are intended to be used with torch.ops.load_library() in our Python test suite.\nadd_library(torchbind_test SHARED\n  ${JIT_TEST_ROOT}/test_custom_class_registrations.h\n  ${JIT_TEST_ROOT}/test_custom_class_registrations.cpp\n)\ntarget_link_libraries(torchbind_test torch)\n```\n\n----------------------------------------\n\nTITLE: Log Entries for ATen Upsample Bilinear Backward Operator\nDESCRIPTION: These log lines detail invocations related to the `aten.upsample_bilinear2d_backward.vec` operator in PyTorch. Each entry shows the call count (`cnt: 2`) and arguments. Arguments typically include the gradient output tensor (`grad_output`, represented by `T(...)` with shape and data type `f16`), an optional target output size (e.g., `[6, 64, 176, 176]`), alignment flags (`False`), and scale factors (`[scale_h, scale_w]`). The line `Operator: aten.upsample_bilinear2d_backward.vec` explicitly identifies the operator for subsequent calls.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Super_SloMo_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\ncnt: 2, ((T([6, 64, 176, 176], f16), None, False, [2.0, 2.0]), {})\nOperator: aten.upsample_bilinear2d_backward.vec\ncnt: 2, ((T([6, 64, 352, 352], f16), None, [6, 64, 176, 176], False, [2.0, 2.0]), {})\ncnt: 2, ((T([6, 128, 176, 176], f16), None, [6, 128, 88, 88], False, [2.0, 2.0]), {})\ncnt: 2, ((T([6, 256, 88, 88], f16), None, [6, 256, 44, 44], False, [2.0, 2.0]), {})\ncnt: 2, ((T([6, 512, 44, 44], f16), None, [6, 512, 22, 22], False, [2.0, 2.0]), {})\ncnt: 2, ((T([6, 512, 22, 22], f16), None, [6, 512, 11, 11], False, [2.0, 2.0]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Tensor Operations\nDESCRIPTION: This code snippet represents a collection of PyTorch tensor operations with their occurrence counts and input parameters. It includes operations like rsub, select_backward, slice_backward, sum, and tril, along with their tensor shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_Longformer_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 12, ((T([2, 1024, 12, 513], f16), [12607488]), {})\ncnt: 12, ((T([24, 3, 512, 64], f16, stride=(98304, 32768, 1, 512)), [1572864]), {})\ncnt: 12, ((T([24, 3, 512, 64], f16), [1572864]), {})\nOperator: aten.rsub.Scalar\ncnt: 1, ((T([2, 1, 1, 1024], f16), 1.0), {})\nOperator: aten.select_backward.default\ncnt: 12, ((T([24, 512, 513], f16), [24, 3, 512, 513], 1, 0), {})\ncnt: 12, ((T([24, 512, 513], f16), [24, 3, 512, 513], 1, -1), {})\nOperator: aten.slice_backward.default\ncnt: 12, ((T([24, 4, 256, 768], f16), [24, 4, 256, 769], 3, 0, -1, 1), {})\ncnt: 12, ((T([24, 4, 256, 769], f16), [24, 4, 256, 769], 2, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 4, 256, 769], f16), [24, 4, 256, 769], 1, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 4, 256, 769], f16), [24, 4, 256, 769], 0, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 4, 196864], f16), [24, 4, 197120], 2, 0, -256, 1), {})\ncnt: 12, ((T([24, 4, 197120], f16), [24, 4, 197120], 1, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 4, 197120], f16), [24, 4, 197120], 0, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 255, 255], f16), [24, 255, 513], 2, -255, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 255, 513], f16), [24, 512, 513], 1, 0, 255, 1), {})\ncnt: 48, ((T([24, 3, 512, 513], f16), [24, 3, 512, 513], 0, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 3, 256, 256], f16), [24, 3, 256, 513], 3, 257, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 3, 256, 513], f16), [24, 3, 512, 513], 2, -257, -1, 1), {})\ncnt: 24, ((T([24, 3, 512, 513], f16), [24, 3, 512, 513], 1, 0, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 256, 257], f16), [24, 256, 513], 2, 0, 257, 1), {})\ncnt: 12, ((T([24, 256, 513], f16), [24, 512, 513], 1, 256, 9223372036854775807, 1), {})\ncnt: 12, ((T([24, 3, 256, 257], f16), [24, 3, 256, 513], 3, 0, 257, 1), {})\ncnt: 12, ((T([24, 3, 256, 513], f16), [24, 3, 512, 513], 2, 0, 256, 1), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([2048, 50265], f16, stride=(0, 0)), [0], True), {})\ncnt: 13, ((T([2048, 768], f16), [0], True), {})\ncnt: 12, ((T([2048, 3072], f16), [0], True), {})\ncnt: 12, ((T([2, 1024, 768], f16), [0, 1], True), {})\ncnt: 36, ((T([1024, 2, 768], f16), [0, 1], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([2, 1024, 50265], f16),), {})\nOperator: aten.tril.default\ncnt: 24, ((T([256, 257], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch HardSigmoid Activation Operations\nDESCRIPTION: HardSigmoid activation function operations on tensors of various shapes, primarily used in squeeze-excitation blocks or similar attention mechanisms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 72, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Threshold Backward Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of threshold backward operations on tensors with various shapes. It's likely part of the backward pass in a neural network, possibly related to ReLU or similar activation functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/shufflenet_v2_x1_0_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16), 0), {})\ncnt: 5, ((T([128, 232, 7, 7], f16, stride=(22736, 49, 7, 1)), T([128, 232, 7, 7], f16), 0), {})\ncnt: 3, ((T([128, 232, 7, 7], f16), T([128, 232, 7, 7], f16), 0), {})\ncnt: 1, ((T([128, 232, 14, 14], f16), T([128, 232, 14, 14], f16), 0), {})\ncnt: 9, ((T([128, 116, 14, 14], f16, stride=(45472, 196, 14, 1)), T([128, 116, 14, 14], f16), 0), {})\ncnt: 7, ((T([128, 116, 14, 14], f16), T([128, 116, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 116, 28, 28], f16), T([128, 116, 28, 28], f16), 0), {})\ncnt: 5, ((T([128, 58, 28, 28], f16, stride=(90944, 784, 28, 1)), T([128, 58, 28, 28], f16), 0), {})\ncnt: 3, ((T([128, 58, 28, 28], f16), T([128, 58, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 58, 56, 56], f16), T([128, 58, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 24, 112, 112], f16), T([128, 24, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.sum.SymInt with Tensor Arguments (Text)\nDESCRIPTION: This section logs calls to the `aten.sum.SymInt` operator, which computes the sum of tensor elements over specified dimensions. The examples show summing float16 (f16) tensors of various shapes (e.g., [128, 1000], [128, 1536, 6, 6]) along different dimensions (e.g., [0], [2, 3]). The `True` argument likely corresponds to `keepdim=True`. Note the explicit stride information `stride=(0, 0)` in one example, indicating a potentially non-contiguous tensor or a specific layout.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_21\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sum.SymInt\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16, stride=(0, 0)), [0], True), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([128, 1536, 6, 6], f16), [2, 3], True), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 6, ((T([128, 1536, 12, 12], f16), [2, 3], True), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 2, ((T([128, 512, 24, 24], f16), [2, 3], True), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([128, 256, 48, 48], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Division Operations in PyTorch\nDESCRIPTION: Log of scalar division operations on tensors with various shapes. These are likely normalizing operations, dividing by batch size or feature count.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([64, 1280, 8, 8], f16, stride=(1280, 1, 0, 0)), 64), {})\ncnt: 2, ((T([64, 256, 16, 16], f16, stride=(256, 1, 0, 0)), 256), {})\ncnt: 2, ((T([64, 128, 32, 32], f16, stride=(128, 1, 0, 0)), 1024), {})\ncnt: 2, ((T([64, 64, 64, 64], f16, stride=(64, 1, 0, 0)), 4096), {})\n```\n\n----------------------------------------\n\nTITLE: BLAS and LAPACK Operations List in RestructuredText\nDESCRIPTION: A list of linear algebra operations in PyTorch implemented using BLAS and LAPACK libraries, including matrix multiplication, decomposition, and solving operations, formatted as a RestructuredText autosummary.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.rst#2025-04-22_snippet_4\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    addbmm\n    addmm\n    addmv\n    addr\n    baddbmm\n    bmm\n    chain_matmul\n    cholesky\n    cholesky_inverse\n    cholesky_solve\n    dot\n    geqrf\n    ger\n    inner\n    inverse\n    det\n    logdet\n    slogdet\n    lu\n    lu_solve\n    lu_unpack\n    matmul\n    matrix_power\n    matrix_exp\n    mm\n    mv\n    orgqr\n    ormqr\n    outer\n    pinverse\n    qr\n    svd\n    svd_lowrank\n    pca_lowrank\n    lobpcg\n    trapz\n    trapezoid\n    cumulative_trapezoid\n    triangular_solve\n    vdot\n```\n\n----------------------------------------\n\nTITLE: Promoting LibTorch Archives\nDESCRIPTION: Script to promote LibTorch archives using environment variables to specify package type and name. Requires AWS access and proper git tag checkout.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPACKAGE_TYPE=libtorch PACKAGE_NAME=libtorch promote/s3_to_s3.sh\n```\n\n----------------------------------------\n\nTITLE: Tensor Addition Operations in PyTorch MobileNetV3\nDESCRIPTION: Summary of tensor addition operations throughout the model. These are likely used for skip connections in the model architecture, allowing feature maps to be combined at different stages of the network. The operations handle tensors of various shapes at different network depths.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 65, ((T([], i64), 1), {})\ncnt: 2, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16)), {})\ncnt: 4, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})\ncnt: 6, ((T([128, 32, 28, 28], f16), T([128, 32, 28, 28], f16)), {})\ncnt: 6, ((T([128, 64, 14, 14], f16), T([128, 64, 14, 14], f16)), {})\ncnt: 6, ((T([128, 112, 14, 14], f16), T([128, 112, 14, 14], f16)), {})\ncnt: 6, ((T([128, 184, 7, 7], f16), T([128, 184, 7, 7], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Instruction Count Benchmark\nDESCRIPTION: Commands to navigate to the instruction_counts directory and execute the main benchmark script from the PyTorch root directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# From pytorch root\ncd benchmarks/instruction_counts\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Appending GCC Standard Libraries to Link Line in CMake\nDESCRIPTION: Checks if the C++ compiler is GCC (CMAKE_COMPILER_IS_GNUCXX) and the target system is not Android. If both conditions are met, it appends the standard GCC libraries 'gcc_s' and 'gcc' to the Caffe2_DEPENDENCY_LIBS list, ensuring they are included during the linking stage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_43\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ Set link flag, handle additional deps for gcc 4.8 and above\nif(CMAKE_COMPILER_IS_GNUCXX AND NOT ANDROID)\n  message(\n    STATUS\n      \"GCC ${CMAKE_CXX_COMPILER_VERSION}: Adding gcc and gcc_s libs to link line\"\n  )\n  list(APPEND Caffe2_DEPENDENCY_LIBS gcc_s gcc)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations Log - PyTorch\nDESCRIPTION: Log of tensor operations showing input/output shapes, data types, and execution counts. Operations include convolutions, batch normalization, activation functions and matrix multiplications using float16 (f16) precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v3_large_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([32, 80, 14, 14], f16), T([32, 184, 14, 14], f16), T([80, 184, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([32, 184, 14, 14], f16), T([32, 184, 14, 14], f16), T([184, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 184, [True, True, False]), {})\ncnt: 2, ((T([32, 184, 14, 14], f16), T([32, 80, 14, 14], f16), T([184, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.threshold_backward.default Calls - PyTorch - Python\nDESCRIPTION: Analyzes backward threshold operations (aten.threshold_backward.default) for various tensor shapes, strides, and value arguments. Requires base and gradient tensors with threshold value, most often used in backpropagation for activation layers like ReLU. Addresses both default and strided tensors as inputs; outputs are gradient tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 1024, 4, 4], f16), T([128, 1024, 4, 4], f16), 0), {})\ncnt: 1, ((T([128, 1280, 4, 4], f16), T([128, 1280, 4, 4], f16), 0), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16), 0), {})\ncnt: 1, ((T([128, 960, 7, 7], f16), T([128, 960, 7, 7], f16), 0), {})\ncnt: 1, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 152, 14, 14], f16, stride=(178752, 196, 14, 1)), T([128, 152, 14, 14], f16), 0), {})\ncnt: 7, ((T([128, 304, 14, 14], f16), T([128, 304, 14, 14], f16), 0), {})\ncnt: 2, ((T([128, 152, 14, 14], f16), T([128, 152, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 152, 14, 14], f16, stride=(119168, 196, 14, 1)), T([128, 152, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 288, 28, 28], f16), T([128, 288, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 72, 28, 28], f16, stride=(338688, 784, 28, 1)), T([128, 72, 28, 28], f16), 0), {})\ncnt: 7, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), 0), {})\ncnt: 2, ((T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 72, 28, 28], f16, stride=(225792, 784, 28, 1)), T([128, 72, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 128, 56, 56], f16), T([128, 128, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 32, 56, 56], f16, stride=(602112, 3136, 56, 1)), T([128, 32, 56, 56], f16), 0), {})\ncnt: 7, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), 0), {})\ncnt: 2, ((T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 32, 56, 56], f16, stride=(401408, 3136, 56, 1)), T([128, 32, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Macros for C10 HIP\nDESCRIPTION: Sets up the C10_HIP_BUILD_SHARED_LIBS variable and generates the hip_cmake_macros.h file from a template.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/hip/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(C10_HIP_BUILD_SHARED_LIBS ${BUILD_SHARED_LIBS}) # used in cmake_macros.h.in\nconfigure_file(\n    ${CMAKE_CURRENT_LIST_DIR}/impl/hip_cmake_macros.h.in\n    ${CMAKE_BINARY_DIR}/c10/hip/impl/hip_cmake_macros.h)\n```\n\n----------------------------------------\n\nTITLE: Tensor Concatenation Operations in PyTorch\nDESCRIPTION: This snippet shows tensor concatenation operations (cat) used to combine tensors along specified dimensions. These operations are used to merge feature maps or detection results from different network stages.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.cat.default\ncnt: 4, (([T([269952, 4], f16), T([67488, 4], f16), T([16872, 4], f16), T([4218, 4], f16), T([1083, 4], f16)],), {})\ncnt: 1, (([T([4, 269952, 1], f16), T([4, 67488, 1], f16), T([4, 16872, 1], f16), T([4, 4218, 1], f16), T([4, 1083, 1], f16)], 1), {})\ncnt: 1, (([T([4, 269952, 4], f16), T([4, 67488, 4], f16), T([4, 16872, 4], f16), T([4, 4218, 4], f16), T([4, 1083, 4], f16)], 1), {})\ncnt: 1, (([T([359613, 4], f16), T([359613, 4], f16), T([359613, 4], f16), T([359613, 4], f16)],), {})\ncnt: 1, (([T([269952], i64), T([67488], i64), T([16872], i64), T([4218], i64), T([1083], i64)],), {})\ncnt: 1, (([T([4, 1000], i64), T([4, 1000], i64), T([4, 1000], i64), T([4, 1000], i64), T([4, 1000], i64)], 1), {})\ncnt: 3, (([T([0, 4], f16), T([0, 4], f16), T([0, 4], f16), T([0, 4], f16)],), {})\ncnt: 2, (([T([0, 1], f16), T([0, 1], f16), T([0, 1], f16), T([0, 1], f16)],), {})\ncnt: 2, (([T([0, 1], f16), T([0, 4], f16)], 1), {})\ncnt: 2, (([T([0], f32), T([0], f32), T([0], f32), T([0], f32)],), {})\ncnt: 1, (([T([0], i64), T([0], i64), T([0], i64), T([0], i64)],), {})\ncnt: 1, (([T([0, 91], f16), T([0, 91], f16), T([0, 91], f16), T([0, 91], f16)],), {})\ncnt: 1, (([T([0, 364], f16), T([0, 364], f16), T([0, 364], f16), T([0, 364], f16)],), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Role in reStructuredText for PyTorch Documentation\nDESCRIPTION: Creates a custom role named 'hidden' with the CSS class 'hidden-section'. This is used to hide certain sections in the rendered documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/classtemplate.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.sigmoid_backward.default Operator (Text)\nDESCRIPTION: Logs calls to the backward pass of the sigmoid function (`aten.sigmoid_backward.default`). Arguments include the gradient w.r.t. the output and the output of the original sigmoid function, both having the same shape (e.g., [128, 1536, 1, 1] f16).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.sigmoid_backward.default\ncnt: 9, ((T([128, 1536, 1, 1], f16), T([128, 1536, 1, 1], f16)), {})\ncnt: 2, ((T([128, 512, 1, 1], f16), T([128, 512, 1, 1], f16)), {})\ncnt: 1, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Describing PyTorch Operator Arguments and Tensor Shapes - Python\nDESCRIPTION: This snippet lists structured tuples detailing tensor shapes and primitives required for various PyTorch operator invocations, such as convolutions, pooling, normalization, and arithmetic. Inputs are PyTorch tensor annotations (with dimensions, type, strides) plus operator-specific parameter lists. The outputs are expected to match operand signatures for PyTorch operators, supporting benchmarking, profiling, or shape-based validation utilities. Dependencies: PyTorch (aten namespace), understanding of tensor layout/stride options and operator arity. Limitations: Not standalone executable; requires higher-level orchestration or test harness for functional execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 512, 8, 8], f16), T([128, 2048, 8, 8], f16), T([512, 2048, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 2048, 8, 8], f16), T([128, 1024, 16, 16], f16), T([2048, 1024, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 640, 16, 16], f16), T([128, 512, 16, 16], f16), T([640, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 512, 16, 16], f16), T([128, 1024, 16, 16], f16), T([512, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 1024, 16, 16], f16), T([128, 256, 16, 16], f16), T([1024, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 384, 16, 16], f16), T([128, 256, 16, 16], f16), T([384, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), T([128, 1024, 16, 16], f16), T([256, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1024, 16, 16], f16), T([128, 512, 32, 32], f16), T([1024, 512, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 1, 256], f16), T([128, 1, 256], f16), T([1, 1, 5], f16), [0], [1], [2], [1], False, [0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), T([128, 256, 32, 32], f16), T([256, 16, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 16, [True, True, False]), {})\ncnt: 1, ((T([128, 256, 32, 32], f16), T([128, 512, 32, 32], f16), T([256, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 512, 32, 32], f16), T([128, 128, 32, 32], f16), T([512, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 1, 128], f16), T([128, 1, 128], f16), T([1, 1, 5], f16), [0], [1], [2], [1], False, [0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 32, 32], f16), T([128, 128, 32, 32], f16), T([128, 16, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 8, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 32, 32], f16), T([128, 512, 32, 32], f16), T([128, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 512, 32, 32], f16), T([128, 256, 64, 64], f16), T([512, 256, 1, 1], f16), [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 32, 32], f16), T([128, 128, 64, 64], f16), T([128, 16, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 8, [True, True, False]), {})\ncnt: 1, ((T([128, 128, 64, 64], f16), T([128, 256, 64, 64], f16), T([128, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 3, ((T([128, 256, 64, 64], f16), T([128, 64, 64, 64], f16), T([256, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 1, 64], f16), T([128, 1, 64], f16), T([1, 1, 3], f16), [0], [1], [1], [1], False, [0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), T([64, 16, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 4, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 64, 64], f16), T([128, 256, 64, 64], f16), T([64, 256, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 64, 64], f16), T([128, 64, 64, 64], f16), T([64, 64, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 64, 128, 128], f16), T([128, 32, 128, 128], f16), T([64, 32, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 24, 128, 128], f16), T([32, 24, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 24, 128, 128], f16), T([128, 3, 256, 256], f16), T([24, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing QNNPACK Project CMake Configuration\nDESCRIPTION: Sets up basic CMake project configuration including minimum version, project name, and build options for QNNPACK library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5 FATAL_ERROR)\n\ninclude(GNUInstallDirs)\n\nproject(PYTORCH_QNNPACK C CXX ASM)\n\nset(PYTORCH_QNNPACK_LIBRARY_TYPE \"default\" CACHE STRING \"Type of library (shared, static, or default) to build\")\nset_property(CACHE PYTORCH_QNNPACK_LIBRARY_TYPE PROPERTY STRINGS default static shared)\noption(PYTORCH_QNNPACK_BUILD_TESTS \"Build QNNPACK unit tests\" ON)\noption(PYTORCH_QNNPACK_BUILD_BENCHMARKS \"Build QNNPACK benchmarks\" ON)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor View Operations\nDESCRIPTION: Shows tensor reshaping operations using unsafe_view with various shape transformations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten._unsafe_view.default((T([32, 3136, 49], f16), [32, 1, 3136, 49]))\naten._unsafe_view.default((T([32, 3136, 64], f16), [32, 1, 3136, 64]))\naten._unsafe_view.default((T([32, 2, 784, 64], f16), [64, 784, 64]))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Creation Operations in PyTorch\nDESCRIPTION: This snippet shows a tensor creation operation using the lift_fresh_copy function, which creates a new tensor with a specific shape and data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetv3_b_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Python Function 'needs_docs' Without Docstring\nDESCRIPTION: This snippet defines a Python function named 'needs_docs' without a docstring. It contains nested function and class definitions, indicating a complex structure that requires documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt.before.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef needs_docs(self):\n    def not_short():\n        class Long:\n```\n\n----------------------------------------\n\nTITLE: Analyzing Summation Operations in PyTorch\nDESCRIPTION: This snippet demonstrates summation operations on 2D tensors. It includes both symbolic integer summation and default summation, likely used for loss calculation or feature aggregation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/shufflenet_v2_x1_0_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16, stride=(0, 0)), [0], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([128, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Appending XNNPACK C++ Compiler Flag in CMake\nDESCRIPTION: If the USE_XNNPACK option is enabled, this snippet appends the preprocessor definition `-DUSE_XNNPACK` to the C++ compiler flags (CMAKE_CXX_FLAGS). This enables code related to the XNNPACK backend.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_39\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_XNNPACK)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_XNNPACK\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Conditional Expressions in TorchScript\nDESCRIPTION: Specifies the syntax for conditional (ternary) expressions in TorchScript. These allow for inline if-else logic within expressions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nconditional_expression ::=  or_expr ['if' or_test 'else' conditional_expression]\nexpression            ::=  conditional_expression\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Functions with Conditional Logic in Python\nDESCRIPTION: This code snippet defines a function 'top' that creates nested functions based on input numbers. It demonstrates complex nesting and conditional function definitions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/block_names.py.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef top(number):\n    if number == 0:\n\n        def fun():\n            if number == 10:\n                def sab():\n                    return 1\n            else:\n                def sub():\n                    return 2\n            return sub\n\n    elif number == 1:\n\n        def fun():\n            if number == 11:\n                def sub():\n                    return 3\n            else:\n                def sub():\n                    return 4\n            return sub\n\n    elif number == 2:\n\n        def fun():\n            if number == 12:\n                def sub():\n                    return 5\n            else:\n                def sab():\n                    return 6\n            return sub\n\n    elif number == 3:\n\n        def run():\n            if number == 12:\n                def sub():\n                    return 5\n            else:\n                def sub():\n                    return 6\n            return sub\n```\n\n----------------------------------------\n\nTITLE: Building and Benchmarking on ARM64 Android\nDESCRIPTION: This snippet provides instructions for compiling the QNNPACK library and running benchmarks on ARM64 Android devices. It covers the setup procedure, including necessary tools and file transfers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Clone PyTorch 1.0 repo\ngit clone --recursive https://github.com/pytorch/pytorch.git\ncd pytorch\n\n# Optional: update QNNPACK submodule to latest revision\ngit submodule update --remote third_party/QNNPACK\n\n# Build Caffe2 (including binaries) for Android, and push to device\nscripts/build_android.sh -DANDROID_ABI=arm64-v8a -DANDROID_TOOLCHAIN=clang -DBUILD_BINARY=ON\nadb push build_android/bin/speed_benchmark /data/local/tmp/speed_benchmark\n\n# Download model weights and copy them to Android device\nwget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/init_net.pb\nadb push init_net.pb /data/local/tmp/init_net.pb\n\n# Download model graph and copy it to Android device\nwget https://s3.amazonaws.com/download.caffe2.ai/models/mobilenet_v2_1.0_224_quant/predict_net.pb\nadb push predict_net.pb /data/local/tmp/predict_net.pb\n\n# Run speed benchmark with 50 warm-up iterations and 10 measurement iterations\nadb shell /data/local/tmp/speed_benchmark \\\n    --net /data/local/tmp/predict_net.pb \\\n    --init_net /data/local/tmp/init_net.pb \\\n    --input data --input_dims 1,3,224,224 --input_type float \\\n    --warmup 50 --iter 10\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch for Android\nDESCRIPTION: Basic command to build PyTorch libraries for Android platform from the PyTorch root directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n#in your PyTorch root directory\nbash scripts/build_android.sh\n```\n\n----------------------------------------\n\nTITLE: Computing Pooling Gradients with ATen AvgPool2DBackward Operator\nDESCRIPTION: The `aten.avg_pool2d_backward` calculates gradients for the average pooling operation, allowing backward pass through the pooling layer in a neural network. Involves input activations and gradient with respect to output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([64, 512, 8, 8], f16), T([64, 512, 16, 16], f16), [2, 2], [2, 2], [0, 0], False, True, None), {})\n```\n\n----------------------------------------\n\nTITLE: Copying Tensor Data with copy_ in PyTorch (Python)\nDESCRIPTION: Performs aten.copy_ which replicates the content from one tensor to another. It's commonly employed when data from a calculated tensor needs to overwrite the contents of a destination tensor, often used in custom model implementations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\naten.copy_.default\ncnt: 1, ((T([16, 512], i64), T([16, 512], i64)), {})\ncnt: 1, ((T([16], i64), T([16], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Installing MONAI Dependencies\nDESCRIPTION: Command to install the MONAI package required for the HighResNet model example.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo_memory_usage.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install monai\n```\n\n----------------------------------------\n\nTITLE: CPU-only Build of PyTorch on Windows - Development Mode - cmd\nDESCRIPTION: Executes a CPU-only build of PyTorch on Windows using the regular Python development install method. All dependencies (such as MKL and OpenMP) must be set up in the environment. Run from the command prompt with the active Python environment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_10\n\nLANGUAGE: cmd\nCODE:\n```\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Log\nDESCRIPTION: Log of PyTorch operator calls showing operation type, count of executions, and tensor shapes/parameters. Includes core operations like convolutions, pooling, normalizations used in ConvNeXt architecture.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/poolformer_m36_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 30, ((T([64, 96, 56, 56], f16), T([64, 96, 56, 56], f16)), {})\ncnt: 30, ((T([64, 192, 28, 28], f16), T([64, 192, 28, 28], f16)), {})\ncnt: 90, ((T([64, 384, 14, 14], f16), T([64, 384, 14, 14], f16)), {})\ncnt: 30, ((T([64, 768, 7, 7], f16), T([64, 768, 7, 7], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Listing PyTorch Models with Batch Sizes\nDESCRIPTION: This snippet provides a comprehensive list of PyTorch models and their corresponding batch sizes. The batch sizes are likely used for training or inference, and may represent optimal or maximum values for efficiency or memory constraints.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/torchbench_models_list.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nBERT_pytorch,128\nBackground_Matting, 16\nLearningToPaint,1024\nalexnet,1024\ndcgan,1024\ndensenet121,64\nhf_Albert,32\nhf_Bart,16\nhf_Bert,16\nhf_GPT2,16\nhf_T5,4\nmnasnet1_0,256\nmobilenet_v2,128\nmobilenet_v3_large,256\nnvidia_deeprecommender,1024\npytorch_unet,8\nresnet18,512\nresnet50,128\nresnext50_32x4d,128\nshufflenet_v2_x1_0,512\nsqueezenet1_1,512\ntimm_nfnet,256\ntimm_efficientnet,128\ntimm_regnet,128\ntimm_resnest,256\ntimm_vision_transformer,256\ntimm_vovnet,128\nvgg16,128\n```\n\n----------------------------------------\n\nTITLE: Adding C10 Subdirectories\nDESCRIPTION: Adds various subdirectories to the build process, including test and benchmark directories as well as optional components like CUDA, ROCm, and XPU based on build configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(test)\nadd_subdirectory(benchmark)\n\nif(USE_CUDA)\n  add_subdirectory(cuda)\nendif()\n\nif(USE_ROCM)\n  # NB: This directory is generated by the HIPIFY script; it's\n  # not checked in\n  add_subdirectory(hip)\nendif()\n\nif(USE_XPU)\n  add_subdirectory(xpu)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring C10 Library Macro Settings\nDESCRIPTION: Configures various compile-time macros and settings for the C10 library, generating a cmake_macros.h file with the appropriate definitions based on build options.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n  # ---[ Configure macro file.\n  set(C10_USE_GFLAGS ${USE_GFLAGS}) # used in cmake_macros.h.in\n  set(C10_USE_GLOG ${USE_GLOG}) # used in cmake_macros.h.in\n  set(C10_BUILD_SHARED_LIBS ${BUILD_SHARED_LIBS}) # used in cmake_macros.h.in\n  set(C10_USE_NUMA ${USE_NUMA})\n  set(C10_USE_MSVC_STATIC_RUNTIME ${CAFFE2_USE_MSVC_STATIC_RUNTIME})\n  set(C10_USE_ROCM_KERNEL_ASSERT ${USE_ROCM_KERNEL_ASSERT})\n  configure_file(\n      ${CMAKE_CURRENT_LIST_DIR}/macros/cmake_macros.h.in\n      ${CMAKE_BINARY_DIR}/c10/macros/cmake_macros.h)\n```\n\n----------------------------------------\n\nTITLE: Documenting PyTorch Compiler Config Module\nDESCRIPTION: ReStructuredText directives for documenting the torch.compiler.config module and its job_id configuration data\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler.config.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: torch.compiler.config\n\n.. autodata:: torch.compiler.config.job_id\n```\n\n----------------------------------------\n\nTITLE: PyTorch Nearest Neighbor Upsampling Backward Operations\nDESCRIPTION: Backward pass operations for nearest neighbor upsampling, showing gradient computation with input and output tensor specifications.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vision_maskrcnn_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.upsample_nearest2d_backward.vec\ncnt: 1, ((T([4, 256, 296, 304], f16), [296, 304], [4, 256, 148, 152], None), {})\ncnt: 1, ((T([4, 256, 148, 152], f16), [148, 152], [4, 256, 74, 76], None), {})\ncnt: 1, ((T([4, 256, 74, 76], f16), [74, 76], [4, 256, 37, 38], None), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Caffe2 Uninstall Target\nDESCRIPTION: Creates a custom uninstall target for Caffe2 if it doesn't already exist. Generates an uninstall script from a template and configures it for use.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_46\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT TARGET caffe2_uninstall)\n  configure_file(\n    ${CMAKE_CURRENT_SOURCE_DIR}/cmake/cmake_uninstall.cmake.in\n    ${CMAKE_CURRENT_BINARY_DIR}/cmake_uninstall.cmake IMMEDIATE @ONLY)\n\n  add_custom_target(\n    caffe2_uninstall COMMAND ${CMAKE_COMMAND} -P\n                             ${CMAKE_CURRENT_BINARY_DIR}/cmake_uninstall.cmake)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Compile Time Benchmark\nDESCRIPTION: These commands demonstrate how to run a newly created benchmark file within the PyTorch project structure. The benchmark is executed with different output files for comparison.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/pr_time_benchmarks/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd benchmarks/dynamo/pr_time_benchmarks\nPYTHONPATH=./ python benchmarks/[YOUR_BENCHMARK].py a.txt\nPYTHONPATH=./ python benchmarks/[YOUR_BENCHMARK].py a.txt\n```\n\n----------------------------------------\n\nTITLE: Disabling Vmap Fallback Warning in Functorch (Python)\nDESCRIPTION: This Python code snippet demonstrates how to disable the verbose vmap fallback warnings in PyTorch's functorch module. Calling `torch._C._functorch._set_vmap_fallback_warning_enabled(False)` suppresses these warnings, which can be helpful if they become excessive during development or debugging.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n`torch._C._functorch._set_vmap_fallback_warning_enabled(False)`\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building Custom Torch Operators - CMake - CMake\nDESCRIPTION: This snippet sets up a CMake-based build environment for compiling a shared library of custom PyTorch operators (op.cpp) and an associated executable (test_custom_ops.cpp). It conditionally includes GPU-specific dependencies, finds the required Torch package, sets compiler features (such as C++17 and range-based for), and links the respective targets against necessary Torch libraries. To use this setup, CMake version 3.15 or later and PyTorch C++ libraries must be installed. Input source files are defined as op.cpp and test_custom_ops.cpp, and output artifacts are the custom_ops shared library and the test_custom_ops executable. Limitations include a dependency on specific CMake and PyTorch versions, and this CMakeLists assumes the presence of relevant files in the working directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/custom_operator/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Basic CMake setup\ncmake_minimum_required(VERSION 3.15 FATAL_ERROR)\nproject(custom_ops)\n\nif(USE_ROCM)\ninclude(utils)\ninclude(LoadHIP)\nendif()\nfind_package(Torch REQUIRED)\n\nadd_library(custom_ops SHARED op.cpp)\nset_property(TARGET custom_ops PROPERTY CXX_STANDARD 17)\n\ntarget_compile_features(custom_ops PUBLIC cxx_range_for)\ntarget_link_libraries(custom_ops \"${TORCH_LIBRARIES}\")\ntarget_compile_definitions(custom_ops PRIVATE custom_ops_EXPORTS)\n\nadd_executable(test_custom_ops test_custom_ops.cpp)\nset_property(TARGET test_custom_ops PROPERTY CXX_STANDARD 17)\ntarget_link_libraries(test_custom_ops custom_ops)\n\n```\n\n----------------------------------------\n\nTITLE: Executing Tensor Copy Operation in PyTorch\nDESCRIPTION: Copies a tensor with varying strides and data types, adjusting the data representation from float32 to float16, int32, or int64, potentially optimizing for CUDA execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"Operator: aten._to_copy.default\\ncnt: 1, ((T([128, 128], f32),), {'dtype': f16})\\ncnt: 1, ((T([2, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\\ncnt: 1, ((T([2, 128], b8),), {'dtype': i32})\\ncnt: 1, ((T([2, 128], i64),), {'dtype': i32, 'layout': torch.strided, 'device': 'cuda'})\\ncnt: 1, ((T([2, 128], i32),), {'dtype': i64})\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Checking Unfused Operations in Python\nDESCRIPTION: This command enables detailed logging to show which operations are not fused and why, using the partition.cpp GRAPH_UPDATE feature.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nPYTORCH_JIT_LOG_LEVEL=\">partition:graph_fuser\" python your_script.py &> log\n```\n\n----------------------------------------\n\nTITLE: Configuring Distributed Training Features in CMake\nDESCRIPTION: This snippet sets compile definitions for various distributed training features like UCC, NCCL, MPI, and Gloo. It also configures RPC and TensorPipe usage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_LIBTORCHLESS)\n  if(USE_UCC)\n    target_link_libraries(torch_python PRIVATE __caffe2_ucc)\n    target_compile_definitions(torch_python PRIVATE USE_UCC)\n  endif()\n\n  if(USE_UCC AND USE_C10D_UCC)\n    target_compile_definitions(torch_python PRIVATE USE_C10D_UCC)\n  endif()\n\n  if(USE_NCCL AND USE_C10D_NCCL)\n    target_compile_definitions(torch_python PRIVATE USE_C10D_NCCL)\n  endif()\n\n  if(USE_DISTRIBUTED)\n    target_compile_definitions(torch_python PRIVATE USE_DISTRIBUTED)\n  endif()\n\n  if(USE_MPI AND USE_C10D_MPI)\n    target_compile_definitions(torch_python PRIVATE USE_C10D_MPI)\n  endif()\n\n  if(USE_GLOO AND USE_C10D_GLOO)\n    target_compile_definitions(torch_python PRIVATE USE_C10D_GLOO)\n  endif()\n\n  if(NOT WIN32)\n    target_compile_definitions(torch_python PRIVATE USE_RPC)\n  endif()\n\n  if(USE_TENSORPIPE)\n    target_compile_definitions(torch_python PRIVATE USE_TENSORPIPE)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tensor Division Operations\nDESCRIPTION: Division operations between tensors and scalar values, including both element-wise divisions and scalar divisions with different data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n((T([5000], f16), 2), {})\n((T([5000], f32), 2.0), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Addition Operations in PyTorch\nDESCRIPTION: Statistics for the aten.add.Tensor operator showing usage count and tensor shapes. The operator performs element-wise addition between tensors of identical shapes, primarily using half-precision (f16) tensors with varying dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 3, ((T([128, 256, 48, 48], f16), T([128, 256, 48, 48], f16)), {})\ncnt: 6, ((T([128, 512, 24, 24], f16), T([128, 512, 24, 24], f16)), {})\ncnt: 18, ((T([128, 1536, 12, 12], f16), T([128, 1536, 12, 12], f16)), {})\ncnt: 8, ((T([128, 1536, 6, 6], f16), T([128, 1536, 6, 6], f16)), {})\ncnt: 1, ((T([128, 128, 48, 48], f16), T([128, 128, 48, 48], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Device Selection (CPU/CUDA) using Argparse in PyTorch\nDESCRIPTION: This Python snippet uses the `argparse` module to handle command-line arguments, specifically allowing a user to disable CUDA usage via `--disable-cuda`. It checks for CUDA availability using `torch.cuda.is_available()` and the flag to set an `args.device` attribute to the appropriate `torch.device` object ('cuda' or 'cpu') for subsequent device-agnostic tensor and model placement. It depends on the `torch` and `argparse` libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport argparse\nimport torch\n\nparser = argparse.ArgumentParser(description='PyTorch Example')\nparser.add_argument('--disable-cuda', action='store_true',\n                    help='Disable CUDA')\nargs = parser.parse_args()\nargs.device = None\nif not args.disable_cuda and torch.cuda.is_available():\n    args.device = torch.device('cuda')\nelse:\n    args.device = torch.device('cpu')\n```\n\n----------------------------------------\n\nTITLE: Configuring ATen Activation and Backward Operator Scenarios - Python\nDESCRIPTION: This set configures testing of activation functions (Hardsigmoid, Hardswish) and their backward counterparts via tensor shape presets. The tuples specify varying tensor sizes designed for neural net feature maps, covering typical batched activation inputs and targets for both in-place and out-of-place operations. Prerequisites include correct tensor shape and data type support in PyTorch, with test coverage extending to both forward and gradient-backward passes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.hardsigmoid.default\ncnt: 1, ((T([128, 72, 1, 1], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 120, 1, 1], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 480, 1, 1], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 672, 1, 1], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 960, 1, 1], f16),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.hardsigmoid_backward.default\ncnt: 2, ((T([128, 960, 1, 1], f16), T([128, 960, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Gathering C10 Source and Header Files\nDESCRIPTION: Uses glob patterns to collect all C++ source and header files for the C10 library from various subdirectories. This defines the core code that will be compiled into the library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n  # Note: if you want to add ANY dependency to the c10 library, make sure you\n  # check with the core PyTorch developers as the dependency will be\n  # transitively passed on to all libraries dependent on PyTorch.\n  file(GLOB C10_SRCS\n          *.cpp\n          core/*.cpp\n          core/impl/*.cpp\n          mobile/*.cpp\n          macros/*.cpp\n          util/*.cpp\n        )\n  file(GLOB C10_HEADERS\n          *.h\n          core/*.h\n          core/impl/*.h\n          mobile/*.h\n          macros/*.h\n          util/*.h\n        )\n```\n\n----------------------------------------\n\nTITLE: Documenting Deprecation Redirect for torch.utils.model_zoo - reStructuredText\nDESCRIPTION: This snippet presents a reStructuredText (reST) section header announcing that \"torch.utils.model_zoo\" has been moved to \"torch.hub\". It uses Sphinx autodoc directives to insert both the module and its function documentation (load_url) if available. The file requires Sphinx to be configured and is intended for integration with PyTorch's automatic documentation pipeline. There is no executable code or parameter input; the main purpose is providing clear documentation context for users and maintainers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/model_zoo.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\ntorch.utils.model_zoo\n===================================\n\nMoved to `torch.hub`.\n\n.. automodule:: torch.utils.model_zoo\n.. autofunction:: load_url\n```\n\n----------------------------------------\n\nTITLE: Including Build Variables and Defining Allowlist in CMake\nDESCRIPTION: Includes a separate CMake file (`cmake/BuildVariables.cmake`) likely containing definitions for various build-related variables. It also defines a cached string variable `CAFFE2_ALLOWLIST`, intended to hold a path to a file specifying which files should be built.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_23\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ Build variables set within the cmake tree\ninclude(cmake/BuildVariables.cmake)\nset(CAFFE2_ALLOWLIST\n    \"\"\n    CACHE STRING \"A allowlist file of files that one should build.\")\n```\n\n----------------------------------------\n\nTITLE: Unsupported Tensor Indexing for Reads in PyTorch ONNX Export\nDESCRIPTION: Example of unsupported tensor indexing pattern for reading operations that includes negative values in tensor indices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_torchscript.rst#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndata[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Patterns\nDESCRIPTION: This code snippet represents a detailed analysis of PyTorch operator usage in a neural network implementation. It includes information about tensor shapes, data types, and operation counts for various layers and functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ese_vovnet19b_dw_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 512, 1, 1], f16), T([128, 512, 1, 1], f16)), {})\ncnt: 1, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16)), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([128, 256, 56, 56], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})\ncnt: 1, ((T([128, 512, 28, 28], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})\ncnt: 1, ((T([128, 768, 14, 14], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([128, 768, 7, 7], f16), T([128, 768, 14, 14], f16), [3, 3], [2, 2], [0, 0], [1, 1], True, T([128, 768, 7, 7], i64)), {})\ncnt: 1, ((T([128, 512, 14, 14], f16), T([128, 512, 28, 28], f16), [3, 3], [2, 2], [0, 0], [1, 1], True, T([128, 512, 14, 14], i64)), {})\ncnt: 1, ((T([128, 256, 28, 28], f16), T([128, 256, 56, 56], f16), [3, 3], [2, 2], [0, 0], [1, 1], True, T([128, 256, 28, 28], i64)), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 256, 56, 56], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 512, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 768, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 1024], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1024], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 2, ((T([128, 256, 56, 56], f16), T([128, 256, 1, 1], f16)), {})\ncnt: 2, ((T([128, 512, 28, 28], f16), T([128, 512, 1, 1], f16)), {})\ncnt: 2, ((T([128, 768, 14, 14], f16), T([128, 768, 1, 1], f16)), {})\ncnt: 2, ((T([128, 1024, 7, 7], f16), T([128, 1024, 1, 1], f16)), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16)), {})\ncnt: 1, ((T([128, 768, 14, 14], f16), T([128, 768, 14, 14], f16)), {})\ncnt: 1, ((T([128, 512, 28, 28], f16), T([128, 512, 28, 28], f16)), {})\ncnt: 1, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 160, 28, 28], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 768, 14, 14], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 224, 7, 7], f16), T([224], f16), T([224], f16), T([224], f16), T([224], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 224, 7, 7], f16), T([128, 224, 7, 7], f16), T([224], f16), T([224], f16), T([224], f16), T([224], f32), T([224], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 768, 14, 14], f16), T([128, 768, 14, 14], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f32), T([768], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 192, 14, 14], f16), T([128, 192, 14, 14], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 512, 28, 28], f16), T([128, 512, 28, 28], f16), T([512], f16), T([512], f16), T([512], f16), T([512], f32), T([512], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 160, 28, 28], f16), T([128, 160, 28, 28], f16), T([160], f16), T([160], f16), T([160], f16), T([160], f32), T([160], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16), T([256], f16), T([256], f16), T([256], f16), T([256], f32), T([256], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 128, 56, 56], f16), T([128, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), True, 1e-05, [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\nOperator: aten.relu_.default\ncnt: 2, ((T([128, 64, 112, 112], f16),), {})\ncnt: 1, ((T([128, 64, 56, 56], f16),), {})\ncnt: 4, ((T([128, 128, 56, 56], f16),), {})\ncnt: 1, ((T([128, 256, 56, 56], f16),), {})\ncnt: 4, ((T([128, 160, 28, 28], f16),), {})\ncnt: 1, ((T([128, 512, 28, 28], f16),), {})\ncnt: 4, ((T([128, 192, 14, 14], f16),), {})\ncnt: 1, ((T([128, 768, 14, 14], f16),), {})\ncnt: 4, ((T([128, 224, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16),), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 768, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 512, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 256, 56, 56], f16), [2, 3], True), {})\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([128, 1024, 7, 7], f16), 0), {})\ncnt: 1, ((T([128, 224, 7, 7], f16, stride=(70560, 49, 7, 1)), T([128, 224, 7, 7], f16), 0), {})\ncnt: 3, ((T([128, 224, 7, 7], f16), T([128, 224, 7, 7], f16), 0), {})\ncnt: 1, ((T([128, 768, 14, 14], f16), T([128, 768, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 192, 14, 14], f16, stride=(213248, 196, 14, 1)), T([128, 192, 14, 14], f16), 0), {})\ncnt: 3, ((T([128, 192, 14, 14], f16), T([128, 192, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 512, 28, 28], f16), T([128, 512, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 160, 28, 28], f16, stride=(577024, 784, 28, 1)), T([128, 160, 28, 28], f16), 0), {})\ncnt: 3, ((T([128, 160, 28, 28], f16), T([128, 160, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 256, 56, 56], f16), T([128, 256, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 128, 56, 56], f16, stride=(1404928, 3136, 56, 1)), T([128, 128, 56, 56], f16), 0), {})\ncnt: 3, ((T([128, 128, 56, 56], f16), T([128, 128, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 56, 56], f16), 0), {})\ncnt: 2, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Static Dispatch Backend Option for PyTorch\nDESCRIPTION: Sets up a CMake option for specifying the backend for which static dispatch code is generated (e.g., CPU). Default is empty string.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\noption(\n  STATIC_DISPATCH_BACKEND\n  \"Name of the backend for which static dispatch code is generated, e.g.: CPU.\"\n  \"\")\n```\n\n----------------------------------------\n\nTITLE: Profiling ATen Operator Usage with PyTorch in Python\nDESCRIPTION: This snippet documents the collection of ATen operator invocations with detailed input and output tensor shapes, data types (e.g., f16, f32, i64), as well as the count of occurrences (cnt) for each operator scenario. It serves to catalog and analyze the runtime characteristics and shapes encountered in modern PyTorch workloads, useful for backend optimization, compiler tracing, or model auditing. No dependencies beyond PyTorch and standard Python are required; all tensor examples and operator invocations use canonical PyTorch types and notations. Inputs represent different combinations of shapes and types, outputs are the cataloged operator info, and limitations include reliance on static analysis and representative execution paths.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XLNetLMHeadModel_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([2048, 32000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([2048, 32000], f16), T([2048, 32000], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 24, ((T([4, 16, 512, 512], f16), 3, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 24, ((T([4, 16, 512, 512], f16), T([4, 16, 512, 512], f16), 3, f16), {})\nOperator: aten._to_copy.default\ncnt: 1, ((T([1024, 4, 1024], f32, stride=(1024, 0, 1)),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 24, ((T([1024, 4, 1024], f32),), {'dtype': f16, 'device': 'cuda'})\nOperator: aten._unsafe_view.default\ncnt: 24, ((T([512, 4, 64, 16, 1], f16), [1, 2048, 1024]), {})\ncnt: 24, ((T([64, 16, 1024, 1, 1], f16), [1, 1024, 1024]), {})\ncnt: 24, ((T([4, 16, 512, 1, 64], f16), [64, 512, 64]), {})\ncnt: 24, ((T([1024, 4, 1, 16, 64], f16), [1, 4096, 1024]), {})\ncnt: 72, ((T([512, 4, 1, 16, 64], f16), [1, 2048, 1024]), {})\nOperator: aten.add.Tensor\ncnt: 48, ((T([512, 4, 16, 64], f16), T([16, 64], f16)), {})\ncnt: 24, ((T([4, 16, 512, 512], f16), T([4, 16, 512, 512], f16)), {})\ncnt: 24, ((T([4, 16, 512, 512], f16), 0), {})\ncnt: 144, ((T([512, 4, 1024], f16), T([512, 4, 1024], f16)), {})\ncnt: 24, ((T([512, 4, 16, 64], f16, stride=(64, 524288, 32768, 1)), T([512, 4, 16, 64], f16, stride=(64, 524288, 32768, 1))), {})\ncnt: 1, ((T([32000, 1024], f16), T([32000, 1024], f16)), {})\nOperator: aten.addmm.default\ncnt: 24, ((T([4096], f16), T([2048, 1024], f16), T([1024, 4096], f16, stride=(1, 1024))), {})\ncnt: 24, ((T([1024], f16), T([2048, 4096], f16), T([4096, 1024], f16, stride=(1, 4096))), {})\ncnt: 1, ((T([32000], f16), T([2048, 1024], f16), T([1024, 32000], f16, stride=(1, 1024))), {})\nOperator: aten.bmm.default\ncnt: 96, ((T([1, 2048, 1024], f16), T([1, 1024, 1024], f16)), {})\ncnt: 24, ((T([1, 4096, 1024], f16), T([1, 1024, 1024], f16)), {})\ncnt: 24, ((T([64, 512, 64], f16, stride=(64, 4096, 1)), T([64, 64, 512], f16, stride=(64, 1, 4096))), {})\ncnt: 24, ((T([64, 512, 64], f16, stride=(64, 4096, 1)), T([64, 64, 1024], f16, stride=(64, 1, 4096))), {})\ncnt: 48, ((T([64, 512, 512], f16), T([64, 512, 64], f16, stride=(64, 4096, 1))), {})\ncnt: 96, ((T([1, 1024, 2048], f16, stride=(2097152, 1, 1024)), T([1, 2048, 1024], f16)), {})\ncnt: 96, ((T([1, 2048, 1024], f16), T([1, 1024, 1024], f16, stride=(1048576, 1, 1024))), {})\ncnt: 24, ((T([64, 512, 512], f16, stride=(262144, 1, 512)), T([64, 512, 64], f16)), {})\ncnt: 24, ((T([64, 512, 64], f16), T([64, 64, 512], f16, stride=(64, 1, 4096))), {})\ncnt: 24, ((T([64, 64, 512], f16, stride=(64, 1, 4096)), T([64, 512, 1024], f16)), {})\ncnt: 24, ((T([64, 512, 1024], f16), T([64, 1024, 64], f16, stride=(64, 4096, 1))), {})\ncnt: 24, ((T([64, 64, 512], f16, stride=(64, 1, 4096)), T([64, 512, 512], f16)), {})\ncnt: 24, ((T([1, 1024, 4096], f16, stride=(4194304, 1, 1024)), T([1, 4096, 1024], f16)), {})\nOperator: aten.cat.default\ncnt: 1, (([T([1024, 512], f32), T([1024, 512], f32)], -1), {})\nOperator: aten.clone.default\ncnt: 2, ((T([4, 512], i64),), {})\nOperator: aten.copy_.default\ncnt: 2, ((T([4, 512], i64), T([4, 512], i64)), {})\ncnt: 24, ((T([1024, 16, 64], f16), T([1024, 16, 64], f16, stride=(1, 1024, 16384))), {})\nOperator: aten.cos.default\ncnt: 1, ((T([1024, 512], f32),), {})\nOperator: aten.div.Tensor\ncnt: 1, ((T([512], f32), 1024), {})\nOperator: aten.embedding.default\ncnt: 1, ((T([32000, 1024], f16), T([512, 4], i64)), {})\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([512, 4, 1024], f16), T([512, 4], i64), 32000, -1, False), {})\nOperator: aten.gelu.default\ncnt: 24, ((T([512, 4, 4096], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 24, ((T([512, 4, 4096], f16), T([512, 4, 4096], f16)), {})\nOperator: aten.index_add.default\ncnt: 24, ((T([4, 16, 512, 1023], f16), 3, T([512], i64), T([4, 16, 512, 512], f16)), {})\nOperator: aten.index_select.default\ncnt: 24, ((T([4, 16, 512, 1023], f16, stride=(8388608, 524288, 1023, 1)), 3, T([512], i64)), {})\nOperator: aten.mm.default\ncnt: 1, ((T([2048, 32000], f16), T([32000, 1024], f16)), {})\ncnt: 1, ((T([32000, 2048], f16, stride=(1, 32000)), T([2048, 1024], f16)), {})\ncnt: 24, ((T([2048, 1024], f16), T([1024, 4096], f16)), {})\ncnt: 24, ((T([1024, 2048], f16, stride=(1, 1024)), T([2048, 4096], f16)), {})\ncnt: 24, ((T([2048, 4096], f16), T([4096, 1024], f16)), {})\ncnt: 24, ((T([4096, 2048], f16, stride=(1, 4096)), T([2048, 1024], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 1, ((T([512], f32), 1), {})\ncnt: 1, ((T([1024, 1], f32), T([1, 512], f32)), {})\ncnt: 48, ((T([4, 16, 512, 512], f16), 0.125), {})\nOperator: aten.native_layer_norm.default\ncnt: 48, ((T([512, 4, 1024], f16), [1024], T([1024], f16), T([1024], f16), 1e-12), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 1, ((T([512, 4, 1024], f16, stride=(1024, 524288, 1)), T([512, 4, 1024], f16), [1024], T([512, 4, 1], f32), T([512, 4, 1], f32), T([1024], f16), T([1024], f16), [True, True, True]), {})\ncnt: 47, ((T([512, 4, 1024], f16), T([512, 4, 1024], f16), [1024], T([512, 4, 1], f32), T([512, 4, 1], f32), T([1024], f16), T([1024], f16), [True, True, True]), {})\nOperator: aten.new_empty_strided.default\ncnt: 24, ((T([1024, 16, 64], f16, stride=(1, 1024, 16384)), [1024, 16, 64], [1024, 64, 1]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten.new_zeros.default\ncnt: 24, ((T([4, 16, 512, 512], f16), [4, 16, 512, 1023]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([2048, 32000], f16), T([2048], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([2048, 32000], f16), T([2048], i64), None, 1, -100), {})\nOperator: aten.pow.Scalar\ncnt: 1, ((10000, T([512], f32)), {})\nOperator: aten.reciprocal.default\ncnt: 1, ((T([512], f32),), {})\nOperator: aten.sin.default\ncnt: 1, ((T([1024, 512], f32),), {})\nOperator: aten.slice_backward.default\ncnt: 24, ((T([4, 16, 1023, 512], f16), [4, 16, 1023, 512], 3, 0, 9223372036854775807, 1), {})\ncnt: 24, ((T([4, 16, 1023, 512], f16), [4, 16, 1024, 512], 2, 1, 9223372036854775807, 1), {})\ncnt: 24, ((T([4, 16, 1024, 512], f16), [4, 16, 1024, 512], 1, 0, 9223372036854775807, 1), {})\ncnt: 24, ((T([4, 16, 1024, 512], f16), [4, 16, 1024, 512], 0, 0, 9223372036854775807, 1), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([2048, 32000], f16), [0], True), {})\ncnt: 24, ((T([2048, 1024], f16), [0], True), {})\ncnt: 24, ((T([2048, 4096], f16), [0], True), {})\ncnt: 48, ((T([512, 4, 16, 64], f16, stride=(64, 524288, 32768, 1)), [0, 1], True), {})\n\n```\n\n----------------------------------------\n\nTITLE: Globbing Source Files\nDESCRIPTION: Uses file(GLOB) commands to collect various source and header files for different components of the ATen library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE ATen_CORE_HEADERS  \"core/*.h\")\nfile(GLOB_RECURSE ATen_CORE_SRCS \"core/*.cpp\")\nfile(GLOB_RECURSE ATen_TRANSFORMER_HEADERS \"native/transformers/*.h\")\nif(NOT BUILD_LITE_INTERPRETER)\n  file(GLOB_RECURSE ATen_CORE_TEST_SRCS \"core/*_test.cpp\")\nendif()\nEXCLUDE(ATen_CORE_SRCS \"${ATen_CORE_SRCS}\" ${ATen_CORE_TEST_SRCS})\n\nfile(GLOB base_h \"*.h\" \"detail/*.h\" \"cpu/*.h\" \"cpu/vec/vec512/*.h\" \"cpu/vec/vec128/*.h\" \"cpu/vec/vec256/*.h\" \"cpu/vec/vec256/vsx/*.h\" \"cpu/vec/vec256/zarch/*.h\" \"cpu/vec/sve/*.h\" \"cpu/vec/*.h\" \"quantized/*.h\" \"functorch/*.h\")\nfile(GLOB base_cpp \"*.cpp\" \"detail/*.cpp\" \"cpu/*.cpp\" \"functorch/*.cpp\")\n# ... (more file globbing commands)\n```\n\n----------------------------------------\n\nTITLE: Conducting Log Softmax Backward Data Operation in PyTorch\nDESCRIPTION: Applies the backward phase of the log softmax on a tensor with the same shape and data type as the input. Utilizes gradient data of type f16 to update model parameters based on error propagation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/XGLMForCausalLM_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n\"\"\"Operator: aten._log_softmax_backward_data.default\\ncnt: 1, ((T([256, 256008], f16), T([256, 256008], f16), 1, f16), {})\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Disabling FALLBACK Path in NVFuser\nDESCRIPTION: This command disables the FALLBACK path in NVFuser, ensuring that errors are properly reported for debugging purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nPYTORCH_NVFUSER_DISABLE=fallback python your_script.py &> log\n```\n\n----------------------------------------\n\nTITLE: Configuring Include-What-You-Use for C10\nDESCRIPTION: Sets up the Include-What-You-Use (IWYU) tool for analyzing and optimizing header inclusions in the C10 library if the option is enabled. Provides specific configuration options for IWYU analysis.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n  option(C10_USE_IWYU \"Use include-what-you-use to clean up header inclusion\" OFF)\n  if(C10_USE_IWYU)\n    find_program(iwyu NAMES include-what-you-use)\n    if(iwyu)\n      set(iwyu_cmd\n          \"include-what-you-use\"\n          \"-Xiwyu\"\n          \"--transitive_includes_only\"\n          \"-Xiwyu\"\n          \"--no_fwd_decls\"\n          \"-Xiwyu\"\n          \"--prefix_header_includes=keep\"\n          \"-Xiwyu\"\n          \"--mapping_file=${CMAKE_CURRENT_LIST_DIR}/../tools/iwyu/all.imp\"\n        )\n      set_property(TARGET c10 PROPERTY CXX_INCLUDE_WHAT_YOU_USE ${iwyu_cmd})\n    endif()\n  endif()\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Addition Operations\nDESCRIPTION: Multiple tensor addition operations across various shapes and dimensions, primarily working with half-precision (fp16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cspdarknet53_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten.add.Tensor((T([], i64), 1), {})\naten.add.Tensor((T([64, 64, 128, 128], f16), T([64, 64, 128, 128], f16, stride=(2097152, 16384, 128, 1))), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Average Pooling Backward Operations in PyTorch\nDESCRIPTION: Records of aten.avg_pool2d_backward.default operator calls for gradient computation. These operations compute gradients for average pooling operations with various tensor shapes and pooling parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([32, 1024, 7, 7], f16), T([32, 1024, 14, 14], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\ncnt: 1, ((T([32, 512, 7, 7], f16), T([32, 512, 14, 14], f16), [3, 3], [2, 2], [1, 1], False, True, None), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), T([32, 512, 28, 28], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\ncnt: 1, ((T([32, 256, 14, 14], f16), T([32, 256, 28, 28], f16), [3, 3], [2, 2], [1, 1], False, True, None), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), T([32, 256, 56, 56], f16), [2, 2], [2, 2], [0, 0], True, False, None), {})\ncnt: 1, ((T([32, 128, 28, 28], f16), T([32, 128, 56, 56], f16), [3, 3], [2, 2], [1, 1], False, True, None), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Copy Operations in PyTorch\nDESCRIPTION: Records instances of the \\\"aten._to_copy.default\\\" operator, specifying how tensors are cast to different data types like float16 from float32 with potential layout and device considerations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([128, 128], f32),), {'dtype': f16})\ncnt: 1, ((T([8, 1, 128, 128], f16, stride=(0, 16384, 128, 1)),), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\n```\n\n----------------------------------------\n\nTITLE: Applying Native Batch Normalization with ATen\nDESCRIPTION: Applies batch normalization to input tensors, a critical step in stabilizing learning by normalizing activations. Requires input tensors, running mean, and variance tensors all shaped accordingly, with specific momentum and epsilon values. Outputs normalized tensors facilitating enhanced convergence and learning rates in training phases.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([1, 1024, 128, 128], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), False, 0.1, 1e-05), {})\ncnt: 2, ((T([1, 2048, 64, 64], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f16), False, 0.1, 1e-05), {})\ncnt: 13, ((T([1, 4096, 32, 32], f16), T([4096], f16), T([4096], f16), T([4096], f16), T([4096], f16), False, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Setting CCache as Compiler Launcher\nDESCRIPTION: This bash snippet configures the compiler launcher to use ccache for C, CXX, and CUDA compilers in order to improve build efficiency when developing PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\nexport CMAKE_C_COMPILER_LAUNCHER=ccache\nexport CMAKE_CXX_COMPILER_LAUNCHER=ccache\nexport CMAKE_CUDA_COMPILER_LAUNCHER=ccache\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Implementing Torch Library in C++\nDESCRIPTION: Macro for implementing a Torch library. Used in conjunction with TORCH_LIBRARY to define the actual implementation of custom operators and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/library.rst#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nTORCH_LIBRARY_IMPL\n```\n\n----------------------------------------\n\nTITLE: ReLU Activation Operations\nDESCRIPTION: In-place ReLU activation operations on tensors of various sizes, processing feature maps with batch size 64 and different spatial dimensions (112x112, 56x56, 28x28, 14x14).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 64, 112, 112], f16),), {})\n((T([64, 64, 56, 56], f16),), {})\n((T([64, 128, 56, 56], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Setting ATen CPU Source Files and Dependencies in CMake\nDESCRIPTION: Sets the `ATen_CPU_SRCS` variable to the list of all CPU C++ source files contained in `all_cpu_cpp`. It also appends the `ATEN_CPU_FILES_GEN_LIB` target (likely representing generated CPU files) to the `ATen_CPU_DEPENDENCY_LIBS` list.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\n# We have two libraries: libATen_cpu.so and libATen_cuda.so,\n# with libATen_cuda.so depending on libATen_cpu.so.  The CPU library\n# contains CPU code only.  libATen_cpu.so is invariant to the setting\n# of USE_CUDA (it always builds the same way); libATen_cuda.so is only\n# built when USE_CUDA=1 and CUDA is available.  (libATen_hip.so works\n# the same way as libATen_cuda.so)\nset(ATen_CPU_SRCS ${all_cpu_cpp})\nlist(APPEND ATen_CPU_DEPENDENCY_LIBS ATEN_CPU_FILES_GEN_LIB)\n```\n\n----------------------------------------\n\nTITLE: Displaying Performance Metrics Table in Markdown\nDESCRIPTION: A markdown table showing performance metrics for different experimental setups in a PyTorch project. It includes measurements for warmup latency, average latency, throughput, and GPU utilization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_64_false.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |\n| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |\n| original | 5.823 +/- 0.541 | 8.024 +/- 0.508 | 460.474 +/- 29.896 | 47.112 +/- 6.576 |\n| h2d_d2h_threads | 3.971 +/- 0.709 | 9.583 +/- 1.030 | 393.394 +/- 26.110 | 37.550 +/- 1.381 |\n| 2_predict_workers | 3.535 +/- 0.124 | 8.258 +/- 1.188 | 441.640 +/- 85.705 | 38.630 +/- 3.499 |\n| 3_predict_workers | 3.724 +/- 0.312 | 7.614 +/- 0.951 | 467.236 +/- 36.619 | 37.834 +/- 2.418 |\n| 4_predict_workers | 4.346 +/- 0.744 | 8.417 +/- 0.541 | 430.672 +/- 17.979 | 34.461 +/- 3.257 |\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Support for PyTorch Python\nDESCRIPTION: Sets up CUDA support for PyTorch Python bindings, including dependencies like cuDNN, cuSPARSELt and NVToolsExt when available.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDA)\n    include(${TORCH_ROOT}/cmake/public/cuda.cmake)\n    append_filelist(\"libtorch_python_cuda_core_sources\" TORCH_PYTHON_SRCS)\n    list(APPEND TORCH_PYTHON_SRCS ${GENERATED_THNN_CXX_CUDA})\n\n    list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_CUDA)\n    if(USE_CUDNN)\n        list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::cudnn)\n        list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_CUDNN)\n    endif()\n    if(USE_CUSPARSELT)\n        list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::cusparselt)\n        list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_CUSPARSELT)\n    endif()\n    if(USE_CUFILE)\n        list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::cufile)\n        list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_CUFILE)\n    endif()\n\n    if(TARGET torch::nvtx3)\n      list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::nvtx3)\n    else()\n      if(TARGET torch::nvtoolsext)\n        list(APPEND TORCH_PYTHON_LINK_LIBRARIES torch::nvtoolsext)\n      endif()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Simplified TensorOptions with Free Functions\nDESCRIPTION: Demonstrates the shorter syntax using free functions for tensor options.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::ones(10, torch::dtype(torch::kFloat32))\n```\n\n----------------------------------------\n\nTITLE: Implementing Local Timer Server in PyTorch Distributed Elastic\nDESCRIPTION: Class for implementing a local timer server using multiprocess.Queue. It is part of the server/client implementations provided by torchelastic.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nLocalTimerServer\n```\n\n----------------------------------------\n\nTITLE: Conditional Mobile Build Configuration in CMake for PyTorch\nDESCRIPTION: Adds specific source files for mobile builds and returns early from the CMake script. This section ensures that mobile builds only include necessary files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/perfkernels/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(INTERN_BUILD_MOBILE)\n  list(APPEND Caffe2_CPU_SRCS\n    \"${CMAKE_CURRENT_SOURCE_DIR}/embedding_lookup_idx.cc\"\n  )\n  set(Caffe2_CPU_SRCS ${Caffe2_CPU_SRCS} PARENT_SCOPE)\n  return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Initializing Vulkan Support Flag in CMake\nDESCRIPTION: Initializes the `USE_VULKAN` CMake variable to `OFF`. This variable controls whether Vulkan support and dependencies are included in the build.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(USE_VULKAN OFF)\n```\n\n----------------------------------------\n\nTITLE: Configuring Micro-Kernel Tests for QNNPACK in CMake\nDESCRIPTION: Sets up test executables for QNNPACK micro-kernels such as q8gemm, q8gemm-sparse, q8conv, q8dwconv, and various other low-level operations. Each test is configured with C++14 standard and appropriate library dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\n# ---[ Build unit tests for micro-kernels\nadd_executable(q8gemm-test test/q8gemm.cc)\nset_target_properties(q8gemm-test PROPERTIES\n  CXX_STANDARD 14\n  CXX_STANDARD_REQUIRED YES\n  CXX_EXTENSIONS NO)\ntarget_include_directories(q8gemm-test PRIVATE src test)\ntarget_link_libraries(q8gemm-test PRIVATE pytorch_qnnpack cpuinfo fp16 gtest gtest_main)\nadd_test(q8gemm-test q8gemm-test)\n```\n\n----------------------------------------\n\nTITLE: Exploring Package Contents Using Unzip Command\nDESCRIPTION: Shows how to use the unzip command to extract and inspect torch package contents from the command line. The example demonstrates extracting a package and viewing its directory structure.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/package.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ unzip my_package.pt && tree my_package\nmy_package\n .data\n    94304870911616.storage\n    94304900784016.storage\n    extern_modules\n    version\n models\n    model_1.pkl\n torchvision\n     models\n         resnet.py\n         utils.py\n~ cd my_package && cat torchvision/models/resnet.py\n...\n```\n\n----------------------------------------\n\nTITLE: Importing Process Context Classes for PyTorch Distributed Processing\nDESCRIPTION: This snippet imports various process context classes from the PyTorch distributed elastic multiprocessing API. These classes are used to manage and interact with distributed processes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/multiprocessing.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.distributed.elastic.multiprocessing.api import PContext, MultiprocessContext, SubprocessContext, RunProcsResult, DefaultLogsSpecs, LogsDest, LogsSpecs\n```\n\n----------------------------------------\n\nTITLE: Defining reStructuredText Section for PyTorch Compiler IRs\nDESCRIPTION: This snippet defines the main section for PyTorch Compiler IRs documentation using reStructuredText syntax. It includes a reference label and a title.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_ir.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _torch.compiler_ir:\n\nIRs\n===============\n```\n\n----------------------------------------\n\nTITLE: Executing aten.clone operation in PyTorch\nDESCRIPTION: Utilizes PyTorch Aten to clone tensors, preserving data type and shape. Input is the tensor to be cloned. Output is a new tensor identical to the input. Make sure to have PyTorch installed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([128, 3, 224, 224], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Expiration Timers in PyTorch Distributed Elastic\nDESCRIPTION: Function to configure the expiration timer settings. It is part of the client methods for interacting with the timer system.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/timer.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch.distributed.elastic.timer.configure\n```\n\n----------------------------------------\n\nTITLE: Working with Saved Tensors in PyTorch Autograd\nDESCRIPTION: This Python code snippet demonstrates how autograd saves intermediary tensor states during the forward pass using PyTorch's mechanism. It illustrates that the attribute '_saved_self' in 'grad_fn' refers to the original tensor, while in some cases, the saved tensor might be a different object but sharing the same storage. Requires 'torch' library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\nprint(x.equal(y.grad_fn._saved_self))  # True\nprint(x is y.grad_fn._saved_self)  # True\n```\n\n----------------------------------------\n\nTITLE: Building C10 HIP Library for Regular Builds\nDESCRIPTION: Creates and configures the c10_hip library with appropriate compile options, visibility settings, and dependencies when not in libtorchless mode.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/hip/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_LIBTORCHLESS)\n  hip_add_library(c10_hip ${C10_HIP_SRCS} ${C10_HIP_HEADERS})\n  torch_compile_options(c10_hip)\n\n  # Propagate HIP_CXX_FLAGS that were set from Dependencies.cmake\n  target_compile_options(c10_hip PRIVATE ${HIP_CXX_FLAGS})\n\n  # caffe2_hip adds a bunch of dependencies like rocsparse, but c10/hip is supposed to be\n  # minimal.  I'm not sure if we need hip_hcc or not; for now leave it out\n\n  # If building shared library, set dllimport/dllexport proper.\n  target_compile_options(c10_hip PRIVATE \"-DC10_HIP_BUILD_MAIN_LIB\")\n  # Enable hidden visibility if compiler supports it.\n  if(${COMPILER_SUPPORTS_HIDDEN_VISIBILITY})\n    target_compile_options(c10_hip PRIVATE \"-fvisibility=hidden\")\n  endif()\n\n  # ---[ Dependency of c10_hip\n  target_link_libraries(c10_hip PUBLIC ${C10_LIB} hip::amdhip64)\n\n  target_include_directories(\n      c10_hip PUBLIC\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/../..>\n      $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}>\n      $<INSTALL_INTERFACE:include>)\n  install(TARGETS c10_hip EXPORT Caffe2Targets DESTINATION lib)\n  set(C10_HIP_LIB c10_hip)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tracking ReLU Activation Calls in PyTorch\nDESCRIPTION: Records in-place ReLU activation function calls for tensors of different shapes throughout the network. These operations apply element-wise rectification to feature maps at various stages of the model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 16, 112, 112], f16),), {})\ncnt: 1, ((T([128, 64, 112, 112], f16),), {})\ncnt: 1, ((T([128, 64, 56, 56], f16),), {})\ncnt: 3, ((T([128, 72, 56, 56], f16),), {})\ncnt: 1, ((T([128, 72, 28, 28], f16),), {})\ncnt: 1, ((T([128, 24, 1, 1], f16),), {})\ncnt: 4, ((T([128, 120, 28, 28], f16),), {})\ncnt: 2, ((T([128, 32, 1, 1], f16),), {})\ncnt: 1, ((T([128, 120, 1, 1], f16),), {})\ncnt: 2, ((T([128, 168, 1, 1], f16),), {})\ncnt: 2, ((T([128, 240, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Building and Configuring C10 Library Target\nDESCRIPTION: Creates the c10 library target, sets compile options, version properties, and visibility settings. This defines how the library should be built and what properties it should have.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_LIBTORCHLESS)\n  add_library(c10 ${C10_SRCS} ${C10_HEADERS})\n  torch_compile_options(c10)\n  if(HAVE_SOVERSION)\n    set_target_properties(c10 PROPERTIES\n        VERSION ${TORCH_VERSION} SOVERSION ${TORCH_SOVERSION})\n  endif()\n  # If building shared library, set dllimport/dllexport proper.\n  target_compile_options(c10 PRIVATE \"-DC10_BUILD_MAIN_LIB\")\n  # Enable hidden visibility if compiler supports it.\n  if(${COMPILER_SUPPORTS_HIDDEN_VISIBILITY})\n    target_compile_options(c10 PRIVATE \"-fvisibility=hidden\")\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler Flags for Non-MSVC Compilers\nDESCRIPTION: Adds compiler flags to ignore certain warnings for non-MSVC compilers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT MSVC)\n  string(APPEND CMAKE_CXX_FLAGS \" -Wno-ignored-qualifiers\")\n  string(APPEND CMAKE_C_FLAGS \" -Wno-ignored-qualifiers\")\n  string(APPEND CMAKE_CXX_FLAGS \" -Wno-absolute-value\")\n  string(APPEND CMAKE_C_FLAGS \" -Wno-absolute-value\")\nendif(NOT MSVC)\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents for Environment Variables Documentation\nDESCRIPTION: ReStructuredText directives defining the documentation structure and table of contents for PyTorch environment variables documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch_environment_variables.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n\n   threading_environment_variables\n   cuda_environment_variables\n   mps_environment_variables\n   debugging_environment_variables\n   miscellaneous_environment_variables\n   logging\n   torch_nccl_environment_variables\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage in Neural Network Model\nDESCRIPTION: This code snippet provides a comprehensive overview of PyTorch operator usage in a neural network model. It includes details on tensor shapes, data types, and operation counts for various layers and functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 49, ((T([], i64), 1), {})\ncnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16)), {})\n# ... (truncated for brevity)\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 320, 7, 7], f16), T([1280, 320, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Matrix Multiplication\nDESCRIPTION: These snippets show matrix multiplication operations between tensors of different shapes, using float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1000], f16, stride=(0, 0)), T([1000, 3072], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([1000, 128], f16, stride=(0, 0)), T([128, 3072], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Undoing CMAKE Compatibility Mode for PyTorch QNNPACK\nDESCRIPTION: Handles CMAKE compatibility by unsetting the policy version minimum if using CMAKE version 4.0.0 or greater. This ensures the build process is compatible with different versions of CMAKE.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n# -- [ Undo cmake-4 compat mode\nif(CMAKE_VERSION VERSION_GREATER_EQUAL \"4.0.0\")\n  unset(CMAKE_POLICY_VERSION_MINIMUM)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Promoting Wheels to PyPI\nDESCRIPTION: Script to promote wheels to PyPI. This is a one-way operation that cannot be undone. Requires PyPI repository access.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npromote/wheel_to_pypi.sh\n```\n\n----------------------------------------\n\nTITLE: Basic TensorOptions Usage in PyTorch C++\nDESCRIPTION: Shows the basic and verbose way of specifying tensor options using TensorOptions class.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_creation.rst#2025-04-22_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::ones(10, torch::TensorOptions().dtype(torch::kFloat32))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Addition Operations in DenseNet PyTorch Implementation\nDESCRIPTION: This snippet shows the tensor shapes and counts for the aten.add.Tensor operation in DenseNet implementation. It records the dimensions, data types, and strides of tensors being added together at different layers of the network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 4, ((T([32, 224, 7, 7], f16, stride=(105056, 49, 7, 1)), T([32, 224, 7, 7], f16)), {})\ncnt: 1, ((T([32, 1024, 7, 7], f16, stride=(105056, 49, 7, 1)), T([32, 1024, 7, 7], f16)), {})\ncnt: 4, ((T([32, 224, 7, 7], f16, stride=(92512, 49, 7, 1)), T([32, 224, 7, 7], f16)), {})\ncnt: 1, ((T([32, 768, 7, 7], f16, stride=(92512, 49, 7, 1)), T([32, 768, 7, 7], f16)), {})\ncnt: 4, ((T([32, 192, 14, 14], f16, stride=(338688, 196, 14, 1)), T([32, 192, 14, 14], f16)), {})\ncnt: 1, ((T([32, 768, 14, 14], f16, stride=(338688, 196, 14, 1)), T([32, 768, 14, 14], f16)), {})\ncnt: 4, ((T([32, 192, 14, 14], f16, stride=(288512, 196, 14, 1)), T([32, 192, 14, 14], f16)), {})\ncnt: 1, ((T([32, 512, 14, 14], f16, stride=(288512, 196, 14, 1)), T([32, 512, 14, 14], f16)), {})\ncnt: 4, ((T([32, 160, 28, 28], f16, stride=(827904, 784, 28, 1)), T([32, 160, 28, 28], f16)), {})\ncnt: 1, ((T([32, 256, 28, 28], f16, stride=(827904, 784, 28, 1)), T([32, 256, 28, 28], f16)), {})\ncnt: 5, ((T([32, 128, 56, 56], f16, stride=(2408448, 3136, 56, 1)), T([32, 128, 56, 56], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Module Name Collision Example in PyTorch Package\nDESCRIPTION: Example showing how module name collisions can occur between local and package-loaded models without mangling, potentially causing issues with module lookups in sys.modules.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/package/mangling.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision.models import resnet18\nlocal_resnet18 = resnet18()\n\n# a loaded resnet18, potentially with a different implementation than the local one!\ni = torch.PackageImporter('my_resnet_18.pt')\nloaded_resnet18 = i.load_pickle('model', 'model.pkl')\n\nprint(type(local_resnet18).__module__)  # 'torchvision.models.resnet18'\nprint(type(loaded_resnet18).__module__)  # ALSO 'torchvision.models.resnet18'\n```\n\n----------------------------------------\n\nTITLE: Interpreting torch.save Serialized File Structure - PyTorch - text\nDESCRIPTION: Explains, in a code-block (not executable code), the directory structure and files contained within a torch.save-generated ZIP64 archive. No dependencies or code execution; purely referential. Important for understanding what data and metadata are stored, including object pickles, storages, versioning, and system byteorder. Useful for advanced users inspecting or manipulating checkpoints at the file level.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_8\n\nLANGUAGE: text\nCODE:\n```\ncheckpoint.pth\n data.pkl\n byteorder  # added in PyTorch 2.1.0\n data/\n    0\n    1\n    2\n    \n version\n```\n\n----------------------------------------\n\nTITLE: Element-wise Multiplication for Scaling and Residual Connections\nDESCRIPTION: Documents tensor multiplication operations used for scaling attention scores and element-wise operations in residual connections. The scaling factor 0.14433756729740643 is approximately 1/48, suggesting scaled dot-product attention.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 36, ((T([2, 16, 576, 48], f16, stride=(1327104, 48, 2304, 1)), 0.14433756729740643), {})\ncnt: 72, ((T([768], f16), T([2, 576, 768], f16)), {})\ncnt: 4, ((T([2, 16, 1, 48], f16), 0.14433756729740643), {})\ncnt: 4, ((T([768], f16), T([2, 1, 768], f16)), {})\ncnt: 1, ((T([2, 1, 768], f16, stride=(443136, 768, 1)), T([768], f16)), {})\ncnt: 1, ((T([2, 1, 768], f16, stride=(443136, 768, 1)), T([2, 1, 768], f16)), {})\ncnt: 3, ((T([2, 1, 768], f16), T([768], f16)), {})\ncnt: 3, ((T([2, 1, 768], f16), T([2, 1, 768], f16)), {})\ncnt: 72, ((T([2, 576, 768], f16), T([768], f16)), {})\ncnt: 72, ((T([2, 576, 768], f16), T([2, 576, 768], f16)), {})\ncnt: 36, ((T([2, 16, 576, 48], f16), 0.14433756729740643), {})\n```\n\n----------------------------------------\n\nTITLE: Incorrect Use of Global Variables with functorch\nDESCRIPTION: This example demonstrates the incorrect way of using global variables with functorch transformations, which will not work properly because functorch requires that all outputs be returned from the function rather than assigned to global variables.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom functorch import grad\n\n# Don't do this\nintermediate = None\n\ndef f(x):\n  global intermediate\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z\n\nx = torch.randn([])\ngrad_x = grad(f)(x)\n```\n\n----------------------------------------\n\nTITLE: Element-Wise Multiplication of Tensors in PyTorch\nDESCRIPTION: The mul.Tensor operator performs element-wise multiplication between two tensors. This operation is prominently used in scaling, gating mechanisms, or pointwise transformations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ncnt: 6, ((T([32, 256, 56, 56], f16), T([32, 256, 1, 1], f16)), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 16, ((T([32, 512, 28, 28], f16), T([32, 512, 1, 1], f16)), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 72, ((T([32, 1024, 14, 14], f16), T([32, 1024, 1, 1], f16)), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 6, ((T([32, 2048, 7, 7], f16), T([32, 2048, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA Tensor Slicing in PyTorch\nDESCRIPTION: This function implements tensor slicing for CUDA devices in PyTorch. It takes a tensor and slicing parameters to extract a subset of the tensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4_ops12slice_Tensor4callERKNS_6TensorElSt8optionalIN3c106SymIntEES8_S7_\n```\n\n----------------------------------------\n\nTITLE: Creating Gradient Computation Function with functorch.grad\nDESCRIPTION: Uses `functorch.grad` to create a new function `ft_compute_grad` from `compute_loss_stateless_model`. `grad` transforms the function so that when `ft_compute_grad` is called with the same arguments as `compute_loss_stateless_model`, it returns the gradient of the original function's output (the loss) with respect to its first argument (the `params`). This function computes the gradient for a single sample.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/per_sample_grads.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nft_compute_grad = grad(compute_loss_stateless_model)\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Unsafe View Operations in PyTorch\nDESCRIPTION: This snippet documents instances where the ATen unsafe view operator is utilized to reshape tensors without regard to memory layout security. Examples show transformations on tensors of various dimensional forms, indicating frequent tensor manipulation and reshaping in computational processes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 18, ((T([16, 12, 128, 64], f16), [192, 128, 64]), {})\ncnt: 6, ((T([16, 12, 64, 128], f16), [192, 64, 128]), {})\ncnt: 6, ((T([192, 128, 128], f16), [16, 12, 128, 128]), {})\ncnt: 6, ((T([192, 128, 64], f16), [16, 12, 128, 64]), {})\ncnt: 12, ((T([16, 128, 12, 64], f16), [16, 128, 768]), {})\ncnt: 6, ((T([16, 128, 768], f16), [2048, 768]), {})\n```\n\n----------------------------------------\n\nTITLE: Unsafe View Reshaping in PyTorch Python\nDESCRIPTION: In PyTorch, the 'aten._unsafe_view' operator is demonstrated for tensor reshaping without memory layout verification, useful for manipulating tensor dimensions efficiently. This operator requires exact input dimensions; incorrect size specifications lead to runtime errors. Primarily leveraged in scenarios prioritizing speed over safety checks, it outputs reshaped tensors based on new size parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 1, ((T([1, 80, 80, 810], f16), [1, 57600, 90]), {})\ncnt: 1, ((T([1, 40, 40, 810], f16), [1, 14400, 90]), {})\n```\n\n----------------------------------------\n\nTITLE: Including Allowlist Configuration in CMake\nDESCRIPTION: Includes the `cmake/Allowlist.cmake` file. This file likely contains logic to process the allowlist file specified by the CAFFE2_ALLOWLIST variable, potentially filtering the source files included in the build.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_42\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ Allowlist file if allowlist is specified\ninclude(cmake/Allowlist.cmake)\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations for Initial Feature Extraction\nDESCRIPTION: Shows the convolution operation used for initial feature extraction from input images. The operation processes 2 images of size 384384 with 3 channels using 768 filters of size 1616 and a stride of 16.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cait_m36_384_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([2, 3, 384, 384], f16),), {})\nOperator: aten.convolution.default\ncnt: 1, ((T([2, 3, 384, 384], f16), T([768, 3, 16, 16], f16), T([768], f16), [16, 16], [0, 0], [1, 1], False, [0, 0], 1), {})\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([2, 768, 24, 24], f16, stride=(442368, 1, 18432, 768)), T([2, 3, 384, 384], f16), T([768, 3, 16, 16], f16), [768], [16, 16], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([2, 3, 384, 384], f16), T([2, 3, 384, 384], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Empty Category Count Template\nDESCRIPTION: Empty code block for counting categories, appears to be a placeholder for future implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/explore.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Counts of categories\n\n```\n\n----------------------------------------\n\nTITLE: Collecting and organizing ATen quantized files in CMake\nDESCRIPTION: This snippet collects all header, source, and test files in the quantized directory, excludes test files from the regular source files, and passes these file lists to the parent scope. This allows the parent CMake file to incorporate these files in the build process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/quantized/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB_RECURSE ATen_QUANTIZED_HEADERS \"*.h\")\nfile(GLOB_RECURSE ATen_QUANTIZED_SRCS \"*.cpp\")\nfile(GLOB_RECURSE ATen_QUANTIZED_TEST_SRCS \"*_test.cpp\")\nEXCLUDE(ATen_QUANTIZED_SRCS \"${ATen_QUANTIZED_SRCS}\" ${ATen_QUANTIZED_TEST_SRCS})\n\n# Pass to parent\nset(ATen_QUANTIZED_HEADERS ${ATen_QUANTIZED_HEADERS} PARENT_SCOPE)\nset(ATen_QUANTIZED_SRCS ${ATen_QUANTIZED_SRCS} PARENT_SCOPE)\nset(ATen_QUANTIZED_TEST_SRCS ${ATen_QUANTIZED_TEST_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module for PyTorch Documentation\nDESCRIPTION: Sets the current module context for the documentation. The actual module name is inserted dynamically using Jinja2 templating.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/autosummary/class.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: {{ module }}\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.threshold_backward.default Operator (Text)\nDESCRIPTION: Logs calls to the backward pass of a threshold function (`aten.threshold_backward.default`), likely related to ReLU's backward pass. Arguments include the gradient w.r.t. the output, the original input tensor (both f16 with same shape, e.g., [128, 768, 1, 1]), and the threshold value (0).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 9, ((T([128, 768, 1, 1], f16), T([128, 768, 1, 1], f16), 0), {})\ncnt: 2, ((T([128, 256, 1, 1], f16), T([128, 256, 1, 1], f16), 0), {})\ncnt: 1, ((T([128, 128, 1, 1], f16), T([128, 128, 1, 1], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten._unsafe_view Operator for Tensor Reshaping - PyTorch - Python\nDESCRIPTION: These snippets log the usage of aten._unsafe_view, an internal PyTorch tensor view operation for reshaping tensors without copying data. Inputs are tensors of various shapes and the desired new shapes. Outputs are views sharing memory with the original if possible. Risks include unsafe reshaping if tensor strides/layouts are incompatible. Prerequisites: torch, understanding of tensor memory layout.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([4, 12, 1024, 64], f16), [48, 1024, 64]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([4, 12, 64, 1024], f16), [48, 64, 1024]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([48, 1024, 1024], f16), [4, 12, 1024, 1024]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([48, 1024, 64], f16), [4, 12, 1024, 64]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 1, ((T([4096, 2], f16), [4, 1024, 2]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 24, ((T([4, 1024, 12, 64], f16), [4, 1024, 768]), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring ROCm Support for PyTorch Python\nDESCRIPTION: Sets up AMD ROCm support for PyTorch Python bindings when the USE_ROCM option is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_ROCM)\n    append_filelist(\"libtorch_python_cuda_core_sources\" TORCH_PYTHON_SRCS)\n    list(APPEND TORCH_PYTHON_SRCS ${GENERATED_THNN_CXX_CUDA})\n\n    list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS\n      USE_ROCM\n      __HIP_PLATFORM_AMD__\n      )\n    if(NOT WIN32)\n      list(APPEND TORCH_PYTHON_LINK_LIBRARIES ${ROCM_ROCTX_LIB})\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.add.Tensor Calls in PyTorch Text Trace\nDESCRIPTION: Lists call signatures for the aten.add.Tensor operator, detailing argument tensor shapes and data types. The trace captures patterns in element-wise addition operations typically appearing in residual connections or intermediate computations in neural network layers. No external code dependencies are required for reading this trace, but it assumes a PyTorch context for operators and tensor representations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 2, ((T([32, 24, 56, 56], f16), T([32, 24, 56, 56], f16)), {})\ncnt: 2, ((T([32, 40, 28, 28], f16), T([32, 40, 28, 28], f16)), {})\ncnt: 4, ((T([32, 80, 14, 14], f16), T([32, 80, 14, 14], f16)), {})\ncnt: 4, ((T([32, 112, 14, 14], f16), T([32, 112, 14, 14], f16)), {})\ncnt: 6, ((T([32, 192, 7, 7], f16), T([32, 192, 7, 7], f16)), {})\ncnt: 4, ((T([32, 1152, 7, 7], f16), T([32, 1152, 7, 7], f16)), {})\ncnt: 1, ((T([32, 672, 7, 7], f16), T([32, 672, 7, 7], f16)), {})\ncnt: 2, ((T([32, 672, 14, 14], f16), T([32, 672, 14, 14], f16)), {})\ncnt: 3, ((T([32, 480, 14, 14], f16), T([32, 480, 14, 14], f16)), {})\ncnt: 1, ((T([32, 240, 14, 14], f16), T([32, 240, 14, 14], f16)), {})\ncnt: 1, ((T([32, 240, 28, 28], f16), T([32, 240, 28, 28], f16)), {})\ncnt: 1, ((T([32, 144, 28, 28], f16), T([32, 144, 28, 28], f16)), {})\ncnt: 1, ((T([32, 144, 56, 56], f16), T([32, 144, 56, 56], f16)), {})\ncnt: 1, ((T([32, 96, 56, 56], f16), T([32, 96, 56, 56], f16)), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Copy Operations in PyTorch\nDESCRIPTION: Log of memory copy operations between tensors with the same shape. This shows a copy between two tensors of shape [64, 3, 256, 256] with half-precision (f16) data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([64, 3, 256, 256], f16), T([64, 3, 256, 256], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Threshold Backward Operations\nDESCRIPTION: This snippet shows backward operations for threshold functions (likely ReLU backward) applied to tensors of shapes [128, 1984, 7, 7] and [128, 1104, 7, 7] with f16 precision, using a threshold value of 0.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 1984, 7, 7], f16), T([128, 1984, 7, 7], f16), 0), {})\ncnt: 8, ((T([128, 1104, 7, 7], f16), T([128, 1104, 7, 7], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.native_layer_norm_backward.default Operator (Text)\nDESCRIPTION: Logs calls to the `aten.native_layer_norm_backward.default` operator. Each line shows the invocation count (`cnt`) and a tuple representing the arguments passed, including tensor shapes and data types (primarily f16). These logs likely represent gradient computations during the backward pass for layer normalization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ncnt: 8, ((T([1, 768, 1536], f16), T([768], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 1536, 1536], f16), T([1536], f16), None, None, None, True, 0.0, 1e-05), {})\ncnt: 1, ((T([1, 3072, 1536], f16), T([3072], f16), None, None, None, True, 0.0, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations\nDESCRIPTION: Forward and backward convolution operations with various kernel sizes, strides, and padding configurations on half-precision tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cspdarknet53_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.convolution.default((T([64, 3, 256, 256], f16), T([32, 3, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\naten.convolution_backward.default((T([64, 1024, 8, 8], f16), T([64, 1024, 8, 8], f16), T([1024, 1024, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Using libtorch C++ API in Android Native Code\nDESCRIPTION: This C++ code snippet demonstrates how to use the libtorch C++ API in Android native code. It includes necessary headers, sets up a JIT call guard, loads a model, and performs a forward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/README.md#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n#include <string>\n#include <ATen/NativeFunctions.h>\n#include <torch/script.h>\nnamespace pytorch_testapp_jni {\nnamespace {\n    struct JITCallGuard {\n      c10::InferenceMode guard;\n      torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard{false};\n    };\n}\n\nvoid loadAndForwardModel(const std::string& modelPath) {\n  JITCallGuard guard;\n  torch::jit::Module module = torch::jit::load(modelPath);\n  module.eval();\n  torch::Tensor t = torch::randn({1, 3, 224, 224});\n  c10::IValue t_out = module.forward({t});\n}\n}\n```\n\n----------------------------------------\n\nTITLE: Organizing MPS Profiler API with Sphinx Autosummary - reStructuredText\nDESCRIPTION: This snippet creates a section for Metal profiler bindings by summarizing the profiler-related APIs of torch.mps. It lists functions pertaining to Metal API tracing and enables autogenerated Sphinx pages for each profiler tool, facilitating discoverability. Sphinx autodoc/autosummary and properly structured API docstrings are prerequisites. Inputs are profiler methods, outputs are linked summary files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/mps.rst#2025-04-22_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\nMPS Profiler\n------------\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    profiler.start\n    profiler.stop\n    profiler.profile\n\n    profiler.is_capturing_metal\n    profiler.is_metal_capture_enabled\n    profiler.metal_capture\n```\n\n----------------------------------------\n\nTITLE: Cloning PyTorch Source - Git Bash Commands - bash\nDESCRIPTION: This snippet demonstrates how to clone the PyTorch repository and synchronize its submodules using Git. Required dependencies include Git and network access. 'git submodule sync' ensures the submodule configurations are up-to-date, and 'git submodule update --init --recursive' initializes and updates all submodules required for building PyTorch. Run these commands from a shell terminal before building PyTorch.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/pytorch/pytorch\\ncd pytorch\\n# if you are updating an existing checkout\\ngit submodule sync\\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Minimum Version and Module Path\nDESCRIPTION: Sets the minimum required CMake version and adds a custom module path.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\nset(CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake ${CMAKE_MODULE_PATH})\n```\n\n----------------------------------------\n\nTITLE: Retrieving SubprocessHandler in PyTorch Distributed Elastic\nDESCRIPTION: Function to retrieve the SubprocessHandler for managing subprocesses in PyTorch's distributed elastic multiprocessing module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/subprocess_handler.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ntorch.distributed.elastic.multiprocessing.subprocess_handler.handlers.get_subprocess_handler\n```\n\n----------------------------------------\n\nTITLE: Configuring ROCm Support in ATen CMake\nDESCRIPTION: Sets up ROCm-related source files, include directories, and dependencies for ATen. It handles different ROCm components and includes various HIP source files for different functionalities.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_ROCM)\n  # NOTE: The PyTorch build does not actually add_subdirectory\n  # third_party/composable_kernel or use it as a CMake library. What is used\n  # is header only, so this should be ok, except that the CMake build generates\n  # a ck/config.h. We just do that part here. Without this, the ck.h from the\n  # ROCM SDK may get accidentally used instead.\n  function(_pytorch_rocm_generate_ck_conf)\n    set(CK_ENABLE_INT8 \"ON\")\n    set(CK_ENABLE_FP16 \"ON\")\n    set(CK_ENABLE_FP32 \"ON\")\n    set(CK_ENABLE_FP64 \"ON\")\n    set(CK_ENABLE_BF16 \"ON\")\n    set(CK_ENABLE_FP8 \"ON\")\n    set(CK_ENABLE_BF8 \"ON\")\n    set(CK_USE_XDL \"ON\")\n    set(CK_USE_WMMA \"ON\")\n    configure_file(\n      \"${Torch_SOURCE_DIR}/third_party/composable_kernel/include/ck/config.h.in\"\n      \"${CMAKE_CURRENT_BINARY_DIR}/composable_kernel/ck/config.h\"\n      )\n  endfunction()\n  list(APPEND ATen_HIP_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/hip)\n  list(APPEND ATen_HIP_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/composable_kernel/include)\n  list(APPEND ATen_HIP_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/composable_kernel/library/include)\n  list(APPEND ATen_HIP_INCLUDE ${CMAKE_CURRENT_BINARY_DIR}/composable_kernel)\n  _pytorch_rocm_generate_ck_conf()\n\n  # Next two lines are needed because TunableOp uses third-party/fmt\n  list(APPEND ATen_HIP_INCLUDE $<TARGET_PROPERTY:fmt::fmt-header-only,INTERFACE_INCLUDE_DIRECTORIES>)\n  list(APPEND ATen_HIP_DEPENDENCY_LIBS fmt::fmt-header-only)\nif(USE_FLASH_ATTENTION)\n  list(APPEND ATen_HIP_INCLUDE ${CMAKE_CURRENT_SOURCE_DIR}/native/transformers/hip/flash_attn/ck)\nendif()\n  list(APPEND ATen_HIP_SRCS\n    ${ATen_HIP_SRCS}\n    ${hip_hip}\n    ${native_hip_hip}\n    ${native_nested_hip_hip}\n    ${native_sparse_hip_hip}\n    ${native_quantized_hip_hip}\n    ${native_transformers_hip_hip} ${native_transformers_src_hip_hip}\n  )\n  if(WIN32) # Windows doesn't support Composable Kernels\n    file(GLOB native_hip_bgemm \"native/hip/bgemm_kernels/*.hip\")\n    file(GLOB native_hip_ck \"native/hip/ck*.hip\")\n    exclude(ATen_HIP_SRCS \"${ATen_HIP_SRCS}\"\n      ${native_hip_bgemm} ${native_hip_ck})\n  endif()\n  # TODO: Codegen separate files for HIP and use those (s/cuda_generated_sources/hip_generated_sources)\n  list(APPEND all_hip_cpp\n    ${native_nested_hip_cpp}\n    ${native_sparse_hip_cpp}\n    ${native_quantized_hip_cpp}\n    ${native_transformers_hip_cpp}\n    ${native_quantized_cudnn_hip_cpp}\n    ${hip_cpp}\n    ${native_hip_cpp}\n    ${native_hip_linalg_cpp}\n    ${cuda_generated_sources}\n    ${ATen_HIP_SRCS}\n    ${native_miopen_cpp}\n    ${native_cudnn_hip_cpp}\n    ${miopen_cpp}\n    ${all_hip_cpp}\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: ReLU In-place Operations in PyTorch with Float16 Tensors\nDESCRIPTION: Records of in-place ReLU activation operations (aten.relu_.default) applied to tensors of various shapes. The counts show how many times each tensor configuration was processed with the operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([8, 64, 112, 112], f16),), {})\ncnt: 6, ((T([8, 128, 56, 56], f16),), {})\ncnt: 4, ((T([8, 256, 56, 56], f16),), {})\ncnt: 7, ((T([8, 256, 28, 28], f16),), {})\ncnt: 5, ((T([8, 512, 28, 28], f16),), {})\ncnt: 11, ((T([8, 512, 14, 14], f16),), {})\ncnt: 7, ((T([8, 1024, 14, 14], f16),), {})\ncnt: 5, ((T([8, 1024, 7, 7], f16),), {})\ncnt: 3, ((T([8, 2048, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Working with External Data in ATen\nDESCRIPTION: Example of creating a Tensor from pre-allocated memory using torch::from_blob. This allows using existing data without copying, though the resulting tensor cannot be resized since ATen doesn't own the memory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nfloat data[] = { 1, 2, 3,\n                 4, 5, 6 };\ntorch::Tensor f = torch::from_blob(data, {2, 3});\n```\n\n----------------------------------------\n\nTITLE: Installing py-spy for PyTorch Benchmarking\nDESCRIPTION: Command to install py-spy, a sampling profiler for Python programs, which is required for generating flame graphs in the benchmarking process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/overrides_benchmark/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install py-spy\n```\n\n----------------------------------------\n\nTITLE: Displaying PyTorch Benchmark Results in Markdown Table\nDESCRIPTION: A markdown table presenting performance metrics for different PyTorch configurations. It includes warmup latency, average latency, throughput, and GPU utilization for various experiments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_256_false.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |\n| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |\n| original | 6.424 +/- 1.141 | 26.361 +/- 0.557 | 573.027 +/- 9.661 | 64.000 +/- 3.405 |\n| h2d_d2h_threads | 4.600 +/- 0.724 | 21.314 +/- 0.403 | 704.344 +/- 9.843 | 71.963 +/- 1.558 |\n| 2_predict_workers | 4.199 +/- 0.363 | 16.772 +/- 1.435 | 864.678 +/- 32.353 | 70.026 +/- 1.403 |\n| 3_predict_workers | 4.496 +/- 0.755 | 15.983 +/- 0.455 | 912.386 +/- 18.299 | 68.283 +/- 2.226 |\n| 4_predict_workers | 4.252 +/- 0.515 | 14.702 +/- 0.259 | 951.261 +/- 7.986 | 70.716 +/- 2.774 |\n```\n\n----------------------------------------\n\nTITLE: Conditional Linking for Linux in CMake\nDESCRIPTION: Conditionally applies specific linker flags (`-Wl,--no-as-needed` and `--as-needed`) when linking the `test_lite_interpreter_runtime` executable against the `backend_with_compiler_runtime` library, but only on Linux systems. This ensures the shared library dependency is correctly handled by the linker. Requires `test_lite_interpreter_runtime` and `backend_with_compiler_runtime` targets, and executes only if CMake's `LINUX` variable is true.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lite_interpreter_runtime/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(LINUX)\n  target_link_libraries(test_lite_interpreter_runtime PRIVATE \"-Wl,--no-as-needed,$<TARGET_FILE:backend_with_compiler_runtime>,--as-needed\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building shared library 'shm' in CMake\nDESCRIPTION: Defines the 'shm' shared library target, sets its source file, and configures compilation definitions and include directories. It also links against the torch and c10 libraries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/lib/libshm_windows/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(shm SHARED core.cpp)\n\ntarget_compile_definitions(shm PRIVATE\n  \"_CRT_SECURE_NO_DEPRECATE=1\"\n  \"SHM_EXPORTS\"\n)\n\ntarget_include_directories(shm PRIVATE\n  ${CMAKE_BINARY_DIR}/aten/src # provides \"ATen/TypeExtendedInterface.h\" to ATen.h\n  ${TORCH_ROOT}/torch/lib # provides \"libshm/libshm.h\"\n  ${CMAKE_CURRENT_SOURCE_DIR}\n  )\n\ntarget_link_libraries(shm torch c10)\n```\n\n----------------------------------------\n\nTITLE: Configuring Distributed Support for PyTorch Python\nDESCRIPTION: Sets up distributed computing support for PyTorch Python bindings, including optional dependencies like NCCL and MPI.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_DISTRIBUTED)\n    if(WIN32)\n      append_filelist(\"libtorch_python_distributed_core_sources\" TORCH_PYTHON_SRCS)\n    else()\n      append_filelist(\"libtorch_python_distributed_sources\" TORCH_PYTHON_SRCS)\n    endif()\n    # Disable certain warnings for GCC-9.X\n    if(CMAKE_COMPILER_IS_GNUCXX)\n      set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/autograd/init.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n      set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/rpc/testing/init.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n      set_source_files_properties(${TORCH_SRC_DIR}/csrc/distributed/c10d/init.cpp PROPERTIES COMPILE_FLAGS \"-Wno-cast-function-type\")\n    endif()\n    # NCCL is a private dependency of libtorch, but libtorch_python includes\n    # some private headers of libtorch, which in turn include NCCL. As a hacky\n    # alternative to making NCCL a public dependency of libtorch, we make it\n    # a private dependency of libtorch_python as well.\n    if(USE_NCCL)\n      list(APPEND TORCH_PYTHON_LINK_LIBRARIES __caffe2_nccl)\n    endif()\n    # Same for MPI.\n    if(USE_MPI)\n      list(APPEND TORCH_PYTHON_LINK_LIBRARIES MPI::MPI_CXX)\n    endif()\n    list(APPEND TORCH_PYTHON_COMPILE_DEFINITIONS USE_C10D)\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Displaying PyTorch Benchmark Results in Markdown Table\nDESCRIPTION: This markdown table presents performance metrics for two PyTorch experiments: 'original' and 'h2d_d2h_threads'. It includes warmup latency, average latency, throughput, and GPU utilization for each configuration, with standard deviations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_64_true.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |\n| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |\n| original | 14.872 +/- 1.142 | 8.001 +/- 0.607 | 461.791 +/- 13.902 | 46.577 +/- 3.598 |\n| h2d_d2h_threads | 12.900 +/- 0.448 | 9.421 +/- 0.845 | 418.964 +/- 25.199 | 47.681 +/- 5.883 |\n```\n\n----------------------------------------\n\nTITLE: Calling Mean Reduction (aten.mean.dim) in PyTorch (Python)\nDESCRIPTION: This snippet represents invocation of the ATen 'mean' operator along specified dimensions on a 4D float16 tensor. It expects a tensor of shape [32, 1280, 7, 7] and reduces along the last two axes, returning a mean-reduced tensor. No explicit dependencies apart from PyTorch are required; shapes and dtype are critical for correct usage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mnasnet1_0_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([32, 1280, 7, 7], f16), [2, 3]), {})\n```\n\n----------------------------------------\n\nTITLE: Lexer Token Stream Output for Indented Code\nDESCRIPTION: The token stream generated by the Lexer for the indented code example, showing how indentation is represented using special tokens.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nTK_IF TK_NEWLINE TK_INDENT . TK_NEWLINE . TK_NEWLINE TK_DEDENT\n```\n\n----------------------------------------\n\nTITLE: Analyzing Convolution Operations in PyTorch MobileNet Model\nDESCRIPTION: This snippet shows convolution operations with different tensor shapes in a MobileNet model. Each line shows a count (cnt) followed by input/output tensor shapes, filter dimensions, stride, padding and other convolution parameters in f16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 32, 28, 28], f16), T([128, 96, 28, 28], f16), T([32, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 28, 28], f16), T([128, 96, 28, 28], f16), T([96, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 96, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 28, 28], f16), T([128, 32, 28, 28], f16), T([96, 32, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 32, 28, 28], f16), T([128, 144, 28, 28], f16), T([32, 144, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 56, 56], f16), T([144, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 144, [True, True, False]), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([128, 24, 56, 56], f16), T([144, 24, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 4, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24, 24, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 24, [True, True, False]), {})\ncnt: 1, ((T([128, 24, 56, 56], f16), T([128, 96, 56, 56], f16), T([24, 96, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), T([128, 96, 112, 112], f16), T([96, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 96, [True, True, False]), {})\ncnt: 1, ((T([128, 96, 112, 112], f16), T([128, 16, 112, 112], f16), T([96, 16, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 2, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16, 16, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 16, [True, True, False]), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 3, 224, 224], f16), T([16, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking Tensor Summation Operations in PyTorch\nDESCRIPTION: Records sum operations across specific dimensions of tensors. These operations typically calculate spatial averages over feature maps or sum across the batch dimension for loss calculation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 2, ((T([128, 960, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 672, 7, 7], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 672, 14, 14], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 480, 14, 14], f16), [2, 3], True), {})\ncnt: 2, ((T([128, 120, 28, 28], f16), [2, 3], True), {})\ncnt: 1, ((T([128, 72, 28, 28], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Sample MNIST Training Output with Lazy Tensor\nDESCRIPTION: Displays sample output from the MNIST training process, showing decreasing loss values as the model trains on the Lazy device.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/lazy/tutorial.md#2025-04-22_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 2.343924\nTrain Epoch: 1 [640/60000 (1%)] Loss: 1.760821\nTrain Epoch: 1 [1280/60000 (2%)]        Loss: 0.802798\nTrain Epoch: 1 [1920/60000 (3%)]        Loss: 0.856164\nTrain Epoch: 1 [2560/60000 (4%)]        Loss: 0.568396\nTrain Epoch: 1 [3200/60000 (5%)]        Loss: 0.399044\nTrain Epoch: 1 [3840/60000 (6%)]        Loss: 0.457996\nTrain Epoch: 1 [4480/60000 (7%)]        Loss: 0.285104\nTrain Epoch: 1 [5120/60000 (9%)]        Loss: 0.193083\nTrain Epoch: 1 [5760/60000 (10%)]       Loss: 0.486165\nTrain Epoch: 1 [6400/60000 (11%)]       Loss: 0.163996\nTrain Epoch: 1 [7040/60000 (12%)]       Loss: 0.200323\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Addition Operations in PyTorch Neural Network\nDESCRIPTION: Summary of tensor addition operations (aten.add.Tensor) in the model, showing the count, tensor shapes and data types. The operations primarily work with half-precision (f16) tensors of various dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_regnet_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add.Tensor\ncnt: 6, ((T([32, 224, 56, 56], f16), T([32, 224, 56, 56], f16)), {})\ncnt: 15, ((T([32, 448, 28, 28], f16), T([32, 448, 28, 28], f16)), {})\ncnt: 33, ((T([32, 896, 14, 14], f16), T([32, 896, 14, 14], f16)), {})\ncnt: 2, ((T([32, 2240, 7, 7], f16), T([32, 2240, 7, 7], f16)), {})\ncnt: 1, ((T([32, 32, 112, 112], f16), T([32, 32, 112, 112], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining a Function with Docstring and Comments\nDESCRIPTION: A function with a proper docstring followed by multiple comment lines.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef long():\n    \"\"\"This docstring, while short, is enough\"\"\"\n    #\n    #\n    #\n    #\n    pass\n```\n\n----------------------------------------\n\nTITLE: Reshaping Tensors Unsafely in PyTorch\nDESCRIPTION: Describes the use of aten._unsafe_view.default to reshape tensors without safety checks. This can lead to undefined behavior if tensors are misaligned, underscoring the importance of cautious usage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaForQuestionAnswering_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 12, ((T([2048, 2304], f16), [4, 512, 2304]), {})\ncnt: 36, ((T([4, 12, 512, 64], f16), [48, 512, 64]), {})\ncnt: 12, ((T([4, 12, 64, 512], f16), [48, 64, 512]), {})\ncnt: 12, ((T([48, 512, 512], f16), [4, 12, 512, 512]), {})\ncnt: 12, ((T([48, 512, 64], f16), [4, 12, 512, 64]), {})\ncnt: 12, ((T([4, 512, 12, 192], f16), [4, 512, 2304]), {})\n```\n\n----------------------------------------\n\nTITLE: Using AutoDispatchBelowADInplaceOrView in Custom Autograd Functions\nDESCRIPTION: Example of using AutoDispatchBelowADInplaceOrView (formerly AutoNonVariableTypeMode) in a custom autograd function to redispatch under Autograd dispatch keys for a ROI align operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/inference_mode.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nclass ROIAlignFunction : public torch::autograd::Function<ROIAlignFunction> {\n public:\n  static torch::autograd::variable_list forward(\n      torch::autograd::AutogradContext* ctx,\n      const torch::autograd::Variable& input,\n      const torch::autograd::Variable& rois,\n      double spatial_scale,\n      int64_t pooled_height,\n      int64_t pooled_width,\n      int64_t sampling_ratio,\n      bool aligned) {\n    ctx->saved_data[\"spatial_scale\"] = spatial_scale;\n    ctx->saved_data[\"pooled_height\"] = pooled_height;\n    ctx->saved_data[\"pooled_width\"] = pooled_width;\n    ctx->saved_data[\"sampling_ratio\"] = sampling_ratio;\n    ctx->saved_data[\"aligned\"] = aligned;\n    ctx->saved_data[\"input_shape\"] = input.sizes();\n    ctx->save_for_backward({rois});\n    // Used to be at::AutoNonVariableTypeMode g;\n    at::AutoDispatchBelowADInplaceOrView guard;\n    auto result = roi_align(\n        input, rois, spatial_scale, pooled_height,\n        pooled_width, sampling_ratio, aligned);\n    return {result};\n  }\n```\n\n----------------------------------------\n\nTITLE: MSVC Version Check\nDESCRIPTION: Checks if the Microsoft Visual C++ compiler version is less than 19.11 (VS 2017) and returns if true\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/test/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n  if(MSVC_VERSION LESS 1911)\n    return()\n  endif()\nendif(MSVC)\n```\n\n----------------------------------------\n\nTITLE: Including Vulkan Dependency Definitions in CMake\nDESCRIPTION: If the `USE_VULKAN` flag is ON, this block initializes empty `Vulkan_LIBS` and `Vulkan_INCLUDES` variables and then includes the `VulkanDependencies.cmake` file. This external file is expected to populate the variables with the necessary Vulkan libraries and include paths.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ Vulkan deps\nif(USE_VULKAN)\n  set(Vulkan_LIBS)\n  set(Vulkan_INCLUDES)\n  include(${CMAKE_CURRENT_LIST_DIR}/../../cmake/VulkanDependencies.cmake)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This code snippet represents a structured log of PyTorch operator usage. It includes operator names, input tensor shapes, data types, and usage counts. This information is crucial for performance analysis and optimization of PyTorch models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForQuestionAnswering_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 2, ((T([32, 128], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 2, ((T([32, 128], f16), T([32, 128], f16), 1, f16), {})\nOperator: aten._softmax.default\ncnt: 6, ((T([32, 12, 128, 128], f16), -1, False), {})\nOperator: aten._softmax_backward_data.default\ncnt: 6, ((T([32, 12, 128, 128], f16), T([32, 12, 128, 128], f16), -1, f16), {})\nOperator: aten._unsafe_view.default\ncnt: 18, ((T([32, 12, 128, 64], f16), [384, 128, 64]), {})\ncnt: 6, ((T([32, 12, 64, 128], f16), [384, 64, 128]), {})\ncnt: 6, ((T([384, 128, 128], f16), [32, 12, 128, 128]), {})\ncnt: 6, ((T([384, 128, 64], f16), [32, 12, 128, 64]), {})\ncnt: 12, ((T([32, 128, 12, 64], f16), [32, 128, 768]), {})\ncnt: 6, ((T([32, 128, 768], f16), [4096, 768]), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([32, 128, 768], f16), T([1, 128, 768], f16)), {})\ncnt: 36, ((T([32, 128, 768], f16), T([32, 128, 768], f16)), {})\ncnt: 1, ((T([], f16), T([], f16)), {})\nOperator: aten.addmm.default\ncnt: 24, ((T([768], f16), T([4096, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\ncnt: 6, ((T([3072], f16), T([4096, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\ncnt: 6, ((T([768], f16), T([4096, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\ncnt: 1, ((T([2], f16), T([4096, 768], f16), T([768, 2], f16, stride=(1, 768))), {})\nOperator: aten.bmm.default\ncnt: 6, ((T([384, 128, 64], f16), T([384, 64, 128], f16)), {})\ncnt: 6, ((T([384, 128, 128], f16), T([384, 128, 64], f16)), {})\ncnt: 6, ((T([384, 128, 128], f16, stride=(16384, 1, 128)), T([384, 128, 64], f16)), {})\ncnt: 6, ((T([384, 128, 64], f16), T([384, 64, 128], f16, stride=(8192, 1, 64))), {})\ncnt: 6, ((T([384, 64, 128], f16, stride=(8192, 1, 64)), T([384, 128, 128], f16)), {})\ncnt: 6, ((T([384, 128, 128], f16), T([384, 128, 64], f16, stride=(8192, 1, 128))), {})\nOperator: aten.cat.default\ncnt: 1, (([T([32, 128, 1], f16), T([32, 128, 1], f16)], 2), {})\nOperator: aten.clamp.default\ncnt: 2, ((T([32], i64), 0, 128), {})\nOperator: aten.clone.default\ncnt: 1, ((T([32, 128], i64),), {})\ncnt: 2, ((T([32], i64),), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([32, 128], i64), T([32, 128], i64)), {})\ncnt: 2, ((T([32], i64), T([32], i64)), {})\nOperator: aten.div.Tensor\ncnt: 6, ((T([32, 12, 128, 64], f16, stride=(98304, 64, 768, 1)), 8.0), {})\ncnt: 2, ((T([], f16), 2), {})\ncnt: 6, ((T([32, 12, 128, 64], f16), 8.0), {})\nOperator: aten.embedding.default\ncnt: 1, ((T([30522, 768], f16), T([32, 128], i64), 0), {})\ncnt: 1, ((T([512, 768], f16), T([1, 128], i64)), {})\nOperator: aten.embedding_dense_backward.default\ncnt: 1, ((T([1, 128, 768], f16), T([1, 128], i64), 512, -1, False), {})\ncnt: 1, ((T([32, 128, 768], f16), T([32, 128], i64), 30522, 0, False), {})\nOperator: aten.eq.Scalar\ncnt: 6, ((T([32, 128], f32), 0), {})\nOperator: aten.gelu.default\ncnt: 6, ((T([32, 128, 3072], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 6, ((T([32, 128, 3072], f16), T([32, 128, 3072], f16)), {})\nOperator: aten.masked_fill.Scalar\ncnt: 6, ((T([32, 12, 128, 128], f16), T([32, 12, 128, 128], b8, stride=(128, 0, 0, 1)), 0), {})\nOperator: aten.masked_fill.Tensor\ncnt: 6, ((T([32, 12, 128, 128], f16), T([32, 12, 128, 128], b8, stride=(128, 0, 0, 1)), T([], f32)), {})\nOperator: aten.mm.default\ncnt: 1, ((T([4096, 2], f16), T([2, 768], f16)), {})\ncnt: 1, ((T([2, 4096], f16, stride=(1, 2)), T([4096, 768], f16)), {})\ncnt: 6, ((T([4096, 768], f16), T([768, 3072], f16)), {})\ncnt: 6, ((T([768, 4096], f16, stride=(1, 768)), T([4096, 3072], f16)), {})\ncnt: 6, ((T([4096, 3072], f16), T([3072, 768], f16)), {})\ncnt: 6, ((T([3072, 4096], f16, stride=(1, 3072)), T([4096, 768], f16)), {})\ncnt: 24, ((T([4096, 768], f16), T([768, 768], f16)), {})\ncnt: 24, ((T([768, 4096], f16, stride=(1, 768)), T([4096, 768], f16)), {})\nOperator: aten.native_layer_norm.default\ncnt: 13, ((T([32, 128, 768], f16), [768], T([768], f16), T([768], f16), 1e-12), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 13, ((T([32, 128, 768], f16), T([32, 128, 768], f16), [768], T([32, 128, 1], f32), T([32, 128, 1], f32), T([768], f16), T([768], f16), [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 2, ((T([], f16), T([32, 128], f16), T([32], i64), None, 1, 128, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 2, ((T([32, 128], f16), T([32], i64), None, 1, 128), {})\nOperator: aten.split.Tensor\ncnt: 1, ((T([32, 128, 2], f16), 1, -1), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([4096, 2], f16), [0], True), {})\ncnt: 30, ((T([4096, 768], f16), [0], True), {})\ncnt: 6, ((T([4096, 3072], f16), [0], True), {})\ncnt: 1, ((T([32, 128, 768], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Initializing Kernel Selection AutoHeuristic in PyTorch\nDESCRIPTION: Constructor initialization for kernel choice selection with AutoHeuristicSelectAlgorithm class. Used for mixed_mm optimization with required parameters for algorithm selection.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nautoheuristic = AutoHeuristicSelectAlgorithm(\n    fallback=fallback,\n    choices=choices,\n    input_nodes=input_nodes,\n    context=context,\n    name=name,\n    augment_context=ops,\n    precondition=precondition,\n)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Batch Normalization Backward Operation\nDESCRIPTION: This code snippet shows the parameters for the native_batch_norm_backward operation in PyTorch. It includes various tensor shapes and their corresponding batch normalization parameters for the backward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([4, 1024, 7, 7], f16), T([4, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f32), T([1024], f32), False, 1e-05, [True, True, True]), {})\ncnt: 16, ((T([4, 128, 7, 7], f16), T([4, 128, 7, 7], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})\n# ... (truncated for brevity)\ncnt: 1, ((T([4, 800, 14, 14], f16), T([4, 800, 14, 14], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f32), T([800], f32), False, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Including aotriton for ROCm Attention Kernels in CMake\nDESCRIPTION: Conditionally includes the `cmake/External/aotriton.cmake` file. This inclusion happens only if building on a UNIX-like system (UNIX is TRUE), ROCm support is enabled (USE_ROCM is TRUE), and either Flash Attention (USE_FLASH_ATTENTION) or Memory Efficient Attention (USE_MEM_EFF_ATTENTION) is enabled. This is necessary for building Triton-based attention kernels for ROCm.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_33\n\nLANGUAGE: cmake\nCODE:\n```\n#\n# Cannot be put into Dependencies.cmake due circular dependency:\n# USE_FLASH_ATTENTION -> USE_ROCM -> Dependencies.cmake -> aotriton.cmake\n#\nif(USE_ROCM)\n  if(UNIX AND (USE_FLASH_ATTENTION OR USE_MEM_EFF_ATTENTION))\n    include(cmake/External/aotriton.cmake)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Tensor Operations with Convolutions and Element-wise Functions\nDESCRIPTION: Series of tensor operations showing convolutions with varying input/output channels and spatial dimensions. Operations include convolutions, hardtanh activations, mean pooling and backward passes using half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Convolution operation examples\n((T([128, 106, 14, 14], f16), T([128, 570, 14, 14], f16), T([106, 570, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n\n# Element-wise hardtanh operation\n((T([128, 32, 112, 112], f16), 0.0, 6.0), {})\n\n# Mean pooling operation\n((T([128, 228, 28, 28], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Documenting Matrix Multiplication Patterns (aten.mm) - PyTorch - Python\nDESCRIPTION: This snippet catalogs batch and non-batch matrix multiplication call patterns using the PyTorch aten.mm operator, listing argument tensor shapes, datatypes, and memory layouts (strides). Relies on the PyTorch tensor API. Inputs are pairs of tensors; no result or side effect tracked.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 2304], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 2304], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Performing Unsafe View Transformations\nDESCRIPTION: The _unsafe_view operator reshapes tensors into new dimensions without altering the data. Prerequisites include PyTorch, and it specifically handles tensors with varying ranks and sizes to produce reshaped tensors with new dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 54, ((T([8, 128, 12, 64], f16), [8, 128, 768]), {})\ncnt: 1, ((T([1024, 50005], f16), [8, 128, 50005]), {})\ncnt: 18, ((T([8, 12, 128, 64], f16), [96, 128, 64]), {})\ncnt: 18, ((T([8, 128, 768], f16), [1024, 768]), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.nll_loss_forward.default Negative Log Likelihood Loss in PyTorch ATen\nDESCRIPTION: Documents observed calls to the Negative Log Likelihood Loss forward pass (`aten.nll_loss_forward.default`) in PyTorch ATen. It lists the input tensor shapes (prediction tensor, target tensor), optional weight tensor (None), reduction type (1 for mean), and ignore index (-100), along with the call count.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nfnet_l0_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Conditionally Installing test_jit Executable and PDB Files in CMake\nDESCRIPTION: Installs the `test_jit` executable to the `bin` directory relative to the installation prefix if `INSTALL_TEST` is enabled. It also sets the RPATH for the installed executable and conditionally installs the corresponding PDB debug file if building shared libraries with MSVC.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\nif(INSTALL_TEST)\n  set_target_properties(test_jit PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n  install(TARGETS test_jit DESTINATION bin)\n  # Install PDB files for MSVC builds\n  if(MSVC AND BUILD_SHARED_LIBS)\n    install(FILES $<TARGET_PDB_FILE:test_jit> DESTINATION bin OPTIONAL)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Mobile NNC Test Sources and Executable in CMake\nDESCRIPTION: Sets up the source files and creates an executable for mobile NNC tests. It links against the torch library and gtest, includes necessary directories, and defines the USE_GTEST compile definition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/nnc/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(MOBILE_NNC_TEST_ROOT ${TORCH_ROOT}/test/mobile/nnc)\n\nset(MOBILE_NNC_TEST_SRCS\n  ${MOBILE_NNC_TEST_ROOT}/test_context.cpp\n  ${MOBILE_NNC_TEST_ROOT}/test_nnc_backend.cpp\n  ${MOBILE_NNC_TEST_ROOT}/test_registry.cpp\n)\n\nadd_executable(test_mobile_nnc\n  ${TORCH_ROOT}/test/cpp/lite_interpreter_runtime/main.cpp\n  ${MOBILE_NNC_TEST_SRCS}\n)\n\ntarget_link_libraries(test_mobile_nnc PRIVATE torch gtest)\ntarget_include_directories(test_mobile_nnc PRIVATE ${ATen_CPU_INCLUDE})\ntarget_compile_definitions(test_mobile_nnc PRIVATE USE_GTEST)\n```\n\n----------------------------------------\n\nTITLE: Execute aten.relu_ Activation in Python\nDESCRIPTION: Performs the in-place rectified linear unit (ReLU) activation on tensors using PyTorch. Commonly used in constructing neural network layers for introducing non-linearity. Dependencies require PyTorchs deep learning capabilities. The snippet accepts tensors, modifying them to zero out negative values, enhancing feature representations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_mixnet_l_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([64, 32, 112, 112], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Building C10 XPU Library\nDESCRIPTION: Defines the library target, sets compile options, links with dependencies, and configures installation when not in libtorchless mode.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/xpu/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_LIBTORCHLESS)\n  add_library(c10_xpu ${C10_XPU_SRCS} ${C10_XPU_HEADERS})\n  torch_compile_options(c10_xpu)\n  target_compile_options(c10_xpu PRIVATE \"-DC10_XPU_BUILD_MAIN_LIB\")\n  # Enable hidden visibility if compiler supports it.\n  if(${COMPILER_SUPPORTS_HIDDEN_VISIBILITY})\n    target_compile_options(c10_xpu PRIVATE \"-fvisibility=hidden\")\n  endif()\n\n  # ---[ Dependency of c10_xpu\n  target_link_libraries(c10_xpu PUBLIC c10 torch::xpurt)\n  target_include_directories(\n      c10_xpu PUBLIC\n      $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/../..>\n      $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}>\n      $<INSTALL_INTERFACE:include>\n      )\n  install(TARGETS c10_xpu EXPORT Caffe2Targets DESTINATION lib)\n  set(C10_XPU_LIB c10_xpu)\n  add_subdirectory(test)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Showing TensorExpr Benchmark Documentation\nDESCRIPTION: Command to display the help documentation for TensorExpr benchmarks module.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/tensorexpr/HowToRun.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython -m benchmarks.tensorexpr --help\n```\n\n----------------------------------------\n\nTITLE: Displaying Performance Benchmark Results for PyTorch with Batch Size 1 and Compilation\nDESCRIPTION: Markdown table showing performance comparison between original PyTorch implementation and 'h2d_d2h_threads' variant. Metrics include warmup latency, average latency, throughput in samples per second, and GPU utilization percentage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_1_true.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |\n| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |\n| original | 13.828 +/- 0.535 | 0.297 +/- 0.034 | 205.657 +/- 14.429 | 15.630 +/- 1.601 |\n| h2d_d2h_threads | 12.515 +/- 0.666 | 0.519 +/- 0.107 | 138.126 +/- 21.821 | 12.482 +/- 1.822 |\n```\n\n----------------------------------------\n\nTITLE: Implementation of newIndices Function - C++\nDESCRIPTION: Illustrates the `newIndices` function implementation, which retrieves indices of a tensor by returning the result of another function, `newNarrow`. This follows the PyTorch guideline of returning or freeing `new` allocated pointers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/README.md#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nTHIndexTensor *THSTensor_(newIndices)(const THSTensor *self) {\n  // ...\n  return THIndexTensor_(newNarrow)(self->indices, 1, 0, self->nnz);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating the Lazy Test Executable Target in CMake\nDESCRIPTION: Defines an executable target named `test_lazy` using CMake's `add_executable` command. It includes a common main file (`common/main.cpp`) and all the source files collected in the `LAZY_TEST_SRCS` list.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(test_lazy\n  ${TORCH_ROOT}/test/cpp/common/main.cpp\n  ${LAZY_TEST_SRCS}\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring MSVC Compiler and Linker Flags for PyTorch\nDESCRIPTION: Comprehensive configuration of MSVC compiler and linker flags, including enabling correct __cplusplus version, Z7 debug info format, static/dynamic runtime selection, and various optimizations for Windows builds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  # MSVC by default does not apply the correct __cplusplus version as specified\n  # by the C++ standard because MSVC is not a completely compliant\n  # implementation. This option forces MSVC to use the appropriate value given\n  # the requested --std option. This fixes a compilation issue mismatch between\n  # GCC/Clang and MSVC.\n  #\n  # See: *\n  # https://learn.microsoft.com/en-us/cpp/build/reference/zc-cplusplus?view=msvc-170\n  # * https://en.cppreference.com/w/cpp/preprocessor/replace#Predefined_macros\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Zc:__cplusplus\")\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Xcompiler  /Zc:__cplusplus\")\n\n  set(CMAKE_NINJA_CMCLDEPS_RC OFF)\n  foreach(\n    flag_var\n    CMAKE_C_FLAGS\n    CMAKE_C_FLAGS_DEBUG\n    CMAKE_C_FLAGS_RELEASE\n    CMAKE_C_FLAGS_MINSIZEREL\n    CMAKE_C_FLAGS_RELWITHDEBINFO\n    CMAKE_CXX_FLAGS\n    CMAKE_CXX_FLAGS_DEBUG\n    CMAKE_CXX_FLAGS_RELEASE\n    CMAKE_CXX_FLAGS_MINSIZEREL\n    CMAKE_CXX_FLAGS_RELWITHDEBINFO)\n    # Replace /Zi and /ZI with /Z7\n    if(MSVC_Z7_OVERRIDE)\n      if(${flag_var} MATCHES \"/Z[iI]\")\n        string(REGEX REPLACE \"/Z[iI]\" \"/Z7\" ${flag_var} \"${${flag_var}}\")\n      endif(${flag_var} MATCHES \"/Z[iI]\")\n    endif(MSVC_Z7_OVERRIDE)\n\n    if(${CAFFE2_USE_MSVC_STATIC_RUNTIME})\n      if(${flag_var} MATCHES \"/MD\")\n        string(REGEX REPLACE \"/MD\" \"/MT\" ${flag_var} \"${${flag_var}}\")\n      endif(${flag_var} MATCHES \"/MD\")\n    else()\n      if(${flag_var} MATCHES \"/MT\")\n        string(REGEX REPLACE \"/MT\" \"/MD\" ${flag_var} \"${${flag_var}}\")\n      endif()\n    endif()\n\n    # /bigobj increases number of sections in .obj file, which is needed to link\n    # against libraries in Python 2.7 under Windows For Visual Studio\n    # generators, if /MP is not added, then we may need to add /MP to the flags.\n    # For other generators like ninja, we don't need to add /MP because it is\n    # already handled by the generator itself.\n    if(CMAKE_GENERATOR MATCHES \"Visual Studio\" AND NOT ${flag_var} MATCHES\n                                                   \"/MP\")\n      set(${flag_var} \"${${flag_var}} /MP /bigobj\")\n    else()\n      set(${flag_var} \"${${flag_var}} /bigobj\")\n    endif()\n  endforeach(flag_var)\n\n  foreach(flag_var\n          CMAKE_C_FLAGS CMAKE_C_FLAGS_RELEASE CMAKE_C_FLAGS_MINSIZEREL\n          CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_RELEASE CMAKE_CXX_FLAGS_MINSIZEREL)\n    if(${flag_var} MATCHES \"/Z[iI7]\")\n      string(REGEX REPLACE \"/Z[iI7]\" \"\" ${flag_var} \"${${flag_var}}\")\n    endif()\n  endforeach(flag_var)\n\n  foreach(\n    flag_var\n    CMAKE_SHARED_LINKER_FLAGS_RELWITHDEBINFO\n    CMAKE_STATIC_LINKER_FLAGS_RELWITHDEBINFO\n    CMAKE_EXE_LINKER_FLAGS_RELWITHDEBINFO\n    CMAKE_MODULE_LINKER_FLAGS_RELWITHDEBINFO\n    CMAKE_SHARED_LINKER_FLAGS_DEBUG\n    CMAKE_STATIC_LINKER_FLAGS_DEBUG\n    CMAKE_EXE_LINKER_FLAGS_DEBUG\n    CMAKE_MODULE_LINKER_FLAGS_DEBUG)\n    # Switch off incremental linking in debug/relwithdebinfo builds\n    if(${flag_var} MATCHES \"/INCREMENTAL\" AND NOT ${flag_var} MATCHES\n                                              \"/INCREMENTAL:NO\")\n      string(REGEX REPLACE \"/INCREMENTAL\" \"/INCREMENTAL:NO\" ${flag_var}\n                           \"${${flag_var}}\")\n    endif()\n  endforeach(flag_var)\n\n  foreach(flag_var CMAKE_SHARED_LINKER_FLAGS CMAKE_STATIC_LINKER_FLAGS\n                   CMAKE_EXE_LINKER_FLAGS CMAKE_MODULE_LINKER_FLAGS)\n    string(APPEND ${flag_var} \" /ignore:4049 /ignore:4217 /ignore:4099\")\n  endforeach(flag_var)\n\n  foreach(flag_var CMAKE_SHARED_LINKER_FLAGS)\n    # https://github.com/pytorch/pytorch/issues/91933: Don't set the manifest\n    # filename explicitly helps fix the linker error when linking\n    # torch_python.dll. The manifest file would still be there in the correct\n    # format torch_python.dll.manifest\n    if(${flag_var} MATCHES \"/MANIFESTFILE:.*\\\\.manifest\")\n      string(REGEX REPLACE \"/MANIFESTFILE:.*\\\\.manifest\" \"\" ${flag_var}\n                           \"${${flag_var}}\")\n    endif()\n  endforeach(flag_var)\n\n  # Try harder\n  string(APPEND CMAKE_CUDA_FLAGS \" -Xcompiler /w -w\")\n\n  string(APPEND CMAKE_CXX_FLAGS \" /FS\")\n  string(APPEND CMAKE_CUDA_FLAGS \" -Xcompiler /FS\")\nendif(MSVC)\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage Patterns\nDESCRIPTION: This snippet shows the usage patterns of various PyTorch operators in a deep learning model. It includes information about tensor shapes, data types, and the number of times each operator is called with specific configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([64, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 121, ((T([], i64), 1), {})\ncnt: 1, ((T([64, 512, 7, 7], f16, stride=(50176, 49, 7, 1)), T([64, 512, 7, 7], f16, stride=(48608, 49, 7, 1))), {})\ncnt: 15, ((T([64, 32, 7, 7], f16, stride=(50176, 49, 7, 1)), T([64, 32, 7, 7], f16, stride=(48608, 49, 7, 1))), {})\n# ... [truncated for brevity] ...\n```\n\n----------------------------------------\n\nTITLE: PyTorch Autograd View Operations\nDESCRIPTION: Implementations of view operations for autograd, including reshape, select, transpose and unsqueeze operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_27\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch15ADInplaceOrView12_GLOBAL__N_110select_intEN3c1014DispatchKeySetERKN2at6TensorElNS2_6SymIntE\n_ZN5torch15ADInplaceOrView12_GLOBAL__N_114_reshape_aliasEN3c1014DispatchKeySetERKN2at6TensorENS2_8ArrayRefINS2_6SymIntEEESA_\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Log Softmax Backward Operations in PyTorch\nDESCRIPTION: This code snippet tracks the usage of the ATen log softmax backward data operator, which involves derivatives calculation. It operates on tensors using specified dimensions, types, and involves a backpropagation aspect to calculate gradients for backprop.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([2048, 30522], f16), T([2048, 30522], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Matrix Multiplication Operations in PyTorch\nDESCRIPTION: Log of matrix multiplication operations showing input and output tensor shapes. These are used in fully connected layers, attention mechanisms, and other linear transformations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 2, ((T([262144, 32], f16), T([32, 63], f16, stride=(1, 32))), {})\ncnt: 2, ((T([65536, 64], f16), T([64, 31], f16, stride=(1, 64))), {})\ncnt: 2, ((T([65536, 128], f16), T([128, 31], f16, stride=(1, 128))), {})\ncnt: 2, ((T([16384, 128], f16), T([128, 15], f16, stride=(1, 128))), {})\ncnt: 1, ((T([64, 1000], f16), T([1000, 1280], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Automatic Transfer of Scalar Tensors Between CPU and GPU in PyTorch\nDESCRIPTION: This code snippet demonstrates how scalar tensors are automatically transferred between CPU and GPU when needed in operations, which is an exception to the general rule that tensors are never moved automatically between devices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/tensor_attributes.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n>>> # two scalars\n>>> torch.ones(()) + torch.ones(()).cuda()  # OK, scalar auto-transferred from CPU to GPU\n>>> torch.ones(()).cuda() + torch.ones(())  # OK, scalar auto-transferred from CPU to GPU\n\n>>> # one scalar (CPU), one vector (GPU)\n>>> torch.ones(()) + torch.ones(1).cuda()  # OK, scalar auto-transferred from CPU to GPU\n>>> torch.ones(1).cuda() + torch.ones(())  # OK, scalar auto-transferred from CPU to GPU\n\n>>> # one scalar (GPU), one vector (CPU)\n```\n\n----------------------------------------\n\nTITLE: Using ATen Copy Operator in PyTorch\nDESCRIPTION: This operator copies data from one tensor to another, potentially changing the dtype. It is crucial in type casting within neural network layers. The operation depends on PyTorch and ATen extensions. Parameters include input tensor, desired dtype, and output tensor. It outputs a clone of the original tensor in the specified dtype.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/ElectraForCausalLM_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([1, 1, 1, 512], f32),), {'dtype': f16})\n```\n\n----------------------------------------\n\nTITLE: Configuring C10 Library Dependencies\nDESCRIPTION: Sets up the various dependencies required by the C10 library, conditionally linking against optional libraries like gflags, glog, NUMA, and system-specific dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n  # ---[ Dependency of c10\n  if(C10_USE_GFLAGS)\n    target_link_libraries(c10 PUBLIC gflags)\n  endif()\n\n  if(C10_USE_GLOG)\n    target_link_libraries(c10 PUBLIC glog::glog)\n  endif()\n  target_link_libraries(c10 PRIVATE fmt::fmt-header-only)\n  target_link_libraries(c10 PRIVATE nlohmann)\n\n  if(C10_USE_NUMA)\n    message(STATUS \"NUMA paths:\")\n    message(STATUS ${Numa_INCLUDE_DIR})\n    message(STATUS ${Numa_LIBRARIES})\n    target_include_directories(c10 PRIVATE ${Numa_INCLUDE_DIR})\n    target_link_libraries(c10 PRIVATE ${Numa_LIBRARIES})\n  else()\n    message(STATUS \"don't use NUMA\")\n  endif()\n\n  if(NOT CMAKE_SYSTEM_PROCESSOR MATCHES \"s390x\" AND NOT CMAKE_SYSTEM_PROCESSOR MATCHES \"ppc64le\")\n    target_link_libraries(c10 PRIVATE cpuinfo)\n  endif()\n\n  find_package(Backtrace)\n  if(Backtrace_FOUND)\n    target_include_directories(c10 PRIVATE ${Backtrace_INCLUDE_DIRS})\n    target_link_libraries(c10 PRIVATE ${Backtrace_LIBRARIES})\n    target_compile_definitions(c10 PRIVATE SUPPORTS_BACKTRACE=1)\n  else()\n    target_compile_definitions(c10 PRIVATE SUPPORTS_BACKTRACE=0)\n  endif()\n\n  if(USE_MIMALLOC)\n    target_link_libraries(c10 PRIVATE \"mimalloc-static\")\n    add_dependencies(c10 mimalloc-static)\n  endif()\n\n  if(LINUX)\n    target_link_libraries(c10 PRIVATE Threads::Threads)\n    target_link_libraries(c10 PRIVATE dl)\n  endif()\n\n  if(ANDROID)\n    target_link_libraries(c10 PRIVATE log)\n  endif()\n```\n\n----------------------------------------\n\nTITLE: ReLU Backward Operations in PyTorch\nDESCRIPTION: This snippet shows backward pass operations for ReLU activations, used during gradient computation. These operations calculate gradients through the ReLU function, zeroing out gradients where input was negative and passing through gradients where input was positive.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 4, ((T([96, 512, 4, 4], f16), T([96, 512, 4, 4], f16), 0), {})\ncnt: 4, ((T([96, 256, 8, 8], f16), T([96, 256, 8, 8], f16), 0), {})\ncnt: 4, ((T([96, 128, 16, 16], f16), T([96, 128, 16, 16], f16), 0), {})\ncnt: 4, ((T([96, 64, 32, 32], f16), T([96, 64, 32, 32], f16), 0), {})\ncnt: 1, ((T([96, 64, 64, 64], f16), T([96, 64, 64, 64], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Batch Normalization Backward Operations\nDESCRIPTION: This snippet shows the usage of the aten.native_batch_norm_backward.default operator for computing gradients in batch normalization layers with various shapes and parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 4, ((T([3, 64, 512, 512], f16), T([3, 64, 512, 512], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f32), T([64], f32), False, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([3, 128, 256, 256], f16), T([3, 128, 256, 256], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f32), T([128], f32), False, 1e-05, [True, True, True]), {})\n# ... (truncated for brevity)\n```\n\n----------------------------------------\n\nTITLE: Convolutional Operations Profiling in PyTorch\nDESCRIPTION: This data shows profiling of convolutional neural network operations with details about input, output, and weight tensor shapes, stride, padding, dilation, groups, and other parameters. Each line shows the count and configuration of a specific convolutional operation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientnet_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 112, 14, 14], f16), T([32, 480, 14, 14], f16), T([112, 480, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Conditional Contiguous Tensor Handling - C++\nDESCRIPTION: This snippet adapts tensor handling based on condition checks by either calling `newContiguous` or retaining the tensor directly. It employs the conditional freeing strategy to ensure proper reference management, suitable for optimizing tensor operations based on stride conditions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/README.md#2025-04-22_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\n  if (!(k_->stride(3) == 1) || !(k_->stride[2] == k_->size(3))) {\n    kernel = THTensor_(newContiguous)(k_);\n  } else {\n    THTensor_(retain)(k_);\n    kernel = k_;\n  }\n  ...\n  c10::raw::intrusive_ptr::decref(kernel);\n```\n\n----------------------------------------\n\nTITLE: Parallel Benchmark Configuration\nDESCRIPTION: Configures and builds the parallel_benchmark executable with required include directories and torch library linkage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(parallel_benchmark ${TORCH_API_TEST_DIR}/parallel_benchmark.cpp)\ntarget_include_directories(parallel_benchmark PRIVATE ${ATen_CPU_INCLUDE})\ntarget_link_libraries(parallel_benchmark PRIVATE torch)\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Benchmarks with Command-Line Help\nDESCRIPTION: Shows how to access help documentation for benchmark command-line options.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython benchmark_runner.py --help\n```\n\n----------------------------------------\n\nTITLE: Downloading A100 Dataset and Generating Heuristic for Pad_mm\nDESCRIPTION: These bash commands download the A100 dataset and generate the heuristic for pad_mm on A100 GPU using existing data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/pad_mm/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nbash get_padmm_dataset.sh # Downloads A100\n```\n\nLANGUAGE: bash\nCODE:\n```\nbash gen_pad_mm_a100.sh # Generates A100 heuristic\n```\n\n----------------------------------------\n\nTITLE: Tensor Copy Operations in PyTorch\nDESCRIPTION: This snippet demonstrates a tensor copy operation, which is used to create a new tensor with the same data as an existing tensor. The operation is performed on 4D tensors with specific shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hrnet_w18_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Cloning Tensors in PyTorch using ATen\nDESCRIPTION: Clones the input tensor, creating a new tensor that has the same content but does not share memory with the input tensor. This operation requires a single tensor input of various shapes, such as [16, 3, 128, 128] or [16, 5], all of data type f16. Key characteristics are its ability to duplicate tensor data without shared memory, useful in computational graphs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/pytorch_stargan_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([16, 3, 128, 128], f16),), {})\ncnt: 1, ((T([16, 5], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Manipulation Operations\nDESCRIPTION: Various tensor operations including slicing, splitting, and summation across different dimensions with specified strides and shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n((T([2048, 8, 8], f16), [2048, 8, 15], 2, 7, 9223372036854775807, 1), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations - Core Functions\nDESCRIPTION: Collection of fundamental PyTorch tensor operations including softmax, log_softmax, view operations, and basic arithmetic operations. Operations are performed on tensors with various shapes and primarily use float16 (f16) precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/AllenaiLongformerBase_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Sample tensor operations\naten._log_softmax.default(T([1024, 50265], f16), 1, False)\naten._softmax.default(T([1, 1024, 12, 513], f16), -1, True)\naten._unsafe_view.default(T([12, 3, 512, 64, 1], f16), [36, 512, 64])\naten.add.Tensor(T([1, 1024, 768], f16), T([1, 1024, 768], f16))\n```\n\n----------------------------------------\n\nTITLE: Illustrating Error When Converting Meta Tensor to CPU in Python\nDESCRIPTION: Shows the `NotImplementedError` that occurs when attempting to directly convert a meta tensor to a concrete device like 'cpu' using `.to(\"cpu\")`. This fails because meta tensors lack the actual data needed for the conversion. Requires the `torch` library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/meta.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> torch.ones(5, device='meta').to(\"cpu\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNotImplementedError: Cannot copy out of meta tensor; no data!\n```\n\n----------------------------------------\n\nTITLE: Setting up Build Environment and Building PyTorch on Linux - bash\nDESCRIPTION: This snippet sets the CMAKE_PREFIX_PATH to include Conda's packages and then builds PyTorch in development mode. It is used for standard Linux builds or after running any necessary platform-specific steps. Run this command after installing all dependencies and activating the conda environment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\\npython setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Tensor Indexing Operations in PyTorch\nDESCRIPTION: This snippet shows various tensor indexing operations in PyTorch. It includes operations like aten.index.Tensor, aten.index_put.default, and aten.index_select.default with different tensor shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n# Example of aten.index.Tensor operation\n((T([1024, 249, 249], f16), [None, T([30876], i64), T([30876], i64)]), {})\n\n# Example of aten.index_put.default operation\n((T([1024, 249, 249], f16), [None, T([30876], i64), T([30876], i64)], T([1024, 30876], f16, stride=(31068, 1)), True), {})\n\n# Example of aten.index_select.default operation\n((T([1024, 192], f16, stride=(47808, 1)), 0, T([54765], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining and Linking Lazy Test Dependencies in CMake\nDESCRIPTION: Sets a CMake variable `LAZY_TEST_DEPENDENCIES` to list the required libraries (`torch`, `gtest`). Then, it links these libraries privately to the `test_lazy` target using `target_link_libraries`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nset(LAZY_TEST_DEPENDENCIES torch gtest)\n\ntarget_link_libraries(test_lazy PRIVATE ${LAZY_TEST_DEPENDENCIES})\n```\n\n----------------------------------------\n\nTITLE: Handling aten.addmm in PyTorch with Python\nDESCRIPTION: The aten.addmm.default operator is analyzed for its application on three-tensor products, detailed by their shapes and data types (f16). It typically involves tensors of various dimensions, used to perform matrix multiplication followed by addition. Dependencies include having three-dimensional f16 tensors ready for such operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/nvidia_deeprecommender_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([512], f16), T([256, 197951], f16), T([197951, 512], f16, stride=(1, 197951))), {})\ncnt: 2, ((T([512], f16), T([256, 512], f16), T([512, 512], f16, stride=(1, 512))), {})\ncnt: 1, ((T([1024], f16), T([256, 512], f16), T([512, 1024], f16, stride=(1, 512))), {})\ncnt: 1, ((T([512], f16), T([256, 1024], f16), T([1024, 512], f16, stride=(1, 1024))), {})\ncnt: 1, ((T([197951], f16), T([256, 512], f16), T([512, 197951], f16, stride=(1, 512))), {})\n```\n\n----------------------------------------\n\nTITLE: Scalar and Tensor Divisions in PyTorch\nDESCRIPTION: This section highlights operations 'aten.div.Scalar' and 'aten.div.Tensor' for element-wise division of tensors in PyTorch, either by a scalar or another tensor. It shows diverse division tasks with implications for scaling or normalizing tensor values, maintaining tensor compatibility with half-precision floats.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/mobilenet_v2_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([96, 1280, 7, 7], f16, stride=(1280, 1, 0, 0)), 49), {})\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 96000), {})\n```\n\n----------------------------------------\n\nTITLE: Copying and Transforming Tensors with aten._to_copy in PyTorch\nDESCRIPTION: Handles tensor copying and transformation based on specified attributes like dtype, layout, and device. It supports various data layouts such as strided and indicates the intended target device like 'cuda'. This operation is essential for type conversion and device alignment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/Speech2Text2ForCausalLM_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\naten._to_copy.default(T([128, 128], f32), {'dtype': f16})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten._to_copy.default(T([64, 1, 128, 128], f16, stride=(0, 16384, 128, 1)), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten._to_copy.default(T([64, 128], b8), {'dtype': i32})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten._to_copy.default(T([64, 128], i64), {'dtype': i32, 'layout': torch.strided, 'device': 'cuda'})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten._to_copy.default(T([64, 128], i32), {'dtype': i64})\n```\n\n----------------------------------------\n\nTITLE: Transpiled HIP Function in c10/hip\nDESCRIPTION: Resulting transpiled code for AMD GPU builds, showing how the CUDA function is converted to a HIP function. This illustrates the namespace change from cuda to hip.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/cuda/README.md#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\n// c10/hip/HIPFoo.h\nnamespace c10 { namespace hip {\n\nvoid my_func();\n\n}}\n```\n\n----------------------------------------\n\nTITLE: Integrating GoogleTest via FetchContent in CMake (CMake)\nDESCRIPTION: Uses FetchContent to automatically download and configure the GoogleTest framework at a specified release version. Disables GMock, enables building GTest, and sets shared CRT usage to ensure consistent runtime linking in MSVC environments. This snippet makes C++ unit-testing setup reproducible and avoids system dependency issues. Requires an internet connection and CMake 3.11+ for FetchContent support.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/inductor/cpp/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n################################\n# GTest\n################################\nproject(googletest-git NONE)\n\ninclude(FetchContent)\nFetchContent_Declare(\n  googletest\n  GIT_REPOSITORY https://github.com/google/googletest.git\n  GIT_TAG        release-1.12.1\n)\n\nset(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE)\nset(BUILD_GMOCK OFF CACHE BOOL \"\" FORCE)\nset(BUILD_GTEST ON CACHE BOOL \"\" FORCE)\n\nFetchContent_MakeAvailable(googletest)\n```\n\n----------------------------------------\n\nTITLE: Setting Warning Options for C10 Compilation\nDESCRIPTION: Configures warning-as-error options for the C10 library when the WERROR flag is enabled. Specifically enables sign-comparison and shadow warnings as errors if supported by the compiler.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n  if(WERROR)\n    target_compile_options_if_supported(c10 PRIVATE \"-Werror=sign-compare\")\n    target_compile_options_if_supported(c10 PRIVATE \"-Werror=shadow\")\n  endif()\n```\n\n----------------------------------------\n\nTITLE: GeLU Activation Function Calls\nDESCRIPTION: GeLU activation function applications on tensors of various shapes using float16 precision, including forward and backward passes\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swin_base_patch4_window7_224_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Forward pass\ncnt: 2, ((T([64, 3136, 512], f16),), {})\ncnt: 2, ((T([64, 784, 1024], f16),), {})\n# Backward pass\ncnt: 2, ((T([64, 49, 4096], f16), T([64, 49, 4096], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Defining MSVC-Specific Installation Rules in CMake\nDESCRIPTION: Adds installation rules specifically for builds using the MSVC compiler (`if(MSVC)`). It installs the PDB debug symbol file (`$<TARGET_PDB_FILE:pytorch_jni>`) to the library directory (`CMAKE_INSTALL_LIBDIR`), marking it as optional. It also explicitly adds an install rule for the target itself to the library directory, which might be redundant but ensures behavior on MSVC.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/android/pytorch_android/CMakeLists.txt#2025-04-22_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n  install(FILES $<TARGET_PDB_FILE:pytorch_jni> DESTINATION ${CMAKE_INSTALL_LIBDIR} OPTIONAL)\n  install(TARGETS ${PYTORCH_JNI_TARGET} DESTINATION ${CMAKE_INSTALL_LIBDIR})\nendif()\n```\n\n----------------------------------------\n\nTITLE: PyTorch GELU Backward Function\nDESCRIPTION: These snippets show the backward pass of the GELU activation function for various tensor shapes, using float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 3072, 6, 6], f16), T([128, 3072, 6, 6], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 768, 6, 6], f16), T([128, 768, 6, 6], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1536, 6, 6], f16), T([128, 1536, 6, 6], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 768, 12, 12], f16), T([128, 768, 12, 12], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1536, 12, 12], f16), T([128, 1536, 12, 12], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 768, 24, 24], f16), T([128, 768, 24, 24], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 512, 24, 24], f16), T([128, 512, 24, 24], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 256, 24, 24], f16), T([128, 256, 24, 24], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 256, 48, 48], f16), T([128, 256, 48, 48], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 128, 48, 48], f16), T([128, 128, 48, 48], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 64, 96, 96], f16), T([128, 64, 96, 96], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 32, 96, 96], f16), T([128, 32, 96, 96], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 16, 96, 96], f16), T([128, 16, 96, 96], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Performing Matrix Multiplication with ATen AddMM Operator\nDESCRIPTION: The `aten.addmm` operation combines matrix multiplication with addition, crucial for neural network forward passes, especially in dense layers. Takes matrices of compatible dimensions like [64, 1280] and [1280, 1000].\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([64, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})\n```\n\n----------------------------------------\n\nTITLE: Embedding Operations with aten.embedding\nDESCRIPTION: Embeddings of input indices into higher dimensional spaces using aten.embedding.default rely on structured matrices. Requires PyTorch support to map indices into rows of an embedding matrix, resulting in projected outputs maintaining index matrix shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PLBartForConditionalGeneration_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.embedding.default\ncnt: 2, ((T([50005, 768], f16), T([8, 128], i64), 1), {})\ncnt: 2, ((T([1026, 768], f16), T([8, 128], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking Matrix Multiplication and Addmm Operators (aten.mm, aten.addmm) - PyTorch - Python\nDESCRIPTION: These snippets record single and batch matrix multiplications with aten.mm and generalized additive matrix multiplication with aten.addmm. They include diverse shapes/strides corresponding to typical MLP or network linear layers. Inputs include two or more tensors, possibly with bias and optional stride info. Outputs are matrices/tensors of expected linear algebraic result shapes. Dependencies: torch, proper shape alignment. Constraints: input dimensions and memory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 12, ((T([2304], f16), T([4096, 768], f16), T([768, 2304], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 12, ((T([768], f16), T([4096, 768], f16), T([768, 768], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 12, ((T([3072], f16), T([4096, 768], f16), T([768, 3072], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 12, ((T([768], f16), T([4096, 3072], f16), T([3072, 768], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([4096, 768], f16), T([768, 2], f16, stride=(1, 768))), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([2, 4096], f16, stride=(1, 2)), T([4096, 768], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([4096, 2], f16), T([2, 768], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([4096, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([3072, 4096], f16, stride=(1, 3072)), T([4096, 768], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([4096, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([768, 4096], f16, stride=(1, 768)), T([4096, 3072], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([4096, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([768, 4096], f16, stride=(1, 768)), T([4096, 768], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([4096, 2304], f16), T([2304, 768], f16, stride=(1, 2304))), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([768, 4096], f16, stride=(1, 768)), T([4096, 2304], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Enabling Full FP16 Accumulation for FP16 GEMMs in PyTorch (Python)\nDESCRIPTION: Demonstrates how to enable full FP16 accumulation during FP16 General Matrix Multiplications (GEMMs). Setting `torch.backends.cuda.matmul.allow_fp16_accumulation` to `True` can increase performance on certain GPUs (Volta or newer) but reduces numerical precision and increases overflow risk.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntorch.backends.cuda.matmul.allow_fp16_accumulation = True\n```\n\n----------------------------------------\n\nTITLE: SubgraphMatcher Usage Example\nDESCRIPTION: Shows how to use SubgraphMatcher to find pattern matches in a larger model graph\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n\nclass LargeModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._weight = torch.nn.Parameter(torch.ones(3, 3))\n        self._bias = torch.nn.Parameter(torch.ones(3, 3))\n\n    def forward(self, x):\n        return torch.ops.aten.addmm.default(self._bias, x, self._weight)\n\nlarge_model_graph = torch.export(LargeModel(), inputs).graph\n\nclass PatternModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._weight_1 = torch.nn.Parameter(torch.ones(5, 5))\n        self._bias_1 = torch.nn.Parameter(torch.ones(5, 5))\n\n    def forward(self, x):\n        return torch.ops.aten.addmm.default(self._bias_1, x, self._weight_1)\n\npattern_graph = torch.export(PatternModel(), inputs).graph\n\nsubgraph_matcher = SubgraphMatcher(pattern_graph)\nmatch_result = subgraph_matcher.match(large_model_graph)\n```\n\n----------------------------------------\n\nTITLE: Tracking NLL Loss Operations in PyTorch\nDESCRIPTION: Records of negative log likelihood loss forward and backward operations. Shows tensor shapes for input predictions, target labels, and loss values with parameters for reduction and ignore index.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for test_jit Executable in CMake\nDESCRIPTION: Sets the `JIT_TEST_DEPENDENCIES` CMake variable to a list containing the core dependencies for the `test_jit` executable: `torch`, `gtest`, and the previously defined custom libraries `jitbackend_test` and `backend_with_compiler`. It also conditionally appends `onnx_library` if building with MSVC.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/jit/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nset(JIT_TEST_DEPENDENCIES torch gtest jitbackend_test backend_with_compiler)\n\nif(MSVC)\n  list(APPEND JIT_TEST_DEPENDENCIES onnx_library)\nendif(MSVC)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor View and Reshape Operations\nDESCRIPTION: Series of tensor view/reshape operations using unsafe_view to modify tensor dimensions while maintaining the underlying data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten._unsafe_view.default((T([1024, 16, 8, 8, 2, 2], f16), [1024, 16, 64, 4]))\naten._unsafe_view.default((T([128, 384, 2, 2, 12, 12], f16), [1024, 48, 4, 144]))\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorch DataPipe Imports\nDESCRIPTION: Basic setup for working with PyTorch DataPipes by importing required modules.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/utils/data/dataframes_pipes.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom importlib import reload\nimport torch\nreload(torch)\nfrom torch.utils.data import IterDataPipe\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows the frequency and tensor shapes for batch normalization operations. It includes input tensors, scale and bias parameters, and running mean/variance statistics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 96, 17, 17], f16), T([128, 96, 17, 17], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 0.001, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: HIPifying PyTorch Codebase for AMD GPUs (Python)\nDESCRIPTION: The build_amd.py script in the amd_build directory is the top-level entry point for HIPifying the PyTorch codebase, enabling support for AMD GPUs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/README.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\npython amd_build/build_amd.py\n```\n\n----------------------------------------\n\nTITLE: Collecting C10 HIP Source and Header Files\nDESCRIPTION: Gathers all C++ source files and header files for the C10 HIP library using glob patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/hip/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfile(GLOB C10_HIP_SRCS\n        *.cpp\n        *.cc\n        impl/*.cpp\n        impl/*.cc\n        )\n\n# Mark the cc files as HIP files, so we call the compiler.  (They have to be\n# suffixed with cc, because the hcc compiler won't accept them otherwise.)\nfile(GLOB __c10_hip_srcs_cpp *.cc impl/*.cc)\nset_source_files_properties(${__c10_hip_srcs_cpp} PROPERTIES HIP_SOURCE_PROPERTY_FORMAT 1)\n\nfile(GLOB_RECURSE C10_HIP_HEADERS *.h)\n```\n\n----------------------------------------\n\nTITLE: Citing Functorch Library (BibTeX)\nDESCRIPTION: Provides the recommended BibTeX entry for citing the functorch library in academic publications or research papers. This entry includes essential metadata such as authors (Horace He, Richard Zou), title, publication source (GitHub URL), and year (2021).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bibtex\nCODE:\n```\n```bibtex\n@Misc{functorch2021,\n  author =       {Horace He, Richard Zou},\n  title =        {functorch: JAX-like composable function transforms for PyTorch},\n  howpublished = {\\url{https://github.com/pytorch/functorch}},\n  year =         {2021}\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplications with addmm and mm in PyTorch (Python)\nDESCRIPTION: Presents usage of batched matrix-matrix multiplications (addmm, mm) for linear layers and dense projections. Requires two or three tensor arguments, often with specified strides for efficient computation. Main parameters are input tensors, expected to have compatible shapes for broadcast or dot-product operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/RobertaForCausalLM_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 49, ((T([768], f16), T([512, 768], f16), T([768, 768], f16, stride=(1, 768))), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 12, ((T([3072], f16), T([512, 768], f16), T([768, 3072], f16, stride=(1, 768))), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 12, ((T([768], f16), T([512, 3072], f16), T([3072, 768], f16, stride=(1, 3072))), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([30522], f16), T([512, 768], f16), T([768, 30522], f16, stride=(1, 768))), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([512, 30522], f16), T([30522, 768], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([30522, 512], f16, stride=(1, 30522)), T([512, 768], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 49, ((T([512, 768], f16), T([768, 768], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 49, ((T([768, 512], f16, stride=(1, 768)), T([512, 768], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([512, 768], f16), T([768, 3072], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([768, 512], f16, stride=(1, 768)), T([512, 3072], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([512, 3072], f16), T([3072, 768], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 12, ((T([3072, 512], f16, stride=(1, 3072)), T([512, 768], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Command for Source Code Generation in CMake\nDESCRIPTION: Defines a custom command in CMake that executes the previously defined `GEN_COMMAND` (Python script) to generate C++ source files. Specifies the output files (`GEN_COMMAND_sources`), the command itself, dependencies (Python scripts, YAML files, templates), and the working directory. This command runs during the build process before compiling the sources.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/edge/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_command(\n        COMMENT \"Generating sources\"\n        OUTPUT ${GEN_COMMAND_sources}\n        COMMAND ${GEN_COMMAND}\n        DEPENDS\n        ${all_python}\n        ${TORCH_ROOT}/aten/src/ATen/native/native_functions.yaml\n        ${TORCH_ROOT}/aten/src/ATen/native/tags.yaml\n        ${TEST_ROOT}/templates/Functions.h\n        ${TEST_ROOT}/templates/NativeFunctions.h\n        ${TEST_ROOT}/templates/RegisterCodegenUnboxedKernels.cpp\n        ${TEST_ROOT}/templates/RegisterDispatchKeyCustomOps.cpp\n        WORKING_DIRECTORY ${TORCH_ROOT}\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Example Inputs for BatchNorm, Pooling, Activations in PyTorch (Python)\nDESCRIPTION: Defines serialized tuples representing the input and parameter specifications for ATen operators, such as native_batch_norm, native_batch_norm_backward, max_pool2d_with_indices, and various activation functions. These inputs are typically used for automated test generation, tracing, or benchmarking, where the tuples encode tensor shapes, types, hyperparameters like kernel sizes, strides, paddings, and boolean flags. All tensors predominantly use the 'f16' dtype; scalar and list inputs are used to set dimensions, flags, and operation settings. Each tuple can be parsed to instantiate test tensors and call the corresponding operator; limitations include highly fixed shapes and a focus on batch sizes of 32 and standard CNN feature map layouts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dpn107_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 296, 56, 56], f16), T([32, 128, 56, 56], f16), T([296, 128, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 128, 112, 112], f16), T([32, 3, 224, 224], f16), T([128, 3, 7, 7], f16), [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([32, 2688, 7, 7], f16, stride=(2688, 1, 0, 0)), 49), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.elu.default\ncnt: 1, ((T([32, 2688, 7, 7], f16), 1.0), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.elu_backward.default\ncnt: 1, ((T([32, 2688, 7, 7], f16), 1.0, 1, 1, False, T([32, 2688, 7, 7], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([32], i64),), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([32, 128, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([32, 128, 56, 56], f16), T([32, 128, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([32, 128, 56, 56], i64)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([32, 2688, 7, 7], f16), [-1, -2], True), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([32, 128, 112, 112], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([32, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 8, ((T([32, 200, 56, 56], f16), T([200], f16), T([200], f16), T([200], f16), T([200], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 316, 56, 56], f16), T([316], f16), T([316], f16), T([316], f16), T([316], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 336, 56, 56], f16), T([336], f16), T([336], f16), T([336], f16), T([336], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 356, 56, 56], f16), T([356], f16), T([356], f16), T([356], f16), T([356], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([32, 376, 56, 56], f16), T([376], f16), T([376], f16), T([376], f16), T([376], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 400, 56, 56], f16), T([400], f16), T([400], f16), T([400], f16), T([400], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 15, ((T([32, 400, 28, 28], f16), T([400], f16), T([400], f16), T([400], f16), T([400], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 704, 28, 28], f16), T([704], f16), T([704], f16), T([704], f16), T([704], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 768, 28, 28], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 832, 28, 28], f16), T([832], f16), T([832], f16), T([832], f16), T([832], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 896, 28, 28], f16), T([896], f16), T([896], f16), T([896], f16), T([896], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 960, 28, 28], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1024, 28, 28], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1088, 28, 28], f16), T([1088], f16), T([1088], f16), T([1088], f16), T([1088], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([32, 1152, 28, 28], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 800, 28, 28], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 39, ((T([32, 800, 14, 14], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1216, 14, 14], f16), T([1216], f16), T([1216], f16), T([1216], f16), T([1216], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1280, 14, 14], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1344, 14, 14], f16), T([1344], f16), T([1344], f16), T([1344], f16), T([1344], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1408, 14, 14], f16), T([1408], f16), T([1408], f16), T([1408], f16), T([1408], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1472, 14, 14], f16), T([1472], f16), T([1472], f16), T([1472], f16), T([1472], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1536, 14, 14], f16), T([1536], f16), T([1536], f16), T([1536], f16), T([1536], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([32, 1600, 14, 14], f16), T([1600], f16), T([1600], f16), T([1600], f16), T([1600], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1664, 14, 14], f16), T([1664], f16), T([1664], f16), T([1664], f16), T([1664], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1728, 14, 14], f16), T([1728], f16), T([1728], f16), T([1728], f16), T([1728], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1792, 14, 14], f16), T([1792], f16), T([1792], f16), T([1792], f16), T([1792], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1856, 14, 14], f16), T([1856], f16), T([1856], f16), T([1856], f16), T([1856], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1920, 14, 14], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1984, 14, 14], f16), T([1984], f16), T([1984], f16), T([1984], f16), T([1984], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2048, 14, 14], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2112, 14, 14], f16), T([2112], f16), T([2112], f16), T([2112], f16), T([2112], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2176, 14, 14], f16), T([2176], f16), T([2176], f16), T([2176], f16), T([2176], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2240, 14, 14], f16), T([2240], f16), T([2240], f16), T([2240], f16), T([2240], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2304, 14, 14], f16), T([2304], f16), T([2304], f16), T([2304], f16), T([2304], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2368, 14, 14], f16), T([2368], f16), T([2368], f16), T([2368], f16), T([2368], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([32, 2432, 14, 14], f16), T([2432], f16), T([2432], f16), T([2432], f16), T([2432], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 5, ((T([32, 1600, 7, 7], f16), T([1600], f16), T([1600], f16), T([1600], f16), T([1600], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2432, 7, 7], f16), T([2432], f16), T([2432], f16), T([2432], f16), T([2432], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2560, 7, 7], f16), T([2560], f16), T([2560], f16), T([2560], f16), T([2560], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2688, 7, 7], f16), T([2688], f16), T([2688], f16), T([2688], f16), T([2688], f16), True, 0.1, 0.001), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([32, 2688, 7, 7], f16), T([32, 2688, 7, 7], f16), T([2688], f16), T([2688], f16), T([2688], f16), T([2688], f32), T([2688], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 5, ((T([32, 1600, 7, 7], f16), T([32, 1600, 7, 7], f16), T([1600], f16), T([1600], f16), T([1600], f16), T([1600], f32), T([1600], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2560, 7, 7], f16), T([32, 2560, 7, 7], f16), T([2560], f16), T([2560], f16), T([2560], f16), T([2560], f32), T([2560], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2432, 7, 7], f16), T([32, 2432, 7, 7], f16), T([2432], f16), T([2432], f16), T([2432], f16), T([2432], f32), T([2432], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([32, 1600, 14, 14], f16), T([32, 1600, 14, 14], f16), T([1600], f16), T([1600], f16), T([1600], f16), T([1600], f32), T([1600], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([32, 2432, 14, 14], f16), T([32, 2432, 14, 14], f16), T([2432], f16), T([2432], f16), T([2432], f16), T([2432], f32), T([2432], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 39, ((T([32, 800, 14, 14], f16), T([32, 800, 14, 14], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f32), T([800], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2368, 14, 14], f16), T([32, 2368, 14, 14], f16), T([2368], f16), T([2368], f16), T([2368], f16), T([2368], f32), T([2368], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2304, 14, 14], f16), T([32, 2304, 14, 14], f16), T([2304], f16), T([2304], f16), T([2304], f16), T([2304], f32), T([2304], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2240, 14, 14], f16), T([32, 2240, 14, 14], f16), T([2240], f16), T([2240], f16), T([2240], f16), T([2240], f32), T([2240], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2176, 14, 14], f16), T([32, 2176, 14, 14], f16), T([2176], f16), T([2176], f16), T([2176], f16), T([2176], f32), T([2176], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2112, 14, 14], f16), T([32, 2112, 14, 14], f16), T([2112], f16), T([2112], f16), T([2112], f16), T([2112], f32), T([2112], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 2048, 14, 14], f16), T([32, 2048, 14, 14], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1984, 14, 14], f16), T([32, 1984, 14, 14], f16), T([1984], f16), T([1984], f16), T([1984], f16), T([1984], f32), T([1984], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1920, 14, 14], f16), T([32, 1920, 14, 14], f16), T([1920], f16), T([1920], f16), T([1920], f16), T([1920], f32), T([1920], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1856, 14, 14], f16), T([32, 1856, 14, 14], f16), T([1856], f16), T([1856], f16), T([1856], f16), T([1856], f32), T([1856], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1792, 14, 14], f16), T([32, 1792, 14, 14], f16), T([1792], f16), T([1792], f16), T([1792], f16), T([1792], f32), T([1792], f32), True, 0.001, [True, True, True]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 1728, 14, 14], f16), T([32, 1728, 14, 14], f16), T([1728], f16), T([1728], f16), T([1728], f16), T([1728], f32), T([1728], f32), True, 0.001, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication in PyTorch\nDESCRIPTION: This snippet shows various matrix multiplication operations using aten.mm.default. It demonstrates different tensor shapes and data types, primarily using half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([1024, 1], f16), T([1, 4000], f16)), {})\ncnt: 1, ((T([1, 1024], f16), T([1024, 4000], f16)), {})\ncnt: 8, ((T([1024, 4000], f16), T([4000, 4000], f16)), {})\ncnt: 8, ((T([4000, 1024], f16, stride=(1, 4000)), T([1024, 4000], f16)), {})\ncnt: 1, ((T([1024, 4000], f16), T([4000, 31068], f16)), {})\ncnt: 1, ((T([4000, 1024], f16, stride=(1, 4000)), T([1024, 31068], f16)), {})\ncnt: 1, ((T([1024, 192], f16), T([192, 1500], f16)), {})\ncnt: 1, ((T([192, 1024], f16, stride=(1, 192)), T([1024, 1500], f16)), {})\ncnt: 2, ((T([1024, 1500], f16), T([1500, 1500], f16)), {})\ncnt: 2, ((T([1500, 1024], f16, stride=(1, 1500)), T([1024, 1500], f16)), {})\ncnt: 1, ((T([1500, 1024], f16, stride=(1, 1500)), T([1024, 2000], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Channel Shuffle Test for QNNPACK in CMake\nDESCRIPTION: Creates and configures the channel shuffle test executable with C++14 standard requirements, includes the necessary directories, and links against required libraries like pytorch_qnnpack, cpuinfo, and gtest.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(channel-shuffle-test test/channel-shuffle.cc)\nset_target_properties(channel-shuffle-test PROPERTIES\n  CXX_STANDARD 14\n  CXX_STANDARD_REQUIRED YES\n  CXX_EXTENSIONS NO)\ntarget_include_directories(channel-shuffle-test PRIVATE src test)\ntarget_link_libraries(channel-shuffle-test PRIVATE pytorch_qnnpack cpuinfo gtest gtest_main)\nadd_test(channel-shuffle-test channel-shuffle-test)\n```\n\n----------------------------------------\n\nTITLE: In-place Tensor Addition Operations in PyTorch\nDESCRIPTION: This snippet shows the in-place tensor addition operations (aten.add_.Tensor) with various tensor shapes. These operations modify tensors in-place without creating new ones, used at different spatial resolutions in the model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add_.Tensor\ncnt: 2, ((T([96, 64, 32, 32], f16), T([96, 64, 32, 32], f16)), {})\ncnt: 2, ((T([96, 128, 16, 16], f16), T([96, 128, 16, 16], f16)), {})\ncnt: 2, ((T([96, 256, 8, 8], f16), T([96, 256, 8, 8], f16)), {})\ncnt: 2, ((T([96, 512, 4, 4], f16), T([96, 512, 4, 4], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Mean Calculation along Dimensions in PyTorch\nDESCRIPTION: Records of mean operations calculated along specific dimensions of tensors. This operation computes the mean over the last two dimensions (-1, -2) of a tensor with shape [128, 1280, 7, 7], keeping the dimensions in the result (keepdim=True).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 1280, 7, 7], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling Pooling Operations in PyTorch\nDESCRIPTION: This snippet shows the tensor shapes and parameters for max pooling operations in the model. It includes both forward and backward passes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([128, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})\n\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([128, 64, 56, 56], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking Matrix Multiplication in Classification Layer\nDESCRIPTION: Documents the usage of matrix multiplication for the final classification layer of the network. Shows the operation between a feature tensor of shape [128, 1280] and a weight matrix of [1280, 1000], with 1000 classes output.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 1280], f16), T([1280, 1000], f16, stride=(1, 1280))), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Shared Memory Manager\nDESCRIPTION: Sets up the build configuration for the shared memory manager executable, including dependencies and installation settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/lib/libshm/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(torch_shm_manager manager.cpp)\nif(BUILD_LIBTORCHLESS)\n  target_link_libraries(torch_shm_manager PRIVATE shm ${C10_LIB})\nelse()\n  target_link_libraries(torch_shm_manager PRIVATE shm c10)\nendif()\nset_target_properties(torch_shm_manager PROPERTIES\n  INSTALL_RPATH \"${_rpath_portable_origin}/../lib\")\n\ninstall(TARGETS shm LIBRARY DESTINATION ${LIBSHM_INSTALL_LIB_SUBDIR})\ninstall(FILES libshm.h DESTINATION \"include\")\ninstall(TARGETS torch_shm_manager DESTINATION \"bin\")\n```\n\n----------------------------------------\n\nTITLE: Accessing PyTorch Version Information in C++\nDESCRIPTION: Example code demonstrating how to access PyTorch version information using version macros. Shows both accessing individual version components (major, minor, patch) and the complete version string using TORCH_VERSION macro.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/versioning.rst#2025-04-22_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/torch.h>\n#include <iostream>\n\nint main() {\n  std::cout << \"PyTorch version from parts: \"\n    << TORCH_VERSION_MAJOR << \".\"\n    << TORCH_VERSION_MINOR << \".\"\n    << TORCH_VERSION_PATCH << std::endl;\n  std::cout << \"PyTorch version: \" << TORCH_VERSION << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Analyzing Element-wise Multiplication in PyTorch\nDESCRIPTION: This snippet shows the usage of element-wise multiplication operations in the model. It includes the shapes of the tensors being multiplied and the frequency of these operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.mul.Tensor\ncnt: 2, ((T([128, 72, 28, 28], f16), T([128, 72, 1, 1], f16)), {})\ncnt: 2, ((T([128, 120, 28, 28], f16), T([128, 120, 1, 1], f16)), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([128, 480, 1, 1], f16)), {})\ncnt: 2, ((T([128, 672, 14, 14], f16), T([128, 672, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations in PyTorch\nDESCRIPTION: Records of matrix multiplication operations between tensors using the aten.mm.default operator. Two operations are shown with different tensor shapes, demonstrating forward and potentially backward passes in a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 1280], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1280], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Backward Pass in PyTorch\nDESCRIPTION: Backward pass operations for convolutions, including gradient calculations for inputs and weights. These are used during the training process to update network parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 2, ((T([128, 2048, 8, 8], f16), T([128, 512, 8, 8], f16), T([2048, 512, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Compiler-Specific Options Configuration\nDESCRIPTION: Sets compiler-specific options to handle warnings and errors, particularly for non-MSVC compilers. Includes special handling for GCC 12+ and various warning suppressions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/api/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT MSVC)\n  target_compile_options_if_supported(test_api \"-Wno-missing-braces\")\n  target_compile_options_if_supported(test_api \"-Wno-maybe-uninitialized\")\n  target_compile_options_if_supported(test_api \"-Wno-unused-but-set-parameter\")\n\n  if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 12)\n    target_compile_options_if_supported(test_api \"-Wno-error=nonnull\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Comparison Operations in TorchScript\nDESCRIPTION: Specifies the syntax for comparison operations in TorchScript, including less than, greater than, equality, and others. These yield boolean values or Tensors and can be chained.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ncomparison    ::=  or_expr (comp_operator or_expr)*\ncomp_operator ::=  '<' | '>' | '==' | '>=' | '<=' | '!=' | 'is' ['not'] | ['not'] 'in'\n```\n\n----------------------------------------\n\nTITLE: Performance Metrics Table in Markdown\nDESCRIPTION: Markdown table comparing performance metrics between original and h2d_d2h_threads experiments, including warmup latency, average latency, throughput, and GPU utilization measures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/results/output_128_true.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Experiment | Warmup_latency (s) | Average_latency (s) | Throughput (samples/sec) | GPU Utilization (%) |\n| ---------- | ------------------ | ------------------- | ------------------------ | ------------------- |\n| original | 14.358 +/- 0.981 | 14.250 +/- 0.758 | 522.998 +/- 20.830 | 55.501 +/- 2.123 |\n| h2d_d2h_threads | 12.520 +/- 0.253 | 12.774 +/- 0.714 | 600.578 +/- 27.662 | 61.534 +/- 3.653 |\n```\n\n----------------------------------------\n\nTITLE: Adding Generated Files to Git (Shell)\nDESCRIPTION: The git_add_generated_dirs.sh script forces the addition of generated files to the Git index, allowing for convenient diff comparisons when working on code generation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/README.md#2025-04-22_snippet_3\n\nLANGUAGE: Shell\nCODE:\n```\n./git_add_generated_dirs.sh\n```\n\n----------------------------------------\n\nTITLE: Defining a Complex Class with Multiple Inheritance\nDESCRIPTION: A class that inherits from multiple built-in types with a docstring and nested class definitions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass ImpossibleCombo(\n    set,\n    tuple,\n    int,\n):\n    # We could have comments\n    # before the doc comment\n    \"\"\"This docstring, while short, is enough\"\"\"\n\n    def needs_docs(self):\n        def not_short():\n            class Long:\n                a = 1\n                b = 1\n                c = 1\n                d = 1\n                e = 1\n\n            class Short:\n                pass\n```\n\n----------------------------------------\n\nTITLE: Appending Vulkan C++ Compiler Flags in CMake\nDESCRIPTION: If the USE_VULKAN option is enabled, this snippet appends the `-DUSE_VULKAN` and `-DUSE_VULKAN_API` preprocessor definitions to CMAKE_CXX_FLAGS. Additionally, it conditionally appends `-DUSE_VULKAN_FP16_INFERENCE` if USE_VULKAN_FP16_INFERENCE is enabled, and `-DUSE_VULKAN_RELAXED_PRECISION` if USE_VULKAN_RELAXED_PRECISION is enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_40\n\nLANGUAGE: cmake\nCODE:\n```\nif(USE_VULKAN)\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_VULKAN\")\n  string(APPEND CMAKE_CXX_FLAGS \" -DUSE_VULKAN_API\")\n\n  if(USE_VULKAN_FP16_INFERENCE)\n    string(APPEND CMAKE_CXX_FLAGS \" -DUSE_VULKAN_FP16_INFERENCE\")\n  endif()\n\n  if(USE_VULKAN_RELAXED_PRECISION)\n    string(APPEND CMAKE_CXX_FLAGS \" -DUSE_VULKAN_RELAXED_PRECISION\")\n  endif()\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Log Entry for Tensor Pair (Shape [128, 128, 56, 56], f16)\nDESCRIPTION: Logs the occurrence (count 3) of a tensor pair, both with shape [128, 128, 56, 56] and dtype f16, using default strides. Likely generated during PyTorch execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: text\nCODE:\n```\ncnt: 3, ((T([128, 128, 56, 56], f16), T([128, 128, 56, 56], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Deconvolution Test for QNNPACK in CMake\nDESCRIPTION: Creates and configures the deconvolution test executable with C++14 standard requirements, includes the necessary directories, and links against required libraries like pytorch_qnnpack, cpuinfo, fp16, and gtest.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(deconvolution-test test/deconvolution.cc)\nset_target_properties(deconvolution-test PROPERTIES\n  CXX_STANDARD 14\n  CXX_STANDARD_REQUIRED YES\n  CXX_EXTENSIONS NO)\ntarget_include_directories(deconvolution-test PRIVATE src test)\ntarget_link_libraries(deconvolution-test PRIVATE pytorch_qnnpack cpuinfo fp16 gtest gtest_main)\nadd_test(deconvolution-test deconvolution-test)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Summation Operations\nDESCRIPTION: Dimension-specific summation operations on tensors of various shapes, maintaining gradient computations with keep_dim=True flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/levit_128_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 1000], f16), [0], True), {})\n((T([128, 128, 640], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of native_batch_norm_backward.default operator with various tensor shapes and data types. It includes counts of each unique configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swsl_resnext101_32x16d_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 4, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16), T([2048], f16), T([2048], f16), T([2048], f16), T([2048], f32), T([2048], f32), True, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([32, 4096, 7, 7], f16), T([32, 4096, 7, 7], f16), T([4096], f16), T([4096], f16), T([4096], f16), T([4096], f32), T([4096], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([32, 4096, 14, 14], f16), T([32, 4096, 14, 14], f16), T([4096], f16), T([4096], f16), T([4096], f16), T([4096], f32), T([4096], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and Related Packages on Intel GPU\nDESCRIPTION: Facilitates the installation of PyTorch, Torchvision, and Torchaudio using pip commands, specifically tailored for Intel GPUs with both release and nightly wheels. Prerequisites include having the Intel GPU driver installed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/get_start_xpu.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu\n```\n\nLANGUAGE: Python\nCODE:\n```\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations\nDESCRIPTION: These snippets represent convolution operations with different input and output shapes, kernel sizes, strides, and padding. They use float16 (f16) precision tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 128, 48, 48], f16), T([128, 128, 48, 48], f16), T([128, 128, 1, 1], f16), [128], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 128, 48, 48], f16), T([128, 64, 97, 97], f16), T([128, 64, 3, 3], f16), [128], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 64, 96, 96], f16), T([128, 32, 96, 96], f16), T([64, 32, 3, 3], f16), [64], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 32, 96, 96], f16), T([128, 16, 96, 96], f16), T([32, 16, 3, 3], f16), [32], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 16, 96, 96], f16), T([128, 3, 193, 193], f16), T([16, 3, 3, 3], f16), [16], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations - PyTorch\nDESCRIPTION: Various matrix multiplication operations (bmm, mm) used in transformer layers for computing attention scores and value projections. Includes operations with different stride patterns and tensor shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_GPT2_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten.bmm.default((T([48, 512, 64], f16), T([48, 64, 512], f16)), {})\naten.mm.default((T([2048, 768], f16), T([768, 50257], f16, stride=(1, 768))), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Split Operations\nDESCRIPTION: Documents the tensor splitting operations (aten.split.Tensor) with various input shapes and split sizes, showing how tensors are divided along specified dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2net50_14w_8s_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.split.Tensor\ncnt: 3, ((T([128, 112, 56, 56], f16), 14, 1), {})\ncnt: 1, ((T([128, 224, 56, 56], f16), 28, 1), {})\ncnt: 3, ((T([128, 224, 28, 28], f16), 28, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Tensor Cloning Operations in PyTorch\nDESCRIPTION: The snippet examines the use of the ATen clone operator, emphasizing instances where tensors are duplicated. Cloning is key when mutable operations are performed to maintain the original data structure, enabling controlled manipulation in workflows.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DistilBertForMaskedLM_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 2, ((T([16, 128], i64),), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing Thread-Safe Metrics Collection in PyTorch\nDESCRIPTION: Uses a semaphore to ensure thread-safe writing to metrics_dict in metrics_thread and gpu_utilization_thread. This change improves the accuracy and reliability of performance metrics collection.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/inference/CHANGELOG.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nsemaphore.acquire()\ntry:\n    # Write to metrics_dict\nfinally:\n    semaphore.release()\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations Analysis\nDESCRIPTION: Log of PyTorch tensor operations showing operation counts, tensor shapes, and parameters. Includes operations like hardtanh_backward, batch normalization, matrix multiplication and loss calculations with mixed precision (fp16) training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv2_100_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 32, 112, 112], f16), 0.0, 6.0), {})\ncnt: 1, ((T([128, 96, 112, 112], f16), 0.0, 6.0), {})\ncnt: 1, ((T([128, 96, 56, 56], f16), 0.0, 6.0), {})\ncnt: 3, ((T([128, 144, 56, 56], f16), 0.0, 6.0), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), 0.0, 6.0), {})\ncnt: 5, ((T([128, 192, 28, 28], f16), 0.0, 6.0), {})\ncnt: 1, ((T([128, 192, 14, 14], f16), 0.0, 6.0), {})\ncnt: 8, ((T([128, 384, 14, 14], f16), 0.0, 6.0), {})\ncnt: 5, ((T([128, 576, 14, 14], f16), 0.0, 6.0), {})\ncnt: 1, ((T([128, 576, 7, 7], f16), 0.0, 6.0), {})\ncnt: 6, ((T([128, 960, 7, 7], f16), 0.0, 6.0), {})\ncnt: 1, ((T([128, 1280, 7, 7], f16), 0.0, 6.0), {})\n```\n\n----------------------------------------\n\nTITLE: Basic MulConstant Autograd Function Implementation\nDESCRIPTION: Implements a basic multiplication constant operation with forward and backward passes using PyTorch's Function class.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        # ctx is a context object that can be used to stash information\n        # for backward computation\n        tensor, constant = inputs\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None\n```\n\n----------------------------------------\n\nTITLE: Sum Along Dimensions in PyTorch\nDESCRIPTION: Using aten.sum.SymInt, sums elements along dimension 0 of tensor shaped [512, 2], the operation allows selective aggregation along desired axes, typically for loss calculations or reductions in complex models.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([512, 2], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Tensor Cloning in PyTorch\nDESCRIPTION: Record of aten.clone.default operator call for creating a deep copy of an input tensor. This operation creates a copy of the input image tensor with batch size 32, 3 channels, and 224x224 resolution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([32, 3, 224, 224], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Performing Sum Operations in PyTorch\nDESCRIPTION: This snippet shows the usage of sum operations on tensors. It includes both symbolic and default sum operations, with details on input tensor shapes and configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.sum.SymInt\ncnt: 1, ((T([32, 1000], f16, stride=(0, 0)), [0], True), {})\nOperator: aten.sum.default\ncnt: 1, ((T([32, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Operator Statistics\nDESCRIPTION: Log of PyTorch operator usage statistics showing operator names, call counts, and tensor shapes. Includes convolution, log_softmax, addition, matrix multiplication and other core operators.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/lcnet_050_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([128, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([128, 1000], f16), T([128, 1000], f16), 1, f16), {})\n# ... [Additional operator stats]\n```\n\n----------------------------------------\n\nTITLE: Building libtorch Using Python Script\nDESCRIPTION: This snippet demonstrates how to build libtorch using a Python script from the tools package. It creates a separate build directory to avoid polluting source directories.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/libtorch.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd <pytorch_root>\n\n# Make a new folder to build in to avoid polluting the source directories\nmkdir build_libtorch && cd build_libtorch\n\n# You might need to export some required environment variables here.\n# Normally setup.py sets good default env variables, but you'll have to do\n# that manually.\npython ../tools/build_libtorch.py\n```\n\n----------------------------------------\n\nTITLE: Complex Vector Alternative Approach in Gradcheck\nDESCRIPTION: Mathematical explanation of why using a complex vector u' would be less efficient for numerical evaluation in complex gradcheck, requiring twice as many real-to-real finite difference evaluations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/gradcheck.rst#2025-04-22_snippet_3\n\nLANGUAGE: math\nCODE:\n```\n\\begin{aligned}\n    2*CW u' &= (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})(ur' + i ui') \\\\\n            &= \\frac{\\partial y}{\\partial a} ur' + i \\frac{\\partial y}{\\partial a} ui' + i \\frac{\\partial y}{\\partial b} ur' - \\frac{\\partial y}{\\partial b} ui'\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Specifying ATen Copy, Division, and Arithmetic Operator Cases - Python\nDESCRIPTION: This snippet block lists parameterizations for element-wise or broadcasted tensor operations, covering copy, division, element-wise multiplication, and reduction operations using float16 tensors. It includes explicit stride information, axes for reductions, broadcastability, and tensor shapes. Dependencies are PyTorch tensor representations and expectations for ATen operator signatures.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 3, ((T([128, 960, 7, 7], f16, stride=(960, 1, 0, 0)), 49), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 672, 7, 7], f16, stride=(672, 1, 0, 0)), 49), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 672, 14, 14], f16, stride=(672, 1, 0, 0)), 196), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 480, 14, 14], f16, stride=(480, 1, 0, 0)), 196), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 120, 28, 28], f16, stride=(120, 1, 0, 0)), 784), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 72, 28, 28], f16, stride=(72, 1, 0, 0)), 784), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking Tensor Copy and Clone Operations via ATen in PyTorch (Python)\nDESCRIPTION: Monitors usage of ATen copy_ and clone operations to record memory duplication and assignment for model tensors. Dependencies: PyTorch, at least two tensors of matching shapes/dtypes for copy, one for clone. Expects tensors as input and outputs copied or cloned tensors of the same shape/type, highlighting memory management steps in model code. Limitation: Only metadata, not the source implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnet18_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([16, 3, 224, 224], f16), T([16, 3, 224, 224], f16)), {})\n```\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.clone.default\ncnt: 1, ((T([16, 3, 224, 224], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Log Entry for Tensor Pair (Shape [128, 64, 112, 112], f16)\nDESCRIPTION: Logs the occurrence (count 1) of a tensor pair, both with shape [128, 64, 112, 112] and dtype f16, using default strides. Likely generated during PyTorch execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([128, 64, 112, 112], f16), T([128, 64, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Log Entry for Tensor Pair (Shape [128, 64, 28, 28], f16)\nDESCRIPTION: Logs the occurrence (count 6) of a tensor pair, both with shape [128, 64, 28, 28] and dtype f16, using default strides. Likely generated during PyTorch execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\ncnt: 6, ((T([128, 64, 28, 28], f16), T([128, 64, 28, 28], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Auto-Convert Example with FakeTensorMode\nDESCRIPTION: Illustrates a potential issue with automatic conversion of real tensors to fake tensors within FakeTensorMode context.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fake_tensor.rst#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith FakeTensorMode():\n    real_tensor.t_()\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation Rules for Test Executables in CMake\nDESCRIPTION: Sets up installation rules for the test executables and PDB files (for MSVC builds). The installation is conditional on the INSTALL_TEST variable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/nnc/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(INSTALL_TEST)\n  install(TARGETS test_mobile_nnc DESTINATION bin)\n  install(TARGETS aot_model_compiler_test DESTINATION bin)\n  # Install PDB files for MSVC builds\n  if(MSVC AND BUILD_SHARED_LIBS)\n    install(FILES $<TARGET_PDB_FILE:test_mobile_nnc> DESTINATION bin OPTIONAL)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for PyTorch Code Coverage Build\nDESCRIPTION: This snippet shows how to configure CMake to build PyTorch with code coverage options enabled. It sets USE_CPP_CODE_COVERAGE and BUILD_TEST to ON, and sets CMAKE_BUILD_TYPE to Debug for accurate results with gcc.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# in build/ folder (all build artifacts must in `build/` folder)\ncmake .. -DUSE_CPP_CODE_COVERAGE=ON -DBUILD_TEST=ON -DCMAKE_BUILD_TYPE=Debug\n```\n\n----------------------------------------\n\nTITLE: NLL Loss Operations\nDESCRIPTION: Negative Log Likelihood Loss calculations in both forward and backward passes, operating on classification outputs with 1000 classes and batch size of 64.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\n((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Negative Log Likelihood Loss Calculation in PyTorch (Python)\nDESCRIPTION: Calculates the negative log likelihood using aten.nll_loss_forward suitable for classification model losses, crucial in scenarios where models need to attribute probabilities to given classes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\naten.nll_loss_forward.default\ncnt: 1, ((T([16, 2], f16), T([16], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Google Test Compile Definition in CMake\nDESCRIPTION: Adds the `USE_GTEST` preprocessor definition privately to the `test_lazy` target. This is marked as temporary, likely for compatibility during a transition phase.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/lazy/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\n# TODO temporary until we can delete the old gtest polyfills.\ntarget_compile_definitions(test_lazy PRIVATE USE_GTEST)\n```\n\n----------------------------------------\n\nTITLE: Basic ScalarTensor Implementation\nDESCRIPTION: Initial implementation of a ScalarTensor class that represents a 2D scalar tensor with diagonal values. This version doesn't include torch function integration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/extending.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass ScalarTensor(object):\n    def __init__(self, N, value):\n        self._N = N\n        self._value = value\n\n    def __repr__(self):\n        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n    def tensor(self):\n        return self._value * torch.eye(self._N)\n```\n\n----------------------------------------\n\nTITLE: Model Transformation Comparison with ResNet18\nDESCRIPTION: Demonstrates incorrect usage of equality comparison between transformed models, showing why torch.allclose() should be used instead of == operator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/fx.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"\n\nassert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Tensor Concatenation Operations\nDESCRIPTION: This snippet demonstrates the usage of the aten.cat.default operator for concatenating tensors along specified dimensions. It shows various tensor shapes and concatenation scenarios.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/Background_Matting_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.cat.default\ncnt: 2, (([T([3, 256, 128, 128], f16), T([3, 256, 128, 128], f16)], 1), {})\ncnt: 1, (([T([3, 256, 128, 128], f16), T([3, 256, 128, 128], f16, stride=(4194304, 1, 32768, 256))], 1), {})\ncnt: 1, (([T([3, 64, 128, 128], f16), T([3, 64, 128, 128], f16), T([3, 64, 128, 128], f16)], 1), {})\ncnt: 1, (([T([3, 256, 128, 128], f16), T([3, 192, 128, 128], f16)], 1), {})\ncnt: 1, (([T([3, 128, 256, 256], f16), T([3, 128, 256, 256], f16)], 1), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Convolution Operations\nDESCRIPTION: Convolution operations with different kernel sizes, strides and padding configurations operating on 4D tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\naten.convolution.default((T([128, 3, 256, 256], f16), T([24, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1))\naten.convolution.default((T([128, 24, 128, 128], f16), T([32, 24, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1))\n```\n\n----------------------------------------\n\nTITLE: Referencing TH Abstraction Violation in C++\nDESCRIPTION: This snippet demonstrates how to reference the TH abstraction violation note in C++ code. It's used to mark places where the abstraction is violated and needs refactoring.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/README.md#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n// Note [TH abstraction violation]\n```\n\n----------------------------------------\n\nTITLE: Tensor Element-wise Multiplication Statistics\nDESCRIPTION: Logs of element-wise multiplication operations (aten.mul.Tensor) between tensors of matching shapes using float16 data type\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 228, 28, 28], f16), T([128, 228, 1, 1], f16)), {})\ncnt: 2, ((T([128, 300, 28, 28], f16), T([128, 300, 1, 1], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Clang Compiler\nDESCRIPTION: This snippet demonstrates how to set environment variables for using the Clang compiler with the coverage tool. It sets the compiler type and the path to LLVM tools.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# set compiler type, the default is auto detected, you can check it at the start of log.txt\nexport COMPILER_TYPE=\"CLANG\"\n# set llvm path for clang, by default is /usr/local/opt/llvm/bin\nexport LLVM_TOOL_PATH=...\n```\n\n----------------------------------------\n\nTITLE: Mathematical Operations on Tensors\nDESCRIPTION: Basic mathematical operations including negation, power, square root, and subtraction on tensors with various shapes and configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convnext_base_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([32, 512, 14, 14], f16, stride=(100352, 1, 7168, 512)),), {})\ncnt: 1, ((T([32, 1, 14, 14], f16), 3), {})\ncnt: 1, ((T([32, 1, 56, 56], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring AOT Model Compiler Test Executable in CMake\nDESCRIPTION: Creates an executable for AOT (Ahead-of-Time) model compiler tests. It links against the torch library and includes necessary directories.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/nnc/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(aot_model_compiler_test\n  ${TORCH_ROOT}/binaries/aot_model_compiler.cc\n)\n\ntarget_link_libraries(aot_model_compiler_test PRIVATE torch)\ntarget_include_directories(aot_model_compiler_test PRIVATE ${ATen_CPU_INCLUDE})\n```\n\n----------------------------------------\n\nTITLE: Copy Operations in PyTorch\nDESCRIPTION: Records of tensor copy operations using aten.copy_ operator, showing input and output tensor shapes. The operation duplicates data from one tensor to another with the same shape.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Limitation with Runtime Autocast Enable Argument\nDESCRIPTION: Example demonstrating that runtime values cannot be used for the 'enabled' parameter in autocast context managers within JIT scripted functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.cpu.amp import autocast\n\n@torch.jit.script\ndef fn(a, b, use_amp: bool):\n    # runtime values for autocast enable argument are not supported\n    with autocast(enabled=use_amp):\n        return torch.mm(a, b)\n```\n\n----------------------------------------\n\nTITLE: Tensor Copy Operations with aten.lift_fresh_copy in PyTorch\nDESCRIPTION: Records of tensor copy operations using the lift_fresh_copy function, which creates a fresh copy of a tensor. This specific operation is performed on a 1D tensor with 128 elements of int64 type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\n```\n\n----------------------------------------\n\nTITLE: Tensor Slicing Input Tuples for aten.slice_backward.default - Python\nDESCRIPTION: Samples for testing 'aten.slice_backward.default', each tuple contains a tensor, slice shape, dimension, and slicing indices (start, stop, step). These parameters validate gradient propagation through slicing, ensuring shape/dtype compatibility, especially for high-dimensional tensors; errors may arise if dimensions/indices are not legal.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.slice_backward.default\ncnt: 4, ((T([128, 960, 7, 7], f16), [128, 960, 7, 7], 3, 0, 9223372036854775807, 1), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 4, ((T([128, 960, 7, 7], f16), [128, 960, 7, 7], 2, 0, 9223372036854775807, 1), {})\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Compiler Config Module\nDESCRIPTION: ReStructuredText directive for setting the current module context to torch.compiler.config\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler.config.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: torch.compiler.config\n```\n\n----------------------------------------\n\nTITLE: Setting Documentation Current Module\nDESCRIPTION: Sets the current module context for documentation using a template variable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/autosummary/classnoinheritance.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: {{ module }}\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA Kernels for PyTorch Operations\nDESCRIPTION: This CUDA code implements various kernels for PyTorch operations, including unary and binary functors, GPU kernels for mathematical operations, and offset calculators for tensor iterators. It uses CUDA-specific optimizations and C++ templates for efficient GPU execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_33\n\nLANGUAGE: CUDA\nCODE:\n```\n_ZN2at6native10gpu_kernelINS0_13AUnaryFunctorIN3c104HalfES4_S4_NS0_15binary_internal10MulFunctorIfEEEEEEvRNS_18TensorIteratorBaseERKT_\n```\n\nLANGUAGE: CUDA\nCODE:\n```\n_ZN2at6native10gpu_kernelINS0_13BinaryFunctorIlllNS0_15binary_internal10MulFunctorIlEEEEEEvRNS_18TensorIteratorBaseERKT_\n```\n\nLANGUAGE: CUDA\nCODE:\n```\n_Z22make_offset_calculatorILi4ELb0EE16OffsetCalculatorIXT_EjXT0_EERKN2at18TensorIteratorBaseE\n```\n\n----------------------------------------\n\nTITLE: Softmax Operation in PyTorch\nDESCRIPTION: The data on aten._softmax.default operator shows its usage of handling a large tensor with shape [1, 24, 512, 512] in float16, computing softmax along the last dimension. Useful for obtaining normalized probability distributions across a dataset.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._softmax.default\ncnt: 24, ((T([1, 24, 512, 512], f16), -1, False), {})\n```\n\n----------------------------------------\n\nTITLE: Padding Tensors with ATen ConstantPadND Operator\nDESCRIPTION: The `aten.constant_pad_nd` demonstrates tensor padding in PyTorch. Utilizes padding values, such as [0, 1] for tensors like [8192, 32, 63]. Allows controlled expansion of tensor dimensionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/sebotnet33ts_256_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.constant_pad_nd.default\ncnt: 2, ((T([8192, 32, 63], f16), [0, 1], 0.0), {})\n```\n\n----------------------------------------\n\nTITLE: TorchDynamo-based ONNX Export Memory Analysis\nDESCRIPTION: Script demonstrating memory usage tracking for TorchDynamo-based ONNX export of a HighResNet model, utilizing FakeTensorMode for reduced memory consumption.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/onnx_dynamo_memory_usage.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom monai.networks.nets import (\n    HighResNet,\n)\n\ntorch.cuda.memory._record_memory_history()\n\nmodel = HighResNet(\n    spatial_dims=3, in_channels=1, out_channels=3, norm_type=\"batch\"\n).eval()\n\nmodel = model.to(\"cuda\")\ndata = torch.randn(30, 1, 48, 48, 48, dtype=torch.float32).to(\"cuda\")\n\nwith torch.no_grad():\n    onnx_program = torch.onnx.export(\n                        model,\n                        data,\n                        \"test_faketensor.onnx\",\n                        dynamo=True,\n                    )\n\nsnapshot_name = f\"torchdynamo_exporter_example.pickle\"\nprint(f\"generate {snapshot_name}\")\n\ntorch.cuda.memory._dump_snapshot(snapshot_name)\nprint(f\"Export is done.\")\n```\n\n----------------------------------------\n\nTITLE: Distributed Computation Example for Dependency Analysis\nDESCRIPTION: Example demonstrating distributed computation with RPC calls, illustrating the challenges in computing dependencies for the distributed autograd graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/rpc/distributed_autograd.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.distributed.rpc as rpc\n\na = torch.rand((3, 3), requires_grad=True)\nb = torch.rand((3, 3), requires_grad=True)\nc = torch.rand((3, 3), requires_grad=True)\n\nd = rpc.rpc_sync(\"worker1\", torch.add, args=(a, b))\ne = rpc.rpc_sync(\"worker1\", torch.mul, args=(b, c))\nloss = d.sum()\n```\n\n----------------------------------------\n\nTITLE: Updating All Test Models for Android and iOS in PyTorch\nDESCRIPTION: These commands update all test models for both Android and iOS platforms. This is useful for ensuring all mobile test models are up-to-date with the latest PyTorch changes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/model_test/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython test/mobile/model_test/gen_test_model.py android\npython test/mobile/model_test/gen_test_model.py ios\n```\n\n----------------------------------------\n\nTITLE: Measuring CUDA Event Timing in PyTorch\nDESCRIPTION: Demonstrates how to accurately measure execution time of CUDA operations using torch.cuda.Event objects with proper synchronization.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nstart_event = torch.cuda.Event(enable_timing=True)\nend_event = torch.cuda.Event(enable_timing=True)\nstart_event.record()\n\n# Run some things here\n\nend_event.record()\ntorch.cuda.synchronize()  # Wait for the events to be recorded!\nelapsed_time_ms = start_event.elapsed_time(end_event)\n```\n\n----------------------------------------\n\nTITLE: Complex Indexing with Assignment in C++ using torch::Tensor::index_put_\nDESCRIPTION: This snippet shows a complex indexing operation with assignment in C++, using multiple index types including ellipsis, integer, boolean, slice, and tensor indices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_indexing.rst#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\ntensor.index_put_({\"...\", 0, true, Slice(1, None, 2), torch::tensor({1, 2})}, 1)\n```\n\n----------------------------------------\n\nTITLE: Disabling PyTorch Compilation for Specific Functions\nDESCRIPTION: Example showing how to use the torch.compiler.disable decorator to exclude problematic functions from compilation\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_troubleshooting.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef bad1_inner(...):\n    # skipped\n\n@torch.compiler.disable\ndef bad1_outer(...):\n    # skipped\n    bad1_inner(...)\n\ndef bad2_inner(...)\n    # traced\n\n@torch.compiler.disable(recursive=False)\ndef bad2_outer(...):\n    # skipped\n    bad2_inner(...)\n\n@torch.compile\ndef fn(...):\n    # graph break\n    bad1_outer(...)\n    ...\n    # graph break\n    bad2_outer(...)\n```\n\n----------------------------------------\n\nTITLE: TIMM Model Optimization Example\nDESCRIPTION: Shows how to optimize a pre-trained ResNeXt model from the TIMM library using torch.compile with the inductor backend.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_get_started.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport timm\nimport torch\nmodel = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(64,3,7,7))\n```\n\n----------------------------------------\n\nTITLE: Nesting InferenceMode States in C++\nDESCRIPTION: Demonstration of how InferenceMode states can be nested with different boolean values, allowing for flexible toggling of inference mode within code blocks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/inference_mode.rst#2025-04-22_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\n{\n  InferenceMode guard(true);\n  // InferenceMode is on\n  {\n    InferenceMode guard(false);\n    // InferenceMode is off\n  }\n  // InferenceMode is on\n}\n// InferenceMode is off\n```\n\n----------------------------------------\n\nTITLE: Analyzing ReLU Activation in PyTorch\nDESCRIPTION: This snippet shows the usage of relu_.default operator with various tensor shapes and counts of each unique configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/swsl_resnext101_32x16d_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([32, 64, 112, 112], f16),), {})\ncnt: 6, ((T([32, 512, 56, 56], f16),), {})\ncnt: 3, ((T([32, 256, 56, 56], f16),), {})\ncnt: 1, ((T([32, 1024, 56, 56], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Using Complex Indexing in C++ with torch::Tensor::index\nDESCRIPTION: This example demonstrates a more complex indexing operation in C++, using multiple index types including ellipsis, integer, boolean, slice, and tensor indices.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_indexing.rst#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\ntensor.index({\"...\", 0, true, Slice(1, None, 2), torch::tensor({1, 2})})\n```\n\n----------------------------------------\n\nTITLE: Creating a Tensor with LibTorch in C++\nDESCRIPTION: Minimal C++ example that creates a random tensor using LibTorch and prints it. This demonstrates basic usage of the PyTorch C++ API.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/installing.rst#2025-04-22_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n#include <torch/torch.h>\n#include <iostream>\n\nint main() {\n  torch::Tensor tensor = torch::rand({2, 3});\n  std::cout << tensor << std::endl;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing masked_fill Kernel for GPU in PyTorch\nDESCRIPTION: This CUDA kernel implements the masked_fill operation for PyTorch tensors on GPU. It handles different data types including float and Half precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_37\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native44_GLOBAL__N__9e5ddf9f_11_Indexing_cu_89862edb18masked_fill_kernelERNS_14TensorIteratorERKN3c106ScalarE\n```\n\n----------------------------------------\n\nTITLE: Importing Error Propagation Module in PyTorch\nDESCRIPTION: This snippet shows how to import the errors module from PyTorch's distributed elastic multiprocessing package. It provides access to error handling and propagation functionality for distributed processing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/errors.rst#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom torch.distributed.elastic.multiprocessing import errors\n```\n\n----------------------------------------\n\nTITLE: Correct CUDA Stream Synchronization\nDESCRIPTION: Demonstrates proper CUDA stream synchronization with wait_stream and record_stream calls to ensure correct execution order.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/cuda.rst#2025-04-22_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ncuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\ns.wait_stream(torch.cuda.default_stream(cuda))  # NEW!\nwith torch.cuda.stream(s):\n    B = torch.sum(A)\nA.record_stream(s)  # NEW!\n```\n\n----------------------------------------\n\nTITLE: Default Sum Operation in PyTorch\nDESCRIPTION: A single record of a default summation operation (aten.sum.default) applied to a tensor with shape [8, 1000] and float16 data type, reducing all dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/resnext50_32x4d_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.sum.default\ncnt: 1, ((T([8, 1000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Less Than Operations in PyTorch\nDESCRIPTION: This snippet logs usage of the \\\"aten.lt.Tensor\\\" which performs element-wise comparisons between tensors to check if one tensor is less than another.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten.lt.Tensor\ncnt: 1, ((T([128], i64), T([128, 1], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: CLOG Test Configuration\nDESCRIPTION: Sets up the test environment using Google Test framework and configures the test executable with appropriate compiler settings and dependencies.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/deps/clog/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_executable(clog-test test/clog.cc)\nset_target_properties(clog-test PROPERTIES\n  CXX_STANDARD 11\n  CXX_EXTENSIONS NO)\nCLOG_TARGET_RUNTIME_LIBRARY(clog-test)\ntarget_link_libraries(clog-test PRIVATE clog gtest gtest_main)\nadd_test(clog-test clog-test)\n```\n\n----------------------------------------\n\nTITLE: Defining C10 XPU Source and Header Files\nDESCRIPTION: Lists all the source and header files that constitute the C10 XPU library implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/xpu/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nset(C10_XPU_SRCS\n    XPUCachingAllocator.cpp\n    XPUFunctions.cpp\n    XPUStream.cpp\n    impl/XPUGuardImpl.cpp\n)\nset(C10_XPU_HEADERS\n    XPUCachingAllocator.h\n    XPUDeviceProp.h\n    XPUException.h\n    XPUFunctions.h\n    XPUMacros.h\n    XPUStream.h\n    impl/XPUGuardImpl.h\n)\n```\n\n----------------------------------------\n\nTITLE: Generating iOS Test Models in PyTorch\nDESCRIPTION: This command generates on-the-fly test models for iOS simulator tests. It creates models with a '_temp' suffix that should not be committed to the repository.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/model_test/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython test/mobile/model_test/gen_test_model.py ios-test\n```\n\n----------------------------------------\n\nTITLE: Disabling JIT Runtime Optimizations in PyTorch\nDESCRIPTION: Commands to globally disable most runtime optimizations in PyTorch JIT, including autodiff and fusion. This provides a way to run with minimal optimizations for debugging purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/OVERVIEW.md#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# Python\ntorch._C._get_graph_executor_optimize(False)\n```\n\nLANGUAGE: cpp\nCODE:\n```\n// C++\ntorch::jit::setGraphExecutorOptimize(false);\n```\n\n----------------------------------------\n\nTITLE: PyTorch Mean Operation\nDESCRIPTION: These snippets show the mean operation applied to tensors along specific dimensions, using float16 precision.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 256, 48, 48], f16), [2, 3], True), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 512, 24, 24], f16), [2, 3], True), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1536, 12, 12], f16), [2, 3], True), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 1536, 6, 6], f16), [2, 3], True), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\n((T([128, 3072, 6, 6], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Preparing AMD ROCm Build for PyTorch\nDESCRIPTION: This snippet shows how to prepare the PyTorch build environment for AMD ROCm support. It involves running a Python script to set up the necessary configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/libtorch.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd <pytorch_root>\n\n# Only run this if you're compiling for ROCm\npython tools/amd_build/build_amd.py\n```\n\n----------------------------------------\n\nTITLE: Configuring XPU Test Binary Compilation in CMake\nDESCRIPTION: Defines and builds test executables for XPU components in PyTorch. Sets up test files, creates executables, configures linking with c10_xpu library and gtest, and handles optional test installation with proper RPATH settings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/xpu/test/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(C10_XPU_ALL_TEST_FILES\n    impl/XPUCachingAllocatorTest.cpp\n    impl/XPUDeviceTest.cpp\n    impl/XPUGuardTest.cpp\n    impl/XPUStreamTest.cpp\n)\nif(BUILD_TEST)\n  foreach(test_src ${C10_XPU_ALL_TEST_FILES})\n    get_filename_component(test_file_name ${test_src} NAME_WE)\n    set(test_name \"c10_xpu_${test_file_name}\")\n    add_executable(${test_name} \"${test_src}\")\n    target_link_libraries(${test_name} ${C10_XPU_LIB} gtest_main)\n    add_test(NAME ${test_name} COMMAND $<TARGET_FILE:${test_name}>)\n    if(INSTALL_TEST)\n      set_target_properties(${test_name} PROPERTIES INSTALL_RPATH \"${CMAKE_INSTALL_RPATH}:${_rpath_portable_origin}/../lib\")\n      install(TARGETS ${test_name} DESTINATION test)\n    endif()\n  endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining ATen Operator Test Cases for Convolutions and Linear Operations - Python\nDESCRIPTION: This set of snippets enumerates test cases for convolution-like operations using synthetic tensor shapes and various parameters for stride, padding, dilation, and groups. The data is prepared to facilitate automated testing or benchmarking of operators such as convolutions, pointwise, and matrix multiplications, using FP16 data representations and tuples to specify each test scenario. Common dependencies include PyTorch's tensor construction utilities and type indicators (e.g., f16 for float16), while inputs and outputs are arrays representing commonly encountered shapes in convolutional networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mobilenetv3_large_100_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 480, 14, 14], f16), T([128, 80, 14, 14], f16), T([480, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 80, 14, 14], f16), T([128, 184, 14, 14], f16), T([80, 184, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 184, 14, 14], f16), T([128, 184, 14, 14], f16), T([184, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 184, [True, True, False]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 2, ((T([128, 184, 14, 14], f16), T([128, 80, 14, 14], f16), T([184, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 80, 14, 14], f16), T([128, 200, 14, 14], f16), T([80, 200, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operation in PyTorch\nDESCRIPTION: PyTorch's addmm operation that performs a matrix multiplication followed by an addition. This specific operation uses 16-bit floating point tensors and is likely part of a fully connected layer in a neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([64, 1024], f16), T([1024, 1000], f16, stride=(1, 1024))), {})\n```\n\n----------------------------------------\n\nTITLE: Using torch.compiler.allow_in_graph for Complex Features\nDESCRIPTION: Example showing how to mark functions containing hard-to-support features (like hooks or autograd.Function) to be included as-is in the TorchDynamo graph.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_fine_grain_apis.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@torch.compiler.allow_in_graph\ndef function_with_hooks():\n    # Function implementation with hooks\n    pass\n```\n\n----------------------------------------\n\nTITLE: Using a Custom Backend with torch.compile\nDESCRIPTION: This snippet demonstrates how to apply a custom backend (in this case, the optimize_for_inference_compiler) to accelerate existing code using the torch.compile decorator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_custom_backends.rst#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@torch.compile(backend=optimize_for_inference_compiler)\ndef code_to_accelerate():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Specifying Autogen for Native Functions in YAML\nDESCRIPTION: Demonstrates how to use the 'autogen' keyword in native_functions.yaml to generate implementations for in-place, functional, and out variants of operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_12\n\nLANGUAGE: YAML\nCODE:\n```\n- func: my_op_(Tensor(a!) self) -> Tensor(a!)\n  ...\n  autogen: my_op, my_op.out\n```\n\n----------------------------------------\n\nTITLE: True Division with torch.div in PyTorch 1.7\nDESCRIPTION: Shows how torch.div performs true division in PyTorch 1.7, similar to Python 3 division.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/serialization.rst#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# PyTorch 1.7\n>>> a = torch.tensor(5)\n>>> b = torch.tensor(3)\n>>> a / b\ntensor(1.6667)\n```\n\n----------------------------------------\n\nTITLE: Installing Nightly PyTorch with ROCm Support\nDESCRIPTION: Uses the tools/nightly.py script to check out a nightly PyTorch branch and install binaries built with ROCm support using the --rocm flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n./tools/nightly.py checkout -b my-nightly-branch --rocm\nsource venv/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\n----------------------------------------\n\nTITLE: Generating Automatic Class Documentation with Inheritance\nDESCRIPTION: Uses Sphinx's autoclass directive to automatically document a class, including its inherited members and regular members.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/_templates/autosummary/class.rst#2025-04-22_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: {{ name }}\n    :inherited-members:\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Tanh Activation Function in PyTorch (Python)\nDESCRIPTION: Parameters for the Forward computation of the hyperbolic tangent function using aten.tanh, a traditional activation function in neural networks, especially recurrent networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\naten.tanh.default\ncnt: 1, ((T([16, 768], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Managing External Dependencies\nDESCRIPTION: Handles downloading and configuration of external dependencies including cpuinfo, FP16, FXdiv, PSimd, pthreadpool, and testing frameworks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(CONFU_DEPENDENCIES_SOURCE_DIR \"${CMAKE_SOURCE_DIR}/deps\"\n  CACHE PATH \"Confu-style dependencies source directory\")\nset(CONFU_DEPENDENCIES_BINARY_DIR \"${CMAKE_BINARY_DIR}/deps\"\n  CACHE PATH \"Confu-style dependencies binary directory\")\n\nif(NOT DEFINED CLOG_SOURCE_DIR)\n  set(CLOG_SOURCE_DIR \"${PROJECT_SOURCE_DIR}/deps/clog\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring reStructuredText Hidden Role\nDESCRIPTION: Defines a custom hidden role for documentation styling with an associated CSS class.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/autosummary/classnoinheritance.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Operations in PyTorch\nDESCRIPTION: Matrix multiplication operations (addmm) between tensors of various sizes using half-precision (f16) format, including bias addition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([1500], f16), T([1024, 2000], f16), T([2000, 1500], f16, stride=(1, 2000))), {})\n```\n\n----------------------------------------\n\nTITLE: Applying In-place ReLU Activation in PyTorch\nDESCRIPTION: This snippet shows the tensor shapes for in-place ReLU activation in a PyTorch model. It demonstrates the operation on tensors of various sizes, likely corresponding to different layers in the network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_inception_v3_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 32, 149, 149], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Creating a DTensor using factory functions\nDESCRIPTION: Uses DTensor factory functions like zeros, ones, empty, full, rand, and randn to create DTensors directly, specifying the DeviceMesh and Placement.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nzeros(size, *, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)\n```\n\nLANGUAGE: Python\nCODE:\n```\nones(size, *, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)\n```\n\nLANGUAGE: Python\nCODE:\n```\nempty(size, *, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)\n```\n\nLANGUAGE: Python\nCODE:\n```\nfull(size, fill_value, *, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)\n```\n\nLANGUAGE: Python\nCODE:\n```\nrand(*size, *, generator=None, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)\n```\n\nLANGUAGE: Python\nCODE:\n```\nrandn(*size, *, generator=None, dtype=None, layout=None, requires_grad=False, device_mesh=None, placements=None)\n```\n\n----------------------------------------\n\nTITLE: Running TorchInductor Benchmark Command\nDESCRIPTION: Command to run benchmark script for model profiling with specific environment variables enabled for kernel naming and benchmarking.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_inductor_profiling.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nTORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1 python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only mixnet_l --disable-cudagraphs --training\n```\n\n----------------------------------------\n\nTITLE: Working with Zero-Dimensional Tensors in ATen\nDESCRIPTION: Example showing how to work with zero-dimensional tensors in ATen. These tensors hold a single value and can be created by indexing operations on larger tensors or by operators that reduce dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_basics.rst#2025-04-22_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\ntorch::Tensor two = torch::rand({10, 20});\ntwo[1][2] = 4;\n// ^^^^^^ <- zero-dimensional Tensor\n```\n\n----------------------------------------\n\nTITLE: Defining Boolean Operations in TorchScript\nDESCRIPTION: Specifies the syntax for boolean operations (and, or, not) in TorchScript. These operators work on boolean values and can be customized for user-defined objects.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nor_test  ::=  and_test | or_test 'or' and_test\nand_test ::=  not_test | and_test 'and' not_test\nnot_test ::=  'bool' '(' or_expr ')' | comparison | 'not' not_test\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Backward Operation Statistics\nDESCRIPTION: Logs of backward pass batch normalization operations (aten.native_batch_norm_backward.default) showing gradient computation details\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/rexnet_100_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: InternalMatch Class Structure\nDESCRIPTION: Defines the structure of match results returned by SubgraphMatcher\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_transformations.rst#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass InternalMatch():\n    # Nodes from which the match was found\n    anchors: List[Node]\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node] = field(default_factory=dict)\n    # Nodes in target graph that are matched placeholder in pattern\n    placeholder_nodes: List[Node] = field(default_factory=list)\n    # Nodes in matched subgraph returned by output\n    returning_nodes: List[Node] = field(default_factory=list)\n```\n\n----------------------------------------\n\nTITLE: Setting Tensor Elements using index_put_() in C++\nDESCRIPTION: This snippet shows how to use the torch::Tensor::index_put_ method to set tensor elements in C++, equivalent to assignment with indexing in Python.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_indexing.rst#2025-04-22_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\ntensor.index_put_({None}, 1)\n```\n\n----------------------------------------\n\nTITLE: Analyzing ATen Unsafe View Operations in PyTorch\nDESCRIPTION: Logs occurrences of \\\"aten._unsafe_view.default\\\" which allows reshaping tensors without copying data, detailing the specifics of tensor shapes before and after reshaping.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/PegasusForCausalLM_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: pseudocode\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([8, 128, 16, 64], f16), [8, 128, 1024]), {})\ncnt: 1, ((T([1024, 50265], f16), [8, 128, 50265]), {})\ncnt: 12, ((T([8, 16, 128, 64], f16), [128, 128, 64]), {})\ncnt: 12, ((T([8, 128, 1024], f16), [1024, 1024]), {})\n```\n\n----------------------------------------\n\nTITLE: Automatically Generated TorchScript Function\nDESCRIPTION: Example of a TorchScript function automatically generated from a benchmark definition. The function's signature and body are derived from the benchmark's signature and Python forward statement.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/instruction_counts/README.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@torch.jit.script\ndef f(x, w):\n    # Paste `benchmark.py_fwd_stmt` into the function body.\n    y = x * w\n    return y  # Set by `-> y` in signature.\n```\n\n----------------------------------------\n\nTITLE: Log Entry for Tensor Pair (Shape [128, 32, 56, 56], f16)\nDESCRIPTION: Logs the occurrence (count 4) of a tensor pair, both with shape [128, 32, 56, 56] and dtype f16, using default strides. Likely generated during PyTorch execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: text\nCODE:\n```\ncnt: 4, ((T([128, 32, 56, 56], f16), T([128, 32, 56, 56], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Reshaping Tensors with aten._unsafe_view.default\nDESCRIPTION: Records usage of the _unsafe_view.default operator for fast tensor reshaping operations. This operator is primarily used to reshape feature maps for YOLO object detection, converting 5D tensors to 4D tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 1, ((T([8, 3, 85, 48, 64], f16), [8, 255, 48, 64]), {})\ncnt: 1, ((T([8, 3, 85, 24, 32], f16), [8, 255, 24, 32]), {})\ncnt: 1, ((T([8, 3, 85, 12, 16], f16), [8, 255, 12, 16]), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Operations in PyTorch\nDESCRIPTION: This snippet shows multiple instances of batch normalization operations with different tensor shapes and channel sizes. It includes input tensors, running mean and variance, and other parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/hardcorenas_a_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 672, 7, 7], f16), T([128, 672, 7, 7], f16), T([672], f16), T([672], f16), T([672], f16), T([672], f32), T([672], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Documenting CallgrindStats Class in PyTorch Benchmark Utils\nDESCRIPTION: This snippet documents the CallgrindStats class from the torch.utils.benchmark module, including all its members.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_4\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: CallgrindStats\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Accessing Tensor Elements using index() in C++\nDESCRIPTION: This snippet demonstrates how to use the torch::Tensor::index method to access tensor elements in C++, equivalent to using square brackets in Python.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_indexing.rst#2025-04-22_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\ntensor.index({None})\n```\n\n----------------------------------------\n\nTITLE: Unsupported Use of .item() with vmap\nDESCRIPTION: This example shows that vmap doesn't support functions that call .item() on tensors, as this breaks the vectorized computation model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/ux_limitations.rst#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef f(x):\n  return x.item()\n\nx = torch.randn(3)\nvmap(f)(x)\n```\n\n----------------------------------------\n\nTITLE: Finding C10 XPU Library for Non-Libtorchless Builds\nDESCRIPTION: Attempts to find the C10 XPU library in the specified path when not building in libtorchless mode.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/xpu/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT BUILD_LIBTORCHLESS)\n  find_library(C10_XPU_LIB c10_xpu PATHS $ENV{LIBTORCH_LIB_PATH} NO_DEFAULT_PATH)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication in PyTorch\nDESCRIPTION: Performs matrix multiplication between two tensors of compatible shapes. Used in fully connected layers and their backward passes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\naten.mm.default((T([64, 1000], f16, stride=(0, 0)), T([1000, 4096], f16)), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\naten.mm.default((T([1000, 64], f16, stride=(0, 0)), T([64, 4096], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Running Code Coverage for Specific Test and Folder\nDESCRIPTION: This example demonstrates how to run the code coverage tool for a specific test (atest) and generate reports only for the 'aten' folder.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/code_coverage/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython oss_coverage.py --run-only=atest --interest-only=aten\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Gradient Operations for Benchmarking\nDESCRIPTION: This code demonstrates how to configure PyTorch gradient operations for benchmarking by enabling requires_grad and using generate_pt_gradient_test.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nself.input_one = torch.rand(M, N, K, requires_grad=True)\ngenerate_pt_gradient_test(long_configs + short_configs, TorchAddBenchmark)\n```\n\n----------------------------------------\n\nTITLE: Jacobian-vector Product (JVP) Transform in PyTorch\nDESCRIPTION: Demonstrates computing Jacobian-vector products using forward-mode automatic differentiation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/whirlwind_tour.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom functorch import jvp\nx = torch.randn(5)\ny = torch.randn(5)\nf = lambda x, y: (x * y)\n_, output = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))\nassert torch.allclose(output, x + y)\n```\n\n----------------------------------------\n\nTITLE: ReLU In-Place Operations\nDESCRIPTION: Collection of in-place ReLU operations (aten.relu_.default) on tensors of various shapes. These operations modify tensors directly without creating new memory allocations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 64, 112, 112], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Logging aten.scatter_add.default Operation Arguments\nDESCRIPTION: These log entries record calls to the `aten.scatter_add.default` PyTorch operator. Each line shows the count (`cnt`) for a specific combination of arguments. The arguments tuple `((self, dim, index, src), {})` includes the target tensor (`T([965], f16)`), the dimension (`0`), the index tensor (`T([...], i64)`), and the source tensor (`T([...], f16)`), followed by empty keyword arguments `{}`.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_16\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.scatter_add.default\ncnt: 1, ((T([965], f16), 0, T([54765], i64), T([54765], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54704], i64), T([54704], f16)), {})\ncnt: 4, ((T([965], f16), 0, T([54786], i64), T([54786], f16)), {})\ncnt: 2, ((T([965], f16), 0, T([54804], i64), T([54804], f16)), {})\ncnt: 3, ((T([965], f16), 0, T([54757], i64), T([54757], f16)), {})\n# ... (many similar lines omitted for brevity)\n```\n\n----------------------------------------\n\nTITLE: Tensor Copy and Manipulation\nDESCRIPTION: Tensor copy operations with different shapes and stride patterns, maintaining the float16 data type.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\naten.copy_.default((T([32, 3, 224, 224], f16), T([32, 3, 224, 224], f16)))\n```\n\n----------------------------------------\n\nTITLE: Defining mz_ulong Type in C\nDESCRIPTION: Temporary fix to define mz_ulong as unsigned long for CRC-32 calculation correction. This change was made to address issues with 64-bit systems.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/third_party/miniz-3.0.2/ChangeLog.md#2025-04-22_snippet_4\n\nLANGUAGE: C\nCODE:\n```\ntypedef unsigned long mz_ulong\n```\n\n----------------------------------------\n\nTITLE: AOT Module with ResNet Example\nDESCRIPTION: Demonstrates how to apply AOT compilation to a complete neural network module using ResNet18 as an example.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/COMPILE_README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torchvision.models import resnet18\naot_module(resnet18(), print_graph(\"forward\"), print_graph(\"backward\"))(torch.randn(1,3,200,200))\n```\n\n----------------------------------------\n\nTITLE: Unsupported Autocast Decorator in JIT Scripting\nDESCRIPTION: Example showing unsupported usage of autocast as a function decorator when calling a decorated function from within a JIT scripted function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/JIT-AUTOCAST.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch.cpu.amp import autocast\n\n@autocast(enabled=True)\ndef helper(x):\n    ...\n\n@torch.jit.script\ndef foo(x):\n    return helper(x) # not supported\n```\n\n----------------------------------------\n\nTITLE: Setting Current CUDA Stream in PyTorch (C++)\nDESCRIPTION: This snippet sets the current CUDA stream to the first stream in the streams1 array. It's a prerequisite for the subsequent operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/cpp/source/notes/tensor_cuda_stream.rst#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\nat::cuda::setCurrentCUDAStream(streams1[0]);\n```\n\n----------------------------------------\n\nTITLE: Gradient Descent Update Rules\nDESCRIPTION: Mathematical formulation of gradient descent update rules for complex variables in real and complex spaces.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/notes/autograd.rst#2025-04-22_snippet_6\n\nLANGUAGE: math\nCODE:\n```\n\\begin{aligned}\n    x_{n+1} &= x_n - (\\alpha/2) * \\frac{\\partial L}{\\partial x}  \\\\\n    y_{n+1} &= y_n - (\\alpha/2) * \\frac{\\partial L}{\\partial y}\n\\end{aligned}\n```\n\n----------------------------------------\n\nTITLE: Backward Convolution Operations in PyTorch\nDESCRIPTION: Backward pass convolution operations for gradient computation, featuring different tensor shapes and configurations. Uses half-precision (f16) format and includes gradient flow control parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/densenet121_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n((T([64, 32, 7, 7], f16, stride=(50176, 49, 7, 1)), T([64, 128, 7, 7], f16), T([32, 128, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Event Class in PyTorch Distributed Elastic\nDESCRIPTION: This class represents an event in the PyTorch distributed elastic system. It likely contains properties and methods for managing event data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/events.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntorch.distributed.elastic.events.api.Event\n```\n\n----------------------------------------\n\nTITLE: Example Program with Graph Breaks\nDESCRIPTION: Basic code structure demonstrating where TorchDynamo attempts to compile tensor operations into a single FX graph\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_faq.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef some_fun(x):\n    ...\n\ntorch.compile(some_fun)(x)\n...\n```\n\n----------------------------------------\n\nTITLE: Defining Python Class 'LongWithoutDocstring' Without Docstring\nDESCRIPTION: This snippet defines a Python class named 'LongWithoutDocstring' without a docstring. It includes a comment that is not considered a proper docstring.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt.before.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass LongWithoutDocstring:\n    # A comment isn't a docstring\n```\n\n----------------------------------------\n\nTITLE: PyTorch Autograd Functions in C++\nDESCRIPTION: Function declarations for PyTorch's autograd functionality, including variable operations and Python bindings.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\n_ZN5torch8autograd5utils4wrapEN2at6TensorE\n_ZN5torch8autogradL14THPVariable_toEP7_objectS2_S2_\n_ZN5torch8autogradL15THPVariable_addEP7_objectS2_S2_\n_ZN5torch8autogradL15THPVariable_mulEP7_objectS2_S2_\n_ZN5torch8autogradL21THPVariable_transposeEP7_objectS2_S2_\n_ZN5torch8autogradL28TypeError_to_NotImplemented_IXadL_ZNS0_L15THPVariable_addEP7_objectS3_S3_EEEES3_S3_S3_S3_\n_ZN5torch8autogradL28TypeError_to_NotImplemented_IXadL_ZNS0_L15THPVariable_mulEP7_objectS3_S3_EEEES3_S3_S3_S3_\n```\n\n----------------------------------------\n\nTITLE: Installing Specific KaTeX Version\nDESCRIPTION: Command to install a specific version of KaTeX (0.13.18) that is compatible with node@6.13.1 for documentation builds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/README.md#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -g katex@0.13.18\n```\n\n----------------------------------------\n\nTITLE: Mean Operation in DenseNet Global Average Pooling\nDESCRIPTION: This snippet shows the mean operation used for global average pooling in DenseNet. It computes the mean across the spatial dimensions (-1, -2) of feature maps with shape [32, 1024, 7, 7], keeping the dimensions for broadcasting compatibility.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.mean.dim\ncnt: 1, ((T([32, 1024, 7, 7], f16), [-1, -2], True), {})\n```\n\n----------------------------------------\n\nTITLE: Collecting New Heuristic Training Data\nDESCRIPTION: Command to collect new training data for matrix multiplication heuristics on specified GPUs\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mm/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash generate_heuristic.sh collect\n```\n\n----------------------------------------\n\nTITLE: Profiling FastRNNs models with nvprof\nDESCRIPTION: Command to profile all FastRNNs models using NVIDIA's nvprof tool, generating performance profiling files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/fastrnns/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m fastrnns.profile\n```\n\n----------------------------------------\n\nTITLE: Constant Padding Operations in PyTorch\nDESCRIPTION: Statistics for the aten.constant_pad_nd.default operator used for padding tensors with constant values. The operation adds padding to various tensors, both for adding boundaries (with zeros) and for trimming tensor edges (with negative values).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.constant_pad_nd.default\ncnt: 1, ((T([128, 3, 192, 192], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 64, 96, 96], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 256, 48, 48], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 768, 24, 24], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 768, 12, 12], f16), [0, 1, 0, 1], 0.0), {})\ncnt: 1, ((T([128, 768, 13, 13], f16), [0, -1, 0, -1]), {})\ncnt: 1, ((T([128, 768, 25, 25], f16), [0, -1, 0, -1]), {})\ncnt: 1, ((T([128, 256, 49, 49], f16), [0, -1, 0, -1]), {})\ncnt: 1, ((T([128, 64, 97, 97], f16), [0, -1, 0, -1]), {})\n```\n\n----------------------------------------\n\nTITLE: Importing torch.ao.ns.fx.utils module in Python\nDESCRIPTION: This snippet shows how to import the torch.ao.ns.fx.utils module. This module contains utility functions for numeric comparisons and is also marked as an early prototype.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.ao.ns._numeric_suite_fx.rst#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport torch.ao.ns.fx.utils\n```\n\n----------------------------------------\n\nTITLE: Basic Math Kernel Operations\nDESCRIPTION: Core mathematical operations like cos, sin, rsqrt implemented as CUDA kernels\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_35\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at6native15cos_kernel_cudaERNS_18TensorIteratorBaseE\n_ZN2at6native15sin_kernel_cudaERNS_18TensorIteratorBaseE\n_ZN2at6native17rsqrt_kernel_cudaERNS_18TensorIteratorBaseE\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for PyTorch Testing\nDESCRIPTION: Instructions for setting environment variables to catch segmentation faults and create reproduction reports during testing.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/onnx/torchlib/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nCREATE_REPRODUCTION_REPORT=1 python -m pytest test/onnx/torchlib/test_ops.py -k div_mode_int\n```\n\n----------------------------------------\n\nTITLE: Disabling High-Level Functionality in C\nDESCRIPTION: Modification to example5.c to disable high-level functionality, including stdio operations. This change was made to demonstrate low-level usage of the library.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/third_party/miniz-3.0.2/ChangeLog.md#2025-04-22_snippet_5\n\nLANGUAGE: C\nCODE:\n```\nMINIZ_NO_STDIO\n```\n\n----------------------------------------\n\nTITLE: Comparing Category Differences\nDESCRIPTION: Identifies differences between known categories and categories found in commits using set operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/explore.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Get the difference between known categories and categories from commits\n\ndiff_categories = set(commit_list_df.category.to_list()) - set(common.categories)\nprint(len(diff_categories))\npprint(diff_categories)\n```\n\n----------------------------------------\n\nTITLE: Windows CMake Build Commands\nDESCRIPTION: Basic command sequence for building a CMake project on Windows, showing the two-step configure and build process.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_38\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build\ncd build\ncmake ..\ncmake --build .\n```\n\n----------------------------------------\n\nTITLE: Defining MZ_FORCEINLINE Macro for Cross-Compiler Compatibility in C\nDESCRIPTION: Introduces the MZ_FORCEINLINE macro to handle differences in inline function declarations between compilers like GCC and MSVC.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/third_party/miniz-3.0.2/ChangeLog.md#2025-04-22_snippet_3\n\nLANGUAGE: C\nCODE:\n```\nMZ_FORCEINLINE\n```\n\n----------------------------------------\n\nTITLE: Importing Python Files in Scripts (Python)\nDESCRIPTION: The module_loader.py script in the shared directory facilitates importing arbitrary Python files in a script without adding them to the PYTHONPATH first.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/README.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom shared.module_loader import import_module\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module Context in Sphinx Documentation\nDESCRIPTION: Sets the current module context for documentation using a template variable. This allows the template to be reused for different modules.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/_templates/classtemplate.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: {{ module }}\n```\n\n----------------------------------------\n\nTITLE: Defining a hidden role in reStructuredText\nDESCRIPTION: Sets up a hidden role that can be used to hide content in the documentation by assigning it the 'hidden-section' class.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/signal.rst#2025-04-22_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication with Bias in PyTorch\nDESCRIPTION: Usage of the aten.addmm.default operator which performs matrix multiplication with an added bias term. This operation is typically used in fully-connected layers, shown here with a 128-batch input being projected to 1000 output dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_nfnet_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([128, 3072], f16), T([3072, 1000], f16, stride=(1, 3072))), {})\n```\n\n----------------------------------------\n\nTITLE: Creating Tensors Filled with Zeros using aten.new_zeros - Python\nDESCRIPTION: Illustrates the usage of aten.new_zeros.default to allocate tensors initialized with zeros, including specifying device, dtype, and memory layout. It handles both strided and contiguous input tensors, on CUDA devices, and is dependent on PyTorch. Shape and stride combinations should be valid.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/hf_BigBird_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.new_zeros.default\ncnt: 12, ((T([2, 12, 12, 64, 64], f16, stride=(786432, 64, 49152, 768, 1)), [1179648]), {})\ncnt: 24, ((T([1008, 64, 64], f16), [384, 64, 64]), {'dtype': f16, 'layout': torch.strided, 'device': 'cuda'})\n```\n\n----------------------------------------\n\nTITLE: Importing torchrun Module Documentation in RST\nDESCRIPTION: This RST directive imports the documentation for the torch.distributed.run module, which contains the torchrun (Elastic Launch) functionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/elastic/run.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: torch.distributed.run\n```\n\n----------------------------------------\n\nTITLE: Element-Wise Bitwise NOT in PyTorch\nDESCRIPTION: The aten.bitwise_not.default operator processes boolean tensors of shape [1, 1, 512, 512] using bitwise NOT operation, crucial for certain conditional operations where logical negation is required.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.bitwise_not.default\ncnt: 24, ((T([1, 1, 512, 512], b8),), {})\n```\n\n----------------------------------------\n\nTITLE: Defining TorchScript Expression Grammar\nDESCRIPTION: Specifies the grammar for TorchScript expressions, including atoms, primaries, arithmetic operations, and function calls. It defines the syntax for various expression constructs like list comprehensions, dictionary displays, and slicing operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/jit_language_reference_v2.rst#2025-04-22_snippet_18\n\nLANGUAGE: TorchScript\nCODE:\n```\natom      ::=  identifier | literal | enclosure\nenclosure ::=  parenth_form | list_display | dict_display\n\nparenth_form ::=  '(' [expression_list] ')'\n\nlist_comprehension ::=  expression comp_for\ncomp_for           ::=  'for' target_list 'in' or_expr\nlist_display       ::=  '[' [expression_list | list_comprehension] ']'\ndict_display       ::=  '{' [key_datum_list | dict_comprehension] '}'\nkey_datum_list     ::=  key_datum (',' key_datum)*\nkey_datum          ::=  expression ':' expression\ndict_comprehension ::=  key_datum comp_for\n\nprimary ::=  atom | attributeref | subscription | slicing | call\n\nattributeref ::=  primary '.' identifier\n\nsubscription ::=  primary '[' expression_list ']'\n\nslicing      ::=  primary '[' slice_list ']'\nslice_list   ::=  slice_item (',' slice_item)* [',']\nslice_item   ::=  expression | proper_slice\nproper_slice ::=  [expression] ':' [expression] [':' [expression] ]\n\ncall          ::=  primary '(' argument_list ')'\nargument_list ::=  args [',' kwargs] | kwargs\nargs          ::=  [arg (',' arg)*]\nkwargs        ::=  [kwarg (',' kwarg)*]\nkwarg         ::=  arg '=' expression\narg           ::=  identifier\n\npower ::=  primary ['**' u_expr]\n\nu_expr ::=  power | '-' power | '~' power\n\nm_expr ::=  u_expr | m_expr '*' u_expr | m_expr '@' m_expr | m_expr '//' u_expr | m_expr '/' u_expr | m_expr '%' u_expr\na_expr ::=  m_expr | a_expr '+' m_expr | a_expr '-' m_expr\n```\n\n----------------------------------------\n\nTITLE: PyTorch Log Softmax Operations\nDESCRIPTION: Log softmax forward and backward operations on tensors with shape [64, 1000] using half-precision (fp16) format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/cspdarknet53_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naten._log_softmax.default((T([64, 1000], f16), 1, False), {})\naten._log_softmax_backward_data.default((T([64, 1000], f16), T([64, 1000], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking Type Conversion Operations with aten._to_copy.default\nDESCRIPTION: Shows usages of the _to_copy.default operator which handles tensor type conversion. Statistics include tensor shape conversion patterns, primarily between float16 and float32 data types, with many operations involving CUDA device transfers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._to_copy.default\ncnt: 1, ((T([1, 1, 12, 16, 2], i64),), {'dtype': f32})\ncnt: 3, ((T([3, 2], f32),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 3, ((T([1, 3, 1, 1, 2], f32),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 1, ((T([1, 1, 24, 32, 2], i64),), {'dtype': f32})\ncnt: 1, ((T([1, 1, 48, 64, 2], i64),), {'dtype': f32})\ncnt: 2, ((T([8, 3, 48, 64, 2], f16),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 2, ((T([8, 3, 48, 64, 2], f32),), {'dtype': f16})\ncnt: 2, ((T([8, 3, 24, 32, 2], f16),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 2, ((T([8, 3, 24, 32, 2], f32),), {'dtype': f16})\ncnt: 2, ((T([8, 3, 12, 16, 2], f16),), {'dtype': f32, 'layout': torch.strided, 'device': 'cuda'})\ncnt: 2, ((T([8, 3, 12, 16, 2], f32),), {'dtype': f16})\n```\n\n----------------------------------------\n\nTITLE: Setting Up Record Function Benchmark Binary with Custom Include Directories\nDESCRIPTION: Creates a 'record_function_benchmark' binary target and configures its include directories to access ATen source files needed by the implementation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/binaries/CMakeLists.txt#2025-04-22_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ncaffe2_binary_target(\"record_function_benchmark.cc\")\ntarget_include_directories(record_function_benchmark PUBLIC\n  ${CMAKE_BINARY_DIR}/aten/src)\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Setup with Environment Variables\nDESCRIPTION: An example of how to pass environment variables to the PyTorch setup command for customization or debugging purposes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nENV_KEY1=ENV_VAL1[, ENV_KEY2=ENV_VAL2]* python setup.py develop\n```\n\n----------------------------------------\n\nTITLE: Setting Compression Level and Flags in C\nDESCRIPTION: Update to allow setting the compression level and flags to MZ_DEFAULT_COMPRESSION. This change provides a default compression option for users.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/third_party/miniz-3.0.2/ChangeLog.md#2025-04-22_snippet_6\n\nLANGUAGE: C\nCODE:\n```\nlevel_and_flags = MZ_DEFAULT_COMPRESSION;\n```\n\n----------------------------------------\n\nTITLE: CPU Test Sources Configuration\nDESCRIPTION: Appends CPU-specific test source files to ATen_CPU_TEST_SRCS list\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/test/CMakeLists.txt#2025-04-22_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND ATen_CPU_TEST_SRCS\n  ${CMAKE_CURRENT_SOURCE_DIR}/Dict_test.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/Dimname_test.cpp\n  ${CMAKE_CURRENT_SOURCE_DIR}/MaybeOwned_test.cpp\n  [...additional files...])\n```\n\n----------------------------------------\n\nTITLE: Tensor Equality Comparisons\nDESCRIPTION: Element-wise equality comparisons between tensors of different shapes and data types.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_efficientdet_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n((T([5000, 4], f32), T([4], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Configuring Class Documentation Generation\nDESCRIPTION: Configures automatic class documentation generation using the autoclass directive with member inclusion.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/autosummary/classnoinheritance.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: {{ name }}\n    :members:\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Operations - Common Patterns\nDESCRIPTION: Collection of frequently used tensor operations including softmax, matrix multiplications (mm, bmm), embeddings, and layer normalization. Operations are shown with their input tensor shapes, data types (primarily float16/f16), and execution counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/BigBird_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example tensor operation patterns:\n\n# Softmax operations\naten._softmax.default((T([1, 12, 64, 1024], f16), -1, False))\n\n# Matrix multiplications\naten.bmm.default((T([12, 64, 64], f16), T([12, 64, 1024], f16)))\n\n# Embedding operations\naten.embedding.default((T([50358, 768], f16), T([1, 1024], i64), 0))\n\n# Layer normalization\naten.native_layer_norm.default((T([1, 1024, 768], f16), [768], T([768], f16), T([768], f16), 1e-12))\n```\n\n----------------------------------------\n\nTITLE: NVFuser Graph Fusion Debugging Command\nDESCRIPTION: Shell command to enable detailed fusion graph logging for debugging NVFuser behavior.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/codegen/cuda/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nPYTORCH_JIT_LOG_LEVEL=\"graph_fuser\" python <your pytorch script>\n```\n\n----------------------------------------\n\nTITLE: NLL Loss Operations\nDESCRIPTION: Negative log likelihood loss calculations for classification tasks with 1000 output classes. Handles both forward and backward passes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tf_efficientnet_b0_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: PyTorch Sum Operations\nDESCRIPTION: Tensor reduction operations along specified dimensions with various stride patterns\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_botnext26ts_256_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 1000], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Computing Convolutions with aten.convolution.default\nDESCRIPTION: Documents convolution operations in a neural network. Operations include initial feature extraction, downsampling through strided convolutions, and detection head layers for a YOLO-like architecture, with most operations using half-precision (float16) data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/yolov3_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([8, 3, 384, 512], f16), T([32, 3, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 32, 384, 512], f16), T([64, 32, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 64, 192, 256], f16), T([32, 64, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 32, 192, 256], f16), T([64, 32, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 64, 192, 256], f16), T([128, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([8, 128, 96, 128], f16), T([64, 128, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 2, ((T([8, 64, 96, 128], f16), T([128, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 128, 96, 128], f16), T([256, 128, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 10, ((T([8, 256, 48, 64], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 11, ((T([8, 128, 48, 64], f16), T([256, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 256, 48, 64], f16), T([512, 256, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 10, ((T([8, 512, 24, 32], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 11, ((T([8, 256, 24, 32], f16), T([512, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 512, 24, 32], f16), T([1024, 512, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 7, ((T([8, 1024, 12, 16], f16), T([512, 1024, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 7, ((T([8, 512, 12, 16], f16), T([1024, 512, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 2048, 12, 16], f16), T([512, 2048, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 1024, 12, 16], f16), T([255, 1024, 1, 1], f16), T([255], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 512, 12, 16], f16), T([256, 512, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 768, 24, 32], f16), T([256, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 512, 24, 32], f16), T([255, 512, 1, 1], f16), T([255], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 256, 24, 32], f16), T([128, 256, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 384, 48, 64], f16), T([128, 384, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([8, 256, 48, 64], f16), T([255, 256, 1, 1], f16), T([255], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten._log_softmax_backward_data Operator - PyTorch - Python\nDESCRIPTION: This snippet traces the backward computation for the log_softmax operation using aten._log_softmax_backward_data. It takes two f16 tensors of shape [4, 2], an axis specification, and outputs the gradient with respect to the input for use in autograd. All tensors use half precision. Dependencies: torch autograd engine. Limitations: f16 requires appropriate hardware.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/GPT2ForSequenceClassification_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([4, 2], f16), T([4, 2], f16), 1, f16), {})\n```\n\n----------------------------------------\n\nTITLE: NLL Loss Operations in PyTorch\nDESCRIPTION: This snippet shows NLL (Negative Log Likelihood) loss operations and their backward passes. It includes operations on 2D tensors with different shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/volo_d1_224_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([64, 1000], f16), T([64], i64), None, 1, -100, T([], f16)), {})\n\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([64, 1000], f16), T([64], i64), None, 1, -100), {})\n```\n\n----------------------------------------\n\nTITLE: Importing torch.signal.windows submodule in reStructuredText documentation\nDESCRIPTION: Documentation directives to import and set the torch.signal.windows submodule as the current module for the documentation.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/signal.rst#2025-04-22_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. automodule:: torch.signal.windows\n.. currentmodule:: torch.signal.windows\n```\n\n----------------------------------------\n\nTITLE: PyTorch Tensor Addition Operations\nDESCRIPTION: Documents tensor addition operations with various shapes and stride patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/twins_pcpvt_base_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.add.Tensor((T([32, 3136, 64], f16), T([32, 3136, 64], f16)))\naten.add.Tensor((T([32, 784, 128], f16), T([32, 784, 128], f16)))\naten.add_.Tensor((T([32, 64, 56, 56], f16, stride=(200704, 1, 3584, 64)), T([32, 64, 56, 56], f16, stride=(200704, 1, 3584, 64))))\n```\n\n----------------------------------------\n\nTITLE: Using CommDebugMode for debugging DTensor operations\nDESCRIPTION: Enables communication debugging mode to track collective operations performed by DTensor.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/distributed.tensor.rst#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nwith CommDebugMode():\n    # Your DTensor operations here\n```\n\n----------------------------------------\n\nTITLE: Running Forward Graph Profile\nDESCRIPTION: Command to profile the forward graph execution with the -p flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/torch.compiler_inductor_profiling.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython fwd.py -p\n```\n\n----------------------------------------\n\nTITLE: PyTorch Pooling Operations\nDESCRIPTION: Max pooling operations with indices, including forward and backward passes, using 3x3 kernel and 2x2 stride\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n((T([4, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Linting Checks\nDESCRIPTION: Executes the same linting steps used in CI for the PyTorch codebase.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nmake lint\n```\n\n----------------------------------------\n\nTITLE: Retaining Intrusive Pointer for TensorImpl in C++\nDESCRIPTION: This function retains an intrusive pointer for a TensorImpl object. It's part of PyTorch's memory management system for tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c1013intrusive_ptrINS_10TensorImplENS_19UndefinedTensorImplEE7retain_Ev.isra.0\n```\n\n----------------------------------------\n\nTITLE: Creating Class Documentation Section with Underlined Header\nDESCRIPTION: Creates an underlined header for the class documentation section using the class name provided by a template variable.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/_templates/autosummary/class.rst#2025-04-22_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n{{ name | underline}}\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations in PyTorch\nDESCRIPTION: Forward and backward convolution operations with various tensor sizes and configurations, using 3x3 filters and half-precision format.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/maml_omniglot_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naten.convolution.default((T([5, 1, 28, 28], f16), T([64, 1, 3, 3], f16), T([64], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1))\naten.convolution_backward.default((T([5, 64, 26, 26], f16, stride=(43264, 1, 1664, 64)), T([5, 1, 28, 28], f16), T([64, 1, 3, 3], f16), [64], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [False, True, True]))\n```\n\n----------------------------------------\n\nTITLE: Analyzing Commit Categories\nDESCRIPTION: Generates descriptive statistics for commit categories in the dataset.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/release_notes/explore.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncommit_list_df.category.describe()\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch ABI Check Test Build in CMake\nDESCRIPTION: Sets up the build configuration for ABI compatibility check tests. Defines test source files, creates a test executable that intentionally does not link against torch library to verify header-only implementation, and configures installation settings including PDB files for MSVC builds.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/aoti_abi_check/CMakeLists.txt#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(AOTI_ABI_CHECK_TEST_ROOT ${TORCH_ROOT}/test/cpp/aoti_abi_check)\n\n# Build the cpp gtest binary containing the cpp-only tests.\nset(AOTI_ABI_CHECK_TEST_SRCS\n  ${AOTI_ABI_CHECK_TEST_ROOT}/main.cpp\n  ${AOTI_ABI_CHECK_TEST_ROOT}/test_cast.cpp\n  ${AOTI_ABI_CHECK_TEST_ROOT}/test_dtype.cpp\n  ${AOTI_ABI_CHECK_TEST_ROOT}/test_math.cpp\n  ${AOTI_ABI_CHECK_TEST_ROOT}/test_rand.cpp\n  ${AOTI_ABI_CHECK_TEST_ROOT}/test_vec.cpp\n)\n\nadd_executable(test_aoti_abi_check\n  ${AOTI_ABI_CHECK_TEST_SRCS}\n)\n\n# TODO temporary until we can delete the old gtest polyfills.\ntarget_compile_definitions(test_aoti_abi_check PRIVATE USE_GTEST)\n\n# WARNING: DO NOT LINK torch!!!\n# The purpose is to check if the used aten/c10 headers are writtern in a header-only way\ntarget_link_libraries(test_aoti_abi_check PRIVATE gtest)\ntarget_include_directories(test_aoti_abi_check PRIVATE ${ATen_CPU_INCLUDE})\n\nif(INSTALL_TEST)\n  install(TARGETS test_aoti_abi_check DESTINATION bin)\n  # Install PDB files for MSVC builds\n  if(MSVC AND BUILD_SHARED_LIBS)\n    install(FILES $<TARGET_PDB_FILE:test_aoti_abi_check> DESTINATION bin OPTIONAL)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Benchmarks for Specific Operator\nDESCRIPTION: Executes benchmarks for a specific operator (add) with thread control.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/operator_benchmark/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython -m benchmark_all_test --operators add --omp-num-threads 1 --mkl-num-threads 1\n```\n\n----------------------------------------\n\nTITLE: Analyzing Batch Normalization Parameters in PyTorch\nDESCRIPTION: This code snippet represents a series of batch normalization parameter sets for various tensor shapes. Each line shows the count of occurrences, tensor shape, data type, and other parameters used in batch normalization operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/densenet121_training.txt#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 24, ((T([4, 128, 14, 14], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([4, 288, 14, 14], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f16), False, 0.1, 1e-05), {})\ncnt: 1, ((T([4, 320, 14, 14], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), False, 0.1, 1e-05), {})\n# ... (truncated for brevity)\ncnt: 1, ((T([4, 800, 14, 14], f16), T([800], f16), T([800], f16), T([800], f16), T([800], f16), False, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Documenting Additional PyTorch Benchmark Modules\nDESCRIPTION: This snippet documents additional modules related to PyTorch benchmarking, including examples, op_fuzzers, utils, and valgrind_wrapper.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/benchmark_utils.rst#2025-04-22_snippet_7\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. py:module:: torch.utils.benchmark.examples\n.. py:module:: torch.utils.benchmark.op_fuzzers\n.. py:module:: torch.utils.benchmark.utils\n.. py:module:: torch.utils.benchmark.utils.valgrind_wrapper\n```\n\n----------------------------------------\n\nTITLE: Applying Threshold Backward Pass in PyTorch\nDESCRIPTION: This snippet shows the frequency and tensor shapes for the backward pass of a thresholding operation, typically used in conjunction with ReLU or similar activation functions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/adv_inception_v3_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 2, ((T([128, 192, 8, 8], f16, stride=(131072, 64, 8, 1)), T([128, 192, 8, 8], f16), 0), {})\ncnt: 8, ((T([128, 384, 8, 8], f16, stride=(131072, 64, 8, 1)), T([128, 384, 8, 8], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Generating a Flame Graph with py-spy\nDESCRIPTION: This bash command generates a flame graph in SVG format from a test script using py-spy, which profiles both native and Python code sections and outputs a viewable file.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\npy-spy record -o profile.svg --native -- python test_tensor_tensor_add.py\n```\n\n----------------------------------------\n\nTITLE: Profiling aten.native_batch_norm.default Calls - PyTorch - Python\nDESCRIPTION: Shows batch normalization operator invocations (aten.native_batch_norm.default) with various tensor shapes, argument lists, and float16 usage. Requires input, weight, bias, running mean/var tensors, and options for training, momentum, and epsilon. Inputs are batch, channel, and spatial dimension tensors; output is normalized tensors per PyTorch specifications. Counts reflect occurrence for specific argument patterns.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/selecsls42b_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.native_batch_norm.default\ncnt: 1, ((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([128, 64, 56, 56], f16), T([64], f16), T([64], f16), T([64], f16), T([64], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 32, 56, 56], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 128, 56, 56], f16), T([128], f16), T([128], f16), T([128], f16), T([128], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([128, 144, 28, 28], f16), T([144], f16), T([144], f16), T([144], f16), T([144], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 288, 28, 28], f16), T([288], f16), T([288], f16), T([288], f16), T([288], f16), True, 0.1, 1e-05), {})\ncnt: 7, ((T([128, 304, 14, 14], f16), T([304], f16), T([304], f16), T([304], f16), T([304], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 152, 14, 14], f16), T([152], f16), T([152], f16), T([152], f16), T([152], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 960, 7, 7], f16), T([960], f16), T([960], f16), T([960], f16), T([960], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 1280, 4, 4], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 1024, 4, 4], f16), T([1024], f16), T([1024], f16), T([1024], f16), T([1024], f16), True, 0.1, 1e-05), {})\n```\n\n----------------------------------------\n\nTITLE: Log Entry for Tensor Pair (Shape [128, 32, 56, 56], f16, Stride)\nDESCRIPTION: Logs the occurrence (count 5) of a tensor pair. The first tensor has shape [128, 32, 56, 56], dtype f16, and specific strides (401408, 3136, 56, 1). The second tensor has the same shape and dtype but default strides. Likely generated during PyTorch execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\ncnt: 5, ((T([128, 32, 56, 56], f16, stride=(401408, 3136, 56, 1)), T([128, 32, 56, 56], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Summation with Tensor SymInt in PyTorch (Python)\nDESCRIPTION: Executes aten.sum with a symbolic integer which performs an element-wise summation over the specified axes of a tensora fundamental operation in loss calculations and metric aggregations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/LayoutLMForSequenceClassification_training.txt#2025-04-22_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\naten.sum.SymInt\ncnt: 1, ((T([16, 2], f16), [0], True), {})\ncnt: 1, ((T([16, 768], f16), [0], True), {})\ncnt: 60, ((T([8192, 768], f16), [0], True), {})\ncnt: 12, ((T([8192, 3072], f16), [0], True), {})\ncnt: 1, ((T([16, 512, 768], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Installing Debug Symbols for MSVC Shared Library Builds\nDESCRIPTION: Sets up installation of PDB debug files when building with MSVC and shared libraries are enabled.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/c10/xpu/CMakeLists.txt#2025-04-22_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC AND C10_XPU_BUILD_SHARED_LIBS)\n  install(FILES $<TARGET_PDB_FILE:c10_xpu> DESTINATION lib OPTIONAL)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Invoking PyTorch Operators with Specific Tensor Signatures in Python\nDESCRIPTION: These snippets provide invocation examples of core PyTorch operators with different tensor shapes, types, and parameters. They are intended for use in testing, documentation, or synthetic trace analysis of deep learning pipelines. These rely on PyTorch's operator definitions and demonstrate operations such as copy, division, mean computation, matrix multiplication, normalization (batch norm and its backward pass), loss calculation (nll_loss), activation functions (relu, threshold), and reduction operations, showing both data forward and backward passes. Inputs generally include floating-point tensors (often with the f16 datatype), integer tensors for indexing or reduction, and flags/parameters matching operator semantics.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mnasnet_100_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 32, 112, 112], f16), T([128, 3, 224, 224], f16), T([32, 3, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [False, True, False]), {})\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 1280, 7, 7], f16, stride=(1280, 1, 0, 0)), 49), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 1280, 7, 7], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 1280], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 1280], f16)), {})\nOperator: aten.native_batch_norm.default\ncnt: 2, ((T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f16), True, 0.1, 1e-05), {})\ncnt: 5, ((T([128, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f16), True, 0.1, 1e-05), {})\ncnt: 6, ((T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f16), True, 0.1, 1e-05), {})\ncnt: 2, ((T([128, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f16), True, 0.1, 1e-05), {})\ncnt: 3, ((T([128, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f16), True, 0.1, 1e-05), {})\ncnt: 4, ((T([128, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f16), True, 0.1, 1e-05), {})\ncnt: 8, ((T([128, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f16), True, 0.1, 1e-05), {})\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f16), True, 0.1, 1e-05), {})\nOperator: aten.native_batch_norm_backward.default\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16), T([1280], f16), T([1280], f16), T([1280], f16), T([1280], f32), T([1280], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 320, 7, 7], f16), T([128, 320, 7, 7], f16), T([320], f16), T([320], f16), T([320], f16), T([320], f32), T([320], f32), True, 1e-05, [True, True, True]), {})\ncnt: 8, ((T([128, 1152, 7, 7], f16), T([128, 1152, 7, 7], f16), T([1152], f16), T([1152], f16), T([1152], f16), T([1152], f32), T([1152], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 192, 7, 7], f16), T([128, 192, 7, 7], f16), T([192], f16), T([192], f16), T([192], f16), T([192], f32), T([192], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 576, 7, 7], f16), T([128, 576, 7, 7], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 576, 14, 14], f16), T([128, 576, 14, 14], f16), T([576], f16), T([576], f16), T([576], f16), T([576], f32), T([576], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 96, 14, 14], f16), T([128, 96, 14, 14], f16), T([96], f16), T([96], f16), T([96], f16), T([96], f32), T([96], f32), True, 1e-05, [True, True, True]), {})\ncnt: 6, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), T([480], f16), T([480], f16), T([480], f16), T([480], f32), T([480], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 80, 14, 14], f16), T([128, 80, 14, 14], f16), T([80], f16), T([80], f16), T([80], f16), T([80], f32), T([80], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([128, 240, 28, 28], f16), T([240], f16), T([240], f16), T([240], f16), T([240], f32), T([240], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 40, 28, 28], f16), T([128, 40, 28, 28], f16), T([40], f16), T([40], f16), T([40], f16), T([40], f32), T([40], f32), True, 1e-05, [True, True, True]), {})\ncnt: 4, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), T([120], f16), T([120], f16), T([120], f16), T([120], f32), T([120], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), True, 1e-05, [True, True, True]), {})\ncnt: 5, ((T([128, 72, 56, 56], f16), T([128, 72, 56, 56], f16), T([72], f16), T([72], f16), T([72], f16), T([72], f32), T([72], f32), True, 1e-05, [True, True, True]), {})\ncnt: 3, ((T([128, 24, 56, 56], f16), T([128, 24, 56, 56], f16), T([24], f16), T([24], f16), T([24], f16), T([24], f32), T([24], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 48, 56, 56], f16), T([128, 48, 56, 56], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 48, 112, 112], f16), T([128, 48, 112, 112], f16), T([48], f16), T([48], f16), T([48], f16), T([48], f32), T([48], f32), True, 1e-05, [True, True, True]), {})\ncnt: 1, ((T([128, 16, 112, 112], f16), T([128, 16, 112, 112], f16), T([16], f16), T([16], f16), T([16], f16), T([16], f32), T([16], f32), True, 1e-05, [True, True, True]), {})\ncnt: 2, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\nOperator: aten.relu_.default\ncnt: 2, ((T([128, 32, 112, 112], f16),), {})\ncnt: 1, ((T([128, 48, 112, 112], f16),), {})\ncnt: 1, ((T([128, 48, 56, 56], f16),), {})\ncnt: 5, ((T([128, 72, 56, 56], f16),), {})\ncnt: 1, ((T([128, 72, 28, 28], f16),), {})\ncnt: 4, ((T([128, 120, 28, 28], f16),), {})\ncnt: 1, ((T([128, 240, 28, 28], f16),), {})\ncnt: 1, ((T([128, 240, 14, 14], f16),), {})\ncnt: 6, ((T([128, 480, 14, 14], f16),), {})\ncnt: 3, ((T([128, 576, 14, 14], f16),), {})\ncnt: 1, ((T([128, 576, 7, 7], f16),), {})\ncnt: 8, ((T([128, 1152, 7, 7], f16),), {})\ncnt: 1, ((T([128, 1280, 7, 7], f16),), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16), 0), {})\ncnt: 8, ((T([128, 1152, 7, 7], f16), T([128, 1152, 7, 7], f16), 0), {})\ncnt: 1, ((T([128, 576, 7, 7], f16), T([128, 576, 7, 7], f16), 0), {})\ncnt: 3, ((T([128, 576, 14, 14], f16), T([128, 576, 14, 14], f16), 0), {})\ncnt: 6, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([128, 240, 28, 28], f16), 0), {})\ncnt: 4, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 72, 28, 28], f16), T([128, 72, 28, 28], f16), 0), {})\ncnt: 5, ((T([128, 72, 56, 56], f16), T([128, 72, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 48, 56, 56], f16), T([128, 48, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 48, 112, 112], f16), T([128, 48, 112, 112], f16), 0), {})\ncnt: 2, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Markdown Code of Conduct Document\nDESCRIPTION: A comprehensive markdown document detailing the code of conduct for the PyTorch project, including sections on pledge, standards, responsibilities, scope, enforcement, and attribution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CODE_OF_CONDUCT.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <conduct@pytorch.org>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n```\n\n----------------------------------------\n\nTITLE: Generating HTML Documentation Using make\nDESCRIPTION: This Bash command uses make to generate HTML documentation files from the `.rst` source files. It requires prerequisites to be installed (such as Python packages and katex) and outputs documentation files to the `docs/build/html` directory. The process is constrained by ensuring all dependencies are set up before execution.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_21\n\nLANGUAGE: Bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Generating Final Heuristic\nDESCRIPTION: Command to generate the final heuristic after collecting all training data\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/mm/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash generate_heuristic_mm.sh generate\n```\n\n----------------------------------------\n\nTITLE: Performing Max Pooling with Indices in PyTorch\nDESCRIPTION: This usage of the max_pool2d_with_indices operator performs a max pooling operation that also returns the indices of the max values. Useful in backward passes when unpooling is required.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/legacy_senet154_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([32, 128, 112, 112], f16), [3, 3], [2, 2], [0, 0], [1, 1], True), {})\n```\n\nLANGUAGE: text\nCODE:\n```\ncnt: 1, ((T([32, 128, 56, 56], f16), T([32, 128, 112, 112], f16), [3, 3], [2, 2], [0, 0], [1, 1], True, T([32, 128, 56, 56], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Basic Mask Computation in PyTorch\nDESCRIPTION: Demonstrates the high-level process of computing a mask by aggregating activations, reducing them, and applying a mask function.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/ao/pruning/_experimental/activation_sparsifier/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\naggregated_tensor = aggregate_fn([activation for activation in all_activations])\nreduced_tensor = reduce_fn(aggregated_tensor)\nmask = mask_fn(reduced_tensor)\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Allocation in C++\nDESCRIPTION: These functions handle memory allocation for CUDA devices. They are part of PyTorch's custom memory management system for GPUs.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104cuda20CUDACachingAllocator6Native22DeviceCachingAllocator6mallocEimP11CUstream_st\n```\n\nLANGUAGE: C++\nCODE:\n```\n_ZN3c104cuda20CUDACachingAllocator6Native22NativeCachingAllocator6mallocEPPvimP11CUstream_st\n```\n\n----------------------------------------\n\nTITLE: Setting Android NDK Path for PyTorch Build\nDESCRIPTION: Sets the ANDROID_NDK environment variable to specify the location of Android NDK required for building PyTorch on Android.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/scripts/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport ANDROID_NDK=YOUR_NDK_PATH\n```\n\n----------------------------------------\n\nTITLE: Generating Dummy Data for Model Evaluation - Python\nDESCRIPTION: Initializes test data with random tensors using the previously set random seed for reproducibility. Defines weight matrix, bias vector, and input feature vector x with dimension D=16. No external dependencies besides PyTorch. Output is a set of initialized tensors for subsequent computations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/notebooks/jacobians_hessians.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nD = 16\nweight = torch.randn(D, D)\nbias = torch.randn(D)\nx = torch.randn(D) # feature vector\n```\n\n----------------------------------------\n\nTITLE: Handling Scalar Operations in ONNX Export for PyTorch\nDESCRIPTION: Example of how scalar operations are dispatched in PyTorch's ONNX export. This snippet shows the difference in dispatch between tensor-tensor addition and tensor-scalar addition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/onnx/README.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nadd(*[self, other], **{\"alpha\": alpha})\n\nadd(*[self], **{\"other\": other, \"alpha\": alpha})\n```\n\n----------------------------------------\n\nTITLE: Layer Normalization Operations in PyTorch\nDESCRIPTION: This snippet demonstrates the usage of aten.native_layer_norm.default and aten.native_layer_norm_backward.default operators for layer normalization in neural networks.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/attention_is_all_you_need_pytorch_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.native_layer_norm.default\ncnt: 13, ((T([256, 33, 512], f16), [512], T([512], f16), T([512], f16), 1e-06), {})\ncnt: 19, ((T([256, 31, 512], f16), [512], T([512], f16), T([512], f16), 1e-06), {})\nOperator: aten.native_layer_norm_backward.default\ncnt: 19, ((T([256, 31, 512], f16), T([256, 31, 512], f16), [512], T([256, 31, 1], f32), T([256, 31, 1], f32), T([512], f16), T([512], f16), [True, True, True]), {})\ncnt: 13, ((T([256, 33, 512], f16), T([256, 33, 512], f16), [512], T([256, 33, 1], f32), T([256, 33, 1], f32), T([512], f16), T([512], f16), [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Class Documentation Template with Heading and Autodoc Directive\nDESCRIPTION: Creates a heading for the class documentation and uses the autoclass directive to automatically generate documentation for the class members. The heading is underlined using a template filter.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/_templates/classtemplate.rst#2025-04-22_snippet_2\n\nLANGUAGE: restructuredtext\nCODE:\n```\n{{ name | underline}}\n\n.. autoclass:: {{ name }}\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Registering Optimized Kernels for CPU Dispatch in C++\nDESCRIPTION: This snippet details the registration of a CPU kernel with divergence in AVX2 and AVX512 performance: using REGISTER_DISPATCH for general or inferior AVX512 performance, or ALSO_REGISTER_AVX512_DISPATCH when optimized. Benchmarks on performance can be determined by environment variables to enforce specific instruction sets during runtime.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/cpu/README.md#2025-04-22_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nREGISTER_DISPATCH(fnNameImpl, &your_kernel)\n```\n\nLANGUAGE: C++\nCODE:\n```\nALSO_REGISTER_AVX512_DISPATCH(fnNameImpl, &your_kernel)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Scalar Division Operation\nDESCRIPTION: This snippet shows a division operation where a tensor of shape [128, 1984, 7, 7] with f16 precision and specific stride pattern is divided by the scalar value 49.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/fbnetc_100_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 1984, 7, 7], f16, stride=(1984, 1, 0, 0)), 49), {})\n```\n\n----------------------------------------\n\nTITLE: Convolution Operations in DenseNet Forward Pass\nDESCRIPTION: This snippet shows the convolution operations used in the forward pass of DenseNet. It includes the initial convolutions, the convolutions in dense blocks, and the transition layers with various kernel sizes, strides, and padding configurations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.convolution.default\ncnt: 1, ((T([32, 3, 224, 224], f16), T([64, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([64, 64, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 64, 112, 112], f16), T([128, 64, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 5, ((T([32, 128, 56, 56], f16), T([128, 128, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 768, 56, 56], f16), T([256, 768, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 256, 28, 28], f16), T([160, 256, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 4, ((T([32, 160, 28, 28], f16), T([160, 160, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 1056, 28, 28], f16), T([512, 1056, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 512, 14, 14], f16), T([192, 512, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 8, ((T([32, 192, 14, 14], f16), T([192, 192, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 1472, 14, 14], f16), T([768, 1472, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 768, 14, 14], f16), T([192, 768, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 1728, 14, 14], f16), T([768, 1728, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 768, 7, 7], f16), T([224, 768, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 8, ((T([32, 224, 7, 7], f16), T([224, 224, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 1888, 7, 7], f16), T([1024, 1888, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 1024, 7, 7], f16), T([224, 1024, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([32, 2144, 7, 7], f16), T([1024, 2144, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\n----------------------------------------\n\nTITLE: NLL Loss Backward Operations in PyTorch\nDESCRIPTION: Records of negative log likelihood loss backward operations for gradient computation. The operation includes gradient output, input, target tensor, weight, reduction mode, and ignore index parameters.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Running Specific Test Method Using Pytest\nDESCRIPTION: Command to run a specific test method (multi limit single dtype) in the ProcessGroup gloo/nccl test using pytest.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/distributed/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest -vs test/distributed/test_c10d_common.py -k test_multi_limit_single_dtype\n```\n\n----------------------------------------\n\nTITLE: Updating Production Ops List for PyTorch Mobile Tests\nDESCRIPTION: This command updates the list of production ops used in PyTorch mobile tests. It requires access to Meta's internal repository and should be run regularly by a Meta employee to keep the test coverage up-to-date.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/mobile/model_test/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython test/mobile/model_test/update_production_ops.py ~/fbsource/xplat/pytorch_models/build/all_mobile_model_configs.yaml\n```\n\n----------------------------------------\n\nTITLE: Analyzing In-place Element-wise Addition Operations in PyTorch\nDESCRIPTION: Records of aten.add_.Tensor operator calls for in-place element-wise addition of tensors. All operations use half-precision (f16) tensors of various shapes, typically corresponding to different network layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_resnest_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.add_.Tensor\ncnt: 1, ((T([32, 256, 56, 56], f16), T([32, 256, 56, 56], f16)), {})\ncnt: 1, ((T([32, 512, 28, 28], f16), T([32, 512, 28, 28], f16)), {})\ncnt: 1, ((T([32, 1024, 14, 14], f16), T([32, 1024, 14, 14], f16)), {})\ncnt: 1, ((T([32, 2048, 7, 7], f16), T([32, 2048, 7, 7], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Disabling FBGEMM on Incompatible x86 Platforms in CMake\nDESCRIPTION: Checks if FBGEMM is requested (USE_FBGEMM is ON) and if the target system is x86. It specifically handles cases where the processor is reported as x86_64 but the pointer size is 4 (indicating a 32-bit build environment) or if the processor is explicitly identified as x86. In these incompatible scenarios, it forces USE_FBGEMM to OFF.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_27\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ Dependencies ---[ FBGEMM doesn't work on x86 32bit and\n# CMAKE_SYSTEM_PROCESSOR thinks its 64bit\nif(USE_FBGEMM\n   AND((CMAKE_SYSTEM_PROCESSOR STREQUAL \"x86_64\" AND CMAKE_SIZEOF_VOID_P EQUAL\n                                                      4)\n        OR CMAKE_SYSTEM_PROCESSOR STREQUAL \"x86\"))\n  set(USE_FBGEMM OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Operator and Mobile Optimization Utilities\nDESCRIPTION: Defines binary targets for utilities that dump operator names and optimize models for mobile deployment.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/binaries/CMakeLists.txt#2025-04-22_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\ncaffe2_binary_target(\"dump_operator_names.cc\")\ncaffe2_binary_target(\"optimize_for_mobile.cc\")\n```\n\n----------------------------------------\n\nTITLE: Running Individual PyTorch Python Test\nDESCRIPTION: Executes a single test within a specific test class and file, in this case the test_Sequential test in the TestJit class.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython test/test_jit.py TestJit.test_Sequential\n```\n\n----------------------------------------\n\nTITLE: Using PythonDispatcher for Dispatch Table Validation\nDESCRIPTION: Demonstrates how to use PythonDispatcher to validate the computed dispatch table for an operator.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ndispatcher = PythonDispatcher()\ndispatcher.register([\"CPU\", \"XLA\", \"AutogradCPU\", \"CompositeImplicitAutograd\"])\nprint(dispatcher.dispatchTable()) # Tells you exactly which kernel is used for certain backend.\n```\n\n----------------------------------------\n\nTITLE: Tensor Division Operations in DenseNet Loss Computation\nDESCRIPTION: This snippet shows tensor division operations where a scalar tensor (empty shape) of half-precision (f16) data type is divided by the constant 32000. This is likely used in the loss computation or normalization during training.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/timm_vovnet_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.div.Tensor\ncnt: 2, ((T([], f16), 32000), {})\n```\n\n----------------------------------------\n\nTITLE: Reshaping Tensors with Unsafe View in PyTorch\nDESCRIPTION: Reshapes tensors using the _unsafe_view operation, which can be faster but may share storage with the original tensor. Used for reshaping between 3D and 2D representations of the data.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/tts_angular_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\naten._unsafe_view.default(T([64, 50, 768], f16), [3200, 768])\n```\n\n----------------------------------------\n\nTITLE: Setting Build Configuration Flags\nDESCRIPTION: Uses the set_bool macro to convert various build configuration options to 0 or 1.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset_bool(AT_BUILD_WITH_BLAS USE_BLAS)\nset_bool(AT_BUILD_WITH_LAPACK USE_LAPACK)\nset_bool(AT_BLAS_F2C BLAS_F2C)\nset_bool(AT_BLAS_USE_CBLAS_DOT BLAS_USE_CBLAS_DOT)\nset_bool(AT_MAGMA_ENABLED USE_MAGMA)\nset_bool(CAFFE2_STATIC_LINK_CUDA_INT CAFFE2_STATIC_LINK_CUDA)\nset_bool(AT_CUDNN_ENABLED CAFFE2_USE_CUDNN)\nset_bool(AT_CUSPARSELT_ENABLED CAFFE2_USE_CUSPARSELT)\n```\n\n----------------------------------------\n\nTITLE: Average Pooling Backward Operation in PyTorch\nDESCRIPTION: This code shows the backward pass for average pooling, used during gradient computation. It propagates gradients from 1x1 spatial dimensions back to 4x4 feature maps, maintaining the same number of channels.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/LearningToPaint_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.avg_pool2d_backward.default\ncnt: 1, ((T([96, 512, 1, 1], f16), T([96, 512, 4, 4], f16), [4, 4], [], [0, 0], False, True, None), {})\n```\n\n----------------------------------------\n\nTITLE: Rebuilding PyTorch with CMake After Adding Tests\nDESCRIPTION: Shows the command to rebuild the PyTorch project using its setup script after adding a new test file. This step is necessary because test files are globbed by CMake, requiring a CMake re-run to detect new files.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/cpp/tensorexpr/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython setup.py build --cmake\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx AutoClass for SobolEngine in PyTorch\nDESCRIPTION: This RST snippet configures the Sphinx autoclass directive to generate documentation for the SobolEngine class. It excludes the MAXBIT and MAXDIM members, includes undocumented members, and is set up to work with a specific module context.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/docs/source/_templates/sobolengine.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. currentmodule:: {{ module }}\n\n\n{{ name | underline}}\n\n.. autoclass:: {{ name }}\n    :members:\n    :exclude-members: MAXBIT, MAXDIM\n    :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Source Template Attribution Comment\nDESCRIPTION: Comment indicating the template file was autogenerated from a specific source template path.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/functorch/docs/source/_templates/autosummary/class.rst#2025-04-22_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. autogenerated from source/_templates/autosummary/class.rst\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Module Path in CMake\nDESCRIPTION: Appends the project's 'cmake/Modules' directory to the CMAKE_MODULE_PATH list. This allows CMake to find custom modules defined within the project.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CMakeLists.txt#2025-04-22_snippet_20\n\nLANGUAGE: cmake\nCODE:\n```\n# ---[ CMake scripts + modules\nlist(APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake/Modules)\n```\n\n----------------------------------------\n\nTITLE: Defining a Class with String Literal Not Used as Docstring\nDESCRIPTION: A class with methods where a string literal appears between methods rather than as a docstring.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass NotDocstring:\n    def short1(self):\n        pass\n\n    \"\"\"This is not a docstring\"\"\"\n\n    def short2(self):\n        pass\n\n    def short3(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Batch Normalization Operations\nDESCRIPTION: Batch normalization forward and backward operations on tensors with shape [32, 768, 32, 32] using float16 precision, with momentum 0.1 and epsilon 1e-05.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/convmixer_768_32_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\naten.native_batch_norm.default((T([32, 768, 32, 32], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f16), True, 0.1, 1e-05), {})\naten.native_batch_norm_backward.default((T([32, 768, 32, 32], f16), T([32, 768, 32, 32], f16), T([768], f16), T([768], f16), T([768], f16), T([768], f32), T([768], f32), True, 1e-05, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Setting Parent Scope Variables\nDESCRIPTION: Propagates all test source lists to the parent CMake scope\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/test/CMakeLists.txt#2025-04-22_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(ATen_CPU_TEST_SRCS ${ATen_CPU_TEST_SRCS} PARENT_SCOPE)\nset(ATen_CUDA_TEST_SRCS ${ATen_CUDA_TEST_SRCS} PARENT_SCOPE)\nset(ATen_HIP_TEST_SRCS ${ATen_HIP_TEST_SRCS} PARENT_SCOPE)\nset(ATen_VULKAN_TEST_SRCS ${ATen_VULKAN_TEST_SRCS} PARENT_SCOPE)\nset(ATen_MOBILE_TEST_SRCS ${ATen_MOBILE_TEST_SRCS} PARENT_SCOPE)\nset(ATen_VEC_TEST_SRCS ${ATen_VEC_TEST_SRCS} PARENT_SCOPE)\nset(ATen_MPS_TEST_SRCS ${ATen_MPS_TEST_SRCS} PARENT_SCOPE)\nset(ATen_XPU_TEST_SRCS ${ATen_XPU_TEST_SRCS} PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Usage Log: aten.relu_.default Operator (Text)\nDESCRIPTION: Logs calls to the in-place ReLU activation function (`aten.relu_.default`). The arguments show the shapes of the input tensors (e.g., [128, 128, 1, 1] f16) being modified. Different counts are observed for various tensor shapes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/dm_nfnet_f0_training.txt#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 128, 1, 1], f16),), {})\ncnt: 2, ((T([128, 256, 1, 1], f16),), {})\ncnt: 9, ((T([128, 768, 1, 1], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Pulling Nightly PyTorch Updates\nDESCRIPTION: Uses the tools/nightly.py script to pull the latest nightly commits into the current branch and recreate the virtual environment with updated dependencies and binaries.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n./tools/nightly.py pull -p my-env\nsource my-env/bin/activate  # or `& .\\venv\\Scripts\\Activate.ps1` on Windows\n```\n\n----------------------------------------\n\nTITLE: Executing PyTorch Operators with Tensor Inputs\nDESCRIPTION: This code snippet shows the execution of various PyTorch operators with different tensor inputs. It includes operations like batch normalization, NLL loss, ReLU activation, and threshold backward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gernet_l_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16), T([32], f16), T([32], f16), T([32], f16), T([32], f32), T([32], f32), True, 1e-05, [True, True, True]), {})\nOperator: aten.nll_loss_backward.default\ncnt: 1, ((T([], f16), T([128, 1000], f16), T([128], i64), None, 1, -100, T([], f16)), {})\nOperator: aten.nll_loss_forward.default\ncnt: 1, ((T([128, 1000], f16), T([128], i64), None, 1, -100), {})\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 32, 128, 128], f16),), {})\ncnt: 2, ((T([128, 128, 64, 64], f16),), {})\ncnt: 4, ((T([128, 192, 32, 32], f16),), {})\ncnt: 1, ((T([128, 160, 32, 32], f16),), {})\ncnt: 11, ((T([128, 160, 16, 16], f16),), {})\ncnt: 6, ((T([128, 640, 16, 16], f16),), {})\ncnt: 1, ((T([128, 1920, 16, 16], f16),), {})\ncnt: 17, ((T([128, 1920, 8, 8], f16),), {})\ncnt: 9, ((T([128, 640, 8, 8], f16),), {})\ncnt: 1, ((T([128, 2560, 8, 8], f16),), {})\nOperator: aten.sum.SymInt\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 2560, 8, 8], f16), T([128, 2560, 8, 8], f16), 0), {})\ncnt: 9, ((T([128, 640, 8, 8], f16), T([128, 640, 8, 8], f16), 0), {})\ncnt: 17, ((T([128, 1920, 8, 8], f16), T([128, 1920, 8, 8], f16), 0), {})\ncnt: 1, ((T([128, 1920, 16, 16], f16), T([128, 1920, 16, 16], f16), 0), {})\ncnt: 6, ((T([128, 640, 16, 16], f16), T([128, 640, 16, 16], f16), 0), {})\ncnt: 11, ((T([128, 160, 16, 16], f16), T([128, 160, 16, 16], f16), 0), {})\ncnt: 1, ((T([128, 160, 32, 32], f16), T([128, 160, 32, 32], f16), 0), {})\ncnt: 4, ((T([128, 192, 32, 32], f16), T([128, 192, 32, 32], f16), 0), {})\ncnt: 2, ((T([128, 128, 64, 64], f16), T([128, 128, 64, 64], f16), 0), {})\ncnt: 1, ((T([128, 32, 128, 128], f16), T([128, 32, 128, 128], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: Generating MPS Metal Shader Headers (Fallback) in CMake\nDESCRIPTION: This is the fallback logic within the `USE_MPS` block when `CAN_COMPILE_METAL` is false. It creates a directory for native MPS artifacts in the build tree. It then iterates through the Metal shader files (`native_mps_metal`) and calls the `metal_to_metallib_h` function (presumably defined in `Metal.cmake`) to generate C++ header files containing the shader code.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/CMakeLists.txt#2025-04-22_snippet_22\n\nLANGUAGE: cmake\nCODE:\n```\n    else()\n        file(MAKE_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}/native/mps\")\n        foreach(SHADER ${native_mps_metal})\n            cmake_path(GET SHADER STEM TGT_STEM)\n            string(CONCAT SHADER_HDR_NAME  \"${CMAKE_CURRENT_BINARY_DIR}\" /native/mps/ ${TGT_STEM} \"_metallib.h\")\n            metal_to_metallib_h(${SHADER} ${SHADER_HDR_NAME})\n        endforeach()\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining a Function with Comments but No Docstring\nDESCRIPTION: A function with multiple comment lines but missing a proper docstring.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/python_code.py.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef long_without_docstring():\n    #\n    #\n    #\n    #\n    pass\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Tensor Operations in C++\nDESCRIPTION: This snippet shows function signatures for various PyTorch tensor operations implemented in C++. It includes operations like convolution, group normalization, and efficient attention mechanisms.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_29\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at4_ops10group_norm4callERKNS_6TensorElRKSt8optionalIS2_ES8_db\n_ZN2at4_ops10reciprocal4callERKNS_6TensorE\n_ZN2at4_ops11convolution4callERKNS_6TensorES4_RKSt8optionalIS2_EN3c108ArrayRefINS9_6SymIntEEESC_SC_bSC_SB_\n_ZN2at4_ops12_convolution4callERKNS_6TensorES4_RKSt8optionalIS2_EN3c108ArrayRefINS9_6SymIntEEESC_SC_bSC_SB_bbbb\n```\n\n----------------------------------------\n\nTITLE: Defining New Inflator Failure Status in C\nDESCRIPTION: Defines a new failure status TINFL_STATUS_FAILED_CANNOT_MAKE_PROGRESS with value -4 for the inflator. This status is returned when the inflator needs more input bytes to make progress but no more input is available.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/third_party/miniz-3.0.2/ChangeLog.md#2025-04-22_snippet_1\n\nLANGUAGE: C\nCODE:\n```\nTINFL_STATUS_FAILED_CANNOT_MAKE_PROGRESS (-4)\n```\n\n----------------------------------------\n\nTITLE: Raw IR Graph Structure\nDESCRIPTION: Shows the intermediate representation (IR) of the graph before optimization, with comments and annotations removed.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/test/HowToWriteTestsUsingFileCheck.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ngraph(%a : Tensor, %b : Tensor):\n      %x : Tensor = aten::mul(%a, %b)\n      %y : Tensor = aten::mul(%a, %b)\n      return (%x, %y)\n```\n\n----------------------------------------\n\nTITLE: Tensor Cloning in PyTorch\nDESCRIPTION: Creates a copy of the input tensor with the same content and size but may have a different storage.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/vgg16_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\naten.clone.default((T([64, 3, 224, 224], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage and Tensor Shapes\nDESCRIPTION: This snippet demonstrates the format used to log PyTorch operator usage, including the operator name, count, and input tensor shapes. It covers various operations like convolution, addition, and softmax, primarily using half-precision (f16) tensors.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/gluon_xception65_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([32, 1000], f16), 1, False), {})\n\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([32, 1000], f16), T([32, 1000], f16), 1, f16), {})\n\nOperator: aten.add.Tensor\ncnt: 2, ((T([32, 128, 75, 75], f16), T([32, 128, 75, 75], f16)), {})\ncnt: 2, ((T([32, 256, 38, 38], f16), T([32, 256, 38, 38], f16)), {})\ncnt: 34, ((T([32, 728, 19, 19], f16), T([32, 728, 19, 19], f16)), {})\ncnt: 1, ((T([32, 1024, 10, 10], f16), T([32, 1024, 10, 10], f16)), {})\ncnt: 1, ((T([32, 64, 150, 150], f16), T([32, 64, 150, 150], f16)), {})\n\nOperator: aten.add_.Tensor\ncnt: 132, ((T([], i64), 1), {})\n\nOperator: aten.addmm.default\ncnt: 1, ((T([1000], f16), T([32, 2048], f16), T([2048, 1000], f16, stride=(1, 2048))), {})\n\nOperator: aten.clone.default\ncnt: 1, ((T([32, 3, 299, 299], f16),), {})\n\nOperator: aten.convolution.default\ncnt: 1, ((T([32, 3, 299, 299], f16), T([32, 3, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 1), {})\n# ... (more convolution operations)\n\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([32, 2048, 10, 10], f16), T([32, 1536, 10, 10], f16), T([2048, 1536, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n# ... (more convolution backward operations)\n```\n\n----------------------------------------\n\nTITLE: Tensor View and Reshape Operations in PyTorch\nDESCRIPTION: This snippet demonstrates the usage of aten._unsafe_view.default operator for reshaping tensors with various dimensions and sizes.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/attention_is_all_you_need_pytorch_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten._unsafe_view.default\ncnt: 36, ((T([8448, 512], f16), [256, 33, 512]), {})\ncnt: 24, ((T([256, 8, 33, 64], f16), [2048, 33, 64]), {})\ncnt: 12, ((T([256, 8, 64, 33], f16), [2048, 64, 33]), {})\ncnt: 6, ((T([2048, 33, 33], f16), [256, 8, 33, 33]), {})\ncnt: 6, ((T([2048, 33, 64], f16), [256, 8, 33, 64]), {})\ncnt: 36, ((T([7936, 512], f16), [256, 31, 512]), {})\ncnt: 30, ((T([256, 8, 31, 64], f16), [2048, 31, 64]), {})\ncnt: 6, ((T([256, 8, 64, 31], f16), [2048, 64, 31]), {})\ncnt: 6, ((T([2048, 31, 31], f16), [256, 8, 31, 31]), {})\ncnt: 12, ((T([2048, 31, 64], f16), [256, 8, 31, 64]), {})\ncnt: 6, ((T([2048, 31, 33], f16), [256, 8, 31, 33]), {})\ncnt: 1, ((T([7936, 9521], f16), [256, 31, 9521]), {})\ncnt: 18, ((T([256, 33, 8, 64], f16), [256, 33, 512]), {})\ncnt: 12, ((T([256, 33, 512], f16), [8448, 512]), {})\ncnt: 18, ((T([256, 31, 8, 64], f16), [256, 31, 512]), {})\ncnt: 6, ((T([256, 31, 512], f16), [7936, 512]), {})\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Tensor Operations\nDESCRIPTION: The code snippets define tensor operations in PyTorch using half-precision floating-point (f16) format. Each operation specifies tensor dimensions, types, and additional parameters, such as stride and convolution settings. These operations likely correspond to configurations in a neural network model.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/mixnet_l_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 52, 1, 1], f16), T([624, 52, 1, 1], f16), T([624], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 624, 14, 14], f16), T([160, 624, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 6, ((T([64, 80, 14, 14], f16, stride=(31360, 196, 14, 1)), T([240, 80, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 120), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 120), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 120), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 9, 9], f16), None, [1, 1], [4, 4], [1, 1], False, [0, 0], 120), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 480, 1, 1], f16), T([80, 480, 1, 1], f16), T([80], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 80, 1, 1], f16), T([480, 80, 1, 1], f16), T([480], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 6, ((T([64, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([80, 240, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 160, 14, 14], f16), T([960, 160, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 3, 3], f16), None, [2, 2], [1, 1], [1, 1], False, [0, 0], 240), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 5, 5], f16), None, [2, 2], [2, 2], [1, 1], False, [0, 0], 240), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 7, 7], f16), None, [2, 2], [3, 3], [1, 1], False, [0, 0], 240), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 9, 9], f16), None, [2, 2], [4, 4], [1, 1], False, [0, 0], 240), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 960, 1, 1], f16), T([80, 960, 1, 1], f16), T([80], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 80, 1, 1], f16), T([960, 80, 1, 1], f16), T([960], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 960, 7, 7], f16), T([264, 960, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 264, 7, 7], f16), T([1584, 264, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 3, 3], f16), None, [1, 1], [1, 1], [1, 1], False, [0, 0], 396), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 5, 5], f16), None, [1, 1], [2, 2], [1, 1], False, [0, 0], 396), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 7, 7], f16), None, [1, 1], [3, 3], [1, 1], False, [0, 0], 396), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 9, 9], f16), None, [1, 1], [4, 4], [1, 1], False, [0, 0], 396), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 1584, 1, 1], f16), T([132, 1584, 1, 1], f16), T([132], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 132, 1, 1], f16), T([1584, 132, 1, 1], f16), T([1584], f16), [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 6, ((T([64, 792, 7, 7], f16, stride=(77616, 49, 7, 1)), T([132, 792, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 264, 7, 7], f16), T([1536, 264, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.convolution_backward.default\ncnt: 1, ((T([64, 1536, 7, 7], f16), T([64, 264, 7, 7], f16), T([1536, 264, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 6, ((T([64, 132, 7, 7], f16, stride=(12936, 49, 7, 1)), T([64, 792, 7, 7], f16, stride=(77616, 49, 7, 1)), T([132, 792, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 1584, 1, 1], f16), T([64, 132, 1, 1], f16), T([1584, 132, 1, 1], f16), [1584], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 132, 1, 1], f16), T([64, 1584, 1, 1], f16), T([132, 1584, 1, 1], f16), [132], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 9, 9], f16), [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 396, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 396, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 396, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([64, 396, 7, 7], f16, stride=(77616, 49, 7, 1)), T([396, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 396, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 1584, 7, 7], f16), T([64, 264, 7, 7], f16), T([1584, 264, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 264, 7, 7], f16), T([64, 960, 7, 7], f16), T([264, 960, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 960, 1, 1], f16), T([64, 80, 1, 1], f16), T([960, 80, 1, 1], f16), [960], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 80, 1, 1], f16), T([64, 960, 1, 1], f16), T([80, 960, 1, 1], f16), [80], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 7, 7], f16, stride=(47040, 49, 7, 1)), T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 9, 9], f16), [0], [2, 2], [4, 4], [1, 1], False, [0, 0], 240, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 7, 7], f16, stride=(47040, 49, 7, 1)), T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 7, 7], f16), [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 240, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 7, 7], f16, stride=(47040, 49, 7, 1)), T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 5, 5], f16), [0], [2, 2], [2, 2], [1, 1], False, [0, 0], 240, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 240, 7, 7], f16, stride=(47040, 49, 7, 1)), T([64, 240, 14, 14], f16, stride=(188160, 196, 14, 1)), T([240, 1, 3, 3], f16), [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 240, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 960, 14, 14], f16), T([64, 160, 14, 14], f16), T([960, 160, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 6, ((T([64, 80, 14, 14], f16, stride=(31360, 196, 14, 1)), T([64, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([80, 240, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 480, 1, 1], f16), T([64, 80, 1, 1], f16), T([480, 80, 1, 1], f16), [480], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 80, 1, 1], f16), T([64, 480, 1, 1], f16), T([80, 480, 1, 1], f16), [80], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 9, 9], f16), [0], [1, 1], [4, 4], [1, 1], False, [0, 0], 120, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 7, 7], f16), [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 120, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 5, 5], f16), [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 120, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 120, 14, 14], f16, stride=(94080, 196, 14, 1)), T([120, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 120, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 6, ((T([64, 240, 14, 14], f16, stride=(94080, 196, 14, 1)), T([64, 80, 14, 14], f16, stride=(31360, 196, 14, 1)), T([240, 80, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 160, 14, 14], f16), T([64, 624, 14, 14], f16), T([160, 624, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 624, 1, 1], f16), T([64, 52, 1, 1], f16), T([624, 52, 1, 1], f16), [624], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 52, 1, 1], f16), T([64, 624, 1, 1], f16), T([52, 624, 1, 1], f16), [52], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 624, 14, 14], f16), T([64, 624, 14, 14], f16), T([624, 1, 3, 3], f16), [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 624, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([64, 624, 14, 14], f16), T([64, 104, 14, 14], f16), T([624, 104, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 6, ((T([64, 52, 14, 14], f16, stride=(20384, 196, 14, 1)), T([64, 312, 14, 14], f16, stride=(122304, 196, 14, 1)), T([52, 312, 1, 1], f16), [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 624, 1, 1], f16), T([64, 26, 1, 1], f16), T([624, 26, 1, 1], f16), [624], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 3, ((T([64, 26, 1, 1], f16), T([64, 624, 1, 1], f16), T([26, 624, 1, 1], f16), [26], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Summing Tensors using ATen in Python\nDESCRIPTION: Uses the \\\"aten.sum.SymInt\\\" operator in ATen to sum across specified dimensions for tensors with different shapes. The operation is demonstrated with f16 tensors, showcasing summation with a keepdim parameter. Dependencies include a working PyTorch setup with accessible ATen functionality.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 768, 7, 7], f16), [0], True), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 384, 14, 14], f16), [0], True), {})\n```\n\nLANGUAGE: Python\nCODE:\n```\ncnt: 1, ((T([128, 192, 28, 28], f16), [0], True), {})\n```\n\n----------------------------------------\n\nTITLE: Max Pooling Operations in PyTorch\nDESCRIPTION: This snippet demonstrates max pooling operations in PyTorch, including both forward and backward passes. It shows the use of 2D max pooling with specific kernel sizes, strides, and padding.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/res2next50_training.txt#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.max_pool2d_with_indices.default\ncnt: 1, ((T([128, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1]), {})\n\nOperator: aten.max_pool2d_with_indices_backward.default\ncnt: 1, ((T([128, 64, 56, 56], f16), T([128, 64, 112, 112], f16), [3, 3], [2, 2], [1, 1], [1, 1], False, T([128, 64, 56, 56], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Activation and Normalization Operations in PyTorch Model\nDESCRIPTION: This code shows the activation functions (GELU) and batch normalization operations used throughout the model architecture. It includes both forward and backward operations for training, with different tensor shapes at various layers of the network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/visformer_small_training.txt#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([128, 3, 224, 224], f16), T([128, 3, 224, 224], f16)), {})\nOperator: aten.div.Scalar\ncnt: 1, ((T([128, 768, 7, 7], f16, stride=(768, 1, 0, 0)), 49), {})\nOperator: aten.gelu.default\ncnt: 14, ((T([128, 384, 28, 28], f16),), {})\ncnt: 4, ((T([128, 1536, 14, 14], f16),), {})\ncnt: 4, ((T([128, 3072, 7, 7], f16),), {})\nOperator: aten.gelu_backward.default\ncnt: 4, ((T([128, 3072, 7, 7], f16), T([128, 3072, 7, 7], f16)), {})\ncnt: 4, ((T([128, 1536, 14, 14], f16), T([128, 1536, 14, 14], f16)), {})\ncnt: 14, ((T([128, 384, 28, 28], f16), T([128, 384, 28, 28], f16)), {})\nOperator: aten.lift_fresh_copy.default\ncnt: 1, ((T([128], i64),), {})\nOperator: aten.mean.dim\ncnt: 1, ((T([128, 768, 7, 7], f16), [-1, -2], True), {})\nOperator: aten.mm.default\ncnt: 1, ((T([128, 1000], f16), T([1000, 768], f16)), {})\ncnt: 1, ((T([1000, 128], f16, stride=(1, 1000)), T([128, 768], f16)), {})\nOperator: aten.mul.Tensor\ncnt: 8, ((T([128, 6, 196, 196], f16), 0.125), {})\ncnt: 8, ((T([128, 6, 49, 49], f16), 0.08838834764831845), {})\n```\n\n----------------------------------------\n\nTITLE: Copy Tensor Content in PyTorch\nDESCRIPTION: Data on aten.copy_.default reveals operator usage for copying data from one tensor to another, especially applicable in preserving tensor values through transformations in operations.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/hf_train/DebertaV2ForQuestionAnswering_training.txt#2025-04-22_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nOperator: aten.copy_.default\ncnt: 1, ((T([1, 512], i64), T([1, 512], i64)), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing PyTorch Operator Usage\nDESCRIPTION: This code snippet demonstrates the usage of various PyTorch operators in a neural network model. It includes operations like convolutions, pooling, tensor additions, and more, with their corresponding tensor shapes and execution counts.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/nasnetalarge_training.txt#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten._log_softmax.default\ncnt: 1, ((T([16, 1000], f16), 1, False), {})\nOperator: aten._log_softmax_backward_data.default\ncnt: 1, ((T([16, 1000], f16), T([16, 1000], f16), 1, f16), {})\nOperator: aten.add.Tensor\ncnt: 1, ((T([], i64), 1), {})\ncnt: 6, ((T([16, 42, 83, 83], f16), T([16, 42, 83, 83], f16)), {})\n# ... (truncated for brevity)\nOperator: aten.convolution.default\ncnt: 1, ((T([16, 3, 331, 331], f16), T([96, 3, 3, 3], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 96, 165, 165], f16), T([42, 96, 1, 1], f16), None, [1, 1], [0, 0], [1, 1], False, [0, 0], 1), {})\ncnt: 1, ((T([16, 42, 169, 169], f16), T([42, 1, 5, 5], f16), None, [2, 2], [0, 0], [1, 1], False, [0, 0], 42), {})\n```\n\n----------------------------------------\n\nTITLE: Tracking SiLU Activation Operations in PyTorch\nDESCRIPTION: Records of SiLU (Swish) activation forward and backward operations with various tensor shapes. Shows the inplace operations (_) and corresponding backward passes for different feature maps.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/tinynet_a_training.txt#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.silu_.default\ncnt: 2, ((T([128, 32, 96, 96], f16),), {})\ncnt: 1, ((T([128, 8, 1, 1], f16),), {})\ncnt: 1, ((T([128, 96, 96, 96], f16),), {})\ncnt: 1, ((T([128, 96, 48, 48], f16),), {})\ncnt: 1, ((T([128, 4, 1, 1], f16),), {})\ncnt: 3, ((T([128, 144, 48, 48], f16),), {})\ncnt: 2, ((T([128, 6, 1, 1], f16),), {})\ncnt: 1, ((T([128, 144, 24, 24], f16),), {})\ncnt: 3, ((T([128, 240, 24, 24], f16),), {})\ncnt: 2, ((T([128, 10, 1, 1], f16),), {})\ncnt: 1, ((T([128, 240, 12, 12], f16),), {})\ncnt: 8, ((T([128, 480, 12, 12], f16),), {})\ncnt: 4, ((T([128, 20, 1, 1], f16),), {})\ncnt: 7, ((T([128, 672, 12, 12], f16),), {})\ncnt: 4, ((T([128, 28, 1, 1], f16),), {})\ncnt: 1, ((T([128, 672, 6, 6], f16),), {})\ncnt: 10, ((T([128, 1152, 6, 6], f16),), {})\ncnt: 5, ((T([128, 48, 1, 1], f16),), {})\ncnt: 1, ((T([128, 1280, 6, 6], f16),), {})\nOperator: aten.silu_backward.default\ncnt: 1, ((T([128, 1280, 6, 6], f16), T([128, 1280, 6, 6], f16)), {})\ncnt: 5, ((T([128, 48, 1, 1], f16), T([128, 48, 1, 1], f16)), {})\ncnt: 10, ((T([128, 1152, 6, 6], f16), T([128, 1152, 6, 6], f16)), {})\ncnt: 4, ((T([128, 28, 1, 1], f16), T([128, 28, 1, 1], f16)), {})\ncnt: 1, ((T([128, 672, 6, 6], f16), T([128, 672, 6, 6], f16)), {})\ncnt: 7, ((T([128, 672, 12, 12], f16), T([128, 672, 12, 12], f16)), {})\ncnt: 4, ((T([128, 20, 1, 1], f16), T([128, 20, 1, 1], f16)), {})\ncnt: 8, ((T([128, 480, 12, 12], f16), T([128, 480, 12, 12], f16)), {})\ncnt: 2, ((T([128, 10, 1, 1], f16), T([128, 10, 1, 1], f16)), {})\ncnt: 1, ((T([128, 240, 12, 12], f16), T([128, 240, 12, 12], f16)), {})\ncnt: 3, ((T([128, 240, 24, 24], f16), T([128, 240, 24, 24], f16)), {})\ncnt: 2, ((T([128, 6, 1, 1], f16), T([128, 6, 1, 1], f16)), {})\ncnt: 1, ((T([128, 144, 24, 24], f16), T([128, 144, 24, 24], f16)), {})\ncnt: 3, ((T([128, 144, 48, 48], f16), T([128, 144, 48, 48], f16)), {})\ncnt: 1, ((T([128, 4, 1, 1], f16), T([128, 4, 1, 1], f16)), {})\ncnt: 1, ((T([128, 96, 48, 48], f16), T([128, 96, 48, 48], f16)), {})\ncnt: 1, ((T([128, 96, 96, 96], f16), T([128, 96, 96, 96], f16)), {})\ncnt: 1, ((T([128, 8, 1, 1], f16), T([128, 8, 1, 1], f16)), {})\ncnt: 2, ((T([128, 32, 96, 96], f16), T([128, 32, 96, 96], f16)), {})\n```\n\n----------------------------------------\n\nTITLE: Logging aten.relu.default Operation Arguments\nDESCRIPTION: These log entries record calls to the `aten.relu.default` PyTorch operator. Each line shows the count (`cnt`) for a specific input tensor shape and data type (`f16`). The arguments are enclosed in a tuple `((T([shape], dtype),), {})`, indicating a single tensor input and no keyword arguments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/fambench_dlrm_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: plaintext\nCODE:\n```\nOperator: aten.relu.default\ncnt: 3, ((T([1024, 1500], f16),), {})\ncnt: 1, ((T([1024, 192], f16),), {})\ncnt: 9, ((T([1024, 4000], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Analyzing ReLU Activation Operations in PyTorch\nDESCRIPTION: This snippet demonstrates the usage of ReLU (Rectified Linear Unit) activation operations on tensors with various shapes. It shows in-place ReLU operations on 4D tensors with different channel and spatial dimensions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/torchbench_train/shufflenet_v2_x1_0_training.txt#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.relu_.default\ncnt: 1, ((T([128, 24, 112, 112], f16),), {})\ncnt: 8, ((T([128, 58, 28, 28], f16),), {})\ncnt: 1, ((T([128, 58, 56, 56], f16),), {})\ncnt: 16, ((T([128, 116, 14, 14], f16),), {})\ncnt: 1, ((T([128, 116, 28, 28], f16),), {})\ncnt: 8, ((T([128, 232, 7, 7], f16),), {})\ncnt: 1, ((T([128, 232, 14, 14], f16),), {})\ncnt: 1, ((T([128, 1024, 7, 7], f16),), {})\n```\n\n----------------------------------------\n\nTITLE: Profiling PyTorch Sum Operator with SymInt - Python\nDESCRIPTION: This snippet contains input configurations for ATen sum.SymInt operator, summing tensors along specified axes, possibly with symbolic-size handling and stride variations. Each entry specifies a tensor, axes (list of ints), and whether to keep dimensions. Inputs highlight high-dimensional tensor reductions. Requires PyTorch supporting SymInt for shape/dim symbolic inference.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/eca_halonext26ts_training.txt#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ncnt: 1, ((T([128, 1000], f16), [0], True), {})\ncnt: 1, ((T([1024, 8, 12, 8, 12], f16, stride=(9216, 144, 1, 1152, 12)), [2], True), {})\ncnt: 1, ((T([1024, 8, 12, 8, 12], f16, stride=(9216, 1152, 12, 144, 1)), [2], True), {})\ncnt: 1, ((T([4096, 4, 12, 4, 12], f16, stride=(2304, 144, 1, 576, 12)), [2], True), {})\ncnt: 1, ((T([4096, 4, 12, 4, 12], f16, stride=(2304, 576, 12, 144, 1)), [2], True), {})\ncnt: 1, ((T([4096, 8, 12, 8, 12], f16, stride=(9216, 144, 1, 1152, 12)), [2], True), {})\ncnt: 1, ((T([4096, 8, 12, 8, 12], f16, stride=(9216, 1152, 12, 144, 1)), [2], True), {})\ncnt: 1, ((T([128, 256, 16, 16], f16), [2, 3], True), {})\ncnt: 2, ((T([128, 128, 32, 32], f16), [2, 3], True), {})\ncnt: 2, ((T([128, 64, 64, 64], f16), [2, 3], True), {})\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.stack.default Operator (Log)\nDESCRIPTION: Log entries detailing calls to the PyTorch `aten.stack.default` operator. Each line shows the invocation count (`cnt`) and the list of input tensors being stacked, including their shapes, data types (f16), and sometimes strides.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_7\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.stack.default\ncnt: 1, (([T([64, 1000], f16), T([64, 1000], f16)],), {})\ncnt: 9, (([T([64, 4, 197, 64], f16), T([64, 4, 197, 64], f16, stride=(50432, 12608, 1, 197)), T([64, 4, 197, 64], f16)],), {})\ncnt: 3, (([T([64, 4, 401, 32], f16), T([64, 4, 401, 32], f16, stride=(51328, 12832, 1, 401)), T([64, 4, 401, 32], f16)],), {})\n```\n\n----------------------------------------\n\nTITLE: Threshold Backward Operations in PyTorch\nDESCRIPTION: Records of backward operations for threshold/ReLU activations with various tensor shapes. Each operation computes gradients for the threshold function with threshold value of 0, corresponding to ReLU backward pass.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/spnasnet_100_training.txt#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nOperator: aten.threshold_backward.default\ncnt: 1, ((T([128, 1280, 7, 7], f16), T([128, 1280, 7, 7], f16), 0), {})\ncnt: 8, ((T([128, 1152, 7, 7], f16), T([128, 1152, 7, 7], f16), 0), {})\ncnt: 1, ((T([128, 576, 7, 7], f16), T([128, 576, 7, 7], f16), 0), {})\ncnt: 1, ((T([128, 576, 14, 14], f16), T([128, 576, 14, 14], f16), 0), {})\ncnt: 6, ((T([128, 288, 14, 14], f16), T([128, 288, 14, 14], f16), 0), {})\ncnt: 2, ((T([128, 480, 14, 14], f16), T([128, 480, 14, 14], f16), 0), {})\ncnt: 7, ((T([128, 240, 14, 14], f16), T([128, 240, 14, 14], f16), 0), {})\ncnt: 1, ((T([128, 240, 28, 28], f16), T([128, 240, 28, 28], f16), 0), {})\ncnt: 6, ((T([128, 120, 28, 28], f16), T([128, 120, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 144, 28, 28], f16), T([128, 144, 28, 28], f16), 0), {})\ncnt: 1, ((T([128, 144, 56, 56], f16), T([128, 144, 56, 56], f16), 0), {})\ncnt: 4, ((T([128, 72, 56, 56], f16), T([128, 72, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 48, 56, 56], f16), T([128, 48, 56, 56], f16), 0), {})\ncnt: 1, ((T([128, 48, 112, 112], f16), T([128, 48, 112, 112], f16), 0), {})\ncnt: 2, ((T([128, 32, 112, 112], f16), T([128, 32, 112, 112], f16), 0), {})\n```\n\n----------------------------------------\n\nTITLE: CMake Module Path References\nDESCRIPTION: References to key CMake module paths that are part of the CUDA compilation fixes implementation. These paths are crucial for maintaining compatibility across different CMake versions.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/Modules_CUDA_fix/README.md#2025-04-22_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n./upstream/CMakeInitializeConfigs.cmake\\n./upstream/FindCUDA.cmake\\n./FindCUDA.cmake\n```\n\n----------------------------------------\n\nTITLE: Invoking aten.upsample_bicubic2d.vec Operator (Log)\nDESCRIPTION: Log entry detailing a call to the PyTorch `aten.upsample_bicubic2d.vec` operator. It shows the invocation count (`cnt`) and arguments: the input tensor (shape, dtype f16), the target output size, an align_corners flag (False), and scale factors (None, inferred from output size).\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/crossvit_9_240_training.txt#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nOperator: aten.upsample_bicubic2d.vec\ncnt: 1, ((T([64, 3, 240, 240], f16), [224, 224], False, None), {})\n```\n\n----------------------------------------\n\nTITLE: Defining Tensor Method Variants in PyTorch\nDESCRIPTION: This snippet explains how to define whether a Tensor method or a namespace function is generated from a declaration. It highlights the requirement for a 'Tensor self' argument in method variants and provides an example with the 'where' function. Common usage patterns are also discussed, differentiating between core Tensor operations and complex neural network layers.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#2025-04-22_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nvariants: function, method\n```\n\n----------------------------------------\n\nTITLE: Backward Convolution Operations in PyTorch\nDESCRIPTION: Multiple backward convolution operations corresponding to the forward passes. These operations compute gradients with respect to inputs, weights, and biases for the convolution layers in the neural network.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/operator_inp_logs/timm_train/ghostnet_100_training.txt#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n((T([128, 1280, 1, 1], f16), T([128, 960, 1, 1], f16), T([1280, 960, 1, 1], f16), [1280], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, True]), {})\n```\n\n----------------------------------------\n\nTITLE: Implementing CUDA Tensor Addition in PyTorch\nDESCRIPTION: This function implements tensor addition for CUDA devices in PyTorch. It takes two input tensors and a scalar value, performing element-wise addition.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/cmake/prioritized_text.txt#2025-04-22_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\n_ZN2at12_GLOBAL__N_123wrapper_CUDA_add_TensorERKNS_6TensorES3_RKN3c106ScalarE\n```\n\n----------------------------------------\n\nTITLE: Updating Version Map for PyTorch Operators\nDESCRIPTION: Demonstrates how to update the version_map.cpp file with new operator versions and their corresponding upgraders.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/operator_upgraders/README.md#2025-04-22_snippet_7\n\nLANGUAGE: C++\nCODE:\n```\n{{\"aten::linspace\",\n  {{12,\n    \"linspace_0_11\",\n    \"aten::linspace(Scalar start, Scalar end, int? steps=None, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\"}}},\n  {{8,\n    \"linspace_0_7\",\n    \"aten::linspace(Scalar start, Scalar end, int? steps=None, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\"}}},\n\n```\n\n----------------------------------------\n\nTITLE: Finalizing and Exporting CMake Variables to Parent Scope\nDESCRIPTION: Propagates the collected source files and dependency libraries to the parent scope. This ensures that the main CMake configuration can access these variables for the final build configuration.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/caffe2/perfkernels/CMakeLists.txt#2025-04-22_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nset(Caffe2_CPU_SRCS ${Caffe2_CPU_SRCS} PARENT_SCOPE)\nset(Caffe2_DEPENDENCY_WHOLE_LINK_LIBS\n    ${Caffe2_DEPENDENCY_WHOLE_LINK_LIBS}\n    PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Pip-compile Command for Generating Bazel Requirements\nDESCRIPTION: The pip-compile command that was used to generate this requirements file. It specifies the --allow-unsafe flag and --generate-hashes option while compiling the requirements.in file from the tools/build/bazel directory.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/build/bazel/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip-compile --allow-unsafe --generate-hashes tools/build/bazel/requirements.in\n```\n\n----------------------------------------\n\nTITLE: Training AutoHeuristic Decision Tree\nDESCRIPTION: Command to train a decision tree heuristic using collected data and generate Python code for the learned heuristic.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/torchgen/_autoheuristic/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython torchgen/_autoheuristic/train_decision.py train.txt --heuristic-name SimpleHeuristic\n```\n\n----------------------------------------\n\nTITLE: Setting Up Google Benchmark for QNNPACK in CMake\nDESCRIPTION: Configures the Google Benchmark library for QNNPACK benchmarks if it's not already available. This snippet is conditionally executed based on the PYTORCH_QNNPACK_BUILD_BENCHMARKS flag.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/quantized/cpu/qnnpack/CMakeLists.txt#2025-04-22_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nif(PYTORCH_QNNPACK_BUILD_BENCHMARKS)\n  # ---[ Build google benchmark\n  if(NOT TARGET benchmark)\n    set(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL \"\")\n    add_subdirectory(\n      \"${GOOGLEBENCHMARK_SOURCE_DIR}\"\n      \"${CONFU_DEPENDENCIES_BINARY_DIR}/googlebenchmark\")\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Initializing a Python Class with Comments\nDESCRIPTION: This snippet defines a class named 'LintInit' with an __init__ method containing placeholder comments.\nSOURCE: https://github.com/pytorch/pytorch/blob/main/tools/test/docstring_linter_testdata/more_python_code.py.txt#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass LintInit:\n    def __init__(self) -> None:\n        # Lots of lines!\n        # Lots of lines!\n        pass\n```"
  }
]
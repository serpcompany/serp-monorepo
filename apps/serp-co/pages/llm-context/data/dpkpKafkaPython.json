[
  {
    "owner": "dpkp",
    "repo": "kafka-python",
    "content": "TITLE: Consuming Messages with KafkaConsumer in Python\nDESCRIPTION: Demonstrates how to use KafkaConsumer to read messages from Kafka topics. Shows various configurations including auto-commit, custom deserializers for JSON and msgpack, consumer timeouts, regex topic patterns, and parallel consumers.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/usage.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import KafkaConsumer\nimport json\nimport msgpack\n\n# To consume latest messages and auto-commit offsets\nconsumer = KafkaConsumer('my-topic',\n                         group_id='my-group',\n                         bootstrap_servers=['localhost:9092'])\nfor message in consumer:\n    # message value and key are raw bytes -- decode if necessary!\n    # e.g., for unicode: `message.value.decode('utf-8')`\n    print (\"%s:%d:%d: key=%s value=%s\" % (message.topic, message.partition,\n                                          message.offset, message.key,\n                                          message.value))\n\n# consume earliest available messages, don't commit offsets\nKafkaConsumer(auto_offset_reset='earliest', enable_auto_commit=False)\n\n# consume json messages\nKafkaConsumer(value_deserializer=lambda m: json.loads(m.decode('ascii')))\n\n# consume msgpack\nKafkaConsumer(value_deserializer=msgpack.unpackb)\n\n# StopIteration if no message after 1sec\nKafkaConsumer(consumer_timeout_ms=1000)\n\n# Subscribe to a regex topic pattern\nconsumer = KafkaConsumer()\nconsumer.subscribe(pattern='^awesome.*')\n\n# Use multiple consumers in parallel w/ 0.9 kafka brokers\n# typically you would run each on a different server / process / CPU\nconsumer1 = KafkaConsumer('my-topic',\n                          group_id='my-group',\n                          bootstrap_servers='my.server.com')\nconsumer2 = KafkaConsumer('my-topic',\n                          group_id='my-group',\n                          bootstrap_servers='my.server.com')\n```\n\n----------------------------------------\n\nTITLE: Producing Messages with KafkaProducer in Python\nDESCRIPTION: Shows how to use KafkaProducer to send messages to Kafka topics. Covers synchronous and asynchronous sending, keyed messages, custom serializers for JSON and msgpack, callbacks, and configuration options like retries.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/usage.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import KafkaProducer\nfrom kafka.errors import KafkaError\nimport msgpack\nimport json\n\nproducer = KafkaProducer(bootstrap_servers=['broker1:1234'])\n\n# Asynchronous by default\nfuture = producer.send('my-topic', b'raw_bytes')\n\n# Block for 'synchronous' sends\ntry:\n    record_metadata = future.get(timeout=10)\nexcept KafkaError:\n    # Decide what to do if produce request failed...\n    log.exception()\n    pass\n\n# Successful result returns assigned partition and offset\nprint (record_metadata.topic)\nprint (record_metadata.partition)\nprint (record_metadata.offset)\n\n# produce keyed messages to enable hashed partitioning\nproducer.send('my-topic', key=b'foo', value=b'bar')\n\n# encode objects via msgpack\nproducer = KafkaProducer(value_serializer=msgpack.dumps)\nproducer.send('msgpack-topic', {'key': 'value'})\n\n# produce json messages\nproducer = KafkaProducer(value_serializer=lambda m: json.dumps(m).encode('ascii'))\nproducer.send('json-topic', {'key': 'value'})\n\n# produce asynchronously\nfor _ in range(100):\n    producer.send('my-topic', b'msg')\n\ndef on_send_success(record_metadata):\n    print(record_metadata.topic)\n    print(record_metadata.partition)\n    print(record_metadata.offset)\n\ndef on_send_error(excp):\n    log.error('I am an errback', exc_info=excp)\n    # handle exception\n\n# produce asynchronously with callbacks\nproducer.send('my-topic', b'raw_bytes').add_callback(on_send_success).add_errback(on_send_error)\n\n# block until all async messages are sent\nproducer.flush()\n\n# configure multiple retries\nproducer = KafkaProducer(retries=5)\n```\n\n----------------------------------------\n\nTITLE: Managing Kafka Topics and Consumer Groups with KafkaAdminClient in Python\nDESCRIPTION: Shows how to use KafkaAdminClient for administrative operations such as creating and deleting topics, listing consumer groups, and retrieving consumer group details and offsets.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/usage.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import KafkaAdminClient\nfrom kafka.admin import NewTopic\n\nadmin = KafkaAdminClient(bootstrap_servers=['broker1:1234'])\n\n# create a new topic\ntopics_list = []\ntopics_list.append(NewTopic(name=\"testtopic\", num_partitions=1, replication_factor=1))\nadmin.create_topics(topics_list,timeout_ms=None, validate_only=False)\n\n# delete a topic\nadmin.delete_topics(['testtopic'])\n\n# list consumer groups\nprint(admin.list_consumer_groups())\n\n# get consumer group details\nprint(admin.describe_consumer_groups('cft-plt-qa.connect'))\n\n# get consumer group offset\nprint(admin.list_consumer_group_offsets('cft-plt-qa.connect'))\n```\n\n----------------------------------------\n\nTITLE: Basic Producer Usage\nDESCRIPTION: Example of using KafkaProducer to send multiple messages to a topic.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import KafkaProducer\nproducer = KafkaProducer(bootstrap_servers='localhost:1234')\nfor _ in range(100):\n    producer.send('foobar', b'some_message_bytes')\n```\n\n----------------------------------------\n\nTITLE: Basic Kafka Consumer Usage\nDESCRIPTION: Simple example of using KafkaConsumer to read messages from a topic. Shows how to create a consumer and iterate over messages.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import KafkaConsumer\nconsumer = KafkaConsumer('my_favorite_topic')\nfor msg in consumer:\n    print (msg)\n```\n\n----------------------------------------\n\nTITLE: Basic KafkaProducer Usage in Python\nDESCRIPTION: Demonstrates how to create a KafkaProducer instance and send messages to a topic.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import KafkaProducer\nproducer = KafkaProducer(bootstrap_servers='localhost:1234')\nfor _ in range(100):\n    producer.send('foobar', b'some_message_bytes')\n```\n\n----------------------------------------\n\nTITLE: JSON Message Serialization\nDESCRIPTION: Shows how to configure a producer to automatically serialize messages as JSON.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport json\nproducer = KafkaProducer(value_serializer=lambda v: json.dumps(v).encode('utf-8'))\nproducer.send('fizzbuzz', {'foo': 'bar'})\n```\n\n----------------------------------------\n\nTITLE: Basic KafkaConsumer Usage in Python\nDESCRIPTION: Demonstrates how to create a KafkaConsumer instance and iterate over messages from a topic.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import KafkaConsumer\nconsumer = KafkaConsumer('my_favorite_topic')\nfor msg in consumer:\n    print (msg)\n```\n\n----------------------------------------\n\nTITLE: KafkaProducer with Compression in Python\nDESCRIPTION: Demonstrates how to create a KafkaProducer that compresses messages using gzip.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nproducer = KafkaProducer(compression_type='gzip')\nfor i in range(1000):\n    producer.send('foobar', b'msg %d' % i)\n```\n\n----------------------------------------\n\nTITLE: Consumer Group Implementation\nDESCRIPTION: Example of creating a consumer with group functionality for dynamic partition assignment and offset commits.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import KafkaConsumer\nconsumer = KafkaConsumer('my_favorite_topic', group_id='my_favorite_group')\nfor msg in consumer:\n    print (msg)\n```\n\n----------------------------------------\n\nTITLE: KafkaConsumer with Consumer Group in Python\nDESCRIPTION: Shows how to create a KafkaConsumer that joins a consumer group for dynamic partition assignment and offset commits.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import KafkaConsumer\nconsumer = KafkaConsumer('my_favorite_topic', group_id='my_favorite_group')\nfor msg in consumer:\n    print (msg)\n```\n\n----------------------------------------\n\nTITLE: KafkaConsumer with Custom Deserialization in Python\nDESCRIPTION: Shows how to use a custom deserialization function (msgpack.loads) for message values in a KafkaConsumer.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconsumer = KafkaConsumer(value_deserializer=msgpack.loads)\nconsumer.subscribe(['msgpackfoo'])\nfor msg in consumer:\n    assert isinstance(msg.value, dict)\n```\n\n----------------------------------------\n\nTITLE: Manual Partition Assignment\nDESCRIPTION: Demonstrates how to manually assign specific partitions to a consumer using TopicPartition.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import TopicPartition\nconsumer = KafkaConsumer(bootstrap_servers='localhost:1234')\nconsumer.assign([TopicPartition('foobar', 2)])\nmsg = next(consumer)\n```\n\n----------------------------------------\n\nTITLE: Manually Assigning Partitions to KafkaConsumer in Python\nDESCRIPTION: Demonstrates how to manually assign a specific partition to a KafkaConsumer instance.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka import TopicPartition\nconsumer = KafkaConsumer(bootstrap_servers='localhost:1234')\nconsumer.assign([TopicPartition('foobar', 2)])\nmsg = next(consumer)\n```\n\n----------------------------------------\n\nTITLE: KafkaProducer with String Key Serialization in Python\nDESCRIPTION: Shows how to use a custom key serialization function with KafkaProducer to send string keys.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nproducer = KafkaProducer(key_serializer=str.encode)\nproducer.send('flipflap', key='ping', value=b'1234')\n```\n\n----------------------------------------\n\nTITLE: Keyed Message Production\nDESCRIPTION: Example of sending messages with keys for hashed partitioning.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nproducer.send('foobar', key=b'foo', value=b'bar')\n```\n\n----------------------------------------\n\nTITLE: KafkaProducer with Keyed Messages in Python\nDESCRIPTION: Shows how to send a message with a key for hashed-partitioning using KafkaProducer.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nproducer.send('foobar', key=b'foo', value=b'bar')\n```\n\n----------------------------------------\n\nTITLE: Synchronous Message Production\nDESCRIPTION: Demonstrates blocking until a message is sent using future.get().\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfuture = producer.send('foobar', b'another_message')\nresult = future.get(timeout=60)\n```\n\n----------------------------------------\n\nTITLE: Blocking KafkaProducer Send in Python\nDESCRIPTION: Shows how to send a message with KafkaProducer and block until it's sent or a timeout occurs.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfuture = producer.send('foobar', b'another_message')\nresult = future.get(timeout=60)\n```\n\n----------------------------------------\n\nTITLE: Producer Flush Operation\nDESCRIPTION: Shows how to flush pending messages to the network.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nproducer.flush()\n```\n\n----------------------------------------\n\nTITLE: Flushing KafkaProducer in Python\nDESCRIPTION: Demonstrates how to flush all pending messages in a KafkaProducer.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nproducer.flush()\n```\n\n----------------------------------------\n\nTITLE: Message Compression\nDESCRIPTION: Demonstrates how to enable message compression in the producer.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nproducer = KafkaProducer(compression_type='gzip')\nfor i in range(1000):\n    producer.send('foobar', b'msg %d' % i)\n```\n\n----------------------------------------\n\nTITLE: Including Record Headers\nDESCRIPTION: Shows how to include headers when sending messages.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nproducer.send('foobar', value=b'c29tZSB2YWx1ZQ==', headers=[('content-encoding', b'base64')])\n```\n\n----------------------------------------\n\nTITLE: Accessing Record Headers\nDESCRIPTION: Example of accessing message headers from consumed records.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor msg in consumer:\n    print (msg.headers)\n```\n\n----------------------------------------\n\nTITLE: String Key Serialization\nDESCRIPTION: Example of configuring a producer to serialize string keys.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nproducer = KafkaProducer(key_serializer=str.encode)\nproducer.send('flipflap', key='ping', value=b'1234')\n```\n\n----------------------------------------\n\nTITLE: Message Deserialization with msgpack\nDESCRIPTION: Shows how to configure a consumer to deserialize msgpack-encoded values automatically.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconsumer = KafkaConsumer(value_deserializer=msgpack.loads)\nconsumer.subscribe(['msgpackfoo'])\nfor msg in consumer:\n    assert isinstance(msg.value, dict)\n```\n\n----------------------------------------\n\nTITLE: Installing Latest Release of kafka-python with Pip\nDESCRIPTION: This command installs the latest stable release of kafka-python using pip package manager.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/install.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install kafka-python\n```\n\n----------------------------------------\n\nTITLE: KafkaProducer with JSON Serialization in Python\nDESCRIPTION: Demonstrates how to use a custom serialization function to send JSON messages with KafkaProducer.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport json\nproducer = KafkaProducer(value_serializer=lambda v: json.dumps(v).encode('utf-8'))\nproducer.send('fizzbuzz', {'foo': 'bar'})\n```\n\n----------------------------------------\n\nTITLE: Producer Metrics\nDESCRIPTION: Example of retrieving producer performance metrics.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmetrics = producer.metrics()\n```\n\n----------------------------------------\n\nTITLE: Getting Consumer Metrics\nDESCRIPTION: Shows how to retrieve metrics from a consumer instance.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmetrics = consumer.metrics()\n```\n\n----------------------------------------\n\nTITLE: Accessing Kafka Cluster Metadata in Python\nDESCRIPTION: Demonstrates how to use ClusterMetadata to retrieve information about brokers, topics, and partitions in a Kafka cluster.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/usage.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kafka.cluster import ClusterMetadata\n\nclusterMetadata = ClusterMetadata(bootstrap_servers=['broker1:1234'])\n\n# get all brokers metadata\nprint(clusterMetadata.brokers())\n\n# get specific broker metadata\nprint(clusterMetadata.broker_metadata('bootstrap-0'))\n\n# get all partitions of a topic\nprint(clusterMetadata.partitions_for_topic(\"topic\"))\n\n# list topics\nprint(clusterMetadata.topics())\n```\n\n----------------------------------------\n\nTITLE: Installing Bleeding-Edge Version of kafka-python\nDESCRIPTION: These commands clone the latest version of kafka-python from GitHub and install it locally.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/install.rst#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/dpkp/kafka-python\npip install ./kafka-python\n```\n\n----------------------------------------\n\nTITLE: Referencing KafkaProducer Class Documentation in reStructuredText\nDESCRIPTION: Uses reStructuredText directives to automatically generate documentation for the KafkaProducer class from the kafka-python library. The autoclass directive includes all members of the class in the documentation.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/apidoc/KafkaProducer.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: kafka.KafkaProducer\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Sphinx Autoclass Directive for KafkaConsumer Documentation\nDESCRIPTION: This RST (reStructuredText) directive instructs Sphinx to automatically generate documentation for the KafkaConsumer class from the kafka-python library. The :members: option tells Sphinx to include documentation for all public methods and attributes of the class.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/apidoc/KafkaConsumer.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: kafka.KafkaConsumer\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Including KafkaAdminClient Class Documentation in ReStructuredText\nDESCRIPTION: This RST directive includes auto-generated documentation for the KafkaAdminClient class from the kafka module. It uses the autoclass directive to automatically document all members of the class.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/apidoc/KafkaAdminClient.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: kafka.KafkaAdminClient\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Producer Implementation - Flush Before Close\nDESCRIPTION: Implementation showing proper producer cleanup by flushing pending records before closing.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/CHANGES.md#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef close(self):\n    self._flush_pending_records()\n    super().close()\n```\n\n----------------------------------------\n\nTITLE: Installing kafka-python with crc32c Support\nDESCRIPTION: This command installs kafka-python with the optional crc32c package for improved performance with Kafka 11+ brokers.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/install.rst#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install 'kafka-python[crc32c]'\n```\n\n----------------------------------------\n\nTITLE: Installing Kafka Python Client\nDESCRIPTION: Command to install the kafka-python package using pip package manager.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/README.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install kafka-python\n```\n\n----------------------------------------\n\nTITLE: Installing kafka-python via pip\nDESCRIPTION: Command to install the kafka-python library using pip package manager.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/index.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install kafka-python\n```\n\n----------------------------------------\n\nTITLE: Installing kafka-python with LZ4 Support\nDESCRIPTION: This command installs kafka-python with LZ4 compression/decompression support using the python-lz4 package.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/install.rst#2025-04-14_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npip install 'kafka-python[lz4]'\n```\n\n----------------------------------------\n\nTITLE: Installing kafka-python with ZSTD Support\nDESCRIPTION: This command installs kafka-python with ZSTD compression/decompression support using the python-zstandard package.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/install.rst#2025-04-14_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npip install 'kafka-python[zstd]'\n```\n\n----------------------------------------\n\nTITLE: Installing kafka-python with Snappy Support\nDESCRIPTION: This command installs kafka-python with Snappy compression support using the python-snappy module.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/install.rst#2025-04-14_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install 'kafka-python[snappy]'\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for kafka-python\nDESCRIPTION: Command to run integration tests with a specific Kafka version. This will download Kafka server binaries if needed and set up Kafka and Zookeeper fixtures for testing.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/tests.rst#2025-04-14_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nKAFKA_VERSION=4.0.0 make test\n```\n\n----------------------------------------\n\nTITLE: Consumer Implementation - Empty Set Return\nDESCRIPTION: Implementation for handling non-existent topics in consumer partitions query.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/CHANGES.md#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef partitions_for_topic(self, topic):\n    if topic not in self.topics():\n        return set()\n```\n\n----------------------------------------\n\nTITLE: BrokerConnection Blocking Connect\nDESCRIPTION: Code implementing blocking connection method for multi-address hostname bootstrap support.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/changelog.rst#2025-04-14_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBrokerConnection.connect_blocking()\n```\n\n----------------------------------------\n\nTITLE: Configuration Example - Client ID Override in Tests\nDESCRIPTION: Example showing how to set a thread-specific client_id in test configurations.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/CHANGES.md#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclient_id = f\"test-consumer-{threading.get_ident()}\"\n```\n\n----------------------------------------\n\nTITLE: GSSAPI Security Layer Negotiation\nDESCRIPTION: Implementation of security layer negotiation for GSSAPI authentication in SASL support.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/changelog.rst#2025-04-14_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nAdd security layer negotiation to the GSSAPI authentication\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for kafka-python\nDESCRIPTION: Examples showing how to run specific unit tests using either pytest directly or make. The first command runs protocol tests only, while the second runs connection tests via the make command.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/tests.rst#2025-04-14_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# run protocol tests only (via pytest)\npytest test/test_protocol.py\n\n# Run conn tests only (via make)\nPYTESTS=test/test_conn.py make test\n```\n\n----------------------------------------\n\nTITLE: Referencing KafkaClient Class Documentation with Sphinx in RST\nDESCRIPTION: A Sphinx documentation directive that automatically generates documentation for the KafkaClient class from the kafka module. The :members: flag instructs Sphinx to include all class methods in the generated documentation.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/apidoc/KafkaClient.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: kafka.KafkaClient\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Setting up Sphinx autoclass documentation for kafka.BrokerConnection in RST\nDESCRIPTION: This RST (reStructuredText) snippet configures Sphinx to automatically generate documentation for the BrokerConnection class from the kafka module. The :members: directive ensures all public members of the class are included in the documentation.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/apidoc/BrokerConnection.rst#2025-04-14_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autoclass:: kafka.BrokerConnection\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Sphinx Directive for Documenting ClusterMetadata Class\nDESCRIPTION: This restructuredText snippet uses the Sphinx autoclass directive to automatically generate documentation for the ClusterMetadata class from the kafka.cluster module, including all of its members.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/apidoc/ClusterMetadata.rst#2025-04-14_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: kafka.cluster.ClusterMetadata\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Installing Snappy Development Libraries on Ubuntu\nDESCRIPTION: This command installs the Snappy development libraries on Ubuntu systems, which are required for Snappy compression support.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/install.rst#2025-04-14_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\napt-get install libsnappy-dev\n```\n\n----------------------------------------\n\nTITLE: Installing Snappy Development Libraries on macOS\nDESCRIPTION: This command installs the Snappy development libraries on macOS systems using Homebrew.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/install.rst#2025-04-14_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nbrew install snappy\n```\n\n----------------------------------------\n\nTITLE: Handling Array Encoding in Python\nDESCRIPTION: This code fixes a TypeError related to array encoding where dict_itemiterator objects have no len().\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/CHANGES.md#2025-04-14_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nFix array encoding TypeError: object of type 'dict_itemiterator' has no len()\n```\n\n----------------------------------------\n\nTITLE: Installing Test Dependencies for kafka-python\nDESCRIPTION: Command to install development dependencies required for running tests. This installs all packages specified in the requirements-dev.txt file.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/tests.rst#2025-04-14_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements-dev.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Sphinx Documentation Dependencies for kafka-python\nDESCRIPTION: This snippet defines the exact versions of Sphinx and its theme required for building documentation. It also includes a commented instruction for installing kafka-python in editable mode to allow Sphinx to extract docstrings.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/requirements.txt#2025-04-14_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nsphinx==8.1.3\nsphinx_rtd_theme==3.0.2\n\n# Install kafka-python in editable mode\n# This allows the sphinx autodoc module\n# to load the Python modules and extract docstrings.\n# -e ..\n```\n\n----------------------------------------\n\nTITLE: Updating Sensors Fetch Lag in Python\nDESCRIPTION: This snippet checks if the unpacked list contains elements before trying to update sensors fetch lag.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/CHANGES.md#2025-04-14_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nOnly try to update sensors fetch lag if the unpacked list contains elements\n```\n\n----------------------------------------\n\nTITLE: Updating Dict Items in Python\nDESCRIPTION: This snippet demonstrates the use of six.viewitems() instead of six.iteritems() to avoid encoding problems in the StickyPartitionAssignor.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/CHANGES.md#2025-04-14_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nUse six.viewitems instead of six.iteritems to avoid encoding problems in StickyPartitionAssignor\n```\n\n----------------------------------------\n\nTITLE: Installing Snappy from Source\nDESCRIPTION: These commands download, build, and install Snappy from source code. This is useful for systems where pre-built packages are not available.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/install.rst#2025-04-14_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/google/snappy/releases/download/1.1.3/snappy-1.1.3.tar.gz\ntar xzvf snappy-1.1.3.tar.gz\ncd snappy-1.1.3\n./configure\nmake\nsudo make install\n```\n\n----------------------------------------\n\nTITLE: Using isinstance for CRC32 in Python\nDESCRIPTION: This code uses isinstance() in the built-in crc32 function for type checking.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/CHANGES.md#2025-04-14_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nUse isinstance in builtin crc32\n```\n\n----------------------------------------\n\nTITLE: Version Check Pattern - Python Socket Connection\nDESCRIPTION: Code snippet showing connection cleanup pattern with selector closing, referenced in version 1.4.2 bugfixes.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/changelog.rst#2025-04-14_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nClose leaked selector in version check\n```\n\n----------------------------------------\n\nTITLE: Fixing SocketPair CVE in Python\nDESCRIPTION: This update addresses the CVE-2024-3219 vulnerability in the socketpair function.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/CHANGES.md#2025-04-14_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nUpdate socketpair w/ CVE-2024-3219 fix\n```\n\n----------------------------------------\n\nTITLE: Requirements List for Kafka Python Client\nDESCRIPTION: Comprehensive list of Python package dependencies including testing tools (pytest, mock), code quality tools (flake8, pylint), compression libraries (lz4, snappy, zstandard), documentation generators (Sphinx), and other utilities. Some dependencies are version-specific, like mock for Python < 3.3.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/requirements-dev.txt#2025-04-14_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncoveralls\ncrc32c\ndocker-py\nflake8\nlz4\nmock; python_version < '3.3'\npy\npylint\npyperf\npytest\npytest-cov\npytest-mock\npytest-pylint\npytest-timeout\npython-snappy\nSphinx\nsphinx-rtd-theme\nxxhash\nzstandard\n```\n\n----------------------------------------\n\nTITLE: Version Control Reference - Kafka-Python Release Tags\nDESCRIPTION: Version control reference showing the sequential releases 1.4.4 (Nov 20, 2018), 1.4.3 (May 26, 2018), 1.4.2 (Mar 10, 2018), and 1.4.1 (Feb 9, 2018) with their respective release dates.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/CHANGES.md#2025-04-14_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\n# 1.4.4 (Nov 20, 2018)\n# 1.4.3 (May 26, 2018)\n# 1.4.2 (Mar 10, 2018)\n# 1.4.1 (Feb 9, 2018)\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents for Kafka-Python API Documentation in reStructuredText\nDESCRIPTION: This snippet defines the table of contents for the kafka-python API documentation using reStructuredText syntax. It lists the main components of the library that are documented separately.\nSOURCE: https://github.com/dpkp/kafka-python/blob/master/docs/apidoc/modules.rst#2025-04-14_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n\n   KafkaConsumer\n   KafkaProducer\n   KafkaAdminClient\n   KafkaClient\n   BrokerConnection\n   ClusterMetadata\n```"
  }
]